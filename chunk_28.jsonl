{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompile all of the contracts in the self. contracts_dir directory and writes the result to the output_dir.", "response": "def compile_all(self):\n        \"\"\"Compiles all of the contracts in the self.contracts_dir directory\n\n        Creates {contract name}.json files in self.output_dir that contain\n        the build output for each contract.\n        \"\"\"\n\n        # Solidity input JSON\n        solc_input = self.get_solc_input()\n\n        # Compile the contracts\n        real_path = os.path.realpath(self.contracts_dir)\n        compilation_result = compile_standard(solc_input, allow_paths=real_path)\n\n        # Create the output folder if it doesn't already exist\n        os.makedirs(self.output_dir, exist_ok=True)\n\n        # Write the contract ABI to output files\n        compiled_contracts = compilation_result['contracts']\n        for contract_file in compiled_contracts:\n            for contract in compiled_contracts[contract_file]:\n                contract_name = contract.split('.')[0]\n                contract_data = compiled_contracts[contract_file][contract_name]\n\n                contract_data_path = self.output_dir + '/{0}.json'.format(contract_name)\n                with open(contract_data_path, \"w+\") as contract_data_file:\n                    json.dump(contract_data, contract_data_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the contract data for a given contract name", "response": "def get_contract_data(self, contract_name):\n        \"\"\"Returns the contract data for a given contract\n\n        Args:\n            contract_name (str): Name of the contract to return.\n\n        Returns:\n            str, str: ABI and bytecode of the contract\n        \"\"\"\n\n        contract_data_path = self.output_dir + '/{0}.json'.format(contract_name)\n        with open(contract_data_path, 'r') as contract_data_file:\n            contract_data = json.load(contract_data_file)\n\n        abi = contract_data['abi']\n        bytecode = contract_data['evm']['bytecode']['object']\n\n        return abi, bytecode"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exception(maxTBlevel=None):\n\n    try:\n        from marrow.util.bunch import Bunch\n\n        cls, exc, trbk = sys.exc_info()\n        excName = cls.__name__\n        excArgs = getattr(exc, 'args', None)\n\n        excTb = ''.join(traceback.format_exception(cls, exc, trbk, maxTBlevel))\n\n        return Bunch(\n                name=excName,\n                cls=cls,\n                exception=exc,\n                trace=trbk,\n                formatted=excTb,\n                args=excArgs\n            )\n\n    finally:\n        del cls, exc, trbk", "response": "Retrieve useful information about an exception."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a given string into a native string.", "response": "def native(s, encoding='utf-8', fallback='iso-8859-1'):\n    \"\"\"Convert a given string into a native string.\"\"\"\n\n    if isinstance(s, str):\n        return s\n\n    if str is unicode:  # Python 3.x ->\n        return unicodestr(s, encoding, fallback)\n\n    return bytestring(s, encoding, fallback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unicodestr(s, encoding='utf-8', fallback='iso-8859-1'):\n\n    if isinstance(s, unicode):\n        return s\n\n    try:\n        return s.decode(encoding)\n    except UnicodeError:\n        return s.decode(fallback)", "response": "Convert a string to unicode if it isn t already."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uvalues(a, encoding='utf-8', fallback='iso-8859-1'):\n\n    try:\n        return encoding, [s.decode(encoding) for s in a]\n\n    except UnicodeError:\n        return fallback, [s.decode(fallback) for s in a]", "response": "Return a list of decoded values from an iterator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build_query(self):\n        if isinstance(self._query_string, QueryString):\n            self._query_dsl = self._query_string\n        elif isinstance(self._query_string, string_types):\n            self._query_dsl = QueryString(self._query_string)\n        else:\n            self._query_dsl = MatchAll()", "response": "Builds the base query dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_filtered_query(self, f, operator):\n        self._filtered = True\n        if isinstance(f, Filter):\n            filter_object = f\n        else:\n            filter_object = Filter(operator).filter(f)\n        self._filter_dsl = filter_object", "response": "Build the filtered query"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a filter to the query", "response": "def filter(self, f, operator=\"and\"):\n        \"\"\"\n        Add a filter to the query\n\n        Takes a Filter object, or a filterable DSL object.\n        \"\"\"\n        if self._filtered:\n            self._filter_dsl.filter(f)\n        else:\n            self._build_filtered_query(f, operator)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nraising TypeError if value doesn t satisfy the constraints Taxonomy", "response": "def check(self, inst, value):\n        \"\"\"Raise TypeError if value doesn't satisfy the constraints\n        for use on instance inst.\n        \"\"\"\n        if not (self.or_none and value is None):\n            if self.seq:\n                self.checktype_seq(value, self.kind,\n                                   unique=self.unique, inst=inst)\n            else:\n                self.checktype(value, self.kind, inst=inst)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn value or a normalized form of it for use on instance inst.", "response": "def normalize(self, inst, value):\n        \"\"\"Return value or a normalized form of it for use on\n        instance inst.\n        \"\"\"\n        if (not (self.or_none and value is None) and\n            self.seq):\n            value = tuple(value)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the classification decision boundary of model on X with labels y.", "response": "def plot_decision_boundary(model, X, y, step=0.1, figsize=(10, 8), alpha=0.4, size=20):\n    \"\"\"Plots the classification decision boundary of `model` on `X` with labels `y`.\n    Using numpy and matplotlib.\n    \"\"\"\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, step),\n                         np.arange(y_min, y_max, step))\n\n    f, ax = plt.subplots(figsize=figsize)\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    ax.contourf(xx, yy, Z, alpha=alpha)\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=size, edgecolor='k')\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nensures that the number of positional arguments provided satisfies conditions.", "response": "def ensure_argcount(args, min_=None, max_=None):\n    \"\"\"Checks whether iterable of positional arguments satisfies conditions.\n\n    :param args: Iterable of positional arguments, received via ``*args``\n    :param min_: Minimum number of arguments\n    :param max_: Maximum number of arguments\n\n    :return: ``args`` if the conditions are met\n    :raise TypeError: When conditions are not met\n    \"\"\"\n    ensure_sequence(args)\n\n    has_min = min_ is not None\n    has_max = max_ is not None\n    if not (has_min or has_max):\n        raise ValueError(\n            \"minimum and/or maximum number of arguments must be provided\")\n    if has_min and has_max and min_ > max_:\n        raise ValueError(\n            \"maximum number of arguments must be greater or equal to minimum\")\n\n    if has_min and len(args) < min_:\n        raise TypeError(\n            \"expected at least %s arguments, got %s\" % (min_, len(args)))\n    if has_max and len(args) > max_:\n        raise TypeError(\n            \"expected at most %s arguments, got %s\" % (max_, len(args)))\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether the keyword arguments provided by kwargs satisfies conditions.", "response": "def ensure_keyword_args(kwargs, mandatory=(), optional=()):\n    \"\"\"Checks whether dictionary of keyword arguments satisfies conditions.\n\n    :param kwargs: Dictionary of keyword arguments, received via ``*kwargs``\n    :param mandatory: Iterable of mandatory argument names\n    :param optional: Iterable of optional argument names\n\n    :return: ``kwargs`` if the conditions are met:\n             all ``mandatory`` arguments are present, and besides that\n             no arguments outside of ``optional`` ones are.\n\n    :raise TypeError: When conditions are not met\n    \"\"\"\n    from taipan.strings import ensure_string\n\n    ensure_mapping(kwargs)\n    mandatory = list(map(ensure_string, ensure_iterable(mandatory)))\n    optional = list(map(ensure_string, ensure_iterable(optional)))\n    if not (mandatory or optional):\n        raise ValueError(\n            \"mandatory and/or optional argument names must be provided\")\n\n    names = set(kwargs)\n    for name in mandatory:\n        try:\n            names.remove(name)\n        except KeyError:\n            raise TypeError(\n                \"no value for mandatory keyword argument '%s'\" % name)\n\n    excess = names - set(optional)\n    if excess:\n        if len(excess) == 1:\n            raise TypeError(\"unexpected keyword argument '%s'\" % excess.pop())\n        else:\n            raise TypeError(\n                \"unexpected keyword arguments: %s\" % (tuple(excess),))\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses an http call against the hipchat api", "response": "def _api_call(self, path, data={}, http_method=requests.get):\n        ''' Process an http call against the hipchat api '''\n        log.info('performing api request', path=path)\n        response = http_method('/'.join([self.api_url, path]),\n                               params={'auth_token': self.api_key},\n                               data=data)\n        log.debug('{} remaining calls'.format(\n            response.headers['x-ratelimit-remaining']))\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef message(self, body, room_id, style='text'):\n        ''' Send a message to the given room '''\n        # TODO Automatically detect body format ?\n        path = 'rooms/message'\n        data = {\n            'room_id': room_id,\n            'message': body,\n            'from': self.name,\n            'notify': 1,\n            'message_format': style,\n            'color': self.bg_color\n        }\n        log.info('sending message to hipchat', message=body, room=room_id)\n        feedback = self._api_call(path, data, requests.post)\n        log.debug(feedback)\n        return feedback", "response": "Send a message to the given room"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef status(self):\n        if self.cluster_id == self.NULL_CLUSTER_ID:\n            return \"Unexpanded\"\n\n        status_dict = self.statuses\n        # determine job status\n        status = \"Various\"\n        for key, val in status_dict.items():\n            if val == self.num_jobs:\n                status = key\n        return status", "response": "The status of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef job_file(self):\n        job_file_name = '%s.job' % (self.name)\n        job_file_path = os.path.join(self.initial_dir, job_file_name)\n        self._job_file = job_file_path\n        return self._job_file", "response": "The path to the submit description file representing this job."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_file(self):\n        log_file = self.get('log')\n        if not log_file:\n            log_file = '%s.log' % (self.name)\n            self.set('log', log_file)\n        return os.path.join(self.initial_dir, self.get('log'))", "response": "The path to the log file for this job."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit the job locally or to a remote server if it is defined.", "response": "def submit(self, queue=None, options=[]):\n        \"\"\"Submits the job either locally or to a remote server if it is defined.\n\n        Args:\n            queue (int, optional): The number of sub-jobs to run. This argmuent will set the num_jobs attribute of\n                this object. Defaults to None, meaning the value of num_jobs will be used.\n            options (list of str, optional): A list of command line options for the condor_submit command. For\n                details on valid options see: http://research.cs.wisc.edu/htcondor/manual/current/condor_submit.html.\n                Defaults to an empty list.\n\n        \"\"\"\n        if not self.executable:\n            log.error('Job %s was submitted with no executable', self.name)\n            raise NoExecutable('You cannot submit a job without an executable')\n\n        self._num_jobs = queue or self.num_jobs\n\n        self._write_job_file()\n\n        args = ['condor_submit']\n        args.extend(options)\n        args.append(self.job_file)\n\n        log.info('Submitting job %s with options: %s', self.name, args)\n        return super(Job, self).submit(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwaits for the job or a sub - job to complete.", "response": "def wait(self, options=[], sub_job_num=None):\n        \"\"\"Wait for the job, or a sub-job to complete.\n\n        Args:\n            options (list of str, optional): A list of command line options for the condor_wait command. For\n                details on valid options see: http://research.cs.wisc.edu/htcondor/manual/current/condor_wait.html.\n                Defaults to an empty list.\n            job_num (int, optional): The number\n        \"\"\"\n        args = ['condor_wait']\n        args.extend(options)\n        job_id = '%s.%s' % (self.cluster_id, sub_job_num) if sub_job_num else str(self.cluster_id)\n        if self._remote:\n            abs_log_file = self.log_file\n        else:\n            abs_log_file = os.path.abspath(self.log_file)\n        args.extend([abs_log_file, job_id])\n        out, err = self._execute(args)\n        return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the value of an attribute from submit description file.", "response": "def get(self, attr, value=None, resolve=True):\n        \"\"\"Get the value of an attribute from submit description file.\n\n        Args:\n            attr (str): The name of the attribute whose value should be returned.\n            value (str, optional): A default value to return if 'attr' doesn't exist. Defaults to None.\n            resolve (bool, optional): If True then resolve references to other attributes in the value of 'attr'. If\n                False then return the raw value of 'attr'. Defaults to True.\n\n        Returns:\n            str: The value assigned to 'attr' if 'attr' exists, otherwise 'value'.\n        \"\"\"\n        try:\n            if resolve:\n                value = self._resolve_attribute(attr)\n            else:\n                value = self.attributes[attr]\n        except KeyError:\n            pass\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the value of an attribute in the submit description file.", "response": "def set(self, attr, value):\n        \"\"\"Set the value of an attribute in the submit description file.\n\n        The value can be passed in as a Python type (i.e. a list, a tuple or a Python boolean).\n        The Python values will be reformatted into strings based on the standards described in\n        the HTCondor manual: http://research.cs.wisc.edu/htcondor/manual/current/condor_submit.html\n\n        Args:\n            attr (str): The name of the attribute to set.\n            value (str): The value to assign to 'attr'.\n\n        \"\"\"\n\n        def escape_new_syntax(value, double_quote_escape='\"'):\n            value = str(value)\n            value = value.replace(\"'\", \"''\")\n            value = value.replace('\"', '%s\"' % double_quote_escape)\n            if ' ' in value or '\\t' in value:\n                value = \"'%s'\" % value\n            return value\n\n        def escape_new_syntax_pre_post_script(value):\n            return escape_new_syntax(value, '\\\\')\n\n        def escape_remap(value):\n            value = value.replace('=', '\\=')\n            value = value.replace(';', '\\;')\n            return value\n\n        def join_function_template(join_string, escape_func):\n            return lambda value: join_string.join([escape_func(i) for i in value])\n\n        def quote_join_function_template(join_string, escape_func):\n            return lambda value: join_function_template(join_string, escape_func)(value)\n\n        join_functions = {'rempas': quote_join_function_template('; ', escape_remap),\n                          'arguments': quote_join_function_template(' ', escape_new_syntax),\n                          'Arguments': quote_join_function_template(' ', escape_new_syntax_pre_post_script)\n                          }\n\n        if value is False:\n            value = 'false'\n        elif value is True:\n            value = 'true'\n        elif isinstance(value, list) or isinstance(value, tuple):\n            join_function = join_function_template(', ', str)\n            for key in list(join_functions.keys()):\n                if attr.endswith(key):\n                    join_function = join_functions[key]\n            value = join_function(value)\n\n        self.attributes[attr] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the current status of the job.", "response": "def _update_status(self, sub_job_num=None):\n        \"\"\"Gets the job status.\n\n        Return:\n            str: The current status of the job\n\n        \"\"\"\n        job_id = '%s.%s' % (self.cluster_id, sub_job_num) if sub_job_num else str(self.cluster_id)\n        format = ['-format', '\"%d\"', 'JobStatus']\n        cmd = 'condor_q {0} {1} && condor_history {0} {1}'.format(job_id, ' '.join(format))\n        args = [cmd]\n        out, err = self._execute(args, shell=True, run_in_job_dir=False)\n        if err:\n            log.error('Error while updating status for job %s: %s', job_id, err)\n            raise HTCondorError(err)\n        if not out:\n            log.error('Error while updating status for job %s: Job not found.', job_id)\n            raise HTCondorError('Job not found.')\n\n        out = out.replace('\\\"', '')\n        log.info('Job %s status: %s', job_id, out)\n\n        if not sub_job_num:\n            if len(out) >= self.num_jobs:\n                out = out[:self.num_jobs]\n            else:\n                msg = 'There are {0} sub-jobs, but {1} status(es).'.format(self.num_jobs, len(out))\n                log.error(msg)\n                raise HTCondorError(msg)\n\n        #initialize status dictionary\n        status_dict = dict()\n        for val in CONDOR_JOB_STATUSES.values():\n            status_dict[val] = 0\n\n        for status_code_str in out:\n            status_code = 0\n            try:\n                status_code = int(status_code_str)\n            except ValueError:\n                pass\n            key = CONDOR_JOB_STATUSES[status_code]\n            status_dict[key] += 1\n\n        return status_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resolve_attribute_match(self, match):\n        if match.group(1) == 'cluster':\n            return str(self.cluster_id)\n\n        return self.get(match.group(1), match.group(0))", "response": "Resolves a match object to a value of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hosts(self, **kwargs):\n        kwargs['channelID'] = self.id\n        return self.connection.listHosts(**kwargs)", "response": "Convenience wrapper around listHosts(...) for this channel ID.\n\n        :param **kwargs: keyword arguments to the listHosts RPC.\n        :returns: deferred that when fired returns a list of hosts (dicts)."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a deferred that when fired returns a list of Tasks.", "response": "def tasks(self, **opts):\n        \"\"\"\n        Convenience wrapper around listTasks(...) for this channel ID.\n\n        Tasks are sorted by priority and creation time.\n\n        :param **opts: \"opts\" dict to the listTasks RPC.\n                       For example, \"state=[task_states.OPEN]\" will return\n                       only the \"OPEN\" tasks.\n        :returns: deferred that when fired returns a list of Tasks.\n        \"\"\"\n        opts['channel_id'] = self.id\n        qopts = {'order': 'priority,create_time'}\n        return self.connection.listTasks(opts, qopts)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef total_capacity(self):\n        # Ensure this task's channel has spare capacity for this task.\n        total_capacity = 0\n        hosts = yield self.hosts(enabled=True)\n        for host in hosts:\n            total_capacity += host.capacity\n        defer.returnValue(total_capacity)", "response": "Get the total task capacity available for this channel."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef record_example(self, example):\n        self.num_examples += 1\n        self.total_real_time += example.real_time\n        self.total_user_time += example.user_time\n        if not example.error:\n            self.num_passed += 1\n            self._write_example_passed(example)\n        else:\n            self.num_failed += 1\n            self._write_example_failed(example)\n            error = ErrorFormat(\n                example.name, example.error, example.traceback,\n                tuple(self.group_stack), example.stdout, example.stderr\n            )\n            self.errors.append(error)\n        return not example.error", "response": "Records an example s results. Returns True if the example passed and False if the example failed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef corpus2clusters(corpus, index,params_clustermap={'vmin':0,'vmax':1,'figsize':[6,6]}):\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    vect = TfidfVectorizer()\n    tfidf = vect.fit_transform(corpus)\n\n    df=pd.DataFrame((tfidf * tfidf.T).A,\n                index=index,\n                columns=index,\n                )\n    clustergrid=sns.clustermap(df,**params_clustermap\n    #                                method='complete', metric='canberra',\n                                  )\n    dclusters=get_clusters(clustergrid,axis=0,criterion='maxclust',clusters_fraction=0.25)\n    return dclusters", "response": "This function takes a list of strings and returns a list of cluster indices for each element in the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngoogling something. Syntax: {command} <term>", "response": "def cmd(send, msg, args):\n    \"\"\"Googles something.\n\n    Syntax: {command} <term>\n\n    \"\"\"\n    if not msg:\n        send(\"Google what?\")\n        return\n    key = args['config']['api']['googleapikey']\n    cx = args['config']['api']['googlesearchid']\n    data = get('https://www.googleapis.com/customsearch/v1', params={'key': key, 'cx': cx, 'q': msg}).json()\n    if 'items' not in data:\n        send(\"Google didn't say much.\")\n    else:\n        url = data['items'][0]['link']\n        send(\"Google says %s\" % url)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the list of items from an apiv2 request object", "response": "def read_list(self, request):\n        \"\"\"\n        :param request: an apiv2 request object\n        :return: request if successful with entities set on request\n        \"\"\"\n        request_filters = request.context_params.get(\n            self.request_filters_property, {})\n        request_filters.update(**self.get_limit_and_offset(request_filters))\n        entities = self.get_entity_list(request, **request_filters)\n        request.context_params[self.list_property_name] = entities\n\n        # offset and limit don't make sense to get aggregates\n        count_request_filters = request_filters.copy()\n        count_request_filters.pop('offset', None)\n        count_request_filters.pop('limit', None)\n        count_request_filters.pop('order_by', None)\n        total_count = self.get_entity_list_total_count(request,\n                                                       **count_request_filters)\n\n        request.context_params[self.entity_list_total_count_property_name] = \\\n            total_count\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the detail property of the object in the request.", "response": "def update_detail(self, request):\n        \"\"\"\n        :param request: an apiv2 request object\n        :return: request if successful with entities set on request\n        \"\"\"\n        entity = request.context_params[self.detail_property_name]\n        updated_entity = self.update_entity(\n            request,\n            entity, **request.context_params['data'])\n        request.context_params[self.updated_property_name] = updated_entity\n        return request"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_detail(self, request):\n        entity = self.create_entity(request, **request.context_params['data'])\n        request.context_params[self.detail_property_name] = entity\n        return request", "response": "Creates an object of the appropriate type for the detail property."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmd(send, msg, args):\n    if args['type'] == 'privmsg':\n        send(\"Note-passing should be done in public.\")\n        return\n    arguments = msg.split()\n    if len(arguments) > 1:\n        send(\"Sorry, I can only perform the summoning ritual for one person at a time\")\n        return\n    elif len(arguments) == 0:\n        send(\"Who shall I summon?\")\n        return\n    nick = arguments[0]\n    message = \"You have been summoned!\"\n    row = Notes(note=message, submitter=\"The Dark Gods\", nick=nick, time=datetime.now())\n    args['db'].add(row)\n    send(\"%s has been summoned!\" % nick)", "response": "Summons a user\n    Syntax: {command} <nick>"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_deps(cfg=None,deps=[]):\n    if not cfg is None:\n        if not 'deps' in cfg:\n            cfg['deps']=deps\n        else:\n            deps=cfg['deps']\n    if not len(deps)==0:\n        for dep in deps:\n            if not dep in cfg:\n                runbashcmd(f'conda install {dep}',test=cfg['test'])\n                cfg[dep]=dep\n    logging.info(f\"{len(deps)} deps installed.\")\n    return cfg", "response": "Installs conda dependencies.\n\n    :param cfg: configuration dict"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mont_pub_from_mont_priv(cls, mont_priv):\n\n        if not isinstance(mont_priv, bytes):\n            raise TypeError(\"Wrong type passed for the mont_priv parameter.\")\n\n        if len(mont_priv) != cls.MONT_PRIV_KEY_SIZE:\n            raise ValueError(\"Invalid value passed for the mont_priv parameter.\")\n\n        return bytes(cls._mont_pub_from_mont_priv(bytearray(mont_priv)))", "response": "Restore the Montgomery public key from a Montgomery private key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mont_priv_to_ed_pair(cls, mont_priv):\n\n        if not isinstance(mont_priv, bytes):\n            raise TypeError(\"Wrong type passed for the mont_priv parameter.\")\n\n        if len(mont_priv) != cls.MONT_PRIV_KEY_SIZE:\n            raise ValueError(\"Invalid value passed for the mont_priv parameter.\")\n\n        ed_priv, ed_pub = cls._mont_priv_to_ed_pair(bytearray(mont_priv))\n\n        return bytes(ed_priv), bytes(ed_pub)", "response": "Derive a Twisted Edwards key pair from a Montgomery private key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mont_pub_to_ed_pub(cls, mont_pub):\n\n        if not isinstance(mont_pub, bytes):\n            raise TypeError(\"Wrong type passed for the mont_pub parameter.\")\n\n        if len(mont_pub) != cls.MONT_PUB_KEY_SIZE:\n            raise ValueError(\"Invalid value passed for the mont_pub parameter.\")\n\n        return bytes(cls._mont_pub_to_ed_pub(bytearray(mont_pub)))", "response": "Derive a Twisted Edwards public key from a Montgomery public key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sign(self, data, nonce = None):\n\n        cls = self.__class__\n\n        if not self.__mont_priv:\n            raise MissingKeyException(\n                \"Cannot sign using this XEdDSA instance, Montgomery private key missing.\"\n            )\n\n        if not isinstance(data, bytes):\n            raise TypeError(\"The data parameter must be a bytes-like object.\")\n\n        if nonce == None:\n            nonce = os.urandom(64)\n\n        if not isinstance(nonce, bytes):\n            raise TypeError(\"Wrong type passed for the nonce parameter.\")\n\n        if len(nonce) != 64:\n            raise ValueError(\"Invalid value passed for the nonce parameter.\")\n\n        ed_priv, ed_pub = cls._mont_priv_to_ed_pair(bytearray(self.__mont_priv))\n\n        return bytes(cls._sign(\n            bytearray(data),\n            bytearray(nonce),\n            ed_priv,\n            ed_pub\n        ))", "response": "Signs the data using the Montgomery private key stored by this instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify the data using the Montgomery public key stored by this instance.", "response": "def verify(self, data, signature):\n        \"\"\"\n        Verify signed data using the Montgomery public key stored by this XEdDSA instance.\n\n        :param data: A bytes-like object containing the data that was signed.\n        :param signature: A bytes-like object encoding the signature with length\n            SIGNATURE_SIZE.\n        :returns: A boolean indicating whether the signature was valid or not.\n        \"\"\"\n\n        cls = self.__class__\n\n        if not isinstance(data, bytes):\n            raise TypeError(\"The data parameter must be a bytes-like object.\")\n\n        if not isinstance(signature, bytes):\n            raise TypeError(\"Wrong type passed for the signature parameter.\")\n\n        if len(signature) != cls.SIGNATURE_SIZE:\n            raise ValueError(\"Invalid value passed for the signature parameter.\")\n\n        return cls._verify(\n            bytearray(data),\n            bytearray(signature),\n            cls._mont_pub_to_ed_pub(bytearray(self.__mont_pub))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(argv=None):\n    if not argv:\n        argv = sys.argv[1:]\n\n    parser = argparse.ArgumentParser(description=_HELP_TEXT)\n    parser.add_argument('input', nargs='?', default=None)\n    parser.add_argument('output', nargs='?', default=None)\n    parser.add_argument('--version', action='version',\n                        version='%(prog)s ' + get_version())\n    args = parser.parse_args(argv)\n\n    if not args.input:\n        print(\"No file specified\", file=sys.stderr)\n        sys.exit(1)\n\n    try:\n        with open(args.input, 'r') as f:\n            res = mangle(f.read())\n            if not args.output:\n                print(res, end='')\n            else:\n                with open(args.output, 'w') as o:\n                    o.write(res)\n    except Exception as ex:\n        print(\"Error mangling {}: {!s}\".format(args.input, ex),\n              file=sys.stderr)\n        sys.exit(1)", "response": "Command line entry point for the\nInsights command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting histograms from a ROOT file and make them available in a dict.", "response": "def get_histograms_in_list(filename: str, list_name: str = None) -> Dict[str, Any]:\n    \"\"\" Get histograms from the file and make them available in a dict.\n\n    Lists are recursively explored, with all lists converted to dictionaries, such that the return\n    dictionaries which only contains hists and dictionaries of hists (ie there are no ROOT ``TCollection``\n    derived objects).\n\n    Args:\n        filename: Filename of the ROOT file containing the list.\n        list_name: Name of the list to retrieve.\n    Returns:\n        Contains hists with keys as their names. Lists are recursively added, mirroring\n            the structure under which the hists were stored.\n    Raises:\n        ValueError: If the list could not be found in the given file.\n    \"\"\"\n    hists: dict = {}\n    with RootOpen(filename = filename, mode = \"READ\") as fIn:\n        if list_name is not None:\n            hist_list = fIn.Get(list_name)\n        else:\n            hist_list = [obj.ReadObj() for obj in fIn.GetListOfKeys()]\n\n        if not hist_list:\n            fIn.ls()\n            # Closing this file appears (but is not entirely confirmed) to be extremely important! Otherwise,\n            # the memory will leak, leading to ROOT memory issues!\n            fIn.Close()\n            raise ValueError(f\"Could not find list with name \\\"{list_name}\\\". Possible names are listed above.\")\n\n        # Retrieve objects in the hist list\n        for obj in hist_list:\n            _retrieve_object(hists, obj)\n\n    return hists"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _retrieve_object(output_dict: Dict[str, Any], obj: Any) -> None:\n    import ROOT\n\n    # Store TH1 or THn\n    if isinstance(obj, ROOT.TH1) or isinstance(obj, ROOT.THnBase):\n        # Ensure that it is not lost after the file is closed\n        # Only works for TH1\n        if isinstance(obj, ROOT.TH1):\n            obj.SetDirectory(0)\n\n        # Explicitly note that python owns the object\n        # From more on memory management with ROOT and python, see:\n        # https://root.cern.ch/root/html/guides/users-guide/PythonRuby.html#memory-handling\n        ROOT.SetOwnership(obj, False)\n\n        # Store the object\n        output_dict[obj.GetName()] = obj\n\n    # Recurse over lists\n    if isinstance(obj, ROOT.TCollection):\n        # Keeping it in order simply makes it easier to follow\n        output_dict[obj.GetName()] = {}\n        # Iterate over the objects in the collection and recursively store them\n        for obj_temp in list(obj):\n            _retrieve_object(output_dict[obj.GetName()], obj_temp)", "response": "Recursively retrieve a ROOT object from a ROOT file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract x y and bin values from a ROOT. TH2 histogram and return a numpy array of the corresponding values.", "response": "def get_array_from_hist2D(hist: Hist, set_zero_to_NaN: bool = True, return_bin_edges: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\" Extract x, y, and bin values from a 2D ROOT histogram.\n\n    Converts the histogram into a numpy array, and suitably processes it for a surface plot\n    by removing 0s (which can cause problems when taking logs), and returning a set of (x, y) mesh\n    values utilziing either the bin edges or bin centers.\n\n    Note:\n        This is a different format than the 1D version!\n\n    Args:\n        hist (ROOT.TH2): Histogram to be converted.\n        set_zero_to_NaN: If true, set 0 in the array to NaN. Useful with matplotlib so that it will\n            ignore the values when plotting. See comments in this function for more details. Default: True.\n        return_bin_edges: Return x and y using bin edges instead of bin centers.\n    Returns:\n        Contains (x values, y values, numpy array of hist data) where (x, y) are values on a\n            grid (from np.meshgrid) using the selected bin values.\n    \"\"\"\n    # Process the hist into a suitable state\n    # NOTE: The shape specific can be somewhat confusing (ie. I would naviely expected to specify the x first.)\n    # This says that the ``GetYaxis().GetNbins()`` number of rows and ``GetXaxis().GetNbins()`` number of columns.\n    shape = (hist.GetYaxis().GetNbins(), hist.GetXaxis().GetNbins())\n    # To keep consistency with the root_numpy 2D hist format, we transpose the final result\n    # This format has x values as columns.\n    hist_array = np.array([hist.GetBinContent(x) for x in range(1, hist.GetNcells()) if not hist.IsBinUnderflow(x) and not hist.IsBinOverflow(x)])\n    # The hist_array was linear, so we need to shape it into our expected 2D values.\n    hist_array = hist_array.reshape(shape)\n    # Transpose the array to better match expectations\n    # In particular, by transposing the array, it means that ``thist_array[1][0]`` gives the 2nd x\n    # value (x_index = 1) and the 1st y value (y_index = 1). This is as we would expect. This is also\n    # the same convention as used by root_numpy\n    hist_array = hist_array.T\n    # Set all 0s to nan to get similar behavior to ROOT. In ROOT, it will basically ignore 0s. This is\n    # especially important for log plots. Matplotlib doesn't handle 0s as well, since it attempts to\n    # plot them and then will throw exceptions when the log is taken.\n    # By setting to nan, matplotlib basically ignores them similar to ROOT\n    # NOTE: This requires a few special functions later which ignore nan when calculating min and max.\n    if set_zero_to_NaN:\n        hist_array[hist_array == 0] = np.nan\n\n    if return_bin_edges:\n        # Bin edges\n        x_bin_edges = get_bin_edges_from_axis(hist.GetXaxis())\n        y_bin_edges = get_bin_edges_from_axis(hist.GetYaxis())\n\n        # NOTE: The addition of epsilon to the max is extremely important! Otherwise, the x and y\n        #       ranges will be one bin short since ``arange`` is not inclusive. This could also be resolved\n        #       by using ``linspace``, but I think this approach is perfectly fine.\n        # NOTE: This epsilon is smaller than the one in ``utils`` because we are sometimes dealing\n        #       with small times (~ns). The other value is larger because (I seem to recall) that\n        #       smaller values didn't always place nice with ROOT, but it is fine here, since we're\n        #       working with numpy.\n        # NOTE: This should be identical to taking the min and max of the axis using\n        #       ``TAxis.GetXmin()`` and ``TAxis.GetXmax()``, but I prefer this approach.\n        epsilon = 1e-9\n        x_range = np.arange(\n            np.amin(x_bin_edges),\n            np.amax(x_bin_edges) + epsilon,\n            hist.GetXaxis().GetBinWidth(1)\n        )\n        y_range = np.arange(\n            np.amin(y_bin_edges),\n            np.amax(y_bin_edges) + epsilon,\n            hist.GetYaxis().GetBinWidth(1)\n        )\n    else:\n        # We want an array of bin centers\n        x_range = np.array([hist.GetXaxis().GetBinCenter(i) for i in range(1, hist.GetXaxis().GetNbins() + 1)])\n        y_range = np.array([hist.GetYaxis().GetBinCenter(i) for i in range(1, hist.GetYaxis().GetNbins() + 1)])\n\n    X, Y = np.meshgrid(x_range, y_range)\n\n    return (X, Y, hist_array)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the bin edges from a ROOT hist axis.", "response": "def get_bin_edges_from_axis(axis) -> np.ndarray:\n    \"\"\" Get bin edges from a ROOT hist axis.\n\n    Note:\n        Doesn't include over- or underflow bins!\n\n    Args:\n        axis (ROOT.TAxis): Axis from which the bin edges should be extracted.\n    Returns:\n        Array containing the bin edges.\n    \"\"\"\n    # Don't include over- or underflow bins\n    bins = range(1, axis.GetNbins() + 1)\n    # Bin edges\n    bin_edges = np.empty(len(bins) + 1)\n    bin_edges[:-1] = [axis.GetBinLowEdge(i) for i in bins]\n    bin_edges[-1] = axis.GetBinUpEdge(axis.GetNbins())\n\n    return bin_edges"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms sistr analyses on Salmonella", "response": "def sistr(self):\n        \"\"\"Perform sistr analyses on Salmonella\"\"\"\n        logging.info('Performing sistr analyses')\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # Create the analysis-type specific attribute\n                setattr(sample, self.analysistype, GenObject())\n                if sample.general.bestassemblyfile != 'NA':\n                    try:\n                        # Only process strains that have been determined to be Salmonella\n                        if sample.general.referencegenus == 'Salmonella':\n                            # Set and create the path of the directory to store the strain-specific reports\n                            sample[self.analysistype].reportdir = os.path.join(sample.general.outputdirectory,\n                                                                               self.analysistype)\n                            # Name of the .json output file\n                            sample[self.analysistype].jsonoutput = os.path.join(sample[self.analysistype].reportdir,\n                                                                                '{}.json'.format(sample.name))\n                            # Set the sistr system call\n                            sample.commands.sistr = \\\n                                'sistr -f json -o {} -t {} -T {} {}'\\\n                                .format(sample[self.analysistype].jsonoutput,\n                                        self.cpus,\n                                        os.path.join(sample[self.analysistype].reportdir, 'tmp'),\n                                        sample.general.bestassemblyfile)\n                            #\n                            sample[self.analysistype].logout = os.path.join(sample[self.analysistype].reportdir, 'logout')\n                            sample[self.analysistype].logerr = os.path.join(sample[self.analysistype].reportdir, 'logerr')\n                            # Only run the analyses if the output json file does not exist\n                            if not os.path.isfile(sample[self.analysistype].jsonoutput):\n                                out, err = run_subprocess(sample.commands.sistr)\n                                write_to_logfile(sample.commands.sistr, sample.commands.sistr, self.logfile,\n                                                 sample.general.logout, sample.general.logerr,\n                                                 sample[self.analysistype].logout, sample[self.analysistype].logerr)\n                                write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr,\n                                                 sample[self.analysistype].logout, sample[self.analysistype].logerr)\n                            self.queue.task_done()\n                    except (ValueError, KeyError):\n                        pass\n        self.queue.join()\n        self.report()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of resistance class of gene set containing all the unique IDs of the resistance classes in the targetpath.", "response": "def classes(targetpath):\n        \"\"\"\n        Uses .tfa files included in the ResFinder database to determine the resistance class of gene matches\n        :param targetpath: Path to database files\n        :return: Dictionary of resistance class: gene set\n        \"\"\"\n        # Initialise dictionary to store results\n        resistance_dict = dict()\n        # Find all the .tfa files in the folder\n        resistance_files = sorted(glob(os.path.join(targetpath, '*.tfa')))\n        # Iterate through each file\n        for fasta in resistance_files:\n            # Extract the resistance class from the file name and path\n            resistance_class = os.path.splitext(os.path.basename(fasta))[0]\n            # Initialise the resistance class as a set in the dictionary\n            resistance_dict[resistance_class] = set()\n            # Open the file\n            with open(fasta) as resistance:\n                # Iterate through the FASTA records\n                for record in SeqIO.parse(resistance, 'fasta'):\n                    # Replace dashes with underscores\n                    record.id = record.id.replace('-', '_')\n                    # Add the gene name to the set\n                    resistance_dict[resistance_class].add(record.id)\n        return resistance_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef gene_name(name):\n        if 'Van' in name or 'mcr' in name or 'aph' in name or 'ddlA' in name or 'ant' in name or 'aadE_Cc' in name:\n            try:\n                if name == \"ant(3'')_Ih_aac(6')_IId_1_AF453998\":\n                    # >aac(3)_Ib_aac(6')_Ib_1_AF355189 yields gname, genename: aac(3)-Ib-aac(6')-Ib, allele:1,\n                    # accession: AF355189\n                    gene1, version1, gene2, version2, allele, accession = name.split('_')\n                    gname = '{g1}-{v1}-{g2}-{v2}'.format(g1=gene1,\n                                                         v1=version1,\n                                                         g2=gene2,\n                                                         v2=version2)\n                    genename = gname\n                elif name == 'ant(3'')_Ia_1_X02340':\n                    # >ant(3'')_Ia_1_X02340\n                    gene, version, allele, accession = name.split('_')\n                    gname = '{g}-{v}'.format(g=gene,\n                                             v=version)\n                    genename = gname\n                elif 'mcr_3' in name or 'mcr_2' in name or 'mcr_1.10' in name:\n                    # >mcr_3.3_1_NG055492 yields genename, gname: mcr-3, allele: 1, accession: NG055492\n                    gene, combinedversion, allele, accession = name.split('_')\n                    version = combinedversion.split('.')[0]\n                    gname = '{gene}-{version}'.format(gene=gene,\n                                                      version=version)\n                    genename = gname\n                else:\n                    # Allow for an additional part to the gene name aph(3'')_Ib_5_AF321551 yields gname: aph(3''),\n                    # genename: aph(3'')-Ib, allele: 5, accession AF321551\n                    try:\n                        pregene, postgene, allele, accession = name.split('_')\n                        gname = '{pre}-{post}'.format(pre=pregene,\n                                                      post=postgene)\n                        genename = gname\n                    except ValueError:\n                        # Allow for underscores in the accession: aac(2')_Ie_1_NC_011896 yields gname: aac(2'),\n                        # genename:  aac('2)-1e, allele: 1, accession NC_011896\n                        pregene, postgene, allele, preaccession, postaccession = name.split('_')\n                        genename = '{pre}-{post}'.format(pre=pregene,\n                                                         post=postgene)\n                        accession = '{pre}_{post}'.format(pre=preaccession,\n                                                          post=postaccession)\n                        gname = pregene\n            except ValueError:\n                # VanC_2_DQ022190\n                genename, allele, accession = name.split('_')\n                gname = genename\n        else:\n            if 'bla' in name or 'aac' in name or 'ARR' in name or 'POM' in name:\n                if 'OKP' in name or 'CTX' in name or 'OXY' in name:\n                    # >blaOKP_B_11_1_AM051161 yields gname: blaOKP-B-11, genename: blaOXP, allele: 1,\n                    # accession: AM051161\n                    gene, version1, version2, allele, accession = name.split('_')\n                    gname = '{g}-{v1}-{v2}'.format(g=gene,\n                                                   v1=version1,\n                                                   v2=version2)\n                    genename = gname\n                elif 'CMY' in name:\n                    # >blaCMY_12_1_Y16785 yields gname, genename: blaCMY, allele: 12\n                    try:\n                        gname, allele, version, accession = name.split('_')\n                    except ValueError:\n                        # blaCMY_59_1_NG_048854\n                        gname, allele, version, pre_accession, post_accession = name.split('_')\n                        accession = '{pre}_{post}'.format(pre=pre_accession,\n                                                          post=post_accession)\n                    genename = gname\n                elif name == \"aac(3)_Ib_aac(6')_Ib_1_AF355189\":\n                    # >aac(3)_Ib_aac(6')_Ib_1_AF355189 yields gname, genename: aac(3)-Ib-aac(6')-Ib, allele:1,\n                    # accession: AF355189\n                    gene1, version1, gene2, version2, allele, accession = name.split('_')\n                    gname = '{g1}-{v1}-{g2}-{v2}'.format(g1=gene1,\n                                                         v1=version1,\n                                                         g2=gene2,\n                                                         v2=version2)\n                    genename = gname\n                elif 'alias' in name:\n                    # >blaSHV_5a_alias_blaSHV_9_1_S82452\n                    gene1, version1, alias, gene2, version2, allele, accession = name.split('_')\n                    gname = '{g1}-{v1}'.format(g1=gene1,\n                                               v1=version1)\n                    genename = gname\n                else:\n                    # Split the name on '_'s: ARR-2_1_HQ141279; gname, genename: ARR-2, allele: 1, accession: HQ141279\n                    try:\n                        genename, allele, accession = name.split('_')\n                        gname = genename\n                    except ValueError:\n                        try:\n                            # >blaACC_1_2_AM939420 yields gname: blaACC-1, genename: blaACC, allele: 2,\n                            # accession: AM939420\n                            genename, version, allele, accession = name.split('_')\n                            gname = '{g}-{v}'.format(g=genename,\n                                                     v=version)\n                        except ValueError:\n                            # >aac(2')_Ie_1_NC_011896 yields gname, genename: aac(2')-Ie, allele: 1,\n                            # accession: NC_011896\n                            genename, version, allele, preaccession, postaccession = name.split('_')\n                            gname = '{g}-{v}'.format(g=genename,\n                                                     v=version)\n                            genename = gname\n                            accession = '{preaccess}_{postaccess}'.format(preaccess=preaccession,\n                                                                          postaccess=postaccession)\n            else:\n                # Split the name on '_'s: ARR-2_1_HQ141279; gname, genename: ARR-2, allele: 1, accession: HQ141279\n                try:\n                    genename, allele, accession = name.split('_')\n                    gname = genename\n                # Some names have a slightly different naming scheme:\n                except ValueError:\n                    # tet(44)_1_NZ_ABDU01000081 yields gname, genename: tet(44), allele: 1,\n                    # accession: NZ_ABDU01000081\n                    genename, allele, preaccession, postaccession = name.split('_')\n                    accession = '{preaccess}_{postaccess}'.format(preaccess=preaccession,\n                                                                  postaccess=postaccession)\n                    gname = genename\n        return gname, genename, accession, allele", "response": "Split the FASTA header string into its components including gene name and accession and allele."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resistance(genename, resistance_dict):\n        # Initialise a list to store the resistance class(es) for the gene\n        resistance_list = list()\n        # Iterate through the dictionary of the resistance class: set of gene names\n        for resistance_class, gene_set in resistance_dict.items():\n            # If the gene is presence in the set\n            if genename in gene_set:\n                # Set the resistance class appropriately\n                resistance_list.append(resistance_class)\n        # Create a comma-separated string of the sorted genes\n        resistance = ','.join(sorted(resistance_list))\n        # Return the calculated resistance class\n        return resistance", "response": "Determines the resistance class of the gene by searching the set of genes included in every resistance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef purgeDeletedWidgets():\n        toremove = []\n        for field in AbstractEditorWidget.funit_fields:\n            if sip.isdeleted(field):\n                toremove.append(field)\n        for field in toremove:\n            AbstractEditorWidget.funit_fields.remove(field)\n\n        toremove = []\n        for field in AbstractEditorWidget.tunit_fields:\n            if sip.isdeleted(field):\n                toremove.append(field)\n        for field in toremove:\n            AbstractEditorWidget.tunit_fields.remove(field)", "response": "Removes old references to stashed fields and deletes them"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef movefastq(self):\n        logging.info('Moving FASTQ files')\n        # Iterate through each sample\n        for sample in self.metadata.runmetadata.samples:\n            # Retrieve the output directory\n            outputdir = os.path.join(self.path, sample.name)\n            # Find any fastq files with the sample name\n            fastqfiles = sorted(glob(os.path.join(self.path, '{}_*.fastq*'.format(sample.name)))) \\\n                if sorted(glob(os.path.join(self.path, '{}_*.fastq*'.format(sample.name)))) \\\n                else sorted(glob(os.path.join(self.path, '{}.fastq*'.format(sample.name)))) \\\n                if sorted(glob(os.path.join(self.path, '{}.fastq*'.format(sample.name)))) \\\n                else sorted(glob(os.path.join(self.path, '{}*.fastq*'.format(sample.name))))\n            # Only try and move the files if the files exist\n            if fastqfiles:\n                make_path(outputdir)\n                # Symlink the fastq files to the directory\n                try:\n                    list(map(lambda x: os.symlink(os.path.join('..', os.path.basename(x)),\n                                                  os.path.join(outputdir, os.path.basename(x))), fastqfiles))\n                except OSError:\n                    pass\n                # Find any fastq files with the sample name\n                fastqfiles = [fastq for fastq in sorted(glob(os.path.join(outputdir, '{}*.fastq*'.format(sample.name))))\n                              if 'trimmed' not in fastq and 'normalised' not in fastq and 'corrected' not in fastq\n                              and 'paired' not in fastq and 'unpaired' not in fastq]\n            else:\n                if outputdir:\n                    # Find any fastq files with the sample name\n                    fastqfiles = [fastq for fastq in sorted(glob(os.path.join(\n                        outputdir, '{}*.fastq*'.format(outputdir, sample.name))))\n                                  if 'trimmed' not in fastq and 'normalised' not in fastq and 'corrected' not in fastq\n                                  and 'paired' not in fastq and 'unpaired' not in fastq]\n            sample.general.fastqfiles = fastqfiles", "response": "Move the fastq files to an appropriately named folder"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_list(self, datatype, url, **kwargs):\n        search_url = [url, '?']\n        kwargs.update({'key': self.api_key})\n        search_url.append(urlencode(kwargs))\n        data = json.loads(urlopen(''.join(search_url)).read())\n        return data[datatype]", "response": "base function for connecting to API"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_box_office_films(self):\n        today = datetime.date.today()\n        next_wednesday = (today + datetime.timedelta((2 - today.weekday()) % 7)).strftime('%Y%m%d')\n        films = self.get_films(cinema=79, date = next_wednesday)\n        \n        films = filter(lambda x: '3D' not in x['title'], films)\n        for film in films:\n            if '2D -' in film['title']:\n                film['title']=film['title'][5:]\n        return films", "response": "returns a list of the latest box office films"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching for films using fuzzy matching", "response": "def film_search(self, title):\n        \"\"\"film search using fuzzy matching\"\"\"\n        films = []\n        #check for cache or update\n        if not hasattr(self, 'film_list'):\n            self.get_film_list()\n        #iterate over films and check for fuzzy string match    \n        for film in self.film_list:\n            strength = WRatio(title, film['title'])\n            if  strength > 80:\n                film.update({u'strength':strength})\n                films.append(film)\n        #sort films by the strength of the fuzzy string match\n        films_sorted = sorted(films, key=itemgetter('strength'), reverse = True)\n        return films_sorted"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_film_id(self, title, three_dimensional=False):\n        films = self.film_search(title)\n        for film in films:\n            if (film['title'].find('3D') is - 1) is not three_dimensional:\n                return film['edi']\n        return -1", "response": "get the film id using the title in conjunction with the searching function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbinning the items into a list of items and replace_at_most_one and leftovers.", "response": "def _bin_update_items(self, items, replace_at_most_one,\n                          replacements, leftovers):\n        \"\"\"\n        Subclassed from omdict._bin_update_items() to make update() and\n        updateall() process lists of values as multiple values.\n\n        <replacements and <leftovers> are modified directly, ala pass by\n        reference.\n        \"\"\"\n        for key, values in items:\n            # <values> is not a list or an empty list.\n            like_list_not_str = self._quacks_like_a_list_but_not_str(values)\n            if not like_list_not_str or (like_list_not_str and not values):\n                values = [values]\n\n            for value in values:\n                # If the value is [], remove any existing leftovers with\n                # key <key> and set the list of values itself to [],\n                # which in turn will later delete <key> when [] is\n                # passed to omdict.setlist() in\n                # omdict._update_updateall().\n                if value == []:\n                    replacements[key] = []\n                    leftovers[:] = [l for l in leftovers if key != l[0]]\n                    continue\n\n                # If there are existing items with key <key> that have\n                # yet to be marked for replacement, mark that item's\n                # value to be replaced by <value> by appending it to\n                # <replacements>.  TODO: Refactor for clarity\n                if (key in self and\n                    (key not in replacements or\n                     (key in replacements and\n                      replacements[key] == []))):\n                    replacements[key] = [value]\n                elif (key in self and not replace_at_most_one and\n                      len(replacements[key]) < len(self.values(key))):\n                    replacements[key].append(value)\n                else:\n                    if replace_at_most_one:\n                        replacements[key] = [value]\n                    else:\n                        leftovers.append((key, value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_calibration(self, attenuations, freqs, frange, calname):\n        self._stimulus.setCalibration(attenuations, freqs, frange)", "response": "Set calibration for this stimulus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_current_stim_parameter(self, param, val):\n        component = self._stimulus.component(0,1)\n        component.set(param, val)", "response": "Sets a parameter on the current stimulus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_to_file(self, data, stamp):\n        self.datafile.append(self.current_dataset_name, data)\n        # save stimulu info\n        info = dict(self._stimulus.componentDoc().items() + self._stimulus.testDoc().items())\n        print 'saving doc', info\n        info['time_stamps'] = [stamp]\n        info['samplerate_ad'] = self.player.aifs\n        self.datafile.append_trace_info(self.current_dataset_name, info)", "response": "Saves data to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow a simple countdown progress bar.", "response": "def countdown_timer(seconds=10):\n    \"\"\"Show a simple countdown progress bar\n\n    Parameters\n    ----------\n    seconds\n        Period of time the progress bar takes to reach zero.\n    \"\"\"\n\n    tick = 0.1  # seconds\n    n_ticks = int(seconds / tick)\n\n    widgets = ['Pause for panic: ', progressbar.ETA(), ' ', progressbar.Bar()]\n    pbar = progressbar.ProgressBar(\n        widgets=widgets, max_value=n_ticks\n    ).start()\n\n    for i in range(n_ticks):\n        pbar.update(i)\n        sleep(tick)\n\n    pbar.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay an adaptive ETA / countdown bar with a message.", "response": "def eta_bar(msg, max_value):\n    \"\"\"Display an adaptive ETA / countdown bar with a message.\n\n    Parameters\n    ----------\n    msg: str\n        Message to prefix countdown bar line with\n\n    max_value: max_value\n        The max number of progress bar steps/updates\n    \"\"\"\n\n    widgets = [\n        \"{msg}:\".format(msg=msg),\n        progressbar.Bar(), ' ', progressbar.AdaptiveETA(),\n    ]\n\n    return progressbar.ProgressBar(widgets=widgets, max_value=max_value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves .csv file with posts data :param dicts: Dictionaries with same values", "response": "def write_dicts_to_csv(self, dicts):\n        \"\"\"Saves .csv file with posts data\n\n        :param dicts: Dictionaries with same values\n        \"\"\"\n        csv_headers = sorted(dicts[0].keys())\n        with open(self.path, \"w\") as out_file:  # write to file\n            dict_writer = csv.DictWriter(\n                out_file, csv_headers, delimiter=\",\", quotechar=\"\\\"\"\n            )\n            dict_writer.writeheader()\n            dict_writer.writerows(dicts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving .csv file with data :param headers: column names :param data: Data", "response": "def write_matrix_to_csv(self, headers, data):\n        \"\"\"Saves .csv file with data\n\n        :param headers: column names\n        :param data: Data\n        \"\"\"\n        with open(self.path, \"w\") as out_file:  # write to file\n            data_writer = csv.writer(out_file, delimiter=\",\")\n            data_writer.writerow(headers)  # write headers\n            data_writer.writerows(data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving .json file with data :param data: Data", "response": "def write_dicts_to_json(self, data):\n        \"\"\"Saves .json file with data\n\n        :param data: Data\n        \"\"\"\n        with open(self.path, \"w\") as out:\n            json.dump(\n                data,  # data\n                out,  # file handler\n                indent=4, sort_keys=True  # pretty print\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_listening(self):\n        self._qlisten()\n        self._halt_threads = False\n        for t in self.queue_threads:\n            t.start()", "response": "Start listener threads for acquistion callback queues"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_listening(self):\n        self._halt_threads = True\n        # wake them up so that they can die\n        for name, queue_waker in self.recieved_signals.items():\n            q, wake_event = queue_waker\n            wake_event.set()", "response": "Stop listening for acquistion queues"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a function to execute when the named acquistion queue has data placed in it.", "response": "def set_queue_callback(self, name, func):\n        \"\"\"Sets a function to execute when the named acquistion queue \n        has data placed in it.\n\n        :param name: name of the queue to pull data from\n        :type name: str\n        :param func: function reference to execute, expects queue contents as argument(s)\n        :type func: callable\n        \"\"\"\n        if name in self.acquisition_hooks:\n            self.acquisition_hooks[name].append(func)\n        else:\n            self.acquisition_hooks[name] = [func]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_calibration(self, datakey, calf=None, frange=None):\n        if datakey is None:\n            calibration_vector, calibration_freqs = None, None\n        else:\n            if calf is None:\n                raise Exception('calibration reference frequency must be specified')    \n            try:\n                cal = self.datafile.get_calibration(datakey, calf)\n            except:\n                print \"Error: unable to load calibration data from: \", datakey\n                raise\n            calibration_vector, calibration_freqs = cal\n        # clear one cache -- affects all StimulusModels\n        StimulusModel.clearCache()\n        logger = logging.getLogger('main')\n        logger.debug('clearing cache')\n        logger.debug('setting explore calibration')\n        self.explorer.set_calibration(calibration_vector, calibration_freqs, frange, datakey)\n        logger.debug('setting protocol calibration')\n        self.protocoler.set_calibration(calibration_vector, calibration_freqs, frange, datakey)\n        logger.debug('setting chart calibration')\n        self.charter.set_calibration(calibration_vector, calibration_freqs, frange, datakey)\n        logger.debug('setting calibrator calibration')\n        self.bs_calibrator.stash_calibration(calibration_vector, calibration_freqs, frange, datakey)\n        logger.debug('setting tone calibrator calibration')\n        self.tone_calibrator.stash_calibration(calibration_vector, calibration_freqs, frange, datakey)", "response": "Sets a calibration for all of the acquisition operations for all of the acquisition operations from an already gathered calibration data set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_calibration_duration(self, dur):\n        self.bs_calibrator.set_duration(dur)\n        self.tone_calibrator.set_duration(dur)", "response": "Sets the duration for the calibration stimulus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_calibration_reps(self, reps):\n        self.bs_calibrator.set_reps(reps)\n        self.tone_calibrator.set_reps(reps)", "response": "Sets the number of repetitions for calibration operations for the unique stimuli in the calendar operations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_data_file(self, fname, filemode='a'):\n        self.close_data()\n        self.datafile = open_acqdata(fname, filemode=filemode)\n\n        self.explorer.set(datafile=self.datafile)\n        self.protocoler.set(datafile=self.datafile)\n        self.charter.set(datafile=self.datafile)\n        self.bs_calibrator.set(datafile=self.datafile)\n        self.tone_calibrator.set(datafile=self.datafile)\n        self.set_calibration(None)\n\n        self.current_cellid = dict(self.datafile.get_info('')).get('total cells', 0)", "response": "Opens an existing data file to append to the current skeleton data set"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset spike detection threshold for the current species", "response": "def set_threshold(self, threshold):\n        \"\"\"Sets spike detection threshold\n\n        :param threshold: electrical potential to determine spikes (V)\n        :type threshold: float\n        \"\"\"\n        self.explorer.set_threshold(threshold)\n        self.protocoler.set_threshold(threshold)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the acquisition parameters for all acquisition types.", "response": "def set(self, **kwargs):\n        \"\"\"Sets acquisition parameters for all acquisition types\n\n        See :meth:`AbstractAcquisitionRunner<sparkle.run.abstract_acquisition.AbstractAcquisitionRunner.set>`\n        \"\"\"\n        self.explorer.set(**kwargs)\n        self.protocoler.set(**kwargs)\n        self.tone_calibrator.set(**kwargs)\n        self.charter.set(**kwargs)\n        self.bs_calibrator.set(**kwargs)\n        self.mphone_calibrator.set(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the microphone calibration for the purpose of calculating recorded dB levels at the specified sensitivity and dB SPL that the calibration was measured at the specified dB SPL.", "response": "def set_mphone_calibration(self, sens, db):\n        \"\"\"Sets the microphone calibration, for the purpose of calculating recorded dB levels\n\n        :param sens: microphone sensitivity (V)\n        :type sens: float\n        :param db: dB SPL that the calibration was measured at\n        :type db: int\n        \"\"\"\n        self.bs_calibrator.set_mphone_calibration(sens, db)\n        self.tone_calibrator.set_mphone_calibration(sens, db)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the number of stimuli presentations for the current calibration selected", "response": "def calibration_total_count(self):\n        \"\"\"The number of stimuli presentations (including reps) for the current calibration selected\n        \n        :returns: int -- number of presentations\n        \"\"\"\n        if self.selected_calibration_index == 2:\n            return self.tone_calibrator.count()\n        else:\n            return self.bs_calibrator.count()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_calibration(self, interval, applycal):\n        if self.selected_calibration_index == 2:\n            self.tone_calibrator.apply_calibration(applycal)\n            self.tone_calibrator.setup(interval)\n            return self.tone_calibrator.run()\n        else:\n            self.bs_calibrator.set_stim_by_index(self.selected_calibration_index)\n            self.bs_calibrator.apply_calibration(applycal)\n            self.bs_calibrator.setup(interval)\n            return self.bs_calibrator.run()", "response": "Runs the calibration operation with the current settings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the stimuli presentation during a chart acquisition", "response": "def run_chart_protocol(self, interval):\n        \"\"\"Runs the stimuli presentation during a chart acquisition\n\n        :param interval: The repetition interval between stimuli presentations (seconds)\n        :type interval: float\n        :returns: :py:class:`threading.Thread` -- the acquisition thread\n        \"\"\"\n        self.charter.setup(interval)\n        return self.charter.run()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_calibration(self, save=True, calf=20000):\n        if self.selected_calibration_index == 2:\n            raise Exception(\"Calibration curve processing not currently supported\")\n        else:\n            results, calname, freq, db = self.bs_calibrator.process_calibration(save)\n        return calname, db", "response": "Processes a completed calibration from the current calendar entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhalting any running operations", "response": "def halt(self):\n        \"\"\"Halts any/all running operations\"\"\"\n        self.explorer.halt()\n        self.protocoler.halt()\n        self.bs_calibrator.halt()\n        self.tone_calibrator.halt()\n        self.charter.halt()\n        self.mphone_calibrator.halt()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close_data(self):\n        # save the total number of cells to make re-loading convient\n        if self.datafile is not None:\n            if self.datafile.filemode != 'r':\n                self.datafile.set_metadata('', {'total cells': self.current_cellid})\n            self.datafile.close()\n            self.datafile = None", "response": "Closes the current data file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the stimulus model for calibration with the specified mode", "response": "def calibration_stimulus(self, mode):\n        \"\"\"Gets the stimulus model for calibration\n\n        :param mode: Type of stimulus to get: tone or noise\n        :type mode: str\n        :returns: :class:`StimulusModel<sparkle.stim.stimulus_model.StimulusModel>`\n        \"\"\"\n        if mode == 'tone':\n            return self.tone_calibrator.stimulus\n        elif mode =='noise':\n            return self.bs_calibrator.stimulus"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calibration_template(self):\n        temp = {}\n        temp['tone_doc'] = self.tone_calibrator.stimulus.templateDoc()\n        comp_doc = []\n        for calstim in self.bs_calibrator.get_stims():\n            comp_doc.append(calstim.stateDict())\n        temp['noise_doc'] = comp_doc\n        return temp", "response": "Gets the template documentation for both the tone curve calibration and noise calibration objects\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreload calibration settings from saved template doc", "response": "def load_calibration_template(self, template):\n        \"\"\"Reloads calibration settings from saved template doc\n\n        :param template: Values for calibration stimuli (see calibration_template function)\n        :type template: dict\n        \"\"\"\n        self.tone_calibrator.stimulus.clearComponents()\n        self.tone_calibrator.stimulus.loadFromTemplate(template['tone_doc'], self.tone_calibrator.stimulus)\n        comp_doc = template['noise_doc']\n        for state, calstim in zip(comp_doc, self.bs_calibrator.get_stims()):\n            calstim.loadState(state)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attenuator_connection(self, connect=True):\n        # all or none will be connected\n        acquisition_modules = [self.explorer, self.protocoler, self.bs_calibrator, self.tone_calibrator, self.charter]\n        if connect:\n            if not acquisition_modules[0].player.attenuator_connected():\n                #attempt to re-connect first\n                for module in acquisition_modules:\n                    success = module.player.connect_attenuator()\n                if success is None:\n                    StimulusModel.setMinVoltage(0.0)\n                    return False\n                else:\n                    StimulusModel.setMinVoltage(0.005)\n                    return True\n            else:\n                StimulusModel.setMinVoltage(0.005)\n                return True\n        else:\n            for module in acquisition_modules:\n                module.player.connect_attenuator(False)\n            StimulusModel.setMinVoltage(0.0)\n            return False", "response": "Checks the connection to the attenuator and attempts to connect if not connected."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to read a line from the queue.", "response": "def readline(self, timeout = 0.1):\n        \"\"\"Try to read a line from the stream queue.\n        \"\"\"\n        try:\n            return self._q.get(block = timeout is not None,\n                               timeout = timeout)\n        except Empty:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of events related to command verification.", "response": "def verification_events(self):\n        \"\"\"\n        Events related to command verification.\n\n        :type: List[:class:`.CommandHistoryEvent`]\n        \"\"\"\n        queued = self._assemble_event('Verifier_Queued')\n        started = self._assemble_event('Verifier_Started')\n        return [x for x in [queued, started] if x]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef events(self):\n        events = [self.acknowledge_event] + self.verification_events\n        return [x for x in events if x]", "response": "Returns a list of all command history events."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the generation time as set by Yamcs.", "response": "def generation_time(self):\n        \"\"\"\n        The generation time as set by Yamcs.\n\n        :type: :class:`~datetime.datetime`\n        \"\"\"\n        entry = self._proto.commandQueueEntry\n        if entry.HasField('generationTimeUTC'):\n            return parse_isostring(entry.generationTimeUTC)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sequence_number(self):\n        entry = self._proto.commandQueueEntry\n        if entry.cmdId.HasField('sequenceNumber'):\n            return entry.cmdId.sequenceNumber\n        return None", "response": "Returns the sequence number of this command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new command history subscription for this command.", "response": "def create_command_history_subscription(self, on_data=None, timeout=60):\n        \"\"\"\n        Create a new command history subscription for this command.\n\n        :param on_data: Function that gets called with  :class:`.CommandHistory`\n                        updates.\n        :param float timeout: The amount of seconds to wait for the request\n                              to complete.\n        :return: Future that can be used to manage the background websocket\n                 subscription\n        :rtype: .CommandHistorySubscription\n        \"\"\"\n        return self._client.create_command_history_subscription(\n            issued_command=self, on_data=on_data, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acknowledged_by(self):\n        if (self.is_acknowledged and\n                self._proto.acknowledgeInfo.HasField('acknowledgedBy')):\n            return self._proto.acknowledgeInfo.acknowledgedBy\n        return None", "response": "Username of the acknowledger."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncommenting provided when acknowledging the alarm.", "response": "def acknowledge_message(self):\n        \"\"\"Comment provided when acknowledging the alarm.\"\"\"\n        if (self.is_acknowledged and\n                self._proto.acknowledgeInfo.HasField('acknowledgeMessage')):\n            return self._proto.acknowledgeInfo.acknowledgeMessage\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef acknowledge_time(self):\n        if (self.is_acknowledged and\n                self._proto.acknowledgeInfo.HasField('acknowledgeTime')):\n            return parse_isostring(self._proto.acknowledgeInfo.acknowledgeTime)\n        return None", "response": "Returns the Processor time when the alarm was acknowledged."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef name(self):\n        if self._proto.id.namespace:\n            return self._proto.id.namespace + '/' + self._proto.id.name\n        return self._proto.id.name", "response": "The name of the attribute that is used to identify the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validity_duration(self):\n        if self._proto.HasField('expireMillis'):\n            return timedelta(milliseconds=self._proto.expireMillis)\n        return None", "response": "The number of milliseconds that this parameter value is valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef range_condition(self):\n        if self._proto.HasField('rangeCondition'):\n            return pvalue_pb2.RangeCondition.Name(self._proto.rangeCondition)\n        return None", "response": "Returns the range condition field."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef smart_str(s, encoding='utf-8', errors='strict'):\n    if not isinstance(s, basestring):\n        try:\n            return str(s)\n        except UnicodeEncodeError:\n            if isinstance(s, Exception):\n                # An Exception subclass containing non-ASCII data that doesn't\n                # know how to print itself properly. We shouldn't raise a\n                # further exception.\n                return ' '.join([smart_str(arg, encoding, errors) for arg in s])\n            return unicode(s).encode(encoding, errors)\n    elif isinstance(s, unicode):\n        return s.encode(encoding, errors)\n    elif s and encoding != 'utf-8':\n        return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s", "response": "Returns a bytestring version of s encoded as specified in encoding."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(self):\n        # Find the target files\n        self.targets()\n        # Use bbduk to bait the FASTQ reads matching the target sequences\n        self.bait()\n        # If desired, use bbduk to bait the target sequences with the previously baited FASTQ files\n        if self.revbait:\n            self.reversebait()\n        # Run the bowtie2 read mapping module\n        self.mapping()\n        # Use samtools to index the sorted bam file\n        self.indexing()\n        # Parse the results\n        self.parsebam()", "response": "Run the methods in the correct order for pipelines\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches the targets folder for FASTA files create the multi - FASTA file of all targets if necessary and populate objects", "response": "def targets(self):\n        \"\"\"\n        Search the targets folder for FASTA files, create the multi-FASTA file of all targets if necessary, and\n        populate objects\n        \"\"\"\n        logging.info('Performing analysis with {at} targets folder'.format(at=self.analysistype))\n        if self.pipeline:\n            for sample in self.runmetadata:\n                setattr(sample, self.analysistype, GenObject())\n                if sample.general.bestassemblyfile != 'NA':\n                    sample[self.analysistype].runanalysis = True\n                    # Set attributes\n                    try:\n                        sample[self.analysistype].targetpath = \\\n                            os.path.join(self.targetpath, self.analysistype, sample.mash.closestrefseqgenus, '')\n                    except AttributeError:\n                        sample[self.analysistype].targetpath = \\\n                            os.path.join(self.targetpath, self.analysistype, sample.general.closestrefseqgenus, '')\n                    # There is a relatively strict databasing scheme necessary for the custom targets. Eventually,\n                    # there will be a helper script to combine individual files into a properly formatted combined file\n                    try:\n                        sample[self.analysistype].baitfile = glob(os.path.join(sample[self.analysistype].targetpath,\n                                                                               '*.fasta'))[0]\n                    # If the fasta file is missing, raise a custom error\n                    except IndexError:\n                        # Combine any .tfa files in the directory into a combined targets .fasta file\n                        tfafiles = glob(os.path.join(sample[self.analysistype].targetpath, '*.tfa'))\n                        if tfafiles:\n                            combinetargets(tfafiles, sample[self.analysistype].targetpath)\n                        try:\n                            self.baitfile = glob(os.path.join(sample[self.analysistype].targetpath, '*.fasta'))[0]\n                        except IndexError as e:\n                            # noinspection PyPropertyAccess\n                            e.args = [\n                                'Cannot find the combined fasta file in {path}. Please note that the file must have a '\n                                '.fasta extension'.format(path=sample[self.analysistype].targetpath)]\n                            if os.path.isdir(sample[self.analysistype].targetpath):\n                                raise\n                            else:\n                                sample[self.analysistype].runanalysis = False\n\n                else:\n                    sample[self.analysistype].runanalysis = False\n            for sample in self.runmetadata:\n                if sample.general.bestassemblyfile != 'NA' and sample[self.analysistype].runanalysis:\n                    # Set the necessary attributes\n                    sample[self.analysistype].outputdir = os.path.join(sample.run.outputdirectory, self.analysistype)\n                    sample[self.analysistype].logout = os.path.join(sample[self.analysistype].outputdir, 'logout.txt')\n                    sample[self.analysistype].logerr = os.path.join(sample[self.analysistype].outputdir, 'logerr.txt')\n                    sample[self.analysistype].baitedfastq = \\\n                        os.path.join(sample[self.analysistype].outputdir,\n                                     '{at}_targetMatches.fastq.gz'.format(at=self.analysistype))\n        else:\n            # There is a relatively strict databasing scheme necessary for the custom targets. Eventually, there will\n            # be a helper script to combine individual files into a properly formatted combined file\n            try:\n                self.baitfile = glob(os.path.join(self.targetpath, '*.fasta'))[0]\n            # If the fasta file is missing, raise a custom error\n            except IndexError:\n                # Combine any .tfa files in the directory into a combined targets .fasta file\n                tfafiles = glob(os.path.join(self.targetpath, '*.tfa'))\n                if tfafiles:\n                    combinetargets(tfafiles, self.targetpath)\n                try:\n                    self.baitfile = glob(os.path.join(self.targetpath, '*.fasta'))[0]\n                except IndexError as e:\n                    # noinspection PyPropertyAccess\n                    e.args = ['Cannot find the combined fasta file in {path}. Please note that the file must have a '\n                              '.fasta extension'.format(path=self.targetpath)]\n                    raise\n            # Set all the necessary attributes\n            for sample in self.runmetadata:\n                setattr(sample, self.analysistype, GenObject())\n                # Set attributes\n                sample[self.analysistype].runanalysis = True\n                sample[self.analysistype].baitfile = self.baitfile\n                sample[self.analysistype].hashfile = self.hashfile\n                sample[self.analysistype].hashcall = self.hashcall\n                sample[self.analysistype].targetpath = self.targetpath\n                sample[self.analysistype].outputdir = os.path.join(sample.run.outputdirectory, self.analysistype)\n                sample[self.analysistype].logout = os.path.join(sample[self.analysistype].outputdir, 'logout.txt')\n                sample[self.analysistype].logerr = os.path.join(sample[self.analysistype].outputdir, 'logerr.txt')\n                sample[self.analysistype].baitedfastq = \\\n                    os.path.join(sample[self.analysistype].outputdir,\n                                 '{at}_targetMatches.fastq.gz'.format(at=self.analysistype))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bait(self, maskmiddle='f', k='19'):\n        logging.info('Performing kmer baiting of fastq files with {at} targets'.format(at=self.analysistype))\n        # There seems to be some sort of issue with java incorrectly calculating the total system memory on certain\n        # computers. For now, calculate the memory, and feed it into the bbduk call\n        if self.kmer_size is None:\n            kmer = k\n        else:\n            kmer = self.kmer_size\n        with progressbar(self.runmetadata) as bar:\n            for sample in bar:\n                if sample.general.bestassemblyfile != 'NA' and sample[self.analysistype].runanalysis:\n                    # Create the folder (if necessary)\n                    make_path(sample[self.analysistype].outputdir)\n                    # Make the system call\n                    if len(sample.general.fastqfiles) == 2:\n                        # Create the command to run the baiting - paired inputs and a single, zipped output\n                        sample[self.analysistype].bbdukcmd = \\\n                            'bbduk.sh -Xmx{mem} ref={ref} in1={in1} in2={in2} k={kmer} maskmiddle={mm} ' \\\n                            'threads={c} outm={om}' \\\n                            .format(mem=self.mem,\n                                    ref=sample[self.analysistype].baitfile,\n                                    in1=sample.general.trimmedcorrectedfastqfiles[0],\n                                    in2=sample.general.trimmedcorrectedfastqfiles[1],\n                                    kmer=kmer,\n                                    mm=maskmiddle,\n                                    c=str(self.cpus),\n                                    om=sample[self.analysistype].baitedfastq)\n                    else:\n                        sample[self.analysistype].bbdukcmd = \\\n                            'bbduk.sh -Xmx{mem} ref={ref} in={in1} k={kmer} maskmiddle={mm} ' \\\n                            'threads={cpus} outm={outm}' \\\n                            .format(mem=self.mem,\n                                    ref=sample[self.analysistype].baitfile,\n                                    in1=sample.general.trimmedcorrectedfastqfiles[0],\n                                    kmer=kmer,\n                                    mm=maskmiddle,\n                                    cpus=str(self.cpus),\n                                    outm=sample[self.analysistype].baitedfastq)\n                    # Run the system call (if necessary)\n                    if not os.path.isfile(sample[self.analysistype].baitedfastq):\n                        out, err = run_subprocess(sample[self.analysistype].bbdukcmd)\n                        write_to_logfile(sample[self.analysistype].bbdukcmd,\n                                         sample[self.analysistype].bbdukcmd,\n                                         self.logfile, sample.general.logout, sample.general.logerr,\n                                         sample[self.analysistype].logout, sample[self.analysistype].logerr)\n                        write_to_logfile(out,\n                                         err,\n                                         self.logfile, sample.general.logout, sample.general.logerr,\n                                         sample[self.analysistype].logout, sample[self.analysistype].logerr)", "response": "Perform baiting of fastq files with k - kmers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reversebait(self, maskmiddle='f', k=19):\n        logging.info('Performing reverse kmer baiting of targets with FASTQ files')\n        if self.kmer_size is None:\n            kmer = k\n        else:\n            kmer = self.kmer_size\n        with progressbar(self.runmetadata) as bar:\n            for sample in bar:\n                if sample.general.bestassemblyfile != 'NA' and sample[self.analysistype].runanalysis:\n                    outfile = os.path.join(sample[self.analysistype].outputdir, 'baitedtargets.fa')\n                    sample[self.analysistype].revbbdukcmd = \\\n                        'bbduk.sh -Xmx{mem} ref={ref} in={in1} k={kmer} threads={cpus} mincovfraction={mcf} ' \\\n                        'maskmiddle={mm} outm={outm}' \\\n                        .format(mem=self.mem,\n                                ref=sample[self.analysistype].baitedfastq,\n                                in1=sample[self.analysistype].baitfile,\n                                kmer=kmer,\n                                cpus=str(self.cpus),\n                                mcf=self.cutoff,\n                                mm=maskmiddle,\n                                outm=outfile)\n                    # Run the system call (if necessary)\n                    if not os.path.isfile(outfile):\n                        out, err = run_subprocess(sample[self.analysistype].revbbdukcmd)\n                        write_to_logfile(sample[self.analysistype].bbdukcmd,\n                                         sample[self.analysistype].bbdukcmd,\n                                         self.logfile, sample.general.logout, sample.general.logerr,\n                                         sample[self.analysistype].logout, sample[self.analysistype].logerr)\n                        write_to_logfile(out,\n                                         err,\n                                         self.logfile, sample.general.logout, sample.general.logerr,\n                                         sample[self.analysistype].logout, sample[self.analysistype].logerr)\n                    # Set the baitfile to use in the mapping steps as the newly created outfile\n                    sample[self.analysistype].baitfile = outfile", "response": "Reverse baiting of the original target files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the dictionaries of the sorted bam files extracted using pysam", "response": "def parsebam(self):\n        \"\"\"\n        Parse the dictionaries of the sorted bam files extracted using pysam\n        \"\"\"\n        # Threading is actually the worst - need multiprocessing to make this work at all\n        logging.info('Parsing BAM files')\n        # The sample objects are too big to get pickled. To hack our way around this, try to dump the sample object to\n        # json, and have the processing function turn the object into a dictionary.\n        json_files = list()\n        with tempfile.TemporaryDirectory() as tmpdir:\n            best_assemblies = list()\n            sample_names = list()\n            for sample in self.runmetadata:\n                json_name = os.path.join(tmpdir, '{sn}.json'.format(sn=sample.name))\n                best_assemblies.append(sample.general.bestassemblyfile)\n                sample_names.append(sample.name)\n                with open(json_name, 'w') as f:\n                    json.dump(sample[self.analysistype].dump(), f, sort_keys=True, indent=4)\n                json_files.append(json_name)\n            p = multiprocessing.Pool(processes=self.cpus)\n            analysis_type_list = [self.analysistype] * len(self.runmetadata)\n            iupac_list = [self.iupac] * len(self.runmetadata)\n            cutoff_list = [self.cutoff] * len(self.runmetadata)\n            depth_list = [self.averagedepth] * len(self.runmetadata)\n            allow_soft_clip_list = [self.allow_soft_clips] * len(self.runmetadata)\n            sample_results = p.starmap(Sippr.parse_one_sample,\n                                       zip(json_files, sample_names, best_assemblies, analysis_type_list,\n                                           iupac_list, cutoff_list, depth_list, allow_soft_clip_list))\n            p.close()\n            p.join()\n        # Since we had to json-ize the sample objects, we now need to update the metadata for everything.\n        for sample in self.runmetadata:\n                sample[self.analysistype].faidict = dict()\n                sample[self.analysistype].results = dict()\n                sample[self.analysistype].avgdepth = dict()\n                sample[self.analysistype].resultssnp = dict()\n                sample[self.analysistype].snplocations = dict()\n                sample[self.analysistype].resultsgap = dict()\n                sample[self.analysistype].gaplocations = dict()\n                sample[self.analysistype].sequences = dict()\n                sample[self.analysistype].maxcoverage = dict()\n                sample[self.analysistype].mincoverage = dict()\n                sample[self.analysistype].standarddev = dict()\n                # Figure out which of the sample results to use.\n                for sample_result in sample_results:\n                    if sample_result['name'] == sample.name:\n                        sample[self.analysistype].faidict = sample_result['faidict']\n                        sample[self.analysistype].results = sample_result['results']\n                        sample[self.analysistype].avgdepth = sample_result['avgdepth']\n                        sample[self.analysistype].resultssnp = sample_result['resultssnp']\n                        sample[self.analysistype].snplocations = sample_result['snplocations']\n                        sample[self.analysistype].resultsgap = sample_result['resultsgap']\n                        sample[self.analysistype].gaplocations = sample_result['gaplocations']\n                        sample[self.analysistype].sequences = sample_result['sequences']\n                        sample[self.analysistype].maxcoverage = sample_result['maxcoverage']\n                        sample[self.analysistype].mincoverage = sample_result['mincoverage']\n                        sample[self.analysistype].standarddev = sample_result['standarddev']\n        logging.info('Done parsing BAM files')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clipper(self):\n        for sample in self.runmetadata:\n            # Create a dictionary to store all the samples that do not have features\n            replacementresults = dict()\n            try:\n                # SixteenS analyses seem to fail if results are filtered out\n                if self.analysistype != 'sixteens_full' and self.analysistype != 'resfinder':\n                    # Iterate through all the baited genes\n                    for gene in sample[self.analysistype].faidict:\n                        try:\n                            percentidentity = sample[self.analysistype].results[gene]\n                            try:\n                                # Create a list to store whether a feature is present in enough reads to discard the\n                                # sample\n                                passingfeature = list()\n                                for location, feature in sample[self.analysistype].features[gene].items():\n                                    # If the feature is present in under 30% of the reads, set the passing variable\n                                    # to true\n                                    if len(feature) < int(float(sample[self.analysistype].avgdepth[gene])) * 0.3:\n                                        passingfeature.append(True)\n                                    # Otherwise set it to false\n                                    else:\n                                        passingfeature.append(False)\n                                # If all the features are 'true' (present in fewer than 30% of the reads), add this\n                                # contig to the list of passing results\n                                if all(passingfeature):\n                                    replacementresults[gene] = percentidentity\n                            # If the allele does not have any features, it is added to the passing list\n                            except KeyError:\n                                replacementresults[gene] = percentidentity\n                        except KeyError:\n                            pass\n                    # Update the .results attribute with the filtered dictionary\n                    sample[self.analysistype].results = replacementresults\n            except AttributeError:\n                pass", "response": "Filter out results based on the presence of cigar features such as internal soft - clipping and internal soft - clipping."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports an object based on a string.", "response": "def import_string(import_name, silent=False):\r\n    \"\"\"Imports an object based on a string.  This is useful if you want to\r\n    use import paths as endpoints or something similar.  An import path can\r\n    be specified either in dotted notation (``xml.sax.saxutils.escape``)\r\n    or with a colon as object delimiter (``xml.sax.saxutils:escape``).\r\n\r\n    If `silent` is True the return value will be `None` if the import fails.\r\n\r\n    :param import_name: the dotted name for the object to import.\r\n    :param silent: if set to `True` import errors are ignored and\r\n                   `None` is returned instead.\r\n    :return: imported object\r\n    \"\"\"\r\n    # force the import name to automatically convert to strings\r\n    # __import__ is not able to handle unicode strings in the fromlist\r\n    # if the module is a package\r\n    import_name = str(import_name).replace(':', '.')\r\n    try:\r\n        try:\r\n            __import__(import_name)\r\n        except ImportError:\r\n            if '.' not in import_name:\r\n                raise\r\n        else:\r\n            return sys.modules[import_name]\r\n\r\n        module_name, obj_name = import_name.rsplit('.', 1)\r\n        try:\r\n            module = __import__(module_name, None, None, [obj_name])\r\n        except ImportError:\r\n            # support importing modules not yet set up by the parent module\r\n            # (or package for that matter)\r\n            module = import_string(module_name)\r\n\r\n        try:\r\n            return getattr(module, obj_name)\r\n        except AttributeError as e:\r\n            raise ImportError(e)\r\n\r\n    except ImportError as e:\r\n        if not silent:\r\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_mapping(self, *mapping, **kwargs):\r\n        mappings = []\r\n        if len(mapping) == 1:\r\n            if hasattr(mapping[0], 'items'):\r\n                mappings.append(mapping[0].items())\r\n            else:\r\n                mappings.append(mapping[0])\r\n        elif len(mapping) > 1:\r\n            raise TypeError('expected at most 1 positional argument, got %d' % len(mapping))\r\n\r\n        mappings.append(kwargs.items())\r\n        for mapping in mappings:\r\n            for key, value in mapping:\r\n                if key.isupper():\r\n                    self[key] = value\r\n        return True", "response": "Updates the config like update ignoring items with non - uppercase keys."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_namespace(self, namespace, lowercase=True, trim_namespace=True):\r\n        rv = {}\r\n        for key, value in six.iteritems(self):\r\n            if not key.startswith(namespace):\r\n                continue\r\n            if trim_namespace:\r\n                key = key[len(namespace):]\r\n            else:\r\n                key = key\r\n            if lowercase:\r\n                key = key.lower()\r\n            rv[key] = value\r\n        return rv", "response": "Returns a dictionary containing a subset of configuration options that match the specified namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n\n    reporter = BugReporter()\n\n    print(\"JSON report:\")\n    print(reporter.as_json())\n    print()\n\n    print(\"Markdown report:\")\n    print(reporter.as_markdown())\n\n    print(\"SQL report:\")\n    print(reporter.as_sql())\n\n    print(\"Choose the appropriate format (if you're submitting a Github Issue \"\n          \"please chose the Markdown report) and paste it!\")", "response": "Pretty - print the bug information as JSON"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget platform info :return: platform info", "response": "def get_platform_info():\n        \"\"\"Gets platform info\n\n        :return: platform info\n        \"\"\"\n\n        try:\n            system_name = platform.system()\n            release_name = platform.release()\n        except:\n            system_name = \"Unknown\"\n            release_name = \"Unknown\"\n\n        return {\n            'system': system_name,\n            'release': release_name,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bug_report():\n        platform_info = BugReporter.get_platform_info()\n        module_info = {\n            'version': hal_version.__version__,\n            'build': hal_version.__build__\n        }\n\n        return {\n            'platform': platform_info,\n            'pyhal': module_info\n        }", "response": "Generate information for a bug report"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_table(self):\n        data = get_inner_data(self.report)\n        labels = data.keys()\n        row = [\n            data[key]\n            for key in labels\n        ]\n        return list(labels), [row]", "response": "Gets report as table with columns"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_sql(self):\n\n        labels, data = self._get_table()\n        table = SqlTable(labels, data, \"{:.3f}\", \"\\n\")\n        return str(table)", "response": "Gets report as json - formatted report\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_markdown(self):\n\n        labels, data = self._get_table()\n        table = MarkdownTable(labels, data)\n        return str(table)", "response": "Gets report as json - formatted report\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a datetime to an ISO String.", "response": "def to_isostring(dt):\n    \"\"\"\n    Converts the given datetime to an ISO String.\n    This assumes the datetime is UTC.\n    \"\"\"\n    if dt.tzinfo is not None and dt.tzinfo.utcoffset(dt) > timedelta(0):\n        logging.warn('Warning: aware datetimes are interpreted as if they were naive')\n\n    # -3 to change microseconds to milliseconds\n    return dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a Protobuf Value into a python native value", "response": "def parse_value(proto):\n    \"\"\"\n    Convers a Protobuf `Value` from the API into a python native value\n    \"\"\"\n    if proto.HasField('floatValue'):\n        return proto.floatValue\n    elif proto.HasField('doubleValue'):\n        return proto.doubleValue\n    elif proto.HasField('sint32Value'):\n        return proto.sint32Value\n    elif proto.HasField('uint32Value'):\n        return proto.uint32Value\n    elif proto.HasField('binaryValue'):\n        return proto.binaryValue\n    elif proto.HasField('timestampValue'):\n        # Don't use the actual 'timestampValue' field, it contains a number\n        # that is difficult to interpret on the client. Instead parse from\n        # the ISO String also set by Yamcs.\n        return parse_isostring(proto.stringValue)\n    elif proto.HasField('stringValue'):\n        return proto.stringValue\n    elif proto.HasField('uint64Value'):\n        return proto.uint64Value\n    elif proto.HasField('sint64Value'):\n        return proto.sint64Value\n    elif proto.HasField('booleanValue'):\n        return proto.booleanValue\n    elif proto.HasField('arrayValue'):\n        return [parse_value(v) for v in proto.arrayValue]\n    elif proto.HasField('aggregateValue'):\n        return OrderedDict(zip(proto.aggregateValue.name, proto.aggregateValue.value))\n    else:\n        logging.warn('Unrecognized value type for update %s', proto)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_correlation_matrix_plot(correlation_matrix, title, feature_list):\n    chart = SimpleChart(title)\n    ax1 = chart.get_ax()\n\n    ax1.set_xticks(list(range(len(feature_list))))\n    ax1.set_xticklabels([feature_list[i] for i in range(len(feature_list))],\n                        rotation=90)\n    ax1.set_yticks(list(range(len(feature_list))))\n    ax1.set_yticklabels([feature_list[i] for i in range(len(feature_list))])\n    cax = ax1.imshow(correlation_matrix, interpolation=\"nearest\",\n                     cmap=cm.get_cmap(\"jet\", 30))\n    chart.get_fig().colorbar(cax, ticks=np.linspace(-1, 1, 21))\n\n    plt.gcf().subplots_adjust(bottom=0.25)", "response": "Creates a plot for the given correlation matrix."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logtrace(logger, msg, *args, **kwargs):\n    '''\n    If esgfpid.defaults.LOG_TRACE_TO_DEBUG, messages are treated\n    like debug messages (with an added [trace]).\n    Otherwise, they are ignored.\n    '''\n    if esgfpid.defaults.LOG_TRACE_TO_DEBUG:\n        logdebug(logger, '[trace] %s' % msg, *args, **kwargs)\n    else:\n        pass", "response": "Log a trace message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog a message at DEBUG level.", "response": "def logdebug(logger, msg, *args, **kwargs):\n    '''\n    Logs messages as DEBUG,\n    unless show=True and esgfpid.defaults.LOG_SHOW_TO_INFO=True,\n    (then it logs messages as INFO).\n    '''\n    if esgfpid.defaults.LOG_DEBUG_TO_INFO:\n        logger.info('DEBUG %s ' % msg, *args, **kwargs)\n    else:\n        logger.debug(msg, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog a message at INFO level.", "response": "def loginfo(logger, msg, *args, **kwargs):\n    '''\n    Logs messages as INFO,\n    unless esgfpid.defaults.LOG_INFO_TO_DEBUG,\n    (then it logs messages as DEBUG).\n    '''\n    if esgfpid.defaults.LOG_INFO_TO_DEBUG:\n        logger.debug(msg, *args, **kwargs)\n    else:\n        logger.info(msg, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_every_x_times(logger, counter, x, msg, *args, **kwargs):\n    '''\n    Works like logdebug, but only prints first and\n    and every xth message.\n    '''\n    if counter==1 or counter % x == 0:\n        #msg = msg + (' (counter %i)' % counter)\n        logdebug(logger, msg, *args, **kwargs)", "response": "Log a message every x times."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the number of entries matching the keyword(s) specified Args: search: (dict) Search query like {\"categ_A\": \"val_A\", \"categ_B\": \"val_B\"}, documented at https://developer.mpds.io/#Categories phases: (list) Phase IDs, according to the MPDS distinct phases concept kwargs: just a mockup Returns: count (int)", "response": "def count_data(self, search, phases=None, **kwargs):\n        \"\"\"\n        Calculate the number of entries matching the keyword(s) specified\n\n        Args:\n            search: (dict) Search query like {\"categ_A\": \"val_A\", \"categ_B\": \"val_B\"},\n                documented at https://developer.mpds.io/#Categories\n            phases: (list) Phase IDs, according to the MPDS distinct phases concept\n            kwargs: just a mockup\n\n        Returns:\n            count (int)\n        \"\"\"\n        result = self._request(search, phases=phases, pagesize=10)\n\n        if result['error']:\n            raise APIError(result['error'], result.get('code', 0))\n\n        if result['npages'] > self.maxnpages:\n            warnings.warn(\n                \"\\r\\nDataset is too big, you may risk to change maxnpages from %s to %s\" % \\\n                (self.maxnpages, int(math.ceil(result['count']/self.pagesize)))\n            )\n\n        return result['count']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data(self, search, phases=None, fields=default_fields):\n        output = []\n        fields = {\n            key: [jmespath.compile(item) if isinstance(item, str) else item() for item in value]\n            for key, value in fields.items()\n        } if fields else None\n\n        tot_count = 0\n\n        phases = list(set(phases)) if phases else []\n\n        if len(phases) > self.maxnphases:\n            all_phases = array_split(phases, int(math.ceil(\n                len(phases)/self.maxnphases\n            )))\n        else: all_phases = [phases]\n\n        nsteps = len(all_phases)\n\n        for step, current_phases in enumerate(all_phases, start=1):\n\n            counter, hits_count = 0, 0\n\n            while True:\n                result = self._request(search, phases=list(current_phases), page=counter)\n                if result['error']:\n                    raise APIError(result['error'], result.get('code', 0))\n\n                if result['npages'] > self.maxnpages:\n                    raise APIError(\n                        \"Too many hits (%s > %s), please, be more specific\" % \\\n                        (result['count'], self.maxnpages * self.pagesize),\n                        2\n                    )\n                output.extend(self._massage(result['out'], fields))\n\n                if hits_count and hits_count != result['count']:\n                    raise APIError(\"API error: hits count has been changed during the query\")\n\n                hits_count = result['count']\n\n                time.sleep(self.chillouttime)\n\n                if counter == result['npages'] - 1:\n                    break\n\n                counter += 1\n\n                if self.verbose:\n                    sys.stdout.write(\"\\r\\t%d%% of step %s from %s\" % (\n                        (counter/result['npages']) * 100, step, nsteps)\n                                    )\n                    sys.stdout.flush()\n\n            tot_count += hits_count\n\n        if len(output) != tot_count:\n            raise APIError(\"API error: collected and declared counts of hits differ\")\n\n        if self.verbose:\n            sys.stdout.write(\"Got %s hits\\r\\n\" % tot_count)\n            sys.stdout.flush()\n\n        return output", "response": "Retrieve data in JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_dataframe(self, *args, **kwargs):\n        columns = kwargs.get('columns')\n        if columns:\n            del kwargs['columns']\n        else:\n            columns = self.default_titles\n\n        return pd.DataFrame(self.get_data(*args, **kwargs), columns=columns)", "response": "Retrieve data as a Pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a crystal structure for the given data row.", "response": "def compile_crystal(datarow, flavor='pmg'):\n        \"\"\"\n        Helper method for representing the MPDS crystal structures in two flavors:\n        either as a Pymatgen Structure object, or as an ASE Atoms object.\n\n        Attention #1. Disordered structures (e.g. fractional indices in the chemical formulae)\n        are not supported by this method, and hence the occupancies are not retrieved.\n        Currently it's up to the user to take care of that (see e.g.\n        https://doi.org/10.1186/s13321-016-0129-3 etc.).\n\n        Attention #2. Pymatgen and ASE flavors are generally not compatible, e.g.\n        primitive vs. crystallographic cell is defaulted,\n        atoms wrapped or non-wrapped into the unit cell etc.\n\n        Note, that the crystal structures are not retrieved by default,\n        so for them one needs to specify the following fields:\n            - cell_abc\n            - sg_n\n            - basis_noneq\n            - els_noneq\n        e.g. like this: {'S':['cell_abc', 'sg_n', 'basis_noneq', 'els_noneq']}\n\n        Args:\n            datarow: (list) Required data to construct crystal structure:\n                [cell_abc, sg_n, basis_noneq, els_noneq]\n            flavor: (str) Either \"pmg\", or \"ase\"\n\n        Returns:\n            - if flavor is pmg, returns Pymatgen Structure object\n            - if flavor is ase, returns ASE Atoms object\n        \"\"\"\n        if not datarow or not datarow[-1]:\n            # this is either a P-entry with the cell data, which meets the search criterion,\n            # or a 'low quality' structure with no basis (just unit cell parameters)\n            return None\n\n        if len(datarow) < 4:\n            raise ValueError(\n                \"Must supply a data row that ends with the entries \"\n                \"'cell_abc', 'sg_n', 'basis_noneq', 'els_noneq'\")\n\n        cell_abc, sg_n, basis_noneq, els_noneq = \\\n            datarow[-4], int(datarow[-3]), datarow[-2], _massage_atsymb(datarow[-1])\n\n        if flavor == 'pmg' and use_pmg:\n            return Structure.from_spacegroup(\n                sg_n,\n                Lattice.from_parameters(*cell_abc),\n                els_noneq,\n                basis_noneq\n            )\n\n        elif flavor == 'ase' and use_ase:\n            atom_data = []\n\n            for num, i in enumerate(basis_noneq):\n                atom_data.append(Atom(els_noneq[num], tuple(i)))\n\n            return crystal(\n                atom_data,\n                spacegroup=sg_n,\n                cellpar=cell_abc,\n                primitive_cell=True,\n                onduplicates='replace'\n            )\n\n        else: raise APIError(\"Crystal structure treatment unavailable\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef grabImage(self, index):\n        # grab an image of the cell we are moving\n        # assume all rows same height\n        row_height = self.rowHeight(0)\n        # -5 becuase it a a little off\n        y = (row_height*index.row()) + row_height - 5\n        x = self.width()\n        rect = QtCore.QRect(5,y,x,row_height)\n        pixmap = QtGui.QPixmap()\n        pixmap = pixmap.grabWidget(self, rect)\n        return pixmap", "response": "Returns an image of the parameter row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mousePressEvent(self, event):\n        index = self.indexAt(event.pos())\n        if index.isValid():\n            self.selectRow(index.row())\n            # selecting the row sets the current index to 0,0 for tab\n            # order to work correctly, we must set the current index\n            self.setCurrentIndex(index)\n            self.parameterChanged.emit(self.model().selection(index))\n            self.edit(index, QtGui.QAbstractItemView.DoubleClicked, event)\n        super(AutoParameterTableView, self).mousePressEvent(event)", "response": "Begins edit on cell clicked if allowed and passes event to super class"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paintEvent(self, event):\n        super(AutoParameterTableView, self).paintEvent(event)\n\n        if self.dragline is not None:\n            pen = QtGui.QPen(QtCore.Qt.blue)\n            painter = QtGui.QPainter(self.viewport())\n            painter.setPen(pen)\n            painter.drawLine(self.dragline)", "response": "Adds cursor line for view if drag active."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the dropped parameter param into the protocol list.", "response": "def dropped(self, param, event):\n        \"\"\"Adds the dropped parameter *param* into the protocol list.\n\n        Re-implemented from :meth:`AbstractDragView<sparkle.gui.abstract_drag_view.AbstractDragView.dropped>`\n        \"\"\"\n        if event.source() == self or isinstance(param, AddLabel):\n            index = self.indexAt(event.pos())\n            self.model().insertRows(index.row(),1)\n            if event.source() == self:\n                self.model().setData(index, param)\n            else:\n                self.hintRequested.emit('Select Components in view to modify -- click to toggle membership of component in auto-parameter')\n                row = index.row()\n                # select rows doesn't work with -ve indexes\n                if row == -1:\n                    row = self.model().rowCount() - 1\n                self.selectRow(row)\n                self.parameterChanged.emit(self.model().selection(index))\n        self.dragActive.emit(False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntoggling the selection of comp from the currently active parameter", "response": "def componentSelection(self, comp):\n        \"\"\"Toggles the selection of *comp* from the currently active parameter\"\"\"\n        # current row which is selected in auto parameters to all component selection to\n        indexes = self.selectedIndexes()\n        index = indexes[0]\n        self.model().toggleSelection(index, comp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles requests to API and return the parsed response as a dict.", "response": "def request(self, path, method, data=None, **kwargs):\n        \"\"\"Handle requests to API\n\n        :param str path: API endpoint's path to request\n        :param str method: HTTP method to use\n        :param dict data: Data to send (optional)\n        :return: Parsed json response as :class:`dict`\n\n        Additional named argument may be passed and are directly transmitted\n        to :meth:`request` method of :class:`requests.Session` object.\n        \"\"\"\n        if self.api_token:\n            self.request_headers['X-Cachet-Token'] = self.api_token\n\n        if not path.startswith('http://') and not path.startswith('https://'):\n            url = \"%s/%s\" % (self.api_endpoint, path)\n        else:\n            url = path\n\n        if data is None:\n            data = {}\n\n        response = self.r_session.request(method, url,\n                                          data=json.dumps(data),\n                                          headers=self.request_headers,\n                                          timeout=self.timeout,\n                                          verify=self.verify,\n                                          **kwargs)\n\n        # If API returns an error, we simply raise and let caller handle it\n        response.raise_for_status()\n\n        try:\n            return response.json()\n        except ValueError:\n            return {'data': response.text}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling paginated requests to API resource cache.", "response": "def paginate_request(self, path, method, data=None, **kwargs):\n        \"\"\"Handle paginated requests to API\n\n        :param str path: API endpoint's path to request\n        :param str method: HTTP method to use\n        :param dict data: Data to send (optional)\n        :return: Response data items (:class:`Generator`)\n\n        Cachet pagination is handled and next pages requested on demand.\n\n        Additional named argument may be passed and are directly transmitted\n        to :meth:`request` method of :class:`requests.Session` object.\n        \"\"\"\n        next_page = path\n        while next_page:\n            response = self.request(next_page, method, data=data, **kwargs)\n\n            if not isinstance(response.get('data'), list):\n                next_page = None\n                yield response['data']\n            else:\n                for entry in response['data']:\n                    yield entry\n\n                # Get next page if it exists\n                try:\n                    links = response['meta']['pagination']['links']\n                    next_page = links.get('next_page')\n                except KeyError:\n                    next_page = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maybe_open(infile, mode='r'):\n    # ENH: Exception safety?\n    if isinstance(infile, basestring):\n        handle = open(infile, mode)\n        do_close = True\n    else:\n        handle = infile\n        do_close = False\n    yield handle\n    if do_close:\n        handle.close()", "response": "Open a file name or a handle and return a handle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and run a subprocess and return the process and execution time after it has completed.", "response": "def run_subprocess(executable_command,\n                   command_arguments = [],\n                   timeout=None,\n                   print_process_output=True,\n                   stdout_file=None,\n                   stderr_file=None,\n                   poll_seconds=.100,\n                   buffer_size=-1,\n                   daemon=False,\n                   return_std=False):\n    \"\"\"Create and run a subprocess and return the process and\n    execution time after it has completed.  The execution time\n    does not include the time taken for file i/o when logging\n    the output if stdout_file and stderr_file arguments are given.\n\n    Positional arguments:\n    executable_command (str) -- executable command to run\n    command_arguments (list) -- command line arguments\n    timeout (int/float) -- how many seconds to allow for process completion\n    print_process_output (bool) -- whether to print the process' live output\n    stdout_file (str) -- file to log stdout to\n    stderr_file (str) -- file to log stderr to\n    poll_seconds(int/float) -- how often in seconds to poll the subprocess\n                                to check for completion\n    daemon(bool) -- whether the process is a daemon. If True, returns process\n                    immediately after creation along with start time rather than\n                    execution time.\n    return_std (bool) -- whether to return a reference to the processes' NBSRW stdout and stderr\n    \"\"\"\n    # validate arguments\n    # list\n    assert_variable_type(command_arguments, list)\n    # strings\n    assert_variable_type(executable_command, str)\n    _string_vars = [stdout_file,\n                    stderr_file]\n    [assert_variable_type(x, [str, NoneType, unicode]) for x in _string_vars + command_arguments]\n    # bools\n    assert_variable_type(print_process_output, bool)\n    assert_variable_type(return_std, bool)\n    # floats\n    _float_vars = [timeout,\n                   poll_seconds]\n    [assert_variable_type(x, [int, float, NoneType]) for x in _float_vars]\n    global process, _nbsr_stdout, _nbsr_stderr\n    process = None\n    _nbsr_stdout = None\n    _nbsr_stderr = None\n    def _exec_subprocess():\n        # create the subprocess to run the external program\n        global process, _nbsr_stdout, _nbsr_stderr\n        process = subprocess.Popen([executable_command] + command_arguments, stdout=subprocess.PIPE, stderr=subprocess.PIPE, bufsize=buffer_size, preexec_fn=os.setsid)\n        # wrap p.stdout with a NonBlockingStreamReader object:\n        _nbsr_stdout = NBSRW(process.stdout, print_process_output, stdout_file)\n        _nbsr_stderr = NBSRW(process.stderr, print_process_output, stderr_file)\n        # if the process is a dameon break\n        # execution time returned is start time\n        if daemon:\n            return\n        # set deadline if timeout was set\n        _deadline = None\n        if timeout is not None:\n            _deadline = timeit.default_timer() + timeout\n        # poll process while it runs\n        while process.poll() is None:\n            # throw TimeoutError if timeout was specified and deadline has passed\n            if _deadline is not None and timeit.default_timer() > _deadline and process.poll() is None:\n                os.killpg(process.pid, signal.SIGTERM)\n                raise TimeoutError(\"Sub-process did not complete before %.4f seconds elapsed\" %(timeout))\n            # sleep to yield for other processes\n            time.sleep(poll_seconds)\n    execution_time = timeit.timeit(_exec_subprocess, number=1)\n    # return process to allow application to communicate with it\n    # and extract whatever info like stdout, stderr, returncode\n    # also return execution_time to allow\n    if return_std:\n        return process, execution_time, _nbsr_stdout, _nbsr_stderr\n    return process, execution_time"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retrieve_file_handles_of_same_dataset(self, **args):\n        '''\n        :return: List of handles, or empty list. Should never return None.\n        :raise: SolrSwitchedOff\n        :raise SolrError: If both strategies to find file handles failed.\n        '''\n        mandatory_args = ['drs_id', 'version_number', 'data_node', 'prefix']\n        esgfpid.utils.check_presence_of_mandatory_args(args, mandatory_args)\n        self.__reset_error_messages()\n\n        # Try plan A\n        file_handles = None\n        try:\n            file_handles = self.__strategy1(args) # can raise SolrError or SolrSwitchedOff, but can't return None\n        except esgfpid.exceptions.SolrError as e:\n            self.__error_messages.append('Error during first query: '+e.message)\n\n        if file_handles is not None and len(file_handles)>0:\n            LOGGER.debug('Retrieved file handles from solr in first query.')\n            return file_handles\n\n        # Try plan B\n        try:\n            file_handles = self.__strategy2(args) # can raise SolrError or SolrSwitchedOff, but can't return None\n        except esgfpid.exceptions.SolrError as e:\n            self.__error_messages.append('Error during second query: '+e.message)\n            msg = '/n'.join(self.__error_messages)\n            raise esgfpid.exceptions.SolrError('Failure in both queries. Messages:\\n'+msg)\n\n        return file_handles", "response": "Retrieve file handles of the same dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_row_tag(row, tag):\n\n        is_empty = True\n        data = []\n        for column_label in row.find_all(tag):  # cycle through all labels\n            data.append(\n                String(column_label.text).strip_bad_html()\n            )\n            if data[-1]:\n                is_empty = False\n\n        if not is_empty:\n            return data\n\n        return None", "response": "Parses a row and gets columns matching a tag"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_row(row):\n\n        data = []\n\n        labels = HtmlTable._get_row_tag(row, \"th\")\n        if labels:\n            data += labels\n\n        columns = HtmlTable._get_row_tag(row, \"td\")\n        if columns:\n            data += columns\n\n        return data", "response": "Parses HTML row to list of values in the navigable table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses data in table", "response": "def parse(self):\n        \"\"\"Parses data in table\n\n        :return: List of list of values in table\n        \"\"\"\n        data = []  # add name of section\n\n        for row in self.soup.find_all(\"tr\"):  # cycle through all rows\n            parsed = self._parse_row(row)\n            if parsed:\n                data.append(parsed)\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete module and sub - modules from sys. module.", "response": "def delete_module(modname):\n    \"\"\"\n    Delete module and sub-modules from `sys.module`\n    \"\"\"\n    try:\n        _ = sys.modules[modname]\n    except KeyError:\n        raise ValueError(\"Module not found in sys.modules: '{}'\".format(modname))\n\n    for module in list(sys.modules.keys()):\n        if module and module.startswith(modname):\n            del sys.modules[module]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreloads the Python module", "response": "def reload_module(module):\n    \"\"\"\n    Reload the Python module\n    \"\"\"\n    try:\n        # For Python 2.x\n        reload(module)\n    except (ImportError, NameError):\n        # For <= Python3.3:\n        import imp\n        imp.reload(module)\n    except (ImportError, NameError):\n        # For >= Python3.4\n        import importlib\n        importlib.reload(module)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lazy_load_modules(*modules):\n    def decorator(function):\n        def wrapper(*args, **kwargs):\n\n            module_dict = {}\n            for module_string in modules:\n                module = __import__(module_string)\n\n                # Add `module` entry in `sys.modules`. After deleting the module\n                # from `sys.modules` and re-importing the module don't update\n                # the module entry in `sys.modules` dict\n                sys.modules[module.__package__] = module\n                reload_module(module)\n                module_dict[module_string] = module\n\n            func_response = function(*args, **kwargs)\n\n            for module_string, module in module_dict.items():\n                # delete idna module\n                delete_module(module_string)\n                del module  # delete reference to idna\n\n            return func_response\n        return wrapper\n    return decorator", "response": "Decorator to load modules and delete the module from imports once the task is done."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format(self, record):\n        if record.levelno == DEBUG:\n            return self.debug_formatter.format(record)\n        if record.levelno == INFO:\n            return self.info_formatter.format(record)\n        if record.levelno == ERROR:\n            return self.error_formatter.format(record)\n        if record.levelno == WARNING:\n            return self.warning_formatter.format(record)\n        if record.levelno == CRITICAL:\n            return self.critical_formatter.format(record)", "response": "Formats the record using the corresponding formatter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_child(self, widget):\n        li_itm = _li(id=self.id + str(self._count))\n        li_itm.add_child(widget)\n\n        super(List, self).add_child(li_itm)\n        self._items.append((widget, li_itm))\n        self._count += 1", "response": "Append a widget to the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a child from the list.", "response": "def remove_child(self, widget):\n        \"\"\"\n        Remove a widget from the list.\n\n        :param widget: Object inheriting :class:`~.widgets.base.BaseElement`\n        \"\"\"\n        raw = list(filter(lambda x: x[0] == widget, self._items))\n        if raw:\n            itm, wrapped = raw[0]\n            self._items.remove(raw[0])\n            super(List, self).remove_child(wrapped)\n        else:\n            raise ValueError(\"Child not in list.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering pages table for ajax function.", "response": "def list_pages_ajax(request, invalid_move=False):\n    \"\"\"Render pages table for ajax function.\"\"\"\n    language = get_language_from_request(request)\n    pages = Page.objects.root()\n    context = {\n        'invalid_move': invalid_move,\n        'language': language,\n        'pages': pages,\n    }\n    return render_to_response(\"admin/basic_cms/page/change_list_table.html\",\n        context,\n        context_instance=RequestContext(request)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef modify_content(request, page_id, content_type, language_id):\n    page = get_object_or_404(Page, pk=page_id)\n    perm = request.user.has_perm('pages.change_page')\n    if perm and request.method == 'POST':\n        content = request.POST.get('content', False)\n        if not content:\n            raise Http404\n        page = Page.objects.get(pk=page_id)\n        if settings.PAGE_CONTENT_REVISION:\n            Content.objects.create_content_if_changed(page, language_id,\n                                                      content_type, content)\n        else:\n            Content.objects.set_or_create_content(page, language_id,\n                                                  content_type, content)\n        page.invalidate()\n        # to update last modification date\n        page.save()\n\n        return HttpResponse('ok')\n    raise Http404", "response": "Modify the content of a page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef move_page(request, page_id, extra_context=None):\n    page = Page.objects.get(pk=page_id)\n\n    target = request.POST.get('target', None)\n    position = request.POST.get('position', None)\n    if target is not None and position is not None:\n        try:\n            target = Page.objects.get(pk=target)\n        except Page.DoesNotExist:\n            pass\n            # TODO: should use the django message system\n            # to display this message\n            # _('Page could not been moved.')\n        else:\n            page.invalidate()\n            target.invalidate()\n            from mptt.exceptions import InvalidMove\n            invalid_move = False\n            try:\n                page.move_to(target, position)\n            except InvalidMove:\n                invalid_move = True\n            return list_pages_ajax(request, invalid_move)\n    return HttpResponseRedirect('../../')", "response": "Move the page to the requested target at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sub_menu(request, page_id):\n    page = Page.objects.get(id=page_id)\n    pages = page.children.all()\n    page_languages = settings.PAGE_LANGUAGES\n    return render_to_response(\"admin/basic_cms/page/sub_menu.html\", {\n        'page': page,\n        'pages': pages,\n        'page_languages': page_languages,\n    }, context_instance=RequestContext(request))", "response": "Render the children of the requested page with the sub_menu\n    template."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a C header from the front and body.", "response": "def make_c_header(name, front, body):\n    \"\"\"\n    Build a C header from the front and body.\n    \"\"\"\n    return \"\"\"\n{0}\n\n\n# ifndef _GU_ZHENGXIONG_{1}_H\n# define _GU_ZHENGXIONG_{1}_H\n\n\n{2}\n\n\n# endif /* {3}.h */\n    \"\"\".strip().format(front, name.upper(), body, name) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_windll(structs):\n    name = 'windll_t'\n    var = 'windll'\n    struct_def = \"\"\"\ntypedef struct _{0} {{\n{1}\n}}\n{0};\n\"\"\".strip().format(name, ''.join(structs))\n    x86 = reloc_var(var, 'reloc_delta', True, name)\n    x64 = '{0} *{1} = &_{1};\\n'.format(name, var)\n    return struct_def, x86, x64", "response": "Build the windll structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reloc_var(var_name, reloc_delta, pointer, var_type):\n    template = '{0} {3}{1} = RELOC_VAR(_{1}, {2}, {0});\\n'\n    return template.format(\n        var_type, var_name, reloc_delta,\n        '*' if pointer else ''\n    )", "response": "Build C source code to relocate a variable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_c_args(arg_pairs):\n    logging.debug(arg_pairs)\n    c_args = [\n        '{} {}'.format(arg_type, arg_name) if arg_name else arg_type\n        for dummy_number, arg_type, arg_name in sorted(arg_pairs)\n    ]\n    return ', '.join(c_args)", "response": "Build a C argument list from return type and arguments pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the InterOp files and load the data into the metadata object.", "response": "def interop_parse(self):\n        \"\"\"\n        Use interop to parse the files in the InterOp folder to extract the number of reads mapping to PhiX as well as\n        the error rate\n        \"\"\"\n        # Parse the files and load the data\n        try:\n            run_metrics = py_interop_run_metrics.run_metrics()\n            valid_to_load = py_interop_run.uchar_vector(py_interop_run.MetricCount, 0)\n            py_interop_run_metrics.list_summary_metrics_to_load(valid_to_load)\n            run_metrics.read(self.path, valid_to_load)\n            summary = py_interop_summary.run_summary()\n            py_interop_summary.summarize_run_metrics(run_metrics, summary)\n            # PhiX error rate for run over all \"usable cycles\"\n            errorrate = summary.total_summary().error_rate()\n            # Percent aligned PhiX\n            pctaligned = summary.total_summary().percent_aligned()\n            # Add the error rate and the percent of reads that align to PhiX to the metadata object\n            for sample in self.metadata:\n                sample.run.error_rate = '{:.2f}'.format(errorrate)\n                sample.run.phix_aligned = '{:.2f}'.format(pctaligned)\n        except:\n            for sample in self.metadata:\n                sample.run.error_rate = 'ND'\n                sample.run.phix_aligned = 'ND'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing by the bowtie _validate method (PRIVATE).", "response": "def _validate_incompatibilities(self, incompatibles):\n        \"\"\"Used by the bowtie _validate method (PRIVATE).\"\"\"\n        for element in incompatibles:\n            if type(element) is list:\n                i = [a for a in element if self._get_parameter(a)]\n                if len(i) > 1:\n                    raise ValueError(\"Options {} are incompatible\".format(\" and \".join(i)))\n            elif type(incompatibles) is dict:\n                if self._get_parameter(element):\n                    for b in incompatibles[element]:\n                        if self._get_parameter(b):\n                            raise ValueError(\"Options %s and %s are incompatible.\"\n                                             % (element, b))\n            else:\n                for a in element:\n                    if self._get_parameter(a):\n                        for b in incompatibles[a]:\n                            if self._get_parameter(b):\n                                raise ValueError(\"Options %s and %s are incompatible.\"\n                                                 % (a, b))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompiles source files and link object files.", "response": "def local_build(native, *args, **kwargs):\n    \"\"\"\n    Compile source files and link object files.\n    \"\"\"\n    method = 'native_build' if native else 'build'\n    logging.debug(_('build type: %s'), method)\n    return getattr(Builder(), method)(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncompiling source files and link object files.", "response": "def build(self, filenames, cl_args=None, link_args=None,\n              x64=False, out_dir=''):\n        \"\"\"\n        Compile source files and link object files.\n        \"\"\"\n        if not cl_args:\n            cl_args = []\n        if not link_args:\n            link_args = []\n        msvc, lib = self.vc.get_bin_and_lib(x64)\n        lib = self.make_lib(lib)\n        if out_dir:\n            cl_args.append('/Fo:' + out_dir + '\\\\')\n        include = self.make_inc(self.vc.inc + self.sdk.inc)\n        cl_args.extend(include + filenames)\n        try:\n            msvc.run_cl('/c', *cl_args)\n        except CalledProcessError as error:\n            logging.error(_('failed to compile: %s'), filenames)\n            logging.error(_('cl.exe returned:\\n%s'), error.output)\n            return False\n        link_args.extend(lib + self.make_objs(filenames, out_dir))\n        try:\n            msvc.run_link(*link_args)\n        except CalledProcessError as error:\n            logging.error(_('failed to link: %s'), filenames)\n            logging.error(_('link.exe returned:\\n%s'), error.output)\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_inc(incs):\n        inc_args = [['/I', inc] for inc in incs]\n        return list(chain.from_iterable(inc_args))", "response": "Make include directory for link. exe.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking object file names for cl. exe and link. exe.", "response": "def make_objs(names, out_dir=''):\n        \"\"\"\n        Make object file names for cl.exe and link.exe.\n        \"\"\"\n        objs = [replace_ext(name, '.obj') for name in names]\n        if out_dir:\n            objs = [os.path.join(out_dir, obj) for obj in objs]\n        return objs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef examples():\n    ''' Examples of how to use. Default are that some functions are commented out in order\n        to not cause harm to existing metadata within the database.\n    '''\n    sci = InterLexClient(\n        api_key = os.environ.get('INTERLEX_API_KEY'),\n        base_url = 'https://beta.scicrunch.org/api/1/', # NEVER CHANGE\n    )\n    entity = {\n        'label': 'brain115',\n        'type': 'fde', # broken at the moment NEEDS PDE HARDCODED\n        'definition': 'Part of the central nervous system',\n        'comment': 'Cannot live without it',\n        'superclass': {\n            'ilx_id': 'ilx_0108124', # ILX ID for Organ\n        },\n        'synonyms': [\n            {\n                'literal': 'Encephalon'\n            },\n            {\n                'literal': 'Cerebro'\n            },\n        ],\n        'existing_ids': [\n            {\n                'iri': 'http://uri.neuinfo.org/nif/nifstd/birnlex_796',\n                'curie': 'BIRNLEX:796',\n            },\n        ],\n    }\n    simple_entity = {\n        'label': entity['label'],\n        'type': entity['type'], # broken at the moment NEEDS PDE HARDCODED\n        'definition': entity['definition'],\n        'comment': entity['comment'],\n        'superclass': entity['superclass']['ilx_id'],\n        'synonyms': [syn['literal'] for syn in entity['synonyms']],\n        'predicates': {'tmp_0381624': 'http://example_dbxref'}\n    }\n    annotation = {\n        'term_ilx_id': 'ilx_0101431', # brain ILX ID\n        'annotation_type_ilx_id': 'tmp_0381624', # hasDbXref ILX ID\n        'annotation_value': 'PMID:12345',\n    }\n    relationship = {\n        'entity1_ilx': 'ilx_0101431', # brain\n        'relationship_ilx': 'ilx_0115023', # Related to\n        'entity2_ilx': 'ilx_0108124', #organ\n    }\n    update_entity_data = {\n        'ilx_id': 'ilx_0101431',\n        'label': 'Brain',\n        'definition': 'update_test!!',\n        'type': 'fde',\n        'comment': 'test comment',\n        'superclass': 'ilx_0108124',\n        'synonyms': ['test', 'test2', 'test2'],\n    }\n    # resp = sci.delete_annotation(**{\n    #     'term_ilx_id': 'ilx_0101431', # brain ILX ID\n    #     'annotation_type_ilx_id': 'ilx_0115071', # hasConstraint ILX ID\n    #     'annotation_value': 'test_12345',\n    # })\n    relationship = {\n        'entity1_ilx': 'http://uri.interlex.org/base/ilx_0100001', # (R)N6 chemical ILX ID\n        'relationship_ilx': 'http://uri.interlex.org/base/ilx_0112772', # Afferent projection ILX ID\n        'entity2_ilx': 'http://uri.interlex.org/base/ilx_0100000', #1,2-Dibromo chemical ILX ID\n    }", "response": "This function returns a list of examples that can be used to create a new object in order to use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_response(self, response: requests.models.Response) -> dict:\n        try:\n            output = response.json()\n        except json.JSONDecodeError: # Server is having a bad day and crashed.\n            raise self.BadResponseError(\n                'Json not returned with status code [' + str(response.status_code) + ']')\n\n        if response.status_code == 400:\n            return output\n\n        if response.status_code not in [200, 201]: # Safety catch.\n            raise self.BadResponseError(\n                str(output) + ': with status code [' + str(response.status_code) +\n                '] and params:' + str(output))\n\n        return output['data']", "response": "Checks for correct data response and status codes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, url: str) -> List[dict]:\n        response = requests.get(\n            url,\n            headers = {'Content-type': 'application/json'},\n            auth = ('scicrunch', 'perl22(query)') # for test2.scicrunch.org\n        )\n        output = self.process_response(response)\n        return output", "response": "Get the data from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post(self, url: str, data: List[dict]) -> List[dict]:\n        data.update({\n            'key': self.api_key,\n        })\n        response = requests.post(\n            url,\n            data = json.dumps(data),\n            headers = {'Content-type': 'application/json'},\n            auth = ('scicrunch', 'perl22(query)') # for test2.scicrunch.org\n        )\n        output = self.process_response(response)\n        return output", "response": "Post data to the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_superclass(self, entity: List[dict]) -> List[dict]:\n        superclass = entity.pop('superclass')\n        label = entity['label']\n        if not superclass.get('ilx_id'):\n            raise self.SuperClassDoesNotExistError(\n                f'Superclass not given an interlex ID for label: {label}')\n        superclass_data = self.get_entity(superclass['ilx_id'])\n        if not superclass_data['id']:\n            raise self.SuperClassDoesNotExistError(\n                'Superclass ILX ID: ' + superclass['ilx_id'] + ' does not exist in SciCrunch')\n        # BUG: only excepts superclass_tid\n        entity['superclasses'] = [{'superclass_tid': superclass_data['id']}]\n        return entity", "response": "Processes the SuperClass field in the given entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_synonyms(self, entity: List[dict]) -> List[dict]:\n        label = entity['label']\n        for synonym in entity['synonyms']:\n            # these are internal errors and users should never see them\n            if 'literal' not in synonym:\n                raise ValueError(f'Synonym not given a literal for label: {label}')\n            elif len(synonym) > 1:\n                raise ValueError(f'Too many keys in synonym for label: {label}')\n        return entity", "response": "Takes a list of dicts and checks that the key value is in proper format for synonyms."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_existing_ids(self, entity: List[dict]) -> List[dict]:\n        label = entity['label']\n        existing_ids = entity['existing_ids']\n        for existing_id in existing_ids:\n            if 'curie' not in existing_id or 'iri' not in existing_id:\n                raise ValueError(\n                    f'Missing needing key(s) in existing_ids for label: {label}')\n            elif len(existing_id) > 2:\n                raise ValueError(\n                    f'Extra keys not recognized in existing_ids for label: {label}')\n        return entity", "response": "Ensures that the key and value of the existing_ids field is in proper format for the entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef crude_search_scicrunch_via_label(self, label:str) -> dict:\n        url = self.base_url + 'term/search/{term}?key={api_key}'.format(\n            term = label,\n            api_key = self.api_key,\n        )\n        return self.get(url)", "response": "Search for a label in the system"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_scicrunch_for_label(self, label: str) -> dict:\n        list_of_crude_matches = self.crude_search_scicrunch_via_label(label)\n        for crude_match in list_of_crude_matches:\n            # If labels match\n            if crude_match['label'].lower().strip() == label.lower().strip():\n                complete_data_of_crude_match = self.get_entity(crude_match['ilx'])\n                crude_match_label = crude_match['label']\n                crude_match_user_id = complete_data_of_crude_match['uid']\n                # If label was created by you\n                if str(self.user_id) == str(crude_match_user_id):\n                    return complete_data_of_crude_match # You created the entity already\n        # No label AND user id match\n        return {}", "response": "Sees if label with your user ID already exists in interlex and if there are multiple labels with your user ID already exists in interlex and there are no more than one label in interlex."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget full meta data from an ILX ID", "response": "def get_entity(self, ilx_id: str) -> dict:\n        \"\"\" Gets full meta data (expect their annotations and relationships) from is ILX ID \"\"\"\n        ilx_id = self.fix_ilx(ilx_id)\n        url = self.base_url + \"ilx/search/identifier/{identifier}?key={api_key}\".format(\n            identifier = ilx_id,\n            api_key = self.api_key,\n        )\n        return self.get(url)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_raw_entity(self, entity: dict) -> dict:\n\n        needed_in_entity = set([\n            'label',\n            'type',\n        ])\n        options_in_entity = set([\n            'label',\n            'type',\n            'definition',\n            'comment',\n            'superclass',\n            'synonyms',\n            'existing_ids'\n        ])\n        prime_entity_url = self.base_url + 'ilx/add'\n        add_entity_url = self.base_url + 'term/add'\n\n        ### Checking if key/value format is correct ###\n        # Seeing if you are missing a needed key\n        if (set(entity) & needed_in_entity) != needed_in_entity:\n            raise self.MissingKeyError(\n                'You need key(s): '+ str(needed_in_entity - set(entity)))\n        # Seeing if you have other options not included in the description\n        elif (set(entity) | options_in_entity) != options_in_entity:\n            raise self.IncorrectKeyError(\n                'Unexpected key(s): ' + str(set(entity) - options_in_entity))\n        entity['type'] = entity['type'].lower() # BUG: server only takes lowercase\n        if entity['type'] not in ['term', 'relationship', 'annotation', 'cde', 'fde', 'pde']:\n            raise TypeError(\n                'Entity should be one of the following: ' +\n                'term, relationship, annotation, cde, fde, pde')\n        if entity.get('superclass'):\n            entity = self.process_superclass(entity)\n        if entity.get('synonyms'):\n            entity = self.process_synonyms(entity)\n        if entity.get('existing_ids'):\n            entity = self.process_existing_ids(entity)\n        entity['uid'] = self.user_id # BUG: php lacks uid update\n\n        ### Adding entity to SciCrunch ###\n        entity['term'] = entity.pop('label') # ilx/add nuance\n        ilx_data = self.post(\n            url = prime_entity_url,\n            data = entity.copy(),\n        ) # requesting spot in server for entity\n        if ilx_data.get('ilx'):\n            ilx_id = ilx_data['ilx']\n        else:\n            ilx_id = ilx_data['fragment'] # beta.scicrunch.org\n        entity['label'] = entity.pop('term') # term/add nuance\n        entity['ilx'] = ilx_id # need entity ilx_id to place entity in db\n\n        output = self.post(\n            url = add_entity_url,\n            data = entity.copy(),\n        ) # data represented in SciCrunch interface\n\n        ### Checking if label already exisits ###\n        if output.get('errormsg'):\n            if 'already exists' in output['errormsg'].lower():\n                prexisting_data = self.check_scicrunch_for_label(entity['label'])\n                if prexisting_data:\n                    print(\n                        'You already added entity', entity['label'],\n                        'with ILX ID:', prexisting_data['ilx'])\n                    return prexisting_data\n                self.Error(output)  # FIXME what is the correct error here?\n            self.Error(output)  # FIXME what is the correct error here?\n\n        # BUG: server output incomplete compared to search via ilx ids\n        output = self.get_entity(output['ilx'])\n\n        return output", "response": "Adds a raw entity to the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the entity with the given label type comment and superclass.", "response": "def update_entity(\n        self,\n        ilx_id: str,\n        label: str = None,\n        type: str = None,\n        definition: str = None,\n        comment: str = None,\n        superclass: str = None,\n        synonyms: list = None) -> dict:\n        \"\"\" Updates pre-existing entity as long as the api_key is from the account that created it\n\n            Args:\n                label: name of entity\n                type: entities type\n                    Can be any of the following: term, cde, fde, pde, annotation, relationship\n                definition: entities definition\n                comment: a foot note regarding either the interpretation of the data or the data itself\n                superclass: entity is a sub-part of this entity\n                    Example: Organ is a superclass to Brain\n                synonyms: entity synonyms\n\n            Returns:\n                Server response that is a nested dictionary format\n        \"\"\"\n\n        template_entity_input = {k:v for k, v in locals().copy().items() if k != 'self'}\n        if template_entity_input.get('superclass'):\n            template_entity_input['superclass'] = self.fix_ilx(template_entity_input['superclass'])\n\n        existing_entity = self.get_entity(ilx_id=ilx_id)\n        if not existing_entity['id']: # TODO: Need to make a proper ilx_id check error\n            raise self.EntityDoesNotExistError(\n                f'ilx_id provided {ilx_id} does not exist')\n\n        update_url = self.base_url + 'term/edit/{id}'.format(id=existing_entity['id'])\n\n        if label:\n            existing_entity['label'] = label\n\n        if type:\n            existing_entity['type'] = type\n\n        if definition:\n            existing_entity['definition'] = definition\n\n        if comment:\n            existing_entity['comment'] = comment\n\n        if superclass:\n            existing_entity['superclass'] = {'ilx_id': superclass}\n            existing_entity = self.process_superclass(existing_entity)\n\n        # If a match use old data, else append new synonym\n        if synonyms:\n            if existing_entity['synonyms']:\n                new_existing_synonyms = []\n                existing_synonyms = {syn['literal'].lower():syn for syn in existing_entity['synonyms']}\n                for synonym in synonyms:\n                    existing_synonym = existing_synonyms.get(synonym.lower())\n                    if not existing_synonym:\n                        new_existing_synonyms.append({'literal': synonym})\n                    else:\n                        new_existing_synonyms.append(existing_synonym)\n                existing_entity['synonyms'] = new_existing_synonyms\n\n        # Just in case I need this...\n        # if synonyms_to_delete:\n        #     if existing_entity['synonyms']:\n        #         remaining_existing_synonyms = []\n        #         existing_synonyms = {syn['literal'].lower():syn for syn in existing_entity['synonyms']}\n        #         for synonym in synonyms:\n        #             if existing_synonyms.get(synonym.lower()):\n        #                 existing_synonyms.pop(synonym.lower())\n        #             else:\n        #                 print('WARNING: synonym you wanted to delete', synonym, 'does not exist')\n        #         existing_entity['synonyms'] = list(existing_synonyms.values())\n\n        response = self.post(\n            url = update_url,\n            data = existing_entity,\n        )\n\n        # BUG: server response is bad and needs to actually search again to get proper format\n        raw_entity_outout = self.get_entity(response['ilx'])\n\n        entity_output = {}\n        ics = [(e['iri'], e['curie'])\n               for e in raw_entity_outout['existing_ids']]\n        entity_output['iri'], entity_output['curie'] = sorted((i, c)\n                                                    for i, c in ics\n                                                    if 'ilx_' in i)[0]\n        ### FOR NEW BETA. Old can have 'ilx_' in the ids ###\n        if 'tmp' in raw_entity_outout['ilx']:\n            _id = raw_entity_outout['ilx'].split('_')[-1]\n            entity_output['iri'] = 'http://uri.interlex.org/base/tmp_' + _id\n            entity_output['curie'] = 'TMP:' + _id\n\n        print(template_entity_input)\n        for key, value in template_entity_input.items():\n            if key == 'superclass':\n                if raw_entity_outout.get('superclasses'):\n                    entity_output[key] = raw_entity_outout['superclasses'][0]['ilx']\n            elif key == 'synonyms':\n                entity_output[key] = [syn['literal']\n                                      for syn in raw_entity_outout['synonyms']]\n            elif key == 'ilx_id':\n                pass\n            else:\n                entity_output[key] = str(raw_entity_outout[key])\n\n        if entity_output.get('superclass'):\n            entity_output['superclass'] = self.ilx_base_url + entity_output['superclass']\n        entity_output['ilx'] = self.ilx_base_url + raw_entity_outout['ilx']\n\n        return entity_output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_annotation(\n        self,\n        term_ilx_id: str,\n        annotation_type_ilx_id: str,\n        annotation_value: str) -> dict:\n        \"\"\" Adding an annotation value to a prexisting entity\n\n        An annotation exists as 3 different parts:\n            1. entity with type term, cde, fde, or pde\n            2. entity with type annotation\n            3. string value of the annotation\n\n        Example:\n            annotation = {\n                'term_ilx_id': 'ilx_0101431', # brain ILX ID\n                'annotation_type_ilx_id': 'ilx_0381360', # hasDbXref ILX ID\n                'annotation_value': 'http://neurolex.org/wiki/birnlex_796',\n            }\n        \"\"\"\n        url = self.base_url + 'term/add-annotation'\n\n        term_data = self.get_entity(term_ilx_id)\n        if not term_data['id']:\n            exit(\n                'term_ilx_id: ' + term_ilx_id + ' does not exist'\n            )\n        anno_data = self.get_entity(annotation_type_ilx_id)\n        if not anno_data['id']:\n            exit(\n                'annotation_type_ilx_id: ' + annotation_type_ilx_id +\n                ' does not exist'\n            )\n\n        data = {\n            'tid': term_data['id'],\n            'annotation_tid': anno_data['id'],\n            'value': annotation_value,\n            'term_version': term_data['version'],\n            'annotation_term_version': anno_data['version'],\n            'orig_uid': self.user_id, # BUG: php lacks orig_uid update\n        }\n\n        output = self.post(\n            url = url,\n            data = data,\n        )\n\n        ### If already exists, we return the actual annotation properly ###\n        if output.get('errormsg'):\n            if 'already exists' in output['errormsg'].lower():\n                term_annotations = self.get_annotation_via_tid(term_data['id'])\n                for term_annotation in term_annotations:\n                    if str(term_annotation['annotation_tid']) == str(anno_data['id']):\n                        if term_annotation['value'] == data['value']:\n                            print(\n                                'Annotation: [' + term_data['label'] + ' -> ' + anno_data['label'] +\n                                ' -> ' + data['value'] + '], already exists.'\n                            )\n                            return term_annotation\n                exit(output)\n            exit(output)\n\n        return output", "response": "Adds an annotation value to a prexisting entity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_annotation(\n        self,\n        term_ilx_id: str,\n        annotation_type_ilx_id: str,\n        annotation_value: str) -> dict:\n        \"\"\" If annotation doesnt exist, add it\n        \"\"\"\n\n        term_data = self.get_entity(term_ilx_id)\n        if not term_data['id']:\n            exit(\n                'term_ilx_id: ' + term_ilx_id + ' does not exist'\n            )\n        anno_data = self.get_entity(annotation_type_ilx_id)\n        if not anno_data['id']:\n            exit(\n                'annotation_type_ilx_id: ' + annotation_type_ilx_id +\n                ' does not exist'\n            )\n\n        entity_annotations = self.get_annotation_via_tid(term_data['id'])\n        annotation_id = ''\n\n        for annotation in entity_annotations:\n            if str(annotation['tid']) == str(term_data['id']):\n                if str(annotation['annotation_tid']) == str(anno_data['id']):\n                    if str(annotation['value']) == str(annotation_value):\n                        annotation_id = annotation['id']\n                        break\n        if not annotation_id:\n            print('''WARNING: Annotation you wanted to delete does not exist ''')\n            return None\n\n        url = self.base_url + 'term/edit-annotation/{annotation_id}'.format(\n            annotation_id = annotation_id\n        )\n\n        data = {\n            'tid': ' ', # for delete\n            'annotation_tid': ' ', # for delete\n            'value': ' ', # for delete\n            'term_version': ' ',\n            'annotation_term_version': ' ',\n        }\n\n        output = self.post(\n            url = url,\n            data = data,\n        )\n\n        # check output\n        return output", "response": "Delete an annotation from the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_relationship(\n        self,\n        entity1_ilx: str,\n        relationship_ilx: str,\n        entity2_ilx: str) -> dict:\n        \"\"\" Adds relationship connection in Interlex\n\n        A relationship exists as 3 different parts:\n            1. entity with type term, cde, fde, or pde\n            2. entity with type relationship that connects entity1 to entity2\n                -> Has its' own meta data, so no value needed\n            3. entity with type term, cde, fde, or pde\n        \"\"\"\n\n        url = self.base_url + 'term/add-relationship'\n\n        entity1_data = self.get_entity(entity1_ilx)\n        if not entity1_data['id']:\n            exit(\n                'entity1_ilx: ' + entity1_ilx + ' does not exist'\n            )\n        relationship_data = self.get_entity(relationship_ilx)\n        if not relationship_data['id']:\n            exit(\n                'relationship_ilx: ' + relationship_ilx + ' does not exist'\n            )\n        entity2_data = self.get_entity(entity2_ilx)\n        if not entity2_data['id']:\n            exit(\n                'entity2_ilx: ' + entity2_ilx + ' does not exist'\n            )\n\n        data = {\n            'term1_id': entity1_data['id'],\n            'relationship_tid': relationship_data['id'],\n            'term2_id': entity2_data['id'],\n            'term1_version': entity1_data['version'],\n            'term2_version': entity2_data['version'],\n            'relationship_term_version': relationship_data['version'],\n            'orig_uid': self.user_id, # BUG: php lacks orig_uid update\n        }\n\n        output = self.post(\n            url = url,\n            data = data,\n        )\n        ### If already exists, we return the actual relationship properly ###\n        if output.get('errormsg'):\n            if 'already exists' in output['errormsg'].lower():\n                term_relationships = self.get_relationship_via_tid(entity1_data['id'])\n                for term_relationship in term_relationships:\n                    if str(term_relationship['term2_id']) == str(entity2_data['id']):\n                        if term_relationship['relationship_tid'] == relationship_data['id']:\n                            print(\n                                'relationship: [' + entity1_data['label'] + ' -> ' +\n                                relationship_data['label'] + ' -> ' + entity2_data['label'] +\n                                '], already exists.'\n                            )\n                            return term_relationship\n                exit(output)\n            exit(output)\n\n        return output", "response": "Adds relationship between two entities."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding relationship connection in Interlex A relationship exists as 3 different parts: 1. entity with type term, cde, fde, or pde 2. entity with type relationship that connects entity1 to entity2 -> Has its' own meta data, so no value needed 3. entity with type term, cde, fde, or pde", "response": "def delete_relationship(\n        self,\n        entity1_ilx: str,\n        relationship_ilx: str,\n        entity2_ilx: str) -> dict:\n        \"\"\" Adds relationship connection in Interlex\n\n        A relationship exists as 3 different parts:\n            1. entity with type term, cde, fde, or pde\n            2. entity with type relationship that connects entity1 to entity2\n                -> Has its' own meta data, so no value needed\n            3. entity with type term, cde, fde, or pde\n        \"\"\"\n\n        entity1_data = self.get_entity(entity1_ilx)\n        if not entity1_data['id']:\n            exit(\n                'entity1_ilx: ' + entity1_data + ' does not exist'\n            )\n        relationship_data = self.get_entity(relationship_ilx)\n        if not relationship_data['id']:\n            exit(\n                'relationship_ilx: ' + relationship_ilx + ' does not exist'\n            )\n        entity2_data = self.get_entity(entity2_ilx)\n        if not entity2_data['id']:\n            exit(\n                'entity2_ilx: ' + entity2_data + ' does not exist'\n            )\n\n        data = {\n            'term1_id': ' ', #entity1_data['id'],\n            'relationship_tid': ' ', #relationship_data['id'],\n            'term2_id': ' ',#entity2_data['id'],\n            'term1_version': entity1_data['version'],\n            'term2_version': entity2_data['version'],\n            'relationship_term_version': relationship_data['version'],\n            'orig_uid': self.user_id, # BUG: php lacks orig_uid update\n        }\n\n        entity_relationships = self.get_relationship_via_tid(entity1_data['id'])\n        # TODO: parse through entity_relationships to see if we have a match; else print warning and return None\n\n        relationship_id = None\n        for relationship in entity_relationships:\n            if str(relationship['term1_id']) == str(entity1_data['id']):\n                if str(relationship['term2_id']) == str(entity2_data['id']):\n                    if str(relationship['relationship_tid']) == str(relationship_data['id']):\n                        relationship_id = relationship['id']\n                        break\n        if not relationship_id:\n            print('''WARNING: Annotation you wanted to delete does not exist ''')\n            return None\n\n        url = self.base_url + 'term/edit-relationship/{id}'.format(id=relationship_id)\n\n        output = self.post(\n            url = url,\n            data = data,\n        )\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(self):\n        self.secret_finder()\n        self.parse_access_token()\n        self.get_session_token()\n        self.parse_session_token()\n        self.get_route()\n        self.download_profile()\n        self.find_loci()\n        self.download_loci()", "response": "Run the appropriate methods in the correct order"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the supplied secret.txt file for the consumer key and secrets", "response": "def secret_finder(self):\n        \"\"\"\n        Parses the supplied secret.txt file for the consumer key and secrets\n        \"\"\"\n        secretlist = list()\n        if os.path.isfile(self.secret_file):\n            # Open the file, and put the contents into a list\n            with open(self.secret_file, 'r') as secret:\n                for line in secret:\n                    secretlist.append(line.rstrip())\n            # Extract the key and secret from the list\n            self.consumer_key = secretlist[0]\n            self.consumer_secret = secretlist[1]\n        else:\n            print('\"Cannot find the secret.txt file required for authorization. '\n                  'Please ensure that this file exists, and that the supplied consumer key is on the '\n                  'first line, and the consumer secret is on he second line. '\n                  'Contact keith.jolley@zoo.ox.ac.uk for an account, and the necessary keys')\n            quit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the access_token file and set the secret and token values appropriately", "response": "def parse_access_token(self):\n        \"\"\"\n        Extract the secret and token values from the access_token file\n        \"\"\"\n        access_file = os.path.join(self.file_path, 'access_token')\n        # Ensure that the access_token file exists\n        if os.path.isfile(access_file):\n            # Initialise a list to store the secret and token\n            access_list = list()\n            with open(access_file, 'r') as access_token:\n                for line in access_token:\n                    value, data = line.split('=')\n                    access_list.append(data.rstrip())\n            # Set the variables appropriately\n            self.access_secret = access_list[0]\n            self.access_token = access_list[1]\n        else:\n            print('Missing access_token')\n            self.get_request_token()\n            self.get_access_token()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_request_token(self):\n        print('Obtaining request token')\n        try:\n            os.remove(os.path.join(self.file_path, 'request_token'))\n        except FileNotFoundError:\n            pass\n        # Create a new session\n        session = OAuth1Session(consumer_key=self.consumer_key,\n                                consumer_secret=self.consumer_secret)\n        # Use the test URL in the GET request\n        r = session.request(method='GET',\n                            url=self.request_token_url,\n                            params={'oauth_callback': 'oob'})\n        # If the status code is '200' (OK), proceed\n        if r.status_code == 200:\n            # Save the JSON-decoded token secret and token\n            self.request_token = r.json()['oauth_token']\n            self.request_secret = r.json()['oauth_token_secret']\n            # Write the token and secret to file\n            self.write_token('request_token', self.request_token, self.request_secret)", "response": "Obtain a request token from the file and save it to self. request_token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the accession token from the server and save it to the file.", "response": "def get_session_token(self):\n        \"\"\"\n        Use the accession token to request a new session token\n        \"\"\"\n        # self.logging.info('Getting session token')\n        # Rather than testing any previous session tokens to see if they are still valid, simply delete old tokens in\n        # preparation of the creation of new ones\n        try:\n            os.remove(os.path.join(self.file_path, 'session_token'))\n        except FileNotFoundError:\n            pass\n        # Create a new session\n        session_request = OAuth1Session(self.consumer_key,\n                                        self.consumer_secret,\n                                        access_token=self.access_token,\n                                        access_token_secret=self.access_secret)\n        # Perform a GET request with the appropriate keys and tokens\n        r = session_request.get(self.session_token_url)\n        # If the status code is '200' (OK), proceed\n        if r.status_code == 200:\n            # Save the JSON-decoded token secret and token\n            self.session_token = r.json()['oauth_token']\n            self.session_secret = r.json()['oauth_token_secret']\n            # Write the token and secret to file\n            self.write_token('session_token', self.session_token, self.session_secret)\n        # Any other status than 200 is considered a failure\n        else:\n            print('Failed:')\n            print(r.json()['message'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_token(self, token_type, token, secret):\n        # Open the file, and write the token and secret strings appropriately\n        with open(os.path.join(self.file_path, token_type), 'w') as token_file:\n            token_file.write('secret=' + secret + '\\n')\n            token_file.write('token=' + token + '\\n')", "response": "Write a token to file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_session_token(self):\n        session_file = os.path.join(self.file_path, 'session_token')\n        # Only try to extract the strings if the file exists\n        if os.path.isfile(session_file):\n            # Create a list to store the data from the file\n            session_list = list()\n            with open(session_file, 'r') as session_token:\n                for line in session_token:\n                    # Split the description e.g. secret= from the line\n                    value, data = line.split('=')\n                    # Add each string to the list\n                    session_list.append(data.rstrip())\n            # Extract the appropriate variable from the list\n            self.session_secret = session_list[0]\n            self.session_token = session_list[1]", "response": "Extract the session secret and token strings from the session token file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_route(self):\n        # Create a new session\n        session = OAuth1Session(self.consumer_key,\n                                self.consumer_secret,\n                                access_token=self.session_token,\n                                access_token_secret=self.session_secret)\n        # Use the test URL in the GET request\n        r = session.get(self.test_rest_url)\n        if r.status_code == 200 or r.status_code == 201:\n            if re.search('json', r.headers['content-type'], flags=0):\n                decoded = r.json()\n            else:\n                decoded = r.text\n            # Extract the URLs from the returned data\n            self.loci = decoded['loci']\n            self.profile = decoded['schemes']", "response": "Returns the URL for the loci and schemes of the loci"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_profile(self):\n        # Set the name of the profile file\n        profile_file = os.path.join(self.output_path, 'profile.txt')\n        size = 0\n        # Ensure that the file exists, and that it is not too small; likely indicating a failed download\n        try:\n            stats = os.stat(profile_file)\n            size = stats.st_size\n        except FileNotFoundError:\n            pass\n        # Only download the profile if the file doesn't exist, or is likely truncated\n        if not os.path.isfile(profile_file) or size <= 100:\n            # Create a new session\n            session = OAuth1Session(self.consumer_key,\n                                    self.consumer_secret,\n                                    access_token=self.session_token,\n                                    access_token_secret=self.session_secret)\n            # The profile file is called profiles_csv on the server. Updated the URL appropriately\n            r = session.get(self.profile + '/1/profiles_csv')\n            # On a successful GET request, parse the returned data appropriately\n            if r.status_code == 200 or r.status_code == 201:\n                if re.search('json', r.headers['content-type'], flags=0):\n                    decoded = r.json()\n                else:\n                    decoded = r.text\n                # Write the profile file to disk\n                with open(profile_file, 'w') as profile:\n                    profile.write(decoded)", "response": "Download the profile from the database and store it in the output_path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the URLs for all allele files in the loci file", "response": "def find_loci(self):\n        \"\"\"\n        Finds the URLs for all allele files\n        \"\"\"\n        session = OAuth1Session(self.consumer_key,\n                                self.consumer_secret,\n                                access_token=self.session_token,\n                                access_token_secret=self.session_secret)\n        # Use the URL for all loci determined above\n        r = session.get(self.loci)\n        if r.status_code == 200 or r.status_code == 201:\n            if re.search('json', r.headers['content-type'], flags=0):\n                decoded = r.json()\n            else:\n                decoded = r.text\n            # Extract all the URLs in the decoded dictionary under the key 'loci'\n            for locus in decoded['loci']:\n                # Add each URL to the list\n                self.loci_url.append(locus)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_threads(self, url):\n        # Set the name of the allele file - split the gene name from the URL\n        output_file = os.path.join(self.output_path, '{}.tfa'.format(os.path.split(url)[-1]))\n        # Check to see whether the file already exists, and if it is unusually small\n        size = 0\n        try:\n            stats = os.stat(output_file)\n            size = stats.st_size\n        except FileNotFoundError:\n            pass\n        # If the file doesn't exist, or is truncated, proceed with the download\n        if not os.path.isfile(output_file) or size <= 100:\n            # Create a new session\n            session = OAuth1Session(self.consumer_key,\n                                    self.consumer_secret,\n                                    access_token=self.session_token,\n                                    access_token_secret=self.session_secret)\n            # The allele file on the server is called alleles_fasta. Update the URL appropriately\n            r = session.get(url + '/alleles_fasta')\n            if r.status_code == 200 or r.status_code == 201:\n                if re.search('json', r.headers['content-type'], flags=0):\n                    decoded = r.json()\n                else:\n                    decoded = r.text\n                # Write the allele to disk\n                with open(output_file, 'w') as allele:\n                    allele.write(decoded)", "response": "Download the threads from the server and store the results in the output_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves the object to the database.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        **uid**: :code:`{office.uid}_{cycle.uid}_race`\n        \"\"\"\n        self.uid = '{}_{}_race'.format(\n            self.office.uid,\n            self.cycle.uid\n        )\n\n        name_label = '{0} {1}'.format(\n            self.cycle.name,\n            self.office.label\n        )\n\n        if self.special:\n            self.uid = '{}:special'.format(\n                self.uid\n            )\n            name_label = '{} Special'.format(\n                name_label\n            )\n\n        self.label = name_label\n        self.name = name_label\n        if not self.slug:\n            self.slug = uuslug(\n                name_label,\n                instance=self,\n                max_length=100,\n                separator='-',\n                start_no=2\n            )\n\n        super(Race, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize a object to string", "response": "def dumps(obj, *args, **kwargs):\n    \"\"\"Serialize a object to string\n\n    Basic Usage:\n\n    >>> import simplekit.objson\n    >>> obj = {'name':'wendy'}\n    >>> print simplekit.objson.dumps(obj)\n\n\n    :param obj: a object which need to dump\n    :param args: Optional arguments that :func:`json.dumps` takes.\n    :param kwargs: Keys arguments that :py:func:`json.dumps` takes.\n    :return: string\n    \"\"\"\n    kwargs['default'] = object2dict\n\n    return json.dumps(obj, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(obj, fp, *args, **kwargs):\n    kwargs['default'] = object2dict\n\n    json.dump(obj, fp, *args, **kwargs)", "response": "Serialize a object to a file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_delay(remainingDrops):\n    ''' Calculate the idle delay\n        Minimum play time for cards to drop is ~20min again. Except for accounts\n        that requested a refund?\n\n        Re-check every 15 mintes if there are more than 1 card drops remaining.\n        If only one drop remains, check every 5 minutes\n    '''\n    global sameDelay, lastDelay\n\n    # Reset lastDelay for new appids\n    if remainingDrops > 1:\n        lastDelay = 5\n        sameDelay = 0\n\n    if remainingDrops > 2:\n        return 15 * 60 # Check every 15 minutes\n    elif remainingDrops == 2:\n        return 10 * 60 # Check every 10 minutes\n    else:\n        # decrease delay by one minute every two calls\n        if lastDelay > 1:\n            if sameDelay == 2:\n                sameDelay = 0\n                lastDelay -= 1\n            sameDelay += 1\n        return lastDelay * 60", "response": "Calculates the idle delay for cards to drop."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_section_data(self, name):\n        logging.debug(_('Obtaining ELF section: %s'), name)\n        section = self.binary.get_section_by_name(name)\n        if section:\n            return section.data()\n        else:\n            logging.error(_('Section no found: %s'), name)\n            return b''", "response": "Get the data of the section."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _deep_json_to_string(value, depth):\n    if is_data(value):\n        if depth == 0:\n            return strings.limit(value2json(value), LOG_STRING_LENGTH)\n\n        return {k: _deep_json_to_string(v, depth - 1) for k, v in value.items()}\n    elif is_sequence(value):\n        return strings.limit(value2json(value), LOG_STRING_LENGTH)\n    elif isinstance(value, number_types):\n        return value\n    elif is_text(value):\n        return strings.limit(value, LOG_STRING_LENGTH)\n    elif is_binary(value):\n        return strings.limit(bytes2base64(value), LOG_STRING_LENGTH)\n    elif isinstance(value, (date, datetime)):\n        return datetime2unix(value)\n    else:\n        return strings.limit(value2json(value), LOG_STRING_LENGTH)", "response": "Convert a deep JSON object to a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse bcl2fastq to create. fastq files from a MiSeqRun", "response": "def createfastq(self):\n        \"\"\"Uses bcl2fastq to create .fastq files from a MiSeqRun\"\"\"\n        # Initialise samplecount\n        samplecount = 0\n        # If the fastq destination folder is not provided, make the default value of :path/:miseqfoldername\n        self.fastqdestination = self.fastqdestination if self.fastqdestination else self.path + self.miseqfoldername\n        # Make the path\n        make_path(self.fastqdestination)\n        # Initialise variables for storing index information\n        index = ''\n        indexlength = int()\n        # bcl2fastq requires an older version of the sample sheet, this recreates the required version\n        # Create the new sample sheet\n        with open('{}/SampleSheet_modified.csv'.format(self.fastqdestination), \"w\") as modifiedsamplesheet:\n            # Write the required headings to the file\n            modifiedsamplesheet.write(\n                \"FCID,Lane,SampleID,SampleRef,Index,Description,Control,Recipe,Operator,SampleProject\\n\")\n            for strain in self.samples:\n                # Create a combined index of index1-index2\n                try:\n                    strain.run.modifiedindex = '{}-{}'.format(strain.run.index, strain.run.index2)\n                    indexlength = 16\n                    index = 'I8,I8'\n                except KeyError:\n                    strain.run.modifiedindex = strain.run.index\n                    indexlength = 6\n                    index = 'I6'\n                # The list of items to print to each line of the modified sample sheet\n                printlist = [self.flowcell, '1', strain.name, str(strain.run.SampleNumber), strain.run.modifiedindex,\n                             strain.run.Description, 'N', 'NA',\n                             strain.run.InvestigatorName, self.projectname]\n                modifiedsamplesheet.write('{}\\n'.format(\",\".join(printlist)))\n                samplecount += 1\n        # Set :forward/reverse length to :header.forward/reverse length if the argument is not provided, or it's 'full',\n        # otherwise  use the supplied argument\n        self.forwardlength = self.metadata.header.forwardlength if self.forwardlength.lower()\\\n            == 'full' else self.forwardlength\n        # Set :reverselength to :header.reverselength\n        self.reverselength = self.metadata.header.reverselength if self.reverselength.lower() \\\n            == 'full' else self.reverselength\n        # As the number of cycles required is the number of forward reads + the index(8) + the second index(8)\n        # Also set the basemask variable as required\n        if self.reverselength != '0':\n            self.readsneeded = int(self.forwardlength) + int(self.reverselength) + indexlength\n            basemask = \"Y{}n*,{},Y{}n*\".format(self.forwardlength, index, self.reverselength)\n            nohup = \"nohup make -j 16 > nohup.out\"\n        else:\n            #  + 1\n            self.readsneeded = int(self.forwardlength) + indexlength\n            basemask = \"Y{}n*,{},n*\".format(self.forwardlength, index)\n            nohup = \"nohup make -j 16 r1 > nohup.out\"\n        # Handle plurality appropriately\n        samples = 'samples' if samplecount > 1 else 'sample'\n        number = 'are' if samplecount > 1 else 'is'\n        printtime('There {} {} {} in this run. '\n                  'Running fastq creating module with the following parameters:\\n'\n                  'MiSeqPath: {},\\n'\n                  'MiSeqFolder: {},\\n'\n                  'Fastq destination: {},\\n'\n                  'SampleSheet: {}'\n                  .format(number, samplecount, samples, self.miseqpath, self.miseqfolder,\n                          self.fastqdestination, '{}/SampleSheet_modified.csv'.format(self.fastqdestination)),\n                  self.start)\n        # Count the number of completed cycles in the run of interest\n        cycles = glob('{}Data/Intensities/BaseCalls/L001/C*'.format(self.miseqfolder))\n        while len(cycles) < self.readsneeded:\n            printtime('Currently at {} cycles. Waiting until the MiSeq reaches cycle {}'.format(len(cycles),\n                      self.readsneeded), self.start)\n            sleep(1800)\n            cycles = glob('{}Data/Intensities/BaseCalls/L001/C*'.format(self.miseqfolder))\n        # configureBClToFastq requires :self.miseqfolder//Data/Intensities/BaseCalls/config.xml in order to work\n        # When you download runs from BaseSpace, this file is not provided. There is an empty config.xml file that\n        # can be populated with run-specific values and moved to the appropriate folder\n        if not os.path.isfile('{}Data/Intensities/BaseCalls/config.xml'.format(self.miseqfolder)):\n            self.configfilepopulator()\n        # Define the bcl2fastq system call\n        bclcall = \"configureBclToFastq.pl --input-dir {}Data/Intensities/BaseCalls \" \\\n                  \"--output-dir {} --force --sample-sheet {}/SampleSheet_modified.csv \" \\\n                  \"--mismatches 1 --no-eamss --fastq-cluster-count 0 --compression none --use-bases-mask {}\"\\\n            .format(self.miseqfolder, self.fastqdestination, self.fastqdestination, basemask)\n        # Define the nohup system call\n        nohupcall = \"cd {} && {}\".format(self.fastqdestination, nohup)\n        # fnull = open(os.devnull, 'wb')\n        if not os.path.isdir(\"{}/Project_{}\".format(self.fastqdestination, self.projectname)):\n            # Call configureBclToFastq.pl\n            printtime('Running bcl2fastq', self.start)\n            # Run the commands\n            threadlock = threading.Lock()\n            outstr = ''\n            outerr = ''\n            out, err = run_subprocess(bclcall)\n            outstr += out\n            outerr += out\n            out, err = run_subprocess(nohupcall)\n            outstr += out\n            outerr += out\n            # call(bclcall, shell=True, stdout=fnull, stderr=fnull)\n            # call(nohupcall, shell=True, stdout=fnull, stderr=fnull)\n            threadlock.acquire()\n            write_to_logfile(bclcall, bclcall, self.logfile)\n            write_to_logfile(nohupcall, nohupcall, self.logfile)\n            write_to_logfile(outstr, outerr, self.logfile)\n            threadlock.release()\n        # Populate the metadata\n        for sample in self.metadata.samples:\n            sample.commands = GenObject()\n            sample.commands.nohup = nohupcall\n            sample.commands.bcl = bclcall\n            sample.run.forwardlength = self.forwardlength\n            sample.run.reverselength = self.reverselength\n        # Copy the fastq files to a central folder so they can be processed\n        self.fastqmover()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate an unpopulated config. xml file with run - specific values and creates the file in the appropriate location", "response": "def configfilepopulator(self):\n        \"\"\"Populates an unpopulated config.xml file with run-specific values and creates\n        the file in the appropriate location\"\"\"\n        # Set the number of cycles for each read and index using the number of reads specified in the sample sheet\n        self.forwardlength = self.metadata.header.forwardlength\n        self.reverselength = self.metadata.header.reverselength\n        # Create a list of lists containing [cycle start, cycle end, and :runid] for each of forward reads, index 1\n        # index 2, and reverse reads\n        cycles = [[1, self.forwardlength, self.runid],\n                  [self.forwardlength + 1, self.forwardlength + 8, self.runid],\n                  [self.forwardlength + 9, self.forwardlength + 16, self.runid],\n                  [self.forwardlength + 17, self.forwardlength + 16 + self.reverselength, self.runid]]\n        # A dictionary of parameters (keys) and the values to use when repopulating the config file\n        parameters = {'RunFolder': self.runid, 'RunFolderDate': self.metadata.date.replace(\"-\", \"\"),\n                      'RunFolderId': self.metadata.runnumber, 'RunFlowcellId': self.metadata.flowcell}\n        # Load the xml file using element tree\n        config = ElementTree.parse(\"{}/config.xml\".format(self.homepath))\n        # Get the root of the tree\n        configroot = config.getroot()\n        # The run node is the only child node of the root\n        for run in configroot:\n            # Iterate through the child nodes. There are three nodes sections that must be populated\n            for child in run:\n                # Find the cycles tag\n                if child.tag == 'Cycles':\n                    # Set the attributes with a dictionary containing the total reads\n                    child.attrib = {'Last': '{}'.format(self.forwardlength + 16 + self.reverselength),\n                                    'Number': '{}'.format(self.totalreads), 'First': '1'}\n                elif child.tag == 'RunParameters':\n                    # Name the child as runparameter for easier coding\n                    runparameters = child\n                    for runparameter in runparameters:\n                        # This replaces data in both 'ImagingReads' and 'Reads' nodes\n                        if 'Reads' in runparameter.tag:\n                            # Enumerate through the run parameters\n                            for indexcount, reads in enumerate(runparameter):\n                                # The values for the index are 1, 2, 3, 4. Subtract one to get the index of the first\n                                # list in cycles\n                                index = int(runparameter.attrib['Index']) - 1\n                                # Set the text value as the appropriate value from cycles\n                                reads.text = str(cycles[index][indexcount])\n                        # Populate the instrument value\n                        if runparameter.tag == 'Instrument':\n                            runparameter.text = self.instrument\n                        # Iterate through the parameters in the parameter dictionary\n                        for parameter in parameters:\n                            # If the key is encountered\n                            if runparameter.tag == parameter:\n                                # Replace the text with the value\n                                runparameter.text = parameters[parameter]\n                        if 'Barcode' in runparameter.tag:\n                            for cycle, barcode in enumerate(runparameter):\n                                # Add the barcode cycles. These are the number of forward reads (+ 1 as the barcode\n                                # starts 1 cycle after the first run) plus the current iterator\n                                barcode.text = str(self.forwardlength + 1 + cycle)\n        # Write the modified config file to the desired location\n        config.write('{}Data/Intensities/BaseCalls/config.xml'.format(self.miseqfolder))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlink .fastq files created above to :self.path/:sample.name/", "response": "def fastqmover(self):\n        \"\"\"Links .fastq files created above to :self.path/:sample.name/\"\"\"\n        # Create the project path variable\n        self.projectpath = self.fastqdestination + \"/Project_\" + self.projectname\n        # Iterate through all the sample names\n        for sample in self.metadata.samples:\n            # Glob all the .gz files in the subfolders - projectpath/Sample_:sample.name/*.gz\n            for fastq in sorted(glob('{}/Sample_{}/*.gz'.format(self.projectpath, sample.name))):\n                # Try/except loop link .gz files to self.path\n                try:\n                    # Move fastq file to the path, but renames them first using the sample number.\n                    move(\n                        fastq, '{}{}'.format(self.path, os.path.basename(\n                            sub('\\w{8}-\\w{8}', 'S{}'.format(\n                                sample.run.SampleNumber), fastq))))\n                # Except os errors\n                except OSError as exception:\n                    # If there is an exception other than the file exists, raise it\n                    if exception.errno != errno.EEXIST:\n                        raise\n            # Repopulate .strainfastqfiles with the freshly-linked files\n            fastqfiles = glob('{}/{}*.fastq*'.format(self.fastqdestination, sample.name))\n            fastqfiles = [fastq for fastq in fastqfiles if 'trimmed' not in fastq]\n            # Populate the metadata object with the name/path of the fastq files\n            sample.general.fastqfiles = fastqfiles\n            # Save the outputdir to the metadata object\n            sample.run.outputdirectory = self.fastqdestination\n        # Copy the sample sheet and the run info files to the path\n        copyfile(self.assertions.samplesheet, os.path.join(self.path, 'SampleSheet.csv'))\n        copyfile(os.path.join(self.miseqfolder, 'RunInfo.xml'), os.path.join(self.path, 'RunInfo.xml'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubscribe to the parameter of the current resource.", "response": "def subscribe_param():\n    \"\"\"Print value of parameter\"\"\"\n    def print_data(data):\n        for parameter in data.parameters:\n            print(parameter)\n\n    processor.create_parameter_subscription('/YSS/SIMULATOR/BatteryVoltage2',\n                                            on_data=print_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the structure of the HolidayClass .", "response": "def _check_holiday_structure(self, times):\n        \"\"\" To check the structure of the HolidayClass\n\n        :param list times: years or months or days or number week\n        :rtype: None or Exception\n        :return: in the case of exception returns the exception\n        \"\"\"\n\n        if not isinstance(times, list):\n            raise TypeError(\"an list is required\")\n\n        for time in times:\n            if not isinstance(time, tuple):\n                raise TypeError(\"a tuple is required\")\n            if len(time) > 5:\n                raise TypeError(\"Target time takes at most 5 arguments\"\n                                \" ('%d' given)\" % len(time))\n            if len(time) < 5:\n                raise TypeError(\"Required argument '%s' (pos '%d')\"\n                                \" not found\" % (TIME_LABEL[len(time)], len(time)))\n\n            self._check_time_format(TIME_LABEL, time)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_time_format(self, labels, values):\n\n        for label, value in zip(labels, values):\n            if value == \"*\":\n                continue\n            if label == \"day_of_week\":\n                if isinstance(value, string_types):\n                    if value not in ORDER_WEEK:\n                        raise ParseError(\"'%s' is not day of the week. \"\n                                         \"character is the only '%s'\" % (\n                                             value, ', '.join(ORDER_WEEK)))\n                elif not isinstance(value, int):\n                    raise TypeError(\"'%s' is not an int\" % value)\n\n            if label in [\"year\", \"month\", \"day\", \"num_of_week\"]:\n                if not isinstance(value, int):\n                    raise TypeError(\"'%s' is not an int\" % value)\n\n            if isinstance(value, int):\n                start, end = TIME_INFO[label]\n                if not start <= value <= end:\n                    raise PeriodRangeError(\"'%d' is outside the scope of the period \"\n                                           \"'%s' range: '%d' to '%d'\" % (\n                                               value, label, start, end))", "response": "Check the format of the timescale."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_holiday(self, date):\n        time = [\n            date.year,\n            date.month,\n            date.day,\n            date.isoweekday(),\n            _extract_week_number(date)\n        ]\n\n        target = []\n        for key, data in list(zip(TIME_LABEL, time)):\n            d = getattr(self, key)\n            asterisk = d.get(\"*\", set())\n            s = asterisk.union(d.get(data, set()))\n            target.append(list(s))\n\n        for result in map(set, product(*target)):\n            if len(result) == 1:\n                return True\n        return False", "response": "Returns True if the object is holiday False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading single image from Landsat on Google Storage.", "response": "def single_download(self,username,password,download ,name,ZIP_DIR):\n\n        \"\"\" Download single image from Landsat on Google Storage\n        Arguments:\n            row - string in this format xxx, e.g. 003\n            path - string in this format xxx, e.g. 003\n            name - zip file name without .tar.bz e.g. LT81360082013127LGN01\n            sat_type - e.g. L7, L8, ...\n        \"\"\"\n        try:\n            request = urllib2.Request(download)\n            base64string = base64.encodestring('%s:%s' % (username, password)).replace('\\n', '')\n            request.add_header(\"Authorization\", \"Basic %s\" % base64string)   \n            result = urllib2.urlopen(request)\n            data = result.read()        \n            result.close()\n            try:\n                f=open(ZIP_DIR+'/'+name+'.tgz', 'wb')\n                f.write(data)\n                f.close()\n                return True\n            except urllib2.HTTPError:  \n                return False\n        except urllib2.HTTPError:  \n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if download file exists in the local directory.", "response": "def checkifDownloadExist(self,username,password,download , name):\n        \"\"\" Download single image from Landsat on Google Storage\n        Arguments:\n            row - string in this format xxx, e.g. 003\n            path - string in this format xxx, e.g. 003\n            name - zip file name without .tar.bz e.g. LT81360082013127LGN01\n            sat_type - e.g. L7, L8, ...\n        \"\"\"\n        try:\n            request = urllib2.Request(download)\n            base64string = base64.encodestring('%s:%s' % (username, password)).replace('\\n', '')\n            request.add_header(\"Authorization\", \"Basic %s\" % base64string)   \n            result = urllib2.urlopen(request)\n            try:\n                f=open(self.zip_dir+'/'+name+'.tgz', 'wb')\n                f.close()\n                return True\n            except urllib2.HTTPError:  \n                return False\n        except urllib2.HTTPError:  \n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, python=None, system_site=False, always_copy=False):\n        command = 'virtualenv'\n        if python:\n\n            command = '{0} --python={1}'.format(command, python)\n\n        if system_site:\n\n            command = '{0} --system-site-packages'.format(command)\n\n        if always_copy:\n\n            command = '{0} --always-copy'.format(command)\n\n        command = '{0} {1}'.format(command, self.path)\n        self._execute(command)", "response": "Create a new virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef epcrparse(self):\n        logging.info('Parsing ePCR results')\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                if 'stx' in sample.general.datastore:\n                    # Initialise count - this allows for the population of vtyperresults with unique values\n                    uniquecount = 0\n                    # This populates vtyperresults with the verotoxin subtypes\n                    toxinlist = []\n                    if os.path.isfile(sample[self.analysistype].resultsfile):\n                        epcrresults = open(sample[self.analysistype].resultsfile, 'r')\n                        for result in epcrresults:\n                            # Only the lines without a # contain results\n                            if \"#\" not in result:\n                                uniquecount += 1\n                                # Split on \\t\n                                data = result.split('\\t')\n                                # The subtyping primer pair is the first entry on lines with results\n                                vttype = data[0].split('_')[0]\n                                # Push the name of the primer pair - stripped of anything after a _ to the dictionary\n                                if vttype not in toxinlist:\n                                    toxinlist.append(vttype)\n\n                    # Create a string of the entries in list1 joined with \";\"\n                    toxinstring = \";\".join(sorted(toxinlist))\n                    # Save the string to the metadata\n                    sample[self.analysistype].toxinprofile = toxinstring\n                else:\n                    setattr(sample, self.analysistype, GenObject())\n                    sample[self.analysistype].toxinprofile = 'NA'\n            else:\n                setattr(sample, self.analysistype, GenObject())\n                sample[self.analysistype].toxinprofile = 'NA'", "response": "Parse the ePCR text file outputs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_subprocess(command):\n    x = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = x.communicate()\n    out = out.decode('utf-8')\n    err = err.decode('utf-8')\n    if x.returncode != 0:\n        print('STDERR from called program: {}'.format(err))\n        print('STDOUT from called program: {}'.format(out))\n        raise subprocess.CalledProcessError(x.returncode, command)\n    return out, err", "response": "Runs a subprocess and returns stdout and stderr from the subprocess as a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating an rdflib graph with these curies", "response": "def populate(cls, graph):\n        \"\"\" populate an rdflib graph with these curies \"\"\"\n        [graph.bind(k, v) for k, v in cls._dict.items()]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping a flask route that checks that the user has authorized via OAuth or redirect the user to the originating endpoint with a delayed redirect back to the originating endpoint.", "response": "def authorized(route):\n    \"\"\"\n    Wrap a flask route. Ensure that the user has authorized via OAuth or\n    redirect the user to the authorization endpoint with a delayed redirect\n    back to the originating endpoint.\n    \"\"\"\n    @wraps(route)\n    def authorized_route(*args, **kwargs):\n        if 'mwoauth_access_token' in flask.session:\n            return route(*args, **kwargs)\n        else:\n            return flask.redirect(\n                flask.url_for('mwoauth.mwoauth_initiate') +\n                \"?next=\" + flask.request.endpoint)\n\n    return authorized_route"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mwapi_session(self, *args, **kwargs):\n        import mwapi\n        auth1 = self.generate_auth()\n        return mwapi.Session(*args, user_agent=self.user_agent, auth=auth1,\n                             **kwargs)", "response": "Create a new session that is authorized for the current user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a requests. Session that is authorized for the current user.", "response": "def requests_session(self, *args, **kwargs):\n        \"\"\"\n        Create :class:`requests.Session` that is authorized for the current\n        user.\n\n        `args` and `kwargs` are passed directly to :class:`requests.Session`\n        \"\"\"\n        import requests\n        auth1 = self.generate_auth()\n        return requests.Session(*args, auth=auth1, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(connection, silent, hgnc_file_path, hcop_file_path, low_memory):\n    database.update(connection=connection,\n                    silent=silent,\n                    hgnc_file_path=hgnc_file_path,\n                    hcop_file_path=hcop_file_path,\n                    low_memory=low_memory)", "response": "Update the database with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_prime(bits):\n    while True:\n        num = random.randrange(2 ** (bits - 1), 2 ** bits)\n        if Integer(str(num)).is_probably_prime():\n            return num", "response": "Creates a prime number of given size"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blum_blum_shub(seed, amount, prime0, prime1):\n    if amount == 0:\n        return []\n\n    assert (prime0 % 4 == 3 and\n            prime1 % 4 == 3)  # primes must be congruent 3 mod 4\n\n    mod = prime0 * prime1\n    rand = [seed]\n\n    for _ in range(amount - 1):\n        last_num = rand[len(rand) - 1]\n        next_num = (last_num * last_num) % mod\n        rand.append(next_num)\n\n    return rand", "response": "Creates pseudo - number generator for a given amount of number."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if prime in very naive way.", "response": "def is_naive_prime(self):\n        \"\"\"Checks if prime in very naive way\n        :return: True iff prime\n        \"\"\"\n        if self.to_int < 2:\n            return False\n        elif self.to_int % 2 == 0:\n            return False\n\n        return self.to_int in LOW_PRIMES"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_probably_prime(self):\n\n        if self.is_naive_prime():\n            return True\n\n        # check if multiple pf low primes\n        for prime in LOW_PRIMES:\n            if self.to_int % prime == 0:\n                return False\n\n        # if all else fails, call rabin to determine if to_int is prime\n        return self.test_miller_rabin(5)", "response": "Tests with miller - rabin\n        to determine if the to_int is prime."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the read length of the FASTQ files.", "response": "def readlength(self):\n        \"\"\"Calculates the read length of the fastq files. Short reads will not be able to be assembled properly with the\n        default parameters used for spades.\"\"\"\n        logging.info('Estimating read lengths of FASTQ files')\n        # Iterate through the samples\n        for sample in self.samples:\n            sample.run.Date = 'NA'\n            sample.run.InvestigatorName = 'NA'\n            sample.run.TotalClustersinRun = 'NA'\n            sample.run.NumberofClustersPF = 'NA'\n            sample.run.PercentOfClusters = 'NA'\n            sample.run.SampleProject = 'NA'\n            # Only perform this step if the forward and reverse lengths have not been loaded into the metadata\n            if not GenObject.isattr(sample.run, 'forwardlength') and not GenObject.isattr(sample.run, 'reverselength'):\n                # Initialise the .header attribute for each sample\n                sample.header = GenObject()\n                sample.commands = GenObject()\n                # Set /dev/null\n                devnull = open(os.devnull, 'wb')\n                # Only process the samples if the file type is a list\n                if type(sample.general.fastqfiles) is list:\n                    # Set the forward fastq to be the first entry in the list\n                    forwardfastq = sorted(sample.general.fastqfiles)[0]\n                    # If the files are gzipped, then zcat must be used instead of cat\n                    if '.gz' in forwardfastq:\n                        command = 'zcat'\n                    else:\n                        command = 'cat'\n                    # Read in the output of the (z)cat of the fastq file piped through head to read only the first 1000\n                    # lines. Will make a string of the first 1000 lines in the file\n                    forwardreads = subprocess.Popen(\"{} {} | head -n 1000\".format(command, forwardfastq),\n                                                    shell=True,\n                                                    stdout=subprocess.PIPE,\n                                                    stderr=devnull).communicate()[0].rstrip()\n                    # Set the length of the reads as follows: the highest value (max) of the length of the sequence. The\n                    # sequence was extracted from the rest of the lines in the fastq file. Example of first four lines:\n                    \"\"\"\n                    @M02466:126:000000000-AKF4P:1:1101:11875:1838 1:N:0:1\n                    TCATAACGCAGTGAAACGCTTTAACAAAAGCGGAGACACGCCACTATTTGTCAATATTTCGTATGATACATTTTTAGAAAATCAAGAAGAGTTGCACGA\n                    +\n                    AA,B89C,@++B,,,,C,:BFF9,C,,,,,6+++++:,C,8C+BE,EFF9FC,6E,EFGF@F<F@9F9E<FFGGGC8,,,,CC<,,,,,,6CE,C<C,,\n                    \"\"\"\n                    # The line with the sequence information occurs every four lines (1, 5, 9, etc). This can be\n                    # represented by linenumber % 4 == 1\n                    try:\n                        # Added due to weird 2to3 conversion issues, was coming\n                        forwardreads = forwardreads.decode('utf-8')\n                    except UnicodeDecodeError:\n                        sample.run.forwardlength = 0\n                    # up as a bytes object when we need it as a string.\n                    try:\n                        forwardlength = max([len(sequence) for iterator, sequence in enumerate(forwardreads.split('\\n'))\n                                             if iterator % 4 == 1])\n                        sample.run.forwardlength = forwardlength\n                    except (ValueError, TypeError):\n                        sample.run.forwardlength = 0\n                    # For paired end analyses, also calculate the length of the reverse reads\n                    if len(sample.general.fastqfiles) == 2:\n                        reversefastq = sorted(sample.general.fastqfiles)[1]\n                        reversereads = subprocess.Popen(\"{} {} | head -n 1000\".format(command, reversefastq),\n                                                        shell=True,\n                                                        stdout=subprocess.PIPE,\n                                                        stderr=devnull).communicate()[0].rstrip()\n                        try:\n                            reversereads = reversereads.decode('utf-8')\n                        except UnicodeDecodeError:\n                            sample.run.reverselength = 0\n                        try:\n                            sample.run.reverselength = max([len(sequence) for iterator, sequence in\n                                                            enumerate(reversereads.split('\\n')) if iterator % 4 == 1])\n                        except (ValueError, TypeError):\n                            sample.run.reverselength = 0\n                    # Populate metadata of single end reads with 'NA'\n                    else:\n                        sample.run.reverselength = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets up SQL table, if it not exists.", "response": "async def setup(self):\n        \"\"\"Setting up SQL table, if it not exists.\"\"\"\n        try:\n            engine = await self.db\n            created = False\n            if not await engine.has_table(self.table_name):\n                # create table\n                logger.info(\"Creating SQL table [{}]\".format(self.table_name))\n                items = self._get_table()\n                await engine.execute(CreateTable(items))\n                # create indeces\n                conn = await engine.connect()\n                await conn.execute(\n                    \"CREATE INDEX `lb_last_updated` ON `{}` (`source_id` DESC,`updated` DESC);\".format(self.table_name))\n                await conn.execute(\n                    \"CREATE INDEX `lb_post` ON `{}` (`target_id` DESC,`post_id` DESC);\".format(self.table_name))\n                await conn.close()\n                created = True\n            # create control table if not already created.\n            if self.control_table_name and not await engine.has_table(self.control_table_name):\n                # create table\n                logger.info(\"Creating SQL control table [{}]\".format(self.control_table_name))\n                items = self._get_control_table()\n                await engine.execute(CreateTable(items))\n                created = True\n            return created\n        except Exception as exc:\n            logger.error(\"[DB] Error when setting up SQL table: {}\".format(exc))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat a datetime object to a string of format HH : MM", "response": "def simple_time(value):\n    \"\"\"\n    Format a datetime or timedelta object to a string of format HH:MM\n    \"\"\"\n    if isinstance(value, timedelta):\n        return ':'.join(str(value).split(':')[:2])\n    return datetime_to_string(value, '%H:%M')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_dst(zonename):\n    tz = pytz.timezone(zonename)\n    now = pytz.utc.localize(datetime.utcnow())\n    return now.astimezone(tz).dst() != timedelta(0)", "response": "Find out if a timezone is Daylight Saving Time in this timezone"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a datetime from a string.", "response": "def load_datetime(value, dt_format):\n    \"\"\"\n    Create timezone-aware datetime object\n    \"\"\"\n    if dt_format.endswith('%z'):\n        dt_format = dt_format[:-2]\n        offset = value[-5:]\n        value = value[:-5]\n        if offset != offset.replace(':', ''):\n            # strip : from HHMM if needed (isoformat() adds it between HH and MM)\n            offset = '+' + offset.replace(':', '')\n            value = value[:-1]\n        return OffsetTime(offset).localize(datetime.strptime(value, dt_format))\n\n    return datetime.strptime(value, dt_format)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserialising all the items in source_list to json", "response": "def list_to_json(source_list):\n    \"\"\"\n    Serialise all the items in source_list to json\n    \"\"\"\n    result = []\n    for item in source_list:\n        result.append(item.to_json())\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_from_json(source_list_json):\n    result = []\n    if source_list_json == [] or source_list_json == None:\n        return result\n    for list_item in source_list_json:\n        item = json.loads(list_item)\n        try:\n            if item['class_name'] == 'Departure':\n                temp = Departure()\n            elif item['class_name'] == 'Disruption':\n                temp = Disruption()\n            elif item['class_name'] == 'Station':\n                temp = Station()\n            elif item['class_name'] == 'Trip':\n                temp = Trip()\n            elif item['class_name'] == 'TripRemark':\n                temp = TripRemark()\n            elif item['class_name'] == 'TripStop':\n                temp = TripStop()\n            elif item['class_name'] == 'TripSubpart':\n                temp = TripSubpart()\n            else:\n                print('Unrecognised Class ' + item['class_name'] + ', skipping')\n                continue\n            temp.from_json(list_item)\n            result.append(temp)\n        except KeyError:\n            print('Unrecognised item with no class_name, skipping')\n            continue\n    return result", "response": "Deserialises all the items in source_list from json"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_diff(list_a, list_b):\n    result = []\n    for item in list_b:\n        if not item in list_a:\n            result.append(item)\n    return result", "response": "Return the items from list_a that differ from list_b"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_same(list_a, list_b):\n    result = []\n    for item in list_b:\n        if item in list_a:\n            result.append(item)\n    return result", "response": "Return the items from list_a that are also on list_b."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_merge(list_a, list_b):\n    #return list(collections.OrderedDict.fromkeys(list_a + list_b))\n    #result = list(list_b)\n    result = []\n    for item in list_a:\n        if not item in result:\n            result.append(item)\n    for item in list_b:\n        if not item in result:\n            result.append(item)\n    return result", "response": "Merge two lists without duplicating items\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the delay of the train for this instance", "response": "def delay(self):\n        \"\"\"\n        Return the delay of the train for this instance\n        \"\"\"\n        delay = {'departure_time': None, 'departure_delay': None, 'requested_differs': None,\n                'remarks': self.trip_remarks, 'parts': []}\n        if self.departure_time_actual > self.departure_time_planned:\n            delay['departure_delay'] = self.departure_time_actual - self.departure_time_planned\n            delay['departure_time'] = self.departure_time_actual\n        if self.requested_time != self.departure_time_actual:\n            delay['requested_differs'] = self.departure_time_actual\n        for part in self.trip_parts:\n            if part.has_delay:\n                delay['parts'].append(part)\n        return delay"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_departure_delay(self, subpartcheck=True):\n        if self.status != 'VOLGENS-PLAN':\n            return True\n        if subpartcheck and self.trip_parts[0].has_delay:\n            return True\n        if self.requested_time != self.departure_time_actual:\n            return True\n        return False", "response": "Returns True if the user has requested a departure delay."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the actual train class for a given time.", "response": "def get_actual(cls, trip_list, time):\n        \"\"\"\n        Look for the train actually leaving at time\n        \"\"\"\n        for trip in trip_list:\n            if simple_time(trip.departure_time_planned) == time:\n                return trip\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the NS API xml result into Disruption objects", "response": "def parse_disruptions(self, xml):\n        \"\"\"\n        Parse the NS API xml result into Disruption objects\n        @param xml: raw XML result from the NS API\n        \"\"\"\n        obj = xmltodict.parse(xml)\n        disruptions = {}\n        disruptions['unplanned'] = []\n        disruptions['planned'] = []\n\n        if obj['Storingen']['Ongepland']:\n            raw_disruptions = obj['Storingen']['Ongepland']['Storing']\n            if isinstance(raw_disruptions, collections.OrderedDict):\n                raw_disruptions = [raw_disruptions]\n            for disruption in raw_disruptions:\n                newdis = Disruption(disruption)\n                #print(newdis.__dict__)\n                disruptions['unplanned'].append(newdis)\n\n        if obj['Storingen']['Gepland']:\n            raw_disruptions = obj['Storingen']['Gepland']['Storing']\n            if isinstance(raw_disruptions, collections.OrderedDict):\n                raw_disruptions = [raw_disruptions]\n            for disruption in raw_disruptions:\n                newdis = Disruption(disruption)\n                #print(newdis.__dict__)\n                disruptions['planned'].append(newdis)\n        return disruptions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_disruptions(self, station=None, actual=True, unplanned=True):\n        url = \"http://webservices.ns.nl/ns-api-storingen?station=${Stationsnaam}&actual=${true or false}&unplanned=${true or false}\"\n        url = \"http://webservices.ns.nl/ns-api-storingen?actual=true&unplanned=true\"\n\n        raw_disruptions = self._request('GET', url)\n        return self.parse_disruptions(raw_disruptions)", "response": "Fetch the current disruptions or even the planned ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the NS API xml result into Departure objects", "response": "def parse_departures(self, xml):\n        \"\"\"\n        Parse the NS API xml result into Departure objects\n        @param xml: raw XML result from the NS API\n        \"\"\"\n        obj = xmltodict.parse(xml)\n        departures = []\n\n        for departure in obj['ActueleVertrekTijden']['VertrekkendeTrein']:\n            newdep = Departure(departure)\n            departures.append(newdep)\n            #print('-- dep --')\n            #print(newdep.__dict__)\n            #print(newdep.to_json())\n            print(newdep.delay)\n\n        return departures"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_departures(self, station):\n        url = 'http://webservices.ns.nl/ns-api-avt?station=' + station\n\n        raw_departures = self._request('GET', url)\n        return self.parse_departures(raw_departures)", "response": "Fetch the current departure times from this station"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the NS API xml result into Trip objects", "response": "def parse_trips(self, xml, requested_time):\n        \"\"\"\n        Parse the NS API xml result into Trip objects\n        \"\"\"\n        obj = xmltodict.parse(xml)\n        trips = []\n\n        if 'error' in obj:\n            print('Error in trips: ' + obj['error']['message'])\n            return None\n\n        try:\n            for trip in obj['ReisMogelijkheden']['ReisMogelijkheid']:\n                newtrip = Trip(trip, requested_time)\n                trips.append(newtrip)\n        except TypeError:\n            # If no options are found, obj['ReisMogelijkheden'] is None\n            return None\n\n        return trips"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of trip possibilities for the specified time.", "response": "def get_trips(self, timestamp, start, via, destination, departure=True, prev_advices=1, next_advices=1):\n        \"\"\"\n        Fetch trip possibilities for these parameters\n        http://webservices.ns.nl/ns-api-treinplanner?<parameters>\n        fromStation\n        toStation\n        dateTime: 2012-02-21T15:50\n        departure: true for starting at timestamp, false for arriving at timestamp\n        previousAdvices\n        nextAdvices\n        \"\"\"\n        timezonestring = '+0100'\n        if is_dst('Europe/Amsterdam'):\n            timezonestring = '+0200'\n        url = 'http://webservices.ns.nl/ns-api-treinplanner?'\n        url = url + 'fromStation=' + start\n        url = url + '&toStation=' + destination\n        if via:\n            url = url + '&via=' + via\n        if len(timestamp) == 5:\n            # Format of HH:MM - api needs yyyy-mm-ddThh:mm\n            timestamp = time.strftime(\"%Y-%m-%d\") + 'T' + timestamp\n            #requested_time = datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M\")\n            # TODO: DST/normal time\n            requested_time = load_datetime(timestamp + timezonestring, \"%Y-%m-%dT%H:%M%z\")\n        else:\n            #requested_time = datetime.strptime(timestamp, \"%d-%m-%Y %H:%M\")\n            requested_time = load_datetime(timestamp + timezonestring, \"%d-%m-%Y %H:%M%z\")\n            timestamp = datetime.strptime(timestamp, \"%d-%m-%Y %H:%M\").strftime(\"%Y-%m-%dT%H:%M\")\n        url = url + '&previousAdvices=' + str(prev_advices)\n        url = url + '&nextAdvices=' + str(next_advices)\n        url = url + '&dateTime=' + timestamp\n        raw_trips = self._request('GET', url)\n        return self.parse_trips(raw_trips, requested_time)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching the list of stations from the ns. nl API.", "response": "def get_stations(self):\n        \"\"\"\n        Fetch the list of stations\n        \"\"\"\n        url = 'http://webservices.ns.nl/ns-api-stations-v2'\n        raw_stations = self._request('GET', url)\n        return self.parse_stations(raw_stations)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop(self):\n        self.log.debug('Stopping bot {}'.format(self._name))\n        self._stop = True\n        for t in self._threads:\n            t.join()\n\n        self.log.debug('Stopping bot {} finished. All threads joined.'.format(self._name))", "response": "Stops this bot.\n\n        Returns as soon as all running threads have finished processing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _listen_comments(self):\n        # Collect comments in a queue\n        comments_queue = Queue(maxsize=self._n_jobs * 4)\n\n        threads = []  # type: List[BotQueueWorker]\n\n        try:\n\n            # Create n_jobs CommentsThreads\n            for i in range(self._n_jobs):\n                t = BotQueueWorker(name='CommentThread-t-{}'.format(i),\n                                   jobs=comments_queue,\n                                   target=self._process_comment)\n                t.start()\n                threads.append(t)\n\n            # Iterate over all comments in the comment stream\n            for comment in self._reddit.subreddit('+'.join(self._subs)).stream.comments():\n\n                # Check for stopping\n                if self._stop:\n                    self._do_stop(comments_queue, threads)\n                    break\n\n                comments_queue.put(comment)\n\n            self.log.debug('Listen comments stopped')\n        except Exception as e:\n            self._do_stop(comments_queue, threads)\n            self.log.error('Exception while listening to comments:')\n            self.log.error(str(e))\n            self.log.error('Waiting for 10 minutes and trying again.')\n            time.sleep(10 * 60)\n\n            # Retry\n            self._listen_comments()", "response": "Start listening to comments using a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the comments stream thread.", "response": "def start(self):\n        \"\"\"\n        Starts this bot in a separate thread. Therefore, this call is non-blocking.\n\n        It will listen to all new comments created in the :attr:`~subreddits` list.\n        \"\"\"\n        super().start()\n        comments_thread = BotThread(name='{}-comments-stream-thread'.format(self._name),\n                                    target=self._listen_comments)\n        comments_thread.start()\n        self._threads.append(comments_thread)\n        self.log.info('Starting comments stream ...')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart listening to submissions using a separate thread.", "response": "def _listen_submissions(self):\n        \"\"\"Start listening to submissions, using a separate thread.\"\"\"\n        # Collect submissions in a queue\n        subs_queue = Queue(maxsize=self._n_jobs * 4)\n\n        threads = []  # type: List[BotQueueWorker]\n\n        try:\n            # Create n_jobs SubmissionThreads\n            for i in range(self._n_jobs):\n                t = BotQueueWorker(name='SubmissionThread-t-{}'.format(i),\n                                   jobs=subs_queue,\n                                   target=self._process_submission)\n                t.start()\n                self._threads.append(t)\n\n            # Iterate over all comments in the comment stream\n            for submission in self._reddit.subreddit('+'.join(self._subs)).stream.submissions():\n\n                # Check for stopping\n                if self._stop:\n                    self._do_stop(subs_queue, threads)\n                    break\n\n                subs_queue.put(submission)\n\n            self.log.debug('Listen submissions stopped')\n        except Exception as e:\n            self._do_stop(subs_queue, threads)\n            self.log.error('Exception while listening to submissions:')\n            self.log.error(str(e))\n            self.log.error('Waiting for 10 minutes and trying again.')\n            time.sleep(10 * 60)\n\n            # Retry:\n            self._listen_submissions()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        super().start()\n        submissions_thread = BotThread(name='{}-submissions-stream-thread'.format(self._name),\n                                       target=self._listen_submissions)\n        submissions_thread.start()\n        self._threads.append(submissions_thread)\n        self.log.info('Starting submissions stream ...')", "response": "Starts this bot in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting listening to messages using a separate thread.", "response": "def _listen_inbox_messages(self):\n        \"\"\"Start listening to messages, using a separate thread.\"\"\"\n        # Collect messages in a queue\n        inbox_queue = Queue(maxsize=self._n_jobs * 4)\n\n        threads = []  # type: List[BotQueueWorker]\n\n        try:\n            # Create n_jobs inbox threads\n            for i in range(self._n_jobs):\n                t = BotQueueWorker(name='InboxThread-t-{}'.format(i),\n                                   jobs=inbox_queue,\n                                   target=self._process_inbox_message)\n                t.start()\n                self._threads.append(t)\n\n            # Iterate over all messages in the messages stream\n            for message in self._reddit.inbox.stream():\n                # Check for stopping\n                if self._stop:\n                    self._do_stop(inbox_queue, threads)\n                    break\n\n                inbox_queue.put(message)\n\n            self.log.debug('Listen inbox stopped')\n        except Exception as e:\n            self._do_stop(inbox_queue, threads)\n            self.log.error('Exception while listening to inbox:')\n            self.log.error(str(e))\n            self.log.error('Waiting for 10 minutes and trying again.')\n            time.sleep(10 * 60)\n\n            # Retry:\n            self._listen_inbox_messages()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start(self):\n        super().start()\n        inbox_thread = BotThread(name='{}-inbox-stream-thread'.format(self._name),\n                                 target=self._listen_inbox_messages)\n        inbox_thread.start()\n        self._threads.append(inbox_thread)\n        self.log.info('Starting inbox stream ...')", "response": "Starts this bot in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_comment(self, comment: praw.models.Comment):\n        self._func_comment(comment, *self._func_comment_args)", "response": "Process a reddit comment. Calls self. _func_comment."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a reddit inbox message. Calls self. _func_message.", "response": "def _process_inbox_message(self, message: praw.models.Message):\n        \"\"\"\n        Process a reddit inbox message. Calls `func_message(message, *func_message_args)`.\n\n        :param message: Item to process\n        \"\"\"\n        self._func_message(message, *self._func_message_args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a reddit submission. Calls self. _func_submission.", "response": "def _process_submission(self, submission: praw.models.Submission):\n        \"\"\"\n        Process a reddit submission. Calls `func_comment(*func_comment_args)`.\n\n        :param submission: Comment to process\n        \"\"\"\n        self._func_submission(submission, *self._func_submission_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new struct class instance from this object.", "response": "def to_struct(self):\n        \"\"\"\n        Initialize properties of the appropriate struct class from this model class.\n        \"\"\"\n        structobj = self.struct_type()\n        for k in structobj.attributes():\n            self.log.info(\"Setting attribute %s to %r\" % (k, getattr(self, k)))\n            setattr(structobj, k, getattr(self, k))\n        return structobj"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves this group to a new parent.", "response": "def move(self, parent, index=None):\n        \"\"\"\n        Move this group to a new parent.\n        \n        :param parent: The new parent group; if None will be root group.\n        :type parent: :class:`keepassdb.model.Group`\n        :param index: The 0-based index within the parent (defaults to appending\n                      group to end of parent's children).\n        :type index: int\n        \"\"\"\n        return self.db.move_group(self, parent, index=index)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move(self, group, index=None):\n        return self.group.db.move_entry(self, group, index=index)", "response": "This method moves the entry to another group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_case(s, separator='-'):\n    s = s.strip()\n    no_spaces = re.sub(' ', '_', s)\n    add_underscores = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', no_spaces)\n    lowercase = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', add_underscores).lower()\n    return re.sub('[-_]', separator, lowercase)", "response": "Changes the case to snake case or kebab - case depending on the separator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_data(data,\n              filename=None,\n              file_type='json',\n              klazz=YapconfError,\n              open_kwargs=None,\n              dump_kwargs=None):\n    \"\"\"Dump data given to file or stdout in file_type.\n\n    Args:\n        data (dict): The dictionary to dump.\n        filename (str, optional): Defaults to None. The filename to write\n        the data to. If none is provided, it will be written to STDOUT.\n        file_type (str, optional): Defaults to 'json'. Can be any of\n        yapconf.FILE_TYPES\n        klazz (optional): Defaults to YapconfError a special error to throw\n        when something goes wrong.\n        open_kwargs (dict, optional): Keyword arguments to open.\n        dump_kwargs (dict, optional): Keyword arguments to dump.\n    \"\"\"\n\n    _check_file_type(file_type, klazz)\n\n    open_kwargs = open_kwargs or {'encoding': 'utf-8'}\n    dump_kwargs = dump_kwargs or {}\n\n    if filename:\n        with open(filename, 'w', **open_kwargs) as conf_file:\n            _dump(data, conf_file, file_type, **dump_kwargs)\n    else:\n        _dump(data, sys.stdout, file_type, **dump_kwargs)", "response": "Dump data given to file or stdout in file_type."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a file with the given file type.", "response": "def load_file(filename,\n              file_type='json',\n              klazz=YapconfError,\n              open_kwargs=None,\n              load_kwargs=None):\n    \"\"\"Load a file with the given file type.\n\n    Args:\n        filename (str): The filename to load.\n        file_type (str, optional): Defaults to 'json'. The file type for the\n        given filename. Supported types are ``yapconf.FILE_TYPES```\n        klazz (optional): The custom exception to raise if something goes\n        wrong.\n        open_kwargs (dict, optional): Keyword arguments for the open call.\n        load_kwargs (dict, optional): Keyword arguments for the load call.\n\n    Raises:\n        klazz: If no klazz was passed in, this will be the ``YapconfError``\n\n    Returns:\n        dict: The dictionary from the file.\n    \"\"\"\n\n    _check_file_type(file_type, klazz)\n\n    open_kwargs = open_kwargs or {'encoding': 'utf-8'}\n    load_kwargs = load_kwargs or {}\n\n    data = None\n    with open(filename, **open_kwargs) as conf_file:\n        if str(file_type).lower() == 'json':\n            data = json.load(conf_file, **load_kwargs)\n        elif str(file_type).lower() == 'yaml':\n            data = yaml.safe_load(conf_file.read())\n        else:\n            raise NotImplementedError('Someone forgot to implement how to '\n                                      'load a %s file_type.' % file_type)\n\n    if not isinstance(data, dict):\n        raise klazz('Successfully loaded %s, but the result was '\n                    'not a dictionary.' % filename)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flatten(dictionary, separator='.', prefix=''):\n    new_dict = {}\n    for key, value in dictionary.items():\n        new_key = prefix + separator + key if prefix else key\n        if isinstance(value, collections.MutableMapping):\n            new_dict.update(flatten(value, separator, new_key))\n\n        elif isinstance(value, list):\n            new_value = []\n            for item in value:\n                if isinstance(item, collections.MutableMapping):\n                    new_value.append(flatten(item, separator, new_key))\n                else:\n                    new_value.append(item)\n            new_dict[new_key] = new_value\n\n        else:\n            new_dict[new_key] = value\n\n    return new_dict", "response": "Flatten the dictionary keys are separated by separator\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadjust the virtual environment settings and optional move it.", "response": "def relocate(source, destination, move=False):\n    \"\"\"Adjust the virtual environment settings and optional move it.\n\n    Args:\n        source (str): Path to the existing virtual environment.\n        destination (str): Desired path of the virtual environment.\n        move (bool): Whether or not to actually move the files. Default False.\n    \"\"\"\n    venv = api.VirtualEnvironment(source)\n    if not move:\n\n        venv.relocate(destination)\n        return None\n\n    venv.move(destination)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Relocate a virtual environment.'\n    )\n    parser.add_argument(\n        '--source',\n        help='The existing virtual environment.',\n        required=True,\n    )\n    parser.add_argument(\n        '--destination',\n        help='The location for which to configure the virtual environment.',\n        required=True,\n    )\n    parser.add_argument(\n        '--move',\n        help='Move the virtual environment to the destination.',\n        default=False,\n        action='store_true',\n    )\n\n    args = parser.parse_args()\n    relocate(args.source, args.destination, args.move)", "response": "Relocate a virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef confirm(prompt='Really?', color='warning', yes_values=('y', 'yes'),\n            abort_on_unconfirmed=False, abort_options=None):\n    \"\"\"Prompt for confirmation.\n\n    Confirmation can be aborted by typing in a no value instead of one\n    of the yes values or with Ctrl-C.\n\n    Args:\n        prompt (str): Prompt to present user [\"Really?\"]\n        color (string|Color|bool) Color to print prompt string; can be\n            ``False`` or ``None`` to print without color [\"yellow\"]\n        yes_values (list[str]): Values user must type in to confirm\n            [(\"y\", \"yes\")]\n        abort_on_unconfirmed (bool|int|str): When user does *not*\n            confirm:\n\n            - If this is an integer, print \"Aborted\" to stdout if\n              it's 0 or to stderr if it's not 0 and then exit with\n              this code\n            - If this is a string, print it to stdout and exit with\n              code 0\n            - If this is ``True`` (or any other truthy value), print\n              \"Aborted\" to stdout and exit with code 0\n\n        abort_options (dict): Options to pass to :func:`abort` when not\n            confirmed (these options will override any options set via\n            ``abort_on_unconfirmed``)\n\n    \"\"\"\n    if isinstance(yes_values, str):\n        yes_values = (yes_values,)\n\n    prompt = '{prompt} [{yes_value}/N] '.format(prompt=prompt, yes_value=yes_values[0])\n\n    if color:\n        prompt = printer.colorize(prompt, color=color)\n\n    try:\n        answer = input(prompt)\n    except KeyboardInterrupt:\n        print()\n        confirmed = False\n    else:\n        answer = answer.strip().lower()\n        confirmed = answer in yes_values\n\n    # NOTE: The abort-on-unconfirmed logic is somewhat convoluted\n    #       because of the special case for return code 0.\n\n    do_abort_on_unconfirmed = not confirmed and (\n        # True, non-zero return code, non-empty string, or any other\n        # truthy value (in the manner of typical Python duck-typing)\n        bool(abort_on_unconfirmed) or\n\n        # Zero return code (special case)\n        (abort_on_unconfirmed == 0 and abort_on_unconfirmed is not False)\n    )\n\n    if do_abort_on_unconfirmed:\n        if abort_options is None:\n            abort_options = {}\n\n        if abort_on_unconfirmed is True:\n            abort_options.setdefault('return_code', 0)\n        elif isinstance(abort_on_unconfirmed, int):\n            abort_options.setdefault('return_code', abort_on_unconfirmed)\n        elif isinstance(abort_on_unconfirmed, str):\n            abort_options.setdefault('message', abort_on_unconfirmed)\n        else:\n            abort_options.setdefault('return_code', 0)\n\n        abort(**abort_options)\n\n    return confirmed", "response": "Prompt for confirmation.\n\n    Confirmation can be aborted by typing in a no value instead of one\n    of the yes values or with Ctrl-C.\n\n    Args:\n        prompt (str): Prompt to present user [\"Really?\"]\n        color (string|Color|bool) Color to print prompt string; can be\n            ``False`` or ``None`` to print without color [\"yellow\"]\n        yes_values (list[str]): Values user must type in to confirm\n            [(\"y\", \"yes\")]\n        abort_on_unconfirmed (bool|int|str): When user does *not*\n            confirm:\n\n            - If this is an integer, print \"Aborted\" to stdout if\n              it's 0 or to stderr if it's not 0 and then exit with\n              this code\n            - If this is a string, print it to stdout and exit with\n              code 0\n            - If this is ``True`` (or any other truthy value), print\n              \"Aborted\" to stdout and exit with code 0\n\n        abort_options (dict): Options to pass to :func:`abort` when not\n            confirmed (these options will override any options set via\n            ``abort_on_unconfirmed``)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mime_message(self):\n        message = MIMEText(\n            \"<html>\" +\n            self.get_email_header() +\n            get_email_content(self.content_file) +\n            self.get_email_footer() +\n            \"</html>\", \"html\"\n        )\n        message[\"subject\"] = self.email_subject\n        return message", "response": "Gets email MIME message"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions decorator for incrementing a statsd stat whenever a function is invoked. >>> from statsdecor.decorators import increment >>> @increment('my.metric') >>> def my_func(): >>> pass", "response": "def increment(name, tags=None):\n    \"\"\"Function decorator for incrementing a statsd stat whenever\n    a function is invoked.\n\n    >>> from statsdecor.decorators import increment\n    >>> @increment('my.metric')\n    >>> def my_func():\n    >>>     pass\n    \"\"\"\n    def wrap(f):\n        @wraps(f)\n        def decorator(*args, **kwargs):\n            stats = client()\n            ret = f(*args, **kwargs)\n            stats.incr(name, tags=tags)\n            return ret\n        return decorator\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decrement(name, tags=None):\n    def wrap(f):\n        @wraps(f)\n        def decorator(*args, **kwargs):\n            stats = client()\n            ret = f(*args, **kwargs)\n            stats.decr(name, tags=tags)\n            return ret\n        return decorator\n    return wrap", "response": "Decorator for decrementing a statsd stat whenever a function is invoked."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning decorator for tracking timing information on a function's invocation. >>> from statsdecor.decorators import timed >>> @timed('my.metric') >>> def my_func(): >>> pass", "response": "def timed(name, tags=None):\n    \"\"\"Function decorator for tracking timing information\n    on a function's invocation.\n\n    >>> from statsdecor.decorators import timed\n    >>> @timed('my.metric')\n    >>> def my_func():\n    >>>     pass\n    \"\"\"\n    def wrap(f):\n        @wraps(f)\n        def decorator(*args, **kwargs):\n            stats = client()\n            with stats.timer(name, tags=tags):\n                return f(*args, **kwargs)\n        return decorator\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the assets file is configured and if not raises an exception if it is not found.", "response": "def check_assets(self):\n        \"\"\"\n        Throws an exception if assets file is not configured or cannot be found.\n        :param assets: path to the assets file\n        \"\"\"\n        if not self.assets_file:\n            raise ImproperlyConfigured(\"You must specify the path to the assets.json file via WEBPACK_ASSETS_FILE\")\n        elif not os.path.exists(self.assets_file):\n            raise ImproperlyConfigured(\n                \"The file `{file}` was not found, make sure to run the webpack build before the collectstatic command\".format(\n                    file=self.assets_file))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmoves file to given directory", "response": "def move_file_to_directory(file_path, directory_path):\n        \"\"\"Moves file to given directory\n\n        :param file_path: path to file to move\n        :param directory_path: path to target directory where to move file\n        \"\"\"\n        file_name = os.path.basename(file_path)  # get name of file\n        if not os.path.exists(directory_path):\n            os.makedirs(directory_path)  # create directory if necessary\n        os.rename(file_path, os.path.join(directory_path,\n                                          file_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move_file_to_file(old_path, new_path):\n        try:\n            os.rename(old_path, new_path)\n        except:\n            old_file = os.path.basename(old_path)\n            target_directory, target_file = os.path.dirname(\n                os.path.abspath(new_path)), os.path.basename(new_path)\n            Document.move_file_to_directory(\n                old_path,\n                target_directory\n            )  # move old file to new directory, change name to new name\n            os.rename(os.path.join(target_directory, old_file),\n                      os.path.join(target_directory, target_file))", "response": "Moves file from old location to new location."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_data(self, data):\n        with open(self.path, \"w\") as writer:\n            writer.write(data)", "response": "Writes given data to given path file\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget path and name of file or folder of song", "response": "def get_path_name(self):\n        \"\"\"Gets path and name of song\n\n        :return: Name of path, name of file (or folder)\n        \"\"\"\n        path = fix_raw_path(os.path.dirname(os.path.abspath(self.path)))\n        name = os.path.basename(self.path)\n        return path, name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_path_name(self):\n        complete_path = os.path.dirname(os.path.abspath(self.path))\n        name = self.path.replace(complete_path + PATH_SEPARATOR, \"\")\n        if name.endswith(\"/\"):\n            name = name[: -1]\n\n        return complete_path, name", "response": "Gets path and name of file or folder"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the PlaceholderNode parameters. Return a tuple with the name and parameters.", "response": "def parse_placeholder(parser, token):\n    \"\"\"Parse the `PlaceholderNode` parameters.\n\n    Return a tuple with the name and parameters.\"\"\"\n    bits = token.split_contents()\n    count = len(bits)\n    error_string = '%r tag requires at least one argument' % bits[0]\n    if count <= 1:\n        raise TemplateSyntaxError(error_string)\n    name = bits[1]\n    remaining = bits[2:]\n    params = {}\n    simple_options = ['parsed', 'inherited', 'untranslated']\n    param_options = ['as', 'on', 'with']\n    all_options = simple_options + param_options\n    while remaining:\n        bit = remaining[0]\n        if bit not in all_options:\n            raise TemplateSyntaxError(\n                \"%r is not an correct option for a placeholder\" % bit)\n        if bit in param_options:\n            if len(remaining) < 2:\n                raise TemplateSyntaxError(\n                \"Placeholder option '%s' need a parameter\" % bit)\n            if bit == 'as':\n                params['as_varname'] = remaining[1]\n            if bit == 'with':\n                params['widget'] = remaining[1]\n            if bit == 'on':\n                params['page'] = remaining[1]\n            remaining = remaining[2:]\n        elif bit == 'parsed':\n            params['parsed'] = True\n            remaining = remaining[1:]\n        elif bit == 'inherited':\n            params['inherited'] = True\n            remaining = remaining[1:]\n        elif bit == 'untranslated':\n            params['untranslated'] = True\n            remaining = remaining[1:]\n    return name, params"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive the name of a placeholder return a Widget subclass like Textarea or TextInput.", "response": "def get_widget(self, page, language, fallback=Textarea):\n        \"\"\"Given the name of a placeholder return a `Widget` subclass\n        like Textarea or TextInput.\"\"\"\n        import sys\n        PY3 = sys.version > '3'\n        is_string = type(self.widget) == type(str())\n        if not PY3:\n            is_string = type(self.widget) == type(str()) or type(self.widget) == type(unicode())\n        if is_string:\n            widget = get_widget(self.widget)\n        else:\n            widget = self.widget\n        try:\n            return widget(page=page, language=language)\n        except:\n            pass\n        return widget()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget eventual extra data for this placeholder from the admin form. This method is called when the Page is saved in the admin form and passed to the Placeholder save method.", "response": "def get_extra_data(self, data):\n        \"\"\"Get eventual extra data for this placeholder from the\n        admin form. This method is called when the Page is\n        saved in the admin and passed to the placeholder save\n        method.\"\"\"\n        result = {}\n        for key in data.keys():\n            if key.startswith(self.name + '-'):\n                new_key = key.replace(self.name + '-', '')\n                result[new_key] = data[key]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, page, language, data, change, extra_data=None):\n        # if this placeholder is untranslated, we save everything\n        # in the default language\n        if self.untranslated:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n\n        # the page is being changed\n        if change:\n            # we need create a new content if revision is enabled\n            if(settings.PAGE_CONTENT_REVISION and self.name\n                not in settings.PAGE_CONTENT_REVISION_EXCLUDE_LIST):\n                Content.objects.create_content_if_changed(\n                    page,\n                    language,\n                    self.name,\n                    data\n                )\n            else:\n                Content.objects.set_or_create_content(\n                    page,\n                    language,\n                    self.name,\n                    data\n                )\n        # the page is being added\n        else:\n            Content.objects.set_or_create_content(\n                page,\n                language,\n                self.name,\n                data\n            )", "response": "Actually save the placeholder data into the Content object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noutput the content of the PlaceholdeNode in the template.", "response": "def render(self, context):\n        \"\"\"Output the content of the `PlaceholdeNode` in the template.\"\"\"\n\n        content = mark_safe(self.get_content_from_context(context))\n        if not content:\n            return ''\n        if self.parsed:\n            try:\n                t = template.Template(content, name=self.name)\n                content = mark_safe(t.render(context))\n            except TemplateSyntaxError as error:\n                if global_settings.DEBUG:\n                    content = PLACEHOLDER_ERROR % {\n                        'name': self.name,\n                        'error': error,\n                    }\n                else:\n                    content = ''\n        if self.as_varname is None:\n            return content\n        context[self.as_varname] = content\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare the action function and returns a task.", "response": "def _prepare(self, kwargs=None):\n        \"\"\"\n        Updates the function arguments and creates a :class:`asyncio.Task`\n        from the Action.\n\n        *kwargs* is an optional dictionnary of additional arguments to pass to\n        the Action function.\n\n        .. warning::\n            *kwargs* will overwrite existing keys in *self.args*.\n\n        .. note::\n            If the Action func is blocking (not a coroutine function), it will\n            be executed in an `Executor`_.\n\n        .. _Executor: https://docs.python.org/3/library/asyncio-eventloop.html#executor\n        \"\"\"\n        if kwargs is not None:\n            # Note: This will overwrite existing keys in self.args.\n            #       This is the wanted behavior.\n            self.args.update(kwargs)\n\n        if asyncio.iscoroutinefunction(self.func):\n            task = asyncio.ensure_future(self.func(**self.args))\n        else:\n            # FIXME: is that clean enough ?\n            task = asyncio.get_event_loop() \\\n                   .run_in_executor(None,\n                                    functools.partial(self.func,\n                                                      **self.args))\n\n        return task"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps the action in a Task and schedules its execution.", "response": "async def run(self, kwargs=None):\n        \"\"\"\n        Wraps the action in a :class:`asyncio.Task` and schedules its\n        execution.\n\n        *kwargs* is an (optional) dictionnary of additional arguments to pass\n        to the Action function.\n        \"\"\"\n        task = self._prepare(kwargs)\n\n        try:\n            await task\n        except Exception as e:\n            # FIXME: write a better Exception handler.\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_string(cls, action_str):\n        args = {}\n\n        try:\n            mod_obj = ast.parse(action_str)\n        except (SyntaxError, ValueError) as e:\n            raise e\n        else:\n            call_obj = mod_obj.body[0].value\n\n            if isinstance(call_obj, ast.Attribute):\n                # Seems like we have a simple function name\n                # (for example `module.function`)\n                module = call_obj.value.id\n                func = call_obj.attr\n\n            elif isinstance(call_obj, ast.Call):\n                # Seems like we have a function call, maybe with\n                # a few parameters.\n                # Note that we only support `module.function()` format.\n                # You can't use `function()`.\n                try:\n                    module = call_obj.func.value.id\n                    func = call_obj.func.attr\n                except AttributeError:\n                    raise UnsupportedActionError(action_str)\n                else:\n                    # If we have arguments, they MUST be named:\n                    for kwarg in call_obj.keywords:\n                        # We only support Strings and Numerics:\n                        if isinstance(kwarg.value, ast.Num):\n                            args.update({kwarg.arg: kwarg.value.n})\n                        elif isinstance(kwarg.value, ast.Str):\n                            args.update({kwarg.arg: kwarg.value.s})\n                        else:\n                            raise UnsupportedActionArgumentError(action_str,\n                                                                 kwarg)\n\n            else:\n                raise UnsupportedActionError(action_str)\n\n        return cls(module, func, args)", "response": "Creates a new Action instance from the given string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadapts array to sqlite3 binary", "response": "def adapt_array(arr):\n    \"\"\"\n    http://stackoverflow.com/a/31312102/190597 (SoulNibbler)\n    \"\"\"\n    out = io.BytesIO()\n    np.save(out, arr)\n    out.seek(0)\n    return sqlite3.Binary(out.read())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a connection to the crystal database.", "response": "def open_data_base_connection(skip_dir_check=False):\n    \"\"\"\n    Creates a connections to the crystal database.\n    :param skip_dir_check: bool, True  -> Skips checking database file and creates a new one if not present.\n                                          This is used by Crystal.py to create a new database the first time.\n                                 False -> Raises a error if the database file is not found.\n    :return: conn, c -> connection and cursor object\n    \"\"\"\n    if not skip_dir_check:\n        assert os.path.isfile(dd.get_database_dir()), \\\n            \"Database file not found in {}. \" \\\n            \"Please ensure that you have written data atleast once.\".format(dd.get_database_dir())\n\n    # Converts np.array to TEXT when inserting\n    sqlite3.register_adapter(np.ndarray, adapt_array)\n\n    # Converts TEXT to np.array when selecting\n    sqlite3.register_converter(\"array\", convert_array)\n\n    conn = sqlite3.connect(dd.get_database_dir(), detect_types=sqlite3.PARSE_DECLTYPES)\n    c = conn.cursor()\n    return conn, c"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a run from a project.", "response": "def drop_run(project_name, run_name):\n    \"\"\"\n    Deletes a run from a desired project. If this causes the run_table to be empty then the entire project gets deleted\n    :param project_name: String, project which contains the desire run_name\n    :param run_name: String, run to delete\n    \"\"\"\n    conn, c = open_data_base_connection()\n    # delete all the variable tables first\n    c.execute(\"SELECT variable_name FROM {}\".format(run_name))\n    try:\n        all_variables = np.array(c.fetchall()).squeeze(axis=1)\n        for i in all_variables:\n            variable_table_name = run_name + '_' + i\n            c.execute(\"\"\"DROP TABLE IF EXISTS {}\"\"\".format(variable_table_name))\n    except np.core._internal.AxisError:\n        print(\"Did not find any values, so deleting run table directly.\")\n\n    c.execute(\"\"\"DROP TABLE IF EXISTS {}\"\"\".format(run_name))\n    c.execute(\"\"\"DELETE FROM {} WHERE run_name='{}'\"\"\".format(project_name + '_run_table', run_name))\n\n    # delete project if project_name+'_run_table' is empty\n    c.execute(\"\"\"SELECT run_name FROM {}\"\"\".format(project_name + '_run_table'))\n    all_runs = c.fetchall()\n    if len(all_runs) == 0:\n        c.execute(\"\"\"DROP TABLE IF EXISTS {}\"\"\".format(project_name + '_run_table'))\n        c.execute(\"\"\"DELETE FROM main_table WHERE project_name='{}'\"\"\".format(project_name))\n\n    conn.commit()\n    print(\"{} table deleted\".format(run_name))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef drop_project(project_name):\n    conn, c = open_data_base_connection()\n    # Need to delete all the run_tables before removing the project_table and the entry from the main_table\n    run_table_name = project_name + '_run_table'\n\n    c.execute(\"SELECT run_name FROM {}\".format(run_table_name))\n    run_names = np.array(c.fetchall()).squeeze(axis=1)\n\n    # remove one run at a time\n    for run in run_names:\n        drop_run(project_name, run)\n\n    c.execute(\"DROP TABLE IF EXISTS {}\".format(run_table_name))\n\n    # Remove the project row from main table\n    c.execute(\"\"\"DELETE FROM main_table WHERE project_name='{}'\"\"\".format(project_name))\n    conn.commit()\n    print(\"{} project deleted\".format(project_name))", "response": "Deletes all the tables associated with a project and removes it from the main_table\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the latest variable names for the run table.", "response": "def get_figure_stats(run_table_name):\n    \"\"\"\n    Returns the latest variable names\n    :param run_table_name: str, required run table name\n    :return: dict -> {<int_keys>: <variable_name>}\n    \"\"\"\n    conn, c = open_data_base_connection()\n\n    # Get latest project\n    c.execute(\"\"\"SELECT variable_name FROM {}\"\"\".format(run_table_name))\n    variable_names = convert_list_to_dict(np.array(c.fetchall()).squeeze(axis=1))\n    conn.close()\n\n    return variable_names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict of projects present in the database.", "response": "def get_projects():\n    \"\"\"\n    Returns a dict of projects present in the database.\n    :return: dict -> {<int_keys>: <project_name>}\n    \"\"\"\n    conn, c = open_data_base_connection()\n    try:\n        c.execute(\"\"\"SELECT project_name FROM main_table\"\"\")\n        project_names = np.array(c.fetchall()).squeeze(axis=1)\n        project_names = convert_list_to_dict(project_names)\n        return project_names\n    except sqlite3.OperationalError:\n        logging.info(\"{} not found\".format(run_name))\n    finally:\n        conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict of runs present in project.", "response": "def get_runs(project_name):\n    \"\"\"\n    Returns a dict of runs present in project.\n    :return: dict -> {<int_keys>: <project_name>}\n    \"\"\"\n    conn, c = open_data_base_connection()\n    try:\n        c.execute(\"\"\"SELECT run_name FROM {}\"\"\".format(project_name + \"_run_table\"))\n        run_names = np.array(c.fetchall()).squeeze(axis=1)\n        run_names = convert_list_to_dict(run_names)\n        return run_names\n    except sqlite3.OperationalError:\n        logging.info(\"{} not found\".format(run_name))\n    finally:\n        conn.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_variables(run_name):\n    conn, c = open_data_base_connection()\n    try:\n        # Get latest project\n        c.execute(\"\"\"SELECT variable_name FROM {}\"\"\".format(run_name))\n        variable_names = np.array(c.fetchall()).squeeze(axis=1)\n        variable_names = convert_list_to_dict(variable_names)\n        return variable_names\n    except sqlite3.OperationalError:\n        logging.info(\"{} not found\".format(run_name))\n    finally:\n        conn.close()", "response": "Returns a dict of variables in the selected run table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying appropriate tables and return data to dashboard in the required format.", "response": "def get_variable_update_dicts(current_index, variable_names, selected_run):\n    \"\"\"\n    Query appropriate tables and return data to dashboard in the required format.\n    :param current_index: int, current index during update\n    :param variable_names: str, variable name to fetch values from\n    :param selected_run: str, run containing the variable\n    :return: dict, {<variable_name>: [<values>]}\n    \"\"\"\n    conn, c = open_data_base_connection()\n    data = {}\n    for _, v_n in variable_names:\n        data[v_n] = {'x': [], 'y': [], 'z': [], 'vn': []}\n\n    try:\n        # values for each variable\n        for _, v_n in variable_names:\n            plot_type = v_n.split(\"_\")[0]\n            if plot_type == \"scalar\":\n                try:\n                    c.execute(\"\"\"SELECT X_value FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    x_values = np.array(c.fetchall()).squeeze().tolist()\n                    c.execute(\"\"\"SELECT Y_value FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    y_values = np.array(c.fetchall()).squeeze().tolist()\n                    data[v_n][\"x\"] = x_values\n                    data[v_n][\"y\"] = y_values\n                    n_values = len(x_values)\n                    current_index[\"{}\".format(v_n)] += n_values\n                    logging.info(\"New value found and updated\")\n                except IndexError:\n                    logging.info(\"No new data point found\")\n            elif plot_type == \"heatmap\":\n                try:\n                    c.execute(\"\"\"SELECT X_value FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    x_values = np.array(c.fetchall()).squeeze().tolist()\n                    c.execute(\"\"\"SELECT Y_value FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    y_values = np.array(c.fetchall()).squeeze().tolist()\n                    c.execute(\"\"\"SELECT V_names FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    v_values = np.array(c.fetchall()).squeeze().tolist()\n                    data[v_n][\"x\"] = x_values\n                    data[v_n][\"z\"] = y_values\n                    data[v_n][\"vn\"] = v_values\n                    n_values = len(x_values)\n                    current_index[\"{}\".format(v_n)] += n_values\n                    logging.info(\"New value found and updated\")\n                except sqlite3.OperationalError:\n                    c.execute(\"\"\"SELECT X_value FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    x_values = np.array(c.fetchall()).squeeze().tolist()\n                    c.execute(\"\"\"SELECT Y_value FROM {} WHERE rowid > {}\"\"\".format(selected_run + \"_\" + v_n,\n                                                                                   current_index[v_n]))\n                    y_values = np.array(c.fetchall()).squeeze().tolist()\n                    data[v_n][\"x\"] = x_values\n                    data[v_n][\"z\"] = y_values\n                    n_values = len(x_values)\n                    current_index[\"{}\".format(v_n)] += n_values\n                    logging.info(\"New value found and updated\")\n                except IndexError:\n                    logging.info(\"No new data point found\")\n    except KeyError:\n        logging.error(\"I think the run variable has changes. So, I'm passing no data.\")\n\n    conn.close()\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_graph_csv(variable_table_name):\n    temp_csv = home_dir + \"/PycharmProjects/crystal/crystal/static/temp.csv\"\n    conn, c = open_data_base_connection()\n\n    # Get variable data\n    c.execute(\"\"\"SELECT * FROM {}\"\"\".format(variable_table_name))\n    with open(temp_csv, \"w\", newline='') as csv_file:\n        csv_writer = csv.writer(csv_file)\n        csv_writer.writerow([i[0] for i in c.description])  # write headers\n        csv_writer.writerows(c)\n        print(\"File saved: {}\".format(temp_csv))\n        conn.close()\n\n        return temp_csv", "response": "Generates a temporary CSV file that contains the data for the selected variable table name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a population of : class : Page <pages. models. Page > objects for testing purpose.", "response": "def populate_pages(self, parent=None, child=5, depth=5):\n        \"\"\"Create a population of :class:`Page <pages.models.Page>`\n        for testing purpose.\"\"\"\n        User = get_user_model()\n        from basic_cms.models import Content\n        author = User.objects.all()[0]\n        if depth == 0:\n            return\n        p = self.model(parent=parent, author=author, status=self.model.PUBLISHED)\n        p.save()\n        p = self.get(id=p.id)\n        Content(body='page-' + str(p.id), type='title', language=settings.PAGE_DEFAULT_LANGUAGE, page=p).save()\n        Content(body='page-' + str(p.id), type='slug', language=settings.PAGE_DEFAULT_LANGUAGE, page=p).save()\n        for child in range(1, child + 1):\n            self.populate_pages(parent=p, child=child, depth=(depth - 1))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a QuerySet of pages that are published on the site_id.", "response": "def on_site(self, site_id=None):\n        \"\"\"Return a :class:`QuerySet` of pages that are published on the site\n        defined by the ``SITE_ID`` setting.\n\n        :param site_id: specify the id of the site object to filter with.\n        \"\"\"\n        if settings.PAGE_USE_SITE_ID:\n            if not site_id:\n                site_id = settings.SITE_ID\n            return self.filter(sites=site_id)\n        return self.all()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a QuerySet of drafts using the page s . publication_date.", "response": "def drafts(self):\n        \"\"\"Creates a :class:`QuerySet` of drafts using the page's\n        :attr:`Page.publication_date`.\"\"\"\n        pub = self.on_site().filter(status=self.model.DRAFT)\n        if settings.PAGE_SHOW_START_DATE:\n            pub = pub.filter(publication_date__gte=datetime.now())\n        return pub"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsanitize a string in order to avoid possible XSS using html5lib.", "response": "def sanitize(self, content):\n        \"\"\"Sanitize a string in order to avoid possible XSS using\n        ``html5lib``.\"\"\"\n        import html5lib\n        from html5lib import sanitizer\n        p = html5lib.HTMLParser(tokenizer=sanitizer.HTMLSanitizer)\n        dom_tree = p.parseFragment(content)\n        return dom_tree.text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget ETA time to process items in the items nacorense", "response": "def get_time_eta(total_done, total, start_time):\n    \"\"\"Gets ETA\n\n    :param total_done: items processed\n    :param total: total # of items to process\n    :param start_time: Time of start processing items\n    :return: Time to go\n    \"\"\"\n    time_done = int(time()) - start_time\n    speed = total_done / time_done\n\n    if time_done > 0 and speed > 0:\n        total_to_go = total - total_done\n        time_to_go = total_to_go / speed\n        minutes, seconds = divmod(time_to_go, 60)\n        hours, minutes = divmod(minutes, 60)\n        percentage = total_done * 100.0 / total\n    else:\n        percentage = 0\n        hours = 0\n        minutes = 0\n        seconds = 0\n\n    return {\n        \"done\": int(total_done),\n        \"tot\": int(total),\n        \"%\": float(\"{0:.2f}\".format(percentage)),\n        \"h\": int(hours),\n        \"m\": int(minutes),\n        \"s\": int(seconds)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef consume(self, cwd=None):\n        first_pass = Grammar.overall.parseString(self.string)\n        lowered = { key.lower(): val for key, val in first_pass.iteritems() }\n\n        self.commands = ['\\n'.join(self._get('commands', lowered))]\n        self.job_options = self._get('job_options', lowered)\n        self.global_options = self._get('options', lowered)\n\n        self.files = self._get('files', lowered)\n        self.paths = self._get('paths', lowered)\n\n        self.files = self._parse(self.files, Grammar.file, True)\n        self.paths = self._parse(self.paths, Grammar.path, True)\n        self.job_options = self._parse(self.job_options, Grammar.line)\n\n        try:\n            command_lines = self._parse(self.commands, Grammar.command_lines)[0]\n        except IndexError:\n            raise ValueError('Did you write any commands?')\n\n        self.commands = []\n        for command_line in command_lines:\n            comments, command = command_line\n            self.commands.append([comments.asList(),\n                self._parse([''.join(command)], Grammar.command)])\n\n        self.job_options = [opt.asList() for opt in self.job_options]\n\n        self.paths = ctf.get_paths(self.paths)\n        self.files = ctf.get_files(self.files)\n\n        self.paths.reverse()\n        self.files.reverse()\n        self.commands.reverse()\n\n        return ctf.get_command_templates(self.commands, self.files[:],\n            self.paths[:], self.job_options)", "response": "Parses the lexer tokens into valid statements. This function returns a list of valid statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get(self, key, parser_result):\n        try:\n            list_data = parser_result[key].asList()\n            if any(isinstance(obj, str) for obj in list_data):\n                txt_lines = [''.join(list_data)]\n            else:\n                txt_lines = [''.join(f) for f in list_data]\n        except KeyError:\n            txt_lines = []\n        return txt_lines", "response": "Given a type and a dict of parser results return the items as a list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a type and a list of lines parse it using the more detailed parse grammar.", "response": "def _parse(self, lines, grammar, ignore_comments=False):\n        \"\"\" Given a type and a list, parse it using the more detailed\n        parse grammar.\n        \"\"\"\n        results = []\n        for c in lines:\n            if c != '' and not (ignore_comments and c[0] == '#'):\n                try:\n                    results.append(grammar.parseString(c))\n                except pyparsing.ParseException as e:\n                    raise ValueError('Invalid syntax. Verify line {} is '\n                        'correct.\\n{}\\n\\n{}'.format(e.lineno, c, e))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef objectprep(self):\n        # Move the files to subfolders and create objects\n        self.runmetadata = createobject.ObjectCreation(self)\n        if self.extension == 'fastq':\n            # To streamline the CLARK process, decompress and combine .gz and paired end files as required\n            logging.info('Decompressing and combining .fastq files for CLARK analysis')\n            fileprep.Fileprep(self)\n        else:\n            logging.info('Using .fasta files for CLARK analysis')\n            for sample in self.runmetadata.samples:\n                sample.general.combined = sample.general.fastqfiles[0]", "response": "Create objects to store data and metadata for each sample. Also perform necessary file manipulations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef settargets(self):\n        # Define the set targets call. Include the path to the script, the database path and files, as well\n        # as the taxonomic rank to use\n        logging.info('Setting up database')\n        self.targetcall = 'cd {} && ./set_targets.sh {} {} --{}'.format(self.clarkpath, self.databasepath,\n                                                                        self.database, self.rank)\n        #\n        subprocess.call(self.targetcall, shell=True, stdout=self.devnull, stderr=self.devnull)", "response": "Set the targets to be used in the analyses."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving reads that contain plasmids and masks phage sequences.", "response": "def clean_sequences(self):\n        \"\"\"Removes reads/contigs that contain plasmids, and masks phage sequences.\"\"\"\n        logging.info('Removing plasmids and masking phages')\n        plasmid_db = os.path.join(self.reffilepath, 'plasmidfinder', 'plasmid_database.fa')\n        phage_db = os.path.join(self.reffilepath, 'prophages', 'combinedtargets.tfa')\n        with progressbar(self.runmetadata.samples) as bar:\n            for sample in bar:\n                plasmid_removal = 'bbduk.sh ref={} in={} out={} overwrite'\\\n                    .format(plasmid_db, sample.general.combined, sample.general.combined.replace('.f', '_noplasmid.f'))\n                subprocess.call(plasmid_removal, shell=True, stdout=self.devnull, stderr=self.devnull)\n                phage_masking = 'bbduk.sh ref={} in={} out={} kmask=N overwrite'\\\n                    .format(phage_db, sample.general.combined.replace('.f', '_noplasmid.f'),\n                            sample.general.combined.replace('.f', '_clean.f'))\n                subprocess.call(phage_masking, shell=True, stdout=self.devnull, stderr=self.devnull)\n                os.remove(sample.general.combined)\n                os.rename(sample.general.combined.replace('.f', '_clean.f'), sample.general.combined)\n                os.remove(sample.general.combined.replace('.f', '_noplasmid.f'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the classify metagenome on the samples", "response": "def classifymetagenome(self):\n        \"\"\"Run the classify metagenome of the CLARK package on the samples\"\"\"\n        logging.info('Classifying metagenomes')\n        # Define the system call\n        self.classifycall = 'cd {} && ./classify_metagenome.sh -O {} -R {} -n {} --light'\\\n            .format(self.clarkpath,\n                    self.filelist,\n                    self.reportlist,\n                    self.cpus)\n        # Variable to store classification state\n        classify = True\n        for sample in self.runmetadata.samples:\n            try:\n                # Define the name of the .csv classification file\n                sample.general.classification = sample.general.combined.split('.')[0] + '.csv'\n                # If the file exists, then set classify to False\n                if os.path.isfile(sample.general.classification):\n                    classify = False\n            except KeyError:\n                pass\n        # Run the system call if the samples have not been classified\n        if classify:\n            # Run the call\n            subprocess.call(self.classifycall, shell=True, stdout=self.devnull, stderr=self.devnull)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprepares the list of files to be processed", "response": "def lists(self):\n        \"\"\"\n        Prepare the list of files to be processed\n        \"\"\"\n        # Prepare the lists to be used to classify the metagenomes\n        with open(self.filelist, 'w') as filelist:\n            with open(self.reportlist, 'w') as reportlist:\n                for sample in self.runmetadata.samples:\n                    if self.extension == 'fastq':\n                        try:\n                            status = sample.run.Description\n                            if status == 'metagenome':\n                                filelist.write(sample.general.combined + '\\n')\n                                reportlist.write(sample.general.combined.split('.')[0] + '\\n')\n                        except AttributeError:\n                            pass\n                    else:\n                        if sample.general.combined != 'NA':\n                            filelist.write(sample.general.combined + '\\n')\n                            reportlist.write(sample.general.combined.split('.')[0] + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nestimate the abundance of taxonomic groups and sends them to threads", "response": "def estimateabundance(self):\n        \"\"\"\n        Estimate the abundance of taxonomic groups\n        \"\"\"\n        logging.info('Estimating abundance of taxonomic groups')\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.estimate, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        with progressbar(self.runmetadata.samples) as bar:\n            for sample in bar:\n                try:\n                    if sample.general.combined != 'NA':\n                        # Set the name of the abundance report\n                        sample.general.abundance = sample.general.combined.split('.')[0] + '_abundance.csv'\n                        # if not hasattr(sample, 'commands'):\n                        if not sample.commands.datastore:\n                            sample.commands = GenObject()\n\n                        # Define system calls\n                        sample.commands.target = self.targetcall\n                        sample.commands.classify = self.classifycall\n                        sample.commands.abundancecall = \\\n                            'cd {} && ./estimate_abundance.sh -D {} -F {} > {}'.format(self.clarkpath,\n                                                                                       self.databasepath,\n                                                                                       sample.general.classification,\n                                                                                       sample.general.abundance)\n                        self.abundancequeue.put(sample)\n                except KeyError:\n                    pass\n        self.abundancequeue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the CLARK report from the abundance estimation", "response": "def reports(self):\n        \"\"\"\n        Create reports from the abundance estimation\n        \"\"\"\n        logging.info('Creating CLARK report for {} files'.format(self.extension))\n        # Create a workbook to store the report. Using xlsxwriter rather than a simple csv format, as I want to be\n        # able to have appropriately sized, multi-line cells\n        workbook = xlsxwriter.Workbook(self.report)\n        make_path(self.reportpath)\n        # New worksheet to store the data\n        worksheet = workbook.add_worksheet()\n        # Add a bold format for header cells. Using a monotype font size 8\n        bold = workbook.add_format({'bold': True, 'font_name': 'Courier New', 'font_size': 8})\n        bold.set_align('center')\n        # Format for data cells. Monotype, size 8, top vertically justified\n        courier = workbook.add_format({'font_name': 'Courier New', 'font_size': 8})\n        courier.set_align('top')\n        # Set the custom width for 5 and 6 to be 15\n        worksheet.set_column(5, 5, 15)\n        worksheet.set_column(6, 6, 20)\n        # Initialise the position within the worksheet to be (0,0)\n        row = 0\n        col = 0\n        # List of the headers to use\n        headers = ['Strain', 'Name', 'TaxID', 'Lineage', 'Count', 'Proportion_All(%)', 'Proportion_Classified(%)']\n        # Add an additional header for .fasta analyses\n        if self.extension == 'fasta':\n            headers.insert(4, 'TotalBP')\n        # Populate the headers\n        for category in headers:\n            # Write the data in the specified cell (row, col) using the bold format\n            worksheet.write(row, col, category, bold)\n            # Move to the next column to write the next category\n            col += 1\n        # Data starts in row 1\n        row = 1\n        # Initialise variables to hold the longest names; used in setting the column width\n        longeststrain = 0\n        longestname = 0\n        longestlineage = 0\n        # Extract all the taxonomic groups that pass the cutoff from the abundance file\n        for sample in self.runmetadata.samples:\n            # Every record starts at column 0\n            col = 0\n            # Write the strain name\n            worksheet.write(row, col, sample.name, courier)\n            col += 1\n            # Initialise a dictionary to store the species above the cutoff in the sample\n            sample.general.passfilter = list()\n            try:\n                # Abundance file as a dictionary\n                abundancedict = DictReader(open(sample.general.abundance))\n                # Filter abundance to taxIDs with at least self.cutoff% of the total proportion\n                for result in abundancedict:\n                    # The UNKNOWN category doesn't contain a 'Lineage' column, and therefore, subsequent columns are\n                    # shifted out of proper alignment, and do not contain the appropriate data\n                    try:\n                        if float(result['Proportion_All(%)']) > self.cutoff:\n                            sample.general.passfilter.append(result)\n                    except ValueError:\n                        pass\n                # Determine the longest name of all the strains, and use it to set the width of column 0\n                if len(sample.name) > longeststrain:\n                    longeststrain = len(sample.name)\n                    worksheet.set_column(0, 0, longeststrain)\n                # Sort the abundance results based on the highest count\n                sortedabundance = sorted(sample.general.passfilter, key=lambda x: int(x['Count']), reverse=True)\n                # Set of contigs from the classification file. For some reason, certain contigs are represented multiple\n                # times in the classification file. As far as I can tell, these multiple representations are always\n                # classified the same, and, therefore, should be treated as duplicates, and ignored\n                contigset = set()\n                for result in sortedabundance:\n                    # Add the total number of base pairs classified for each TaxID. As only the total number of contigs\n                    # classified as a particular TaxID are in the report, it can be misleading if a large number\n                    # of small contigs are classified to a particular TaxID e.g. 56 contigs map to TaxID 28901, and 50\n                    # contigs map to TaxID 630, however, added together, those 56 contigs are 4705838 bp, while the 50\n                    # contigs added together are only 69602 bp. While this is unlikely a pure culture, only\n                    # 69602 / (4705838 + 69602) = 1.5% of the total bp map to TaxID 630 compared to 45% of the contigs\n                    if self.extension == 'fasta':\n                        # Initialise a variable to store the total bp mapped to the TaxID\n                        result['TotalBP'] = int()\n                        # Read the classification file into a dictionary\n                        classificationdict = DictReader(open(sample.general.classification))\n                        # Read through each contig classification in the dictionary\n                        for contig in classificationdict:\n                            # Pull out each contig with a TaxID that matches the TaxID of the result of interest, and\n                            # is not present in a set of contigs that have already been added to the dictionary\n                            if result['TaxID'] == contig[' Assignment'] and contig['Object_ID'] not in contigset:\n                                # Increment the total bp mapping to the TaxID by the integer of each contig\n                                result['TotalBP'] += int(contig[' Length'])\n                                # Avoid duplicates by adding the contig name to the set of contigs\n                                contigset.add(contig['Object_ID'])\n                    # Print the results to file\n                    # Ignore the first header, as it is the strain name, which has already been added to the report\n                    dictionaryheaders = headers[1:]\n                    for header in dictionaryheaders:\n                        data = result[header]\n                        worksheet.write(row, col, data, courier)\n                        col += 1\n                        # Determine the longest name of all the matches, and use it to set the width of column 0\n                        if len(result['Name']) > longestname:\n                            longestname = len(result['Name'])\n                            worksheet.set_column(1, 1, longestname)\n                        # Do the same for the lineages\n                        if len(result['Lineage']) > longestlineage:\n                            longestlineage = len(result['Lineage'])\n                            worksheet.set_column(3, 3, longestlineage)\n                    # Increase the row\n                    row += 1\n                    # Set the column to 1\n                    col = 1\n            except KeyError:\n                # Increase the row\n                row += 1\n        # Close the workbook\n        workbook.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_command_templates(command_tokens, file_tokens=[], path_tokens=[],\n    job_options=[]):\n    \"\"\" Given a list of tokens from the grammar, return a\n    list of commands.\n    \"\"\"\n    files = get_files(file_tokens)\n    paths = get_paths(path_tokens)\n    job_options = get_options(job_options)\n\n    templates = _get_command_templates(command_tokens, files, paths,\n        job_options)\n\n    for command_template in templates:\n        command_template._dependencies = _get_prelim_dependencies(\n            command_template, templates)\n    return templates", "response": "Given a list of tokens from the grammar return a list of commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a list of parser file tokens return a list of input objects for them.", "response": "def get_files(file_tokens, cwd=None):\n    \"\"\" Given a list of parser file tokens, return a list of input objects\n    for them.\n    \"\"\"\n    if not file_tokens:\n        return []\n\n    token = file_tokens.pop()\n    try:\n        filename = token.filename\n    except AttributeError:\n        filename = ''\n\n    if cwd:\n        input = Input(token.alias, filename, cwd=cwd)\n    else:\n        input = Input(token.alias, filename)\n\n    return [input] + get_files(file_tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_paths(path_tokens):\n    if len(path_tokens) == 0:\n        return []\n\n    token = path_tokens.pop()\n    path = PathToken(token.alias, token.path)\n    return [path] + get_paths(path_tokens)", "response": "Given a list of parser path tokens return a list of path objects\n    for them."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_command_templates(command_tokens, files=[], paths=[], job_options=[],\n    count=1):\n    \"\"\" Reversivly create command templates. \"\"\"\n    if not command_tokens:\n        return []\n\n    comment_tokens, command_token = command_tokens.pop()\n    parts = []\n\n    parts += job_options + _get_comments(comment_tokens)\n    for part in command_token[0]:\n        # Check for file\n        try:\n            parts.append(_get_file_by_alias(part, files))\n            continue\n        except (AttributeError, ValueError):\n            pass\n\n        # Check for path/string\n        for cut in part.split():\n            try:\n                parts.append(_get_path_by_name(cut, paths))\n                continue\n            except ValueError:\n                pass\n\n            parts.append(cut)\n\n    command_template = CommandTemplate(alias=str(count), parts=parts)\n    [setattr(p, 'alias', command_template.alias)\n        for p in command_template.output_parts]\n    return [command_template] + _get_command_templates(command_tokens,\n        files, paths, job_options, count+1)", "response": "Reversivly create command templates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a command_template and a list of all templates determine which other templates it depends on.", "response": "def _get_prelim_dependencies(command_template, all_templates):\n    \"\"\" Given a command_template determine which other templates it\n    depends on. This should not be used as the be-all end-all of\n    dependencies and before calling each command, ensure that it's\n    requirements are  met.\n    \"\"\"\n    deps = []\n    for input in command_template.input_parts:\n        if '.' not in input.alias:\n            continue\n        for template in all_templates:\n            for output in template.output_parts:\n                if input.fuzzy_match(output):\n                    deps.append(template)\n                    break\n    return list(set(deps))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a command part find the file that represents the command file alias.", "response": "def _get_file_by_alias(part, files):\n    \"\"\" Given a command part, find the file it represents. If not found,\n    then returns a new token representing that file.\n    :throws ValueError: if the value is not a command file alias.\n    \"\"\"\n    # Make Output\n    if _is_output(part):\n        return Output.from_string(part.pop())\n\n    # Search/Make Input\n    else:\n        inputs = [[]]\n\n        if part.magic_or:\n            and_or = 'or'\n        else:\n            and_or = 'and'\n\n        for cut in part.asList():\n            if cut == OR_TOKEN:\n                inputs.append([])\n                continue\n            if cut == AND_TOKEN:\n                continue\n\n            input = Input(cut, filename=cut, and_or=and_or)\n            for file in files:\n                if file.alias == cut:\n                    # Override the filename\n                    input.filename = file.filename\n                    inputs[-1].append(input)\n                    break\n            else:\n                inputs[-1].append(input)\n\n\n        return [input for input in inputs if input]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a command part find the path that it represents.", "response": "def _get_path_by_name(part, paths):\n    \"\"\" Given a command part, find the path it represents.\n    :throws ValueError: if no valid file is found.\n    \"\"\"\n    for path in paths:\n        if path.alias == part:\n            return path\n    raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether the given part represents an output variable.", "response": "def _is_output(part):\n    \"\"\" Returns whether the given part represents an output variable. \"\"\"\n    if part[0].lower() == 'o':\n        return True\n    elif part[0][:2].lower() == 'o:':\n        return True\n    elif part[0][:2].lower() == 'o.':\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search_browser(self, text):\n    self.impl.get(self.base_url)\n\n    search_div = self.impl.find_element_by_id(\"search\")\n    search_term = search_div.find_element_by_id(\"term\")\n    search_term.send_keys(text)\n    search_div.find_element_by_id(\"submit\").click()\n    e = self.impl.find_element_by_css_selector(\"table.list tr td a\")\n    return e.get_attribute(\"href\")", "response": "do a slow search via the website and return the first match"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndo a sloppy quick search via the json index", "response": "def search_fast(self, text):\n    \"\"\"do a sloppy quick \"search\" via the json index\"\"\"\n\n    resp = self.impl.get(\n        \"{base_url}/{text}/json\".format(base_url=self.base_url, text=text)\n    )\n    return resp.json()[\"info\"][\"package_url\"]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(search, query):\n    url = search.search(query)\n    print(url)\n    search.open_page(url)", "response": "main function that does the search"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef camel_to_underscore(name):\n    name = re.sub(r'(?<!\\b)(?<!_)([A-Z][a-z])', r'_\\1', name)\n    name = re.sub(r'(?<!\\b)(?<!_)([a-z])([A-Z])', r'\\1_\\2', name)\n    name = name.lower()\n    return name", "response": "Convert camel case name to underscore name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlaunches something according to the provided arguments", "response": "def launch(self, args, unknown):\n        \"\"\"Launch something according to the provided arguments\n\n        :param args: arguments from the launch parser\n        :type args: Namespace\n        :param unknown: list of unknown arguments\n        :type unknown: list\n        :returns: None\n        :rtype: None\n        :raises: SystemExit\n        \"\"\"\n        pm = mayaplugins.MayaPluginManager.get()\n        addon = pm.get_plugin(args.addon)\n        isgui = isinstance(addon, coreplugins.JB_StandaloneGuiPlugin)\n        print \"Launching %s...\" % args.addon\n        addon.run()\n        if isgui:\n            app = guimain.get_qapp()\n            sys.exit(app.exec_())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, args, unknown):\n        pm = mayaplugins.MayaPluginManager.get()\n        plugs = pm.get_all_plugins()\n        if not plugs:\n            print \"No standalone addons found!\"\n            return\n        print \"Addons:\"\n        for p in plugs:\n            if isinstance(p, coreplugins.JB_StandalonePlugin):\n                print \"\\t%s\" % p.__class__.__name__", "response": "List all addons that can be launched by the launch parser."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_args(self, args=None):\n        if args is None:\n            args = sys.argv[1:]\n        return self.parser.parse_known_args(args)", "response": "Parse the given arguments and return a list of parsed arguments and all unknown arguments"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_objects(self, prefix=None, delimiter=None):\n        return self._client.list_objects(\n            instance=self._instance, bucket_name=self.name, prefix=prefix,\n            delimiter=delimiter)", "response": "List the objects in this bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_object(self, object_name):\n        return self._client.download_object(\n            self._instance, self.name, object_name)", "response": "Download an object.\n\n        :param str object_name: The object to fetch."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads an object to this bucket.", "response": "def upload_object(self, object_name, file_obj):\n        \"\"\"\n        Upload an object to this bucket.\n\n        :param str object_name: The target name of the object.\n        :param file file_obj: The file (or file-like object) to upload.\n        :param str content_type: The content type associated to this object.\n                                 This is mainly useful when accessing an object\n                                 directly via a web browser. If unspecified, a\n                                 content type *may* be automatically derived\n                                 from the specified ``file_obj``.\n        \"\"\"\n        return self._client.upload_object(\n            self._instance, self.name, object_name, file_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_object(self, object_name):\n        self._client.remove_object(self._instance, self.name, object_name)", "response": "Remove an object from this bucket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of objects in this listing.", "response": "def objects(self):\n        \"\"\"\n        The objects in this listing.\n\n        :type: List[:class:`.ObjectInfo`]\n        \"\"\"\n        return [ObjectInfo(o, self._instance, self._bucket, self._client)\n                for o in self._proto.object]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves this object from the object store.", "response": "def delete(self):\n        \"\"\"Remove this object.\"\"\"\n        self._client.remove_object(self._instance, self._bucket, self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces the content of this object.", "response": "def upload(self, file_obj):\n        \"\"\"\n        Replace the content of this object.\n\n        :param file file_obj: The file (or file-like object) to upload.\n        \"\"\"\n        return self._client.upload_object(\n            self._instance, self._bucket, self.name, file_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the moving average over an array.", "response": "def moving_average(arr: np.ndarray, n: int = 3) -> np.ndarray:\n    \"\"\" Calculate the moving overage over an array.\n\n    Algorithm from: https://stackoverflow.com/a/14314054\n\n    Args:\n        arr (np.ndarray): Array over which to calculate the moving average.\n        n (int): Number of elements over which to calculate the moving average. Default: 3\n    Returns:\n        np.ndarray: Moving average calculated over n.\n    \"\"\"\n    ret = np.cumsum(arr, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recursive_getattr(obj: Any, attr: str, *args) -> Any:\n    def _getattr(obj, attr):\n        return getattr(obj, attr, *args)\n    return functools.reduce(_getattr, [obj] + attr.split('.'))", "response": "Recursive ``getattar``.\n\n    This can be used as a drop in for the standard ``getattr(...)``. Credit to:\n    https://stackoverflow.com/a/31174427\n\n    Args:\n        obj: Object to retrieve the attribute from.\n        attr: Name of the attribute, with each successive attribute separated by a \".\".\n    Returns:\n        The requested attribute. (Same as ``getattr``).\n    Raises:\n        AttributeError: If the attribute was not found and no default was provided. (Same as ``getattr``)."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_array_for_fit(observables: dict, track_pt_bin: int, jet_pt_bin: int) -> histogram.Histogram1D:\n    for name, observable in observables.items():\n        if observable.track_pt_bin == track_pt_bin and observable.jet_pt_bin == jet_pt_bin:\n            return histogram.Histogram1D.from_existing_hist(observable.hist)\n\n    raise ValueError(\"Cannot find fit with jet pt bin {jet_pt_bin} and track pt bin {track_pt_bin}\")", "response": "Returns a histogram. Histogram1D object for the given jet and track pt bin."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate threads and create BLAST database of the primers and targets for each sample.", "response": "def primers(self):\n        \"\"\"Setup and create threads for ePCR\"\"\"\n        # Create the threads for the ePCR analysis\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                threads = Thread(target=self.epcr, args=())\n                threads.setDaemon(True)\n                threads.start()\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                setattr(sample, self.analysistype, GenObject())\n                # Get the primers ready\n                try:\n                    sample[self.analysistype].primers = glob(os.path.join(self.reffilepath,\n                                                                          self.analysistype,\n                                                                          sample.general.referencegenus,\n                                                                          'primers',\n                                                                          '*.txt'))[0]\n                    # Find the name of the probe file\n                    sample[self.analysistype].probes = glob(os.path.join(self.reffilepath,\n                                                                         self.analysistype,\n                                                                         sample.general.referencegenus,\n                                                                         'probes',\n                                                                         '*.fa'))[0]\n                    # Create the BLAST database of the probes (if necessary)\n                    self.makeblastdb(sample[self.analysistype].probes)\n                    # Initialise a list to store the names of the targets\n                    sample[self.analysistype].targets = list()\n                    # Open the primer file, and read the names of the targets into a list\n                    with open(sample[self.analysistype].primers, 'r') as primerfile:\n                        for line in primerfile:\n                            sample[self.analysistype].targets.append(line.split('\\t')[0])\n                # Organisms without primer/probe files will fail. Populate metadata with 'NA' values\n                except IndexError:\n                    sample[self.analysistype].primers = 'NA'\n                    sample[self.analysistype].probes = 'NA'\n                # Only try to process organisms with primer files\n                if sample[self.analysistype].primers != 'NA':\n                    # Make the output path\n                    sample[self.analysistype].reportdir = os.path.join(sample.general.outputdirectory,\n                                                                       self.analysistype)\n                    make_path(sample[self.analysistype].reportdir)\n                    # Set the base name of the output file\n                    outfile = sample[self.analysistype].reportdir + sample.name\n                    # Set the hashing and mapping commands\n                    sample.commands.famap = 'famap -b {}.famap {}.fasta'.format(outfile, sample.general.filenoext)\n                    sample.commands.fahash = 'fahash -b {}.hash {}.famap'.format(outfile, outfile)\n                    # re-PCR uses the subtyping primers list to search the contigs file using the following parameters\n                    # -S {hash file} (Perform STS lookup using hash-file), -r + (Enable/disable reverse STS lookup)\n                    # -m 10000 (Set variability for STS size for lookup),\n                    # -n 1 (Set max allowed mismatches per primer for lookup)\n                    # -g 0 (Set max allowed indels per primer for lookup),\n                    # -G (Print alignments in comments), -o {output file}\n                    sample.commands.epcr = 're-PCR -S {}.hash -r + -m 10000 -n 2 -g 0 -G -q -o {}.txt {}' \\\n                        .format(outfile, outfile, sample[self.analysistype].primers)\n                    # Add the variables to the queue\n                    self.epcrqueue.put((sample, outfile))\n        self.epcrqueue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the ePCR results and run BLAST on the parsed results", "response": "def epcrparsethreads(self):\n        \"\"\"\n        Parse the ePCR results, and run BLAST on the parsed results\n        \"\"\"\n        from Bio import SeqIO\n        # Create the threads for the BLAST analysis\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                threads = Thread(target=self.epcrparse, args=())\n                threads.setDaemon(True)\n                threads.start()\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                if sample[self.analysistype].primers != 'NA':\n                    # Initialise a dictionary to store the SeqIO records of each assembly\n                    record = dict()\n                    # Initialise dictionaries to store results in the object\n                    sample[self.analysistype].blastresults = dict()\n                    sample[self.analysistype].rawblastresults = dict()\n                    # Load the records from the assembly into the dictionary\n                    for rec in SeqIO.parse(sample.general.bestassemblyfile, 'fasta'):\n                        record[rec.id] = str(rec.seq)\n                    # Iterate through the ePCR results\n                    for line in sample[self.analysistype].epcrresults:\n                        # The data of interest is in the lines that do not start with a #\n                        # TLH 2016-SEQ-0359_4_length_321195_cov_28.6354_ID_3773 + 227879 228086 0\t0 208/1000-1000\n                        if not line.startswith('#'):\n                            # Add the variables to the queue\n                            self.epcrparsequeue.put((sample, record, line))\n        self.epcrparsequeue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the ePCR output from the ePCR command line and store the results in the object attribute", "response": "def epcrparse(self):\n        \"\"\"\n        Run BLAST, and record results to the object\n        \"\"\"\n        from Bio.Blast.Applications import NcbiblastnCommandline\n        while True:\n            sample, record, line = self.epcrparsequeue.get()\n            # Split the data on tabs\n            gene, chromosome, strand, start, end, m_match, gaps, act_len_exp_len = line.split('\\t')\n            # Extract the gene sequence from the contigs\n            # The record dictionary has the contig name, and the sequence. Splice out the data using the start and\n            # end coordinates specified by ePCR\n            genesequence = record[chromosome][int(start) - 1:int(end)]\n            # Set up BLASTn using blastn-short, as the probe sequences tend to be very short\n            blastn = NcbiblastnCommandline(db=sample[self.analysistype].probes.split('.')[0],\n                                           num_threads=12,\n                                           task='blastn-short',\n                                           num_alignments=1,\n                                           outfmt=\"'6 qseqid sseqid positive mismatch gaps \"\n                                                  \"evalue bitscore slen length'\")\n            # Run the BLASTn, with the gene sequence as stdin\n            out, err = blastn(stdin=genesequence)\n            # Split the output string on tabs\n            results = out.rstrip().split('\\t')\n            # Populate the raw blast results\n            sample[self.analysistype].rawblastresults[gene] = results\n            # Create named variables from the list\n            positives = float(results[2])\n            mismatches = float(results[3])\n            gaps = float(results[4])\n            subjectlength = float(results[7])\n            # Calculate the percent identity\n            percentidentity = float('{:0.2f}'.format((positives - gaps) / subjectlength * 100))\n            # Create a dictionary with the desired values to store in the metadata object\n            resultdict = {\n                'matches': positives,\n                'mismatches': mismatches,\n                'gaps': gaps,\n                'subject_length': subjectlength,\n                'percent_identity': percentidentity,\n                'match_length': results[8].split('\\n')[0]\n            }\n            # Populate the metadata object with the dictionary\n            sample[self.analysistype].blastresults[gene] = resultdict\n            self.epcrparsequeue.task_done()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makeblastdb(self, fastapath):\n        # remove the path and the file extension for easier future globbing\n        db = fastapath.split('.')[0]\n        nhr = '{}.nhr'.format(db)  # add nhr for searching\n        if not os.path.isfile(str(nhr)):  # if check for already existing dbs\n            # Create the databases\n            threadlock = threading.Lock()\n            command = 'makeblastdb -in {} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {}'.format(fastapath, db)\n            out, err = run_subprocess(command)\n            threadlock.acquire()\n            write_to_logfile(out, err, self.logfile)\n            threadlock.release()\n        dotter()", "response": "Make blast database files from targets as necessary\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef report(self):\n        # Initialise a variable to store the results\n        data = ''\n        for sample in self.metadata:\n            if sample[self.analysistype].primers != 'NA':\n                # Set the name of the strain-specific report\n                sample[self.analysistype].report = os.path.join(sample[self.analysistype].reportdir,\n                                                                '{}_{}.csv'.format(sample.name, self.analysistype))\n                # Populate the strain-specific string with header, and strain name\n                strainspecific = 'Strain,{},\\n{},'.format(','.join(sorted(sample[self.analysistype].targets)),\n                                                          sample.name)\n                # Iterate through all the genes in the organism-specific analysis\n                for gene in sorted(sample[self.analysistype].targets):\n                    try:\n                        # Extract the percent identity\n                        percentidentity = sample[self.analysistype].blastresults[gene]['percent_identity']\n                        # If the % identity is greater than the cutoff of 50%, the gene is considered to be present\n                        if percentidentity > 50:\n                            strainspecific += '{},'.format(percentidentity)\n                        else:\n                            strainspecific += '-,'\n                    # If there are no BLAST results, then the gene is absent\n                    except KeyError:\n                        strainspecific += '-,'\n                strainspecific += '\\n'\n                # Open and write the data to the strain-specific report\n                with open(sample[self.analysistype].report, 'w') as specificreport:\n                    specificreport.write(strainspecific)\n                # Add all the data from each strain to the cumulative data string\n                data += strainspecific\n        # Open and write the cumulative data to the cumulative report\n        with open(os.path.join(self.reportdir, '{}.csv'.format(self.analysistype)), 'w') as report:\n            report.write(data)", "response": "Create the findings - specific report for each strain - specific analysis and write the data to the cumulative report"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting up neccessary environment variables for mayapy.", "response": "def setup_environment():\n    \"\"\"Set up neccessary environment variables\n\n    This appends all path of sys.path to the python path\n    so mayapy will find all installed modules.\n    We have to make sure, that we use maya libs instead of\n    libs of the virtual env. So we insert all the libs for mayapy\n    first.\n\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    osinter = ostool.get_interface()\n    pypath = osinter.get_maya_envpath()\n    for p in sys.path:\n        pypath = os.pathsep.join((pypath, p))\n    os.environ['PYTHONPATH'] = pypath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute_mayapy(args, wait=True):\n    osinter = ostool.get_interface()\n    mayapy = osinter.get_maya_python()\n    allargs = [mayapy]\n    allargs.extend(args)\n    print \"Executing mayapy with: %s\" % allargs\n    mayapyprocess = subprocess.Popen(allargs)\n    if wait:\n        rc = mayapyprocess.wait()\n        print \"Process mayapy finished!\"\n        return rc\n    else:\n        return mayapyprocess", "response": "Execute maya python with the given arguments capture and return the output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart a new subprocess with mayapy and call the main_func.", "response": "def mayapy_launcher(args=None, wait=True):\n    \"\"\"Start a new subprocess with mayapy and call the :func:`jukeboxmaya.launcher.main_func`.\n\n    So this can be used when launching jukeboxmaya from an external intepreter but\n    you want to actually use the mayapy intepreter instead (because it\\'s less buggy).\n\n    :param args: arguments for the launcher. If None, sys.argv[1:] is used\n    :type args: list\n    :param wait: If True, waits for the process to finish and returns the returncode.\n                 If False, just returns the process\n    :type wait: bool\n    :returns: if wait is True, the returncode, else the process\n    :rtype: int|:class:`subprocess.Popen`\n    \"\"\"\n    if args is None:\n        args = sys.argv[1:]\n    arguments = [\"-m\",  \"jukeboxmaya.launcher\"]\n    arguments.extend(args)\n    setup_environment()\n    return execute_mayapy(arguments, wait)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setDoc(self, doc):\n        self.ui.overAtten.setNum(doc['overloaded_attenuation'])\n        # also set composite stim type\n        # self.ui.traceType.setText(doc['testtype'])\n\n        self.ui.componentDetails.clearDoc()\n        self.ui.componentDetails.setDoc(doc['components'])", "response": "Presents the documentation for the stimulusModel. i. e. returned from \n"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef increase_by_changes(self, changes_amount, ratio):\n        increases = round(changes_amount * ratio)\n        return self.increase(int(increases))", "response": "Increase version by amount of changes done\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a string version of a set of entries and returns a Subsystem object.", "response": "def from_str(string, max_number=9, separator=\".\"):\n        \"\"\"Parses string\n\n        :param string: Version\n        :param max_number: Max number reachable by sub\n        :param separator: Version numbers are separated with this split\n        :return: Parses string and returns object\n        \"\"\"\n        tokens = string.split(separator)\n        tokens = list(reversed(tokens))  # reverse order of importance\n        most_important = tokens[-1]  # cannot be parsed like the others\n        levels = [\n            Level(max_number, int(token)) for token in tokens[:-1]\n        ]\n        levels.append(\n            Level(float(\"inf\"), int(most_important))\n        )\n\n        return Subsystem(levels, separator)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving the name of a Maya UI element return the corresponding QWidget or QAction.", "response": "def wrap_maya_ui(mayaname):\n    \"\"\"Given the name of a Maya UI element of any type,\n    return the corresponding QWidget or QAction.\n    If the object does not exist, returns None\n\n    :param mayaname: the maya ui element\n    :type mayaname: str\n    :returns: the wraped object\n    :rtype: QObject | None\n    :raises: None\n    \"\"\"\n    ptr = apiUI.MQtUtil.findControl(mayaname)\n    if ptr is None:\n        ptr = apiUI.MQtUtil.findLayout(mayaname)\n    if ptr is None:\n        ptr = apiUI.MQtUtil.findMenuItem(mayaname)\n    if ptr is not None:\n        return wrap(long(ptr))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery the return type and argument list of the specified function in the specified database.", "response": "def query_args(self, name):\n        \"\"\"Query the return type and argument list of the specified\n        function in the specified database.\n        \"\"\"\n        sql = 'select type, id from code_items ' \\\n              'where kind = 22 and name = ?'\n        logging.debug('%s %s', sql, (name,))\n        self.cursor.execute(sql, (name,))\n        func = self.cursor.fetchone()\n        if func:\n            sql = 'select param_number, type, name ' \\\n                  'from code_items where parent_id = ?'\n            logging.debug('%s %s', sql, (func[1],))\n            self.cursor.execute(sql, (func[1],))\n            args = self.cursor.fetchall()\n            ret_type = clean_ret_type(func[0])\n            args = [\n                (arg_number, sanitize_type(arg_type), arg_name)\n                for arg_number, arg_type, arg_name in args\n            ]\n            return ret_type, name, args\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the information of the name in the database.", "response": "def query_info(self, name, like, kind):\n        \"\"\"Query the information of the name in the database.\"\"\"\n        kind = self._make_kind_id(kind)\n        # Database from VS2015 does not have assoc_text.\n        #\n        # sql = 'select name, kind, file_id, type, assoc_text ' \\\n        #       'from code_items ' \\\n        #       'where name {} ?'.format('like' if like else '=')\n        sql = 'select name, kind, file_id, type ' \\\n              'from code_items ' \\\n              'where name {} ?'.format('like' if like else '=')\n        args = (name,)\n        if like:\n            sql += ' escape ?'\n            args = (name, '\\\\')\n        if kind:\n            sql += ' and kind = ?'\n            args = (name, kind)\n        if like and kind:\n            args = (name, '\\\\', kind)\n        logging.debug('%s %s', sql, args)\n        self.cursor.execute(sql, args)\n        return self.cursor.fetchall(), self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query_names(self, name, like, kind):\n        kind = self._make_kind_id(kind)\n        sql = 'select id, name from files ' \\\n              'where leaf_name {} ?'.format('like' if like else '=')\n        args = (name,)\n        if like:\n            sql += ' escape ?'\n            args = (name, '\\\\')\n        logging.debug('%s %s', sql, args)\n        self.cursor.execute(sql, args)\n        ids = self.cursor.fetchall()\n        files = []\n        for file_id, header in ids:\n            sql = 'select name from code_items ' \\\n                  'where file_id = ?'\n            args = (file_id,)\n            if kind:\n                sql += 'and kind = ?'\n                args = (file_id, kind)\n            logging.debug('%s %s', sql, args)\n            self.cursor.execute(sql, args)\n            files.append((header, self.cursor.fetchall()))\n        return files", "response": "Query function declarations in the files."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a file id to the file name.", "response": "def file_id_to_name(self, file_id):\n        \"\"\"Convert a file id to the file name.\"\"\"\n        sql = 'select name from files where id = ?'\n        logging.debug('%s %s', sql, (file_id,))\n        self.cursor.execute(sql, (file_id,))\n        name = self.cursor.fetchone()\n        if name:\n            return name[0]\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes kind_id from kind_name or kind_id.", "response": "def _make_kind_id(self, name_or_id):\n        \"\"\"Make kind_id from kind_name or kind_id.\"\"\"\n        if not name_or_id:\n            return None\n        if name_or_id.isdigit():\n            return name_or_id\n        return self.kind_name_to_id(name_or_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a dictionary mapping kind ids to the names.", "response": "def _init_kind_converter(self):\n        \"\"\"Make a dictionary mapping kind ids to the names.\"\"\"\n        from ..utils import invert_dict\n\n        kinds = self.session.query(Kind).all()\n        self._kind_id_to_name = {\n            kind.id: kind.name for kind in kinds\n        }\n        self._kind_name_to_id = invert_dict(self._kind_id_to_name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_export(self, exports):\n        sql = 'drop table if exists export'\n        logging.debug(sql)\n        self.cursor.execute(sql)\n        sql = 'create table if not exists export ' \\\n              '(func text unique, module text)'\n        logging.debug(sql)\n        self.cursor.execute(sql)\n        for module in exports:\n            logging.debug(_('insering exports from %s'), module)\n            sql = 'insert into export values (?, ?)'\n            for func in exports[module]:\n                if func:\n                    try:\n                        self.cursor.execute(sql, (func, module))\n                    except sqlite3.IntegrityError:\n                        pass\n        self.con.commit()", "response": "Populate library exported function data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query_func_module(self, func):\n        exp = self.session.query(Export).filter_by(\n            func=func).first()\n        if exp:\n            return exp\n        logging.debug(_('Function not found: %s'), func)\n        alt = func + 'A'\n        exp = self.session.query(Export).filter_by(\n            func=alt).first()\n        if exp:\n            logging.warning(_('Using ANSI version: %s'), alt)\n            return exp\n        logging.warning(_('Not handled: %s or %s'), func, alt)\n        return None", "response": "Query the module name of the specified function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying the functions in the specified module.", "response": "def query_module_funcs(self, module):\n        \"\"\"Query the functions in the specified module.\"\"\"\n        funcs = self.session.query(Export).filter_by(\n            module=module).all()\n        return funcs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _wrap_callback_parse_parameter_data(subscription, on_data, message):\n    if message.type == message.REPLY:\n        data = web_pb2.ParameterSubscriptionResponse()\n        data.ParseFromString(message.reply.data)\n        subscription.subscription_id = data.subscriptionId\n    elif (message.type == message.DATA and\n          message.data.type == yamcs_pb2.PARAMETER):\n        parameter_data = ParameterData(getattr(message.data, 'parameterData'))\n        #pylint: disable=protected-access\n        subscription._process(parameter_data)\n        if on_data:\n            on_data(parameter_data)", "response": "Wraps an optional user callback to parse ParameterData from a WebSocket message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps an optional user callback to parse CommandHistoryEntry from a WebSocket data message.", "response": "def _wrap_callback_parse_cmdhist_data(subscription, on_data, message):\n    \"\"\"\n    Wraps an (optional) user callback to parse CommandHistoryEntry\n    from a WebSocket data message\n    \"\"\"\n    if (message.type == message.DATA and\n            message.data.type == yamcs_pb2.CMD_HISTORY):\n        entry = getattr(message.data, 'command')\n        #pylint: disable=protected-access\n        rec = subscription._process(entry)\n        if on_data:\n            on_data(rec)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap an optional user callback to parse Alarm data from a WebSocket message.", "response": "def _wrap_callback_parse_alarm_data(subscription, on_data, message):\n    \"\"\"\n    Wraps an (optional) user callback to parse Alarm data\n    from a WebSocket data message\n    \"\"\"\n    if (message.type == message.DATA and\n            message.data.type == yamcs_pb2.ALARM_DATA):\n        proto = getattr(message.data, 'alarmData')\n        alarm_event = AlarmEvent(proto)\n        #pylint: disable=protected-access\n        subscription._process(alarm_event)\n        if on_data:\n            on_data(alarm_event)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a NamedObjectId from a parameter.", "response": "def _build_named_object_id(parameter):\n    \"\"\"\n    Builds a NamedObjectId. This is a bit more complex than it really\n    should be. In Python (for convenience) we allow the user to simply address\n    entries by their alias via the NAMESPACE/NAME convention. Yamcs is not\n    aware of this convention so we decompose it into distinct namespace and\n    name fields.\n    \"\"\"\n    named_object_id = yamcs_pb2.NamedObjectId()\n    if parameter.startswith('/'):\n        named_object_id.name = parameter\n    else:\n        parts = parameter.split('/', 1)\n        if len(parts) < 2:\n            raise ValueError('Failed to process {}. Use fully-qualified '\n                             'XTCE names or, alternatively, an alias in '\n                             'in the format NAMESPACE/NAME'.format(parameter))\n        named_object_id.namespace = parts[0]\n        named_object_id.name = parts[1]\n    return named_object_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a list of NamedObjectId.", "response": "def _build_named_object_ids(parameters):\n    \"\"\"Builds a list of NamedObjectId.\"\"\"\n    if isinstance(parameters, str):\n        return [_build_named_object_id(parameters)]\n    return [_build_named_object_id(parameter) for parameter in parameters]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a list of CommandId.", "response": "def _build_command_ids(issued_commands):\n    \"\"\"Builds a list of CommandId.\"\"\"\n    if isinstance(issued_commands, IssuedCommand):\n        entry = issued_commands._proto.commandQueueEntry\n        return [entry.cmdId]\n    else:\n        return [issued_command._proto.commandQueueEntry.cmdId\n                for issued_command in issued_commands]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a unique cache key for the given commandId.", "response": "def _cache_key(cmd_id):\n        \"\"\"commandId is a tuple. Make a 'unique' key for it.\"\"\"\n        return '{}__{}__{}__{}'.format(\n            cmd_id.generationTime, cmd_id.origin, cmd_id.sequenceNumber,\n            cmd_id.commandName)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_command_history(self, issued_command):\n        #pylint: disable=protected-access\n        entry = issued_command._proto.commandQueueEntry\n        key = self._cache_key(entry.cmdId)\n        if key in self._cache:\n            return self._cache[key]\n        return None", "response": "Gets locally cached CommandHistory for the specified command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd one or more parameters to this subscription.", "response": "def add(self,\n            parameters,\n            abort_on_invalid=True,\n            send_from_cache=True):\n        \"\"\"\n        Add one or more parameters to this subscription.\n\n        :param parameters: Parameter(s) to be added\n        :type parameters: Union[str, str[]]\n        :param bool abort_on_invalid: If ``True`` one invalid parameter\n                                      means any other parameter in the\n                                      request will also not be added\n                                      to the subscription.\n        :param bool send_from_cache: If ``True`` the last processed parameter\n                                     value is sent from parameter cache.\n                                     When ``False`` only newly processed\n                                     parameters are received.\n        \"\"\"\n\n        # Verify that we already know our assigned subscription_id\n        assert self.subscription_id != -1\n\n        if not parameters:\n            return\n\n        options = web_pb2.ParameterSubscriptionRequest()\n        options.subscriptionId = self.subscription_id\n        options.abortOnInvalid = abort_on_invalid\n        options.sendFromCache = send_from_cache\n        options.id.extend(_build_named_object_ids(parameters))\n\n        self._manager.send('subscribe', options)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, parameters):\n\n        # Verify that we already know our assigned subscription_id\n        assert self.subscription_id != -1\n\n        if not parameters:\n            return\n\n        options = web_pb2.ParameterSubscriptionRequest()\n        options.subscriptionId = self.subscription_id\n        options.id.extend(_build_named_object_ids(parameters))\n\n        self._manager.send('unsubscribe', options)", "response": "Removes one or more parameters from this subscription."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the current value of the specified parameter.", "response": "def get_parameter_value(self, parameter, from_cache=True, timeout=10):\n        \"\"\"\n        Retrieve the current value of the specified parameter.\n\n        :param str parameter: Either a fully-qualified XTCE name or an alias in the\n                              format ``NAMESPACE/NAME``.\n        :param bool from_cache: If ``False`` this call will block until a\n                                fresh value is received on the processor.\n                                If ``True`` the server returns the latest\n                                value instead (which may be ``None``).\n        :param float timeout: The amount of seconds to wait for a fresh value.\n                              (ignored if ``from_cache=True``).\n        :rtype: .ParameterValue\n        \"\"\"\n        params = {\n            'fromCache': from_cache,\n            'timeout': int(timeout * 1000),\n        }\n        parameter = adapt_name_for_rest(parameter)\n        url = '/processors/{}/{}/parameters{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.get_proto(url, params=params)\n        proto = pvalue_pb2.ParameterValue()\n        proto.ParseFromString(response.content)\n\n        # Server returns ParameterValue with only 'id' set if no\n        # value existed. Convert this to ``None``.\n        if proto.HasField('rawValue') or proto.HasField('engValue'):\n            return ParameterValue(proto)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_parameter_values(self, parameters, from_cache=True, timeout=10):\n        params = {\n            'fromCache': from_cache,\n            'timeout': int(timeout * 1000),\n        }\n        req = rest_pb2.BulkGetParameterValueRequest()\n        req.id.extend(_build_named_object_ids(parameters))\n        url = '/processors/{}/{}/parameters/mget'.format(\n            self._instance, self._processor)\n        response = self._client.post_proto(url, params=params,\n                                           data=req.SerializeToString())\n        proto = rest_pb2.BulkGetParameterValueResponse()\n        proto.ParseFromString(response.content)\n\n        pvals = []\n        for parameter_id in req.id:\n            match = None\n            for pval in proto.value:\n                if pval.id == parameter_id:\n                    match = pval\n                    break\n            pvals.append(ParameterValue(match) if match else None)\n        return pvals", "response": "Retrieves the current value of the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_parameter_value(self, parameter, value):\n        parameter = adapt_name_for_rest(parameter)\n        url = '/processors/{}/{}/parameters{}'.format(\n            self._instance, self._processor, parameter)\n        req = _build_value_proto(value)\n        self._client.put_proto(url, data=req.SerializeToString())", "response": "Sets the value of the specified parameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the value of multiple QL parameters.", "response": "def set_parameter_values(self, values):\n        \"\"\"\n        Sets the value of multiple  parameters.\n\n        :param dict values: Values keyed by parameter name. This name can be either\n                            a fully-qualified XTCE name or an alias in the format\n                            ``NAMESPACE/NAME``.\n        \"\"\"\n        req = rest_pb2.BulkSetParameterValueRequest()\n        for key in values:\n            item = req.request.add()\n            item.id.MergeFrom(_build_named_object_id(key))\n            item.value.MergeFrom(_build_value_proto(values[key]))\n        url = '/processors/{}/{}/parameters/mset'.format(\n            self._instance, self._processor)\n        self._client.post_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef issue_command(self, command, args=None, dry_run=False, comment=None):\n        req = rest_pb2.IssueCommandRequest()\n        req.sequenceNumber = SequenceGenerator.next()\n        req.origin = socket.gethostname()\n        req.dryRun = dry_run\n        if comment:\n            req.comment = comment\n        if args:\n            for key in args:\n                assignment = req.assignment.add()\n                assignment.name = key\n                assignment.value = str(args[key])\n\n        command = adapt_name_for_rest(command)\n        url = '/processors/{}/{}/commands{}'.format(\n            self._instance, self._processor, command)\n        response = self._client.post_proto(url, data=req.SerializeToString())\n        proto = rest_pb2.IssueCommandResponse()\n        proto.ParseFromString(response.content)\n        return IssuedCommand(proto, self)", "response": "Issue a command to the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_alarms(self, start=None, stop=None):\n        # TODO implement continuation token on server\n        params = {\n            'order': 'asc'\n        }\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        url = '/processors/{}/{}/alarms'.format(self._instance, self._processor)\n        response = self._client.get_proto(path=url, params=params)\n        message = rest_pb2.ListAlarmsResponse()\n        message.ParseFromString(response.content)\n        alarms = getattr(message, 'alarm')\n        return iter([Alarm(alarm) for alarm in alarms])", "response": "Lists the active alarms on the current processor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_default_calibrator(self, parameter, type, data):  # pylint: disable=W0622\n        req = mdb_pb2.ChangeParameterRequest()\n        req.action = mdb_pb2.ChangeParameterRequest.SET_DEFAULT_CALIBRATOR\n        if  type:\n            _add_calib(req.defaultCalibrator, type, data)\n                   \n        url = '/mdb/{}/{}/parameters/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())", "response": "Applies a calibrator while processing raw values of the specified\n            parameter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_calibrators(self, parameter, calibrators):\n        req = mdb_pb2.ChangeParameterRequest()\n        req.action = mdb_pb2.ChangeParameterRequest.SET_CALIBRATORS\n        for c in calibrators:\n            if c.context :\n                context_calib = req.contextCalibrator.add()\n                context_calib.context = rs.context\n                calib_info = context_calib.calibrator\n            else :                \n                calib_info = req.defaultCalibrator\n            \n            _add_calib(calib_info, c.type, c.data)\n            \n            \n        url = '/mdb/{}/{}/parameters/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())\n        pti = mdb_pb2.ParameterTypeInfo()", "response": "Applies an ordered set of calibrators for the specified parameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset all calibrators for the specified parameter to their original MDB value.", "response": "def reset_calibrators(self, parameter):\n        \"\"\"\n        Reset all calibrators for the specified parameter to their original MDB value.\n        \"\"\"\n        req = mdb_pb2.ChangeParameterRequest()\n        req.action = mdb_pb2.ChangeParameterRequest.RESET_CALIBRATORS                  \n        calib_info = req.defaultCalibrator\n        url = '/mdb/{}/{}/parameters/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_default_alarm_ranges(self, parameter, watch=None, warning=None,\n                                 distress=None, critical=None, severe=None,\n                                 min_violations=1):\n        \"\"\"\n        Generate out-of-limit alarms for a parameter using the specified\n        alarm ranges.\n\n        This replaces any previous default alarms on this parameter.\n\n        .. note::\n\n            Contextual range sets take precedence over the default alarm\n            ranges. See :meth:`set_alarm_range_sets` for setting contextual\n            range sets.\n\n        :param str parameter: Either a fully-qualified XTCE name or an alias\n                              in the format ``NAMESPACE/NAME``.\n        :param (float,float) watch: Range expressed as a tuple ``(lo, hi)``\n                                where lo and hi are assumed exclusive.\n        :param (float,float) warning: Range expressed as a tuple ``(lo, hi)``\n                                  where lo and hi are assumed exclusive.\n        :param (float,float) distress: Range expressed as a tuple ``(lo, hi)``\n                                   where lo and hi are assumed exclusive.\n        :param (float,float) critical: Range expressed as a tuple ``(lo, hi)``\n                                   where lo and hi are assumed exclusive.\n        :param (float,float) severe: Range expressed as a tuple ``(lo, hi)``\n                                 where lo and hi are assumed exclusive.\n        :param int min_violations: Minimum violations before an alarm is\n                                   generated.\n        \"\"\"\n        req = mdb_pb2.ChangeParameterRequest()\n        req.action = mdb_pb2.ChangeParameterRequest.SET_DEFAULT_ALARMS\n        if(watch or warning or distress or critical or severe):\n            _add_alarms(req.defaultAlarm, watch, warning, distress, critical, severe, min_violations)\n        \n        url = '/mdb/{}/{}/parameters/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())", "response": "Generate out - of - limit alarms for a parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply an ordered list of alarm range sets for the specified parameter.", "response": "def set_alarm_range_sets(self, parameter, sets):\n        \"\"\"\n        Apply an ordered list of alarm range sets for the specified parameter.\n        This replaces existing alarm sets (if any).\n\n        Each RangeSet may have a context, which indicates when\n        its effects may be applied. Only the first matching set is\n        applied.\n\n        A RangeSet with context ``None`` represents the *default* set of\n        alarm ranges.  There can be only one such set, and it is always\n        applied at the end when no other set of contextual ranges is\n        applicable.\n\n        :param str parameter: Either a fully-qualified XTCE name or an alias\n                              in the format ``NAMESPACE/NAME``.\n        :param .RangeSet[] sets: List of range sets (either contextual or not)\n        \"\"\"\n        req = mdb_pb2.ChangeParameterRequest()\n        req.action = mdb_pb2.ChangeParameterRequest.SET_ALARMS\n        for rs in sets:\n            if rs.context :\n                context_alarm = req.contextAlarm.add()\n                context_alarm.context = rs.context\n                alarm_info = context_alarm.alarm\n            else :                \n                alarm_info = req.defaultAlarm\n                \n            _add_alarms(alarm_info, rs.watch, rs.warning, rs.distress, rs.critical, rs.severe, rs.min_violations)\n            \n        url = '/mdb/{}/{}/parameters/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())\n        pti = mdb_pb2.ParameterTypeInfo()\n        pti.ParseFromString(response.content)\n        print(pti)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting all alarm limits for the specified parameter to their original MDB value.", "response": "def reset_alarm_ranges(self, parameter):\n        \"\"\"\n        Reset all alarm limits for the specified parameter to their original MDB value.\n        \"\"\"\n        req = mdb_pb2.ChangeParameterRequest()\n        req.action = mdb_pb2.ChangeParameterRequest.RESET_ALARMS        \n        \n        url = '/mdb/{}/{}/parameters/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nacknowledge a specific alarm associated with a parameter.", "response": "def acknowledge_alarm(self, alarm, comment=None):\n        \"\"\"\n        Acknowledges a specific alarm associated with a parameter.\n\n        :param alarm: Alarm instance\n        :type alarm: :class:`.Alarm`\n        :param str comment: Optional comment to associate with the state\n                            change.\n        \"\"\"\n        url = '/processors/{}/{}/parameters{}/alarms/{}'.format(\n            self._instance, self._processor, alarm.name, alarm.sequence_number)\n        req = rest_pb2.EditAlarmRequest()\n        req.state = 'acknowledged'\n        if comment is not None:\n            req.comment = comment\n        self._client.put_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_command_history_subscription(self,\n                                            issued_command=None,\n                                            on_data=None,\n                                            timeout=60):\n        \"\"\"\n        Create a new command history subscription.\n\n        :param .IssuedCommand[] issued_command: (Optional) Previously issued\n                                                commands. If not provided updates\n                                                from any command are received.\n        :param on_data: Function that gets called with  :class:`.CommandHistory`\n                        updates.\n        :param float timeout: The amount of seconds to wait for the request\n                              to complete.\n        :return: Future that can be used to manage the background websocket\n                 subscription\n        :rtype: .CommandHistorySubscription\n        \"\"\"\n        options = web_pb2.CommandHistorySubscriptionRequest()\n        options.ignorePastCommands = True\n        if issued_command:\n            options.commandId.extend(_build_command_ids(issued_command))\n\n        manager = WebSocketSubscriptionManager(\n            self._client, resource='cmdhistory', options=options)\n\n        # Represent subscription as a future\n        subscription = CommandHistorySubscription(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_cmdhist_data, subscription, on_data)\n\n        manager.open(wrapped_callback, instance=self._instance,\n                     processor=self._processor)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription", "response": "Create a new command history subscription."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_parameter_subscription(self,\n                                      parameters,\n                                      on_data=None,\n                                      abort_on_invalid=True,\n                                      update_on_expiration=False,\n                                      send_from_cache=True,\n                                      timeout=60):\n        \"\"\"\n        Create a new parameter subscription.\n\n        :param str[] parameters: Parameter names (or aliases).\n        :param on_data: Function that gets called with  :class:`.ParameterData`\n                        updates.\n        :param bool abort_on_invalid: If ``True`` an error is generated when\n                                      invalid parameters are specified.\n        :param bool update_on_expiration: If ``True`` an update is received\n                                          when a parameter value has become\n                                          expired. This update holds the\n                                          same value as the last known valid\n                                          value, but with status set to\n                                          ``EXPIRED``.\n        :param bool send_from_cache: If ``True`` the last processed parameter\n                                     value is sent from parameter cache.\n                                     When ``False`` only newly processed\n                                     parameters are received.\n        :param float timeout: The amount of seconds to wait for the request\n                              to complete.\n\n        :return: A Future that can be used to manage the background websocket\n                 subscription.\n        :rtype: .ParameterSubscription\n        \"\"\"\n        options = web_pb2.ParameterSubscriptionRequest()\n        options.subscriptionId = -1  # This means 'create a new subscription'\n        options.abortOnInvalid = abort_on_invalid\n        options.updateOnExpiration = update_on_expiration\n        options.sendFromCache = send_from_cache\n        options.id.extend(_build_named_object_ids(parameters))\n\n        manager = WebSocketSubscriptionManager(\n            self._client, resource='parameter', options=options)\n\n        # Represent subscription as a future\n        subscription = ParameterSubscription(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_parameter_data, subscription, on_data)\n\n        manager.open(wrapped_callback, instance=self._instance,\n                     processor=self._processor)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription", "response": "Create a new parameter subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_alarm_subscription(self,\n                                  on_data=None,\n                                  timeout=60):\n        \"\"\"\n        Create a new alarm subscription.\n\n        :param on_data: Function that gets called with  :class:`.AlarmEvent`\n                        updates.\n        :param float timeout: The amount of seconds to wait for the request\n                              to complete.\n\n        :return: A Future that can be used to manage the background websocket\n                 subscription.\n        :rtype: .AlarmSubscription\n        \"\"\"\n        manager = WebSocketSubscriptionManager(self._client, resource='alarms')\n\n        # Represent subscription as a future\n        subscription = AlarmSubscription(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_alarm_data, subscription, on_data)\n\n        manager.open(wrapped_callback, instance=self._instance,\n                     processor=self._processor)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription", "response": "Create a new alarm subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchanges an algorithm text. Can only be peformed on JavaScript or Python algorithms.", "response": "def set_algorithm(self, parameter, text):\n        \"\"\"\n        Change an algorithm text. Can only be peformed on JavaScript or Python algorithms.\n        \n        :param string text: new algorithm text (as it would appear in excel or XTCE)\n        \n        :param str parameter: Either a fully-qualified XTCE name or an alias\n                              in the format ``NAMESPACE/NAME``.\n        \"\"\"\n        req = mdb_pb2.ChangeAlgorithmRequest()\n        \n        req.action = mdb_pb2.ChangeAlgorithmRequest.SET\n        req.algorithm.text = text\n        \n        url = '/mdb/{}/{}/algorithms/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nresets the original algorithm text to its original definition from MDB eventData", "response": "def reset_algorithm(self, parameter):\n        \"\"\"\n        Reset the algorithm text to its original definition from MDB\n        \n        :param str parameter: Either a fully-qualified XTCE name or an alias\n                              in the format ``NAMESPACE/NAME``.\n        \"\"\"\n        req = mdb_pb2.ChangeAlgorithmRequest()\n        \n        req.action = mdb_pb2.ChangeAlgorithmRequest.RESET        \n        \n        url = '/mdb/{}/{}/algorithms/{}'.format(\n            self._instance, self._processor, parameter)\n        response = self._client.post_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_by(self, name):\n    return next((item for item in self if item.name == name), None)", "response": "get element by name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the FASTQ files for the current assembly of the current sample.", "response": "def validate_fastq(self):\n        \"\"\"\n        Runs reformat.sh on the FASTQ files. If a CalledProcessError arises, do not proceed with the assembly of\n        these files\n        \"\"\"\n        logging.info('Validating FASTQ files')\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # Tiny files can pass the validation tests - ensure that they don't\n                size = os.path.getsize(sample.general.fastqfiles[0])\n                if size >= 1000000:\n                    # Try to run reformat.sh on the reads - on any errors try to run repair.sh\n                    try:\n                        out, err, cmd = bbtools.validate_reads(forward_in=sample.general.fastqfiles[0],\n                                                               returncmd=True)\n                        write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr,\n                                         None, None)\n                    except CalledProcessError:\n                        # Set the file names for the reformatted and repaired files\n                        outputfile1 = os.path.join(sample.general.outputdirectory, '{}_reformatted_R1.fastq.gz'\n                                                   .format(sample.name))\n                        repair_file1 = os.path.join(sample.general.outputdirectory, '{}_repaired_R1.fastq.gz'\n                                                    .format(sample.name))\n                        if len(sample.general.fastqfiles) == 2:\n                            outputfile2 = os.path.join(sample.general.outputdirectory, '{}_reformatted_R2.fastq.gz'\n                                                       .format(sample.name))\n                            repair_file2 = os.path.join(sample.general.outputdirectory, '{}_repaired_R2.fastq.gz'\n                                                        .format(sample.name))\n                        else:\n                            outputfile2 = str()\n                            repair_file2 = str()\n                        # Use reformat.sh to repair the reads. If this fails, discard the sample from the analyses\n                        try:\n                            logging.warning('Errors detected in FASTQ files for sample {sample}. '\n                                            'Please check the following files for details {log} {logout} {logerr}. '\n                                            'The pipeline will use reformat.sh to attempt to repair issues'\n                                            .format(sample=sample.name,\n                                                    log=self.logfile,\n                                                    logout=sample.general.logout,\n                                                    logerr=sample.general.logerr))\n                            if not os.path.isfile(outputfile1):\n                                # Run reformat.sh\n                                out, err, cmd = bbtools.reformat_reads(forward_in=sample.general.fastqfiles[0],\n                                                                       forward_out=outputfile1,\n                                                                       returncmd=True)\n                                write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr,\n                                                 None, None)\n                                # Run repair.sh (if necessary)\n                                if outputfile2:\n                                    out, err, cmd = bbtools.repair_reads(forward_in=outputfile1,\n                                                                         forward_out=repair_file1,\n                                                                         returncmd=True)\n                                    write_to_logfile(out, err, self.logfile, sample.general.logout,\n                                                     sample.general.logerr, None, None)\n                            # Ensure that the output file(s) exist before declaring this a success\n                            if os.path.isfile(outputfile1):\n                                # Update the fastqfiles attribute to point to the repaired files\n                                sample.general.fastqfiles = [repair_file1, repair_file2] if repair_file2 \\\n                                    else [outputfile1]\n                        except CalledProcessError:\n                            # The file(s) can be created even if there is STDERR from reformat.sh\n                            if os.path.isfile(outputfile1) and outputfile2:\n                                try:\n                                    out, err, cmd = bbtools.repair_reads(forward_in=outputfile1,\n                                                                         forward_out=repair_file1,\n                                                                         returncmd=True)\n                                    write_to_logfile(out, err, self.logfile, sample.general.logout,\n                                                     sample.general.logerr, None, None)\n                                    # Update the fastqfiles attribute to point to the repaired files\n                                    sample.general.fastqfiles = [repair_file1, repair_file2] if repair_file2 else \\\n                                        [repair_file1]\n                                except CalledProcessError:\n                                    # Write in the logs that there was an error detected in the FASTQ files\n                                    write_to_logfile('An error was detected in the FASTQ files for sample {}. '\n                                                     'These files will not be processed further'.format(sample.name),\n                                                     'An error was detected in the FASTQ files for sample {}. '\n                                                     'These files will not be processed further'.format(sample.name),\n                                                     self.logfile,\n                                                     sample.general.logout,\n                                                     sample.general.logerr,\n                                                     None,\n                                                     None)\n                                    # Update metadata objects with error\n                                    self.error(sample, 'fastq_error')\n                            else:\n                                # Write in the logs that there was an error detected in the FASTQ files\n                                write_to_logfile('An error was detected in the FASTQ files for sample {}. '\n                                                 'These files will not be processed further'.format(sample.name),\n                                                 'An error was detected in the FASTQ files for sample {}. '\n                                                 'These files will not be processed further'.format(sample.name),\n                                                 self.logfile,\n                                                 sample.general.logout,\n                                                 sample.general.logerr,\n                                                 None,\n                                                 None)\n\n                                # Update metadata objects with error\n                                self.error(sample, 'fastq_error')\n                else:\n                    # Update metadata objects with error\n                    self.error(sample, 'files_too_small')\n        # Print the metadata to file\n        metadataprinter.MetadataPrinter(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef error(sample, message):\n        # Set the .fastqfiles attribute to an empty list\n        sample.general.fastqfiles = list()\n        # Ensure that the run attribute exists\n        if GenObject.isattr(sample, 'run'):\n            # If the Description attribute exists, overwrite it, otherwise create and populate it\n            if GenObject.isattr(sample.run, 'status'):\n                sample.run.status = message\n            else:\n                setattr(sample.run, 'status', message)\n        # Otherwise create and populate the attribute\n        else:\n            setattr(sample, 'run', GenObject())\n            sample.run.Description = message", "response": "Add an error message to the sample. run. Description attribute."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fastqc(self):\n        while True:  # while daemon\n            threadlock = threading.Lock()\n            # Unpack the variables from the queue\n            (sample, systemcall, outputdir, fastqcreads) = self.qcqueue.get()\n            # Check to see if the output HTML file already exists\n            try:\n                _ = glob(os.path.join(outputdir, '*.html'))[0]\n            except IndexError:\n                # Make the output directory\n                make_path(outputdir)\n                # Run the system calls\n                outstr = str()\n                errstr = str()\n                out, err = run_subprocess(systemcall)\n                outstr += out\n                errstr += err\n                out, err = run_subprocess(fastqcreads)\n                outstr += out\n                errstr += err\n                # Acquire thread lock, and write the logs to file\n                threadlock.acquire()\n                write_to_logfile(systemcall, systemcall, self.logfile, sample.general.logout, sample.general.logerr,\n                                 None, None)\n                write_to_logfile(fastqcreads, fastqcreads, self.logfile, sample.general.logout, sample.general.logerr,\n                                 None, None)\n                write_to_logfile(outstr, errstr, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n                threadlock.release()\n                # Rename the outputs\n                try:\n                    shutil.move(os.path.join(outputdir, 'stdin_fastqc.html'),\n                                os.path.join(outputdir, '{}_fastqc.html'.format(sample.name)))\n                    shutil.move(os.path.join(outputdir, 'stdin_fastqc.zip'),\n                                os.path.join(outputdir, '{}_fastqc.zip'.format(sample.name)))\n                except IOError:\n                    pass\n            # Signal to qcqueue that job is done\n            self.qcqueue.task_done()", "response": "Run the fastqc system calls and rename the outputs to the output directory"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing bbduk to trim the quality scores of the fastq files in the metadata file", "response": "def trimquality(self):\n        \"\"\"Uses bbduk from the bbmap tool suite to quality and adapter trim\"\"\"\n        logging.info(\"Trimming fastq files\")\n        # Iterate through strains with fastq files\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # As the metadata can be populated with 'NA' (string) if there are no fastq files, only process if\n                # :fastqfiles is a list\n                if type(sample.general.fastqfiles) is list:\n                    # Check to see if the fastq files exist\n                    fastqfiles = sorted(sample.general.fastqfiles)\n                    # Define the output directory\n                    outputdir = sample.general.outputdirectory\n                    # Define the name of the trimmed fastq files\n                    cleanforward = os.path.join(outputdir, '{}_R1_trimmed.fastq.gz'.format(sample.name))\n                    cleanreverse = os.path.join(outputdir, '{}_R2_trimmed.fastq.gz'.format(sample.name))\n                    # Incorporate read length into the minlength parameter - set it to 50 unless one or more of the\n                    # reads has a lower calculated length than 50\n                    try:\n                        lesser_length = min(int(sample.run.forwardlength), int(sample.run.reverselength))\n                    except ValueError:\n                        lesser_length = int(sample.run.forwardlength)\n                    min_len = 50 if lesser_length >= 50 else lesser_length\n                    # Initialise a variable to store the number of bases to automatically trim from the beginning of\n                    # each read, as these bases tend to have lower quality scores. If trimming the reads will cause\n                    trim_left = 0\n                    # If, for some reason, only the reverse reads are present, use the appropriate output file name\n                    try:\n                        if 'R2' in fastqfiles[0]:\n                            if not os.path.isfile(cleanreverse):\n                                out, \\\n                                    err, \\\n                                    bbdukcall = bbtools.bbduk_trim(forward_in=fastqfiles[0],\n                                                                   reverse_in=None,\n                                                                   forward_out=cleanreverse,\n                                                                   trimq=10,\n                                                                   minlength=min_len,\n                                                                   forcetrimleft=trim_left,\n                                                                   returncmd=True)\n                            else:\n                                bbdukcall = str()\n                                out = str()\n                                err = str()\n                        else:\n                            if not os.path.isfile(cleanforward):\n                                    out, \\\n                                        err, \\\n                                        bbdukcall = bbtools.bbduk_trim(forward_in=fastqfiles[0],\n                                                                       forward_out=cleanforward,\n                                                                       trimq=10,\n                                                                       minlength=min_len,\n                                                                       forcetrimleft=trim_left,\n                                                                       returncmd=True)\n                            else:\n                                bbdukcall = str()\n                                out = str()\n                                err = str()\n                    except (IndexError, CalledProcessError):\n                        bbdukcall = str()\n                        out = str()\n                        err = str()\n                    # Write the command, stdout, and stderr to the logfile\n                    write_to_logfile(bbdukcall, bbdukcall, self.logfile, sample.general.logout, sample.general.logerr,\n                                     None, None)\n                    write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n                    # Add the trimmed fastq files to a list\n                    trimmedfastqfiles = sorted(glob(os.path.join(sample.general.outputdirectory, '*trimmed.fastq.gz')))\n                    # Populate the metadata if the files exist\n                    sample.general.trimmedfastqfiles = trimmedfastqfiles if trimmedfastqfiles else list()\n        # Add all the trimmed files to the metadata\n        logging.info('Fastq files trimmed')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contamination_finder(self, input_path=None, report_path=None):\n        logging.info('Calculating contamination in reads')\n        if input_path is not None:\n            input_dir = input_path\n        else:\n            input_dir = self.path\n        if report_path is not None:\n            reportpath = report_path\n        else:\n            reportpath = os.path.join(input_dir, 'confindr')\n        confindr_report = os.path.join(input_dir, 'confindr', 'confindr_report.csv')\n        pipeline_report = os.path.join(reportpath, 'confindr_report.csv')\n        # Only proceed if the confindr report doesn't exist\n        if not os.path.isfile(confindr_report):\n            # # Create an object to store attributes to pass to confinder\n            # Clear and recreate the output folder\n            try:\n                shutil.rmtree(reportpath)\n            except IOError:\n                pass\n            make_path(reportpath)\n            # Run confindr\n            systemcall = 'confindr.py -i {input_dir} -o {output_dir} -d {database_dir} -bf 0.05'\\\n                .format(input_dir=input_dir,\n                        output_dir=os.path.join(input_dir, 'confindr'),\n                        database_dir=os.path.join(self.reffilepath, 'ConFindr', 'databases'))\n            # Run the call\n            out, err = run_subprocess(systemcall)\n            write_to_logfile(systemcall, systemcall, self.logfile, None, None, None, None)\n            write_to_logfile(out, err, self.logfile, None, None, None, None)\n            logging.info('Contamination detection complete!')\n        # Load the confindr report into a dictionary using pandas\n        # https://stackoverflow.com/questions/33620982/reading-csv-file-as-dictionary-using-pandas\n        confindr_results = pandas.read_csv(confindr_report, index_col=0).T.to_dict()\n        # Find the results for each of the samples\n        for sample in self.metadata:\n            # Create a GenObject to store the results\n            sample.confindr = GenObject()\n            # Iterate through the dictionary to find the outputs for each sample\n            for line in confindr_results:\n                # If the current line corresponds to the sample of interest\n                if sample.name in line:\n                    # Set the values using the appropriate keys as the attributes\n                    sample.confindr.genus = confindr_results[line]['Genus'] if type(confindr_results[line]['Genus']) \\\n                                                                               is not float else 'ND'\n                    sample.confindr.num_contaminated_snvs = confindr_results[line]['NumContamSNVs']\n                    sample.confindr.contam_status = confindr_results[line]['ContamStatus']\n                    # Don't break parsing previous ConFindr reports that lack the percent contamination calculations\n                    try:\n                        sample.confindr.percent_contam = confindr_results[line]['PercentContam'] if \\\n                            str(confindr_results[line]['PercentContam']) != 'nan' else 0\n                    except KeyError:\n                        sample.confindr.percent_contam = 'ND'\n                    try:\n                        sample.confindr.percent_contam_std = \\\n                            confindr_results[line]['PercentContamStandardDeviation'] if \\\n                            str(confindr_results[line]['PercentContamStandardDeviation']) != 'nan' else 0\n                    except KeyError:\n                        sample.confindr.percent_contam_std = 'ND'\n                    if sample.confindr.contam_status is True:\n                        sample.confindr.contam_status = 'Contaminated'\n                    elif sample.confindr.contam_status is False:\n                        sample.confindr.contam_status = 'Clean'\n        # Re-write the output to be consistent with the rest of the pipeline\n        with open(pipeline_report, 'w') as csv:\n            data = 'Strain,Genus,NumContamSNVs,ContamStatus,PercentContam,PercentContamSTD\\n'\n            for sample in self.metadata:\n                data += '{str},{genus},{numcontamsnv},{status},{pc},{pcs}\\n'.format(\n                    str=sample.name,\n                    genus=sample.confindr.genus,\n                    numcontamsnv=sample.confindr.num_contaminated_snvs,\n                    status=sample.confindr.contam_status,\n                    pc=sample.confindr.percent_contam,\n                    pcs=sample.confindr.percent_contam_std\n                )\n            csv.write(data)", "response": "This function is used to get the contamination in reads from the confindr pipeline and run the confindr. py to find the contamination in reads."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse kmercountexact to estimate the genome size of the object", "response": "def estimate_genome_size(self):\n        \"\"\"\n        Use kmercountexact from the bbmap suite of tools to estimate the size of the genome\n        \"\"\"\n        logging.info('Estimating genome size using kmercountexact')\n        for sample in self.metadata:\n            # Initialise the name of the output file\n            sample[self.analysistype].peaksfile = os.path.join(sample[self.analysistype].outputdir, 'peaks.txt')\n            # Run the kmer counting command\n            out, err, cmd = bbtools.kmercountexact(forward_in=sorted(sample.general.fastqfiles)[0],\n                                                   peaks=sample[self.analysistype].peaksfile,\n                                                   returncmd=True,\n                                                   threads=self.cpus)\n            # Set the command in the object\n            sample[self.analysistype].kmercountexactcmd = cmd\n            # Extract the genome size from the peaks file\n            sample[self.analysistype].genomesize = bbtools.genome_size(sample[self.analysistype].peaksfile)\n            write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms error correction of the reads in the object.", "response": "def error_correction(self):\n        \"\"\"\n        Use tadpole from the bbmap suite of tools to perform error correction of the reads\n        \"\"\"\n        logging.info('Error correcting reads')\n        for sample in self.metadata:\n            sample.general.trimmedcorrectedfastqfiles = [fastq.split('.fastq.gz')[0] + '_trimmed_corrected.fastq.gz'\n                                                         for fastq in sorted(sample.general.fastqfiles)]\n            try:\n                if not os.path.isfile(sample.general.trimmedcorrectedfastqfiles[0]):\n                    try:\n                        out, err, cmd = bbtools.tadpole(forward_in=sorted(sample.general.trimmedfastqfiles)[0],\n                                                        forward_out=sample.general.trimmedcorrectedfastqfiles[0],\n                                                        returncmd=True,\n                                                        mode='correct',\n                                                        threads=self.cpus)\n                        # Set the command in the object\n                        sample[self.analysistype].errorcorrectcmd = cmd\n                        write_to_logfile(out=out,\n                                         err=err,\n                                         logfile=self.logfile,\n                                         samplelog=sample.general.logout,\n                                         sampleerr=sample.general.logerr,\n                                         analysislog=None,\n                                         analysiserr=None)\n                    except IndexError:\n                        sample.general.trimmedcorrectedfastqfiles = list()\n            except CalledProcessError:\n                sample.general.trimmedcorrectedfastqfiles = sample.general.trimmedfastqfiles\n            except AttributeError:\n                sample.general.trimmedcorrectedfastqfiles = list()\n            except IndexError:\n                sample.general.trimmedcorrectedfastqfiles = list()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalise_reads(self):\n        logging.info('Normalising reads to a kmer depth of 100')\n        for sample in self.metadata:\n            # Set the name of the normalised read files\n            sample.general.normalisedreads = [fastq.split('.fastq.gz')[0] + '_normalised.fastq.gz'\n                                              for fastq in sorted(sample.general.fastqfiles)]\n            try:\n                # Run the normalisation command\n                out, err, cmd = bbtools.bbnorm(forward_in=sorted(sample.general.trimmedcorrectedfastqfiles)[0],\n                                               forward_out=sample.general.normalisedreads[0],\n                                               returncmd=True,\n                                               threads=self.cpus)\n                sample[self.analysistype].normalisecmd = cmd\n                write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n            except CalledProcessError:\n                sample.general.normalisedreads = sample.general.trimmedfastqfiles\n            except IndexError:\n                sample.general.normalisedreads = list()", "response": "Normalise the reads in the metadata files to a kmer depth of 100"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_pairs(self):\n        logging.info('Merging paired reads')\n        for sample in self.metadata:\n            # Can only merge paired-end\n            if len(sample.general.fastqfiles) == 2:\n                # Set the name of the merged, and unmerged files\n                sample.general.mergedreads = \\\n                    os.path.join(sample.general.outputdirectory, '{}_paired.fastq.gz'.format(sample.name))\n                sample.general.unmergedforward = \\\n                    os.path.join(sample.general.outputdirectory, '{}_unpaired_R1.fastq.gz'.format(sample.name))\n                sample.general.unmergedreverse = \\\n                    os.path.join(sample.general.outputdirectory, '{}_unpaired_R2.fastq.gz'.format(sample.name))\n                try:\n                    # Run the merging command - forward_in=sample.general.normalisedreads[0],\n                    out, err, cmd = bbtools.bbmerge(forward_in=sorted(sample.general.trimmedcorrectedfastqfiles)[0],\n                                                    merged_reads=sample.general.mergedreads,\n                                                    returncmd=True,\n                                                    outu1=sample.general.unmergedforward,\n                                                    outu2=sample.general.unmergedreverse,\n                                                    threads=self.cpus)\n                    sample[self.analysistype].bbmergecmd = cmd\n                    write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n                except CalledProcessError:\n                    delattr(sample.general, 'mergedreads')\n                    delattr(sample.general, 'unmergedforward')\n                    delattr(sample.general, 'unmergedreverse')\n                except IndexError:\n                    delattr(sample.general, 'mergedreads')\n                    delattr(sample.general, 'unmergedforward')\n                    delattr(sample.general, 'unmergedreverse')\n            else:\n                sample.general.mergedreads = sorted(sample.general.trimmedcorrectedfastqfiles)[0]", "response": "Merge paired - end reads into one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(self):\n        self.fasta_records()\n        self.fasta_stats()\n        self.find_largest_contig()\n        self.find_genome_length()\n        self.find_num_contigs()\n        self.find_n50()\n        self.perform_pilon()\n        self.clear_attributes()", "response": "Run all the methods required for pipeline outputs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a dictionary of all records for each object in the metadata", "response": "def fasta_records(self):\n        \"\"\"\n        Use SeqIO to create dictionaries of all records for each FASTA file\n        \"\"\"\n        for sample in self.metadata:\n            # Create the analysis-type specific attribute\n            setattr(sample, self.analysistype, GenObject())\n            # Create a dictionary of records for each file\n            try:\n                record_dict = SeqIO.to_dict(SeqIO.parse(sample.general.bestassemblyfile, \"fasta\"))\n            except FileNotFoundError:\n                record_dict = dict()\n            # Set the records dictionary as the attribute for the object\n            sample[self.analysistype].record_dict = record_dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fasta_stats(self):\n        for sample in self.metadata:\n            # Initialise variables to store appropriate values parsed from contig records\n            contig_lengths = list()\n            fasta_sequence = str()\n            for contig, record in sample[self.analysistype].record_dict.items():\n                # Append the length of the contig to the list\n                contig_lengths.append(len(record.seq))\n                # Add the contig sequence to the string\n                fasta_sequence += record.seq\n            # Set the reverse sorted (e.g. largest to smallest) list of contig sizes as the value\n            sample[self.analysistype].contig_lengths = sorted(contig_lengths, reverse=True)\n            try:\n                # Calculate the GC% of the total genome sequence using GC - format to have two decimal places\n                sample[self.analysistype].gc = float('{:0.2f}'.format(GC(fasta_sequence)))\n            except TypeError:\n                sample[self.analysistype].gc = 'NA'", "response": "Parse the lengths of all contigs for each sample and store the total GC% of the total genome sequence"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_largest_contig(self):\n        # for file_name, contig_lengths in contig_lengths_dict.items():\n        for sample in self.metadata:\n            # As the list is sorted in descending order, the largest contig is the first entry in the list\n            sample[self.analysistype].longest_contig = sample[self.analysistype].contig_lengths", "response": "Determine the largest contig for each strain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the total length of all the contigs for each strain in the metadata", "response": "def find_genome_length(self):\n        \"\"\"\n        Determine the total length of all the contigs for each strain\n        \"\"\"\n        for sample in self.metadata:\n            # Use the sum() method to add all the contig lengths in the list\n            sample[self.analysistype].genome_length = sum(sample[self.analysistype].contig_lengths)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount the total number of contigs for each strain", "response": "def find_num_contigs(self):\n        \"\"\"\n        Count the total number of contigs for each strain\n        \"\"\"\n        for sample in self.metadata:\n            # Use the len() method to count the number of entries in the list\n            sample[self.analysistype].num_contigs = len(sample[self.analysistype].contig_lengths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_n50(self):\n        for sample in self.metadata:\n            # Initialise the N50 attribute in case there is no assembly, and the attribute is not created in the loop\n            sample[self.analysistype].n50 = '-'\n            # Initialise a variable to store a running total of contig lengths\n            currentlength = 0\n            for contig_length in sample[self.analysistype].contig_lengths:\n                # Increment the current length with the length of the current contig\n                currentlength += contig_length\n                # If the current length is now greater than the total genome / 2, the current contig length is the N50\n                if currentlength >= sample[self.analysistype].genome_length * 0.5:\n                    # Populate the dictionary, and break the loop\n                    sample[self.analysistype].n50 = contig_length\n                    break", "response": "Find the N50 for each strain in the metadata dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef perform_pilon(self):\n        for sample in self.metadata:\n            try:\n                if sample[self.analysistype].num_contigs > 500 or sample.confindr.contam_status == 'Contaminated':\n                    sample.general.polish = False\n                else:\n                    sample.general.polish = True\n            except AttributeError:\n                sample.general.polish = True", "response": "Determine if pilon polishing should be attempted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the record_dict attribute from the object as SeqRecords are not JSON - serializable.", "response": "def clear_attributes(self):\n        \"\"\"\n        Remove the record_dict attribute from the object, as SeqRecords are not JSON-serializable. Also remove\n        the contig_lengths and longest_contig attributes, as they are large lists that make the .json file ugly\n        \"\"\"\n        for sample in self.metadata:\n            try:\n                delattr(sample[self.analysistype], 'record_dict')\n                delattr(sample[self.analysistype], 'contig_lengths')\n                delattr(sample[self.analysistype], 'longest_contig')\n            except AttributeError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_qaml(self):\n        logging.info('Running GenomeQAML quality assessment')\n        qaml_call = 'classify.py -t {tf} -r {rf}'\\\n            .format(tf=self.qaml_path,\n                    rf=self.qaml_report)\n        make_path(self.reportpath)\n        # Only attempt to assess assemblies if the report doesn't already exist\n        if not os.path.isfile(self.qaml_report):\n            # Run the system calls\n            out, err = run_subprocess(qaml_call)\n            # Acquire thread lock, and write the logs to file\n            self.threadlock.acquire()\n            write_to_logfile(qaml_call, qaml_call, self.logfile)\n            write_to_logfile(out, err, self.logfile)\n            self.threadlock.release()", "response": "Create and run the GenomeQAML system call and write the logs to file\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_qaml(self):\n        logging.info('Parsing GenomeQAML outputs')\n        # A dictionary to store the parsed excel file in a more readable format\n        nesteddictionary = dict()\n        # Use pandas to read in the CSV file, and convert the pandas data frame to a dictionary (.to_dict())\n        dictionary = pandas.read_csv(self.qaml_report).to_dict()\n        # Iterate through the dictionary - each header from the CSV file\n        for header in dictionary:\n            # Sample is the primary key, and value is the value of the cell for that primary key + header combination\n            for sample, value in dictionary[header].items():\n                # Update the dictionary with the new data\n                try:\n                    nesteddictionary[sample].update({header: value})\n                # Create the nested dictionary if it hasn't been created yet\n                except KeyError:\n                    nesteddictionary[sample] = dict()\n                    nesteddictionary[sample].update({header: value})\n        # Get the results into the metadata object\n        for sample in self.metadata:\n            # Initialise the plasmid extractor genobject\n            setattr(sample, self.analysistype, GenObject())\n            # Initialise the list of all plasmids\n            sample[self.analysistype].prediction = str()\n            # Iterate through the dictionary of results\n            for line in nesteddictionary:\n                # Extract the sample name from the dictionary\n                name = nesteddictionary[line]['Sample']\n                # Ensure that the names match\n                if name == sample.name:\n                    # Append the plasmid name extracted from the dictionary to the list of plasmids\n                    sample[self.analysistype].prediction = nesteddictionary[line]['Predicted_Class']", "response": "Parse the GenomeQAML report and populate the metadata objects with the results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the plugin. Do nothing.", "response": "def init(self, ):\n        \"\"\"Initialize the plugin. Do nothing.\n\n        This function gets called when the plugin is loaded by the plugin manager.\n\n        :returns:\n        :rtype:\n        :raises:\n        \"\"\"\n        self.gw = None\n        pm = MayaPluginManager.get()\n        genesis =  pm.get_plugin(\"Genesis\")\n        self.GenesisWin = self.subclass_genesis(genesis.GenesisWin)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, *args, **kwargs):\n        if self.gw and shiboken.isValid(self.gw):\n            self.gw.deleteLater()\n        mayawin = maya_main_window()\n        self.gw = self.GenesisWin(parent=mayawin)\n        self.gw.last_file.connect(self.save_lastfile)\n        if not self.gw.get_current_file():\n            c = self.get_config()\n            try:\n                f = models.TaskFile.objects.get(pk=c['lastfile'])\n            except models.TaskFile.DoesNotExist:\n                pass\n            else:\n                self.gw.browser.set_selection(f)\n        self.gw.show()", "response": "Start genesis\n\n        :returns: None\n        :rtype: None\n        :raises: None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_lastfile(self, tfi):\n        tf = models.TaskFile.objects.get(task=tfi.task, version=tfi.version, releasetype=tfi.releasetype,\n                                         descriptor=tfi.descriptor, typ=tfi.typ)\n        c = self.get_config()\n        c['lastfile'] = tf.pk\n        c.write()", "response": "Save the last selected taskfile in the config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subclass_genesis(self, genesisclass):\n        class MayaGenesisWin(genesisclass):\n            \"\"\"Implementation of Genesis for maya\n            \"\"\"\n\n            def open_shot(self, taskfile):\n                \"\"\"Open the given taskfile\n\n                :param taskfile: the taskfile for the shot\n                :type taskfile: :class:`djadapter.models.TaskFile`\n                :returns: True if opening was successful\n                :rtype: bool\n                :raises: none\n                \"\"\"\n                return self.open_file(taskfile)\n\n            def save_shot(self, jbfile, tf):\n                \"\"\"Save the shot to the location of jbfile\n\n                :param jbfile: the jbfile that can be used to query the location\n                :type jbfile: :class:`jukebox.core.filesys.JB_File`\n                :param tf: the taskfile that is saved\n                :type tf: :class:`djadapter.models.TaskFile`\n                :returns: None\n                :rtype: None\n                :raises: None\n                \"\"\"\n                self.update_scene_node(tf)\n                self.save_file(jbfile)\n\n            def open_asset(self, taskfile):\n                \"\"\"Open the given taskfile\n\n                :param taskfile: the taskfile for the asset\n                :type taskfile: :class:`djadapter.models.TaskFile`\n                :returns: True if opening was successful\n                :rtype: bool\n                :raises: None\n                \"\"\"\n                return self.open_file(taskfile)\n\n            def save_asset(self, jbfile, tf):\n                \"\"\"Save the asset to the location of jbfile\n\n                :param jbfile: the jbfile that can be used to query the location\n                :type jbfile: :class:`jukebox.core.filesys.JB_File`\n                :param tf: the taskfile that is saved\n                :type tf: :class:`djadapter.models.TaskFile`\n                :returns: None\n                :rtype: None\n                :raises: NotImplementedError\n                \"\"\"\n                self.update_scene_node(tf)\n                self.save_file(jbfile)\n\n            def save_file(self, jbfile):\n                \"\"\"Physically save current scene to jbfile\\'s location\n\n                :param jbfile: the jbfile that can be used to query the location\n                :type jbfile: :class:`jukebox.core.filesys.JB_File`\n                :returns: None\n                :rtype: None\n                :raises: None\n                \"\"\"\n                p = jbfile.get_fullpath()\n                p = os.path.expanduser(p)\n                typ = 'mayaBinary'\n                if jbfile.get_ext() == 'ma':\n                    typ = 'mayaAscii'\n                cmds.file(rename = p)\n                cmds.file(save=True, defaultExtensions=False, type=typ)\n\n            def open_file(self, taskfile):\n                \"\"\"Open the given jbfile in maya\n\n                :param taskfile: the taskfile for the asset\n                :type taskfile: :class:`djadapter.models.TaskFile`\n                :returns: True if opening was successful\n                :rtype: bool\n                :raises: None\n                \"\"\"\n                r = self.check_modified()\n                if r is False:\n                    return False\n                cmds.file(taskfile.path, open=True, force=True, ignoreVersion=True)\n                return True\n\n            def get_current_file(self, ):\n                \"\"\"Return the taskfile that is currently open or None if no taskfile is open\n\n                :returns: the open taskfile or None if no taskfile is open\n                :rtype: :class:`djadapter.models.TaskFile` | None\n                :raises: None\n                \"\"\"\n                node = jbscene.get_current_scene_node()\n                if not node:\n                    return\n                tfid = cmds.getAttr('%s.taskfile_id' % node)\n                try:\n                    return djadapter.taskfiles.get(id=tfid)\n                except djadapter.models.TaskFile.DoesNotExist:\n                    log.error(\"No taskfile with id %s was found. Get current scene failed. Check your jb_sceneNode \\'%s\\'.\" % (tfid, node))\n                    return\n\n            def get_scene_node(self, ):\n                \"\"\"Return the current scenen node or create one if it does not exist\n\n                :returns: Name of the scene node\n                :rtype: str\n                :raises: None\n                \"\"\"\n                scenenodes = cmds.ls(':jb_sceneNode*')\n                if len(scenenodes) > 1:\n                    cmds.delete(scenenodes)\n                node = jbscene.get_current_scene_node()\n                if node is None:\n                    cmds.namespace(set=':')\n                    node = cmds.createNode('jb_sceneNode')\n                return node\n\n            def update_scene_node(self, tf):\n                \"\"\"Update the current scene node\n\n                :param tf: the taskfile that is saved\n                :type tf: :class:`djadapter.models.TaskFile`\n                :returns: None\n                :rtype: None\n                :raises: None\n                \"\"\"\n                node = self.get_scene_node()\n                cmds.setAttr('%s.taskfile_id' % node, lock=False)\n                cmds.setAttr('%s.taskfile_id' % node, tf.id)\n                cmds.setAttr('%s.taskfile_id' % node, lock=True)\n\n            def check_modified(self, ):\n                \"\"\"Check if the current scene was modified and ask the user to continue\n\n                This might save the scene if the user accepts to save before continuing.\n\n                :returns: True if the user accepted to continue.\n                :rtype: bool\n                :raises: None\n                \"\"\"\n                if not cmds.file(q=1, modified=1):\n                    return True\n                curfile = cmds.file(q=1, sceneName=1)\n                r = cmds.confirmDialog( title='Save Changes', message='Save changes to %s?' % curfile,\n                                       button=['Save', 'Don\\'t Save' ,'Cancel'],\n                                       defaultButton='Save', cancelButton='Cancel',\n                                       dismissString='Cancel')\n                if r == 'Cancel':\n                    return False\n                if r == 'Save':\n                    cmds.file(save=True, force=True)\n                return True\n\n        MayaGenesisWin.set_filetype(djadapter.FILETYPES['mayamainscene'],)\n        return MayaGenesisWin", "response": "Subclass the given GenesisWin class and implement all abstract methods\n        and return the subclass of MayaGenesisWin class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stash_calibration(self, attenuations, freqs, frange, calname):\n        self.calibration_vector = attenuations\n        self.calibration_freqs = freqs\n        self.calibration_frange = frange\n        self.calname = calname", "response": "Save it for later"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_stim_by_index(self, index):\n        # remove any current components\n        self.stimulus.clearComponents()\n        # add one to index because of tone curve\n        self.stimulus.insertComponent(self.stim_components[index])", "response": "Sets the stimulus to be generated to the one referenced by index"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the duration of the stimulus.", "response": "def set_duration(self, dur):\n        \"\"\"See :meth:`AbstractCalibrationRunner<sparkle.run.calibration_runner.AbstractCalibrationRunner.set_duration>`\"\"\"\n        # this may be set at any time, and is not checked before run, so set\n        # all stim components\n        for comp in self.stim_components:\n            comp.setDuration(dur)\n        self.reftone.setDuration(dur)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the calibration signal and saves the calibration signal to file.", "response": "def process_calibration(self, save=True):\n        \"\"\"processes calibration control signal. Determines transfer function\n        of speaker to get frequency vs. attenuation curve.\n\n        :param save: Whether to save this calibration data to file\n        :type save: bool\n        :returns: numpy.ndarray, str, int, float -- frequency response (in dB), dataset name, calibration reference frequency, reference intensity\n        \"\"\"\n        if not self.save_data:\n            raise Exception(\"Cannot process an unsaved calibration\")\n            \n        avg_signal = np.mean(self.datafile.get_data(self.current_dataset_name + '/signal'), axis=0)\n\n        diffdB = attenuation_curve(self.stimulus.signal()[0], avg_signal,\n                                        self.stimulus.samplerate(), self.calf)\n        logger = logging.getLogger('main')\n        logger.debug('The maximum dB attenuation is {}, caldB {}'.format(max(diffdB), self.caldb))\n\n        # save a vector of only the calibration intensity results\n        self.datafile.init_data(self.current_dataset_name, mode='calibration',\n                                dims=diffdB.shape,\n                                nested_name='calibration_intensities')\n        self.datafile.append(self.current_dataset_name, diffdB,\n                             nested_name='calibration_intensities')\n\n        relevant_info = {'frequencies': 'all', 'calibration_dB':self.caldb,\n                         'calibration_voltage': self.calv, 'calibration_frequency': self.calf,\n                         }\n        self.datafile.set_metadata('/'.join([self.current_dataset_name, 'calibration_intensities']),\n                                   relevant_info)\n\n        mean_reftone = np.mean(self.datafile.get_data(self.current_dataset_name + '/reference_tone'), axis=0)\n        tone_amp = signal_amplitude(mean_reftone, self.player.get_aifs())\n        db = calc_db(tone_amp, self.mphonesens, self.mphonedb)\n        # remove the reference tone from protocol\n        self.protocol_model.remove(0)\n        \n        return diffdB, self.current_dataset_name, self.calf, db"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_reps(self, reps):\n        self.stimulus.setRepCount(reps)\n        self.refstim.setRepCount(reps)", "response": "set the number of repetitions for the stimuli"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the data gathered in a calibration run returns the resultant dB", "response": "def process_calibration(self, save=False):\n        \"\"\"processes the data gathered in a calibration run (does not work if multiple\n            calibrations), returns resultant dB\"\"\"\n        \n        if not self.save_data:\n            raise Exception(\"Runner must be set to save when run, to be able to process\")\n\n        vfunc = np.vectorize(calc_db, self.mphonesens, self.mphonedb)\n\n        if USE_FFT:\n            peaks = np.mean(abs(self.datafile.get_data(self.current_dataset_name + '/fft_peaks')), axis=1)\n        else:\n            peaks = np.mean(abs(self.datafile.get_data(self.current_dataset_name + '/vamp')), axis=1)\n\n        # print 'calibration frequencies', self.calibration_frequencies\n        # cal_index = self.calibration_indexes[self.calibration_frequencies.index(self.calf)]\n        # cal_peak = peaks[cal_index]\n        # cal_vmax = vmaxes[cal_index]\n\n        # print 'vfunc inputs', vmaxes, self.caldb, cal_vmax\n\n        resultant_dB = vfunc(peaks, self.calpeak) * -1 #db attenuation\n\n        print 'calibration frequences', self.calibration_frequencies, 'indexes', self.calibration_indexes\n        print 'attenuations', resultant_dB\n\n        calibration_vector = resultant_dB[self.calibration_indexes].squeeze()\n        # Not currenly saving resultant intensity\n\n        return resultant_dB, '', self.calf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setModel(self, model):\n        \"Sets the StimulusModel for this editor\"\n        self._model = model\n        self.ui.aofsSpnbx.setValue(model.samplerate())", "response": "Sets the StimulusModel for this editor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setStimIndex(self, row, stimIndex):\n        \"Change out the component type in row to the one indexed by stimIndex\"\n        newcomp = self._allComponents[row][stimIndex]\n        self._model.removeComponent(row, 1)\n        self._model.insertComponent(newcomp, row, 1)", "response": "Change out the component type in row to the one indexed by stimIndex"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addComponentEditor(self):\n        row = self._model.rowCount()\n\n        comp_stack_editor = ExploreComponentEditor()\n        self.ui.trackStack.addWidget(comp_stack_editor)\n\n        idx_button = IndexButton(row)\n        idx_button.pickMe.connect(self.ui.trackStack.setCurrentIndex)\n        self.trackBtnGroup.addButton(idx_button)\n        self.ui.trackBtnLayout.addWidget(idx_button)\n        self.ui.trackStack.setCurrentIndex(row)\n\n        comp_stack_editor.closePlease.connect(self.removeComponentEditor)\n\n        delay = Silence()\n        comp_stack_editor.delaySpnbx.setValue(delay.duration())\n        self._model.insertComponent(delay, row,0)\n\n        self._allComponents.append([x() for x in self.stimuli_types if x.explore])\n        for stim in self._allComponents[row]:\n            editor = wrapComponent(stim).showEditor()\n            comp_stack_editor.addWidget(editor, stim.name)\n\n        exvocal = comp_stack_editor.widgetForName(\"Vocalization\")\n        if exvocal is not None:\n            exvocal.filelistView.setSelectionMode(QtGui.QAbstractItemView.SingleSelection)\n\n        initcomp = self._allComponents[row][0]\n        self._model.insertComponent(initcomp, row, 1)\n\n        self.buttons.append(idx_button)\n\n        comp_stack_editor.exploreStimTypeCmbbx.currentIndexChanged.connect(lambda x : self.setStimIndex(row, x))\n        comp_stack_editor.delaySpnbx.valueChanged.connect(lambda x : self.setDelay(row, x))\n        comp_stack_editor.valueChanged.connect(self.valueChanged.emit)\n        return comp_stack_editor", "response": "Adds a new component to the model and returns the editor for this component"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_space_systems(self, page_size=None):\n        params = {}\n\n        if page_size is not None:\n            params['limit'] = page_size\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/mdb/{}/space-systems'.format(self._instance),\n            params=params,\n            response_class=mdb_pb2.ListSpaceSystemsResponse,\n            items_key='spaceSystem',\n            item_mapper=SpaceSystem,\n        )", "response": "Lists the space systems visible to this client."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a single space system by its unique name.", "response": "def get_space_system(self, name):\n        \"\"\"\n        Gets a single space system by its unique name.\n\n        :param str name: A fully-qualified XTCE name\n        :rtype: .SpaceSystem\n        \"\"\"\n        url = '/mdb/{}/space-systems{}'.format(self._instance, name)\n        response = self._client.get_proto(url)\n        message = mdb_pb2.SpaceSystemInfo()\n        message.ParseFromString(response.content)\n        return SpaceSystem(message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_parameters(self, parameter_type=None, page_size=None):\n        params = {'details': True}\n\n        if parameter_type is not None:\n            params['type'] = parameter_type\n        if page_size is not None:\n            params['limit'] = page_size\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/mdb/{}/parameters'.format(self._instance),\n            params=params,\n            response_class=mdb_pb2.ListParametersResponse,\n            items_key='parameter',\n            item_mapper=Parameter,\n        )", "response": "Lists the parameters visible to this client."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_parameter(self, name):\n        name = adapt_name_for_rest(name)\n        url = '/mdb/{}/parameters{}'.format(self._instance, name)\n        response = self._client.get_proto(url)\n        message = mdb_pb2.ParameterInfo()\n        message.ParseFromString(response.content)\n        return Parameter(message)", "response": "Gets a single parameter by its name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the containers visible to this client.", "response": "def list_containers(self, page_size=None):\n        \"\"\"\n        Lists the containers visible to this client.\n\n        Containers are returned in lexicographical order.\n\n        :rtype: :class:`.Container` iterator\n        \"\"\"\n        params = {}\n\n        if page_size is not None:\n            params['limit'] = page_size\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/mdb/{}/containers'.format(self._instance),\n            params=params,\n            response_class=mdb_pb2.ListContainersResponse,\n            items_key='container',\n            item_mapper=Container,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_container(self, name):\n        name = adapt_name_for_rest(name)\n        url = '/mdb/{}/containers{}'.format(self._instance, name)\n        response = self._client.get_proto(url)\n        message = mdb_pb2.ContainerInfo()\n        message.ParseFromString(response.content)\n        return Container(message)", "response": "Gets a single container by its unique name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the commands visible to this client.", "response": "def list_commands(self, page_size=None):\n        \"\"\"\n        Lists the commands visible to this client.\n\n        Commands are returned in lexicographical order.\n\n        :rtype: :class:`.Command` iterator\n        \"\"\"\n        params = {}\n\n        if page_size is not None:\n            params['limit'] = page_size\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/mdb/{}/commands'.format(self._instance),\n            params=params,\n            response_class=mdb_pb2.ListCommandsResponse,\n            items_key='command',\n            item_mapper=Command,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a single command by its unique name.", "response": "def get_command(self, name):\n        \"\"\"\n        Gets a single command by its unique name.\n\n        :param str name: Either a fully-qualified XTCE name or an alias in the\n                         format ``NAMESPACE/NAME``.\n        :rtype: .Command\n        \"\"\"\n        name = adapt_name_for_rest(name)\n        url = '/mdb/{}/commands{}'.format(self._instance, name)\n        response = self._client.get_proto(url)\n        message = mdb_pb2.CommandInfo()\n        message.ParseFromString(response.content)\n        return Command(message)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the algorithms visible to this client.", "response": "def list_algorithms(self, page_size=None):\n        \"\"\"\n        Lists the algorithms visible to this client.\n\n        Algorithms are returned in lexicographical order.\n\n        :rtype: :class:`.Algorithm` iterator\n        \"\"\"\n        params = {}\n\n        if page_size is not None:\n            params['limit'] = page_size\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/mdb/{}/algorithms'.format(self._instance),\n            params=params,\n            response_class=mdb_pb2.ListAlgorithmsResponse,\n            items_key='algorithm',\n            item_mapper=Algorithm,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a single algorithm by its unique name.", "response": "def get_algorithm(self, name):\n        \"\"\"\n        Gets a single algorithm by its unique name.\n\n        :param str name: Either a fully-qualified XTCE name or an alias in the\n                         format ``NAMESPACE/NAME``.\n        :rtype: .Algorithm\n        \"\"\"\n        name = adapt_name_for_rest(name)\n        url = '/mdb/{}/algorithms{}'.format(self._instance, name)\n        response = self._client.get_proto(url)\n        message = mdb_pb2.AlgorithmInfo()\n        message.ParseFromString(response.content)\n        return Algorithm(message)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sample_name_from_vcf_header_lines(header_lines):\n    '''Given a list of header lines (made by either\n    vcf_file_to_dict() or vcf_file_to_list()),\n    returns the sample name. Assumes only one sample\n    in the file.\n    Raises error if badly formatted #CHROM line.\n    Returns None if no #CHROM line found'''\n    # We want the #CHROM line, which should be the last line\n    # of the header\n    for line in reversed(header_lines):\n        if line.startswith('#CHROM'):\n            fields = line.rstrip().split('\\t')\n            required_cols = ['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO']\n            if fields[:len(required_cols)] != required_cols:\n                raise Error('Error! #CHROM line must have these for first 8 columns: ' + ', '.join(required_cols) + '\\nat this line of file: ' + line)\n\n            if len(fields) == len(required_cols):\n                return None\n\n            required_cols.append('FORMAT')\n            format_column = fields[len(required_cols) - 1]\n            if format_column != required_cols[-1]:\n                raise Error('Error! #CHROM line has 9^th column, which should be \"FORMAT\" but is: \"' + format_column + '\" at this line of file: ' + line)\n\n            if len(fields) == len(required_cols):\n                logging.warning('FORMAT column in file, but no sample names at line: ' + line.rstrip())\n                return None\n\n            sample = fields[len(required_cols)]\n\n            if len(fields) > len(required_cols) + 1:\n                logging.warning('More than one sample found. Using the name of the first sample (' + sample + ') at line: ' + line.rstrip())\n\n            return sample\n\n    return None", "response": "Given a list of header lines returns the sample name. Assumes only one sample in the file. Returns None if no sample found."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads just the variant info from input VCF file.", "response": "def vcf_file_to_dict_of_vars(infile, reference_seqs):\n    '''Loads just the variant info from input VCF file.\n    If reference_seqs is given, should be a dict of seq name -> pyfastaq Fastaq sequence,\n    and will be used to sanity check variants in input file. Any where CHROM\n    is not in the dict, or REF string does not match the ref sequence, are ignored\n    Output is a dictionary of:\n    ref name -> position -> {ref string -> {set of alt strings}}'''\n    variants = {}\n    _, vcf_records = vcf_file_to_dict(infile, sort=False, remove_asterisk_alts=True, remove_useless_start_nucleotides=True)\n\n    for ref_name, variant_list in vcf_records.items():\n        for record in variant_list:\n            if record.POS < 0:\n                logging.warning(f'VCF record with negative POS in file {infile}. Ignoring: {record}')\n                continue\n            elif record.CHROM not in reference_seqs:\n                logging.warning(f'CHROM not recognised in VCF record in file {infile}. Ignoring: {record}')\n                continue\n            elif reference_seqs[record.CHROM][record.POS:record.POS + len(record.REF)] != record.REF:\n                logging.warning(f'REF string does not match reference seq in file {infile}. Ignoring: {record}')\n                continue\n\n            if ref_name not in variants:\n                variants[ref_name] = {}\n\n            if record.POS not in variants[ref_name]:\n                variants[ref_name][record.POS] = {}\n\n            if record.REF not in variants[ref_name][record.POS]:\n                variants[ref_name][record.POS][record.REF] = set()\n\n            variants[ref_name][record.POS][record.REF].update(record.ALT)\n\n    return variants"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator over the buckets for an instance.", "response": "def list_buckets(self, instance):\n        \"\"\"\n        List the buckets for an instance.\n\n        :param str instance: A Yamcs instance name.\n        :rtype: ~collections.Iterable[.Bucket]\n        \"\"\"\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        response = self._client.get_proto(path='/buckets/' + instance)\n        message = rest_pb2.ListBucketsResponse()\n        message.ParseFromString(response.content)\n        buckets = getattr(message, 'bucket')\n        return iter([\n            Bucket(bucket, instance, self) for bucket in buckets])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_objects(self, instance, bucket_name, prefix=None, delimiter=None):\n        url = '/buckets/{}/{}'.format(instance, bucket_name)\n        params = {}\n        if prefix is not None:\n            params['prefix'] = prefix\n        if delimiter is not None:\n            params['delimiter'] = delimiter\n        response = self._client.get_proto(path=url, params=params)\n        message = rest_pb2.ListObjectsResponse()\n        message.ParseFromString(response.content)\n        return ObjectListing(message, instance, bucket_name, self)", "response": "List the objects in a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_bucket(self, instance, bucket_name):\n        req = rest_pb2.CreateBucketRequest()\n        req.name = bucket_name\n        url = '/buckets/{}'.format(instance)\n        self._client.post_proto(url, data=req.SerializeToString())", "response": "Create a new bucket in the specified instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_bucket(self, instance, bucket_name):\n        url = '/buckets/{}/{}'.format(instance, bucket_name)\n        self._client.delete_proto(url)", "response": "Remove a bucket from the specified instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_object(self, instance, bucket_name, object_name):\n        url = '/buckets/{}/{}/{}'.format(instance, bucket_name, object_name)\n        response = self._client.get_proto(path=url)\n        return response.content", "response": "Download an object from Yamcs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload_object(self, instance, bucket_name, object_name, file_obj,\n                      content_type=None):\n        \"\"\"\n        Upload an object to a bucket.\n\n        :param str instance: A Yamcs instance name.\n        :param str bucket_name: The name of the bucket.\n        :param str object_name: The target name of the object.\n        :param file file_obj: The file (or file-like object) to upload.\n        :param str content_type: The content type associated to this object.\n                                 This is mainly useful when accessing an object\n                                 directly via a web browser. If unspecified, a\n                                 content type *may* be automatically derived\n                                 from the specified ``file_obj``.\n        \"\"\"\n        url = '/buckets/{}/{}/{}'.format(instance, bucket_name, object_name)\n        with open(file_obj, 'rb') as f:\n            if content_type:\n                files = {object_name: (object_name, f, content_type)}\n            else:\n                files = {object_name: (object_name, f)}\n            self._client.request(path=url, method='post', files=files)", "response": "Uploads an object to a Yamcs instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_object(self, instance, bucket_name, object_name):\n        url = '/buckets/{}/{}/{}'.format(instance, bucket_name, object_name)\n        self._client.delete_proto(url)", "response": "Remove an object from a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a config file and return a dictionary with the settings", "response": "def read_settings(filename=None, defs=None):\n    \"\"\"\n    :param filename: Force load a file\n    :param defs: arguments you want to accept\n    :param default_filename: A config file from an environment variable (a fallback config file, if no other provided)\n    :return:\n    \"\"\"\n    # READ SETTINGS\n    defs = listwrap(defs)\n    defs.append({\n        \"name\": [\"--config\", \"--settings\", \"--settings-file\", \"--settings_file\"],\n        \"help\": \"path to JSON file with settings\",\n        \"type\": str,\n        \"dest\": \"filename\",\n        \"default\": None,\n        \"required\": False\n    })\n    args = argparse(defs)\n\n    args.filename = coalesce(filename, args.filename, \"./config.json\")\n    settings_file = File(args.filename)\n    if not settings_file.exists:\n        Log.error(\"Can not read configuration file {{filename}}\", {\n            \"filename\": settings_file.abspath\n        })\n    settings = mo_json_config.get_file(settings_file)\n    settings.args = args\n    return settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_now_utc():\n    ''' date in UTC, ISO format'''\n\n    # Helper class for UTC time\n    # Source: http://stackoverflow.com/questions/2331592/datetime-datetime-utcnow-why-no-tzinfo\n    ZERO = datetime.timedelta(0)\n    class UTC(datetime.tzinfo):\n        \"\"\"UTC\"\"\"\n        def utcoffset(self, dt):\n            return ZERO\n        def tzname(self, dt):\n            return \"UTC\"\n        def dst(self, dt):\n            return ZERO\n\n    #now = datetime.datetime.now(timezone.utc) # Python 3.2\n    now = datetime.datetime.now(UTC())\n    return now", "response": "Returns a date in UTC ISO format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef switchDisplay(self, display):\n        if display in self.displays:\n            self.setWidget(self.displays[display])\n            self._current = display\n        else:\n            raise Exception(\"Undefined display type \"+ display)", "response": "Switches the visible widget to the one named display"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget value at position", "response": "def get(self, position):\n        \"\"\"Gets value at index\n\n        :param position: index\n        :return: value at position\n        \"\"\"\n\n        counter = 0\n        current_node = self.head\n\n        while current_node is not None and counter <= position:\n            if counter == position:\n                return current_node.val\n\n            current_node = current_node.next_node\n            counter += 1\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tail(self):\n        node = self.head\n        last_node = self.head\n\n        while node is not None:\n            last_node = node\n            node = node.next_node\n\n        return last_node", "response": "Gets the tail of the linked list\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the length of the items in the linked list", "response": "def length(self):\n        \"\"\"Gets length\n\n        :return: How many items in linked list of linked list\n        \"\"\"\n        item = self.head\n        counter = 0\n\n        while item is not None:\n            counter += 1\n            item = item.next_node\n\n        return counter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_first(self, val):\n\n        self.head = Node(val, next_node=self.head)\n        return True", "response": "Insert in the head node if the object is not already in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert(self, val, position=0):\n        if position <= 0:  # at beginning\n            return self.insert_first(val)\n\n        counter = 0\n        last_node = self.head\n        current_node = self.head\n\n        while current_node is not None and counter <= position:\n            if counter == position:\n                last_node.next_node = Node(val, current_node)\n                return True\n\n            last_node = current_node\n            current_node = current_node.next_node\n            counter += 1\n\n        if current_node is None:  # append to last element\n            last_node.next_node = Node(val, None)\n\n        return True", "response": "Insert in position\n\n        :param val: Object to insert\n        :param position: Index of insertion\n        :return: bool: True iff insertion completed successfully"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves first node in the sequence. Returns True if the first node has been removed False otherwise.", "response": "def remove_first(self):\n        \"\"\"Removes first\n\n        :return: True iff head has been removed\n        \"\"\"\n\n        if self.head is None:\n            return False\n\n        self.head = self.head.next_node\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_last(self):\n\n        if self.length() <= 1:\n            self.head = None\n            return True\n\n        node = self.head\n\n        while node is not None:\n            is_last_but_one = node.next_node is not None and \\\n                              node.next_node.next_node is None\n            print(node.val, is_last_but_one)\n            if is_last_but_one:  # this is the last\n                node.next_node = None  # get to last but one element\n                return True\n\n            node = node.next_node\n\n        return False", "response": "Removes the last element in the list. Returns True iff the last element has been removed False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove(self, position):\n        if position <= 0:  # at beginning\n            return self.remove_first()\n\n        if position >= self.length() - 1:  # at end\n            return self.remove_last()\n\n        counter = 0\n        last_node = self.head\n        current_node = self.head\n\n        while current_node is not None and counter <= position:\n            if counter == position:\n                last_node.next_node = current_node.next_node  # remove current\n                return True\n\n            last_node = current_node\n            current_node = current_node.next_node\n            counter += 1\n\n        return False", "response": "Removes at the specified position in the list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_lst(self):\n        out = []\n        node = self.head\n\n        while node is not None:\n            out.append(node.val)\n            node = node.next_node\n\n        return out", "response": "Cycle all items and puts them in a list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting function on each item in the sequence", "response": "def execute(self, func, *args, **kwargs):\n        \"\"\"Executes function on each item\n\n        :param func: Function to execute on each item\n        :param args: args of function\n        :param kwargs: extra args of function\n        :return: list: Results of calling the function on each item\n        \"\"\"\n        return [\n            func(item, *args, **kwargs) for item in self.to_lst()\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_list(lst):\n        if not lst:\n            return None\n\n        head = Node(lst[0], None)\n\n        if len(lst) == 1:\n            return head\n\n        head.next_node = LinkedList.from_list(lst[1:])\n        return head", "response": "Parses list of elements\n        and returns a LinkedList of Nodes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_specification(specification, env_prefix=None, separator='.',\n                       parent_names=None):\n    \"\"\"Used to create YapconfItems from a specification dictionary.\n\n    Args:\n        specification (dict): The specification used to\n            initialize ``YapconfSpec``\n        env_prefix (str): Prefix to add to environment names\n        separator (str): Separator for nested items\n        parent_names (list): Parents names of any given item\n\n    Returns:\n        A dictionary of names to YapconfItems\n\n    \"\"\"\n    items = {}\n    for item_name, item_info in six.iteritems(specification):\n        names = copy.copy(parent_names) if parent_names else []\n        items[item_name] = _generate_item(item_name,\n                                          item_info,\n                                          env_prefix,\n                                          separator,\n                                          names)\n    return items", "response": "Used to create YapconfItems from a dictionary of names to YapconfItem objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_default(self, new_default, respect_none=False):\n        if new_default is not None:\n            self.default = new_default\n        elif new_default is None and respect_none:\n            self.default = None", "response": "Update our current default with the new_default."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef migrate_config(self, current_config, config_to_migrate,\n                       always_update, update_defaults):\n        \"\"\"Migrate config value in current_config, updating config_to_migrate.\n\n        Given the current_config object, it will attempt to find a value\n        based on all the names given. If no name could be found, then it\n        will simply set the value to the default.\n\n        If a value is found and is in the list of previous_defaults, it will\n        either update or keep the old value based on if update_defaults is\n        set.\n\n        If a non-default value is set it will either keep this value or update\n        it based on if ``always_update`` is true.\n\n        Args:\n            current_config (dict): Current configuration.\n            config_to_migrate (dict): Config to update.\n            always_update (bool): Always update value.\n            update_defaults (bool): Update values found in previous_defaults\n        \"\"\"\n        value = self._search_config_for_possible_names(current_config)\n        self._update_config(config_to_migrate, value,\n                            always_update, update_defaults)", "response": "Migrate config value in current_config to config_to_migrate."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd this item as an argument to the given parser.", "response": "def add_argument(self, parser, bootstrap=False):\n        \"\"\"Add this item as an argument to the given parser.\n\n        Args:\n            parser (argparse.ArgumentParser): The parser to add this item to.\n            bootstrap: Flag to indicate whether you only want to mark this\n                item as required or not\n        \"\"\"\n        if self.cli_expose:\n            args = self._get_argparse_names(parser.prefix_chars)\n            kwargs = self._get_argparse_kwargs(bootstrap)\n            parser.add_argument(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the configuration value from all overrides.", "response": "def get_config_value(self, overrides, skip_environment=False):\n        \"\"\"Get the configuration value from all overrides.\n\n        Iterates over all overrides given to see if a value can be pulled\n        out from them. It will convert each of these values to ensure they\n        are the correct type.\n\n        Args:\n            overrides: A list of tuples where each tuple is a label and a\n                dictionary representing a configuration.\n            skip_environment: Skip looking through the environment.\n\n        Returns:\n            The converted configuration value.\n\n        Raises:\n            YapconfItemNotFound: If an item is required but could not be found\n                in the configuration.\n            YapconfItemError: If a possible value was found but the type\n                cannot be determined.\n            YapconfValueError: If a possible value is found but during\n                conversion, an exception was raised.\n\n        \"\"\"\n        label, override, key = self._search_overrides(\n            overrides, skip_environment\n        )\n\n        if override is None and self.default is None and self.required:\n            raise YapconfItemNotFound(\n                'Could not find config value for {0}'.format(self.fq_name),\n                self\n            )\n\n        if override is None:\n            self.logger.debug(\n                'Config value not found for {0}, falling back to default.'\n                .format(self.name)\n            )\n            value = self.default\n        else:\n            value = override[key]\n\n        if value is None:\n            return value\n\n        converted_value = self.convert_config_value(value, label)\n        self._validate_value(converted_value)\n        return converted_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an argument to the given parser.", "response": "def add_argument(self, parser, bootstrap=False):\n        \"\"\"Add boolean item as an argument to the given parser.\n\n        An exclusive group is created on the parser, which will add\n        a boolean-style command line argument to the parser.\n\n        Examples:\n            A non-nested boolean value with the name 'debug' will result\n            in a command-line argument like the following:\n\n            '--debug/--no-debug'\n\n        Args:\n            parser (argparse.ArgumentParser): The parser to add this item to.\n            bootstrap (bool): Flag to indicate whether you only want to mark\n                this item as required or not.\n        \"\"\"\n        tmp_default = self.default\n        exclusive_grp = parser.add_mutually_exclusive_group()\n        self.default = True\n        args = self._get_argparse_names(parser.prefix_chars)\n        kwargs = self._get_argparse_kwargs(bootstrap)\n        exclusive_grp.add_argument(*args, **kwargs)\n\n        self.default = False\n        args = self._get_argparse_names(parser.prefix_chars)\n        kwargs = self._get_argparse_kwargs(bootstrap)\n        exclusive_grp.add_argument(*args, **kwargs)\n\n        self.default = tmp_default"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert all Truthy and Falsy values to True and False.", "response": "def convert_config_value(self, value, label):\n        \"\"\"Converts all 'Truthy' values to True and 'Falsy' values to False.\n\n        Args:\n            value: Value to convert\n            label: Label of the config which this item was found.\n\n        Returns:\n\n        \"\"\"\n        if isinstance(value, six.string_types):\n            value = value.lower()\n\n        if value in self.TRUTHY_VALUES:\n            return True\n        elif value in self.FALSY_VALUES:\n            return False\n        else:\n            raise YapconfValueError(\"Cowardly refusing to interpret \"\n                                    \"config value as a boolean. Name: \"\n                                    \"{0}, Value: {1}\"\n                                    .format(self.name, value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the item as an argument to the given parser.", "response": "def add_argument(self, parser, bootstrap=False):\n        \"\"\"Add list-style item as an argument to the given parser.\n\n        Generally speaking, this works mostly like the normal append\n        action, but there are special rules for boolean cases. See the\n        AppendReplace action for more details.\n\n        Examples:\n            A non-nested list value with the name 'values' and a child name of\n            'value' will result in a command-line argument that will correctly\n            handle arguments like the following:\n\n            ['--value', 'VALUE1', '--value', 'VALUE2']\n\n        Args:\n            parser (argparse.ArgumentParser): The parser to add this item to.\n            bootstrap (bool): Flag to indicate whether you only want to mark\n                this item as required or not.\n        \"\"\"\n        if self.cli_expose:\n            if isinstance(self.child, YapconfBoolItem):\n                original_default = self.child.default\n\n                self.child.default = True\n                args = self.child._get_argparse_names(parser.prefix_chars)\n                kwargs = self._get_argparse_kwargs(bootstrap)\n                parser.add_argument(*args, **kwargs)\n\n                self.child.default = False\n                args = self.child._get_argparse_names(parser.prefix_chars)\n                kwargs = self._get_argparse_kwargs(bootstrap)\n                parser.add_argument(*args, **kwargs)\n\n                self.child.default = original_default\n            else:\n                super(YapconfListItem, self).add_argument(parser, bootstrap)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding dict - style item as an argument to the given parser.", "response": "def add_argument(self, parser, bootstrap=False):\n        \"\"\"Add dict-style item as an argument to the given parser.\n\n        The dict item will take all the nested items in the dictionary and\n        namespace them with the dict name, adding each child item as\n        their own CLI argument.\n\n        Examples:\n            A non-nested dict item with the name 'db' and children named\n            'port' and 'host' will result in the following being valid\n            CLI args:\n\n            ['--db-host', 'localhost', '--db-port', '1234']\n\n        Args:\n            parser (argparse.ArgumentParser): The parser to add this item to.\n            bootstrap (bool): Flag to indicate whether you only want to mark\n                this item as required or not.\n        \"\"\"\n        if self.cli_expose:\n            for child in self.children.values():\n                child.add_argument(parser, bootstrap)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_triple(self, subject, predicate, object):\n\n        def filter_ontid(ontid):\n            if ontid.startswith('http://'):\n                pass\n            elif ontid.prefix == 'ILXTEMP':\n                ontid = 'tmp_' + ontid.suffix\n            else:\n                ontid = 'ilx_' + ontid.suffix\n            return ontid\n\n        # this split between annotations and relationships is severely annoying\n        # because you have to know before hand which one it is (sigh)\n        s = OntId(subject)\n        p = OntId(predicate)\n        o = self._get_type(object)\n        if type(o) == str:\n            func = self.ilx_cli.add_annotation\n        elif type(o) == OntId:\n            func = self.ilx_cli.add_relationship\n            o = filter_ontid(o)\n        else:\n            raise TypeError(f'what are you giving me?! {object!r}')\n\n        s = filter_ontid(s)\n        p = filter_ontid(p)\n\n        resp = func(s, p, o)\n        return resp", "response": "Add a triple of curied or full iris to the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_triple(self, subject, predicate, object):\n\n        def filter_ontid(ontid):\n            if ontid.startswith('http://'):\n                pass\n            elif ontid.prefix == 'ILXTEMP':\n                ontid = 'tmp_' + ontid.suffix\n            else:\n                ontid = 'ilx_' + ontid.suffix\n            return ontid\n\n        # this split between annotations and relationships is severely annoying\n        # because you have to know before hand which one it is (sigh)\n        s = OntId(subject)\n        p = OntId(predicate)\n        o = self._get_type(object)\n        if type(o) == str:\n            func = self.ilx_cli.delete_annotation\n        elif type(o) == OntId:\n            func = self.ilx_cli.delete_relationship\n            o = filter_ontid(o)\n        else:\n            raise TypeError(f'what are you giving me?! {object!r}')\n\n        s = filter_ontid(s)\n        p = filter_ontid(p)\n\n        # TODO: check if add_relationship works\n        resp = func(s, p, o)\n        return resp", "response": "Delete a triple of curied or full iris."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sketch(*args, output_sketch='sketch.msh', threads=1, returncmd=False, **kwargs):\n    options = kwargs_to_string(kwargs)\n    if len(args) == 0:\n        raise ValueError('At least one file to sketch must be specified. You specified 0 files.')\n    cmd = 'mash sketch '\n    for arg in args:\n        cmd += arg + ' '\n    cmd += '-o {} -p {} {}'.format(output_sketch, str(threads), options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Wrapper for mash sketch."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the head link.", "response": "def build(self, link_type, path):\n        super(HeadLink, self).build()\n        \"\"\"\n    :param link_type: Link type\n    :param target: Link target\n    \"\"\"\n        self.target = path\n        self.link_type = link_type\n        self.autoclosing = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the page title.", "response": "def build(self, text):\n        super(PageTitle, self).build()\n        \"\"\"\n    :param text: Page title\n    \"\"\"\n        self.content = text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self, js_path):\n        super(Script, self).build()\n        \"\"\"\n    :param js_path: Javascript source code.\n    \"\"\"\n        self.source = js_path", "response": "Build the object from the javascript source code."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute function and logs time :param func: function to call :return: function result", "response": "def log_time(func):\n    \"\"\"Executes function and logs time\n\n    :param func: function to call\n    :return: function result\n    \"\"\"\n\n    @functools.wraps(func)\n    def _execute(*args, **kwargs):\n        \"\"\"Executes function and logs time\n\n        :param args: args of function\n        :param kwargs: extra args of function\n        :param *args: args\n        :param **kwargs: extra args\n        :return: function result\n        \"\"\"\n\n        func_name = get_method_name(func)\n        timer = Timer()\n        log_message(func_name, \"has started\")\n\n        with timer:\n            result = func(*args, **kwargs)\n\n        seconds = \"{:.3f}\".format(timer.elapsed_time())\n        log_message(func_name, \"has finished. Execution time:\", seconds, \"s\")\n\n        return result\n\n    return _execute"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload an analysis configuration from a file.", "response": "def load_configuration(yaml: yaml.ruamel.yaml.YAML, filename: str) -> DictLike:\n    \"\"\" Load an analysis configuration from a file.\n\n    Args:\n        yaml: YAML object to use in loading the configuration.\n        filename: Filename of the YAML configuration file.\n    Returns:\n        dict-like object containing the loaded configuration\n    \"\"\"\n    with open(filename, \"r\") as f:\n        config = yaml.load(f)\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the override options for a particular configuration.", "response": "def override_options(config: DictLike, selected_options: Tuple[Any, ...], set_of_possible_options: Tuple[enum.Enum, ...], config_containing_override: DictLike = None) -> DictLike:\n    \"\"\" Determine override options for a particular configuration.\n\n    The options are determined by searching following the order specified in selected_options.\n\n    For the example config,\n\n    .. code-block:: yaml\n\n        config:\n            value: 3\n            override:\n                2.76:\n                    track:\n                        value: 5\n\n    value will be assigned the value 5 if we are at 2.76 TeV with a track bias, regardless of the event\n    activity or leading hadron bias. The order of this configuration is specified by the order of the\n    selected_options passed. The above example configuration is from the jet-hadron analysis.\n\n    Since anchors aren't kept for scalar values, if you want to override an anchored value, you need to\n    specify it as a single value in a list (or dict, but list is easier). After the anchor values propagate,\n    single element lists can be converted into scalar values using ``simplify_data_representations()``.\n\n    Args:\n        config: The dict-like configuration from ruamel.yaml which should be overridden.\n        selected_options: The selected analysis options. They will be checked in the order with which\n            they are passed, so make certain that it matches the order in the configuration file!\n        set_of_possible_options (tuple of enums): Possible options for the override value categories.\n        config_containing_override: The dict-like config containing the override options in a map called\n            \"override\". If it is not specified, it will look for it in the main config.\n    Returns:\n        dict-like object: The updated configuration\n    \"\"\"\n    if config_containing_override is None:\n        config_containing_override = config\n    override_opts = config_containing_override.pop(\"override\")\n    override_dict = determine_override_options(selected_options, override_opts, set_of_possible_options)\n    logger.debug(f\"override_dict: {override_dict}\")\n\n    # Set the configuration values to those specified in the override options\n    # Cannot just use update() on config because we need to maintain the anchors.\n    for k, v in override_dict.items():\n        # Check if key is there and if it is not None! (The second part is important)\n        if k in config:\n            try:\n                # If it has an anchor, we know that we want to preserve the type. So we check for the anchor\n                # by trying to access it (Note that we don't actually care what the value is - just that it\n                # exists). If it fails with an AttributeError, then we know we can just assign the value. If it\n                # has an anchor, then we want to preserve the anchor information.\n                config[k].anchor\n                logger.debug(f\"type: {type(config[k])}, k: {k}\")\n                if isinstance(config[k], list):\n                    # Clear out the existing list entries\n                    del config[k][:]\n                    if isinstance(override_dict[k], (str, int, float, bool)):\n                        # We have to treat str carefully because it is an iterable, but it will be expanded as\n                        # individual characters if it's treated the same as a list, which is not the desired\n                        # behavior! If we wrap it in [], then it will be treated as the only entry in the list\n                        # NOTE: We also treat the basic types this way because they will be passed this way if\n                        #       overriding indirectly with anchors (since the basic scalar types don't yet support\n                        #       reassignment while maintaining their anchors).\n                        config[k].append(override_dict[k])\n                    else:\n                        # Here we just assign all entries of the list to all entries of override_dict[k]\n                        config[k].extend(override_dict[k])\n                elif isinstance(config[k], dict):\n                    # Clear out the existing entries because we are trying to replace everything\n                    # Then we can simply update the dict with our new values\n                    config[k].clear()\n                    config[k].update(override_dict[k])\n                elif isinstance(config[k], (int, float, bool)):\n                    # This isn't really very good (since we lose information), but there's nothing that can be done\n                    # about it at the moment (Dec 2018)\n                    logger.debug(\"Overwriting YAML anchor object. It is currently unclear how to reassign this value.\")\n                    config[k] = v\n                else:\n                    # Raise a value error on all of the cases that we aren't already aware of.\n                    raise ValueError(f\"Object {k} (type {type(config[k])}) somehow has an anchor, but is something other than a list or dict. Attempting to assign directly to it.\")\n            except AttributeError:\n                # If no anchor, just overwrite the value at this key\n                config[k] = v\n        else:\n            raise KeyError(k, f\"Trying to override key \\\"{k}\\\" that it is not in the config.\")\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsimplifies the data representation of a single entry list into a single entry list.", "response": "def simplify_data_representations(config: DictLike) -> DictLike:\n    \"\"\" Convert one entry lists to the scalar value\n\n    This step is necessary because anchors are not kept for scalar values - just for lists and dictionaries.\n    Now that we are done with all of our anchor references, we can convert these single entry lists to\n    just the scalar entry, which is more usable.\n\n    Some notes on anchors in ruamel.yaml are here: https://stackoverflow.com/a/48559644\n\n    Args:\n        config: The dict-like configuration from ruamel.yaml which should be simplified.\n    Returns:\n        The updated configuration.\n    \"\"\"\n    for k, v in config.items():\n        if v and isinstance(v, list) and len(v) == 1:\n            logger.debug(\"v: {}\".format(v))\n            config[k] = v[0]\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the iterable values that should be used to create objects for a given configuration.", "response": "def determine_selection_of_iterable_values_from_config(config: DictLike, possible_iterables: Mapping[str, Type[enum.Enum]]) -> Dict[str, List[Any]]:\n    \"\"\" Determine iterable values to use to create objects for a given configuration.\n\n    All values of an iterable can be included be setting the value to ``True`` (Not as a single value list,\n    but as the only value.). Alternatively, an iterator can be disabled by setting the value to ``False``.\n\n    Args:\n        config: The dict-like configuration from ruamel.yaml which should be overridden.\n        possible_iterables: Key value pairs of names of enumerations and their values.\n    Returns:\n        dict: Iterables values that were requested in the config.\n    \"\"\"\n    iterables = {}\n    requested_iterables = config[\"iterables\"]\n    for k, v in requested_iterables.items():\n        if k not in possible_iterables:\n            raise KeyError(k, f\"Cannot find requested iterable in possible_iterables: {possible_iterables}\")\n        logger.debug(f\"k: {k}, v: {v}\")\n        additional_iterable: List[Any] = []\n        enum_values = possible_iterables[k]\n        # Check for a string. This is wrong, and the user should be notified.\n        if isinstance(v, str):\n            raise TypeError(type(v), f\"Passed string {v} when must be either bool or list\")\n        # Allow the possibility to skip\n        if v is False:\n            continue\n        # Allow the possibility to including all possible values in the enum.\n        elif v is True:\n            additional_iterable = list(enum_values)\n        else:\n            if enum_values is None:\n                # The enumeration values are none, which means that we want to take\n                # all of the values defined in the config.\n                additional_iterable = list(v)\n            else:\n                # Otherwise, only take the requested values.\n                for el in v:\n                    additional_iterable.append(enum_values[el])\n        # Store for later\n        iterables[k] = additional_iterable\n\n    return iterables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _key_index_iter(self) -> Iterator[Tuple[str, Any]]:\n    for k, v in vars(self).items():\n        yield k, v", "response": "Returns an iterator over the KeyIndex names and values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_key_index_object(key_index_name: str, iterables: Dict[str, Any]) -> Any:\n    # Validation\n    # We are going to use the iterators when determining the fields, so we need to notify if an iterator was\n    # passed, as this will cause a problem later. Instead of passing an iterator, a iterable should be passed,\n    # which can recreate the iter.\n    # See: https://effectivepython.com/2015/01/03/be-defensive-when-iterating-over-arguments/\n    for name, iterable in iterables.items():\n        if iter(iterable) == iter(iterable):\n            raise TypeError(\n                f\"Iterable {name} is in iterator which can be exhausted. Please pass the iterable\"\n                f\" in a container that can recreate the iterable. See the comments here for more info.\"\n            )\n\n    # We need the types of the fields to create the dataclass. However, we are provided with iterables\n    # in the values of the iterables dict. Thus, we need to look at one value of each iterable, and use\n    # that to determine the type of that particular iterable. This is safe to do because the iterables\n    # must always have at least one entry (or else they wouldn't be one of the iterables).\n    # NOTE: The order here matters when we create the ``KeyIndex`` later, so we cannot just take all\n    #       objects from the iterables and blindly use set because set won't preserve the order.\n    fields = [(name, type(next(iter(iterable)))) for name, iterable in iterables.items()]\n    KeyIndex = dataclasses.make_dataclass(\n        key_index_name,\n        fields,\n        frozen = True\n    )\n    # Allow for iteration over the key index values\n    KeyIndex.__iter__ = _key_index_iter\n\n    return KeyIndex", "response": "Create a key index object based on the passed attributes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate objects for each set of values based on the given arguments.", "response": "def create_objects_from_iterables(obj, args: dict, iterables: Dict[str, Any], formatting_options: Dict[str, Any], key_index_name: str = \"KeyIndex\") -> Tuple[Any, Dict[str, Any], dict]:\n    \"\"\" Create objects for each set of values based on the given arguments.\n\n    The iterable values are available under a key index ``dataclass`` which is used to index the returned\n    dictionary. The names of the fields are determined by the keys of iterables dictionary. The values are\n    the newly created object. Note that the iterable values must be convertible to a str() so they can be\n    included in the formatting dictionary.\n\n    Each set of values is also included in the object args.\n\n    As a basic example,\n\n    .. code-block:: python\n\n        >>> create_objects_from_iterables(\n        ...     obj = obj,\n        ...     args = {},\n        ...     iterables = {\"a\" : [\"a1\",\"a2\"], \"b\" : [\"b1\", \"b2\"]},\n        ...     formatting_options = {}\n        ... )\n        (\n            KeyIndex,\n            {\"a\": [\"a1\", \"a2\"], \"b\": [\"b1\", \"b2\"]}\n            {\n                KeyIndex(a = \"a1\", b = \"b1\"): obj(a = \"a1\", b = \"b1\"),\n                KeyIndex(a = \"a1\", b = \"b2\"): obj(a = \"a1\", b = \"b2\"),\n                KeyIndex(a = \"a2\", b = \"b1\"): obj(a = \"a2\", b = \"b1\"),\n                KeyIndex(a = \"a2\", b = \"b2\"): obj(a = \"a2\", b = \"b2\"),\n            }\n        )\n\n    Args:\n        obj (object): The object to be constructed.\n        args: Arguments to be passed to the object to create it.\n        iterables: Iterables to be used to create the objects, with entries of the form\n            ``\"name_of_iterable\": iterable``.\n        formatting_options: Values to be used in formatting strings in the arguments.\n        key_index_name: Name of the iterable key index.\n    Returns:\n        (object, list, dict, dict): Roughly, (KeyIndex, iterables, objects). Specifically, the\n            key_index is a new dataclass which defines the parameters used to create the object, iterables\n            are the iterables used to create the objects, which names as keys and the iterables as values.\n            The objects dictionary keys are KeyIndex objects which describe the iterable arguments passed to the\n            object, while the values are the newly constructed arguments. See the example above.\n    \"\"\"\n    # Setup\n    objects = {}\n    names = list(iterables)\n    logger.debug(f\"iterables: {iterables}\")\n    # Create the key index object, where the name of each field is the name of each iterable.\n    KeyIndex = create_key_index_object(\n        key_index_name = key_index_name,\n        iterables = iterables,\n    )\n    # ``itertools.product`` produces all possible permutations of the iterables values.\n    # NOTE: Product preserves the order of the iterables values, which is important for properly\n    #       assigning the values to the ``KeyIndex``.\n    for values in itertools.product(*iterables.values()):\n        logger.debug(f\"Values: {values}\")\n        # Skip if we don't have a sufficient set of values to create an object.\n        if not values:\n            continue\n\n        # Add in the values into the arguments and formatting options.\n        # NOTE: We don't need a deep copy for the iterable values in the args and formatting options\n        #       because the values will be overwritten for each object.\n        for name, val in zip(names, values):\n            # We want to keep the original value for the arguments.\n            args[name] = val\n            # Here, we convert the value, regardless of type, into a string that can be displayed.\n            formatting_options[name] = str(val)\n\n        # Apply formatting options\n        # If we formatted in place, we would need to deepcopy the args to ensure that the iterable dependent\n        # values in the formatted values are properly set for each iterable object individually.\n        # However, by formatting into new variables, we can avoid a deepcopy, which greatly improves performance!\n        # NOTE: We don't need a deep copy do this for iterable value names themselves because they will be overwritten\n        #       for each object. They are set in the block above.\n        object_args = copy.copy(args)\n        logger.debug(f\"object_args pre format: {object_args}\")\n        object_args = apply_formatting_dict(object_args, formatting_options)\n        # Print our results for debugging purposes. However, we skip printing the full\n        # config because it is quite long\n        print_args = {k: v for k, v in object_args.items() if k != \"config\"}\n        print_args[\"config\"] = \"...\"\n        logger.debug(f\"Constructing obj \\\"{obj}\\\" with args: \\\"{print_args}\\\"\")\n\n        # Finally create the object.\n        objects[KeyIndex(*values)] = obj(**object_args)\n\n    # If nothing has been created at this point, then we are didn't iterating over anything and something\n    # has gone wrong.\n    if not objects:\n        raise ValueError(iterables, \"There appear to be no iterables to use in creating objects.\")\n\n    return (KeyIndex, iterables, objects)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over an analysis dictionary with selected attributes.", "response": "def iterate_with_selected_objects(analysis_objects: Mapping[Any, Any], **selections: Mapping[str, Any]) -> Iterator[Tuple[Any, Any]]:\n    \"\"\" Iterate over an analysis dictionary with selected attributes.\n\n    Args:\n        analysis_objects: Analysis objects dictionary.\n        selections: Keyword arguments used to select attributes from the analysis dictionary.\n    Yields:\n        object: Matching analysis object.\n    \"\"\"\n    for key_index, obj in analysis_objects.items():\n        # If selections is empty, we return every object. If it's not empty, then we only want to return\n        # objects which are selected in through the selections.\n        selected_obj = not selections or all([getattr(key_index, selector) == selected_value for selector, selected_value in selections.items()])\n\n        if selected_obj:\n            yield key_index, obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iterate_with_selected_objects_in_order(analysis_objects: Mapping[Any, Any],\n                                           analysis_iterables: Dict[str, Sequence[Any]],\n                                           selection: Union[str, Sequence[str]]) -> Iterator[List[Tuple[Any, Any]]]:\n    \"\"\" Iterate over an analysis dictionary, yielding the selected attributes in order.\n\n    So if there are three iterables, a, b, and c, if we selected c, then we iterate over a and b,\n    and return c in the same order each time for each set of values of a and b. As an example, consider\n    the set of iterables:\n\n    .. code-block:: python\n\n        >>> a = [\"a1\", \"a2\"]\n        >>> b = [\"b1\", \"b2\"]\n        >>> c = [\"c1\", \"c2\"]\n\n    then it will effectively return:\n\n    .. code-block:: python\n\n        >>> for a_val in a:\n        ...     for b_val in b:\n        ...         for c_val in c:\n        ...             obj(a_val, b_val, c_val)\n\n    This will yield:\n\n    .. code-block:: python\n\n        >>> output = list(iterate_with_selected_objects_in_order(..., selection = [\"a\"]))\n        [[(\"a1\", \"b1\", \"c1\"), (\"a2\", \"b1\", \"c1\")], [(\"a1\", \"b2\", \"c1\"), (\"a2\", \"b2\", \"c1\")], ...]\n\n    This is particularly nice because we can then select on a set of iterables to be returned without\n    having to specify the rest of the iterables that we don't really care about.\n\n    Args:\n        analysis_objects: Analysis objects dictionary.\n        analysis_iterables: Iterables used in constructing the analysis objects.\n        selection: Selection of analysis selections to return. Can be either a string or a sequence of\n            selections.\n    Yields:\n        object: Matching analysis object.\n    \"\"\"\n    # Validation\n    if isinstance(selection, str):\n        selection = [selection]\n    # Help out mypy. We don't check if it is a list to allow for other sequences.\n    assert not isinstance(selection, str)\n    # We don't want to impact the original analysis iterables when we pop some values below.\n    analysis_iterables = copy.copy(analysis_iterables)\n\n    # Extract the selected iterators from the possible iterators so we can select on them later.\n    # First, we want want each set of iterators to be of the form:\n    # {\"selection1\": [value1, value2, ...], \"selection2\": [value3, value4, ...]}\n    selected_iterators = {}\n    for s in selection:\n        selected_iterators[s] = analysis_iterables.pop(s)\n\n    logger.debug(f\"Initial analysis_iterables: {analysis_iterables}\")\n    logger.debug(f\"Initial selected_iterators: {selected_iterators}\")\n\n    # Now, we convert them to the form:\n    # [[(\"selection1\", value1), (\"selection1\", value2)], [(\"selection2\", value3), (\"selection2\", value4)]]\n    # This allows them to iterated over conveniently via itertools.product(...)\n    selected_iterators = [[(k, v) for v in values] for k, values in selected_iterators.items()]  # type: ignore\n    analysis_iterables = [[(k, v) for v in values] for k, values in analysis_iterables.items()]  # type: ignore\n\n    logger.debug(f\"Final analysis_iterables: {analysis_iterables}\")\n    logger.debug(f\"Final selected_iterators: {selected_iterators}\")\n    # Useful debug information, but too verbose for standard usage.\n    #logger.debug(f\"analysis_iterables product: {list(itertools.product(*analysis_iterables))}\")\n    #logger.debug(f\"selected_iterators product: {list(itertools.product(*selected_iterators))}\")\n\n    for values in itertools.product(*analysis_iterables):\n        selected_analysis_objects = []\n        for selected_values in itertools.product(*selected_iterators):\n            for key_index, obj in analysis_objects.items():\n                selected_via_analysis_iterables = all(\n                    getattr(key_index, k) == v for k, v in values\n                )\n                selected_via_selected_iterators = all(\n                    getattr(key_index, k) == v for k, v in selected_values\n                )\n                selected_obj = selected_via_analysis_iterables and selected_via_selected_iterators\n\n                if selected_obj:\n                    selected_analysis_objects.append((key_index, obj))\n\n        logger.debug(f\"Yielding: {selected_analysis_objects}\")\n        yield selected_analysis_objects", "response": "Iterate over an analysis dictionary yielding the selected attributes in order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering ist of post_id for new ones.", "response": "async def filter_new_posts(self, source_id, post_ids):\n        \"\"\"Filters ist of post_id for new ones.\n\n        :param source_id: id of the source\n        :type string:\n        :param post_ids: list of post ids\n        :type list:\n        :returns: list of unknown post ids.\"\"\"\n        new_ids = []\n        try:\n            db_client = self._db\n            posts_in_db = await db_client.get_known_posts(source_id, post_ids)\n            new_ids = [p for p in post_ids if p not in posts_in_db]\n        except Exception as exc:\n            logger.error(\"Error when filtering for new posts {} {}\".format(source_id, post_ids))\n            logger.exception(exc)\n        return new_ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the last update - timestamp from storage for source.", "response": "async def get_last_updated(self, source_id):\n        \"\"\"Returns latest update-timestamp from storage for source.\n\n        :param source_id: id of the source (source_id, ticker_id, blog_id pp)\n        :type string:\n        :returns: :py:class:`datetime.datetime` object of latest update datetime in db.\"\"\"\n        last_updated = await self._db.get_last_updated(source_id)\n        logger.info(\"LAST UPDATED: {} {}\".format(last_updated, self))\n        return last_updated"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clearParameters(self):\n        self.beginRemoveRows(QtCore.QModelIndex(), 0, self.rowCount())\n        self.model.clear_parameters()\n        self.endRemoveRows()", "response": "Removes all parameters from model"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data for the item at the given index", "response": "def data(self, index, role=QtCore.Qt.UserRole):\n        \"\"\"Used by the view to determine data to present\n\n        See :qtdoc:`QAbstractItemModel<QAbstractItemModel.data>`, \n        and :qtdoc:`subclassing<qabstractitemmodel.subclassing>`\n        \"\"\"\n        if role == QtCore.Qt.DisplayRole:\n            row = index.row()\n            field = self._headers[index.column()]\n            val = self.model.paramValue(row, field)\n            if 1 <= index.column() <= 3:\n                # standard units for data, not necessary current for UI\n                # view will scale and convert appropriately\n                unit = self.model.getDetail(index.row(), 'unit')\n                if val is not None and unit is not None:\n                    return str(val) + ' ' + unit\n                else:\n                    return val\n            else:\n                return val\n\n        elif role == QtCore.Qt.EditRole:\n            row = index.row()\n            field = self._headers[index.column()]\n            return self.model.paramValue(row, field)\n            # return self.model.paramValue(row, field)\n        elif role == QtCore.Qt.ForegroundRole:\n            # color the background red for bad values\n            if not self.checkValidCell(index):\n                return QtGui.QBrush(ERRCELL)\n        elif role == QtCore.Qt.FontRole:\n            # color the background red for bad values\n            if not self.checkValidCell(index):\n                f = QtGui.QFont()\n                f.setWeight(QtGui.QFont.Bold)\n                return f\n\n        elif role == QtCore.Qt.UserRole or role == AbstractDragView.DragRole:  #return the whole python object\n            param = self.model.param(index.row())\n            for comp in param['selection']:\n                comp.clean()\n            return param\n\n        elif role == self.SelectionModelRole:\n            # may need to translate to QModelIndexes\n            return self.model.selection(self.model.param(index.row()))\n        \n        elif role == CursorRole:\n            col = index.column()\n            if not index.isValid():\n                return QtGui.QCursor(QtCore.Qt.ArrowCursor)\n            elif col == 0:\n                return cursors.pointyHand()\n            elif col < 4:\n                return cursors.handEdit()\n            else:\n                return cursors.openHand()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setData(self, index, value, role=QtCore.Qt.UserRole):\n        if role == QtCore.Qt.EditRole:\n            if isinstance(value, QtCore.QVariant):\n                value = value.toPyObject()\n            elif isinstance(value, QtCore.QString):\n                value = str(value)\n            self.model.setVerifiedValue(index.row(), self._headers[index.column()], value)\n            self.countChanged.emit()\n        elif role == QtCore.Qt.UserRole:\n            row = index.row()\n            if row == -1:\n                row = self.rowCount() -1\n            self.model.overwriteParam(row, value)\n        return True", "response": "Sets the data at index to value in underlying data structure"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nask the model if the value at index * is valid", "response": "def checkValidCell(self, index):\n        \"\"\"Asks the model if the value at *index* is valid\n\n        See :meth:`isFieldValid<sparkle.stim.auto_parameter_model.AutoParameterModel.isFieldValid>`\n        \"\"\"\n        col = index.column()\n        row = index.row()\n        return self.model.isFieldValid(row, self._headers[index.column()])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insertRows(self, position, rows, parent = QtCore.QModelIndex()):\n        self.beginInsertRows(parent, position, position + rows - 1)\n        for i in range(rows):\n            self.model.insertRow(position)\n            # self._selectionmap[self._paramid].hintRequested.connect(self.hintRequested)\n        self.endInsertRows()\n        if self.rowCount() == 1:\n            self.emptied.emit(False)\n        return True", "response": "Inserts new parameters and emits an emptied False signal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving parameters from the model.", "response": "def removeRows(self, position, rows, parent = QtCore.QModelIndex()):\n        \"\"\"Removes parameters from the model. Emits and emptied True signal, if there are no parameters left.\n\n        :param position: row location of parameters to remove\n        :type position: int\n        :param rows: number of parameters to remove\n        :type rows: int\n        :param parent: Required by QAbstractItemModel, can be safely ignored\n        \"\"\"\n        self.beginRemoveRows(parent, position, position + rows - 1)\n        for i in range(rows):\n            self.model.removeRow(position)\n            # cannot purge selection model, or else we have no way of \n            # recovering it when reordering\n        self.endRemoveRows()\n        if self.rowCount() == 0:\n            self.emptied.emit(True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insertItem(self, index, item):\n        row = index.row()\n        self.beginInsertRows(QtCore.QModelIndex(), row, row)\n        self.model.insertRow(row)\n        self.endInsertRows()\n        self.model.overwriteParam(index.row(), item)", "response": "Inserts parameter item at index"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine interaction allowed with table cells.", "response": "def flags(self, index):\n        \"\"\"\"Determines interaction allowed with table cells.\n\n        See :qtdoc:`QAbstractItemModel<QAbstractItemModel.flags>`, \n        and :qtdoc:`subclassing<qabstractitemmodel.subclassing>`\n        \"\"\"\n        if index.isValid():\n            if self.model.editableRow(index.row()) and index.column() < 4:\n                return QtCore.Qt.ItemIsDragEnabled | \\\n                       QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable | \\\n                       QtCore.Qt.ItemIsEditable\n            else:\n                return QtCore.Qt.ItemIsSelectable | QtCore.Qt.ItemIsEnabled\n        else:\n            print 'flags: index invalid'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toggleSelection(self, index, comp):\n        self.model.toggleSelection(index.row(), comp)", "response": "Toggles a component in or out of the currently \n        selected parameter s compnents list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to replicate the Illumina rules to create file names from Sample_Name", "response": "def samplenamer(listofdata, indexposition=0):\n    \"\"\"Tries to replicate the Illumina rules to create file names from 'Sample_Name'\n    :param listofdata: a list of data extracted from a file\n    :param indexposition:\n    \"\"\"\n    samplename = listofdata[indexposition].rstrip().replace(\" \", \"-\").replace(\".\", \"-\").replace(\"=\", \"-\")\\\n        .replace(\"+\", \"\").replace(\"/\", \"-\").replace(\"#\", \"\").replace(\"---\", \"-\").replace(\"--\", \"-\")\n    return samplename"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parseruninfo(self):\n        # Check if the RunInfo.xml file is provided, otherwise, yield N/A\n        try:\n            runinfo = ElementTree.ElementTree(file=self.runinfo)\n            # Get the run id from the\n            for elem in runinfo.iter():\n                for run in elem:\n                    try:\n                        self.runid = run.attrib['Id']\n                        self.runnumber = run.attrib['Number']\n                    except KeyError:\n                        break\n            # pull the text from flowcell and instrument values using the .iter(tag=\"X\") function\n            for elem in runinfo.iter(tag=\"Flowcell\"):\n                self.flowcell = elem.text\n            for elem in runinfo.iter(tag=\"Instrument\"):\n                self.instrument = elem.text\n        except IOError:\n            pass\n        # Extract run statistics from either GenerateRunStatistics.xml or indexingQC.txt\n        self.parserunstats()", "response": "Extracts the flowcell ID and instrument name from RunInfo. xml."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the sample sheet to determine certain values in the assembly report", "response": "def parsesamplesheet(self):\n        \"\"\"Parses the sample sheet (SampleSheet.csv) to determine certain values\n        important for the creation of the assembly report\"\"\"\n        # Open the sample sheet\n        with open(self.samplesheet, \"r\") as samplesheet:\n            # Iterate through the sample sheet\n            samples, prev, header = False, 0, []\n            for count, line in enumerate(samplesheet):\n                # Remove new lines, and split on commas\n                # line = line.decode('utf-8')  # Turn from bytes to string, since python3 is finicky.\n                data = line.rstrip().split(\",\")\n                if any(data):\n                    if \"[Settings]\" in line:\n                        samples = False\n                    if not line.startswith(\"[\") and not samples and not data == ['']:\n                        # Grab an data not in the [Data] Section\n                        setattr(self.header, data[0].replace(\" \", \"\"), \"\".join(data[1:]))\n                    elif \"[Data]\" in line or \"[Reads]\" in line:\n                        samples = True\n                    elif samples and \"Sample_ID\" in line:\n                        header.extend([x.replace(\"_\", \"\").replace(' ', \"\") for x in data])\n                        prev = count\n                    elif header:\n                        # Try and replicate the Illumina rules to create file names from \"Sample_Name\"\n                        samplename = samplenamer(data)\n                        # Create an object for storing nested static variables\n                        strainmetadata = MetadataObject()\n                        # Set the sample name in the object\n                        strainmetadata.name = samplename\n                        # Add the header object to strainmetadata\n                        # strainmetadata.__setattr__(\"run\", GenObject(dict(self.header)))\n                        strainmetadata.run = GenObject(copy.copy(self.header.datastore))\n                        # Create the run object, so it will be easier to populate the object (eg run.SampleName = ...\n                        # instead of strainmetadata.run.SampleName = ...\n                        run = strainmetadata.run\n                        # Capture Sample_ID, Sample_Name, I7_Index_ID, index1, I5_Index_ID,\tindex2, Sample_Project\n                        for idx, item in enumerate(data):\n                            setattr(run, header[idx], item) if item else setattr(run, header[idx], \"NA\")\n                        # Add the sample number\n                        run.SampleNumber = count - prev\n                        # Create the 'General' category for strainmetadata\n                        strainmetadata.general = GenObject({'outputdirectory': os.path.join(self.path, samplename),\n                                                            'pipelinecommit': self.commit})\n                        strainmetadata.general.logout = os.path.join(self.path, samplename,\n                                                                     '{}_log_out.txt'.format(samplename))\n                        strainmetadata.general.logerr = os.path.join(self.path, samplename,\n                                                                     '{}_log_err.txt'.format(samplename))\n                        # Add the output directory to the general category\n                        # Append the strainmetadata object to a list\n                        self.samples.append(strainmetadata)\n                    elif samples:\n                        setattr(self.header, 'forwardlength', data[0]) \\\n                            if 'forwardlength' not in self.header.datastore else \\\n                            setattr(self.header, 'reverselength', data[0])\n                        self.totalreads += int(data[0])\n        self.date = self.header.Date if \"Date\" in self.header.datastore else self.date\n        for sample in self.samples:\n            if 'InvestigatorName' not in sample.run.datastore:\n                sample.run.InvestigatorName = 'NA'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parserunstats(self):\n        # metadata = GenObject()\n        # If the default file GenerateFASTQRunStatistics.xml is present, parse it\n        if os.path.isfile(os.path.join(self.path, \"GenerateFASTQRunStatistics.xml\")):\n            # Create a list of keys for which values are to be extracted\n            datalist = [\"SampleNumber\", \"SampleID\", \"SampleName\", \"NumberOfClustersPF\"]\n            # Load the file as an xml ElementTree object\n            runstatistics = ElementTree.ElementTree(file=os.path.join(self.path, \"GenerateFASTQRunStatistics.xml\"))\n            # Iterate through all the elements in the object\n            # .iterfind() allow for the matching and iterating though matches\n            # This is stored as a float to allow subsequent calculations\n            tclusterspf = [float(element.text) for element in runstatistics.iterfind(\"RunStats/NumberOfClustersPF\")][0]\n            # Iterate through all the elements (strains) in the OverallSamples/SummarizedSampleStatistics category\n            for element in runstatistics.iterfind(\"OverallSamples/SummarizedSampleStatistics\"):\n                # List comprehension. Essentially iterate through each element for each category in datalist:\n                # (element.iter(category) and pull out the value for nestedelement\n                straindata = [nestedelement.text for category in datalist for nestedelement in element.iter(category)]\n                # Try and replicate the Illumina rules to create file names from \"Sample_Name\"\n                samplename = samplenamer(straindata, 1)\n                # Calculate the percentage of clusters associated with each strain\n                # noinspection PyTypeChecker\n                percentperstrain = \"{:.2f}\".format((float(straindata[3]) / tclusterspf * 100))\n                try:\n                    # Use the sample number -1 as the index in the list of objects created in parsesamplesheet\n                    strainindex = int(straindata[0]) - 1\n                    # Set run to the .run object of self.samples[index]\n                    run = self.samples[strainindex].run\n                    # An assertion that compares the sample computed above to the previously entered sample name\n                    # to ensure that the samples are the same\n                    assert self.samples[strainindex].name == samplename, \\\n                        \"Sample name does not match object name {0!r:s}\".format(straindata[1])\n                    # Add the appropriate values to the strain metadata object\n                    run.SampleNumber = straindata[0]\n                    run.NumberofClustersPF = straindata[3]\n                    run.TotalClustersinRun = tclusterspf\n                    run.PercentOfClusters = percentperstrain\n                    run.flowcell = self.flowcell\n                    run.instrument = self.instrument\n                except IndexError:\n                    pass\n        else:\n            strainindex = 0\n            for i in range(len(self.samples)):\n                # Set run to the .run object of self.samples[index]\n                run = self.samples[strainindex].run\n                # Update the object with the variables\n                run.SampleNumber = strainindex + 1\n                run.NumberofClustersPF = 'NA'\n                run.TotalClustersinRun = 'NA'\n                run.PercentOfClusters = 'NA'\n                run.flowcell = self.flowcell\n                run.instrument = self.instrument\n                strainindex += 1", "response": "Parses the XML run statistics file and creates a GenObject containing the information needed to create the object that is used to store the original data in the Indexing QC tab of the run on Basespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_lines(self):\n        with open(self.path, \"r\") as data:\n            self.lines = data.readlines()  # store data in arrays\n\n        return self.lines", "response": "Gets lines in file\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_matrix(self):\n        data = []\n        with open(self.path, encoding=self.encoding) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=\",\", quotechar=\"\\\"\")\n            for row in csv_reader:\n                data.append(row)\n\n        return data", "response": "Stores values in array store lines in array\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets dicts in file", "response": "def get_dicts(self):\n        \"\"\"Gets dicts in file\n\n        :return: (generator of) of dicts with data from .csv file\n        \"\"\"\n        reader = csv.DictReader(open(self.path, \"r\", encoding=self.encoding))\n        for row in reader:\n            if row:\n                yield row"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget currency by ID", "response": "def get_by_id(self, id_code: str) -> Currency or None:\n        \"\"\" Get currency by ID\n\n        :param id_code: set, like \"R01305\"\n        :return: currency or None.\n        \"\"\"\n        try:\n            return [_ for _ in self.currencies if _.id == id_code][0]\n        except IndexError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stimuli_models():\n    package_path = os.path.dirname(__file__)\n\n    mod = '.'.join(get_stimuli_models.__module__.split('.'))\n    if mod == '__main__':\n        mod = ''\n    else:\n        mod = mod + '.'\n\n    module_files = glob.glob(package_path+os.sep+'[a-zA-Z]*.py')\n    module_names = [os.path.splitext(os.path.basename(x))[0] for x in module_files]\n\n    module_paths = [mod+x for x in module_names]\n    modules = [__import__(x, fromlist=['*']) for x in module_paths]\n \n    stimuli = []\n    for module in modules:\n        for name, attr in module.__dict__.iteritems():\n            #test if attr is subclass of AbstractStim\n            if type(attr) == type and issubclass(attr, AbstractStimulusComponent):\n                # print 'found subclass', name, '!!!'\n                stimuli.append(attr)\n\n    # print stimuli\n    return stimuli", "response": "Returns all subclasses of AbstractStimulusComponent in python files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_wait_time(text: str) -> int:\n    val = RATELIMIT.findall(text)\n    if len(val) > 0:\n        try:\n            res = val[0]\n            if res[1] == 'minutes':\n                return int(res[0]) * 60\n\n            if res[1] == 'seconds':\n                return int(res[0])\n        except Exception as e:\n            util_logger.warning('Could not parse ratelimit: ' + str(e))\n    return 1 * 60", "response": "Parse the waiting time from the exception"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_rate_limit(func: Callable[[Any], Any], *args, **kwargs) -> Any:\n    error_count = 0\n    while True:\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            if error_count > 3:\n                util_logger.error('Retried to call <{}> 3 times without success. '\n                                  'Continuing without calling it.'.format(func.__name__))\n                break\n\n            if 'DELETED_COMMENT' in str(e):\n                util_logger.warning('The comment has been deleted. '\n                                    'Function <{}> was not executed.'.format(func.__name__))\n                break\n            wait = parse_wait_time(str(e))\n            util_logger.error(e)\n            util_logger.warning('Waiting ~{} minutes'.format(round(float(wait + 30) / 60)))\n            time.sleep(wait + 30)\n            error_count += 1", "response": "Calls the given function with given arguments and handles rate limit exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_comment_depth(comment: praw.models.Comment, max_depth=3) -> bool:\n    count = 0\n    while not comment.is_root:\n        count += 1\n        if count > max_depth:\n            return False\n\n        comment = comment.parent()\n\n    return True", "response": "Checks if a comment is in a allowed depth range between 0 and max_depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the reddit session by reading the credentials from the file at creds_path.", "response": "def init_reddit(creds_path='creds.props') -> praw.Reddit:\n    \"\"\"Initialize the reddit session by reading the credentials from the file at :code:`creds_path`.\n\n    :param creds_path: Properties file with the credentials.\n\n    **Example file**::\n\n        client_id=CLIENT_ID\n        client_secret=CLIENT_SECRET\n        password=PASSWORD\n        user_agent=USER_AGENT\n        username=USERNAME\n    \"\"\"\n    with open(creds_path) as f:\n        prop_lines = [l.replace('\\n','').split('=') for l in f.readlines()]\n        f.close()\n        props = {l[0]: l[1] for l in prop_lines}\n        return praw.Reddit(**props)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_subs(subs_file='subreddits.txt', blacklist_file='blacklist.txt') -> List[str]:\n    # Get subs and blacklisted subs\n    subsf = open(subs_file)\n    blacklf = open(blacklist_file)\n    subs = [b.lower().replace('\\n','') for b in subsf.readlines()]\n    blacklisted = [b.lower().replace('\\n','') for b in blacklf.readlines()]\n    subsf.close()\n    blacklf.close()\n\n    # Filter blacklisted\n    subs_filtered = list(sorted(set(subs).difference(set(blacklisted))))\n    return subs_filtered", "response": "Get subs based on a file of subreddits and a file of blacklisted subreddits."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the alignment with the alphabet used in GaussDCA", "response": "def load_a3m(fasta, max_gap_fraction=0.9):\n    \"\"\" load alignment with the alphabet used in GaussDCA \"\"\"\n    mapping = {'-': 21, 'A': 1, 'B': 21, 'C': 2, 'D': 3, 'E': 4, 'F': 5,\n               'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10, 'M': 11,\n               'N': 12, 'O': 21, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17,\n               'V': 18, 'W': 19, 'Y': 20,\n               'U': 21, 'Z': 21, 'X': 21, 'J': 21}\n\n    # We want to exclude the lowercase, not ignore the uppercase because of gaps.\n    lowercase = set('abcdefghijklmnopqrstuvwxyz')\n\n    # Figure out the length of the sequence\n    f = open(fasta)\n    for line in f:\n        if line.startswith('>'):\n            continue\n        seq_length = len(line.strip())\n        break\n    else:\n        raise RuntimeError('I cannot find the first sequence')\n    f.seek(0)\n\n    parsed = []\n    for line in f:\n        if line.startswith('>'):\n            continue\n        line = line.strip()\n        gap_fraction = line.count('-') / seq_length\n        if gap_fraction <= max_gap_fraction:\n            parsed.append([mapping.get(ch, 22) for ch in line\n                           if ch not in lowercase])\n\n    return np.array(parsed, dtype=np.int8).T"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the necessary methods in the correct order and create the reports", "response": "def main(self):\n        \"\"\"\n        Run the necessary methods in the correct order\n        \"\"\"\n        if not os.path.isfile(self.gdcs_report):\n            logging.info('Starting {} analysis pipeline'.format(self.analysistype))\n            # Run the analyses\n            ShortKSippingMethods(self, self.cutoff)\n            # Create the reports\n            self.reporter()\n        else:\n            self.report_parse()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates and run the plasmid extractor system call", "response": "def run_plasmid_extractor(self):\n        \"\"\"\n        Create and run the plasmid extractor system call\n        \"\"\"\n        logging.info('Extracting plasmids')\n        # Define the system call\n        extract_command = 'PlasmidExtractor.py -i {inf} -o {outf} -p {plasdb} -d {db} -t {cpus} -nc' \\\n            .format(inf=self.path,\n                    outf=self.plasmid_output,\n                    plasdb=os.path.join(self.plasmid_db, 'plasmid_db.fasta'),\n                    db=self.plasmid_db,\n                    cpus=self.cpus)\n        # Only attempt to extract plasmids if the report doesn't already exist\n        if not os.path.isfile(self.plasmid_report):\n            # Run the system calls\n            out, err = run_subprocess(extract_command)\n            # Acquire thread lock, and write the logs to file\n            self.threadlock.acquire()\n            write_to_logfile(extract_command, extract_command, self.logfile)\n            write_to_logfile(out, err, self.logfile)\n            self.threadlock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_report(self):\n        logging.info('Parsing Plasmid Extractor outputs')\n        # A dictionary to store the parsed excel file in a more readable format\n        nesteddictionary = dict()\n        # Use pandas to read in the CSV file, and convert the pandas data frame to a dictionary (.to_dict())\n        dictionary = pandas.read_csv(self.plasmid_report).to_dict()\n        # Iterate through the dictionary - each header from the CSV file\n        for header in dictionary:\n            # Sample is the primary key, and value is the value of the cell for that primary key + header combination\n            for sample, value in dictionary[header].items():\n                # Update the dictionary with the new data\n                try:\n                    nesteddictionary[sample].update({header: value})\n                # Create the nested dictionary if it hasn't been created yet\n                except KeyError:\n                    nesteddictionary[sample] = dict()\n                    nesteddictionary[sample].update({header: value})\n        # Get the results into the metadata object\n        for sample in self.metadata:\n            # Initialise the plasmid extractor genobject\n            setattr(sample, self.analysistype, GenObject())\n            # Initialise the list of all plasmids\n            sample[self.analysistype].plasmids = list()\n            # Iterate through the dictionary of results\n            for line in nesteddictionary:\n                # Extract the sample name from the dictionary in a manner consistent with the rest of the COWBAT\n                # pipeline e.g. 2014-SEQ-0276_S2_L001 becomes 2014-SEQ-0276\n                sample_name = nesteddictionary[line]['Sample']\n                # Use the filer method to extract the name\n                name = list(filer([sample_name]))[0]\n                # Ensure that the names match\n                if name == sample.name:\n                    # Append the plasmid name extracted from the dictionary to the list of plasmids\n                    sample[self.analysistype].plasmids.append(nesteddictionary[line]['Plasmid'])\n        # Copy the report to the folder containing all reports for the pipeline\n        try:\n            shutil.copyfile(self.plasmid_report, os.path.join(self.reportpath, 'plasmidReport.csv'))\n        except IOError:\n            pass", "response": "Parse the plasmid extractor report and populate the metadata objects with the results of the pipeline"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the analyses in the correct order and create the reports and reports.", "response": "def runner(self):\n        \"\"\"\n        Run the necessary methods in the correct order\n        \"\"\"\n        sero_report = os.path.join(self.reportpath, 'serosippr.csv')\n        if os.path.isfile(sero_report):\n            self.report_parse(sero_report)\n        else:\n            logging.info('Starting {} analysis pipeline'.format(self.analysistype))\n            # Run the analyses\n            ShortKSippingMethods(self, self.cutoff)\n            self.serotype_escherichia()\n            self.serotype_salmonella()\n            # Create the reports\n            self.reporter()\n            # Print the metadata\n            MetadataPrinter(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the methods in the correct order for pipelines", "response": "def main(self):\n        \"\"\"\n        Run the methods in the correct order for pipelines\n        \"\"\"\n        # Find the target files\n        self.targets()\n        kmer = 15 if self.analysistype == 'GDCS' else 17\n        # Use bbduk to bait the FASTQ reads matching the target sequences\n        self.bait(maskmiddle='t', k=kmer)\n        # If desired, use bbduk to bait the target sequences with the previously baited FASTQ files\n        if self.revbait:\n            self.reversebait(maskmiddle='t', k=kmer)\n        # Run the bowtie2 read mapping module\n        self.mapping()\n        # Use samtools to index the sorted bam file\n        self.indexing()\n        # Parse the results\n        # self.parsing()\n        self.parsebam()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a report of the results of the current runmetadata", "response": "def reporter(self):\n        \"\"\"\n        Creates a report of the results\n        \"\"\"\n        logging.info('Creating {at} report'.format(at=self.analysistype))\n        resistance_classes = ResistanceNotes.classes(self.targetpath)\n        # Find unique gene names with the highest percent identity\n        for sample in self.runmetadata.samples:\n            try:\n                if sample[self.analysistype].results:\n                    # Initialise a dictionary to store the unique genes, and their percent identities\n                    sample[self.analysistype].uniquegenes = dict()\n                    for name, identity in sample[self.analysistype].results.items():\n                        # Split the name of the gene from the string e.g. ARR-2_1_HQ141279 yields ARR-2\n                        genename = name.split('_')[0]\n                        # Set the best observed percent identity for each unique gene\n                        try:\n                            # Pull the previous best identity from the dictionary\n                            bestidentity = sample[self.analysistype].uniquegenes[genename]\n                            # If the current identity is better than the old identity, save it\n                            if float(identity) > float(bestidentity):\n                                sample[self.analysistype].uniquegenes[genename] = float(identity)\n                        # Initialise the dictionary if necessary\n                        except KeyError:\n                            sample[self.analysistype].uniquegenes[genename] = float(identity)\n            except AttributeError:\n                pass\n        # Create the path in which the reports are stored\n        make_path(self.reportpath)\n        # Initialise strings to store the results\n        data = 'Strain,Resistance,Gene,Allele,Accession,PercentIdentity,Length,FoldCoverage\\n'\n        with open(os.path.join(self.reportpath, self.analysistype + '.csv'), 'w') as report:\n            for sample in self.runmetadata.samples:\n                # Create an attribute to store the string for the eventual pipeline report\n                sample[self.analysistype].pipelineresults = list()\n                data += sample.name + ','\n                if sample[self.analysistype].results:\n                    # If there are multiple results for a sample, don't write the name in each line of the report\n                    multiple = False\n                    for name, identity in sorted(sample[self.analysistype].results.items()):\n                        # Extract the necessary variables from the gene name string\n                        gname, genename, accession, allele = ResistanceNotes.gene_name(name)\n                        # Retrieve the best identity for each gene\n                        try:\n                            percentid = sample[self.analysistype].uniquegenes[gname]\n                        # Beta-lactamases will not have the allele and version from the gene name defined above\n                        except KeyError:\n                            percentid = sample[self.analysistype].uniquegenes[gname.split('-')[0]]\n                        # If the percent identity of the current gene matches the best percent identity, add it to\n                        # the report - there can be multiple occurrences of genes e.g.\n                        # sul1,1,AY224185,100.00,840 and sul1,2,CP002151,100.00,927 are both included because they\n                        # have the same 100% percent identity\n                        if float(identity) == percentid:\n                            try:\n                                # Determine resistance phenotype of the gene\n                                res = ResistanceNotes.resistance(name, resistance_classes)\n                                # Treat the initial vs subsequent results for each sample slightly differently - instead\n                                # of including the sample name, use an empty cell instead\n                                if multiple:\n                                    data += ','\n                                # Populate the results\n                                data += '{},{},{},{},{},{},{}\\n'.format(\n                                    res,\n                                    genename,\n                                    allele,\n                                    accession,\n                                    identity,\n                                    len(sample[self.analysistype].sequences[name]),\n                                    sample[self.analysistype].avgdepth[name])\n                                sample[self.analysistype].pipelineresults.append(\n                                    '{rgene} ({pid}%) {rclass}'.format(rgene=genename,\n                                                                       pid=identity,\n                                                                       rclass=res)\n                                )\n                                multiple = True\n                            except KeyError:\n                                pass\n                else:\n                    data += '\\n'\n            # Write the strings to the file\n            report.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sequencenames(contigsfile):\n        sequences = list()\n        for record in SeqIO.parse(open(contigsfile, \"rU\", encoding=\"iso-8859-15\"), \"fasta\"):\n            sequences.append(record.id)\n        return sequences", "response": "Takes a multifasta file and returns a list of sequence names\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef object_clean(self):\n        for sample in self.metadata:\n            try:\n                delattr(sample[self.analysistype], 'aaidentity')\n                delattr(sample[self.analysistype], 'aaalign')\n                delattr(sample[self.analysistype], 'aaindex')\n                delattr(sample[self.analysistype], 'ntalign')\n                delattr(sample[self.analysistype], 'ntindex')\n                delattr(sample[self.analysistype], 'dnaseq')\n                delattr(sample[self.analysistype], 'blastresults')\n            except AttributeError:\n                pass", "response": "Remove large attributes from the metadata objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef runner(self):\n\n        vir_report = os.path.join(self.reportpath, 'virulence.csv')\n        if os.path.isfile(vir_report):\n            self.report_parse(vir_report)\n        else:\n            logging.info('Starting {} analysis pipeline'.format(self.analysistype))\n            if not self.pipeline:\n                general = None\n                for sample in self.runmetadata.samples:\n                    general = getattr(sample, 'general')\n                if general is None:\n                    # Create the objects to be used in the analyses\n                    objects = Objectprep(self)\n                    objects.objectprep()\n                    self.runmetadata = objects.samples\n            # Run the analyses\n            Sippr(self, self.cutoff)\n            # Create the reports\n            self.reporter()\n            # Print the metadata\n            MetadataPrinter(self)", "response": "Run the necessary methods in the correct order"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reporter(self):\n        # Create a set of all the gene names without alleles or accessions e.g. sul1_18_AY260546 becomes sul1\n        genedict = dict()\n        # Load the notes file to a dictionary\n        notefile = os.path.join(self.targetpath, 'notes.txt')\n        with open(notefile, 'r') as notes:\n            for line in notes:\n                # Ignore comment lines - they will break the parsing\n                if line.startswith('#'):\n                    continue\n                # Split the line on colons e.g. stx1Aa:  Shiga toxin 1, subunit A, variant a: has three variables after\n                # the split: gene(stx1Aa), description(Shiga toxin 1, subunit A, variant a), and _(\\n)\n                try:\n                    gene, description, _ = line.split(':')\n                # There are exceptions to the parsing. Some lines only have one :, while others have three. Allow for\n                # these possibilities.\n                except ValueError:\n                    try:\n                        gene, description = line.split(':')\n                    except ValueError:\n                        gene, description, _, _ = line.split(':')\n                # Set up the description dictionary\n                genedict[gene] = description.replace(', ', '_').strip()\n        # Find unique gene names with the highest percent identity\n        for sample in self.runmetadata.samples:\n            try:\n                if sample[self.analysistype].results:\n                    # Initialise a dictionary to store the unique genes, and their percent identities\n                    sample[self.analysistype].uniquegenes = dict()\n                    for name, identity in sample[self.analysistype].results.items():\n                        # Split the name of the gene from the string e.g. stx1:11:Z36899:11 yields stx1\n                        if ':' in name:\n                            sample[self.analysistype].delimiter = ':'\n                        else:\n                            sample[self.analysistype].delimiter = '_'\n                        genename = name.split(sample[self.analysistype].delimiter)[0]\n                        # Set the best observed percent identity for each unique gene\n                        try:\n                            # Pull the previous best identity from the dictionary\n                            bestidentity = sample[self.analysistype].uniquegenes[genename]\n                            # If the current identity is better than the old identity, save it\n                            if float(identity) > float(bestidentity):\n                                sample[self.analysistype].uniquegenes[genename] = float(identity)\n                        # Initialise the dictionary if necessary\n                        except KeyError:\n                            sample[self.analysistype].uniquegenes[genename] = float(identity)\n            except AttributeError:\n                raise\n        # Create the path in which the reports are stored\n        make_path(self.reportpath)\n        # Initialise strings to store the results\n        data = 'Strain,Gene,Subtype/Allele,Description,Accession,PercentIdentity,FoldCoverage\\n'\n        with open(os.path.join(self.reportpath, self.analysistype + '.csv'), 'w') as report:\n            for sample in self.runmetadata.samples:\n                try:\n                    if sample[self.analysistype].results:\n                        # If there are many results for a sample, don't write the sample name in each line of the report\n                        for name, identity in sorted(sample[self.analysistype].results.items()):\n                            # Check to see which delimiter is used to separate the gene name, allele, accession, and\n                            # subtype information in the header\n                            if len(name.split(sample[self.analysistype].delimiter)) == 4:\n                                # Split the name on the delimiter: stx2A:63:AF500190:d; gene: stx2A, allele: 63,\n                                # accession: AF500190, subtype: d\n                                genename, allele, accession, subtype = name.split(sample[self.analysistype].delimiter)\n                            elif len(name.split(sample[self.analysistype].delimiter)) == 3:\n                                # Treat samples without a subtype e.g. icaC:intercellular adhesion protein C: differently.\n                                # Extract the allele as the 'subtype', and the gene name, and accession as above\n                                genename, subtype, accession = name.split(sample[self.analysistype].delimiter)\n                            else:\n                                genename = name\n                                subtype = ''\n                                accession = ''\n                            # Retrieve the best identity for each gene\n                            percentid = sample[self.analysistype].uniquegenes[genename]\n                            # If the percent identity of the current gene matches the best percent identity, add it to\n                            # the report - there can be multiple occurrences of genes e.g.\n                            # sul1,1,AY224185,100.00,840 and sul1,2,CP002151,100.00,927 are both included because they\n                            # have the same 100% percent identity\n                            if float(identity) == percentid:\n                                # Treat the initial vs subsequent results for each sample slightly differently - instead\n                                # of including the sample name, use an empty cell instead\n                                try:\n                                    description = genedict[genename]\n                                except KeyError:\n                                    description = 'na'\n                                # Populate the results\n                                data += '{samplename},{gene},{subtype},{description},{accession},{identity},{depth}\\n'\\\n                                    .format(samplename=sample.name,\n                                            gene=genename,\n                                            subtype=subtype,\n                                            description=description,\n                                            accession=accession,\n                                            identity=identity,\n                                            depth=sample[self.analysistype].avgdepth[name])\n                    else:\n                        data += sample.name + '\\n'\n                except (KeyError, AttributeError):\n                    data += sample.name + '\\n'\n            # Write the strings to the file\n            report.write(data)", "response": "Creates a report of the results of the clustered sample."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the scales that the user chose from the user.", "response": "def values(self):\n        \"\"\"Gets the scales that the user chose\n\n        | For frequency: 1 = Hz, 1000 = kHz\n        | For time: 1 = seconds, 0.001 = ms\n\n        :returns: float, float -- frequency scaling, time scaling\n        \"\"\"\n        if self.ui.hzBtn.isChecked():\n            fscale = SmartSpinBox.Hz\n        else:\n            fscale = SmartSpinBox.kHz\n\n        if self.ui.msBtn.isChecked():\n            tscale = SmartSpinBox.MilliSeconds\n        else:\n            tscale = SmartSpinBox.Seconds\n\n        return fscale, tscale"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts a value into the trie.", "response": "def insert_trie(trie, value):  # aka get_subtrie_or_insert\n    \"\"\" Insert a value into the trie if it is not already contained in the trie.\n        Return the subtree for the value regardless of whether it is a new value\n        or not. \"\"\"\n    if value in trie:\n        return trie[value]\n    multi_check = False\n    for key in tuple(trie.keys()):\n        if len(value) > len(key) and value.startswith(key):\n            return insert_trie(trie[key], value)\n        elif key.startswith(value):  # we know the value is not in the trie\n            if not multi_check:\n                trie[value] = {}\n                multi_check = True  # there can be multiple longer existing prefixes\n            dict_ = trie.pop(key)  # does not break strie since key<->dict_ remains unchanged\n            trie[value][key] = dict_\n    if value not in trie:\n        trie[value] = {}\n    return trie[value]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dataframe of images present with valid being a list of cell indecies that can be included in the image.", "response": "def get_valid_cell_indecies(self):\n        \"\"\"\n        Return a dataframe of images present with 'valid' being a list of cell indecies that can be included\n        \"\"\"\n        return pd.DataFrame(self).groupby(self.frame_columns).apply(lambda x: list(x['cell_index'])).\\\n            reset_index().rename(columns={0:'valid'})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprune all cell - cell contacts in the current DataFrame.", "response": "def prune_neighbors(self):\n        \"\"\"\n        If the CellDataFrame has been subsetted, some of the cell-cell contacts may no longer be part of the the dataset.  This prunes those no-longer existant connections.\n\n        Returns:\n            CellDataFrame: A CellDataFrame with only valid cell-cell contacts\n        \"\"\"\n        def _neighbor_check(neighbors,valid):\n            if not neighbors==neighbors: return np.nan\n            valid_keys = set(valid)&set(neighbors.keys())\n            d = dict([(k,v) for k,v in neighbors.items() if k in valid_keys])\n            return d\n        fixed = self.copy()\n        valid = self.get_valid_cell_indecies()\n        valid = pd.DataFrame(self).merge(valid,on=self.frame_columns).set_index(self.frame_columns+['cell_index'])\n        valid = valid.apply(lambda x: _neighbor_check(x['neighbors'],x['valid']),1).reset_index().\\\n            rename(columns={0:'new_neighbors'})\n        fixed = fixed.merge(valid,on=self.frame_columns+['cell_index']).drop(columns='neighbors').\\\n            rename(columns={'new_neighbors':'neighbors'})\n        fixed.microns_per_pixel = self.microns_per_pixel\n        fixed.db = self.db\n        #fixed.loc[:,'neighbors'] = list(new_neighbors)\n        return fixed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_hdf(self,path,key,mode='a'):\n        pd.DataFrame(self.serialize()).to_hdf(path,key,mode=mode,format='table',complib='zlib',complevel=9)\n        f = h5py.File(path,'r+')\n        f[key].attrs[\"microns_per_pixel\"] = float(self.microns_per_pixel) if self.microns_per_pixel is not None else np.nan\n        f.close()", "response": "Save the CellDataFrame to an hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef phenotypes_to_scored(self,phenotypes=None,overwrite=False):\n        if not self.is_uniform(): raise ValueError(\"inconsistent phenotypes\")\n        if phenotypes is None: \n            phenotypes = self.phenotypes\n        elif isinstance(phenotypes,str):\n            phenotypes = [phenotypes]\n        def _post(binary,phenotype_label,phenotypes,overwrite):\n            d = binary.copy()\n            if len(set(phenotypes)&set(list(binary.keys()))) > 0 and overwrite==False:\n                raise ValueError(\"Error, phenotype already exists as a scored type\")\n            for label in phenotypes: d[label] = 0\n            if phenotype_label == phenotype_label and phenotype_label in phenotypes:\n                d[phenotype_label] = 1\n            return d\n        output = self.copy()\n        output['scored_calls'] = output.apply(lambda x: \n                _post(x['scored_calls'],x['phenotype_label'],phenotypes,overwrite)\n            ,1)\n        return output", "response": "Add mutually exclusive phenotypes to the scored calls."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef concat(self,array_like):\n        arr = list(array_like)\n        if len(set([x.microns_per_pixel for x in arr])) != 1:\n            raise ValueError(\"Multiple microns per pixel set\")\n        cdf = CellDataFrame(pd.concat([pd.DataFrame(x) for x in arr]))\n        cdf.microns_per_pixel = arr[0].microns_per_pixel\n        return cdf", "response": "Concatenate multiple CellDataFrames with 1 or more CellDataFrames with microns_per_pixel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread a CellDataFrame from an hdf5 file.", "response": "def read_hdf(cls,path,key=None):\n        \"\"\"\n        Read a CellDataFrame from an hdf5 file.\n\n        Args:\n            path (str): the path to read from\n            key (str): the name of the location to read from\n\n        Returns:\n            CellDataFrame\n        \"\"\"\n        df = pd.read_hdf(path,key)\n        df['scored_calls'] = df['scored_calls'].apply(lambda x: json.loads(x))\n        df['channel_values'] = df['channel_values'].apply(lambda x: json.loads(x))\n        df['regions'] = df['regions'].apply(lambda x: json.loads(x))\n        df['phenotype_calls'] = df['phenotype_calls'].apply(lambda x: json.loads(x))\n        df['neighbors'] = df['neighbors'].apply(lambda x: json.loads(x))\n        df['neighbors'] = df['neighbors'].apply(lambda x:\n                np.nan if not isinstance(x,dict) else dict(zip([int(y) for y in x.keys()],x.values()))\n            )\n        df['frame_shape'] = df['frame_shape'].apply(lambda x: tuple(json.loads(x)))\n        df = cls(df)\n        f = h5py.File(path,'r')\n        mpp = f[key].attrs[\"microns_per_pixel\"]\n        if not np.isnan(mpp): df.microns_per_pixel = mpp\n        f.close()\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serialize(self):\n        df = self.copy()\n        df['scored_calls'] = df['scored_calls'].apply(lambda x: json.dumps(x))\n        df['channel_values'] = df['channel_values'].apply(lambda x: json.dumps(x))\n        df['regions'] = df['regions'].apply(lambda x: json.dumps(x))\n        df['phenotype_calls'] = df['phenotype_calls'].apply(lambda x: json.dumps(x))\n        df['neighbors'] = df['neighbors'].apply(lambda x: json.dumps(x))\n        df['frame_shape'] = df['frame_shape'].apply(lambda x: json.dumps(x))\n        return df", "response": "Convert the data to one that can be saved in h5 structures\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_uniform(self,verbose=True):\n        uni = pd.Series(self['phenotype_calls'].apply(lambda x: json.dumps(x)).unique()).\\\n            apply(lambda x: json.loads(x)).apply(lambda x: tuple(sorted(x.keys()))).unique()\n        if len(uni) > 1: \n            if verbose: sys.stderr.write(\"WARNING: phenotypes differ across the dataframe \\n\"+str(uni)+\"\\n\")\n            return False\n        uni = pd.Series(self['scored_calls'].apply(lambda x: json.dumps(x)).unique()).\\\n            apply(lambda x: json.loads(x)).apply(lambda x: tuple(sorted(x.keys()))).unique()\n        if len(uni) > 1: \n            if verbose: sys.stderr.write(\"WARNING: scored_calls differ across the dataframe \\n\"+str(uni)+\"\\n\")\n            return False\n        return True", "response": "Check to make sure that phenotype calls or scored calls are consistent across all images / samples\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dataframe with the regions and region sizes and region area pixels.", "response": "def get_measured_regions(self):\n        \"\"\"\n        Returns:\n            pandas.DataFrame: Output a dataframe with regions and region sizes\n        \"\"\"\n        mergeon = ['project_id','project_name',\n                'sample_id','sample_name',\n                'frame_id','frame_name',\n                ]\n        temp = self.loc[:,mergeon+['regions']].\\\n            set_index(mergeon)['regions'].apply(json.dumps).\\\n            reset_index().drop_duplicates()\n        temp['regions'] = temp['regions'].apply(json.loads)\n        rows = []\n        for i,r in temp.iterrows():\n            for label in r['regions']:\n                a = list(r.index)\n                b = list(r.values)\n                a = a+['region_label','region_area_pixels']\n                b = b+[label,r['regions'][label]]\n                rows.append(dict(zip(a,b)))\n        rows = pd.DataFrame(rows).drop(columns='regions').\\\n            drop_duplicates()[mergeon+['region_label','region_area_pixels']]\n        #rows = rows.loc[rows['region_area_pixels']>0].copy()\n        return rows"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing the segmented images to create per - image graphics for the image.", "response": "def segmentation_images(self,*args,**kwargs):\n        \"\"\"\n        Use the segmented images to create per-image graphics\n\n        Args:\n            verbose (bool): output more details if true\n\n        Returns:\n            SegmentationImages: returns a class used to construct the image graphics\n        \"\"\"\n        if not self.db: raise ValueError(\"Need to set db\")\n        segs = SegmentationImages.read_cellframe(self,*args,**kwargs)\n        segs.microns_per_pixel = segs.microns_per_pixel\n        return segs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the cellframe and returns a NearestNeighbors class that holds the nearest neighbor information for each of the phenotypes present in the cell.", "response": "def nearestneighbors(self,*args,**kwargs):\n        \"\"\"\n        Use the segmented images to create per-image graphics\n\n        Args:\n            verbose (bool): output more details if true\n            measured_regions (pandas.DataFrame): explicitly list the measured images and regions\n            measured_phenotypes (list): explicitly list the phenotypes present\n\n        Returns:\n            NearestNeighbors: returns a class that holds nearest neighbor information for whatever phenotypes were in the CellDataFrame before execution.  This class is suitable for nearest neighbor and proximity operations.\n        \"\"\"\n        n = NearestNeighbors.read_cellframe(self,*args,**kwargs)\n        if 'measured_regions' in kwargs: n.measured_regions = kwargs['measured_regions']\n        else: n.measured_regions = self.get_measured_regions()\n        if 'measured_phenotypes' in kwargs: n.measured_phenotypes = kwargs['measured_phenotypes']\n        else: n.measured_phenotypes = self.phenotypes\n        n.microns_per_pixel = self.microns_per_pixel\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new object containing the cell - to - cell contacts recorded in the CellDataFrame.", "response": "def contacts(self,*args,**kwargs):\n        \"\"\"\n        Use assess the cell-to-cell contacts recorded in the celldataframe\n\n        Returns:\n            Contacts: returns a class that holds cell-to-cell contact information for whatever phenotypes were in the CellDataFrame before execution.  \n        \"\"\"\n        n = Contacts.read_cellframe(self,prune_neighbors=True)\n        if 'measured_regions' in kwargs: n.measured_regions = kwargs['measured_regions']\n        else: n.measured_regions = self.get_measured_regions()\n        if 'measured_phenotypes' in kwargs: n.measured_phenotypes = kwargs['measured_phenotypes']\n        else: n.measured_phenotypes = self.phenotypes\n        n.microns_per_pixel = self.microns_per_pixel\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a class that can be used to create honeycomb plots for the area.", "response": "def cartesian(self,subsets=None,step_pixels=100,max_distance_pixels=150,*args,**kwargs):\n        \"\"\"\n        Return a class that can be used to create honeycomb plots\n\n        Args:\n            subsets (list): list of SubsetLogic objects\n            step_pixels (int): distance between hexagons\n            max_distance_pixels (int): the distance from each point by which to caclulate the quanitty of the phenotype for that area\n\n        Returns:\n            Cartesian: returns a class that holds the layout of the points to plot.\n        \"\"\"\n        n = Cartesian.read_cellframe(self,subsets=subsets,step_pixels=step_pixels,max_distance_pixels=max_distance_pixels,prune_neighbors=False,*args,**kwargs)\n        if 'measured_regions' in kwargs: n.measured_regions = kwargs['measured_regions']\n        else: n.measured_regions = self.get_measured_regions()\n        if 'measured_phenotypes' in kwargs: n.measured_phenotypes = kwargs['measured_phenotypes']\n        else: n.measured_phenotypes = self.phenotypes\n        n.microns_per_pixel = self.microns_per_pixel\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Counts object that can be used to access count densities for a given species.", "response": "def counts(self,*args,**kwargs):\n        \"\"\"\n        Return a class that can be used to access count densities\n\n        Args:\n            measured_regions (pandas.DataFrame): Dataframe of regions that are being measured (defaults to all the regions)\n            measured_phenotypes (list): List of phenotypes present (defaults to all the phenotypes)\n            minimum_region_size_pixels (int): Minimum region size to calculate counts on in pixels (Default: 1)\n\n        Returns:\n            Counts: returns a class that holds the counts.\n        \"\"\"\n        n = Counts.read_cellframe(self,prune_neighbors=False)\n        if 'measured_regions' in kwargs: n.measured_regions = kwargs['measured_regions']\n        else: n.measured_regions = self.get_measured_regions()\n        if 'measured_phenotypes' in kwargs: n.measured_phenotypes = kwargs['measured_phenotypes']\n        else: n.measured_phenotypes = self.phenotypes\n        n.microns_per_pixel = self.microns_per_pixel\n        if 'minimum_region_size_pixels' in kwargs: n.minimum_region_size_pixels = kwargs['minimum_region_size_pixels']\n        else: n.minimum_region_size_pixels = 1\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine the scores in a single cell dataframe with the addition of another cell.", "response": "def merge_scores(self,df_addition,reference_markers='all',\n                                      addition_markers='all',on=['project_name','sample_name','frame_name','cell_index']):\n        \"\"\"\n        Combine CellDataFrames that differ by score composition\n\n        Args:\n            df_addition (CellDataFrame): The CellDataFrame to merge scores in from\n            reference_markers (list): which scored call names to keep in the this object (default: all)\n            addition_markers (list): which scored call names to merge in (default: all)\n            on (list): the features to merge cells on\n\n        Returns:\n            CellDataFrame,CellDataFrame: returns a passing CellDataFrame where merge criteria were met and a fail CellDataFrame where merge criteria were not met.\n        \"\"\"\n        if isinstance(reference_markers, str):\n            reference_markers = self.scored_names\n        elif reference_markers is None: reference_markers = []\n        if isinstance(addition_markers, str):\n            addition_markers = df_addition.scored_names\n        elif addition_markers is None: addition_markers = []\n\n        df_addition = df_addition.copy()\n        df_addition['_key'] = 1\n        df = self.merge(df_addition[['scored_calls','_key']+on].rename(columns={'scored_calls':'_addition'}),\n                            on = on,\n                            how = 'left'\n                        )\n\n        df['_sub1'] = df['scored_calls'].apply(lambda x:\n                dict((k,x[k]) for k in reference_markers)\n            )\n        df['_sub2'] = df['_addition'].apply(lambda x:\n                dict({}) if x!=x else dict((k,x[k]) for k in addition_markers) # handle NaN where we fail to match properly treat as empty\n            )\n        # combine the two dictionaries\n        df['scored_calls'] = df.apply(lambda x:\n                {**x['_sub1'],**x['_sub2']}                    \n            ,1)\n        df = df.drop(columns=['_sub1','_sub2','_addition'])\n        df = df.drop(columns='_key').copy(),df[df['_key'].isna()].drop(columns='_key').copy()\n        if self.microns_per_pixel: df[0].microns_per_pixel = self.microns_per_pixel\n        if self.microns_per_pixel: df[1].microns_per_pixel = self.microns_per_pixel\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new CellDataFrame with the names of scored call names renamed.", "response": "def rename_scored_calls(self,change):\n        \"\"\"\n        Change the names of scored call names, input dictionary change with {<current name>:<new name>} format, new name must not already exist\n\n        Args:\n            change (dict): a dictionary of current name keys and new name values\n\n        Returns:\n            CellDataFrame: The CellDataFrame modified.\n        \"\"\"\n        output = self.copy()\n        output['scored_calls'] = output.apply(lambda x:\n          _dict_rename(x['scored_calls'],change)\n          ,1)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling in missing phenotypes and scored types by listing any missing data as negative", "response": "def zero_fill_missing_phenotypes(self):\n        \"\"\"\n        Fill in missing phenotypes and scored types by listing any missing data as negative\n\n        Returns:\n            CellDataFrame: The CellDataFrame modified.\n        \"\"\"\n        if self.is_uniform(verbose=False): return self.copy()\n        output = self.copy()\n        def _do_fill(d,names):\n            old_names = list(d.keys())\n            old_values = list(d.values())\n            missing = set(names)-set(old_names)\n            return dict(zip(old_names+list(missing),old_values+([0]*len(missing))))\n        ## Need to make these uniform\n        pnames = self.phenotypes\n        output['phenotype_calls']= output.apply(lambda x:\n            _do_fill(x['phenotype_calls'],pnames)\n            ,1)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndrops the scored calls from the scored calls.", "response": "def drop_scored_calls(self,names):\n        \"\"\"\n        Take a name or list of scored call names and drop those from the scored calls\n\n        Args:\n            names (list): list of names to drop or a single string name to drop\n\n        Returns:\n            CellDataFrame: The CellDataFrame modified.\n        \"\"\"\n        def _remove(calls,names):\n            d = dict([(k,v) for k,v in calls.items() if k not in names])\n            return d\n        if isinstance(names, str):\n            names = [names]\n        output = self.copy()\n        output['scored_calls'] = output['scored_calls'].\\\n            apply(lambda x: _remove(x,names))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a specific phenotype based on a logic object.", "response": "def subset(self,logic,update=False):\n        \"\"\"\n        subset create a specific phenotype based on a logic, \n        logic is a 'SubsetLogic' class, \n        take union of all the phenotypes listed.  If none are listed use all phenotypes. \n        take the intersection of all the scored calls.\n\n        Args:\n            logic (SubsetLogic): A subsetlogic object to slice on\n            update (bool): (default False) change the name of the phenotype according to the label in the subset logic\n\n        Returns:\n            CellDataFrame: The CellDataFrame modified.\n        \"\"\"\n        pnames = self.phenotypes\n        snames = self.scored_names\n        data = self.copy()\n        values = []\n        phenotypes = logic.phenotypes\n        if len(phenotypes)==0: phenotypes = pnames\n        removing = set(self.phenotypes)-set(phenotypes)\n        for k in phenotypes:\n            if k not in pnames: raise ValueError(\"phenotype must exist in defined\")\n            temp = data.loc[data['phenotype_calls'].apply(lambda x: x[k]==1)].copy()\n            if len(removing) > 0 and temp.shape[0] > 0:\n                temp['phenotype_calls'] = temp.apply(lambda x:\n                    dict([(k,v) for k,v in x['phenotype_calls'].items() if k not in removing])\n                    ,1)\n            values.append(temp)\n        data = pd.concat(values)\n        for k,v in logic.scored_calls.items():\n            if k not in snames: raise ValueError(\"Scored name must exist in defined\")\n            myfilter = 0 if v == '-' else 1\n            data = data.loc[data['scored_calls'].apply(lambda x: x[k]==myfilter)]\n        data.microns_per_pixel = self.microns_per_pixel\n        if update: \n            data['phenotype_calls'] = data['phenotype_calls'].apply(lambda x: {logic.label:1})\n        data.fill_phenotype_label(inplace=True)\n        data.db = self.db\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef threshold(self,phenotype,scored_name,positive_label=None,negative_label=None):\n        if positive_label is None and negative_label is not None or \\\n           negative_label is None and positive_label is not None: raise ValueError(\"Error if you want to specify labels, give both positive and negative\")\n        if phenotype not in self.phenotypes: raise ValueError(\"Error phenotype \"+str(phenotype)+\" is not in the data.\")\n        if scored_name not in self.scored_names: raise ValueError(\"Error scored_name \"+str(scored_name)+\" is not in the data.\")\n        if positive_label is None and negative_label is None:\n            positive_label = phenotype+' '+scored_name+'+'\n            negative_label = phenotype+' '+scored_name+'-'\n        elif positive_label == negative_label: raise ValueError(\"Cant have the same label for positive and negative.\")\n        def _swap_in(d,pheno,scored,phenotype_calls,scored_calls,pos,neg):\n            if pheno not in phenotype_calls.keys(): return d\n            keepers = [(k,v) for k,v in phenotype_calls.items() if k!=phenotype]\n            if scored not in scored_calls.keys(): raise ValueError(\"Error scored calls are not unified across samples\")\n            scored_value = scored_calls[scored]\n            phenotype_value = phenotype_calls[pheno]\n            if phenotype_value == 0:\n                keepers += [(pos,0),(neg,0)]\n            elif scored_value == 1:\n                keepers += [(pos,1),(neg,0)]\n            elif scored_value == 0:\n                keepers += [(pos,0),(neg,1)]\n            else: raise ValueError(\"Format error.  These values should only ever be zero or one.\")\n            return dict(keepers)\n        data = self.copy()\n        data['phenotype_calls'] = self.apply(lambda x:\n                _swap_in(x,phenotype,scored_name,x['phenotype_calls'],x['scored_calls'],positive_label,negative_label)\n            ,1)\n        def _set_label(d):\n            vals = [k for k,v in d.items() if v==1]\n            return np.nan if len(vals) == 0 else vals[0]\n        data['phenotype_label'] = data.apply(lambda x:\n                _set_label(x['phenotype_calls'])\n            ,1)\n        return data.copy()", "response": "Return a new CellDataFrame with the values of the specified phenotype and scored call."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collapse_phenotypes(self,input_phenotype_labels,output_phenotype_label,verbose=True):\n        if isinstance(input_phenotype_labels,str): input_phenotype_labels = [input_phenotype_labels]\n        bad_phenotypes = set(input_phenotype_labels)-set(self.phenotypes)\n        if len(bad_phenotypes) > 0: raise ValueError(\"Error phenotype(s) \"+str(bad_phenotypes)+\" are not in the data.\")\n        data = self.copy()\n        if len(input_phenotype_labels) == 0: return data\n        def _swap_in(d,inputs,output):\n            # Get the keys we need to merge together\n            overlap = set(d.keys()).intersection(inputs)\n            # if there are none to merge we're done already\n            if len(overlap) == 0: return d\n            keepers = [(k,v) for k,v in d.items() if k not in inputs]\n            # combine anything thats not a keeper\n            return dict(keepers+\\\n                        [(output_phenotype_label,max([d[x] for x in overlap]))])\n        data['phenotype_calls'] = data.apply(lambda x:\n            _swap_in(x['phenotype_calls'],input_phenotype_labels,output_phenotype_label)\n            ,1)\n        def _set_label(d):\n            vals = [k for k,v in d.items() if v==1]\n            return np.nan if len(vals) == 0 else vals[0]\n        data['phenotype_label'] = data.apply(lambda x:\n                _set_label(x['phenotype_calls']),1)\n        return data", "response": "Returns a new CellDataFrame with the new phenotype names merged."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining one or more input regions into a single output region.", "response": "def combine_regions(self,input_region_labels,output_region_label,verbose=True):\n        \"\"\"\n        Combine/rename one or more input regions to a single output region\n\n        Args:\n            input_region_labels (list): A str name or list of names to combine\n            output_region_label (list): A str name to change the phenotype names to\n            verbose (bool): output more details\n\n        Returns:\n            CellDataFrame: The CellDataFrame modified.\n        \"\"\"\n        if isinstance(input_region_labels,str): input_region_labels = [input_region_labels]\n        bad_regions = set(input_region_labels)-set(self.regions)\n        if len(bad_regions) > 0: raise ValueError(\"Error regions(s) \"+str(bad_regions)+\" are not in the data.\")\n        data = self.copy()\n        if len(input_region_labels) == 0: return data\n        def _swap_in(d,inputs,output):\n            # Get the keys we need to merge together\n            overlap = set(d.keys()).intersection(inputs)\n            # if there are none to merge we're done already\n            if len(overlap) == 0: return d\n            keepers = [(k,v) for k,v in d.items() if k not in inputs]\n            # combine anything thats not a keeper\n            return dict(keepers+\\\n                        [(output_region_label,sum([d[x] for x in overlap]))])\n        data['regions'] = data.apply(lambda x:\n            _swap_in(x['regions'],input_region_labels,output_region_label)\n            ,1)\n        data.loc[data['region_label'].isin(input_region_labels),'region_label'] = output_region_label\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfills the phenotype_label column according to our rules for mutual exclusion.", "response": "def fill_phenotype_label(self,inplace=False):\n        \"\"\"\n        Set the phenotype_label column according to our rules for mutual exclusion\n        \"\"\"\n        def _get_phenotype(d):\n            vals = [k for k,v in d.items() if v ==  1]\n            return np.nan if len(vals) == 0 else vals[0]\n        if inplace:\n            if self.shape[0] == 0: return self\n            self['phenotype_label'] = self.apply(lambda x: _get_phenotype(x['phenotype_calls']),1)\n            return\n        fixed = self.copy()\n        if fixed.shape[0] == 0: return fixed\n        fixed['phenotype_label'] = fixed.apply(lambda x: _get_phenotype(x['phenotype_calls']),1)\n        return fixed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fill_phenotype_calls(self,phenotypes=None,inplace=False):\n        if phenotypes is None: phenotypes = list(self['phenotype_label'].unique())\n        def _get_calls(label,phenos):\n            d =  dict([(x,0) for x in phenos])\n            if label!=label: return d # np.nan case\n            d[label] = 1\n            return d\n        if inplace:\n            self['phenotype_calls'] = self.apply(lambda x: _get_calls(x['phenotype_label'],phenotypes),1)\n            return\n        fixed = self.copy()\n        fixed['phenotype_calls'] = fixed.apply(lambda x: _get_calls(x['phenotype_label'],phenotypes),1)\n        return fixed", "response": "Fill the phenotype_calls attribute of the current object with the values of the specified phenotypes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scored_to_phenotype(self,phenotypes):\n        def _apply_score(scored_calls,phenotypes):\n            present = sorted(list(set(phenotypes)&set(scored_calls.keys())))\n            total = sum([scored_calls[x] for x in present])\n            if total > 1: \n                raise ValueError(\"You cant extract phenotypes from scores if they are not mutually exclusive\")\n            if total == 0: return np.nan\n            for label in present:\n                if scored_calls[label] == 1: return label\n            raise ValueError(\"Should have hit an exit criteria already\")\n        output = self.copy()\n        output['phenotype_label'] = output.apply(lambda x: _apply_score(x['scored_calls'],phenotypes),1)\n        # now update the phenotypes with these\n        output['phenotype_calls'] = output.apply(lambda x: \n            dict([(y,1 if x['phenotype_label']==y else 0) for y in phenotypes])\n        ,1)\n        return output", "response": "Convert binary pehnotypes to mutually exclusive phenotypes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nissue a command to the server and listen to command history updates of a single issued command.", "response": "def issue_and_listen_to_command_history():\n    \"\"\"Listen to command history updates of a single issued command.\"\"\"\n    def tc_callback(rec):\n        print('TC:', rec)\n\n    command = processor.issue_command('/YSS/SIMULATOR/SWITCH_VOLTAGE_OFF', args={\n        'voltage_num': 1,\n    }, comment='im a comment')\n    command.create_command_history_subscription(on_data=tc_callback)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new table that links the given tables table1 and table2.", "response": "def get_many2many_table(table1, table2):\n    \"\"\"Creates a many-to-many table that links the given tables table1 and table2.\n\n    :param str table1: Tablename of left hand table without TABLE_PREFIX.\n    :param str table2: Tablename of right hand table without TABLE_PREFIX.\n    :return:\n    \"\"\"\n    table_name = ('{}{}__{}'.format(TABLE_PREFIX, table1, table2))\n    return Table(table_name, Base.metadata,\n                 Column('{}_id'.format(table1), Integer, ForeignKey('{}{}.id'.format(TABLE_PREFIX, table1))),\n                 Column('{}_id'.format(table2), Integer, ForeignKey('{}{}.id'.format(TABLE_PREFIX, table2)))\n                 )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch for a match in an executor and awaits for it.", "response": "async def search(self, regex):\n        \"\"\"\n        Wraps the search for a match in an `executor`_ and awaits for it.\n\n        .. _executor: https://docs.python.org/3/library/asyncio-eventloop.html#executor\n        \"\"\"\n        coro = self._loop.run_in_executor(None, self._search, regex)\n        match = await coro\n\n        return match"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints to stdout help on how to answer properly.", "response": "def show_help(self):\n        \"\"\"Prints to stdout help on how to answer properly\"\"\"\n        print(\"Sorry, not well understood.\")\n        print(\"- use\", str(self.yes_input), \"to answer 'YES'\")\n        print(\"- use\", str(self.no_input), \"to answer 'NO'\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef re_ask(self, with_help=True):\n        if with_help:\n            self.show_help()\n\n        return self.get_answer(self.last_question)", "response": "Re - asks the user the last question"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking user a question then gets user answer", "response": "def get_answer(self, question):\n        \"\"\"Asks user a question, then gets user answer\n\n        :param question: Question: to ask user\n        :return: User answer\n        \"\"\"\n        self.last_question = str(question).strip()\n        user_answer = input(self.last_question)\n        return user_answer.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if question is yes or no", "response": "def get_yes_no(self, question):\n        \"\"\"Checks if question is yes (True) or no (False)\n\n        :param question: Question to ask user\n        :return: User answer\n        \"\"\"\n        user_answer = self.get_answer(question).lower()\n        if user_answer in self.yes_input:\n            return True\n\n        if user_answer in self.no_input:\n            return False\n\n        is_yes = self.is_yes(user_answer)  # check if similar to yes/no choices\n        is_no = self.is_no(user_answer)\n        if is_yes and not is_no:\n            return True\n\n        if is_no and not is_yes:\n            return False\n\n        if self.interactive:\n            self.show_help()\n            return self.get_yes_no(self.last_question)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses answer and gets number of user answer from the question and returns user answer", "response": "def get_number(self, question, min_i=float(\"-inf\"), max_i=float(\"inf\"),\n                   just_these=None):\n        \"\"\"Parses answer and gets number\n\n        :param question: Question: to ask user\n        :param min_i: min acceptable number\n        :param max_i: max acceptable number\n        :param just_these: Accept only these numbers\n        :return: User answer\n        \"\"\"\n        try:\n            user_answer = self.get_answer(question)\n            user_answer = float(user_answer)\n\n            if min_i < user_answer < max_i:\n                if just_these:\n                    if user_answer in just_these:\n                        return user_answer\n\n                    exc = \"Number cannot be accepted. Just these: \"\n                    exc += str(just_these)\n                    raise Exception(exc)\n\n                return user_answer\n\n            exc = \"Number is not within limits. \"\n            exc += \"Min is \" + str(min_i) + \". Max is \" + str(max_i) + \"\"\n            raise Exception(exc)\n        except Exception as exc:\n            print(str(exc))\n            return self.get_number(\n                self.last_question,\n                min_i=min_i,\n                max_i=max_i,\n                just_these=just_these\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing answer and gets list of items with this char", "response": "def get_list(self, question,\n                 splitter=\",\", at_least=0, at_most=float(\"inf\")):\n        \"\"\"Parses answer and gets list\n\n        :param question: Question: to ask user\n        :param splitter: Split list elements with this char\n        :param at_least: List must have at least this amount of elements\n        :param at_most: List must have at most this amount of elements\n        :return: User answer\n        \"\"\"\n        try:\n            user_answer = self.get_answer(question)  # ask question\n            user_answer = user_answer.split(splitter)  # split items\n            user_answer = [str(item).strip() for item in user_answer]  # strip\n\n            if at_least < len(user_answer) < at_most:\n                return user_answer\n\n            exc = \"List is not correct. \"\n            exc += \"There must be at least \" + str(at_least) + \" items, \"\n            exc += \"and at most \" + str(at_most) + \". \"\n            exc += \"Use '\" + str(splitter) + \"' to separate items\"\n            raise Exception(exc)\n        except Exception as exc:\n            print(str(exc))\n            return self.get_list(\n                self.last_question,\n                at_least=at_least,\n                at_most=at_most\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef batlab2sparkle(experiment_data):\n    # This is mostly for convention.. attribute that matters most is samplerate, \n    # since it is used in the GUI to calculate things like duration\n    nsdata = {}\n    for attr in ['computername', 'pst_filename', 'title', 'who', 'date', 'program_date']:\n        nsdata[attr] = experiment_data[attr]\n    for itest, test in enumerate(experiment_data['test']):\n        setname = 'test_{}'.format(itest+1)\n        nsdata[setname] = {}\n        nsdata[setname]['samplerate_ad'] = test['trace'][0]['samplerate_ad']\n        nsdata[setname]['comment'] = test['comment']\n        nsdata[setname]['start'] = test['time']\n        nsdata[setname]['mode'] = 'finite'\n        nsdata[setname]['user_tag'] = ''\n\n        if test['full_testtype'] == 'General Auto Test' and test['testtype'] == 'tone':\n            nsdata[setname]['testtype'] = 'Tuning Curve'\n        else:\n            nsdata[setname]['testtype'] = test['full_testtype']\n\n        stims = []\n        for itrace, trace in enumerate(test['trace']):\n            try:\n                stim = {'samplerate_da': trace['samplerate_da'],\n                        'overloaded_attenuation': 0,}\n                components = []\n                for icomp, component in enumerate(trace['stimulus']):\n                    # always add in silence component to match batlab's delay parameter\n                    delay_comp = {'index': [icomp, 0], 'stim_type': 'silence', \n                            'intensity': 0, 'duration': component['delay']/1000., \n                            'start_s': 0, 'risefall': 0}\n                    components.append(delay_comp)\n                    # FIXME need to pull in speaker calibration to get real intensity\n                    comp = {'risefall' : component['rise_fall']/1000., \n                            'index': [icomp, 1], \n                            'duration': component['duration']/1000.,\n                            'start_s': component['delay']/1000.,\n                            'intensity': 100 - component['attenuation']}\n                    if component['soundtype_name'] == 'vocalization':\n                        # print component\n                        comp['stim_type'] = 'Vocalization'\n                        comp['filename'] = component['vocal_call_file']\n                        comp['browsedir'] = ''\n                    elif component['soundtype_name'] == 'fmsweep':\n                        comp['stim_type'] = 'FM Sweep'\n                        usweep = 1 if component['usweep'] else -1\n                        comp['start_f'] = component['frequency'] - (component['bandwidth']/2)*usweep\n                        comp['stop_f'] = component['frequency'] + (component['bandwidth']/2)*usweep\n                    elif component['soundtype_name'] == 'tone':\n                        comp['stim_type'] = 'Pure Tone'\n                        comp['frequency'] = component['frequency']\n                    else:\n                        # print 'FOUND UNKNOWN STIM', component['soundtype_name']\n                        # raise ValueError\n                        comp['stim_type'] = component['soundtype_name']\n\n                    components.append(comp)\n                stim['components'] = components\n                stims.append(stim)\n            except TypeError:\n                print 'PROBLEM with', itest, itrace\n                print 'component', component\n                continue\n\n        nsdata[setname]['stim'] = stims\n\n    return nsdata", "response": "Convert the batlab experiment data to Sparkle format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sanitize_type(raw_type):\n    cleaned = get_printable(raw_type).strip()\n    for bad in [\n            r'__drv_aliasesMem', r'__drv_freesMem',\n            r'__drv_strictTypeMatch\\(\\w+\\)',\n            r'__out_data_source\\(\\w+\\)',\n            r'_In_NLS_string_\\(\\w+\\)',\n            r'_Frees_ptr_', r'_Frees_ptr_opt_', r'opt_',\n            r'\\(Mem\\) '\n    ]:\n        cleaned = re.sub(bad, '', cleaned).strip()\n    if cleaned in ['_EXCEPTION_RECORD *', '_EXCEPTION_POINTERS *']:\n        cleaned = cleaned.strip('_')\n    cleaned = cleaned.replace('[]', '*')\n    return cleaned", "response": "Sanitize the raw type string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clean_ret_type(ret_type):\n    ret_type = get_printable(ret_type).strip()\n    if ret_type == 'LRESULT LRESULT':\n        ret_type = 'LRESULT'\n    for bad in [\n            'DECLSPEC_NORETURN', 'NTSYSCALLAPI', '__kernel_entry',\n            '__analysis_noreturn', '_Post_equals_last_error_',\n            '_Maybe_raises_SEH_exception_',\n            '_CRT_STDIO_INLINE', '_ACRTIMP'\n    ]:\n        if bad in ret_type:\n            ret_type = ret_type.replace(bad, '').strip()\n            logging.debug(_('cleaned %s'), bad)\n    return ret_type", "response": "Clean the erraneous parsed return type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the DynamoDB table if it not exists.", "response": "async def setup(self):\n        \"\"\"Setting up DynamoDB table, if it not exists.\"\"\"\n        try:\n            client = await self.db\n            response = await client.list_tables()\n            created = False\n            # create table if not already created.\n            if self.table_name not in response[\"TableNames\"]:\n                logger.info(\"Creating DynamoDB table [{}]\".format(self.table_name))\n                resp = await client.create_table(**self.table_schema)\n                if resp.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 200:\n                    logger.info(\"DynamoDB table [{}] successfully created!\".format(self.table_name))\n                    created = True\n            # create control table if not already created.\n            if self.control_table_name and self.control_table_name not in response[\"TableNames\"]:\n                logger.info(\"Creating DynamoDB control_table [{}]\".format(self.control_table_name))\n                resp = await client.create_table(**self.control_table_schema)\n                if resp.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\") == 200:\n                    logger.info(\"DynamoDB control table [{}] successfully created!\".format(self.control_table_name))\n                    created = True\n            return created\n        except Exception as exc:\n            logger.error(\"[DB] Error when setting up DynamoDB.\")\n            logger.error(exc)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef maxRange(self):\n        try:\n            x, freqs = self.datafile.get_calibration(str(self.ui.calChoiceCmbbx.currentText()), self.calf)\n            self.ui.frangeLowSpnbx.setValue(freqs[0])\n            self.ui.frangeHighSpnbx.setValue(freqs[-1])\n            print 'set freq range', freqs[0], freqs[-1], freqs[0], freqs[-1]\n        except IOError:\n            QtGui.QMessageBox.warning(self, \"File Read Error\", \"Unable to read calibration file\")\n        except KeyError:\n            QtGui.QMessageBox.warning(self, \"File Data Error\", \"Unable to find data in file\")", "response": "Sets the maximum range for the currently selection calibration"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing a calibration curve in a separate window of the currently selected calibration", "response": "def plotCurve(self):\n        \"\"\"Shows a calibration curve, in a separate window, of the currently selected calibration\"\"\"\n        try:\n            attenuations, freqs = self.datafile.get_calibration(str(self.ui.calChoiceCmbbx.currentText()), self.calf)\n            self.pw = SimplePlotWidget(freqs, attenuations, parent=self)\n            self.pw.setWindowFlags(QtCore.Qt.Window)\n            self.pw.setLabels('Frequency', 'Attenuation', 'Calibration Curve')\n            self.pw.show()\n        except IOError:\n            QtGui.QMessageBox.warning(self, \"File Read Error\", \"Unable to read calibration file\")\n        except KeyError:\n            QtGui.QMessageBox.warning(self, \"File Data Error\", \"Unable to find data in file\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the values the user input to this dialog", "response": "def values(self):\n        \"\"\"Gets the values the user input to this dialog\n\n        :returns: dict of inputs:\n        |               *'use_calfile'*: bool, -- whether to apply calibration at all\n        |               *'calname'*: str, -- the name of the calibration dataset to use\n        |               *'frange'*: (int, int), -- (min, max) of the frequency range to apply calibration to\n        \"\"\"\n        results = {}\n        results['use_calfile'] = self.ui.calfileRadio.isChecked()\n        results['calname'] = str(self.ui.calChoiceCmbbx.currentText())\n        results['frange'] = (self.ui.frangeLowSpnbx.value(), self.ui.frangeHighSpnbx.value())\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naccepting the inputs if all values are valid and congruent. i. e. Valid datafile and frequency range within the given calibration dataset.", "response": "def conditional_accept(self):\n        \"\"\"Accepts the inputs if all values are valid and congruent.\n        i.e. Valid datafile and frequency range within the given calibration dataset.\"\"\"\n        if self.ui.calfileRadio.isChecked() and str(self.ui.calChoiceCmbbx.currentText()) == '':\n            self.ui.noneRadio.setChecked(True)\n        if self.ui.calfileRadio.isChecked():\n            try:\n                x, freqs = self.datafile.get_calibration(str(self.ui.calChoiceCmbbx.currentText()), self.calf)\n            except IOError:\n                QtGui.QMessageBox.warning(self, \"File Read Error\", \"Unable to read calibration file\")\n                return\n            except KeyError:\n                QtGui.QMessageBox.warning(self, \"File Data Error\", \"Unable to find data in file\")\n                return\n            if self.ui.frangeLowSpnbx.value()    < freqs[0] or \\\n                self.ui.frangeHighSpnbx.value() > freqs[-1]:\n                QtGui.QMessageBox.warning(self, \"Invalid Frequency Range\", \n                    \"Provided frequencys outside of calibration file range of {} - {} Hz\".format(freqs[0], freqs[-1]))\n                return\n\n        self.accept()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef customized_warning(message, category=UserWarning,\n                       filename='', lineno=-1, file=None, line=None):\n    \"\"\"\n    Customized function to display warnings.\n    Monkey patch for `warnings.showwarning`.\n    \"\"\"\n    print(\"WARNING: {0}\".format(message))", "response": "Customized warning function to display a message in the availabe."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses command line arguments and returns a dictionary of command line arguments.", "response": "def read_cmdline():\n    \"\"\"\n    Parses optional command line arguments.\n    \"\"\"\n    info = {\n            \"prog\": \"Ellis\",\n            \"description\": \"%(prog)s version {0}\".format(__version__),\n            \"epilog\": \"For further help please head over to {0}\"\n                      .format(__url__),\n            \"usage\": argparse.SUPPRESS,\n    }\n\n    argp = argparse.ArgumentParser(**info)\n\n    # Add an optional string argument 'config':\n    argp.add_argument(\"-c\", \"--config\",\n                      dest='config_file',\n                      metavar='FILE',\n                      help=\"read configuration from FILE\",\n                      type=str)\n\n    # Parse command line:\n    args = argp.parse_args()\n\n    return vars(args)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    # Monkey patch warnings.showwarning:\n    warnings.showwarning = customized_warning\n\n    # Read command line args, if any:\n    args = read_cmdline()\n\n    # Configuration file, if given on the command line:\n    config_file = args['config_file']\n\n    try:\n        ellis = Ellis(config_file)\n    except NoRuleError:\n        msg = (\"There are no valid rules in the config file. \"\n               \"Ellis can not run without rules.\")\n        print_err(msg)\n    else:\n        ellis.start()", "response": "Entry point for Ellis.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, *args, **kwargs):\n        self.uid = 'electiontype:{}'.format(self.slug)\n        super(ElectionType, self).save(*args, **kwargs)", "response": "Save the object to the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshift buffers in order to change shape", "response": "def shape(self, shape=None):\n        \"\"\"We need to shift buffers in order to change shape\"\"\"\n        if shape is None:\n            return self._shape\n        data, color = self.renderer.manager.set_shape(self.model.id, shape)\n        self.model.data = data\n        self.color = color\n        self._shape = shape"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reply(self, timeout=None):\n        self._wait_on_signal(self._response_received)\n        if self._response_exception is not None:\n            msg = self._response_exception.message\n            raise YamcsError(msg)\n        return self._response_reply", "response": "Returns the initial reply. This function returns the initial reply."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstop all sleep tasks to allow bridges to end.", "response": "async def stop_bridges(self):\n        \"\"\"Stop all sleep tasks to allow bridges to end.\"\"\"\n        for task in self.sleep_tasks:\n            task.cancel()\n        for bridge in self.bridges:\n            bridge.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef str_to_date(date: str) -> datetime.datetime:\n    date = date.split('.')\n    date.reverse()\n    y, m, d = date\n    return datetime.datetime(int(y), int(m), int(d))", "response": "Convert cbr. ru API date ste to python datetime"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(self, data, size=None):\n        self.bind()\n        if size is None:\n            # ffi's sizeof understands arrays\n            size = sizeof(data)\n        if size == self.buffer_size:\n            # same size - no need to allocate new buffer, just copy\n            glBufferSubData(\n                self.array_type,\n                0,\n                size,\n                to_raw_pointer(data)\n            )\n        else:\n            # buffer size has changed - need to allocate new buffer in the GPU\n            glBufferData(\n                self.array_type,\n                size,\n                to_raw_pointer(data),\n                self.draw_type\n            )\n            self.buffer_size = size\n        self.unbind()", "response": "Load the data into the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_mayaplugins():\n    mpp = os.environ.get('MAYA_PLUG_IN_PATH')\n    if mpp is not None:\n        ';'.join([mpp, MAYA_PLUGIN_PATH])\n    else:\n        mpp = MAYA_PLUGIN_PATH\n\n    # to simply load all plugins inside our plugin path, we override pluginpath temporarly\n    os.environ['MAYA_PLUG_IN_PATH'] = MAYA_PLUGIN_PATH\n    cmds.loadPlugin(allPlugins=True)\n    # then we set the MAYA_PLUG_IN_PATH to the correct value\n    # NOTE: this ignores the order of paths in MAYA_PLUG_IN_PATH completely\n    os.environ['MAYA_PLUG_IN_PATH'] = mpp", "response": "Loads the maya plugins of the pipeline"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the pipeline in maya so everything works", "response": "def init():\n    \"\"\"Initialize the pipeline in maya so everything works\n\n    Init environment and load plugins.\n    This also creates the initial Jukebox Menu entry.\n\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    main.init_environment()\n    pluginpath = os.pathsep.join((os.environ.get('JUKEBOX_PLUGIN_PATH', ''), BUILTIN_PLUGIN_PATH))\n    os.environ['JUKEBOX_PLUGIN_PATH'] = pluginpath\n    try:\n        maya.standalone.initialize()\n        jukeboxmaya.STANDALONE_INITIALIZED = True\n    except RuntimeError as e:\n        jukeboxmaya.STANDALONE_INITIALIZED = False\n        if str(e) == \"maya.standalone may only be used from an external Python interpreter\":\n            mm = MenuManager.get()\n            mainmenu = mm.create_menu(\"Jukebox\", tearOff=True)\n            mm.create_menu(\"Help\", parent=mainmenu, command=show_help)\n    # load plugins\n    pmanager = MayaPluginManager.get()\n    pmanager.load_plugins()\n    load_mayaplugins()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all_jobs(self):\n        return list(set(self.complete + self.failed + self.queue + self.running))", "response": "Returns a list of all jobs submitted to the queue complete in - progess or failed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the percentage current and total number of active jobs in the queue.", "response": "def progress(self):\n        \"\"\" Returns the percentage, current and total number of\n        jobs in the queue.\n        \"\"\"\n        total = len(self.all_jobs)\n        remaining = total - len(self.active_jobs) if total > 0 else 0\n        percent = int(100 * (float(remaining) / total)) if total > 0 else 0\n        return percent"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ready(self, job):\n        no_deps = len(job.depends_on) == 0\n        all_complete = all(j.is_complete() for j in self.active_jobs\n                if j.alias in job.depends_on)\n        none_failed = not any(True for j in self.failed\n                if j.alias in job.depends_on)\n        queue_is_open = len(self.running) < self.MAX_CONCURRENT_JOBS\n        return queue_is_open and (no_deps or (all_complete and none_failed))", "response": "Determines if the job is ready to be sumitted to the\n        queue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if the queue is locked.", "response": "def locked(self):\n        \"\"\" Determines if the queue is locked. \"\"\"\n        if len(self.failed) == 0:\n            return False\n        for fail in self.failed:\n            for job in self.active_jobs:\n                if fail.alias in job.depends_on:\n                    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tick(self):\n        self.on_start()\n        while not self.is_empty:\n            cruft = []\n            for job in self.queue:\n                if not self.ready(job):\n                    continue\n                self.on_ready(job)\n                try:\n                    job.submit()\n                except ValueError:\n                    if job.should_retry:\n                        self.on_error(job)\n                        job.attempts += 1\n                    else:\n                        self.on_fail(job)\n                        cruft.append(job)\n                        self.failed.append(job)\n                else:\n                    self.running.append(job)\n                    self.on_submit(job)\n                    cruft.append(job)\n\n            self.queue = [job for job in self.queue if job not in cruft]\n\n            cruft = []\n            for job in self.running:\n                if job.is_running() or job.is_queued():\n                    pass\n                elif job.is_complete():\n                    self.on_complete(job)\n                    cruft.append(job)\n                    self.complete.append(job)\n                elif job.is_fail():\n                    self.on_fail(job)\n                    cruft.append(job)\n                    self.failed.append(job)\n                elif job.is_error():\n                    self.on_error(job)\n                    cruft.append(job)\n                else:\n                    pass\n            self.running = [job for job in self.running if job not in cruft]\n\n            if self.locked() and self.on_locked():\n                raise RuntimeError\n            self.on_tick()\n            yield\n        self.on_end()", "response": "This function yields at the end of the queue and yields at the end of the queue."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_args(**kwargs):\n    if kwargs.get(\"control\"):\n        args = Namespace(control=kwargs[\"control\"])\n    elif config.CONTROLFILE:\n        args = Namespace(control=config.CONTROLFILE)\n    elif config.DB.get(\"control_table_name\"):\n        args = Namespace(control=\"sql\")\n    elif config.AWS.get(\"control_table_name\"):\n        args = Namespace(control=\"dynamodb\")\n    else:\n        # read cli args\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\"--control\", required=True, help=\"Control file, can be path.\")\n        args = parser.parse_args()\n    return args", "response": "Read command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new instance of the same class with the same attributes and values.", "response": "def roll_mc(self, num_hist=10**5):\r\n        \"\"\"\r\n        Runs the initialized Monte Carlo simulation to determine the equivalent\r\n        Point Buy of an ability score rolling method. NOTE- This method must\r\n        be called BEFORE \"plot_histogram\".\r\n\r\n        This method returns self, so you can stack the \"plot_histogram\" and\r\n        \"get_results\" methods, like pbe1.roll_mc().plot_histogram()\r\n\r\n        :param num_hist: Number of Monte Carlo histories to run. Suggest 10**5\r\n        \"\"\"\r\n        # This is hard to read. I wanted to keep the underlying functions\r\n        # static, so this is essentially just a wrapper around _find_best_pbe.\r\n        self.arr_res, self.pbe_res = self._find_best_pbe(num_hist,\r\n            self.num_dice, self.dice_type, self.add_val, self.num_attribute,\r\n            self.keep_dice, self.reroll, self.num_arrays, self.keep_attribute,\r\n            self.pbe_map, self.roll_low_limit, self.roll_high_limit,\r\n            self.pbe_low_limit, self.pbe_high_limit)\r\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_histogram(self, title_prefix=\"\", title_override=\"\",\r\n                       figsize=(8, 6)):\r\n        \"\"\"\r\n        Plots a histogram of the results after the Monte Carlo simulation is\r\n        run. NOTE- This method must be called AFTER \"roll_mc\".\r\n\r\n        :param title_prefix: If desired, prefix the title (such as \"Alg 1\")\r\n        :param title_override: Override the title string entirely\r\n        :param figsize: The size of the histogram plot\r\n        :return: a seaborn figure of the histogram\r\n        \"\"\"\r\n        # Check that roll_mc has been called\r\n        if not self.arr_res:\r\n            raise ValueError(\"Call roll_mc before plotting the histogram.\")\r\n        # Find a title using either the override or _construct_title method\r\n        if title_override:\r\n            title = title_override\r\n        else:\r\n            title = title_prefix + PBE._construct_title(self.num_dice,\r\n                self.dice_type, self.add_val, self.num_attribute,\r\n                self.keep_attribute, self.keep_dice, self.reroll,\r\n                self.num_arrays)\r\n        # Construct the histogram\r\n        f = self._plot_hist(self.arr_res, self.pbe_res, title, figsize)\r\n        return f", "response": "Plots a histogram of the results of a Monte Carlo simulation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple of the raw array results and the PBE results as a table.", "response": "def get_results(self, title_prefix=\"\", title_override=\"\", rnd_dig=2):\r\n        \"\"\"\r\n        Constructs a summary of the results as an array, which might be\r\n        useful for writing the results of multiple algorithms to a table.\r\n        NOTE- This method must be called AFTER \"roll_mc\".\r\n\r\n        :param title_prefix: If desired, prefix the title (such as \"Alg 1 \")\r\n        :param title_override: Override the title string entirely\r\n        :param rnd_dig: the number of digits to round to\r\n        :return: A tuple of the raw array results and PBE results, as:\r\n                 [Description, Typical Array, Mean, Std, 5%, 95%]\r\n        \"\"\"\r\n        # Check that roll_mc has been called\r\n        if not self.arr_res:\r\n            raise ValueError(\"Call roll_mc before getting results.\")\r\n        # Find a title using either the override or _construct_title method\r\n        if title_override:\r\n            title = title_override\r\n        else:\r\n            ctitle = PBE._construct_title(self.num_dice, self.dice_type,\r\n                        self.add_val, self.num_attribute, self.keep_attribute,\r\n                        self.keep_dice, self.reroll, self.num_arrays)\r\n            title = title_prefix + ctitle\r\n        # Find the typical array\r\n        typ_arr = \"; \".join([str(round(x, rnd_dig))\r\n                             for x in self.arr_res[\"means\"]])\r\n        res_row = [title, typ_arr,\r\n                   round(self.pbe_res[\"means\"], rnd_dig),\r\n                   round(self.pbe_res[\"stds\"], rnd_dig),\r\n                   round(self.pbe_res[\"5percentile\"], rnd_dig),\r\n                   round(self.pbe_res[\"95percentile\"], rnd_dig)]\r\n        return res_row"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine whether the contigs. fasta output file from SPAdes is present. Set the. bestassemblyfile attribute to NA.", "response": "def best_assemblyfile(self):\n        \"\"\"\n        Determine whether the contigs.fasta output file from SPAdes is present. If not, set the .bestassembly\n        attribute to 'NA'\n        \"\"\"\n        for sample in self.metadata:\n            # Set the name of the unfiltered spades assembly output file\n            assembly_file = os.path.join(sample.general.spadesoutput, 'contigs.fasta')\n            if os.path.isfile(assembly_file):\n                sample.general.bestassemblyfile = assembly_file\n            else:\n                sample.general.bestassemblyfile = 'NA'\n            # Set the name of the filtered assembly file\n            filteredfile = os.path.join(sample.general.outputdirectory, '{}.fasta'.format(sample.name))\n            # Add the name and path of the filtered file to the metadata\n            sample.general.filteredfile = filteredfile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the assembly command in a multi - threaded fashion", "response": "def assemble(self):\n        \"\"\"Run the assembly command in a multi-threaded fashion\"\"\"\n        threadlock = threading.Lock()\n        while True:\n            (sample, command) = self.assemblequeue.get()\n            if command and not os.path.isfile(os.path.join(sample.general.spadesoutput, 'contigs.fasta')):\n                # execute(command)\n                out, err = run_subprocess(command)\n                threadlock.acquire()\n                write_to_logfile(command, command, self.logfile, sample.general.logout, sample.general.logerr,\n                                 None, None)\n                write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n                threadlock.release()\n                #\n                call(command, shell=True, stdout=open(os.devnull, 'wb'), stderr=open(os.devnull, 'wb'))\n            dotter()\n            # Signal to the queue that the job is done\n            self.assemblequeue.task_done()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retrieve_dataset_handles_or_version_numbers_of_all_versions(self, drs_id, prefix):\n        '''\n        :return: Dict. Should never return None.\n        :raise: SolrSwitchedOff\n        :raise SolrError: If ...\n        '''\n        self.__reset_error_messages()\n        response_json = self.__ask_solr_for_handles_or_version_numbers_of_all_versions(drs_id)\n        result_dict = self.__parse_result_handles_or_version_numbers_of_all_versions(response_json, prefix)\n        return result_dict", "response": "Retrieve dataset handles or version numbers of all versions of a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_totals(self):\n        total_added = 0\n        total_removed = 0\n\n        patch = PatchSet(self.diff)\n        total_added += sum([\n            edit.added for edit in patch\n        ])\n        total_removed += sum([\n            edit.removed for edit in patch\n        ])\n\n        return {\n            self.ADD: total_added,\n            self.DEL: total_removed\n        }", "response": "Calculates total additions and deletions of the entry in the dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting author of commit", "response": "def get_author(self):\n        \"\"\"Gets author\n\n        :return: author of commit\n        \"\"\"\n        author = self.commit.author\n\n        out = \"\"\n        if author.name is not None:\n            out += author.name\n\n        if author.email is not None:\n            out += \" (\" + author.email + \")\"\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_diff_amounts(self):\n        diffs = []\n\n        last_commit = None\n        for commit in self.repo.iter_commits():\n            if last_commit is not None:\n                diff = self.get_diff(commit.hexsha, last_commit.hexsha)\n                total_changed = diff[Diff.ADD] + diff[Diff.DEL]\n                diffs.append(total_changed)\n\n            last_commit = commit\n\n        return diffs", "response": "Gets list of total diff between 2 consecutive commits since start\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating total additions and deletions and returns a dictionary with the total additions and deletions", "response": "def get_diff(self, commit, other_commit):\n        \"\"\"Calculates total additions and deletions\n\n        :param commit: First commit\n        :param other_commit: Second commit\n        :return: dictionary: Dictionary with total additions and deletions\n        \"\"\"\n        print(other_commit, \"VS\", commit)\n        diff = self.repo.git.diff(commit, other_commit)\n        return Diff(diff).get_totals()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_version(self, diff_to_increase_ratio):\n        diffs = self.get_diff_amounts()\n        version = Version()\n\n        for diff in diffs:\n            version.increase_by_changes(diff, diff_to_increase_ratio)\n\n        return version", "response": "Gets version of this code based on commits"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a new version of the last version known .", "response": "def get_new_version(self, last_version, last_commit,\n                        diff_to_increase_ratio):\n        \"\"\"Gets new version\n\n        :param last_version: last version known\n        :param last_commit: hash of commit of last version\n        :param diff_to_increase_ratio: Ratio to convert number of changes into\n        :return: new version\n        \"\"\"\n\n        version = Version(last_version)\n        diff = self.get_diff(last_commit, self.get_last_commit_hash())\n        total_changed = diff[Diff.ADD] + diff[Diff.DEL]\n\n        version.increase_by_changes(total_changed, diff_to_increase_ratio)\n        return version"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pretty_version(self, diff_to_increase_ratio):\n        version = self.get_version(diff_to_increase_ratio)\n        build = self.get_last_commit_hash()\n        return str(version) + \" (\" + build + \")\"", "response": "Returns a pretty version of the repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a MIME message from the given subject and text.", "response": "def get_mime_message(subject, text):\n    \"\"\"Creates MIME message\n\n    :param subject: Subject of email\n    :param text: Email content\n    :return: Email formatted as HTML ready to be sent\n    \"\"\"\n    message = MIMEText(\n        \"<html>\" +\n        str(text).replace(\"\\n\", \"<br>\") +\n        \"</html>\", \"html\"\n    )\n    message[\"subject\"] = str(subject)\n    return message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend an email to me with this message", "response": "def send_email(sender, msg, driver):\n    \"\"\"Sends email to me with this message\n\n    :param sender: Sender of email\n    :param msg: Message to send to me\n    :param driver: GMail authenticator\n    \"\"\"\n    driver.users().messages().send(\n        userId=sender,\n        body=msg\n    ).execute()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the contents of the README. rst file as a Unicode string.", "response": "def get_readme():\n    \"\"\"Get the contents of the ``README.rst`` file as a Unicode string.\"\"\"\n    try:\n        import pypandoc\n        description = pypandoc.convert('README.md', 'rst')\n    except (IOError, ImportError):\n        description = open('README.md').read()\n\n    return description"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_absolute_path(*args):\n    directory = os.path.dirname(os.path.abspath(__file__))\n    return os.path.join(directory, *args)", "response": "Transform relative pathnames into absolute pathnames."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the version of the package.", "response": "def get_version():\n    \"\"\"Get the version of `package` (by extracting it from the source code).\"\"\"\n    module_path = get_absolute_path('pip_save', '__init__.py')\n    with open(module_path) as handle:\n        for line in handle:\n            match = re.match(r'^__version__\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']$', line)\n            if match:\n                return match.group(1)\n    raise Exception(\"Failed to extract version from %s!\" % module_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _repack(h5file):\n    f1, opened = _openfile(h5file) \n    filename1 = f1.filename\n    filename2 = filename1 + '_repack_tmp'\n    f2 = h5py.File(filename2, 'w')\n    for key in f1.keys():\n        # print 'copying', key\n        f1.copy(key, f2)\n    f1.close()\n    f2.close()\n    filename_tmp = filename1 + '_repack_rename_tmp'\n    os.rename(filename1, filename_tmp)\n    os.rename(filename2, filename1) \n    if opened:\n        f = None  \n    else:\n        f = h5py.File(filename1)\n    os.remove(filename_tmp)\n    return f", "response": "Repacks the archive to remove freespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _openfile(h5file):\n    if isinstance(h5file, h5py.File):\n        f = h5file\n        opened = False\n    elif isinstance(h5file, basestring):\n        f = h5py.File(h5file)\n        opened = True\n    else:\n        msg = \"h5file must be a h5py.File object or a string (path).\"\n        raise TypeError, msg    \n    return f, opened", "response": "Open an archive if input is a path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving empty rows from dataset.", "response": "def trim(self, key):\n        \"\"\"\n        Removes empty rows from dataset... I am still wanting to use this???\n\n        :param key: the dataset to trim\n        :type key: str\n        \"\"\"\n        current_index = self.meta[key]['cursor']\n        self.hdf5[key].resize(current_index, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consolidate(self, key):\n        if self.meta[key]['mode'] not in ['continuous']:\n            print \"consolidation not supported for mode: \", self.meta[key]['mode']\n            return\n\n        # get a copy of the attributes saved, then delete placeholder\n        attr_tmp = self.hdf5[key].attrs.items()\n        del self.hdf5[key]\n\n        setnum = self.meta[key]['set_counter']\n        setnum -= 1 # convert from 1-indexed to 0-indexed\n        current_index = self.meta[key]['cursor']\n        total_samples = (self.chunk_size * setnum) + current_index\n        self.datasets[key] = self.hdf5.create_dataset(key, (total_samples,))\n        self.datasets[key].attrs['stim'] = '[ ' # space in case empty, closing replaces the space and not the [\n        self.datasets[key].attrs['start'] = self.meta[key]['start']\n        self.datasets[key].attrs['mode'] = 'continuous'\n\n        for iset in range(0, setnum):\n            self.datasets[key][iset*self.chunk_size:(iset+1)*self.chunk_size] = self.datasets[key+'_set'+str(iset+1)][:]\n            self.datasets[key].attrs['stim'] = self.datasets[key].attrs['stim'] + self.datasets[key+'_set'+str(iset+1)].attrs['stim']\n        \n        # last set may not be complete\n        if current_index != 0:\n            self.datasets[key][setnum*self.chunk_size:(setnum*self.chunk_size)+current_index] = self.datasets[key+'_set'+str(setnum+1)][:current_index]\n            self.datasets[key].attrs['stim'] = self.datasets[key].attrs['stim'] + self.datasets[key+'_set'+str(setnum+1)].attrs['stim']\n\n        # make sure we have a closing bracket\n        if self.datasets[key].attrs['stim'][-1] != ']':\n            self.datasets[key].attrs['stim'] = self.datasets[key].attrs['stim'] + ']'\n        \n        # copy back attributes from placeholder\n        for k, v in attr_tmp:\n            self.datasets[key].attrs[k] = v\n\n        # now go ahead and delete fractional sets.\n        for iset in range(setnum+1):\n            del self.datasets[key+'_set'+str(iset+1)]\n            del self.hdf5[key+'_set'+str(iset+1)]\n\n        print 'consolidated', self.hdf5.keys()\n        print 'stim attr', self.datasets[key].attrs['stim']\n        print\n        self.needs_repack = True", "response": "This function consolidates a continuous acquisition into a single dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend restful post http request decorator Provide a brief way to manipulate restful api, :param method: :class:`str`, :return: :class:`func`", "response": "def request(method='GET'):\r\n    \"\"\"send restful post http request decorator\r\n\r\n    Provide a brief way to manipulate restful api,\r\n\r\n    :param method: :class:`str`,\r\n\r\n    :return: :class:`func`\r\n    \"\"\"\r\n\r\n    def decorator(func):\r\n        @functools.wraps(func)\r\n        def action(self, *args, **kwargs):\r\n            f = furl(self.server)\r\n            path, body = func(self, *args, **kwargs)\r\n\r\n            # deal with query string\r\n            query = dict()\r\n            if isinstance(path, tuple):\r\n                path, query = path\r\n            f.path.add(path)\r\n            f.query.set(query)\r\n\r\n            status_code, result = send_rest(f.url, method=method.upper(),\r\n                                            body=body,\r\n                                            session=self.session,\r\n                                            headers=self.headers)\r\n            if status_code != httplib.OK:\r\n                self.logger.error(\"{impl} {url} headers: {headers}, code: {code}\".format(\r\n                    impl=method, url=f.url, headers=self.headers, code=status_code))\r\n            return status_code, result\r\n\r\n        return action\r\n\r\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the image name into three element tuple like below", "response": "def parse_image_name(name):\r\n    \"\"\"\r\n    parse the image name into three element tuple, like below:\r\n    (repository, name, version)\r\n    :param name: `class`:`str`, name\r\n    :return: (repository, name, version)\r\n    \"\"\"\r\n    name = name or \"\"\r\n    if '/' in name:\r\n        repository, other = name.split('/')\r\n    else:\r\n        repository, other = None, name\r\n\r\n    if ':' in other:\r\n        name, version = other.split(':')\r\n    else:\r\n        name, version = other, 'latest'\r\n\r\n    return repository, name, version"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_channel_page(self):\n        channel_url = YOUTUBE_USER_BASE_URL + self.channel_name  # url\n        source_page = Webpage(\n            channel_url).get_html_source()  # get source page of channel\n        return source_page", "response": "Fetches source page of youtube channel\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_channel_id(self):\n        soup = BeautifulSoup(\n            self.get_channel_page(), \"lxml\"\n        )  # parser for source page\n        channel_id = soup.find_all(\n            \"span\",\n            {\n                \"class\": \"channel-header-subscription-button-container\"\n            }\n        )  # get all good spans\n        channel_id = channel_id[0].find_all(\"button\")[\n            0]  # get button in first span\n        channel_id = channel_id[\"data-channel-external-id\"]  # get id\n        return channel_id", "response": "Fetches id of channel in youtube"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget channel id and then creates feed url from url of video", "response": "def get_feed_url_from_video(video_url):\n        \"\"\"Gets channel id and then creates feed url\n\n        :param video_url: Url of video\n        :return: feed url\n        \"\"\"\n        web_page = Webpage(video_url)\n        web_page.get_html_source()\n        channel_id = \\\n            web_page.soup.find_all(\"div\", {\"class\": \"yt-user-info\"})[0].a[\n                \"href\"]\n        channel_id = str(channel_id).strip().replace(\"/channel/\",\n                                                     \"\")  # get channel id\n        return YoutubeChannel.get_feed_url_from_id(channel_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening a single labeled image at path and get needed information return as a dictionary", "response": "def process_file(path):\n    \"\"\" Open a single labeled image at path and get needed information, return as a dictionary\"\"\"\n    info = dict()\n    with fits.open(path) as hdu:\n        head = hdu[0].header\n        data = hdu[0].data\n        labels = {theme: value for value, theme in list(hdu[1].data)}\n    info['filename'] = os.path.basename(path)\n    info['trainer'] = head['expert']\n    info['date-label'] = dateparser.parse(head['date-lab'])\n    info['date-observation'] = dateparser.parse(head['date-end'])\n    for theme in themes:\n        info[theme + \"_count\"] = np.sum(data == labels[theme])\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot the counts of a given theme from a created database over time", "response": "def plot_counts(df, theme):\n    \"\"\" plot the counts of a given theme from a created database over time\"\"\"\n    dates, counts = df['date-observation'], df[theme + \"_count\"]\n    fig, ax = plt.subplots()\n    ax.set_ylabel(\"{} pixel counts\".format(\" \".join(theme.split(\"_\"))))\n    ax.set_xlabel(\"observation date\")\n    ax.plot(dates, counts, '.')\n    fig.autofmt_xdate()\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef datetime(value):\n    if isinstance(value, (date, builtin_datetime)):\n        pass\n    elif value < 10000000000:\n        value = unix2datetime(value)\n    else:\n        value = milli2datetime(value)\n\n    return datetime2string(value, \"%Y-%m-%d %H:%M:%S.%f\").rstrip(\".000000\").rstrip(\"000\")", "response": "Convert unix timestamp to GMT string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unix(value):\n    if isinstance(value, (date, builtin_datetime)):\n        pass\n    elif value < 10000000000:\n        value = unix2datetime(value)\n    else:\n        value = milli2datetime(value)\n\n    return str(datetime2unix(value))", "response": "Convert a date or datetime to unix timestamp\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert value to JSON", "response": "def json(value, pretty=True):\n    \"\"\"\n    convert value to JSON\n    :param value:\n    :param pretty:\n    :return:\n    \"\"\"\n    if not _Duration:\n        _late_import()\n    return _json_encoder(value, pretty=pretty)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tab(value):\n    if is_data(value):\n        h, d = transpose(*wrap(value).leaves())\n        return (\n            \"\\t\".join(map(value2json, h)) +\n            CR +\n            \"\\t\".join(map(value2json, d))\n        )\n    else:\n        text_type(value)", "response": "convert single value to tab - delimited form including a header"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef indent(value, prefix=u\"\\t\", indent=None):\n    if indent != None:\n        prefix = prefix * indent\n\n    value = toString(value)\n    try:\n        content = value.rstrip()\n        suffix = value[len(content):]\n        lines = content.splitlines()\n        return prefix + (CR + prefix).join(lines) + suffix\n    except Exception as e:\n        raise Exception(u\"Problem with indent of value (\" + e.message + u\")\\n\" + text_type(toString(value)))", "response": "indent given string using prefix * indent as prefix for each line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef outdent(value):\n    try:\n        num = 100\n        lines = toString(value).splitlines()\n        for l in lines:\n            trim = len(l.lstrip())\n            if trim > 0:\n                num = min(num, len(l) - len(l.lstrip()))\n        return CR.join([l[num:] for l in lines])\n    except Exception as e:\n        if not _Log:\n            _late_import()\n\n        _Log.error(\"can not outdent value\", e)", "response": "remove common whitespace prefix from lines\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef round(value, decimal=None, digits=None, places=None):\n    value = float(value)\n    if value == 0.0:\n        return \"0\"\n\n    digits = coalesce(digits, places)\n    if digits != None:\n        left_of_decimal = int(math.ceil(math.log10(abs(value))))\n        decimal = digits - left_of_decimal\n\n    right_of_decimal = max(decimal, 0)\n    format = \"{:.\" + text_type(right_of_decimal) + \"f}\"\n    return format.format(_round(value, decimal))", "response": "ROUND THE VALUE TO ROUND"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn index of find in value beginning at start", "response": "def find(value, find, start=0):\n    \"\"\"\n    Return index of `find` in `value` beginning at `start`\n    :param value:\n    :param find:\n    :param start:\n    :return: If NOT found, return the length of `value` string\n    \"\"\"\n    l = len(value)\n    if is_list(find):\n        m = l\n        for f in find:\n            i = value.find(f, start)\n            if i == -1:\n                continue\n            m = min(m, i)\n        return m\n    else:\n        i = value.find(find, start)\n        if i == -1:\n            return l\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving white space from a string.", "response": "def strip(value):\n    \"\"\"\n    REMOVE WHITESPACE (INCLUDING CONTROL CHARACTERS)\n    \"\"\"\n    if not value or (ord(value[0]) > 32 and ord(value[-1]) > 32):\n        return value\n\n    s = 0\n    e = len(value)\n    while s < e:\n        if ord(value[s]) > 32:\n            break\n        s += 1\n    else:\n        return \"\"\n\n    for i in reversed(range(s, e)):\n        if ord(value[i]) > 32:\n            return value[s:i + 1]\n\n    return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn first substring between prefix and suffix", "response": "def between(value, prefix, suffix, start=0):\n    \"\"\"\n    Return first substring between `prefix` and `suffix`\n    :param value:\n    :param prefix: if None then return the prefix that ends with `suffix`\n    :param suffix: if None then return the suffix that begins with `prefix`\n    :param start: where to start the search\n    :return:\n    \"\"\"\n    value = toString(value)\n    if prefix == None:\n        e = value.find(suffix, start)\n        if e == -1:\n            return None\n        else:\n            return value[:e]\n\n    s = value.find(prefix, start)\n    if s == -1:\n        return None\n    s += len(prefix)\n\n    e = value.find(suffix, s)\n    if e == -1:\n        return None\n\n    s = value.rfind(prefix, start, e) + len(prefix)  # WE KNOW THIS EXISTS, BUT THERE MAY BE A RIGHT-MORE ONE\n\n    return value[s:e]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef right_align(value, length):\n    if length <= 0:\n        return u\"\"\n\n    value = text_type(value)\n\n    if len(value) < length:\n        return (\" \" * (length - len(value))) + value\n    else:\n        return value[-length:]", "response": "Aligns a string to the right"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting the value as a list of comma separated strings.", "response": "def comma(value):\n    \"\"\"\n    FORMAT WITH THOUSANDS COMMA (,) SEPARATOR\n    \"\"\"\n    try:\n        if float(value) == _round(float(value), 0):\n            output = \"{:,}\".format(int(value))\n        else:\n            output = \"{:,}\".format(float(value))\n    except Exception:\n        output = text_type(value)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a JSON - quoted version of the value.", "response": "def quote(value):\n    \"\"\"\n    return JSON-quoted value\n    :param value:\n    :return:\n    \"\"\"\n    if value == None:\n        output = \"\"\n    elif is_text(value):\n        output = encode_basestring(value)\n    else:\n        output = _json.dumps(value)\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand a template with a value", "response": "def expand_template(template, value):\n    \"\"\"\n    :param template: A UNICODE STRING WITH VARIABLE NAMES IN MOUSTACHES `{{.}}`\n    :param value: Data HOLDING THE PARAMTER VALUES\n    :return: UNICODE STRING WITH VARIABLES EXPANDED\n    \"\"\"\n    value = wrap(value)\n    if is_text(template):\n        return _simple_expand(template, (value,))\n\n    return _expand(template, (value,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves NON-ALPHANUMERIC CHARACTERS FOR SOME REASON translate CAN NOT BE CALLED: ERROR: translate() takes exactly one argument (2 given) File \"C:\\Python27\\lib\\string.py\", line 493, in translate", "response": "def deformat(value):\n    \"\"\"\n    REMOVE NON-ALPHANUMERIC CHARACTERS\n\n    FOR SOME REASON translate CAN NOT BE CALLED:\n        ERROR: translate() takes exactly one argument (2 given)\n        File \"C:\\Python27\\lib\\string.py\", line 493, in translate\n    \"\"\"\n    output = []\n    for c in value:\n        if c in delchars:\n            continue\n        output.append(c)\n    return \"\".join(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _expand(template, seq):\n    if is_text(template):\n        return _simple_expand(template, seq)\n    elif is_data(template):\n        # EXPAND LISTS OF ITEMS USING THIS FORM\n        # {\"from\":from, \"template\":template, \"separator\":separator}\n        template = wrap(template)\n        assert template[\"from\"], \"Expecting template to have 'from' attribute\"\n        assert template.template, \"Expecting template to have 'template' attribute\"\n\n        data = seq[-1][template[\"from\"]]\n        output = []\n        for d in data:\n            s = seq + (d,)\n            output.append(_expand(template.template, s))\n        return coalesce(template.separator, \"\").join(output)\n    elif is_list(template):\n        return \"\".join(_expand(t, seq) for t in template)\n    else:\n        if not _Log:\n            _late_import()\n\n        _Log.error(\"can not handle\")", "response": "Expand a list of objects into a single string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a diff to a text.", "response": "def apply_diff(text, diff, reverse=False, verify=True):\n    \"\"\"\n    SOME EXAMPLES OF diff\n    #@@ -1 +1 @@\n    #-before china goes live, the content team will have to manually update the settings for the china-ready apps currently in marketplace.\n    #+before china goes live (end January developer release, June general audience release) , the content team will have to manually update the settings for the china-ready apps currently in marketplace.\n    @@ -0,0 +1,3 @@\n    +before china goes live, the content team will have to manually update the settings for the china-ready apps currently in marketplace.\n    +\n    +kward has the details.\n    @@ -1 +1 @@\n    -before china goes live (end January developer release, June general audience release), the content team will have to manually update the settings for the china-ready apps currently in marketplace.\n    +before china goes live , the content team will have to manually update the settings for the china-ready apps currently in marketplace.\n    @@ -3 +3 ,6 @@\n    -kward has the details.+kward has the details.\n    +\n    +Target Release Dates :\n    +https://mana.mozilla.org/wiki/display/PM/Firefox+OS+Wave+Launch+Cross+Functional+View\n    +\n    +Content Team Engagement & Tasks : https://appreview.etherpad.mozilla.org/40\n    \"\"\"\n\n    if not diff:\n        return text\n    output = text\n    hunks = [\n        (new_diff[start_hunk], new_diff[start_hunk+1:end_hunk])\n        for new_diff in [[d.lstrip() for d in diff if d.lstrip() and d != \"\\\\ No newline at end of file\"] + [\"@@\"]]  # ANOTHER REPAIR\n        for start_hunk, end_hunk in pairwise(i for i, l in enumerate(new_diff) if l.startswith('@@'))\n    ]\n    for header, hunk_body in (reversed(hunks) if reverse else hunks):\n        matches = DIFF_PREFIX.match(header.strip())\n        if not matches:\n            if not _Log:\n                _late_import()\n\n            _Log.error(\"Can not handle \\n---\\n{{diff}}\\n---\\n\",  diff=diff)\n\n        removes = tuple(int(i.strip()) for i in matches.group(1).split(\",\"))  # EXPECTING start_line, length TO REMOVE\n        remove = Data(start=removes[0], length=1 if len(removes) == 1 else removes[1])  # ASSUME FIRST LINE\n        adds = tuple(int(i.strip()) for i in matches.group(2).split(\",\"))  # EXPECTING start_line, length TO ADD\n        add = Data(start=adds[0], length=1 if len(adds) == 1 else adds[1])\n\n        if add.length == 0 and add.start == 0:\n            add.start = remove.start\n\n        def repair_hunk(hunk_body):\n            # THE LAST DELETED LINE MAY MISS A \"\\n\" MEANING THE FIRST\n            # ADDED LINE WILL BE APPENDED TO THE LAST DELETED LINE\n            # EXAMPLE: -kward has the details.+kward has the details.\n            # DETECT THIS PROBLEM FOR THIS HUNK AND FIX THE DIFF\n            if reverse:\n                last_lines = [\n                    o\n                    for b, o in zip(reversed(hunk_body), reversed(output))\n                    if b != \"+\" + o\n                ]\n                if not last_lines:\n                    return hunk_body\n\n                last_line = last_lines[0]\n                for problem_index, problem_line in enumerate(hunk_body):\n                    if problem_line.startswith('-') and problem_line.endswith('+' + last_line):\n                        split_point = len(problem_line) - (len(last_line) + 1)\n                        break\n                    elif problem_line.startswith('+' + last_line + \"-\"):\n                        split_point = len(last_line) + 1\n                        break\n                else:\n                    return hunk_body\n            else:\n                if not output:\n                    return hunk_body\n                last_line = output[-1]\n                for problem_index, problem_line in enumerate(hunk_body):\n                    if problem_line.startswith('+') and problem_line.endswith('-' + last_line):\n                        split_point = len(problem_line) - (len(last_line) + 1)\n                        break\n                    elif problem_line.startswith('-' + last_line + \"+\"):\n                        split_point = len(last_line) + 1\n                        break\n                else:\n                    return hunk_body\n\n            new_hunk_body = (\n                hunk_body[:problem_index] +\n                [problem_line[:split_point], problem_line[split_point:]] +\n                hunk_body[problem_index + 1:]\n            )\n            return new_hunk_body\n        hunk_body = repair_hunk(hunk_body)\n\n        if reverse:\n            new_output = (\n                output[:add.start - 1] +\n                [d[1:] for d in hunk_body if d and d[0] == '-'] +\n                output[add.start + add.length - 1:]\n            )\n        else:\n            new_output = (\n                output[:add.start - 1] +\n                [d[1:] for d in hunk_body if d and d[0] == '+'] +\n                output[add.start + remove.length - 1:]\n            )\n        output = new_output\n\n    if verify:\n        original = apply_diff(output, diff, not reverse, False)\n        if set(text) != set(original):  # bugzilla-etl diffs are a jumble\n\n            for t, o in zip_longest(text, original):\n                if t in ['reports: https://goo.gl/70o6w6\\r']:\n                    break  # KNOWN INCONSISTENCIES\n                if t != o:\n                    if not _Log:\n                        _late_import()\n                    _Log.error(\"logical verification check failed\")\n                    break\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef utf82unicode(value):\n    try:\n        return value.decode(\"utf8\")\n    except Exception as e:\n        if not _Log:\n            _late_import()\n\n        if not is_binary(value):\n            _Log.error(\"Can not convert {{type}} to unicode because it's not bytes\",  type= type(value).__name__)\n\n        e = _Except.wrap(e)\n        for i, c in enumerate(value):\n            try:\n                c.decode(\"utf8\")\n            except Exception as f:\n                _Log.error(\"Can not convert charcode {{c}} in string index {{i}}\", i=i, c=ord(c), cause=[e, _Except.wrap(f)])\n\n        try:\n            latin1 = text_type(value.decode(\"latin1\"))\n            _Log.error(\"Can not explain conversion failure, but seems to be latin1\", e)\n        except Exception:\n            pass\n\n        try:\n            a = text_type(value.decode(\"latin1\"))\n            _Log.error(\"Can not explain conversion failure, but seems to be latin1\", e)\n        except Exception:\n            pass\n\n        _Log.error(\"Can not explain conversion failure of \" + type(value).__name__ + \"!\", e)", "response": "Convert a string to unicode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pairwise(values):\n    i = iter(values)\n    a = next(i)\n\n    for b in i:\n        yield (a, b)\n        a = b", "response": "Yields the pair of entries in a sequence of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the model for this editor", "response": "def setModel(self, model):\n        \"\"\"Sets the QStimulusModel for this editor\"\"\"\n        self.stimModel = model\n        self.parameterModel = model.autoParams()\n        tone = self.stimModel.data(self.stimModel.index(0,0), QtCore.Qt.UserRole+1)\n        info = tone.auto_details()\n\n        # set max/mins\n        fmax = info['frequency']['max']\n        self.ui.freqStartSpnbx.setMaximum(fmax)\n        self.ui.freqStopSpnbx.setMaximum(fmax)\n        self.ui.freqStepSpnbx.setMaximum(500000)\n        dbmax = info['intensity']['max']\n        self.ui.dbStartSpnbx.setMaximum(dbmax)\n        self.ui.dbStopSpnbx.setMaximum(dbmax)\n        self.ui.dbStepSpnbx.setMaximum(500000)\n        self.ui.durSpnbx.setMaximum(info['duration']['max'])\n        self.ui.risefallSpnbx.setMaximum(info['risefall']['max'])\n\n        self.fmapper.setModel(self.parameterModel)\n        self.dbmapper.setModel(self.parameterModel)\n        self.fmapper.addMapping(self.ui.freqStartSpnbx, 1)\n        self.fmapper.addMapping(self.ui.freqStopSpnbx, 2)\n        self.fmapper.addMapping(self.ui.freqStepSpnbx, 3)\n        self.fmapper.addMapping(self.ui.freqNstepsLbl, 4, 'text')\n        self.dbmapper.addMapping(self.ui.dbStartSpnbx, 1)\n        self.dbmapper.addMapping(self.ui.dbStopSpnbx, 2)\n        self.dbmapper.addMapping(self.ui.dbStepSpnbx, 3)\n        self.dbmapper.addMapping(self.ui.dbNstepsLbl, 4, 'text')\n        self.fmapper.toFirst()\n        self.dbmapper.setCurrentIndex(1)\n\n        self.ui.durSpnbx.setValue(tone.duration())\n        self.ui.nrepsSpnbx.setValue(self.stimModel.repCount())\n        self.ui.risefallSpnbx.setValue(tone.risefall())\n        self.tone = tone"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setStimDuration(self):\n        duration = self.ui.durSpnbx.value()\n        self.tone.setDuration(duration)", "response": "Sets the duration of the StimulusModel from values pulled from\n        this widget"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the reps of the StimulusModel from values pulled from this widget", "response": "def setStimReps(self):\n        \"\"\"Sets the reps of the StimulusModel from values pulled from\n        this widget\"\"\"\n        reps = self.ui.nrepsSpnbx.value()\n        self.stimModel.setRepCount(reps)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the Risefall of the StimulusModel s tone from values pulled from this widget", "response": "def setStimRisefall(self):\n        \"\"\"Sets the Risefall of the StimulusModel's tone from values pulled from\n        this widget\"\"\"\n        rf = self.ui.risefallSpnbx.value()\n        self.tone.setRisefall(rf)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding all items to the parser passed in.", "response": "def add_arguments(self, parser, bootstrap=False):\n        \"\"\"Adds all items to the parser passed in.\n\n        Args:\n            parser (argparse.ArgumentParser): The parser to add all items to.\n            bootstrap (bool): Flag to indicate whether you only want to mark\n                bootstrapped items as required on the command-line.\n\n        \"\"\"\n        [item.add_argument(parser, bootstrap)\n         for item in self._get_items(bootstrap=False)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a source to the spec.", "response": "def add_source(self, label, source_type, **kwargs):\n        \"\"\"Add a source to the spec.\n\n        Sources should have a unique label. This will help tracing where your\n        configurations are coming from if you turn up the log-level.\n\n        The keyword arguments are significant. Different sources require\n        different keyword arguments. Required keys for each source_type are\n        listed below, for a detailed list of all possible arguments, see the\n        individual source's documentation.\n\n        source_type: dict\n            required keyword arguments:\n                - data - A dictionary\n\n        source_type: environment\n            No required keyword arguments.\n\n        source_type: etcd\n            required keyword arguments:\n                - client - A client from the python-etcd package.\n\n        source_type: json\n            required keyword arguments:\n                - filename - A JSON file.\n                - data - A string representation of JSON\n\n        source_type: kubernetes\n            required keyword arguments:\n                - client - A client from the kubernetes package\n                - name - The name of the ConfigMap to load\n\n        source_type: yaml\n            required keyword arguments:\n                - filename - A YAML file.\n\n        Args:\n            label (str): A label for the source.\n            source_type (str): A source type, available source types depend\n            on the packages installed. See ``yapconf.ALL_SUPPORTED_SOURCES``\n            for a complete list.\n\n        \"\"\"\n        self._sources[label] = get_source(label, source_type, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_item(self, fq_name):\n        names = fq_name.split(self._separator)\n        current = self._yapconf_items\n        for name in names:\n            if isinstance(current, (YapconfDictItem, YapconfListItem)):\n                current = current.children\n\n            if name not in current:\n                return None\n            current = current[name]\n        return current", "response": "Find an item in the specification by fully qualified name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a particular item in the specification.", "response": "def get_item(self, name, bootstrap=False):\n        \"\"\"Get a particular item in the specification.\n\n        Args:\n            name (str): The name of the item to retrieve.\n            bootstrap (bool): Only search bootstrap items\n\n        Returns (YapconfItem):\n            A YapconfItem if it is found, None otherwise.\n\n        \"\"\"\n        for item in self._get_items(bootstrap):\n            if item.name == name:\n                return item\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate items defaults to the values in new_defaults dict.", "response": "def update_defaults(self, new_defaults, respect_none=False):\n        \"\"\"Update items defaults to the values in the new_defaults dict.\n\n        Args:\n            new_defaults (dict): A key-value pair of new defaults to be\n                applied.\n            respect_none (bool): Flag to indicate if ``None`` values should\n                constitute an update to the default.\n\n        \"\"\"\n        for key, value in six.iteritems(new_defaults):\n            item = self.get_item(key)\n            if item is None:\n                raise YapconfItemNotFound(\"Cannot update default for {0}, \"\n                                          \"there is no config item by the \"\n                                          \"name of {1}\".format(key, key), None)\n\n            item.update_default(value, respect_none)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the documentation for this application.", "response": "def generate_documentation(self, app_name, **kwargs):\n        \"\"\"Generate documentation for this specification.\n\n        Documentation is generated in Markdown format. An example\n        of the generated documentation can be found at:\n\n        https://github.com/loganasherjones/yapconf/blob/master/example/doc.md\n\n        Args:\n            app_name (str): The name of your application.\n\n        Keyword Args:\n            output_file_name (str): If provided, will write to this file.\n            encoding (str): The encoding to use for the output file. Default\n            is utf-8.\n\n        Returns:\n            A string representation of the documentation.\n\n        \"\"\"\n        output_file = kwargs.get('output_file_name')\n        encoding = kwargs.get('encoding', 'utf-8')\n        doc_string = generate_markdown_doc(app_name, self)\n        if output_file:\n            with open(output_file, 'w', encoding=encoding) as doc_file:\n                doc_file.write(doc_string)\n\n        return doc_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a configuration file for the specified set of items.", "response": "def load_config(self, *args, **kwargs):\n        \"\"\"Load a config based on the arguments passed in.\n\n        The order of arguments passed in as \\*args is significant. It indicates\n        the order of precedence used to load configuration values. Each\n        argument can be a string, dictionary or a tuple. There is a special\n        case string called 'ENVIRONMENT', otherwise it will attempt to load the\n        filename passed in as a string.\n\n        By default, if a string is provided, it will attempt to load the\n        file based on the file_type passed in on initialization. If you\n        want to load a mixture of json and yaml files, you can specify them\n        as the 3rd part of a tuple.\n\n        Examples:\n            You can load configurations in any of the following ways:\n\n            >>> my_spec = YapconfSpec({'foo': {'type': 'str'}})\n            >>> my_spec.load_config('/path/to/file')\n            >>> my_spec.load_config({'foo': 'bar'})\n            >>> my_spec.load_config('ENVIRONMENT')\n            >>> my_spec.load_config(('label', {'foo': 'bar'}))\n            >>> my_spec.load_config(('label', '/path/to/file.yaml', 'yaml'))\n            >>> my_spec.load_config(('label', '/path/to/file.json', 'json'))\n\n            You can of course combine each of these and the order will be\n            held correctly.\n\n        Args:\n            *args:\n            **kwargs: The only supported keyword argument is 'bootstrap'\n                which will indicate that only bootstrap configurations\n                should be loaded.\n\n        Returns:\n            box.Box: A Box object which is subclassed from dict. It should\n                behave exactly as a dictionary. This object is guaranteed to\n                contain at least all of your required configuration items.\n\n        Raises:\n            YapconfLoadError: If we attempt to load your args and something\n                goes wrong.\n            YapconfItemNotFound: If an item is required but could not be found\n                in the configuration.\n            YapconfItemError: If a possible value was found but the type\n                cannot be determined.\n            YapconfValueError: If a possible value is found but during\n                conversion, an exception was raised.\n        \"\"\"\n        bootstrap = kwargs.get('bootstrap', False)\n        overrides = self._generate_overrides(*args)\n        config = self._generate_config_from_overrides(overrides, bootstrap)\n        return Box(config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nspawns a config watcher in a separate daemon thread.", "response": "def spawn_watcher(self, label, target=None, eternal=False):\n        \"\"\"Spawns a config watcher in a separate daemon thread.\n\n        If a particular config value changes, and the item has a\n        ``watch_target`` defined, then that method will be called.\n\n        If a ``target`` is passed in, then it will call the ``target``\n        anytime the config changes.\n\n        Args:\n            label (str): Should match a label added through ``add_source``\n            target (func): Should be a function that takes two arguments,\n            the old configuration and the new configuration.\n            eternal (bool): Determines if watcher threads should be restarted\n            if they die.\n\n        Returns:\n            The thread that was spawned.\n\n        \"\"\"\n\n        if label not in self._sources:\n            raise YapconfSourceError(\n                'Cannot watch %s no source named %s' % (label, label)\n            )\n\n        current_config = self._sources[label].get_data()\n        handler = ConfigChangeHandler(current_config, self, target)\n        return self._sources[label].watch(handler, eternal)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmigrating a configuration file.", "response": "def migrate_config_file(\n        self,\n        config_file_path,\n        always_update=False,\n        current_file_type=None,\n        output_file_name=None,\n        output_file_type=None,\n        create=True,\n        update_defaults=True,\n        dump_kwargs=None,\n        include_bootstrap=True,\n    ):\n        \"\"\"Migrates a configuration file.\n\n        This is used to help you update your configurations throughout the\n        lifetime of your application. It is probably best explained through\n        example.\n\n        Examples:\n            Assume we have a JSON config file ('/path/to/config.json')\n            like the following:\n            ``{\"db_name\": \"test_db_name\", \"db_host\": \"1.2.3.4\"}``\n\n            >>> spec = YapconfSpec({\n            ...    'db_name': {\n            ...        'type': 'str',\n            ...        'default': 'new_default',\n            ...        'previous_defaults': ['test_db_name']\n            ...    },\n            ...    'db_host': {\n            ...        'type': 'str',\n            ...        'previous_defaults': ['localhost']\n            ...    }\n            ... })\n\n            We can migrate that file quite easily with the spec object:\n\n            >>> spec.migrate_config_file('/path/to/config.json')\n\n            Will result in /path/to/config.json being overwritten:\n            ``{\"db_name\": \"new_default\", \"db_host\": \"1.2.3.4\"}``\n\n        Args:\n            config_file_path (str): The path to your current config\n            always_update (bool): Always update values (even to None)\n            current_file_type (str): Defaults to self._file_type\n            output_file_name (str): Defaults to the current_file_path\n            output_file_type (str): Defaults to self._file_type\n            create (bool): Create the file if it doesn't exist (otherwise\n                error if the file does not exist).\n            update_defaults (bool): Update values that have a value set to\n                something listed in the previous_defaults\n            dump_kwargs (dict): A key-value pair that will be passed to dump\n            include_bootstrap (bool): Include bootstrap items in the output\n\n        Returns:\n            box.Box: The newly migrated configuration.\n        \"\"\"\n\n        current_file_type = current_file_type or self._file_type\n        output_file_type = output_file_type or self._file_type\n        output_file_name = output_file_name or config_file_path\n\n        current_config = self._get_config_if_exists(config_file_path,\n                                                    create,\n                                                    current_file_type)\n\n        migrated_config = {}\n\n        if include_bootstrap:\n            items = self._yapconf_items.values()\n        else:\n            items = [\n                item for item in self._yapconf_items.values()\n                if not item.bootstrap\n            ]\n        for item in items:\n            item.migrate_config(current_config, migrated_config,\n                                always_update, update_defaults)\n\n        if create:\n            yapconf.dump_data(migrated_config,\n                              filename=output_file_name,\n                              file_type=output_file_type,\n                              klazz=YapconfLoadError,\n                              dump_kwargs=dump_kwargs)\n\n        return Box(migrated_config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refractory(times, refract=0.002):\n\ttimes_refract = []\n\ttimes_refract.append(times[0])\n\tfor i in range(1,len(times)):\n\t\tif times_refract[-1]+refract <= times[i]:\n\t\t\ttimes_refract.append(times[i])        \n\treturn times_refract", "response": "Removes spikes in times that do not satisfy refractor period in seconds\n\t returns the list of times in seconds\n\t"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef spike_times(signal, threshold, fs, absval=True):\n    times = []\n    if absval:\n        signal = np.abs(signal)\n    over, = np.where(signal>threshold)\n    segments, = np.where(np.diff(over) > 1)\n\n    if len(over) > 1:\n        if len(segments) == 0:\n            segments = [0, len(over)-1]\n        else:\n            # add end points to sections for looping\n            if segments[0] != 0:\n                segments = np.insert(segments, [0], [0])\n            else:\n                #first point in singleton\n                times.append(float(over[0])/fs)\n                if 1 not in segments:\n                    # make sure that first point is in there\n                    segments[0] = 1\n            if segments[-1] != len(over)-1:\n                segments = np.insert(segments, [len(segments)], [len(over)-1])\n            else:\n                times.append(float(over[-1])/fs)\n\n        for iseg in range(1,len(segments)):\n            if segments[iseg] - segments[iseg-1] == 1:\n                # only single point over threshold\n                idx = over[segments[iseg]]\n            else:\n                segments[0] = segments[0]-1                \n                # find maximum of continuous set over max\n                idx = over[segments[iseg-1]+1] + np.argmax(signal[over[segments[iseg-1]+1]:over[segments[iseg]]])\n            times.append(float(idx)/fs)\n    elif len(over) == 1:\n        times.append(float(over[0])/fs)\n        \n    if len(times)>0:\n    \treturn refractory(times)\n    else:\n    \treturn times", "response": "Detect spikes from a given signal and return the time of the maximum of the spike."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bin_spikes(spike_times, binsz):\n    bins = np.empty((len(spike_times),), dtype=int)\n    for i, stime in enumerate(spike_times):\n        # around to fix rounding errors\n        bins[i] = np.floor(np.around(stime/binsz, 5))\n    return bins", "response": "Sort spike times into bins"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spike_latency(signal, threshold, fs):\n    over, = np.where(signal>threshold)\n    segments, = np.where(np.diff(over) > 1)\n\n    if len(over) > 1:\n        if len(segments) == 0:\n            # only signal peak\n            idx = over[0] + np.argmax(signal[over[0]:over[-1]])\n            latency = float(idx)/fs\n        elif segments[0] == 0:\n            #first point in singleton\n            latency = float(over[0])/fs\n        else:\n            idx = over[0] + np.argmax(signal[over[0]:over[segments[0]]])\n            latency = float(idx)/fs\n    elif len(over) > 0:\n        latency = float(over[0])/fs\n    else:\n        latency = np.nan\n\n    return latency", "response": "Find the latency of the first spike over threshold"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef firing_rate(spike_times, window_size=None):\n    if len(spike_times) == 0:\n        return 0\n\n    if window_size is None:\n        if len(spike_times) > 1:\n            window_size = spike_times[-1] - spike_times[0]\n        elif len(spike_times) > 0:\n            # Only one spike, and no window - what to do?\n            window_size = 1\n        else:\n            window_size = 0\n\n    rate = window_size/len(spike_times)\n    return rate", "response": "Calculate the firing rate of a list of spikes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts the number of spikes in a dataset.", "response": "def dataset_spike_counts(dset, threshold, fs):\n    \"\"\"Dataset should be of dimensions (trace, rep, samples)\"\"\"\n    if len(dset.shape) == 3:\n        results = np.zeros(dset.shape[0])\n        for itrace in range(dset.shape[0]):\n            results[itrace] = count_spikes(dset[itrace], threshold, fs)\n        return results\n    elif len(dset.shape == 2):\n        return count_spikes(dset, threshold, fs)\n    else:\n        raise Exception(\"Improper data dimensions\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the job s output or log file to determing if the completion criteria is met.", "response": "def is_complete(self):\n        \"\"\" Checks the job's output or log file to determing if\n        the completion criteria was met.\n        \"\"\"\n        qstat = self._grep_qstat('complete')\n        comp = self._grep_status('complete')\n        if qstat and comp:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck to see if the job errored out.", "response": "def is_error(self):\n        \"\"\" Checks to see if the job errored out. \"\"\"\n        qstat = self._grep_qstat('error')\n        err = self._grep_status('error')\n        if qstat and err:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _grep_qstat(self, status_type='complete'):\n        args = \"qstat -e {}\".format(self.id).split()\n        res, _ = call(args)\n        if res == '': return False\n        res = res.split('\\n')[2].split()[4]\n\n        if status_type == 'complete' and res == 'C':\n            return True\n        elif status_type == 'error' and (res == 'E' or res == 'C'):\n            return True\n        elif status_type == 'running' and res == 'R':\n            return True\n        elif status_type == 'queued' and res == 'Q':\n            return True\n        elif status_type == 'gone' and 'unknown job id' in str(res).lower():\n            return True\n        else:\n            return False", "response": "Greps qstat for information from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitiating an OAuth handshake with MediaWiki.", "response": "def initiate(mw_uri, consumer_token, callback='oob',\n             user_agent=defaults.USER_AGENT):\n    \"\"\"\n    Initiate an oauth handshake with MediaWiki.\n\n    :Parameters:\n        mw_uri : `str`\n            The base URI of the MediaWiki installation.  Note that the URI\n            should end in ``\"index.php\"``.\n        consumer_token : :class:`~mwoauth.ConsumerToken`\n            A token representing you, the consumer.  Provided by MediaWiki via\n            ``Special:OAuthConsumerRegistration``.\n        callback : `str`\n            Callback URL. Defaults to 'oob'.\n\n    :Returns:\n        A `tuple` of two values:\n\n        * a MediaWiki URL to direct the user to\n        * a :class:`~mwoauth.RequestToken` representing a request for access\n\n\n    \"\"\"\n    auth = OAuth1(consumer_token.key,\n                  client_secret=consumer_token.secret,\n                  callback_uri=callback)\n\n    r = requests.post(url=mw_uri,\n                      params={'title': \"Special:OAuth/initiate\"},\n                      auth=auth,\n                      headers={'User-Agent': user_agent})\n\n    credentials = parse_qs(r.content)\n\n    if credentials is None or credentials == {}:\n        raise OAuthException(\n            \"Expected x-www-form-urlencoded response from \" +\n            \"MediaWiki, but got something else: \" +\n            \"{0}\".format(repr(r.content)))\n\n    elif b('oauth_token') not in credentials or \\\n         b('oauth_token_secret') not in credentials:\n\n        raise OAuthException(\n            \"MediaWiki response lacks token information: \"\n            \"{0}\".format(repr(credentials)))\n\n    else:\n\n        request_token = RequestToken(\n            credentials.get(b('oauth_token'))[0],\n            credentials.get(b('oauth_token_secret'))[0]\n        )\n\n    params = {'title': \"Special:OAuth/authenticate\",\n              'oauth_token': request_token.key,\n              'oauth_consumer_key': consumer_token.key}\n\n    return (\n        mw_uri + \"?\" + urlencode(params),\n        request_token\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef complete(mw_uri, consumer_token, request_token, response_qs,\n             user_agent=defaults.USER_AGENT):\n    \"\"\"\n    Complete an OAuth handshake with MediaWiki by exchanging an\n\n    :Parameters:\n        mw_uri : `str`\n            The base URI of the MediaWiki installation.  Note that the URI\n            should end in ``\"index.php\"``.\n        consumer_token : :class:`~mwoauth.ConsumerToken`\n            A key/secret pair representing you, the consumer.\n        request_token : :class:`~mwoauth.RequestToken`\n            A temporary token representing the user.  Returned by\n            `initiate()`.\n        response_qs : `bytes`\n            The query string of the URL that MediaWiki forwards the user back\n            after authorization.\n\n    :Returns:\n        An `AccessToken` containing an authorized key/secret pair that\n        can be stored and used by you.\n    \"\"\"\n\n    callback_data = parse_qs(_ensure_bytes(response_qs))\n\n    if callback_data is None or callback_data == {}:\n        raise OAuthException(\n            \"Expected URL query string, but got \" +\n            \"something else instead: {0}\".format(str(response_qs)))\n\n    elif b('oauth_token') not in callback_data or \\\n         b('oauth_verifier') not in callback_data:\n\n        raise OAuthException(\n            \"Query string lacks token information: \"\n            \"{0}\".format(repr(callback_data)))\n\n    else:\n        # Check if the query string references the right temp resource owner\n        # key\n        request_token_key = callback_data.get(b(\"oauth_token\"))[0]\n        # Get the verifier token\n        verifier = callback_data.get(b(\"oauth_verifier\"))[0]\n\n    if not request_token.key == request_token_key:\n        raise OAuthException(\n            \"Unexpect request token key \" +\n            \"{0}, expected {1}.\".format(request_token_key, request_token.key))\n\n    # Construct a new auth with the verifier\n    auth = OAuth1(consumer_token.key,\n                  client_secret=consumer_token.secret,\n                  resource_owner_key=request_token.key,\n                  resource_owner_secret=request_token.secret,\n                  verifier=verifier)\n\n    # Send the verifier and ask for an authorized resource owner key/secret\n    r = requests.post(url=mw_uri,\n                      params={'title': \"Special:OAuth/token\"},\n                      auth=auth,\n                      headers={'User-Agent': user_agent})\n\n    # Parse response and construct an authorized resource owner\n    credentials = parse_qs(r.content)\n\n    if credentials is None:\n        raise OAuthException(\n            \"Expected x-www-form-urlencoded response, \" +\n            \"but got some else instead: {0}\".format(r.content))\n\n    access_token = AccessToken(\n        credentials.get(b('oauth_token'))[0],\n        credentials.get(b('oauth_token_secret'))[0]\n    )\n\n    return access_token", "response": "Complete an OAuth handshake with MediaWiki by exchanging an OAuth handshake with MediaWiki."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef identify(mw_uri, consumer_token, access_token, leeway=10.0,\n             user_agent=defaults.USER_AGENT):\n    \"\"\"\n    Gather identifying information about a user via an authorized token.\n\n    :Parameters:\n        mw_uri : `str`\n            The base URI of the MediaWiki installation.  Note that the URI\n            should end in ``\"index.php\"``.\n        consumer_token : :class:`~mwoauth.ConsumerToken`\n            A token representing you, the consumer.\n        access_token : :class:`~mwoauth.AccessToken`\n            A token representing an authorized user.  Obtained from\n            `complete()`\n        leeway : `int` | `float`\n            The number of seconds of leeway to account for when examining a\n            tokens \"issued at\" timestamp.\n\n    :Returns:\n        A dictionary containing identity information.\n    \"\"\"\n\n    # Construct an OAuth auth\n    auth = OAuth1(consumer_token.key,\n                  client_secret=consumer_token.secret,\n                  resource_owner_key=access_token.key,\n                  resource_owner_secret=access_token.secret)\n\n    # Request the identity using auth\n    r = requests.post(url=mw_uri,\n                      params={'title': \"Special:OAuth/identify\"},\n                      auth=auth,\n                      headers={'User-Agent': user_agent})\n\n    # Special:OAuth/identify unhelpfully returns 200 status even when there is\n    # an error in the API call. Check for error messages manually.\n    if r.content.startswith(b'{'):\n        try:\n            resp = r.json()\n            if 'error' in resp:\n                raise OAuthException(\n                    \"A MediaWiki API error occurred: {0}\".format(resp['message']))\n        except ValueError:\n            raise OAuthException(\n                \"An error occurred while trying to read json \" +\n                \"content: {0}\".format(e))\n\n\n    # Decode json & stuff\n    try:\n        identity = jwt.decode(r.content, consumer_token.secret,\n                              audience=consumer_token.key,\n                              algorithms=[\"HS256\"],\n                              leeway=leeway)\n    except jwt.InvalidTokenError as e:\n        raise OAuthException(\n            \"An error occurred while trying to read json \" +\n            \"content: {0}\".format(e))\n\n    # Verify the issuer is who we expect (server sends $wgCanonicalServer)\n    issuer = urlparse(identity['iss']).netloc\n    expected_domain = urlparse(mw_uri).netloc\n    if not issuer == expected_domain:\n        raise OAuthException(\n            \"Unexpected issuer \" +\n            \"{0}, expected {1}\".format(issuer, expected_domain))\n\n    # Check that the identity was issued in the past.\n    now = time.time()\n    issued_at = float(identity['iat'])\n    if not now >= (issued_at - leeway):\n        raise OAuthException(\n            \"Identity issued {0} \".format(issued_at - now) +\n            \"seconds in the future!\")\n\n    # Verify that the nonce matches our request one,\n    # to avoid a replay attack\n    authorization_header = force_unicode(r.request.headers['Authorization'])\n    request_nonce = re.search(r'oauth_nonce=\"(.*?)\"',\n                              authorization_header).group(1)\n    if identity['nonce'] != request_nonce:\n        raise OAuthException(\n            'Replay attack detected: {0} != {1}'.format(\n                identity['nonce'], request_nonce))\n\n    return identity", "response": "This function requests an OAuth request to identify a user via an authorized token."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configure(*args, **kwargs):\n    global _stats_client\n\n    log.debug('statsd.configure(%s)' % kwargs)\n    _config.update(kwargs)\n\n    _stats_client = _create_client(**_config)", "response": "Configure the module level statsd client that will\n    be used in all library operations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef incr(name, value=1, rate=1, tags=None):\n    client().incr(name, value, rate, tags)", "response": "Increment a metric by value."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decr(name, value=1, rate=1, tags=None):\n    client().decr(name, value, rate, tags)", "response": "Decrement a metric by value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value for a gauge.", "response": "def gauge(name, value, rate=1, tags=None):\n    \"\"\"Set the value for a gauge.\n\n    >>> import statsdecor\n    >>> statsdecor.gauge('my.metric', 10)\n    \"\"\"\n    client().gauge(name, value, rate, tags)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timing(name, delta, rate=1, tags=None):\n    return client().timing(name, delta, rate=rate, tags=tags)", "response": "Sends new timing information. delta is in milliseconds. rate is in milliseconds. tags is a dict of tags to add to the timing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the object to the database.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        **uid**: :code:`division_cycle_ballotmeasure:{number}`\n        \"\"\"\n        self.uid = '{}_{}_ballotmeasure:{}'.format(\n            self.division.uid,\n            self.election_day.uid,\n            self.number\n        )\n        super(BallotMeasure, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fieldsets(self, request, obj=None):\n\n        # some ugly business to remove freeze_date\n        # from the field list\n        general_module = {\n            'fields': list(self.general_fields),\n            'classes': ('module-general',),\n        }\n\n        default_fieldsets = list(self.fieldsets)\n        if not request.user.has_perm('pages.can_freeze'):\n            general_module['fields'].remove('freeze_date')\n        if not request.user.has_perm('pages.can_publish'):\n            general_module['fields'].remove('status')\n\n        default_fieldsets[0][1] = general_module\n\n        placeholder_fieldsets = []\n        template = get_template_from_request(request, obj)\n        for placeholder in get_placeholders(template):\n            if placeholder.name not in self.mandatory_placeholders:\n                placeholder_fieldsets.append(placeholder.name)\n\n        additional_fieldsets = []\n\n        # meta fields\n        metadata_fieldsets = [f['name'] for f in self.metadata_fields]\n        additional_fieldsets.append((_('Metadata'), {\n            'fields': metadata_fieldsets,\n            'classes': ('module-content', 'grp-collapse grp-closed'),\n        }))\n        additional_fieldsets.append((_('Content'), {\n            'fields': placeholder_fieldsets,\n            'classes': ('module-content',),\n        }))\n\n        return default_fieldsets + additional_fieldsets", "response": "Returns a list of fieldsets for the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_form(self, request, obj=None, **kwargs):\n        #form = super(PageAdmin, self).get_form(request, obj, **kwargs)\n        template = get_template_from_request(request, obj)\n        form = make_form(self.model, get_placeholders(template))\n\n        language = get_language_from_request(request)\n        form.base_fields['language'].initial = language\n        if obj:\n            initial_slug = obj.slug(language=language, fallback=False)\n            initial_title = obj.title(language=language, fallback=False)\n            form.base_fields['slug'].initial = initial_slug\n            form.base_fields['title'].initial = initial_title\n            form.base_fields['slug'].label = _('Slug')\n\n        template = get_template_from_request(request, obj)\n        page_templates = settings.get_page_templates()\n        if len(page_templates) > 0:\n            template_choices = list(page_templates)\n            template_choices.insert(0, (settings.PAGE_DEFAULT_TEMPLATE,\n                    _('Default template')))\n            form.base_fields['template'].choices = template_choices\n            form.base_fields['template'].initial = force_text(template)\n\n        for placeholder in get_placeholders(template):\n            name = placeholder.name\n            if obj:\n                initial = placeholder.get_content(obj, language, name)\n            else:\n                initial = None\n            form.base_fields[name] = placeholder.get_field(obj,\n                language, initial=initial)\n\n        for placeholder in self.metadata_fields:\n            name = placeholder['name']\n            initial = None\n            if obj:\n                try:\n                    initial = Content.objects.get(page=obj, language=language, type=name).body\n                except Content.DoesNotExist:\n                    pass\n            form.base_fields[name] = placeholder['field']\n            form.base_fields[name].initial = initial\n\n        return form", "response": "Get a : class : PageAdmin. forms. PageForm for the the\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all pages in the admin site.", "response": "def list_pages(self, request, template_name=None, extra_context=None):\n        \"\"\"List root pages\"\"\"\n        if not self.admin_site.has_permission(request):\n            return self.admin_site.login(request)\n        language = get_language_from_request(request)\n\n        query = request.POST.get('q', '').strip()\n\n        if query:\n            page_ids = list(set([c.page.pk for c in\n                Content.objects.filter(body__icontains=query)]))\n            pages = Page.objects.filter(pk__in=page_ids)\n        else:\n            pages = Page.objects.root()\n        if settings.PAGE_HIDE_SITES:\n            pages = pages.filter(sites=settings.SITE_ID)\n\n        context = {\n            'can_publish': request.user.has_perm('pages.can_publish'),\n            'language': language,\n            'name': _(\"page\"),\n            'pages': pages,\n            'opts': self.model._meta,\n            'q': query\n        }\n\n        context.update(extra_context or {})\n        change_list = self.changelist_view(request, context)\n\n        return change_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates threads for each sample and add them to the header queue", "response": "def headerthreads(self):\n        \"\"\"\n        The contig ID must be twenty characters or fewer. The names of the headers created following SPAdes assembly\n        are usually far too long. This renames them as the sample name\n        \"\"\"\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.headers, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.runmetadata.samples:\n            # Create an attribute to store the path/file name of the fasta file with fixed headers\n            sample.general.fixedheaders = sample.general.bestassemblyfile.replace('.fasta', '.ffn')\n            self.headerqueue.put(sample)\n        self.headerqueue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef codingthreads(self):\n        printtime('Extracting CDS features', self.start)\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.codingsequences, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.runmetadata.samples:\n            self.codingqueue.put(sample)\n        self.codingqueue.join()\n        # Create CDS files and determine gene presence/absence\n        self.corethreads()", "response": "Create threads for each CDS feature in the analysis and send them to the appropriate destination function\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the CDS file and stores the results in the appropriate dictionaries.", "response": "def cdsparse(self, record):\n        \"\"\"\n        Finds core genes, and records gene names and sequences in dictionaries\n        :param record: SeqIO record\n        \"\"\"\n        try:\n            # Find genes that are present in all strains of interest - the number of times the gene is found is\n            # equal to the number of strains. Earlier parsing ensures that the same gene is not present in a strain\n            # more than once\n            if self.genes[self.genenames[record.id]] == len(self.runmetadata.samples):\n                # Add the gene names and sequences to the appropriate dictionaries\n                try:\n                    self.genesequence[self.genenames[record.id]].add(str(record.seq))\n                # Initialise the dictionary as required, then populate as above\n                except KeyError:\n                    self.genesequence[self.genenames[record.id]] = set()\n                    self.genesequence[self.genenames[record.id]].add(str(record.seq))\n                try:\n                    self.coresequence[str(record.seq)].add(record.id)\n                except KeyError:\n                    self.coresequence[str(record.seq)] = set()\n                    self.coresequence[str(record.seq)].add(record.id)\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates fasta files containing all alleles for each gene and the associated sequence.", "response": "def corewriter(self):\n        \"\"\"\n        Creates .fasta files containing all alleles for each gene\n        \"\"\"\n        printtime('Creating core allele files', self.start)\n        for gene in sorted(self.genesequence):\n            self.geneset.add(gene)\n            # Set the name of the allele file\n            genefile = os.path.join(self.coregenelocation, '{}.fasta'.format(gene))\n            # If the file doesn't exist, create it\n            if not os.path.isfile(genefile):\n                with open(genefile, 'w') as core:\n                    for count, sequence in enumerate(self.genesequence[gene]):\n                        # The definition line is the gene name, and the allele number (count (+ 1 to compensate for\n                        # base zero))\n                        definitionline = '{}-{}'.format(gene, count + 1)\n                        # Create a sequence record using BioPython\n                        fasta = SeqRecord(Seq(sequence),\n                                          # Without this, the header will be improperly formatted\n                                          description='',\n                                          # Use >:definitionline as the header\n                                          id=definitionline)\n                        # Use the SeqIO module to properly format the new sequence record\n                        SeqIO.write(fasta, core, 'fasta')\n                        for strain in self.coresequence[sequence]:\n                            # Record the strain name, the gene name, and the allele number.\n                            # [:-6] removes the contig number: 2014-SEQ-0276_00001 becomes 2014-SEQ-0276\n                            try:\n                                self.corealleles[strain[:-6]].update({gene: count + 1})\n                            except KeyError:\n                                self.corealleles[strain[:-6]] = {gene: count + 1}\n            else:\n                # If the file exists, don't recreate it; only iterate through the dictionary of gene sequences\n                for count, sequence in enumerate(self.genesequence[gene]):\n                    for strain in self.coresequence[sequence]:\n                        # Populate the dictionary as above\n                        try:\n                            self.corealleles[strain[:-6]].update({gene: count + 1})\n                        except KeyError:\n                            self.corealleles[strain[:-6]] = {gene: count + 1}\n        # Create a combined file of all the core genes to be used in typing strain(s) of interest\n        if not os.path.isfile(os.path.join(self.coregenelocation, 'core_combined.fasta')):\n            fastafiles = glob(os.path.join(self.coregenelocation, '*.fasta'))\n            # Run the method for each allele\n            self.combinealleles(fastafiles)\n        # Run the profiler\n        self.profiler()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef combinealleles(self, alleles):\n        printtime('Creating combined core allele file', self.start)\n        if not os.path.isfile(os.path.join(self.coregenelocation, 'core_combined.tfa')):\n            with open(os.path.join(self.coregenelocation, 'core_combined.tfa'), 'w') as combinedfile:\n                # Open each allele file\n                for allele in sorted(alleles):\n                    for record in SeqIO.parse(open(allele, \"rU\"), \"fasta\"):\n                        # Extract the sequence record from each entry in the multifasta\n                        # Remove and dashes or 'N's from the sequence data - makeblastdb can't handle sequences\n                        # with gaps\n                        # noinspection PyProtectedMember\n                        record.seq._data = record.seq._data.replace('-', '').replace('N', '')\n                        # Clear the name and description attributes of the record\n                        record.name = ''\n                        record.description = ''\n                        # Write each record to the combined file\n                        SeqIO.write(record, combinedfile, 'fasta')", "response": "Creates a large multi - fasta file from all core genes in the analysis"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the core profile for each strain and stores the count of each core in the profile. txt file.", "response": "def profiler(self):\n        \"\"\"\n        Calculates the core profile for each strain\n        \"\"\"\n        printtime('Calculating core profiles', self.start)\n        # Only create the profile if it doesn't exist already\n        # if not os.path.isfile('{}/profile.txt'.format(self.profilelocation)):\n        for strain in self.corealleles:\n            # Add the gene name and allele number pair for each core gene in each strain\n            self.coreset.add(tuple(sorted(self.corealleles[strain].items())))\n        # Set the header to be similar to an MLST profile - ST,gene1,gene2,etc\n        header = 'ST,{}\\n'.format(','.join(sorted(self.geneset)))\n        data = ''\n        for count, core in sorted(enumerate(self.coreset)):\n            # Increment count now to account for 0-based numbering\n            count += 1\n            # Add the sequence type number to the profile\n            data += '{}'.format(count)\n            # Store the sequence type for each strain\n            for strain in self.corealleles:\n                if tuple(sorted(self.corealleles[strain].items())) == core:\n                    self.profiles[strain] = count\n            # Add the allele number for each gene\n            for gene in sorted(core):\n                data += ',{}'.format(gene[1])\n            data += '\\n'\n        # Write the profile\n        with open(os.path.join(self.profilelocation, 'profile.txt'), 'w') as profile:\n            profile.write(header)\n            profile.write(data)\n        # Create a list of which strains correspond to the sequence types\n        self.linker()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef linker(self):\n\n        strainprofile = os.path.join(self.profilelocation, 'strainprofiles.txt')\n        if not os.path.isfile(strainprofile):\n            header = 'Strain,SequenceType\\n'\n            data = ''\n            # Sort the profiles based on sequence type\n            sortedprofiles = sorted(self.profiles.items(), key=operator.itemgetter(1))\n            # Associate the sequence type with each strain\n            for strain, seqtype in sortedprofiles:\n                for sample in self.runmetadata.samples:\n                    if sample.name == strain:\n                        sample.general.coretype = seqtype\n                        data += '{},{}\\n'.format(strain, seqtype)\n            # Write the results to file\n            with open(strainprofile, 'w') as profile:\n                profile.write(header)\n                profile.write(data)", "response": "Create a. csv file of the linkages\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, index):\n        assert index <= self.count\n        assert index < self.size\n        offset = index * self.chunk_size\n        return self.data[offset:offset + self.chunk_size]", "response": "Get a chunk by index"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new(self, init=None):\n        if self.count >= self.size:\n            self.resize(self.count * 2)\n        chunk = self.get(self.count)\n        if init is not None:\n            assert len(init) == self.chunk_size\n            chunk[0:self.chunk_size] = init\n        self.count += 1\n        return chunk", "response": "Return the last currently unused chunk resizing if needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resize(self, new_size):\n        assert new_size > self.size\n        new_data = self._allocate(new_size)\n        # copy\n        new_data[0:self.size * self.chunk_size] = self.data\n        self.size = new_size\n        self.data = new_data", "response": "Create a new larger array and copy data over"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the chunk at index.", "response": "def remove(self, index):\n        \"\"\"Remove chunk at index.\n\n        Doesn't actually delete data, copies last chunk's data over data to be\n        removed, and decreases the count\"\"\"\n        assert index < self.count\n        last_index = self.count - 1\n        data = self.get(index)\n\n        if index == last_index:\n            # easy case - nothing to do except zero last chunk\n            last_data = data\n            moved = None\n        else:\n            last_data = self.get(last_index)\n            # copy the last chunk's data over the data to be deleted\n            data[0:self.chunk_size] = last_data\n            moved = last_index\n\n        # zero last chunk's data\n        last_data[0:self.chunk_size] = [0] * self.chunk_size\n        self.count -= 1\n\n        # provide which index has now moved\n        return moved"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_turtle(self, id, shape, model_init, color_init):\n        assert id not in self.id_to_shape\n        data = self._create_turtle(id, shape, model_init, color_init)\n        self.id_to_shape[id] = shape\n        return data", "response": "Create a slice of memory for turtle data storage"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies the turtle data from the old shape buffer to the new shape buffer.", "response": "def set_shape(self, id, new_shape):\n        \"\"\"Copies the turtle data from the old shape buffer to the new\"\"\"\n        old_shape = self.id_to_shape[id]\n        old_buffer = self.get_buffer(old_shape)\n        model, color = old_buffer.get(id)\n        new_data = self._create_turtle(id, new_shape, model, color)\n        old_buffer.remove(id)\n        self.id_to_shape[id] = new_shape\n        return new_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a checksum node into this object.", "response": "def _parse_tree(self, node):\n        \"\"\" Parse a <checksum> object \"\"\"\n        if 'filename' in node.attrib:\n            self.filename = node.attrib['filename']\n        if 'type' in node.attrib:\n            self.kind = node.attrib['type']\n        if 'target' in node.attrib:\n            self.target = node.attrib['target']\n        self.value = node.text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a review node and sets the attributes of the object.", "response": "def _parse_tree(self, node):\n        \"\"\" Parse a <review> object \"\"\"\n        if 'date' in node.attrib:\n            dt = dateutil.parser.parse(node.attrib['date'])\n            self.date = int(dt.strftime(\"%s\"))\n        if 'id' in node.attrib:\n            self.id = node.attrib['id']\n        if 'karma' in node.attrib:\n            self.karma = int(node.attrib['karma'])\n        if 'score' in node.attrib:\n            self.score = int(node.attrib['score'])\n        if 'rating' in node.attrib:\n            self.rating = int(node.attrib['rating'])\n        for c3 in node:\n            if c3.tag == 'lang':\n                self.locale = c3.text\n            if c3.tag == 'version':\n                self.version = c3.text\n            if c3.tag == 'reviewer_id':\n                self.reviewer_id = c3.text\n            if c3.tag == 'reviewer_name':\n                self.reviewer_name = c3.text\n            if c3.tag == 'summary':\n                self.summary = c3.text\n            if c3.tag == 'description':\n                self.description = _parse_desc(c3)\n            if c3.tag == 'metadata':\n                for c4 in c3:\n                    if c4.tag == 'value':\n                        if 'key' in c4.attrib:\n                            self.metadata[c4.attrib['key']] = c4.text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_checksum_by_target(self, target):\n        for csum in self.checksums:\n            if csum.target == target:\n                return csum\n        return None", "response": "returns a checksum of a specific kind"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_checksum(self, csum):\n        for csum_tmp in self.checksums:\n            if csum_tmp.target == csum.target:\n                self.checksums.remove(csum_tmp)\n                break\n        self.checksums.append(csum)", "response": "Add a checksum to a release object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_tree(self, node):\n        if 'timestamp' in node.attrib:\n            self.timestamp = int(node.attrib['timestamp'])\n        if 'date' in node.attrib:\n            dt = dateutil.parser.parse(node.attrib['date'])\n            self.timestamp = int(dt.strftime(\"%s\"))\n        if 'urgency' in node.attrib:\n            self.urgency = node.attrib['urgency']\n        if 'version' in node.attrib:\n            self.version = node.attrib['version']\n            # fix up hex value\n            if self.version.startswith('0x'):\n                self.version = str(int(self.version[2:], 16))\n        for c3 in node:\n            if c3.tag == 'description':\n                self.description = _parse_desc(c3)\n            if c3.tag == 'size':\n                if 'type' not in c3.attrib:\n                    continue\n                if c3.attrib['type'] == 'installed':\n                    self.size_installed = int(c3.text)\n                if c3.attrib['type'] == 'download':\n                    self.size_download = int(c3.text)\n            elif c3.tag == 'checksum':\n                csum = Checksum()\n                csum._parse_tree(c3)\n                self.add_checksum(csum)", "response": "Parses a XML tree node and sets the properties of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a xml. dom Node object and set attributes.", "response": "def _parse_tree(self, node):\n        \"\"\" Parse a <image> object \"\"\"\n        if 'type' in node.attrib:\n            self.kind = node.attrib['type']\n        if 'width' in node.attrib:\n            self.width = int(node.attrib['width'])\n        if 'height' in node.attrib:\n            self.height = int(node.attrib['height'])\n        self.url = node.text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_image_by_kind(self, kind):\n        for ss in self.images:\n            if ss.kind == kind:\n                return ss\n        return None", "response": "returns a specific image of a specific kind"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds an image to a screenshot object", "response": "def add_image(self, im):\n        \"\"\" Add a image to a screenshot object \"\"\"\n        for im_tmp in self.images:\n            if im_tmp.kind == im.kind:\n                self.images.remove(im_tmp)\n                break\n        self.images.append(im)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_tree(self, node):\n        if 'type' in node.attrib:\n            self.kind = node.attrib['type']\n        for c3 in node:\n            if c3.tag == 'caption':\n                self.caption = _parse_desc(c3)\n            elif c3.tag == 'image':\n                im = Image()\n                im._parse_tree(c3)\n                self.add_image(im)", "response": "Parse a screenshot node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a <provide > object", "response": "def _parse_tree(self, node):\n        \"\"\" Parse a <provide> object \"\"\"\n        if node.tag == 'firmware':\n            if 'type' in node.attrib and node.attrib['type'] == 'flashed':\n                self.kind = 'firmware-flashed'\n            self.value = node.text.lower()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a xml. dom Node object and set attributes.", "response": "def _parse_tree(self, node):\n        \"\"\" Parse a <require> object \"\"\"\n        self.kind = node.tag\n        if 'compare' in node.attrib:\n            self.compare = node.attrib['compare']\n        if 'version' in node.attrib:\n            self.version = node.attrib['version']\n        self.value = node.text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_release(self, release):\n        for r in self.releases:\n            if r.version == release.version:\n                return\n        self.releases.append(release)", "response": "Add a release object to the list of releases."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_review(self, review):\n        for r in self.reviews:\n            if r.id == review.id:\n                return\n        self.reviews.append(review)", "response": "Add a review object to the release object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_screenshot(self, screenshot):\n        if screenshot in self.screenshots:\n            return\n        self.screenshots.append(screenshot)", "response": "Add a screenshot object to the list of screenshots."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_provide(self, provide):\n        for p in self.provides:\n            if p.value == provide.value:\n                return\n        self.provides.append(provide)", "response": "Add a provide object to the list of provide objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_provides_by_kind(self, kind):\n        provs = []\n        for p in self.provides:\n            if p.kind == kind:\n                provs.append(p)\n        return provs", "response": "Returns an array of provides of a certain kind"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a require object to the list of requires.", "response": "def add_require(self, require):\n        \"\"\" Add a require object if it does not already exist \"\"\"\n        for p in self.requires:\n            if p.value == require.value:\n                return\n        self.requires.append(require)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a requires object of a specific value", "response": "def get_require_by_kind(self, kind, value):\n        \"\"\" Returns a requires object of a specific value \"\"\"\n        for r in self.requires:\n            if r.kind == kind and r.value == value:\n                return r\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, xml_data):\n\n        # parse tree\n        if isinstance(xml_data, string_types):\n            # Presumably, this is textual xml data.\n            try:\n                root = ET.fromstring(xml_data)\n            except StdlibParseError as e:\n                raise ParseError(str(e))\n        else:\n            # Otherwise, assume it has already been parsed into a tree\n            root = xml_data\n\n        # get type\n        if 'type' in root.attrib:\n            self.kind = root.attrib['type']\n\n        # parse component\n        for c1 in root:\n\n            # <id>\n            if c1.tag == 'id':\n                self.id = c1.text\n\n            # <updatecontact>\n            elif c1.tag == 'updatecontact' or c1.tag == 'update_contact':\n                self.update_contact = c1.text\n\n            # <metadata_license>\n            elif c1.tag == 'metadata_license':\n                self.metadata_license = c1.text\n\n            # <releases>\n            elif c1.tag == 'releases':\n                for c2 in c1:\n                    if c2.tag == 'release':\n                        rel = Release()\n                        rel._parse_tree(c2)\n                        self.add_release(rel)\n\n            # <reviews>\n            elif c1.tag == 'reviews':\n                for c2 in c1:\n                    if c2.tag == 'review':\n                        rev = Review()\n                        rev._parse_tree(c2)\n                        self.add_review(rev)\n\n            # <screenshots>\n            elif c1.tag == 'screenshots':\n                for c2 in c1:\n                    if c2.tag == 'screenshot':\n                        ss = Screenshot()\n                        ss._parse_tree(c2)\n                        self.add_screenshot(ss)\n\n            # <provides>\n            elif c1.tag == 'provides':\n                for c2 in c1:\n                    prov = Provide()\n                    prov._parse_tree(c2)\n                    self.add_provide(prov)\n\n            # <requires>\n            elif c1.tag == 'requires':\n                for c2 in c1:\n                    req = Require()\n                    req._parse_tree(c2)\n                    self.add_require(req)\n\n            # <kudos>\n            elif c1.tag == 'kudos':\n                for c2 in c1:\n                    if not c2.tag == 'kudo':\n                        continue\n                    self.kudos.append(c2.text)\n\n            # <keywords>\n            elif c1.tag == 'keywords':\n                for c2 in c1:\n                    if not c2.tag == 'keyword':\n                        continue\n                    self.keywords.append(c2.text)\n\n            # <categories>\n            elif c1.tag == 'categories':\n                for c2 in c1:\n                    if not c2.tag == 'category':\n                        continue\n                    self.categories.append(c2.text)\n\n            # <custom>\n            elif c1.tag == 'custom':\n                for c2 in c1:\n                    if not c2.tag == 'value':\n                        continue\n                    if 'key' not in c2.attrib:\n                        continue\n                    self.custom[c2.attrib['key']] = c2.text\n\n            # <project_license>\n            elif c1.tag == 'project_license' or c1.tag == 'licence':\n                self.project_license = c1.text\n\n            # <developer_name>\n            elif c1.tag == 'developer_name':\n                self.developer_name = _join_lines(c1.text)\n\n            # <name>\n            elif c1.tag == 'name' and not self.name:\n                self.name = _join_lines(c1.text)\n\n            # <pkgname>\n            elif c1.tag == 'pkgname' and not self.pkgname:\n                self.pkgname = _join_lines(c1.text)\n\n            # <summary>\n            elif c1.tag == 'summary' and not self.summary:\n                self.summary = _join_lines(c1.text)\n\n            # <description>\n            elif c1.tag == 'description' and not self.description:\n                self.description = _parse_desc(c1)\n\n            # <url>\n            elif c1.tag == 'url':\n                key = 'homepage'\n                if 'type' in c1.attrib:\n                    key = c1.attrib['type']\n                self.urls[key] = c1.text\n\n            elif c1.tag == 'icon':\n                key = c1.attrib.pop('type', 'unknown')\n                c1.attrib['value'] = c1.text\n                self.icons[key] = self.icons.get(key, []) + [c1.attrib]", "response": "Parse the XML data into this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_valid_time_stamp():\n    time_stamp = str(datetime.datetime.now())\n    time_stamp = \"time_\" + time_stamp.replace(\"-\", \"_\").replace(\":\", \"_\").replace(\" \", \"_\").replace(\".\", \"_\")\n    return time_stamp", "response": "Get a valid time stamp without illegal characters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scalar(self, value, step, name):\n        # Spaces not allowed for scalar variable name\n        assert len(name.split(\" \")) < 2, \"Ensure that you don't have spaces in your variable name, use '_' instead.\"\n        name = \"scalar_\" + name\n        self.previous.append(name)\n        if self.previous[-1] not in self.previous[:-1]:\n            self.c.execute(\"\"\"INSERT INTO {time_stamp_table} (\n                              variable_name, variable_type\n                              ) VALUES ('{variable}', '{type}')\"\"\"\n                           .format(time_stamp_table=self.time_stamp, variable=name, type=\"scalar\"))\n        else:\n            self.previous.pop()\n\n        self.c.execute(\"\"\"CREATE TABLE IF NOT EXISTS {variable_table_name} (\n                          X_value FLOAT, Y_value FLOAT, time VARCHAR\n                          )\"\"\".format(variable_table_name=self.time_stamp + '_' + name))\n\n        self.c.execute(\"\"\"INSERT INTO {variable_table_name} (\n                          X_value, Y_value, time) VALUES ('{x}', '{y}', '{time}'\n                          )\"\"\".format(variable_table_name=self.time_stamp + '_' + name,\n                                      x=step, y=value, time=datetime.datetime.now()))\n        self.conn.commit()", "response": "Plots a scalar value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nshow image on the Crystal server.", "response": "def image(self, image, name):\n        \"\"\"\n        Show image on the Crystal server.\n        :param image:\n        :param name:\n        :return:\n        \"\"\"\n        assert len(name.split(\" \")) < 2, \"Ensure that you don't have spaces in your variable name, use '_' instead.\"\n        name = \"image_\" + name\n        self.previous.append(name)\n        if self.previous[-1] not in self.previous[:-1]:\n            self.c.execute(\"\"\"INSERT INTO {time_stamp_table} (\n                              variable_name, variable_type\n                              ) VALUES ('{variable}', '{type}')\"\"\"\n                           .format(time_stamp_table=self.time_stamp, variable=name, type=\"image\"))\n        else:\n            self.previous.pop()\n\n        self.c.execute(\"\"\"CREATE TABLE IF NOT EXISTS {variable_table_name} (\n                          images BLOB, time VARCHAR\n                          )\"\"\".format(variable_table_name=self.time_stamp + '_' + name))\n\n        self.c.execute(\"\"\"INSERT INTO {variable_table_name} (\n                          images, time) VALUES ('{img}', '{time}'\n                          )\"\"\".format(variable_table_name=self.time_stamp + '_' + name,\n                                      img=sqlite3.Binary(np.array(image).tobytes()), time=datetime.datetime.now()))\n        self.conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_file(self, filename):\n\n        # save compressed file\n        xml = self.to_xml()\n        f = gzip.open(filename, 'wb')\n        try:\n            f.write(xml.encode('utf-8'))\n        finally:\n            f.close()", "response": "Save the store to disk"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_file(self, filename):\n        with gzip.open(filename, 'rb') as f:\n            self.parse(f.read())", "response": "Open the store from disk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all the applications from the store", "response": "def get_components(self):\n        \"\"\" Returns all the applications from the store \"\"\"\n        components = []\n        for app_id in self.components:\n            components.append(self.components[app_id])\n        return components"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, component):\n\n        # if already exists, just add the release objects\n        old = self.get_component(component.id)\n        if old:\n            old.releases.extend(component.releases)\n            return\n        self.components[component.id] = component", "response": "Add a component to the store"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the XML data into this object.", "response": "def parse(self, xml_data):\n        \"\"\" Parse XML data \"\"\"\n\n        # parse tree\n        try:\n            root = ET.fromstring(xml_data)\n        except StdlibParseError as e:\n            raise ParseError(str(e))\n\n        self.origin = root.attrib['origin']\n\n        for child in root:\n            component = Component()\n            component.parse(child)\n            self.components[component.id] = component"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self, val):\n        # Just copied from original KeePassX source\n        y, mon, d, h, min_, s = val.timetuple()[:6]\n\n        dw1 = 0x0000FFFF & ((y >> 6) & 0x0000003F)\n        dw2 = 0x0000FFFF & ((y & 0x0000003F) << 2 | ((mon >> 2) & 0x00000003))\n        dw3 = 0x0000FFFF & (((mon & 0x0000003) << 6) | ((d & 0x0000001F) << 1) \\\n                | ((h >> 4) & 0x00000001))\n        dw4 = 0x0000FFFF & (((h & 0x0000000F) << 4) | ((min_ >> 2) & 0x0000000F))\n        dw5 = 0x0000FFFF & (((min_ & 0x00000003) << 6) | (s & 0x0000003F))\n\n        return struct.pack('<5B', dw1, dw2, dw3, dw4, dw5)", "response": "Encodes the python datetime value into the bytes needed for database format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict of all this structures attributes and values skipping any attributes that start with an underscore.", "response": "def attributes(self):\n        \"\"\"\n        Returns a dict of all this structures attributes and values, skipping\n        any attributes that start with an underscore (assumed they should be ignored).\n        \"\"\"\n        return dict([(name, getattr(self, name)) for (name, _) in self.format.values() if name is not None and not name.startswith('_')])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding the binary string representation of a set of object attributes from the buffer.", "response": "def decode(self, buf):\n        \"\"\"\n        Set object attributes from binary string representation.\n        \n        :param buf: The binary string representation of this object in database.\n        :type buf: str\n        :raises: :class:`keepassdb.exc.ParseError` - If errors encountered parsing struct.\n        \"\"\"\n        index = 0\n        while True:\n            #self.log.debug(\"buffer state: index={0}, buf-ahead={1!r}\".format(index, buf[index:]))\n            substr = buf[index:index + 6]\n            index += 6\n            if index > len(buf):\n                raise ValueError(\"Group header offset is out of range: {0}\".format(index))\n            (typ, siz) = struct.unpack('<H L', substr)\n            self.order.append((typ, siz))\n            \n            substr = buf[index:index + siz]\n            index += siz\n            encoded = struct.unpack('<%ds' % siz, substr)[0]\n            \n            (name, marshall) = self.format[typ]\n            if name is None:\n                break\n            try:\n                value = marshall.decode(encoded)\n                self.log.debug(\"Decoded field [{0}] to value {1!r}\".format(name, value))\n            except struct.error, msg:\n                msg = '%s, typ=%d[size=%d] -> %s [buf = \"%r\"]' % \\\n                    (msg, typ, siz, self.format[typ], encoded)\n                raise exc.ParseError(msg)\n            setattr(self, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a binary string representation of the object.", "response": "def encode(self):\n        \"\"\"\n        Return binary string representation of object.\n        \n        :rtype: str\n        \"\"\"\n        buf = bytearray()\n        for typ in sorted(self.format.keys()):\n            encoded = None\n            if typ != 0xFFFF: # end of block\n                (name, marshall) = self.format[typ]\n                value = getattr(self, name, None)\n                if value is not None:\n                    try:\n                        encoded = marshall.encode(value)\n                        self.log.debug(\"Encoded field [{0}] to value {1!r}\".format(name, encoded))\n                    except:\n                        self.log.exception(\"Error encoding key/value: key={0}, value={1!r}\".format(name, value))\n                        raise\n            \n            # Note, there is an assumption here that encode() func is returning\n            # a byte string (so len = num bytes).  That should be a safe assumption.\n            size = len(encoded) if encoded is not None else 0\n            packed = struct.pack('<H', typ)\n            packed += struct.pack('<I', size)\n            if encoded is not None:\n                if isinstance(encoded, bytearray):\n                    encoded = str(encoded)\n                elif isinstance(encoded, unicode):\n                    encoded = encoded.encode('utf-8')\n                packed += struct.pack('<%ds' % size, encoded)\n                \n            buf += packed\n            \n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a binary string representation of this struct.", "response": "def encode(self):\n        \"\"\"\n        Returns binary string representation of this struct.\n        \n        :returns: Structure encoded as binary string for keepass database.\n        :rtype: bytes\n        \"\"\"\n        ret = bytearray()\n        for name, len, typecode in self.format:\n            value = getattr(self, name)\n            buf = struct.pack('<' + typecode, value)\n            ret.extend(buf)\n        return bytes(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(self, buf):\n        index = 0\n        if self.length > len(buf):\n            raise exc.ParseError(\"Insufficient data for reading header.\") \n        for (name, nbytes, typecode) in self.format:\n            string = buf[index:index + nbytes]\n            index += nbytes\n            value = struct.unpack('<' + typecode, string)[0]\n            setattr(self, name, value)\n        if const.DB_SIGNATURE1 != self.signature1 or \\\n                const.DB_SIGNATURE2 != self.signature2:\n            msg = 'Bad signatures: {0} {0}'.format(hex(self.signature1),\n                                                   hex(self.signature2))\n            raise exc.InvalidDatabase(msg)", "response": "Decode the object attributes from binary string buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reporter(self):\n        logging.info('Creating summary report')\n        header = '{}\\n'.format(','.join(self.headers))\n        # Create a string to store all the results\n        data = str()\n        for sample in self.metadata:\n            # Add the value of the appropriate attribute to the results string\n            data += GenObject.returnattr(sample, 'name')\n            # SampleName\n            data += GenObject.returnattr(sample.run, 'SamplePlate')\n            # Genus\n            data += GenObject.returnattr(sample.general, 'closestrefseqgenus')\n            # SequencingDate\n            data += GenObject.returnattr(sample.run, 'Date')\n            # Analyst\n            data += GenObject.returnattr(sample.run, 'InvestigatorName')\n            # SamplePurity\n            data += GenObject.returnattr(sample.confindr, 'contam_status')\n            # N50\n            n50 = GenObject.returnattr(sample.quality_features_polished, 'n50',\n                                       number=True)\n            if n50 != '-,':\n                data += n50\n            else:\n                data += '0,'\n            # NumContigs\n            data += GenObject.returnattr(sample.quality_features_polished, 'num_contigs',\n                                         number=True)\n            # TotalLength\n            data += GenObject.returnattr(sample.quality_features_polished, 'genome_length',\n                                         number=True)\n            # MeanInsertSize\n            data += GenObject.returnattr(sample.mapping, 'MeanInsertSize',\n                                         number=True)\n            # InsertSizeSTD\n            data += GenObject.returnattr(sample.mapping, 'StdInsertSize',\n                                         number=True)\n            # AverageCoverageDepth\n            data += GenObject.returnattr(sample.mapping, 'MeanCoveragedata',\n                                         number=True)\n            # CoverageDepthSTD\n            data += GenObject.returnattr(sample.mapping, 'StdCoveragedata',\n                                         number=True)\n            # PercentGC\n            data += GenObject.returnattr(sample.quality_features_polished, 'gc',\n                                         number=True)\n            # MASH_ReferenceGenome\n            data += GenObject.returnattr(sample.mash, 'closestrefseq')\n            # MASH_NumMatchingHashes\n            data += GenObject.returnattr(sample.mash, 'nummatches')\n            # 16S_result\n            data += GenObject.returnattr(sample.sixteens_full, 'sixteens_match')\n            # rMLST_Result\n            try:\n                # If the number of matches to the closest reference profile is 53, return the profile number\n                if sample.rmlst.matches == 53:\n                    rmlst_seq_type = GenObject.returnattr(sample.rmlst, 'sequencetype')\n                    rmlst_seq_type = rmlst_seq_type if rmlst_seq_type != 'ND,' else 'new,'\n                    data += rmlst_seq_type\n                else:\n                    # Otherwise the profile is set to new\n                    data += 'new,'\n            except AttributeError:\n                data += 'new,'\n            # MLST_Result\n            try:\n                if sample.mlst.matches == 7:\n                    data += GenObject.returnattr(sample.mlst, 'sequencetype')\n                else:\n                    data += 'new,'\n                    # # Create a set of all the genes present in the results (gene name split from allele)\n                    # mlst_gene_set = {gene.split('_')[0] for gene in sample.mlst.results}\n                    # # If there are all the genes present, but no perfect match to a reference profile, state that\n                    # # the profile is new\n                    # if len(mlst_gene_set) == 7:\n                    #     data += 'new,'\n                    # # Otherwise indicate that the profile is ND\n                    # else:\n                    #     data += 'ND,'\n            except AttributeError:\n                data += 'new,'\n            # MLST_gene_X_alleles\n            try:\n                # Create a set of all the genes present in the results (gene name split from allele)\n                gene_set = {gene.split('_')[0] for gene in sample.mlst.results}\n                for gene in sorted(gene_set):\n                    allele_list = list()\n                    # Determine all the alleles that are present for each gene\n                    for allele in sample.mlst.results:\n                        if gene in allele:\n                            allele_list.append(allele)\n                    # If there is more than one allele in the sample, add both to the string separated by a ';'\n                    if len(allele_list) > 1:\n                        data += '{},'.format(';'.join(allele_list))\n                    # Otherwise add the only allele\n                    else:\n                        data += allele_list[0] + ','\n                # If there are fewer than seven matching alleles, add a ND for each missing result\n                if len(gene_set) < 7:\n                    data += (7 - len(gene_set)) * 'ND,'\n            except AttributeError:\n                # data += '-,-,-,-,-,-,-,'\n                data += 'ND,ND,ND,ND,ND,ND,ND,'\n            # CoreGenesPresent\n            data += GenObject.returnattr(sample.coregenome, 'coreresults')\n            # E_coli_Serotype\n            try:\n                # If no O-type was found, set the output to be O-untypeable\n                if ';'.join(sample.serosippr.o_set) == '-':\n                    otype = 'O-untypeable'\n                else:\n                    otype = '{oset} ({opid})'.format(oset=';'.join(sample.serosippr.o_set),\n                                                     opid=sample.serosippr.best_o_pid)\n                # Same as above for the H-type\n                if ';'.join(sample.serosippr.h_set) == '-':\n                    htype = 'H-untypeable'\n\n                else:\n                    htype = '{hset} ({hpid})'.format(hset=';'.join(sample.serosippr.h_set),\n                                                     hpid=sample.serosippr.best_h_pid)\n                serotype = '{otype}:{htype},'.format(otype=otype,\n                                                     htype=htype)\n                # Add the serotype to the data string unless neither O-type not H-type were found; add ND instead\n                data += serotype if serotype != 'O-untypeable:H-untypeable,' else 'ND,'\n            except AttributeError:\n                data += 'ND,'\n            # SISTR_serovar_antigen\n            data += GenObject.returnattr(sample.sistr, 'serovar_antigen').rstrip(';')\n            # SISTR_serovar_cgMLST\n            data += GenObject.returnattr(sample.sistr, 'serovar_cgmlst')\n            # SISTR_serogroup\n            data += GenObject.returnattr(sample.sistr, 'serogroup')\n            # SISTR_h1\n            data += GenObject.returnattr(sample.sistr, 'h1').rstrip(';')\n            # SISTR_h2\n            data += GenObject.returnattr(sample.sistr, 'h2').rstrip(';')\n            # SISTR_serovar\n            data += GenObject.returnattr(sample.sistr, 'serovar')\n            # GeneSeekr_Profile\n            try:\n                if sample.genesippr.report_output:\n                    data += ';'.join(sample.genesippr.report_output) + ','\n                else:\n                    data += 'ND,'\n            except AttributeError:\n                data += 'ND,'\n            # Vtyper_Profile\n            data += GenObject.returnattr(sample.legacy_vtyper, 'toxinprofile')\n            # AMR_Profile and resistant/sensitive status\n            if sample.resfinder_assembled.pipelineresults:\n                # Profile\n                for resistance, resistance_set in sorted(sample.resfinder_assembled.pipelineresults.items()):\n                    data += '{res}({r_set});'.format(res=resistance.replace(',', ';'),\n                                                     r_set=';'.join(sorted(list(resistance_set))))\n                data += ','\n                # Resistant/Sensitive\n                data += 'Resistant,'\n            else:\n                # Profile\n                data += 'ND,'\n                # Resistant/Sensitive\n                data += 'Sensitive,'\n            # Plasmid Result'\n            if sample.mobrecon.pipelineresults:\n                for plasmid, details in sorted(sample.mobrecon.pipelineresults.items()):\n                    data += '{plasmid}({details});'.format(plasmid=plasmid,\n                                                           details=details)\n                data += ','\n            else:\n                data += 'ND,'\n            # TotalPredictedGenes\n            data += GenObject.returnattr(sample.prodigal, 'predictedgenestotal',\n                                         number=True)\n            # PredictedGenesOver3000bp\n            data += GenObject.returnattr(sample.prodigal, 'predictedgenesover3000bp',\n                                         number=True)\n            # PredictedGenesOver1000bp\n            data += GenObject.returnattr(sample.prodigal, 'predictedgenesover1000bp',\n                                         number=True)\n            # PredictedGenesOver500bp\n            data += GenObject.returnattr(sample.prodigal, 'predictedgenesover500bp',\n                                         number=True)\n            # PredictedGenesUnder500bp\n            data += GenObject.returnattr(sample.prodigal, 'predictedgenesunder500bp',\n                                         number=True)\n            # NumClustersPF\n            data += GenObject.returnattr(sample.run, 'NumberofClustersPF')\n            # Percent of reads mapping to PhiX control\n            data += GenObject.returnattr(sample.run, 'phix_aligned')\n            # Error rate calculated from PhiX control\n            data += GenObject.returnattr(sample.run, 'error_rate')\n            # LengthForwardRead\n            data += GenObject.returnattr(sample.run, 'forwardlength',\n                                         number=True)\n            # LengthReverseRead\n            data += GenObject.returnattr(sample.run, 'reverselength',\n                                         number=True)\n            # Real time strain\n            data += GenObject.returnattr(sample.run, 'Description')\n            # Flowcell\n            data += GenObject.returnattr(sample.run, 'flowcell')\n            # MachineName\n            data += GenObject.returnattr(sample.run, 'instrument')\n            # PipelineVersion\n            data += self.commit + ','\n            # AssemblyDate\n            data += datetime.now().strftime('%Y-%m-%d')\n            # Append a new line to the end of the results for this sample\n            data += '\\n'\n        # Replace any NA values with -\n        cleandata = data.replace('NA', 'ND')\n        with open(os.path.join(self.reportpath, 'combinedMetadata.csv'), 'w') as metadatareport:\n            metadatareport.write(header)\n            metadatareport.write(cleandata)", "response": "Creates the summary report for the metadata objects in the file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a report that contains the legacy metadata for each sample.", "response": "def legacy_reporter(self):\n        \"\"\"\n        Creates an output that is compatible with the legacy metadata reports. This method will be removed once\n        a new database scheme is implemented\n        \"\"\"\n        from collections import OrderedDict\n        logging.info('Creating legacy summary report')\n        row = ''\n        # Create a dictionary of tuples to be printed in the final report\n        for sample in self.metadata:\n            data = OrderedDict([\n                ('SampleName', sample.name),\n                ('N50', str(sample.quality_features_polished.n50)),\n                ('NumContigs', str(sample.quality_features_polished.num_contigs)),\n                ('TotalLength', str(sample.quality_features_polished.genome_length)),\n                ('MeanInsertSize', sample.mapping.MeanInsertSize),\n                ('AverageCoverageDepth', sample.mapping.MeanCoveragedata.split(\"X\")[0]),\n                ('ReferenceGenome', sample.mash.closestrefseq),\n                ('RefGenomeAlleleMatches', '-'),\n                ('16sPhylogeny', sample.sixteens_full.genus),\n                ('rMLSTsequenceType', sample.rmlst.sequencetype),\n                ('MLSTsequencetype', sample.mlst.sequencetype),\n                ('MLSTmatches', str(sample.mlst.matchestosequencetype)),\n                ('coreGenome', GenObject.returnattr(sample.coregenome, 'coreresults').rstrip(',')),\n                ('SeroType', '{oset}:{hset}'\n                    .format(oset=';'.join(sample.serosippr.o_set),\n                            hset=';'.join(sample.serosippr.h_set))),\n                ('geneSeekrProfile', ';'.join(result for result, pid in sorted(sample.genesippr.results.items()))),\n                ('vtyperProfile', ';'.join(sorted(sample.legacy_vtyper.toxinprofile))),\n                ('percentGC', str(sample.quality_features_polished.gc)),\n                ('TotalPredictedGenes', str(sample.prodigal.predictedgenestotal)),\n                ('predictedgenesover3000bp', str(sample.prodigal.predictedgenesover3000bp)),\n                ('predictedgenesover1000bp', str(sample.prodigal.predictedgenesover1000bp)),\n                ('predictedgenesover500bp', str(sample.prodigal.predictedgenesover500bp)),\n                ('predictedgenesunder500bp', str(sample.prodigal.predictedgenesunder500bp)),\n                ('SequencingDate', sample.run.Date),\n                ('Investigator', sample.run.InvestigatorName),\n                ('TotalClustersinRun', str(sample.run.TotalClustersinRun)),\n                ('NumberofClustersPF', str(sample.run.NumberofClustersPF)),\n                ('PercentOfClusters', str(sample.run.PercentOfClusters)),\n                ('LengthofForwardRead', str(sample.run.forwardlength)),\n                ('LengthofReverseRead', str(sample.run.reverselength)),\n                ('Project', str(sample.run.SampleProject)),\n                ('PipelineVersion', self.commit)\n            ])\n\n            if not row:\n                row += ','.join([key for key, value in data.items()])\n            row += '\\n'\n            row += ','.join([value for key, value in data.items()])\n        cleanrow = row.replace('NA', '').replace(',-,', ',,')\n        with open(os.path.join(self.reportpath, 'legacy_combinedMetadata.csv'), 'w') as metadatareport:\n            metadatareport.write(cleanrow)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_mapper(row):\n    try:\n        mapper = getattr(row, '__mapper__')\n    except AttributeError as e:\n        e.args = ('Row must be instance of the declarative base class, '\n                  'got %s instead' % type(row).__name__,)\n        raise\n\n    return mapper", "response": "Tries to get the mapper from a row instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef row2dict(row, depth=None, exclude=None, exclude_pk=None,\n             exclude_underscore=None, only=None, fk_suffix=None):\n    \"\"\"\n    Recursively walk row attributes to serialize ones into a dict.\n\n    :param row: instance of the declarative base class\n    :param depth: number that represent the depth of related relationships\n    :param exclude: set of attributes names to exclude\n    :param exclude_pk: are foreign keys (e.g. fk_name_id) excluded\n    :param exclude_underscore: are private and protected attributes excluded\n    :param only: set of attributes names to include\n    :param fk_suffix: str that represent a foreign key suffix\n\n    :return: dict with attributes of current depth level\n    \"\"\"\n    if depth == 0:\n        return None\n    d, mapper = {}, get_mapper(row)\n    if depth is None:\n        depth = getattr(row, ATTR_DEPTH, DEFAULT_DEPTH) - 1\n    else:\n        depth -= 1\n    if exclude is None:\n        exclude = getattr(row, ATTR_EXCLUDE, DEFAULT_EXCLUDE)\n    if exclude_pk is None:\n        exclude_pk = getattr(row, ATTR_EXCLUDE_PK, DEFAULT_EXCLUDE_PK)\n    if exclude_underscore is None:\n        exclude_underscore = getattr(row, ATTR_EXCLUDE_UNDERSCORE,\n                                     DEFAULT_EXCLUDE_UNDERSCORE)\n    if only is None:\n        only = getattr(row, ATTR_ONLY, DEFAULT_ONLY)\n    if fk_suffix is None:\n        fk_suffix = getattr(row, ATTR_FK_SUFFIX, DEFAULT_FK_SUFFIX)\n    for c in mapper.columns.keys() + mapper.synonyms.keys():\n        if c in exclude or \\\n                check_exclude_pk(c, exclude_pk, fk_suffix=fk_suffix) or \\\n                check_exclude_underscore(c, exclude_underscore) or \\\n                check_only(c, only):\n            continue\n        d[c] = getattr(row, c)\n    for r in mapper.relationships.keys():\n        if r in exclude or check_only(r, only):\n            continue\n        attr = getattr(row, r)\n        backref = get_backref(mapper.relationships[r])\n        if backref:\n            exclude.add(backref)\n        kwargs = dict(depth=depth, exclude=exclude, exclude_pk=exclude_pk,\n                      exclude_underscore=exclude_underscore, only=only,\n                      fk_suffix=fk_suffix)\n        if isinstance(attr, collections.InstrumentedList):\n            d[r] = [row2dict(i, **kwargs) for i in attr if depth]\n        else:\n            d[r] = row2dict(attr, **kwargs)\n\n    return d", "response": "This function recursively walks the row attributes to serialize ones into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dict2row(d, model, rel=None, exclude=None, exclude_pk=None,\n             exclude_underscore=None, only=None, fk_suffix=None):\n    \"\"\"\n    Recursively walk dict attributes to serialize ones into a row.\n\n    :param d: dict that represent a serialized row\n    :param model: class nested from the declarative base class\n    :param rel: dict of key (relationship name) -value (class) pairs\n    :param exclude: set of attributes names to exclude\n    :param exclude_pk: are foreign keys (e.g. fk_name_id) excluded\n    :param exclude_underscore: are private and protected attributes excluded\n    :param only: set of attributes names to include\n    :param fk_suffix: str that represent a foreign key suffix\n\n    :return: instance of the declarative base class\n    \"\"\"\n    if not isinstance(d, dict):\n        raise TypeError('Source must be instance of dict, got %s instead' %\n                        type(d).__name__)\n    row = model()\n    mapper = get_mapper(row)\n    if rel is None:\n        rel = getattr(row, ATTR_REL, DEFAULT_REL)\n    if exclude is None:\n        exclude = getattr(row, ATTR_EXCLUDE, DEFAULT_EXCLUDE)\n    if exclude_pk is None:\n        exclude_pk = getattr(row, ATTR_EXCLUDE_PK, DEFAULT_EXCLUDE_PK)\n    if exclude_underscore is None:\n        exclude_underscore = getattr(row, ATTR_EXCLUDE_UNDERSCORE,\n                                     DEFAULT_EXCLUDE_UNDERSCORE)\n    if only is None:\n        only = getattr(row, ATTR_ONLY, DEFAULT_ONLY)\n    if fk_suffix is None:\n        fk_suffix = getattr(row, ATTR_FK_SUFFIX, DEFAULT_FK_SUFFIX)\n    for c in mapper.columns.keys() + mapper.synonyms.keys():\n        if c not in d or c in exclude or \\\n                check_exclude_pk(c, exclude_pk, fk_suffix=fk_suffix) or \\\n                check_exclude_underscore(c, exclude_underscore) or \\\n                check_only(c, only):\n            continue\n        setattr(row, c, d[c])\n    for r in mapper.relationships.keys():\n        if r not in d or r not in rel or check_only(r, only):\n            continue\n        kwargs = dict(rel=rel, exclude=exclude, exclude_pk=exclude_pk,\n                      exclude_underscore=exclude_underscore, only=only,\n                      fk_suffix=fk_suffix)\n        if isinstance(d[r], list):\n            setattr(row, r, collections.InstrumentedList())\n            for i in d[r]:\n                getattr(row, r).append(dict2row(i, rel[r], **kwargs))\n        else:\n            if not exclude_pk:\n                rpk = d[r].get('id') if isinstance(d[r], dict) else None\n                setattr(row, r + fk_suffix, rpk)\n            setattr(row, r, dict2row(d[r], rel[r], **kwargs))\n\n    return row", "response": "This function takes a dict that represents a row and returns a dict that represents the serialized row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncopy source file to destination.", "response": "def copy_file(source, destination, follow_symlinks=True,\n              template: arg(type=bool_or(str), choices=('format', 'string')) = False,\n              context=None):\n    \"\"\"Copy source file to destination.\n\n    The destination may be a file path or a directory. When it's a\n    directory, the source file will be copied into the directory\n    using the file's base name.\n\n    When the source file is a template, ``context`` will be used as the\n    template context. The supported template types are 'format' and\n    'string'. The former uses ``str.format_map()`` and the latter uses\n    ``string.Template()``.\n\n    .. note:: :func:`shutil.copy()` from the standard library is used to\n        do the copy operation.\n\n    \"\"\"\n    if not template:\n        # Fast path for non-templates.\n        return shutil.copy(source, destination, follow_symlinks=follow_symlinks)\n\n    if os.path.isdir(destination):\n        destination = os.path.join(destination, os.path.basename(source))\n\n    with open(source) as source:\n        contents = source.read()\n\n    if template is True or template == 'format':\n        contents = contents.format_map(context)\n    elif template == 'string':\n        string_template = string.Template(contents)\n        contents = string_template.substitute(context)\n    else:\n        raise ValueError('Unknown template type: %s' % template)\n\n    with tempfile.NamedTemporaryFile('w', delete=False) as temp_file:\n        temp_file.write(contents)\n\n    path = shutil.copy(temp_file.name, destination)\n    os.remove(temp_file.name)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the version of the git repository.", "response": "def git_version(short: 'Get short hash' = True, show: 'Print version to stdout' = False):\n    \"\"\"Get tag associated with HEAD; fall back to SHA1.\n\n    If HEAD is tagged, return the tag name; otherwise fall back to\n    HEAD's short SHA1 hash.\n\n    .. note:: Only annotated tags are considered.\n\n    .. note:: The output isn't shown by default. To show it, pass the\n        ``--show`` flag.\n\n    \"\"\"\n    result = local(\n        ['git', 'rev-parse', '--is-inside-work-tree'],\n        stdout='hide', stderr='hide', echo=False, raise_on_error=False)\n\n    if not result:\n        # Not a git directory\n        return None\n\n    # Return a tag if possible\n    result = local(\n        ['git', 'describe', '--exact-match'],\n        stdout='capture', stderr='hide', echo=False, raise_on_error=False)\n\n    if result:\n        return result.stdout\n\n    # Fall back to hash\n    result = local(\n        ['git', 'rev-parse', '--short' if short else None, 'HEAD'],\n        stdout='capture', stderr='hide', echo=False, raise_on_error=False)\n\n    if result:\n        version = result.stdout.strip()\n        if show:\n            print(version)\n        return version\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef local(args: arg(container=list),\n          cd=None,\n          environ: arg(type=dict) = None,\n          replace_env=False,\n          paths=(),\n          shell: arg(type=bool) = None,\n          stdout: arg(type=StreamOptions) = None,\n          stderr: arg(type=StreamOptions) = None,\n          echo=False,\n          raise_on_error=True,\n          dry_run=False,\n          ) -> Result:\n    \"\"\"Run a local command via :func:`subprocess.run`.\n\n    Args:\n        args (list|str): A list of args or a shell command.\n        cd (str): Working directory to change to first.\n        environ (dict): Additional environment variables to pass to the\n            subprocess.\n        replace_env (bool): If set, only pass env variables from\n            ``environ`` to the subprocess.\n        paths (list): A list of additional paths.\n        shell (bool): Run as a shell command? The default is to run in\n            shell mode if ``args`` is a string. This flag can be used to\n            force a list of args to be run as a shell command too.\n        stdout (StreamOptions): What to do with stdout (capture, hide,\n            or show).\n        stderr (StreamOptions): Same as ``stdout``.\n        echo (bool): Whether to echo the command before running it.\n        raise_on_error (bool): Whether to raise an exception when the\n            subprocess returns a non-zero exit code.\n        dry_run (bool): If set, print command instead of running it.\n\n    Returns:\n        Result\n\n    Raises:\n        Result: When the subprocess returns a non-zero exit code (and\n            ``raise_on_error`` is set).\n\n    \"\"\"\n    if isinstance(args, str):\n        if shell is None:\n            shell = True\n    else:\n        args = flatten_args(args, join=shell)\n\n    if cd:\n        cd = abs_path(cd)\n        cd_passed = True\n    else:\n        cd_passed = False\n\n    environ = {k: str(v) for k, v in (environ or {}).items()}\n\n    if replace_env:\n        subprocess_env = environ.copy()\n    else:\n        subprocess_env = os.environ.copy()\n        if environ:\n            subprocess_env.update(environ)\n\n    if paths:\n        paths = [paths] if isinstance(paths, str) else paths\n        paths = [abs_path(p) for p in paths]\n        current_path = subprocess_env.get('PATH')\n        if current_path:\n            paths.append(current_path)\n        path = ':'.join(paths)\n        subprocess_env['PATH'] = path\n\n    if stdout:\n        stdout = StreamOptions[stdout] if isinstance(stdout, str) else stdout\n        stdout = stdout.option\n\n    if stderr:\n        stderr = StreamOptions[stderr] if isinstance(stderr, str) else stderr\n        stderr = stderr.option\n\n    kwargs = {\n        'cwd': cd,\n        'env': subprocess_env,\n        'shell': shell,\n        'stdout': stdout,\n        'stderr': stderr,\n        'universal_newlines': True,\n    }\n\n    display_str = args if shell else ' '.join(shlex.quote(a) for a in args)\n\n    if echo:\n        if cd_passed:\n            printer.echo('{cd}>'.format_map(locals()), end=' ')\n        if not dry_run:\n            printer.echo(display_str)\n\n    if dry_run:\n        printer.echo('[DRY RUN]', display_str)\n        result = Result(args, 0, None, None)\n    else:\n        result = subprocess.run(args, **kwargs)\n        result = Result.from_subprocess_result(result)\n\n    if result.return_code and raise_on_error:\n        raise result\n\n    return result", "response": "Run a local command via subprocess. run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun a remote command via SSH.", "response": "def remote(cmd: arg(container=list),\n           host,\n           user=None,\n           port=None,\n           sudo=False,\n           run_as=None,\n           shell='/bin/sh',\n           cd=None,\n           environ: arg(container=dict) = None,\n           paths=(),\n           # Args passed through to local command:\n           stdout: arg(type=StreamOptions) = None,\n           stderr: arg(type=StreamOptions) = None,\n           echo=False,\n           raise_on_error=True,\n           dry_run=False,\n           ) -> Result:\n    \"\"\"Run a remote command via SSH.\n\n    Runs a remote shell command using ``ssh`` in a subprocess like so:\n\n    ssh -q [-t] [<user>@]<host> [sudo [-u <run_as>] -H] /bin/sh -c '\n        [cd <cd> &&]\n        [export XYZ=\"xyz\" &&]\n        [export PATH=\"<path>\" &&]\n        <cmd>\n    '\n\n    Args:\n        cmd (list|str): The command to run. If this is a list, it will\n            be flattened into a string.\n        host (str): Remote host to SSH into.\n        user (str): Remote user to log in as (defaults to current local\n            user).\n        port (int): SSH port on remote host.\n        sudo (bool): Run the remote command as root using ``sudo``.\n        run_as (str): Run the remote command as a different user using\n            ``sudo -u <run_as>``.\n        shell (str): The remote user's default shell will be used to run\n            the remote command unless this is set to a different shell.\n        cd (str): Where to run the command on the remote host.\n        environ (dict): Extra environment variables to set on the remote\n            host.\n        paths (list): Additional paths to prepend to the remote\n            ``$PATH``.\n        stdout: See :func:`local`.\n        stderr: See :func:`local`.\n        echo: See :func:`local`.\n        raise_on_error: See :func:`local`.\n        dry_run: See :func:`local`.\n\n    \"\"\"\n    if not isinstance(cmd, str):\n        cmd = flatten_args(cmd, join=True)\n\n    ssh_options = ['-q']\n    if isatty(sys.stdin):\n        ssh_options.append('-t')\n    if port is not None:\n        ssh_options.extend(('-p', port))\n\n    ssh_connection_str = '{user}@{host}'.format_map(locals()) if user else host\n\n    remote_cmd = []\n\n    if sudo:\n        remote_cmd.extend(('sudo', '-H'))\n    elif run_as:\n        remote_cmd.extend(('sudo', '-H', '-u', run_as))\n\n    remote_cmd.extend((shell, '-c'))\n\n    inner_cmd = []\n\n    if cd:\n        inner_cmd.append('cd {cd}'.format_map(locals()))\n\n    if environ:\n        inner_cmd.extend('export {k}=\"{v}\"'.format_map(locals()) for k, v in environ.items())\n\n    if paths:\n        inner_cmd.append('export PATH=\"{path}:$PATH\"'.format(path=':'.join(paths)))\n\n    inner_cmd.append(cmd)\n    inner_cmd = ' &&\\n    '.join(inner_cmd)\n    inner_cmd = '\\n    {inner_cmd}\\n'.format_map(locals())\n    inner_cmd = shlex.quote(inner_cmd)\n\n    remote_cmd.append(inner_cmd)\n    remote_cmd = ' '.join(remote_cmd)\n\n    args = ('ssh', ssh_options, ssh_connection_str, remote_cmd)\n    return local(\n        args, stdout=stdout, stderr=stderr, echo=echo, raise_on_error=raise_on_error,\n        dry_run=dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the object to the database.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        **uid**: :code:`cycle:{year}`\n        \"\"\"\n        self.slug = slugify(self.name)\n        self.uid = 'cycle:{}'.format(self.slug)\n        super(ElectionCycle, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_mean_and_median(hist: Hist) -> Tuple[float, float]:\n    # Median\n    # See: https://root-forum.cern.ch/t/median-of-histogram/7626/5\n    x = ctypes.c_double(0)\n    q = ctypes.c_double(0.5)\n    # Apparently needed to be safe(?)\n    hist.ComputeIntegral()\n    hist.GetQuantiles(1, x, q)\n\n    mean = hist.GetMean()\n\n    return (mean, x.value)", "response": "Retrieve the mean and median of a ROOT histogram."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprojecting the input histogram to the particle level axis.", "response": "def _project_to_part_level(hist: Hist, outliers_removal_axis: OutliersRemovalAxis) -> Hist:\n    \"\"\" Project the input histogram to the particle level axis.\n\n    Args:\n        hist: Histogram to check for outliers.\n        outliers_removal_axis: Axis along which outliers removal will be performed. Usually\n            the particle level aixs.\n    Returns:\n        The histogram to check for outliers.\n    \"\"\"\n    # Setup the projector\n    import ROOT\n    if isinstance(hist, (ROOT.TH2, ROOT.TH3)):\n        projection_information: Dict[str, Any] = {}\n        output_object = _OutputObject(None)\n        projector = projectors.HistProjector(\n            observable_to_project_from = hist,\n            output_observable = output_object,\n            output_attribute_name = \"output\",\n            projection_name_format = \"outliers_removal_hist\",\n            projection_information = projection_information,\n        )\n        # No additional_axis_cuts or projection_dependent_cut_axes\n        # Projection axis\n        projector.projection_axes.append(\n            projectors.HistAxisRange(\n                axis_type = outliers_removal_axis,\n                axis_range_name = \"outliers_removal_axis\",\n                min_val = projectors.HistAxisRange.apply_func_to_find_bin(None, 1),\n                max_val = projectors.HistAxisRange.apply_func_to_find_bin(ROOT.TAxis.GetNbins),\n            )\n        )\n\n        # Perform the actual projection and return the output.\n        projector.project()\n        return output_object.output\n\n    # If we already have a 1D hist, just return that existing hist.\n    return hist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine the index of the outliers in a 1D histogram.", "response": "def _determine_outliers_index(hist: Hist,\n                              moving_average_threshold: float = 1.0,\n                              number_of_values_to_search_ahead: int = 5,\n                              limit_of_number_of_values_below_threshold: int = None) -> int:\n    \"\"\" Determine the location of where outliers begin in a 1D histogram.\n\n    When the moving average falls below the limit, we consider the outliers to have begun.\n\n    To determine the location of outliers:\n\n    - Calculate the moving average for number_of_values_to_search_ahead values.\n    - First, the moving average must go above the limit at least once to guard against a random cut\n      in a low pt bin causing most of the data to be cut out.\n    - Next, we look for a consecutive number of entries below limit_of_number_of_values_below_threshold.\n    - If we meet that condition, we have found the index where the outliers begin. We then return the ROOT\n      bin index of the value.\n    - If not, we return -1.\n\n    Note:\n        The index returned is when the moving average first drops below the threshold for a moving average\n        calculated with that bin at the center. This is somewhat different from a standard moving average\n        calculation which would only look forward in the array.\n\n    Args:\n        hist: Histogram to be checked for outliers.\n        moving_average_threshold: Value of moving average under which we consider the moving average\n            to be 0. Default: 2.\n        number_of_values_to_search_ahead: Number of values to search ahead in the array when calculating\n            the moving average. Default: 5.\n        limit_of_number_of_values_below_threshold: Number of consecutive bins below the threshold to be considered\n            the beginning of outliers. Default: None, which will correspond to number_of_values_to_search_ahead - 1.\n    Returns:\n        ROOT (ie 1-indexed) index of the histogram axes where the outliers begin.\n    \"\"\"\n    # Validation\n    import ROOT\n    if isinstance(hist, (ROOT.TH2, ROOT.TH3, ROOT.THnBase)):\n        raise ValueError(\n            f\"Given histogram '{hist.GetName()}' of type {type(hist)}, but can only\"\n            \" determine the outlier location of a 1D histogram. Please project to\"\n            \" the particle level axis first.\"\n        )\n\n    if limit_of_number_of_values_below_threshold is None:\n        # In principle, this could be another value. However, this is what was used in the previous outliers\n        # removal implementation.\n        limit_of_number_of_values_below_threshold = number_of_values_to_search_ahead - 1\n\n    # It is much more convenient to work with a numpy array.\n    hist_to_check = histogram.Histogram1D.from_existing_hist(hist)\n\n    # Calculate the moving average for the entire axis, looking ahead including the current bin + 4 = 5 ahead.\n    number_of_values_to_search_ahead = 5\n    moving_average = utils.moving_average(hist_to_check.y, n = number_of_values_to_search_ahead)\n\n    #logger.debug(f\"y: {hist_to_check.y}\")\n    #logger.debug(f\"moving_average: {moving_average}\")\n\n    cut_index = _determine_outliers_for_moving_average(\n        moving_average = moving_average,\n        moving_average_threshold = moving_average_threshold,\n        number_of_values_to_search_ahead = number_of_values_to_search_ahead,\n        limit_of_number_of_values_below_threshold = limit_of_number_of_values_below_threshold,\n    )\n\n    if cut_index != -1:\n        # ROOT histograms are 1 indexed, so we add another 1.\n        cut_index += 1\n\n    return cut_index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine outliers to remove from a given moving average. Note: The index returned is when the moving average first drops below the threshold for a moving average calculated with that bin at the center. This is somewhat different from a standard moving average calculation which would only look forward in the array. Args: moving_average: Moving average. moving_average_threshold: Value of moving average under which we consider the moving average to be 0. Default: 2. number_of_values_to_search_ahead: Number of values to search ahead in the array when calculating the moving average. Default: 5. limit_of_number_of_values_below_threshold: Number of consecutive bins below the threshold to be considered the beginning of outliers. Default: None, which will correspond to number_of_values_to_search_ahead - 1. Returns: 0-indexed index of the histogram axes where the outliers begin.", "response": "def _determine_outliers_for_moving_average(moving_average: np.ndarray,\n                                           moving_average_threshold: float,\n                                           number_of_values_to_search_ahead: int,\n                                           limit_of_number_of_values_below_threshold: int) -> int:\n    \"\"\" Determine outliers to remove from a given moving average.\n\n    Note:\n        The index returned is when the moving average first drops below the threshold for a moving average\n        calculated with that bin at the center. This is somewhat different from a standard moving average\n        calculation which would only look forward in the array.\n\n    Args:\n        moving_average: Moving average.\n        moving_average_threshold: Value of moving average under which we consider the moving average\n            to be 0. Default: 2.\n        number_of_values_to_search_ahead: Number of values to search ahead in the array when calculating\n            the moving average. Default: 5.\n        limit_of_number_of_values_below_threshold: Number of consecutive bins below the threshold to be considered\n            the beginning of outliers. Default: None, which will correspond to number_of_values_to_search_ahead - 1.\n    Returns:\n        0-indexed index of the histogram axes where the outliers begin.\n    \"\"\"\n    below_threshold = moving_average < moving_average_threshold\n\n    # Build up a list of values to check if they are below threshold. This list allows us to easily look\n    # forward in the below_threshold array.\n    values_to_check = []\n    for i in range(limit_of_number_of_values_below_threshold):\n        # Basically, this gives us (for limit_of_number_of_values_below_threshold = 4):\n        # below_threshold[0:-3], below_threshold[1:-2], below_threshold[2:-1], below_threshold[3:None]\n        values_to_check.append(\n            below_threshold[i:-(limit_of_number_of_values_below_threshold - 1 - i) or None]\n        )\n\n    # Some helpful logging information.\n    #logger.debug(f\"values_to_check: {values_to_check}\")\n    #logger.debug(f\"moving avg length: {len(moving_average)}, length of values_to_check entries: {[len(v) for v in values_to_check]}\")\n\n    # Must have at least one bin above the specified threshold.\n    found_at_least_one_bin_above_threshold = False\n    # Index we will search for from which outliers will be cut.\n    cut_index = -1\n\n    # Determine the index where the limit_of_number_of_values_below_threshold bins are consequentially below the threshold.\n    for i, values in enumerate(zip(*values_to_check)):\n        # Skip the first bin because some old pt hard bin trains had a large number of erroneous entries\n        # in the first bin (regardless of the actual pt hard bin). This should be resolved in the embedding\n        # helper now. In any case, it doesn't make sense to encounter outliers in the first bin, so this is a\n        # fine bin to skip.\n        if i == 0:\n            continue\n\n        # True if below threshold, so check if not True.\n        above_threshold = [not value for value in values]\n        # We require the values to go above the moving average threshold at least once.\n        if any(above_threshold):\n            #logger.debug(f\"Found bin i {i} above threshold with moving average: {moving_average[i]}\")\n            found_at_least_one_bin_above_threshold = True\n\n        # All values from which we are looking ahead must be below the threshold to consider the index\n        # as below threshold.\n        if found_at_least_one_bin_above_threshold and all(np.invert(above_threshold)):\n            # The previous outlier removal implementation used a moving average centered on a value\n            # (ie. it checked ``arr[-2 + current_index:current_index + 3]``). Thus, we need to\n            # shift the cut_index that we assign by limit_of_number_of_values_below_threshold // 2 for\n            # the index where we have found all values below the threshold.\n            logger.debug(f\"i at found cut_index: {i} with moving_average: {moving_average[i]}\")\n            cut_index = i + limit_of_number_of_values_below_threshold // 2\n            break\n\n    return cut_index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove outliers from a given histogram.", "response": "def _remove_outliers_from_hist(hist: Hist, outliers_start_index: int, outliers_removal_axis: OutliersRemovalAxis) -> None:\n    \"\"\" Remove outliers from a given histogram.\n\n    Args:\n        hist: Histogram to check for outliers.\n        outliers_start_index: Index in the truth axis where outliers begin.\n        outliers_removal_axis: Axis along which outliers removal will be performed. Usually\n            the particle level aixs.\n    Returns:\n        None. The histogram is modified in place.\n    \"\"\"\n    # Use on TH1, TH2, and TH3 since we don't start removing immediately, but instead only after the limit\n    if outliers_start_index > 0:\n        #logger.debug(\"Removing outliers\")\n        # Check for values above which they should be removed by translating the global index\n        x = ctypes.c_int(0)\n        y = ctypes.c_int(0)\n        z = ctypes.c_int(0)\n        # Maps axis to valaues\n        # This is kind of dumb, but it works.\n        outliers_removal_axis_values: Dict[OutliersRemovalAxis, ctypes.c_int] = {\n            projectors.TH1AxisType.x_axis: x,\n            projectors.TH1AxisType.y_axis: y,\n            projectors.TH1AxisType.z_axis: z,\n        }\n        for index in range(0, hist.GetNcells()):\n            # Get the bin x, y, z from the global bin\n            hist.GetBinXYZ(index, x, y, z)\n            # Watch out for any problems\n            if hist.GetBinContent(index) < hist.GetBinError(index):\n                logger.warning(f\"Bin content < error. Name: {hist.GetName()}, Bin content: {hist.GetBinContent(index)}, Bin error: {hist.GetBinError(index)}, index: {index}, ({x.value}, {y.value})\")\n            if outliers_removal_axis_values[outliers_removal_axis].value >= outliers_start_index:\n                #logger.debug(\"Cutting for index {}. x bin {}. Cut index: {}\".format(index, x, cutIndex))\n                hist.SetBinContent(index, 0)\n                hist.SetBinError(index, 0)\n    else:\n        logger.info(f\"Hist {hist.GetName()} did not have any outliers to cut\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shapefile(self, file):\n        \n        driver = ogr.GetDriverByName('ESRI Shapefile')\n        dataset = driver.Open(file)\n        if dataset is not None:\n            # from Layer\n            layer = dataset.GetLayer()\n            spatialRef = layer.GetSpatialRef()\n            # from Geometry\n            feature = layer.GetNextFeature()\n            geom = feature.GetGeometryRef()\n            spatialRef = geom.GetSpatialReference()\n            \n            #WGS84\n            outSpatialRef = osr.SpatialReference()\n            outSpatialRef.ImportFromEPSG(4326)\n    \n            coordTrans = osr.CoordinateTransformation(spatialRef, outSpatialRef)\n    \n            env = geom.GetEnvelope()\n            xmin = env[0]\n            ymin = env[2]\n            xmax = env[1]\n            ymax = env[3]\n    \n            pointMAX = ogr.Geometry(ogr.wkbPoint)\n            pointMAX.AddPoint(env[1], env[3])\n            pointMAX.Transform(coordTrans)\n            \n            pointMIN = ogr.Geometry(ogr.wkbPoint)\n            pointMIN.AddPoint(env[0], env[2])\n            pointMIN.Transform(coordTrans)\n    \n    \n            self.bbox = str(pointMIN.GetPoint()[0])+','+str(pointMIN.GetPoint()[1])+','+str(pointMAX.GetPoint()[0])+','+str(pointMAX.GetPoint()[1])\n            self.query = None\n        else:\n            exit(\" shapefile not found. Please verify your path to the shapefile\")", "response": "reprojette en WGS84 et recupere l 'extend\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_comment(self, cellid, comment):\n        info = {'cellid': cellid, 'comment': comment}\n        self.datafile.set_metadata(self.current_dataset_name, info)", "response": "Saves the provided comment to the current dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mousePressEvent(self, event):\n        if event.x() < 50:\n            super(PlotMenuBar, self).mousePressEvent(event)\n        else:\n            # ignore to allow proper functioning of float\n            event.ignore()", "response": "Marshalls behaviour depending on location of the mouse click"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a command template add it as a job to the queue.", "response": "def add(self, command_template, job_class):\n        \"\"\" Given a command template, add it as a job to the queue. \"\"\"\n        job = JobTemplate(command_template.alias,\n            command_template=command_template,\n            depends_on=command_template.depends_on, queue=self.queue,\n            job_class=job_class)\n        self.queue.push(job)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        iterations = 0\n        queue = self.queue.tick()\n        while True:\n            try:\n                next(queue)\n            except StopIteration:\n                break\n\n            iterations += 1\n            sleep(self.sleep_time)\n        return iterations", "response": "Starts the runtime execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_dynamic_class(typename, field_names):\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(\",\", \" \").split()\n    field_names = map(str, field_names)\n\n    safe_fields_names = map(_encode_property_name, field_names)\n\n    attr = dict((safe_name, _property(name)) for name, safe_name in zip(field_names, safe_fields_names))\n    attr['__doc__'] = typename\n    attr['__identifier__'] = \"dolphin\"\n    attr['__init__'] = _dynamic__init\n    attr['__getitem__'] = lambda self, key: self.__dict__.get(key)\n    attr['__setitem__'] = _dynamic__setitem\n    attr['__iter__'] = lambda self: iter(self.__dict__)\n    attr['__repr__'] = lambda self: \"{%s}\" % (', '.join([\n                                                            \"%s=%r\" % (key, self[key]) for key in\n                                                            sorted(self.__dict__.keys())\n                                                            ]))\n\n    return type(typename, (object,), attr)", "response": "a factory function to create a dynamic class from a string containing a list of field names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting RAM memory usage of this process", "response": "def get_memory_usage():\n    \"\"\"Gets RAM memory usage\n\n    :return: MB of memory used by this process\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    mem = process.memory_info().rss\n    return mem / (1024 * 1024)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an empty SQLite database for library spectra.", "response": "def create_db(file_pth):\n    \"\"\" Create an empty SQLite database for library spectra.\n\n    Example:\n        >>> from msp2db.db import create_db\n        >>> db_pth = 'library.db'\n        >>> create_db(file_pth=db_pth)\n\n    Args:\n        file_pth (str): File path for SQLite database\n\n    \"\"\"\n    conn = sqlite3.connect(file_pth)\n    c = conn.cursor()\n\n    c.execute('DROP TABLE IF EXISTS library_spectra_source')\n    c.execute('''CREATE TABLE library_spectra_source (\n                          id integer PRIMARY KEY,\n                          name text NOT NULL,\n                          created_at date,\n                          parsing_software text\n                          )'''\n              )\n\n    c.execute('DROP TABLE IF EXISTS metab_compound')\n    c.execute('''CREATE TABLE metab_compound (\n                  inchikey_id text PRIMARY KEY,\n                  name text,\n                  pubchem_id text,\n                  chemspider_id text,\n                  other_names text,\n                  exact_mass real,\n                  molecular_formula text,\n                  molecular_weight real,\n                  compound_class text,\n                  smiles text,\n                  created_at date,\n                  updated_at date\n\n                                           )''')\n\n    c.execute('DROP TABLE IF EXISTS library_spectra_meta')\n    c.execute('''CREATE TABLE library_spectra_meta (\n                                   id integer PRIMARY KEY,\n                                   name text,\n                                   collision_energy text,\n                                   ms_level real,\n                                   accession text NOT NULL,\n                                   resolution text,\n                                   polarity integer,\n                                   fragmentation_type text,\n                                   precursor_mz real,\n                                   precursor_type text,\n                                   instrument_type text,\n                                   instrument text,\n                                   copyright text,\n                                   column text,\n                                   mass_accuracy real,\n                                   mass_error real,\n                                   origin text,\n                                   splash text,\n                                   retention_index real, \n                                   retention_time real,\n                                   library_spectra_source_id integer NOT NULL,\n                                   inchikey_id text NOT NULL,\n                                   FOREIGN KEY(library_spectra_source_id) REFERENCES library_spectra_source(id),\n                                   FOREIGN KEY(inchikey_id) REFERENCES metab_compound(inchikey_id)\n                                   )'''\n              )\n\n    c.execute('DROP TABLE IF EXISTS library_spectra')\n    c.execute('''CREATE TABLE library_spectra (\n                                          id integer PRIMARY KEY,\n                                          mz real NOT NULL,\n                                          i real NOT NULL,\n                                          other text,\n                                          library_spectra_meta_id integer NOT NULL,\n                                          FOREIGN KEY (library_spectra_meta_id) REFERENCES library_spectra_meta(id)\n                                          )'''\n              )\n\n    c.execute('DROP TABLE IF EXISTS library_spectra_annotation')\n    c.execute('''CREATE TABLE library_spectra_annotation (\n                                          id integer PRIMARY KEY,\n                                          mz real,\n                                          tentative_formula text,\n                                          mass_error real,\n                                          library_spectra_meta_id integer NOT NULL,\n                                          FOREIGN KEY (library_spectra_meta_id) REFERENCES library_spectra_meta(id)\n                                          )'''\n              )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a connection to a SQL database. Can be used for SQLite MySQL or Django MySQL database.", "response": "def get_connection(db_type, db_pth, user=None, password=None, name=None):\n    \"\"\" Get a connection to a SQL database. Can be used for SQLite, MySQL or Django MySQL database\n\n    Example:\n        >>> from msp2db.db import get_connection\n        >>> conn = get_connection('sqlite', 'library.db')\n\n    If using \"mysql\" mysql.connector needs to be installed.\n\n    If using \"django_mysql\" Django needs to be installed.\n\n    Args:\n        db_type (str): Type of database can either be \"sqlite\", \"mysql\" or \"django_mysql\"\n\n\n    Returns:\n       sql connection object\n\n    \"\"\"\n    if db_type == 'sqlite':\n        print(db_pth)\n        conn = sqlite3.connect(db_pth)\n    elif db_type == 'mysql':\n        import mysql.connector\n        conn = mysql.connector.connect(user=user, password=password, database=name)\n    elif db_type == 'django_mysql':\n        from django.db import connection as conn\n    else:\n        print('unsupported database type: {}, choices are \"sqlite\", \"mysql\" or \"django_mysql\"'.format(db_type))\n\n    return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary of the library spectra from a database.", "response": "def db_dict(c):\n    \"\"\" Get a dictionary of the library spectra from a database\n\n    Example:\n        >>> from msp2db.db import get_connection\n        >>> conn = get_connection('sqlite', 'library.db')\n        >>> test_db_d = db_dict(conn.cursor())\n\n    If using a large database the resulting dictionary will be very large!\n\n    Args:\n        c (cursor): SQL database connection cursor\n\n    Returns:\n       A dictionary with the following keys 'library_spectra', 'library_spectra_meta', 'library_spectra_annotations',\n       'library_spectra_source' and 'metab_compound'. Where corresponding values for each key are list of list containing\n       all the rows in the database.\n\n    \"\"\"\n    db_d = {}\n    c.execute('SELECT * FROM library_spectra')\n    db_d['library_spectra'] = [list(row) for row in c]\n\n    c.execute('SELECT * FROM library_spectra_meta')\n    db_d['library_spectra_meta'] = [list(row) for row in c]\n\n    c.execute('SELECT * FROM library_spectra_annotation')\n    db_d['library_spectra_annotations'] = [list(row) for row in c]\n\n    c.execute('SELECT * FROM library_spectra_source')\n    db_d['library_spectra_source'] = [list(row) for row in c]\n\n    c.execute('SELECT * FROM metab_compound')\n    db_d['metab_compound'] = [list(row) for row in c]\n\n    return db_d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting python list of tuples into SQL table Taxonomy", "response": "def insert_query_m(data, table, conn, columns=None, db_type='mysql'):\n    \"\"\" Insert python list of tuples into SQL table\n\n    Args:\n        data (list): List of tuples\n        table (str): Name of database table\n        conn (connection object): database connection object\n        columns (str): String of column names to use if not assigned then all columns are presumed to be used [Optional]\n        db_type (str): If \"sqlite\" or \"mysql\"\n\n    \"\"\"\n    # if length of data is very large we need to break into chunks the insert_query_m is then used recursively untill\n    # all data has been inserted\n    if len(data) > 10000:\n        _chunk_query(data, 10000, columns, conn, table, db_type)\n    else:\n        # sqlite and mysql have type string (? or %s) reference to use\n        if db_type == 'sqlite':\n            type_sign = '?'\n        else:\n            type_sign = '%s'\n        # create a string of types for the insertion string (e.g. ?,?,? if inserting 3 columns of data)\n        type_com = type_sign + \", \"\n        type = type_com * (len(data[0]) - 1)\n        type = type + type_sign\n\n        # if using specific columns to insert data\n        if columns:\n            stmt = \"INSERT INTO \" + table + \"( \" + columns + \") VALUES (\" + type + \")\"\n        else:\n            stmt = \"INSERT INTO \" + table + \" VALUES (\" + type + \")\"\n\n        # execute query\n        cursor = conn.cursor()\n        cursor.executemany(stmt, data)\n        conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _chunk_query(l, n, cn, conn, table, db_type):\n    # For item i in a range that is a length of l,\n    [insert_query_m(l[i:i + n], table, conn, cn, db_type) for i in range(0, len(l), n)]", "response": "Call for inserting SQL query in chunks based on n rows\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert any python list of lists or tuples into a list of lists so that the strings are formatted correctly for insertion into the .", "response": "def _make_sql_compatible(ll):\n    \"\"\" Convert any python list of lists (or tuples) so that the strings are formatted correctly for insertion into\n\n    Args:\n        ll (list): List of lists (or tuples)\n    \"\"\"\n\n    new_ll = []\n    for l in ll:\n        new_l = ()\n        for i in l:\n            if not i:\n                new_l = new_l + (None,)\n            else:\n\n                if isinstance(i, str):\n                    if sys.version_info < (3, 0):\n\n                        val = i.decode('utf8').encode('ascii', errors='ignore')\n                    else:\n                        # in py3 strings should be ok...\n                        val = i\n                else:\n                    val = i\n                new_l = new_l + (val,)\n        new_ll.append(new_l)\n\n    return new_ll"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_badges_pages(self, appid_filter=None):\n        ''' Iterates over all badges pages of a steam profile\n            Parses all badges (using parse_badge()) to return the appId, play time ('till now) and the number of card drops left.\n\n            @param appid_filter only look for appids listed here\n        '''\n        appid_filter = appid_filter or []\n        filter_appids = True if appid_filter else False\n        parsed_apps = {}\n        currentPage = badgePages = 1\n\n        retry = False\n        while currentPage <= badgePages and (filter_appids == False or len(appid_filter) > 0):\n            r = self.swb.get('https://steamcommunity.com/my/badges', params={'p': currentPage})\n            if r.status_code == 302:\n                if retry:\n                    # We already tries to force a login\n                    raise Exception('Unable to fetch badges')\n                # Looks like we've been redirected. Force a login and retry\n                print('Need to login again')\n                self.swb.login()\n                retry = True\n                continue\n\n            soup = BeautifulSoup(r.content, 'html.parser')\n            if currentPage == 1:\n                try:\n                    badgePages = int(soup.find_all('a', {'class': 'pagelink'})[-1].get_text())\n                except:\n                    pass\n\n            for b in soup.find_all('div', {'class': 'badge_title_stats'}):\n                try:\n                    app = self.parse_badge(b)\n                except PageParserError:\n                    # Could not correctly parse app info, continue with the next one\n                    continue\n\n                # AppId's where given as filter, check if this AppId is one of those\n                if filter_appids:\n                    if app.appid in appid_filter:\n                        appid_filter.remove(app.appid)\n                    else:\n                        # This appid is NOT in the filter list.\n                        # don't include it in the returned list\n                        continue\n\n                # Add app info to the list of parsed badges\n                parsed_apps[app.appid] = app\n\n                if filter_appids and len(appid_filter) == 0:\n                    # AppId's where given as filter and all of them where found already\n                    # so we are done.\n                    break\n            # Continue with next page\n            currentPage += 1\n\n        if not parsed_apps:\n            #FIXME: empty badges page == no good.\n            print('ERRPOR: Could not find any badges on badge page')\n            from tempfile import NamedTemporaryFile\n            tmpf = NamedTemporaryFile(prefix='quickDump_', suffix='.html', delete=False)\n            print('Dumping r.content to: \"file://%s\"' % tmpf.name)\n            tmpf.write(r.content)\n            tmpf.close()\n            rname = tmpf.name.replace('.html', '.dat')\n            with open(rname, 'w') as tmpf:\n                print('Dumping r to: \"file://%s\"' % rname)\n                tmpf.write(vars(r))\n\n        return parsed_apps", "response": "Parses all badges pages of a steam profile and returns the appId play time and number of cards left."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndrop all info about an app from shelve", "response": "def drop_app_cache(self, appid):\n        ''' Drop all info about an app (images from filesystem and app info from shelve)\n        '''\n        with shelve.open(self.shelve_path) as appshelve:\n            try:\n                del appshelve[str(appid)]\n            except KeyError:\n                # No such key in shelve, ignore\n                pass\n        for imgtype in ('icon', 'logosmall'):\n            filename = '%d_%s.jpg' % (appid, imgtype)\n            imagepath = os.path.join(self.image_path, filename)\n            if os.path.exists(imagepath):\n                os.unlink(imagepath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_apps(self, appid_filter=None, fetch_images=True):\n        ''' Parse the badge pages, add app info (like name and icon) if needed\n            fetch and store the icons and cache app info in shelve.\n\n            Return a dict of all apps on badges page (with and without remaining drops):\n            {<appid>: <App istance>, <appid>: <App instance>, ...}\n        '''\n        appid_filter = appid_filter or []\n        apps = self.parse_badges_pages(appid_filter)\n        #apps = mockSome()\n\n        # check for appids not in shelve\n        appshelve = shelve.open(self.shelve_path)\n        appids_not_in_shelve = []\n        for appid in apps.keys():\n            if str(appid) in appshelve:\n                # Load aditional app info from shelve\n                a = appshelve[str(appid)]\n                apps[appid].name = a['name']\n            else:\n                appids_not_in_shelve.append(str(appid))\n\n        if appids_not_in_shelve:\n            # GetAppInfo only returns info for 100 apps at once\n            # TODO: Call https://api.steampowered.com/IPlayerService/GetOwnedGames/v0001/ with access_token instead?\n            # params = {'access_token': self.swb.oauth_access_token, 'appids_filter': ','.join(appids_not_in_shelve), 'steamid': swb.steamid, 'format': 'json', 'include_appinfo': 1}\n\n            appinfos = []\n            self.logger.debug('Requesting %d appids from GetAppInfo:', len(appids_not_in_shelve))\n            for appid_chunk in chunks(appids_not_in_shelve, 100):\n                params = {\n                    'access_token': self.swb.oauth_access_token,\n                    'appids': ','.join(appid_chunk)\n                }\n                self.logger.debug('Requesting a chunk of %d appids from GetAppInfo:', len(appid_chunk))\n                r = self.swb.get('https://api.steampowered.com/ISteamGameOAuth/GetAppInfo/v1/', params=params)\n                ainfo = r.json().get('apps', [])\n                appinfos.extend(ainfo)\n                self.logger.debug('GetAppInfo returned data for %d appids:', len(ainfo))\n\n            if fetch_images is True:\n                # Retrieve and store icon and logosmall\n                tasks = multiprocessing.JoinableQueue()\n                num_consumers = multiprocessing.cpu_count() * 2\n\n                consumers = []\n                for _i in range(num_consumers):\n                    c = FetchImages(tasks, self.image_path)\n                    c.start()\n                    consumers.append(c)\n\n                for appinfo in appinfos:\n                    tasks.put(appinfo)\n\n                # Add a poison pill for each consumer\n                for _i in range(num_consumers):\n                    tasks.put(None)\n\n                # Wait for all of the tasks to finish\n                tasks.join()\n\n            # Merge new data with data from shelve and store new values\n            for appinfo in appinfos:\n                appid = appinfo.get('appid')\n                name = appinfo.get('name')\n                apps[appid].name = name\n                # Store in shelve\n                data = {\n                    'name': name,\n                }\n                appshelve[str(appid)] = data\n\n        appshelve.close()\n        return apps", "response": "Get all apps on badges page"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pull_image(self, name, tag='latest'):\r\n        name = \"{0}/{1}:{2}\".format(DOCKER_NEG, name, tag)\r\n        return \"/dockerapi/v2/images\", dict(image=name)", "response": "pull the image from repository"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_container(self, name, image, hostname='dfis', networkmode='bridge', ports=None, volumes=None, env=None,\r\n                         restartpolicy='no', restartretrycount='2', command=\"\"):\r\n        \"\"\"testing\r\n\r\n        :param name:\r\n        :param image:\r\n        :param hostname:\r\n        :param networkmode: `class`:`str`, host | bridge\r\n        :param ports: `class`:`list`, [{'type':'tcp', 'publicport':8080, 'privateport':80, 'ip':'0.0.0.0}]\r\n        :param volumes: `class`:`list`, [{\"containervolume\":\"/app-conf\", \"hostvolume\":\"/opt/app/app-conf\"}]\r\n        :param env: `class`:`list`, [\"var=value\", \"var1=value1\"]\r\n        :param restartpolicy: `class`:`str`, always | on-failure | no(default)\r\n        :param restartretrycount: \u4ec5\u5f53 restartpolicy \u662f on-failure \u65f6\u624d\u6709\u7528\r\n        :param command:\r\n        :return:\r\n        \"\"\"\r\n        restartpolicy = restartpolicy.lower()\r\n        repository, image_name, version = utils.parse_image_name(image)\r\n        image = '{0}/{1}:{2}'.format(DOCKER_NEG, image_name, version)\r\n        body = dict(name=name, image=image, hostname=hostname, networkmode=networkmode,\r\n                    ports=ports or [],\r\n                    volumes=volumes or [],\r\n                    env=env or [],\r\n                    restartpolicy=restartpolicy,\r\n                    command=command)\r\n        if restartpolicy == 'on-failure':\r\n            body['restartretrycount'] = restartretrycount\r\n\r\n        return \"/dockerapi/v2/containers\", body", "response": "create a new container"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_containers_by_name(self, name):\r\n        code, containers = self.get_containers()\r\n\r\n        if code != httplib.OK:\r\n            return []\r\n\r\n        return [container for container in containers if\r\n                any(map(lambda x: x.startswith(name), container.Names))]", "response": "get all containers which start with name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_container_2(self, name):\r\n        code, container = self.get_container(name)\r\n        if code == httplib.NOT_FOUND:\r\n            return True\r\n        elif code != httplib.OK:\r\n            self.logger.error(\"Container %s on %s not exists. %d\", name, self._host, code)\r\n            return False\r\n\r\n        if container.status.Running:\r\n            code, message = self.change_container(name)\r\n            if code != httplib.OK:\r\n                self.logger.error(\"Stop container %s on %s error, status code %d, message %s\", name, self._host, code,\r\n                                  objson.dumps(message))\r\n                return False\r\n\r\n        code, message = self.delete_container(name)\r\n        if code != httplib.OK:\r\n            self.logger.error(\"Delete container %s on %s error, status code %d, message %s\", name, self._host, code,\r\n                              objson.dumps(message))\r\n            return False\r\n        return True", "response": "Delete a container and return True if delete success otherwise return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_image(self, container_name, image_name):\r\n        code, container = self.get_container(container_name)\r\n        if code != httplib.OK:\r\n            self.logger.error(\"Container %s is not exists. error code %s, error message %s\", container_name, code,\r\n                              container)\r\n            return False\r\n\r\n        _, old_image_name, _ = utils.parse_image_name(container.image)\r\n        repository, name, version = utils.parse_image_name(image_name)\r\n        if not repository or repository.lower() != DOCKER_NEG:\r\n            self.logger.error(\"You image %s must have a 'docker.neg/' prefix string\", image_name)\r\n            return False\r\n\r\n        if not repo.image_exists(name, tag=version):\r\n            self.logger.error(\"You image %s must be location in docker.neg repository.\", image_name)\r\n            return False\r\n\r\n        if old_image_name.lower() != name.lower():\r\n            self.logger.error(\"You image %s must be same with container's Image.\", image_name, container.image)\r\n            return False\r\n\r\n        code, result = self.update(container_name, tag=version)\r\n        if code != httplib.OK:\r\n            self.logger.error(\"Update container %s with image failure, code %s, result %s\", container_name, code,\r\n                              result)\r\n            return False\r\n\r\n        return True", "response": "Update a container s image"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_image_2(self, container_name, image_name):\r\n        code, container = self.get_container(container_name)\r\n        if code == httplib.NOT_FOUND:\r\n            raise ContainerNotFound(container_name)\r\n        elif code != httplib.OK:\r\n            raise GeneralError(code)\r\n\r\n        _, old_image_name, _ = utils.parse_image_name(container.image)\r\n        repository, name, version = utils.parse_image_name(image_name)\r\n        if not repository or repository.lower() != DOCKER_NEG:\r\n            image_name = '{0}/{1}:{2}'.format(DOCKER_NEG, name, version)\r\n\r\n        if not repo.image_exists(name, tag=version):\r\n            raise ImageNotFound(\"{0} do not location in docker.neg repository.\".format(image_name))\r\n\r\n        if old_image_name.lower() != name.lower():\r\n            raise ImageConflict(\"{0} is not be same with container's Image.\".format(image_name))\r\n\r\n        code, result = self.pull_image(name, version)\r\n        if code != httplib.OK:\r\n            raise GeneralError(\r\n                'pull image {0}:{1} failure, status code {2}, result: {3}'.format(name, version, code, result))\r\n\r\n        code, result = self.update(container_name, tag=version)\r\n        if code != httplib.OK:\r\n            raise GeneralError(\r\n                'Update container {0} failure, status code {1}, result: {2}'.format(container_name, code, result))\r\n\r\n        return True", "response": "update a container s image"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def send(from_addr, to_addrs, subject=\"Ellis\", msg=\"\", **kwargs):\n    async with SMTP() as client:\n        msg = \"Subject: {0}\\n\\n{1}\".format(subject, msg)\n\n        if kwargs:\n            # To append kwargs to the given message, we first\n            # transform it into a more human friendly string:\n            values = \"\\n\".join([\"{0}: {1}\".format(k, v)\n                                for k, v\n                                in kwargs.items()])\n\n            # Actually append caught values to the message:\n            msg = (\"{0}\\n\\nThe following variables have been caught:\"\n                   \"\\n{1}\".format(msg, values))\n\n        try:\n            await client.sendmail(from_addr, to_addrs, msg)\n        except:\n            # FIXME: print a friendly message to stdout.\n            raise", "response": "Sends an e - mail to the provided addresses."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_correlation_matrix(self, correlation_matrix):\n        cr_plot.create_correlation_matrix_plot(\n            correlation_matrix, self.title, self.headers_to_test\n        )\n        pyplot.show()", "response": "Shows the given correlation matrix as image\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_correlation_matrix_from_columns(self):\n        header_to_column = {}  # create index of headers\n        for header in self.headers:\n            header_to_column[header] = self.headers.index(header)\n\n        data_to_test = []\n        for header in self.headers_to_test:\n            header_column = Matrix(self.data) \\\n                .get_column(header_to_column[header])\n\n            for i, value in enumerate(header_column):\n                header_column[i] = float(value)  # get float\n\n            data_to_test.append(header_column)\n\n        return self.get_correlation_matrix(data_to_test)", "response": "Computes the correlation matrix of columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_to_file(self, out_file):\n        correlation_matrix = self.get_correlation_matrix_from_columns()\n        cr_plot.create_correlation_matrix_plot(\n            correlation_matrix, self.title, self.headers_to_test)\n\n        fig = pyplot.gcf()  # get reference to figure\n        fig.set_size_inches(23.4, 23.4)\n        pyplot.savefig(out_file, dpi=120)", "response": "Saves correlation matrix of selected headers to file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_correlation_matrix_from_folder(folder_path):\n        file_name = \"output-\" + str(int(time.time()))\n        output_folder = os.path.join(folder_path, file_name)\n        os.makedirs(output_folder)  # make necessary folders to create directory\n\n        for file in list_content(folder_path, False, False):\n            if is_file(file) and str(file).endswith(\"csv\"):\n                print(\"Analysing file \", str(file))\n\n                file_name = Document(file).name.strip()\n                output_file_name = file_name + \".png\"  # save output as image\n                output_file_path = os.path.join(output_folder, output_file_name)\n                headers, data = CSVParser.get_headers_data(file)  # parse\n                matrix = CorrelationMatrix(\n                    \"Correlation of logs data for file \" + file_name,\n                    headers,\n                    headers,\n                    data\n                )\n                matrix.save_to_file(output_file_path)", "response": "Saves each file s correlation matrix of common headers and data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, *args, **kwargs):\n        pm = MayaPluginManager.get()\n        guerilla =  pm.get_plugin(\"GuerillaMGMT\")\n        mayawin = maya_main_window()\n        guerilla.run(parent=mayawin)", "response": "Start the tool\n\n        :returns: None\n        :rtype: None\n        :raises: None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog into GitHub using an existing token.", "response": "def login_github(token_path=None, token=None):\n    \"\"\"Log into GitHub using an existing token.\n\n    Parameters\n    ----------\n    token_path : str, optional\n        Path to the token file. The default token is used otherwise.\n\n    token: str, optional\n        Literal token string. If specified, this value is used instead of\n        reading from the token_path file.\n\n    Returns\n    -------\n    gh : :class:`github.GitHub` instance\n        A GitHub login instance.\n    \"\"\"\n\n    token = codetools.github_token(token_path=token_path, token=token)\n    g = Github(token)\n    debug_ratelimit(g)\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind a tag in a github Repository and return it.", "response": "def find_tag_by_name(repo, tag_name, safe=True):\n    \"\"\"Find tag by name in a github Repository\n\n    Parameters\n    ----------\n    repo: :class:`github.Repository` instance\n\n    tag_name: str\n        Short name of tag (not a fully qualified ref).\n\n    safe: bool, optional\n        Defaults to `True`. When `True`, `None` is returned on failure. When\n        `False`, an exception will be raised upon failure.\n\n    Returns\n    -------\n    gh : :class:`github.GitRef` instance or `None`\n\n    Raises\n    ------\n    github.UnknownObjectException\n        If git tag name does not exist in repo.\n    \"\"\"\n    tagfmt = 'tags/{ref}'.format(ref=tag_name)\n\n    try:\n        ref = repo.get_git_ref(tagfmt)\n        if ref and ref.ref:\n            return ref\n    except github.UnknownObjectException:\n        if not safe:\n            raise\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds teams in org by name.", "response": "def get_teams_by_name(org, team_names):\n    \"\"\"Find team(s) in org by name(s).\n\n    Parameters\n    ----------\n    org: github.Organization.Organization\n        org to search for team(s)\n\n    teams: list(str)\n        list of team names to search for\n\n    Returns\n    -------\n    list of github.Team.Team objects\n\n    Raises\n    ------\n    github.GithubException\n        Upon error from github api\n    \"\"\"\n    assert isinstance(org, github.Organization.Organization), type(org)\n\n    try:\n        org_teams = list(org.get_teams())\n    except github.RateLimitExceededException:\n        raise\n    except github.GithubException as e:\n        msg = 'error getting teams'\n        raise CaughtOrganizationError(org, e, msg) from None\n\n    found_teams = []\n    for name in team_names:\n        debug(\"looking for team: {o}/'{t}'\".format(\n            o=org.login,\n            t=name\n        ))\n\n        t = next((t for t in org_teams if t.name == name), None)\n        if t:\n            debug('  found')\n            found_teams.append(t)\n        else:\n            debug('  not found')\n\n    return found_teams"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs debug of github ratelimit information from last API call", "response": "def debug_ratelimit(g):\n    \"\"\"Log debug of github ratelimit information from last API call\n\n    Parameters\n    ----------\n    org: github.MainClass.Github\n        github object\n    \"\"\"\n    assert isinstance(g, github.MainClass.Github), type(g)\n\n    debug(\"github ratelimit: {rl}\".format(rl=g.rate_limiting))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if the teams in allow_teams and deny_teams are in the list of teams in team_names. Raises a RepositoryTeamMembershipError if the team names in team_names are not in allow_teams or deny_teams are not in the list of teams in repo.", "response": "def check_repo_teams(repo, allow_teams, deny_teams, team_names=None):\n    \"\"\"Check if repo teams match allow/deny lists\n\n    Parameters\n    ----------\n    repo: github.Repository.Repository\n        repo to check for membership\n\n    allow_teams: list(str)\n        list of team names that repo MUST belong to at least one of.\n\n    deny_teams: list(str)\n        list of team that repo MUST NOT be a member of.\n\n    team_names: list(str)\n        list of the team name which the repo is a member of (optional).\n        Providing this list saves retrieving the list of teams from the github\n        API.\n\n    Raises\n    ------\n    RepositoryTeamMembershipError\n        Upon permission error\n    \"\"\"\n    assert isinstance(repo, github.Repository.Repository), type(repo)\n\n    # fetch team names if a list was not passed\n    if not team_names:\n        try:\n            team_names = [t.name for t in repo.get_teams()]\n        except github.RateLimitExceededException:\n            raise\n        except github.GithubException as e:\n            msg = 'error getting teams'\n            raise CaughtRepositoryError(repo, e, msg) from None\n\n    if not any(x in team_names for x in allow_teams)\\\n       or any(x in team_names for x in deny_teams):\n        raise RepositoryTeamMembershipError(\n            repo,\n            team_names,\n            allow_teams=allow_teams,\n            deny_teams=deny_teams\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_ref(repo):\n    assert isinstance(repo, github.Repository.Repository), type(repo)\n\n    # XXX this probably should be resolved via repos.yaml\n    default_branch = repo.default_branch\n    default_branch_ref = \"heads/{ref}\".format(ref=default_branch)\n\n    # if accessing the default branch fails something is seriously wrong...\n    try:\n        head = repo.get_git_ref(default_branch_ref)\n    except github.RateLimitExceededException:\n        raise\n    except github.GithubException as e:\n        msg = \"error getting ref: {ref}\".format(ref=default_branch_ref)\n        raise CaughtRepositoryError(repo, e, msg) from None\n\n    return head", "response": "Get the HEAD of the default branch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(argv=None):\n    '''this is called if run from command line'''\n\n    t = CrfTokenizer()\n    print t.tokenize(\"This is a sentence.\")\n    print t.tokenize(\"Buy???This...Now!!!\")\n    print t.tokenize(\"The <bold>only</bold> source.\")\n    print t.tokenize(\"The<bold>only</bold>source.\")\n    print t.tokenize(\"Big&gt;little.\")\n    print t.tokenize(\"Big & little.\")\n    print t.tokenize(\"blond&curly.\")\n    print t.tokenize(\"&brokenHtml\")\n    t.setGroupPunctuation(True)\n    t.setRecognizeHtmlTags(True)\n    t.setRecognizeHtmlEntities(True)\n    print t.tokenize(\"Buy???This...Now!!!\")\n    print t.tokenize(\"The <bold>only</bold> source.\")\n    print t.tokenize(\"The<bold>only</bold>source.\")\n    print t.tokenize(\"Big&gt;little.\")\n    print t.tokenize(\"Big & little.\")\n    print t.tokenize(\"blond&curly.\")\n    print t.tokenize(\"&brokenHtml\")\n    t.setSkipHtmlTags(True)\n    t.setSkipHtmlEntities(True)\n    print t.tokenize(\"Buy???This...Now!!!\")\n    print t.tokenize(\"The <bold>only</bold> source.\")\n    print t.tokenize(\"The<bold>only</bold>source.\")\n    print t.tokenize(\"Big&gt;little.\")\n    print t.tokenize(\"Big & little.\")\n    print t.tokenize(\"blond&curly.\")\n    print t.tokenize(\"&brokenHtml\")\n    t.setTokenPrefix(\"X:\")\n    print t.tokenize(\"Tokenize with prefixes.\")\n    t.setTokenPrefix(None)\n    print t.tokenize(\"No more  prefixes.\")\n    t.setRecognizePunctuation(False)\n    print t.tokenize(\"This is a sentence.\")\n    print t.tokenize(\"Buy???This...Now!!!\")\n    print t.tokenize(\"The <bold>only</bold> source.\")\n    print t.tokenize(\"The<bold>only</bold>source.\")\n    print t.tokenize(\"Big&gt;little.\")\n    print t.tokenize(\"Big & little.\")\n    print t.tokenize(\"blond&curly.\")\n    print t.tokenize(\"&brokenHtml\")\n    print t.tokenize(\"A line break goes here\\n\\t \\rand a new line starts\")\n    t.setRecognizeLinebreaks(True)\n    print t.tokenize(\"A line break goes here\\n\\r \\rand a new line starts\")", "response": "This is called if run from command line. It will parse the input file and then parse the output file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokenize (self, value):\n\n        # This code uses a state machine:\n        class STATE:\n            NORMAL = 0\n            GROUP_PUNCTUATION = 1\n            PROCESS_HTML_TAG = 2\n            PROCESS_HTML_ENTITY = 3\n            GROUP_LINEBREAKS = 4\n\n        state_names = {\n            STATE.NORMAL: \"normal\",\n            STATE.GROUP_PUNCTUATION: \"punctuation\",\n            STATE.PROCESS_HTML_TAG: \"html\",\n            STATE.PROCESS_HTML_ENTITY: \"html_entity\",\n            STATE.GROUP_LINEBREAKS: \"break\"\n        }\n\n        # \"state\" and \"token\" have array values to allow their\n        # contents to be modified within finishToken().\n        state = [STATE.NORMAL]\n        token = [\"\"] # The current token being assembled.\n        tokens = [] # The tokens extracted from the input.\n        index = -1\n\n        def clearToken():\n            \"\"\"Clear the current token and return to normal state.\"\"\"\n            token[0] = \"\"\n            state[0] = STATE.NORMAL\n\n        def emitToken():\n            \"\"\"Emit the current token, if any, and return to normal state.\"\"\"\n            if len(token[0]) > 0:\n                # add character end and start\n                char_start, char_end = index, index + len(token[0])\n                if self.create_structured_tokens:\n                    new_token = {'value': token[0], 'type': state_names[state[0]], 'char_start': char_start, 'char_end': char_end}\n                    tokens.append(new_token)\n                else:\n                    tokens.append(token[0])\n            clearToken()\n\n        def fixBrokenHtmlEntity():\n            # This is not a valid HTML entity.\n            # TODO: embedded \"#\" characters should be treated better\n            # here.\n            if not self.recognizePunctuation:\n                # If we aren't treating punctuation specially, then just treat\n                # the broken HTML entity as an ordinary token.\n                #\n                # TODO: This is not quite correct.  \"x& \" should\n                # be treated as a single token, althouth \"s & \"\n                # should result in two tokens.\n                state[0] = STATE.NORMAL\n                return\n            if self.groupPunctuation:\n                # If all the saved tokens are punctuation characters, then\n                # enter STATE.GROUP_PUNCTUATION insted of STATE.NORMAL.\n                sawOnlyPunctuation = True\n                for c in token[0]:\n                    if c not in CrfTokenizer.punctuationSet:\n                        sawOnlyPunctuation = False\n                        break\n                if sawOnlyPunctuation:\n                    state[0] = STATE.GROUP_PUNCTUATION\n                    return\n\n            # Emit the ampersand that began the prospective entity and use the\n            # rest as a new current token.\n            saveToken = token[0]\n            token[0] = saveToken[0:1]\n            emitToken()\n            if len(saveToken) > 1:\n                token[0] = saveToken[1:]\n            # The caller should continue processing with the current\n            # character.\n\n        # Process each character in the input string:\n        for c in value:\n            index += 1\n            if state[0] == STATE.PROCESS_HTML_TAG:\n                if c in CrfTokenizer.whitespaceSet:\n                    continue # Suppress for safety. CRF++ doesn't like spaces in tokens, for example.\n                token[0] += c\n                if c == CrfTokenizer.END_HTML_TAG_CHAR:\n                    if self.skipHtmlTags:\n                        clearToken()\n                    else:\n                        emitToken()\n                continue\n\n            if state[0] == STATE.PROCESS_HTML_ENTITY:\n                # Parse an HTML entity name. TODO: embedded \"#\"\n                # characters imply more extensive parsing rules should\n                # be performed here.\n                if c == CrfTokenizer.END_HTML_ENTITY_CHAR:\n                    if len(token[0]) == 1:\n                        # This is the special case of \"&;\", which is not a\n                        # valid HTML entity.  If self.groupPunctuation is\n                        # True, return to normal parsing state in case more\n                        # punctuation follows.  Otherwise, emit \"&\" and \";\" as\n                        # separate tokens.\n                        if not self.recognizePunctuation:\n                            # TODO: This is not quite correct.  \"x&;\" should\n                            # be treated as a single token, althouth \"s &;\"\n                            # should result in two tokens.\n                            token[0] = token[0] + c\n                            state[0] = STATE.NORMAL\n                        elif self.groupPunctuation:\n                            token[0] = token[0] + c\n                            state[0] = STATE.GROUP_PUNCTUATION\n                        else:\n                            emitToken() # Emit the \"&\" as a seperate token.\n                            token[0] = token[0] + c\n                            emitToken() # Emit the \";' as a seperate token.\n                        continue\n                    token[0] = token[0] + c\n                    if self.skipHtmlEntities:\n                        clearToken()\n                    else:\n                        emitToken()\n                    continue\n                elif c in CrfTokenizer.htmlEntityNameCharacterSet:\n                    token[0] = token[0] + c\n                    continue\n                else:\n                    # This is not a valid HTML entity.\n                    fixBrokenHtmlEntity()\n                    # intentional fall-through\n\n            if state[0] == STATE.GROUP_LINEBREAKS:\n                # we will look for \\n\\r and ignore spaces\n                if c in CrfTokenizer.linebreaking_character_set:\n                    token[0] += c\n                    continue\n                elif c in CrfTokenizer.whitespaceSet:\n                    continue\n                else:\n                    emitToken()\n                    state[0] = STATE.NORMAL\n\n            if c in CrfTokenizer.whitespaceSet:\n                # White space terminates the current token, then is dropped.\n                emitToken()\n                # Check to see whether we should look for line breaks\n                if c in CrfTokenizer.linebreaking_start_character_set and self.recognize_linebreaks:\n                    state[0] = STATE.GROUP_LINEBREAKS\n                    token[0] = c\n\n            elif c == CrfTokenizer.START_HTML_TAG_CHAR and self.recognizeHtmlTags:\n                emitToken()\n                state[0] = STATE.PROCESS_HTML_TAG\n                token[0] = c\n\n            elif c == CrfTokenizer.START_HTML_ENTITY_CHAR and self.recognizeHtmlEntities:\n                emitToken()\n                state[0] = STATE.PROCESS_HTML_ENTITY\n                token[0] = c\n\n            elif c in CrfTokenizer.punctuationSet and self.recognizePunctuation:\n                if self.groupPunctuation:\n                    # Finish any current token.  Concatenate\n                    # contiguous punctuation into a single token:\n                    if state[0] != STATE.GROUP_PUNCTUATION:\n                        emitToken()\n                        state[0] = STATE.GROUP_PUNCTUATION\n                    token[0] = token[0] + c\n                else:\n                    # Finish any current token and form a token from\n                    # the punctuation character:\n                    emitToken()\n                    token[0] = c\n                    emitToken()\n\n            else:\n                # Everything else goes here. Presumably, that includes\n                # Unicode characters that aren't ASCII\n                # strings. Further work is needed.\n                if state[0] != STATE.NORMAL:\n                    emitToken()\n                token[0] = token[0] + c\n\n        # Finish any final token and return the array of tokens:\n        if state[0] == STATE.PROCESS_HTML_ENTITY:\n            fixBrokenHtmlEntity()\n        emitToken()\n\n        # Was a token prefix requested? If so, we'll apply it now.  If the\n        # normal case is not to apply a token prefix, this might be a little\n        # more efficient than applying the prefix in emitToken().\n        if self.tokenPrefix is not None and len(self.tokenPrefix) > 0:\n            tokens = map(lambda x: self.tokenPrefix + x, tokens)\n\n        return tokens", "response": "Take a string and break it into tokens. Return the tokens as a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking all inputs and stimuli are valid and consistent.", "response": "def verifyInputs(self, mode):\n        \"\"\"Goes through and checks all stimuli and input settings are valid\n        and consistent. Prompts user with a message if there is a condition\n        that would prevent acquisition.\n\n        :param mode: The mode of acquisition trying to be run. Options are\n            'chart', or anthing else ('explore', 'protocol', 'calibration')\n        :type mode: str\n        :returns: bool -- Whether all inputs and stimuli are valid\n        \"\"\"\n        if len(self._aichans) < 1:\n            failmsg = \"Must have at least one input channel selected\"\n            QtGui.QMessageBox.warning(self, \"Invalid Setting\", failmsg)\n            return False\n        if mode == 'chart':\n            if self.ui.aifsSpnbx.value()*self.fscale > 100000:\n                QtGui.QMessageBox.warning(self, \"Invalid Input\", \"Recording samplerate cannot exceed 100kHz for chart acquisition\")\n                return False\n        elif mode is not None:\n            # if (1./self.ui.reprateSpnbx.value()) < self.ui.windowszSpnbx.value()*self.tscale + 0.05:\n            #     QtGui.QMessageBox.warning(self, \"Invalid Input\", \"A minimum of 50ms time between repetitions required. Current interval {}, required {}\".format((1./self.ui.reprateSpnbx.value()), self.ui.windowszSpnbx.value()*self.tscale + 0.05))\n            #     return False\n            if self.ui.tabGroup.currentWidget().objectName() == 'tabExplore':\n                # each widget should be in charge of putting its own stimulus together\n                self.ui.exploreStimEditor.saveToObject()\n                failmsg = self.ui.exploreStimEditor.verify(self.ui.windowszSpnbx.value())\n                if failmsg:\n                    QtGui.QMessageBox.warning(self, \"Invalid Input\", failmsg)\n                    return False\n                # if selectedStim.intensity() > self.calvals['caldb']:\n                #     QtGui.QMessageBox.warning(self, \"Invalid Input\",\n                #             \"Intensity must be below calibrated maximum {}dB SPL\".format(self.calvals['caldb']))\n                #     return False\n            elif self.ui.tabGroup.currentWidget().objectName() == 'tabProtocol':\n                protocol_model = self.acqmodel.protocol_model()\n                # protocol delegates to each test to verify itself and report\n                failure = protocol_model.verify(float(self.ui.windowszSpnbx.value()))\n                if failure:\n                    QtGui.QMessageBox.warning(self, \"Invalid Input\", failure)\n                    return False\n            elif self.ui.tabGroup.currentWidget().objectName() == 'tabCalibrate':\n                if len(self._aichans) > 1:\n                    failmsg = \"Speaker calibration only supported for single channel, currently {} channels selected; select 1 input channel.\".format(len(self._aichans))\n                    QtGui.QMessageBox.warning(self, \"Invalid Setting\", failmsg)\n                    return False\n                # get what stimulus is about to be presented\n                if self.ui.calibrationWidget.ui.savecalCkbx.isChecked() or not self.ui.calibrationWidget.currentSelection() == 'Tone Curve':\n                    calibration_stimulus = self.acqmodel.calibration_stimulus('noise')\n                    self.ui.calibrationWidget.saveToObject()\n                else:\n                    calibration_stimulus = self.acqmodel.calibration_stimulus('tone')\n\n                failmsg = calibration_stimulus.verify(float(self.ui.windowszSpnbx.value()))\n                if failmsg:\n                    QtGui.QMessageBox.warning(self, \"Invalid Input\", failmsg)\n                    return False\n                # also check that the recording samplerate is high enough in this case\n                failmsg = calibration_stimulus.verifyExpanded(samplerate=self.ui.aifsSpnbx.value())\n                if failmsg:\n                    failmsg = failmsg.replace('Generation', 'Recording')\n                    QtGui.QMessageBox.warning(self, \"Invalid Input\", failmsg)\n                    return False\n            if self.advanced_options['use_attenuator'] and not self.acqmodel.attenuator_connection():\n                failmsg = \"Error Connection to attenuator, make sure it it turned on and connected, and try again\"\n                QtGui.QMessageBox.warning(self, \"Connection Error\", failmsg)\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset_device_channels(self):\n        # clear boxes first\n        self.ui.aochanBox.clear()\n        devname = self.advanced_options['device_name']\n        device_list = get_devices()\n        if devname in device_list:\n            cnames = get_ao_chans(devname)\n            self.ui.aochanBox.addItems(cnames)\n            cnames = get_ai_chans(devname)\n            # filter list for channels that are present in current device\n            self._aichans = [chan for chan in self._aichans if chan in cnames]\n            self._aichan_details = {chan: deets for chan, deets in self._aichan_details.items() if chan in cnames}\n        elif devname == '' and len(device_list) > 0:\n            devname = device_list[0]\n            cnames = get_ao_chans(devname)\n            self.ui.aochanBox.addItems(cnames)\n            self.advanced_options['device_name'] = devname\n            self._aichans = []\n            self._aichan_details = {}\n        else:\n            self._aichans = []\n            self._aichan_details = {}\n\n        self.ui.chanNumLbl.setText(str(len(self._aichans)))\n        # remove all plots and re-add from new list\n        self.display.removeResponsePlot(*self.display.responseNameList())\n        self.display.addResponsePlot(*self._aichans)\n        # update details on plots\n        for name, deets in self._aichan_details.items():\n            self.display.setThreshold(deets['threshold'], name)\n            self.display.setRasterBounds(deets['raster_bounds'], name)\n            self.display.setAbs(deets['abs'], name)\n\n        # can't find a function in DAQmx that gets the trigger\n        # channel names, so add manually\n        self.ui.trigchanBox.addItems(['/'+devname+'/PFI0', '/'+devname+'/PFI1'])", "response": "Updates the input channel selection boxes based on the current device name stored in this object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the values in the input fields so they can be loaded next time the GUI is run.", "response": "def saveInputs(self, fname):\n        \"\"\"Save the values in the input fields so they can be loaded\n        next time the GUI is run\n\n        :param fname: file path of location to store values at\n        :type fname: str\n        \"\"\"\n        # save current inputs to file for loading next time\n        if not fname:\n            return\n            \n        appdir = systools.get_appdir()\n        if not os.path.isdir(appdir):\n            os.makedirs(appdir)\n        fname = os.path.join(appdir, fname)\n\n        savedict = {}\n        savedict['binsz'] = self.ui.binszSpnbx.value()\n        savedict['aifs'] = self.ui.aifsSpnbx.value()\n        savedict['tscale'] = self.tscale\n        savedict['fscale'] = self.fscale\n        savedict['saveformat'] = self.saveformat\n        savedict['ex_nreps'] = self.ui.exploreStimEditor.repCount()\n        savedict['reprate'] = self.ui.reprateSpnbx.value()\n        savedict['windowsz'] = self.ui.windowszSpnbx.value()\n        savedict['specargs'] = self.specArgs\n        savedict['viewSettings'] = self.viewSettings\n        savedict['calvals'] = self.calvals\n        savedict['calparams'] = self.acqmodel.calibration_template()\n        savedict['calreps'] = self.ui.calibrationWidget.ui.nrepsSpnbx.value()\n        savedict['mphonesens'] = self.ui.mphoneSensSpnbx.value()\n        savedict['mphonedb'] = self.ui.mphoneDBSpnbx.value()\n        savedict['vocalpaths'] = Vocalization.paths\n        savedict['aichans'] = self._aichans\n        savedict['aichan_details'] = self._aichan_details\n\n        # parameter settings -- save all tracks present\n        savedict['explorestims'] = self.ui.exploreStimEditor.saveTemplate()\n\n        savedict['advanced_options'] = self.advanced_options\n        savedict['stim_view_defaults'] = StimulusView.getDefaults()\n\n        savedict['tuning_curve'] = TCFactory.defaultInputs\n\n        # filter out and non-native python types that are not json serializable\n        savedict = convert2native(savedict)\n        try:\n            with open(fname, 'w') as jf:\n                json.dump(savedict, jf)\n        except:\n            logger = logging.getLogger('main')\n            logger.exception(\"Unable to save app data to file: {}\".format(fname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loadInputs(self, fname):\n        inputsfname = os.path.join(systools.get_appdir(), fname)\n        try:\n            with open(inputsfname, 'r') as jf:\n                inputsdict = json.load(jf)\n        except:\n            logger = logging.getLogger('main')\n            logger.warning(\"Unable to load app data from file: {}\".format(inputsfname))\n            inputsdict = {}\n\n        # self.display.spiketracePlot.setThreshold(inputsdict.get('threshold', 0.5))\n        self._thesholds = inputsdict.get('threshold', {})\n        self.stashedAisr = inputsdict.get('aifs', 100000)\n        self.ui.aifsSpnbx.setValue(self.stashedAisr)\n        self.ui.windowszSpnbx.setValue(inputsdict.get('windowsz', 0.1))\n        self.ui.binszSpnbx.setValue(inputsdict.get('binsz', 0.005))        \n        self.saveformat = inputsdict.get('saveformat', 'hdf5')\n        self.ui.exploreStimEditor.setReps((inputsdict.get('ex_nreps', 5)))\n        self.ui.reprateSpnbx.setValue(inputsdict.get('reprate', 1))\n        # self.display.spiketracePlot.setRasterBounds(inputsdict.get('raster_bounds', (0.5,1)))\n        self.specArgs = inputsdict.get('specargs',{u'nfft':512, u'window':u'hanning', u'overlap':90, 'colormap':{'lut':None, 'state':None, 'levels':None}})\n        # self.display.setSpecArgs(**self.specArgs)  \n        SpecWidget.setSpecArgs(**self.specArgs)\n        self.viewSettings = inputsdict.get('viewSettings', {'fontsz': 10, 'display_attributes':{}})\n        self.ui.stimDetails.setDisplayAttributes(self.viewSettings['display_attributes'])\n        font = QtGui.QFont()\n        font.setPointSize(self.viewSettings['fontsz'])\n        QtGui.QApplication.setFont(font)\n        self.ui.calibrationWidget.ui.nrepsSpnbx.setValue(inputsdict.get('calreps', 5))\n        self.calvals = inputsdict.get('calvals', {'calf':20000, 'caldb':100, \n                                      'calv':0.1, 'use_calfile':False, \n                                      'frange':(5000, 1e5), 'calname': ''})\n        self.calvals['use_calfile'] = False\n        self.calvals['calname'] = ''\n        self.ui.refDbSpnbx.setValue(self.calvals['caldb'])\n        self.ui.mphoneSensSpnbx.setValue(inputsdict.get('mphonesens', 0.004))\n        self.ui.mphoneDBSpnbx.setValue(MPHONE_CALDB)\n        # self.ui.mphoneDBSpnbx.setValue(inputsdict.get('mphonedb', 94))\n        Vocalization.paths = inputsdict.get('vocalpaths', [])\n\n        # load the previous sessions scaling\n        self.tscale = inputsdict.get('tscale', SmartSpinBox.MilliSeconds)\n        self.fscale = inputsdict.get('fscale', SmartSpinBox.kHz)\n        try:\n            self.updateUnitLabels(self.tscale, self.fscale)\n        except:\n            self.tscale = 'ms'\n            self.fscale = 'kHz'\n            self.updateUnitLabels(self.tscale, self.fscale)\n\n        cal_template = inputsdict.get('calparams', None)\n        if cal_template is not None:\n            try:\n                self.acqmodel.load_calibration_template(cal_template)\n            except:\n                logger = logging.getLogger('main')\n                logger.exception(\"Unable to load previous calibration settings\")\n        else:\n            logger = logging.getLogger('main')\n            logger.debug('No saved calibration stimului inputs')\n\n        if 'explorestims' in inputsdict:\n            self.ui.exploreStimEditor.loadTemplate(inputsdict['explorestims'])\n        else:\n            logger = logging.getLogger('main')\n            logger.debug('No saved explore stimului inputs')\n\n        # load the previous session's Tuning Curve defaults\n        TCFactory.defaultInputs.update(inputsdict.get('tuning_curve', TCFactory.defaultInputs))\n\n        # set defaults then merge\n        self.advanced_options = {'device_name':'', \n                                 'max_voltage':1.5,\n                                 'device_max_voltage': 10.0,\n                                 'volt_amp_conversion': 0.1,\n                                 'use_attenuator': False }\n        if 'advanced_options' in inputsdict:\n            self.advanced_options.update(inputsdict['advanced_options'])\n        StimulusModel.setMaxVoltage(self.advanced_options['max_voltage'], self.advanced_options['device_max_voltage'])\n        self.display.setAmpConversionFactor(self.advanced_options['volt_amp_conversion'])\n        if self.advanced_options['use_attenuator']:\n            self.acqmodel.attenuator_connection(True)\n        else:\n            self.acqmodel.attenuator_connection(False)\n        self._aichans = inputsdict.get('aichans', [])\n        self._aichan_details = inputsdict.get('aichan_details', {})\n        for name, deets in self._aichan_details.items():\n            # make sure all field as present in details for each channel\n            self._aichan_details[name]['threshold'] = deets.get('threshold', 5)\n            self._aichan_details[name]['polarity'] = deets.get('polarity', 1)\n            self._aichan_details[name]['raster_bounds'] = deets.get('raster_bounds', (0.5,0.9))\n            self._aichan_details[name]['abs'] = deets.get('abs', True)\n\n        self.reset_device_channels()\n\n        stim_defaults = inputsdict.get('stim_view_defaults', {})\n        for name, state in stim_defaults.items():\n            StimulusView.updateDefaults(name, state)", "response": "Load previsouly saved input values and load them to GUI widgets"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef closeEvent(self, event):\n        self.acqmodel.stop_listening() # close listener threads\n        self.saveInputs(self.inputsFilename)\n\n        # save GUI size\n        settings = QtCore.QSettings(\"audiolab\")\n        settings.setValue(\"geometry\", self.saveGeometry())\n        settings.setValue(\"windowState\", self.saveState())\n        logger = logging.getLogger('main')\n        logger.info('All user settings saved')\n\n        self.garbage_timer.stop()\n        gc.enable()", "response": "Closes listening threads and saves GUI data for later use."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ordered_async_call(func_list):\n\n    def worker(function, f_args, f_kwargs, queue, index):\n        \"\"\"\n        Runs the function and appends the output to list, and the Exception in the case of error\n        \"\"\"\n        response = {\n            'index': index,  # For tracking the index of each function in actual list.\n            # Since, this function is called asynchronously, order in\n            # queue may differ\n            'data': None,\n            'error': None\n        }\n\n        # Handle error in the function call\n        try:\n            response['data'] = function(*f_args, **f_kwargs)\n        except Exception as e:\n            response['error'] = e  # send back the exception along with the queue\n\n        queue.put(response)\n\n    queue = Queue()   # For preserving state across threads\n    processes = [Process(target=worker, args=(func, args, kwargs, queue, i)) \\\n                 for i, (func, args, kwargs) in enumerate(func_list)]\n\n    for process in processes:\n        process.start()\n\n    response_list = []\n    for process in processes:\n        # Wait for process to finish\n        process.join()\n\n        # Get back the response from the queue\n        response = queue.get()\n        if response['error']:\n            raise response['error']  # Raise exception if the function call failed\n        response_list.append(response)\n\n    return [content['data'] for content in sorted(response_list, key=lambda x: x['index'])]", "response": "Runs the list of functions asynchronously and returns the response maintaining the order\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds params to url", "response": "def add_params_to_url(url, params):\n    \"\"\"Adds params to url\n\n    :param url: Url\n    :param params: Params to add\n    :return: original url with new params\n    \"\"\"\n    url_parts = list(urlparse.urlparse(url))  # get url parts\n    query = dict(urlparse.parse_qsl(url_parts[4]))  # get url query\n    query.update(params)  # add new params\n    url_parts[4] = urlencode(query)\n    return urlparse.urlunparse(url_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_internet_on(host=\"8.8.8.8\", port=53, timeout=3):\n    socket.setdefaulttimeout(timeout)\n    socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect((host, port))", "response": "Checks if the machine is internet on"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_until_internet(time_between_attempts=3, max_attempts=10):\n    counter = 0\n    while not is_internet_on():\n        time.sleep(time_between_attempts)  # wait until internet is on\n        counter += 1\n\n        if counter > max_attempts:\n            return False\n\n    return True", "response": "Waits until the machine has internet connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nderives the correct master key from the seed_key seed_rand and num rounds.", "response": "def derive_key(seed_key, seed_rand, rounds, password=None, keyfile=None):\n    \"\"\"\n    Derives the correct (final) master key from the password and/or keyfile and\n    sepcified transform seed & num rounds.\n    \"\"\"\n    if password == '': password = None\n    if keyfile == '': keyfile = None\n    if password is None and keyfile is None:\n        raise ValueError(\"Password and/or keyfile is required.\")\n        \n    if password is None:\n        masterkey = key_from_keyfile(keyfile)\n    elif password and keyfile:\n        passwordkey = key_from_password(password)\n        filekey = key_from_keyfile(keyfile)\n        sha = SHA256.new()\n        sha.update(passwordkey + filekey)\n        masterkey = sha.digest()\n    else:\n        masterkey = key_from_password(password)\n\n    # Create the key that is needed to...\n    final_key = transform_key(masterkey, seed_key=seed_key, seed_rand=seed_rand, rounds=rounds)\n    \n    return final_key"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transform_key(startkey, seed_key, seed_rand, rounds):\n    masterkey = startkey\n    aes = AES.new(seed_key, AES.MODE_ECB)\n\n    # Encrypt the created hash <rounds> times\n    for _i in range(rounds):\n        masterkey = aes.encrypt(masterkey)\n\n    # Finally, hash it again...\n    masterkey = hashlib.sha256(masterkey).digest()\n    # ...and hash the result together with the randomseed\n    return hashlib.sha256(seed_rand + masterkey).digest()", "response": "This method creates the key to decrypt the database and then hash it with seed_rand."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encrypt_aes_cbc(cleartext, key, iv):\n    if isinstance(cleartext, unicode):\n        cleartext = cleartext.encode('utf8')\n    elif isinstance(cleartext, bytearray):\n        cleartext = bytes(cleartext)\n    if not isinstance(cleartext, bytes):\n        raise TypeError(\"content to encrypt must by bytes.\")\n    \n    aes = AES.new(key, AES.MODE_CBC, iv)\n    padding = AES.block_size - (len(cleartext) % AES.block_size)\n    cleartext += chr(padding).encode('utf-8') * padding # the encode() is for py3k compat\n    return aes.encrypt(cleartext)", "response": "This method encrypts the content."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind completions for current command line.", "response": "def complete(command_line,\n             current_token,\n             position,\n             shell: arg(choices=('bash', 'fish'))):\n    \"\"\"Find completions for current command.\n\n    This assumes that we'll handle all completion logic here and that\n    the shell's automatic file name completion is disabled.\n\n    Args:\n        command_line: Command line\n        current_token: Token at cursor\n        position: Current cursor position\n        shell: Name of shell\n\n    \"\"\"\n    position = int(position)\n    tokens = shlex.split(command_line[:position])\n\n    all_argv, run_argv, command_argv = run.partition_argv(tokens[1:])\n    run_args = run.parse_args(run_argv)\n\n    module = run_args.get('commands_module')\n    module = module or DEFAULT_COMMANDS_MODULE\n    module = normalize_path(module)\n\n    try:\n        collection = Collection.load_from_module(module)\n    except Exception:\n        collection = {}\n\n    found_command = find_command(collection, tokens) or run\n\n    if current_token:\n        # Completing either a command name, option name, or path.\n        if current_token.startswith('-'):\n            if current_token not in found_command.option_map:\n                print_command_options(found_command, current_token)\n        else:\n            print_commands(collection, shell)\n            path = os.path.expanduser(current_token)\n            path = os.path.expandvars(path)\n            paths = glob.glob('%s*' % path)\n            if paths:\n                for entry in paths:\n                    if os.path.isdir(entry):\n                        print('%s/' % entry)\n                    else:\n                        print(entry)\n    else:\n        # Completing option value. If a value isn't expected, show the\n        # options for the current command and the list of commands\n        # instead.\n        option = found_command.option_map.get(tokens[-1])\n\n        if option and option.takes_value:\n            if option.choices:\n                for choice in option.choices:\n                    print(choice)\n            else:\n                for entry in os.listdir():\n                    if os.path.isdir(entry):\n                        print('%s/' % entry)\n                    else:\n                        print(entry)\n        else:\n            print_command_options(found_command)\n            print_commands(collection, shell)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef install_package(self, name, index=None, force=False, update=False):\n        cmd = 'install'\n        if force:\n\n            cmd = '{0} {1}'.format(cmd, '--force-reinstall')\n\n        if update:\n\n            cmd = '{0} {1}'.format(cmd, '--update')\n\n        if index:\n\n            cmd = '{0} {1}'.format(cmd, '--index-url {0}'.format(index))\n\n        self.pip('{0} {1}'.format(cmd, name))", "response": "Install a given package."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninstalling packages from a requirements. txt file.", "response": "def install_requirements(self, path, index=None):\n        \"\"\"Install packages from a requirements.txt file.\n\n        Args:\n            path (str): The path to the requirements file.\n            index (str): The URL for a pypi index to use.\n        \"\"\"\n        cmd = 'install -r {0}'.format(path)\n        if index:\n\n            cmd = 'install --index-url {0} -r {1}'.format(index, path)\n\n        self.pip(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting next day of week", "response": "def get_next(weekday, including_today=False):\n        \"\"\"Gets next day of week\n\n        :param weekday: day of week\n        :param including_today: If today is sunday and requesting next sunday\n        :return: Date of next monday, tuesday ..\n        \"\"\"\n        now = datetime.datetime.now()\n        if now.weekday() == weekday.value and including_today:\n            delta = datetime.timedelta(days=0)\n        elif now.weekday() == weekday.value and not including_today:\n            delta = datetime.timedelta(days=7)\n        else:\n            delta = datetime.timedelta(\n                (7 + weekday.value - now.weekday()) % 7\n            )  # times delta to next instance\n        return Day(now + delta).get_just_date()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_just_date(self):\n        return datetime.datetime(\n            self.date_time.year,\n            self.date_time.month,\n            self.date_time.day\n        )", "response": "Parses just date from date - time\nEffective."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_in_this_week(self):\n        return self.is_date_in_between(\n            Weekday.get_last(self.week_end, including_today=True),\n            Weekday.get_next(self.week_end),\n            include_end=False\n        )", "response": "Checks if date is in this week"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a date is in between dates", "response": "def is_date_in_between(self, start, end, include_start=True,\n                           include_end=True):\n        \"\"\"Checks if date is in between dates\n\n        :param start: Date cannot be before this date\n        :param end: Date cannot be after this date\n        :param include_start: True iff date is start\n        :param include_end: True iff date is end\n        :return: True iff date is in between dates\n        \"\"\"\n\n        start = Day(start).get_just_date()\n        now = self.get_just_date()\n        end = Day(end).get_just_date()\n\n        if start < now < end:\n            return True\n\n        if include_start and now == start:\n            return True\n\n        if include_end and now == end:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets next weekday in the calendar", "response": "def get_next_weekday(self, including_today=False):\n        \"\"\"Gets next week day\n\n        :param including_today: If today is sunday and requesting next sunday\n        :return: Date of next monday, tuesday ..\n        \"\"\"\n        weekday = self.date_time.weekday()\n        return Weekday.get_next(weekday, including_today=including_today)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_last_weekday(self, including_today=False):\n        weekday = self.date_time.weekday()\n        return Weekday.get_last(weekday, including_today=including_today)", "response": "Gets the last weekday of the current date"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cli(context, verbose, quiet, database, sense):\n    logger = logging.getLogger()\n    handler = logging.StreamHandler(sys.stderr)\n    handler.setFormatter(LevelFormatter())\n    logger.addHandler(handler)\n    logger.setLevel(logging.WARNING + (quiet-verbose)*10)\n    logging.debug(_('Subcommand: %s'), context.invoked_subcommand)\n    context.obj['database'] = Database(database)\n    try:\n        context.obj['sense'] = SenseWithExport(sense).__enter__()\n    except Exception:\n        pass", "response": "Position Independent Programming For Humans."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(filenames, uri, cl_args, link_args, x64, native):\n    logging.info(_('This is source file building mode.'))\n    logging.debug(_('filenames: %s'), filenames)\n    logging.debug(_('uri: %s'), uri)\n    logging.debug(_('cl_args: %s'), cl_args)\n    logging.debug(_('link_args: %s'), link_args)\n    logging.debug(_('native: %s'), native)\n    logging.debug(_('x64: %s'), x64)\n\n    if is_windows():\n        pass\n        # ret = msbuild(uri, native, list(filenames), x64=x64,\n        #               cl_args=cl_args, link_args=link_args)\n    else:\n        builder = LinuxBuilder()\n        ret = builder.build(filenames, x64, 'src', 'out')\n    sys.exit(ret)", "response": "Build a new file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery Windows identifiers and locations.", "response": "def search(context, keywords, module, raw, kind):\n    \"\"\"Query Windows identifiers and locations.\n\n    Windows database must be prepared before using this.\n    \"\"\"\n    logging.info(_('Entering search mode'))\n    sense = context.obj['sense']\n    func = sense.query_names if module else sense.query_info\n    none = True\n    for keyword in keywords:\n        output = func(keyword, raw, kind)\n        if output:\n            none = False\n            print(output)\n        else:\n            logging.warning(_('No results: %s'), keyword)\n    sys.exit(1 if none else 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery Win32 API declarations.", "response": "def winapi(context, names):\n    \"\"\"Query Win32 API declarations.\n\n    Windows database must be prepared before using this.\n    \"\"\"\n    logging.info(_('Entering winapi mode'))\n    sense = context.obj['sense']\n    none = True\n    for name in names:\n        code = sense.query_args(name)\n        if code:\n            none = False\n            print(stylify_code(code))\n        else:\n            logging.warning(_('Function not found: %s'), name)\n    sys.exit(1 if none else 0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noperate on IntelliSense kind ids and names.", "response": "def kinds(context, show_all, ids_or_names):\n    \"\"\"Operate on IntelliSense kind ids and names.\n\n    Without an argument, list all available kinds and their ids.\n\n    Windows database must be prepared before using this.\n    \"\"\"\n    logging.info(_('Entering kind mode'))\n    logging.debug('args: %s', ids_or_names)\n    sense = context.obj['sense']\n    none = True\n    if show_all:\n        none = False\n        print(sense.query_kinds(None))\n    else:\n        for id_or_name in ids_or_names:\n            id_name = sense.query_kinds(id_or_name)\n            if id_name:\n                none = False\n                print(id_name)\n    sys.exit(1 if none else 0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export(context, keywords, module, update):\n    logging.info(_('Export Mode'))\n    database = context.obj['sense']\n    none = True\n    if update:\n        exports = OrderedDict()\n        from .executables.pe import PE\n        for filename in keywords:\n            module = split_ext(filename, basename=True)[0]\n            with open(filename, 'rb') as stream:\n                exports.update(\n                    {module: PE(stream).get_export_table()})\n        database.make_export(exports)\n        none = False\n    elif module:\n        for module_name in keywords:\n            funcs = database.query_module_funcs(module_name)\n            if funcs:\n                none = False\n                print(', '.join(map(str, funcs)))\n            else:\n                logging.warning(_('No function for module: %s'),\n                                module_name)\n    else:\n        for func_name in keywords:\n            module_name = database.query_func_module(func_name)\n            if module_name:\n                none = False\n                print(repr(module_name))\n            else:\n                logging.warning(_('No module for function: %s'),\n                                func_name)\n    sys.exit(1 if none else 0)", "response": "Operate on libraries and exported functions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(context, filenames):\n    logging.info(_('Current Mode: Add Linux data'))\n    context.obj['database'].add_data(filenames)\n    sys.exit(0)", "response": "Add data on Linux system calls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake binaries from sources.", "response": "def make(filenames, x64, cl_args, link_args, output):\n    \"\"\"Make binaries from sources.\n\n    Note that this is incomplete.\n    \"\"\"\n    from .msbuild import Builder\n    builder = Builder()\n    builder.build(list(filenames), x64=x64,\n                  cl_args=cl_args, link_args=link_args,\n                  out_dir=output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef info(context, keywords, x86, x64, x32, common):\n    logging.info(_('Current Mode: Find in Linux'))\n    database = context.obj['database']\n    for one in keywords:\n        abis = ['i386', 'x64', 'common', 'x32']\n        if x86:\n            abis = ['i386']\n        if x64:\n            abis = ['x64', 'common']\n        if x32:\n            abis = ['x32', 'common']\n        if common:\n            abis = ['common']\n        items = database.query_item(one, abis)\n        if not items:\n            logging.warning(_('Item not found: %s %s'), one, abis)\n            continue\n        for item in items:\n            print(item.name, item.abi, item.number)\n            decl = database.query_decl(name=item.name)\n            if not decl:\n                logging.warning(_('Decl not found: %s'), item.name)\n                continue\n            for one in decl:\n                print(one.decl(), '/* {} */'.format(one.filename))\n    sys.exit(0)", "response": "Print out information about the items and decls in the Linux system calls."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert binary. Extract bytes in the given section from binary files and construct C source code that can be used to test as shellcode. Supported executable formats: ELF via pyelftools and PE via pefile.", "response": "def conv(arg, source, target, filename, section):\n    \"\"\"Convert binary.\n\n    Extract bytes in the given section from binary files\n    and construct C source code\n    that can be used to test as shellcode.\n\n    Supported executable formats:\n    ELF via pyelftools and PE via pefile.\n    \"\"\"\n    logging.info(_('This is Binary Conversion mode.'))\n    section = section.encode('utf-8')\n    if source == 'sec':\n        arg = open(arg, 'rb')\n    if source == 'sec':\n        kwargs = dict(section_name=section)\n    else:\n        kwargs = dict()\n    result = Converter.uni_from(source, arg, **kwargs).uni_to(target)\n    if result:\n        if filename:\n            logging.info(\n                _('Writing shellcode to the file: %s'), filename)\n            mode = 'wb' if target == 'bin' else 'w'\n            with open(filename, mode) as output:\n                output.write(result)\n        else:\n            print(result)\n    else:\n        logging.error(_('Failed.'))\n    if source == 'sec':\n        arg.close()\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_source(label, source_type, **kwargs):\n    if source_type not in yapconf.ALL_SUPPORTED_SOURCES:\n        raise YapconfSourceError(\n            'Invalid source type %s. Supported types are %s.' %\n            (source_type, yapconf.ALL_SUPPORTED_SOURCES)\n        )\n    if source_type not in yapconf.SUPPORTED_SOURCES:\n        raise YapconfSourceError(\n            'Unsupported source type \"%s\". If you want to use this type, you '\n            'will need to install the correct client for it (try `pip install '\n            'yapconf[%s]. Currently supported types are %s. All supported '\n            'types are %s' %\n            (source_type, source_type, yapconf.SUPPORTED_SOURCES,\n             yapconf.ALL_SUPPORTED_SOURCES)\n        )\n\n    # We pop arguments from kwargs because the individual config sources\n    # have better error messages if a keyword argument is missed.\n    if source_type == 'dict':\n        return DictConfigSource(label, data=kwargs.get('data'))\n\n    elif source_type == 'json':\n        return JsonConfigSource(label, **kwargs)\n\n    elif source_type == 'yaml':\n        filename = kwargs.get('filename')\n        if 'filename' in kwargs:\n            kwargs.pop('filename')\n        return YamlConfigSource(label, filename, **kwargs)\n\n    elif source_type == 'environment':\n        return EnvironmentConfigSource(label)\n\n    elif source_type == 'etcd':\n        return EtcdConfigSource(\n            label, kwargs.get('client'), kwargs.get('key', '/')\n        )\n\n    elif source_type == 'kubernetes':\n        name = kwargs.get('name')\n        if 'name' in kwargs:\n            kwargs.pop('name')\n\n        client = kwargs.get('client')\n        if 'client' in kwargs:\n            kwargs.pop('client')\n        return KubernetesConfigSource(label, client, name, **kwargs)\n\n    else:\n        raise NotImplementedError(\n            'No implementation for source type %s' % source_type\n        )", "response": "Returns a config source based on the given label and source type and keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn one new VcfRecord that can be used as input to gramtools.", "response": "def make_one_merged_vcf_record_for_gramtools(self, ref_seq, max_alleles=5000):\n        '''Returns one new VcfRecord that can be used as input to gramtools.\n        It pads the reference if necessary, and lists all the variants\n        (including all combinations of SNPs) in the ALT field of the\n        VcfRecord.\n        Note: gramtools needs PASS in the filter column, so the returned\n        VcfRecord always has PASS.'''\n        if len(self) == 0:\n            return None\n        elif len(self) == 1:\n            record = copy.copy(self[0])\n            record.FILTER = {'PASS'}\n            return record\n\n        logging.debug('make_one_merged_vcf_record_for_gramtools() start. Number of records: ' +  str(len(self)))\n        for record in self.vcf_records:\n            logging.debug('make_one_merged_vcf_record_for_gramtools() input record: ' + str(record))\n\n        # Gather together the SNP and non-SNP alleles.\n        # Also sanity check that the CHROM names are all the same\n        # and determine the final start and end positions of the\n        # vcf record we will output\n        nucleotides = {'A', 'C', 'G', 'T'}\n        snps = {} # position => set of alts (and the ref nucleotide)\n        non_snps = [] #\u00a0list of tuples (ref position, ref seq, alt seq)\n        chrom_names = set()\n        final_start = float('Inf')\n        final_end = -1\n\n        for record in self.vcf_records:\n            final_start = min(final_start, record.POS)\n            final_end = max(final_end, record.ref_end_pos())\n            chrom_names.add(record.CHROM)\n\n            if record.REF in nucleotides:\n                for alt in record.ALT:\n                    if alt in nucleotides:\n                        if record.POS not in snps:\n                            snps[record.POS] = {record.REF}\n                        snps[record.POS].add(alt)\n                    else:\n                        non_snps.append((record.POS, record.REF, alt))\n            else:\n                for alt in record.ALT:\n                    non_snps.append((record.POS, record.REF, alt))\n\n        if len(chrom_names) != 1:\n            raise Error('Error! More than one CHROM found. Got:' + str(chrom_names))\n        chrom_name = chrom_names.pop()\n\n        # generate all the allele combinations from the SNPs.\n        snp_positions = []\n        snp_nucleotides = []\n        for position in sorted(snps):\n            snp_positions.append(position)\n            snp_nucleotides.append(sorted(list(snps[position])))\n        ref_seq_for_vcf = ref_seq[final_start:final_end+1]\n        alleles = set()\n\n        # work out min total alleles without making them. Making them could\n        #\u00a0take a long time if too many! Can onnly put lower bound on\n        # the final unique number because all the combinations may have duplicates.\n        total_alleles_lower_bound = 1\n        for x in snp_nucleotides:\n            total_alleles_lower_bound *= len(x)\n        total_alleles_lower_bound += len(non_snps)\n\n        if max_alleles is not None and total_alleles_lower_bound > max_alleles:\n            return None\n\n        for combination in itertools.product(*snp_nucleotides):\n            alt_seq = list(ref_seq_for_vcf)\n            for i, position in enumerate(snp_positions):\n                if position < 1:\n                    raise ValueError(\"POS value (%d) less than 1. Ensure VCF is valid.\" % position)\n                alt_seq[position - final_start] = combination[i]\n            alleles.add(''.join(alt_seq))\n            for non_snp_pos, non_snp_ref, non_snp_alt in non_snps:\n                start_pos = non_snp_pos - final_start\n                new_seq = alt_seq[:start_pos] + list(non_snp_alt) + alt_seq[start_pos + len(non_snp_ref):]\n                alleles.add(''.join(new_seq))\n\n        # remove the ref allele (if it's there), because it goes in the REF\n        # column of the VCF\n        try:\n            alleles.remove(ref_seq_for_vcf)\n        except:\n            pass\n\n        if max_alleles is not None and len(alleles) > max_alleles:\n            return None\n        alleles = sorted(list(alleles))\n        fields = [chrom_name, str(final_start + 1), '.', ref_seq_for_vcf,\n                  ','.join(alleles), '.', 'PASS', 'SVTYPE=COMPLEX']\n        logging.debug('make_one_merged_vcf_record_for_gramtools number of alts: ' + str(len(alleles)))\n        return vcf_record.VcfRecord('\\t'.join(fields))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_simple_merged_vcf_with_no_combinations(self, ref_seq):\n        '''Does a simple merging of all variants in this cluster.\n        Assumes one ALT in each variant. Uses the ALT for each\n        variant, making one new vcf_record that has all the variants\n        put together'''\n        if len(self) <= 1:\n            return\n\n        merged_vcf_record = self.vcf_records[0]\n\n        for i in range(1, len(self.vcf_records), 1):\n            if self.vcf_records[i].intersects(merged_vcf_record):\n                return\n            else:\n                merged_vcf_record = merged_vcf_record.merge(self.vcf_records[i], ref_seq)\n\n        self.vcf_records = [merged_vcf_record]", "response": "Does a simple merging of all variants in this cluster with no combinations"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_simple_gt_aware_merged_vcf_with_no_combinations(self, ref_seq):\n        '''Does a simple merging of all variants in this cluster.\n        Assumes one ALT in each variant. Uses the called allele for each\n        variant, making one new vcf_record that has all the variants\n        put together'''\n        if len(self) <= 1:\n            return\n\n        merged_vcf_record = self.vcf_records[0]\n\n        for i in range(1, len(self.vcf_records), 1):\n            if self.vcf_records[i].intersects(merged_vcf_record):\n                return\n            else:\n                merged_vcf_record = merged_vcf_record.gt_aware_merge(self.vcf_records[i], ref_seq)\n\n        self.vcf_records = [merged_vcf_record]", "response": "Does a simple merging of all variants in this cluster with no combinations"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_separate_indels_and_one_alt_with_all_snps_no_combinations(self, ref_seq):\n        '''Returns a VCF record, where each indel from this\n        cluster is in a separate ALT. Then all the remaining SNPs are\n        applied to make one ALT. If >1 SNP in same place, either one\n        might be used'''\n        final_start_position = min([x.POS for x in self.vcf_records])\n        final_end_position = max([x.ref_end_pos() for x in self.vcf_records])\n        snps = []\n        new_vcf_records = []\n\n        for record in self.vcf_records:\n            if record.is_snp():\n                snps.append(copy.copy(record))\n            else:\n                new_record = copy.copy(record)\n                new_record.add_flanking_seqs(ref_seq, final_start_position, final_end_position)\n                new_vcf_records.append(new_record)\n\n        if len(snps):\n            new_record = copy.copy(snps[0])\n            for snp in snps[1:]:\n                merged = new_record.merge(snp, ref_seq)\n                if merged is not None:\n                    new_record = merged\n            new_record.add_flanking_seqs(ref_seq, final_start_position, final_end_position)\n            new_vcf_records.append(new_record)\n\n        alts = ','.join(sorted(list(set([x.ALT[0] for x in new_vcf_records]))))\n        new_record = vcf_record.VcfRecord('\\t'.join([self.vcf_records[0].CHROM, str(final_start_position + 1), '.', new_vcf_records[0].REF, alts, '.', 'PASS', '.']))\n\n        return new_record", "response": "Returns a VCF record where each indel from thisCOOKIE is in a separate ALT. Then all the remaining SNPs are applied to make one ALT."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef createfastq(self):\n        # If the fastq destination folder is not provided, make the default value of :path/:miseqfoldername\n        self.fastqdestination = self.fastqdestination if self.fastqdestination else \\\n            os.path.join(self.path, self.miseqfoldername)\n        # Make the path\n        make_path(self.fastqdestination)\n        # Create a new sample sheet using self.project name instead of the provided Sample_Project. This ensures\n        # that all the FASTQ files are stored in the same output folder\n        projectsamplesheet = os.path.join(self.fastqdestination, 'SampleSheet_modified.csv')\n        with open(projectsamplesheet, \"w\") as modifiedsamplesheet:\n            # Use the 'original' sample sheet as the template for the new sheet\n            with open(self.customsamplesheet) as samplesheet:\n                # Iterate through the template sheet, and write lines until the header for the data portion of the sheet\n                for line in samplesheet:\n                    modifiedsamplesheet.write(line)\n                    if 'Sample_ID' in line:\n                        # Create a list of the header values\n                        header = line.split(',')\n                        for subline in samplesheet:\n                            # Split the line on commas\n                            data = subline.split(',')\n                            # Initialise a list to store the values for each sample\n                            updateddata = list()\n                            # Iterate through the entries in the header, and extract the corresponding value\n                            for i, value in enumerate(header):\n                                # Find the Sample_Project value, and update it to be self.projectname\n                                if data[i] in self.projectlist:\n                                    data[i] = self.projectname\n                                # If demultiplexing is disabled, don't add the samples to the SampleSheet\n                                if self.demultiplex:\n                                    # Add the (modified) data to the list\n                                    updateddata.append(data[i])\n                            # Write the updated string to the new sheet\n                            modifiedsamplesheet.write(','.join(updateddata))\n        # Set :forward/reverse length to :header.forward/reverse length if the argument is not provided, or it's 'full',\n        # otherwise  use the supplied argument\n        self.forwardlength = self.header.forwardlength if self.forwardlength.lower()\\\n            == 'full' else self.forwardlength\n        # Set :reverselength to :header.reverselength\n        self.reverselength = self.header.reverselength if self.reverselength.lower() \\\n            == 'full' else self.reverselength\n        # As the number of cycles required is the number of forward reads + the index(8) + the second index(8)\n        # Also set the basemask variable as required\n        if self.reverselength != '0':\n            self.readsneeded = int(self.forwardlength) + int(self.reverselength) + self.indexlength\n            basemask = \"Y{}n*,{},Y{}n*\".format(self.forwardlength, self.index, self.reverselength)\n        else:\n            self.readsneeded = int(self.forwardlength) + self.indexlength\n            basemask = \"Y{}n*,{},n*\".format(self.forwardlength, self.index)\n        # Handle plurality appropriately\n        samples = 'samples' if self.samplecount != 1 else 'sample'\n        number = 'are' if self.samplecount != 1 else 'is'\n        logging.info('There {num} {num_samples} {plural} in this run.\\n'\n                     'MiSeqPath: {miseqpath},\\n'\n                     'MiSeqFolder: {miseqfolder},\\n'\n                     'FASTQ destination: {destination},\\n'\n                     'SampleSheet: {sample_sheet}'\n                     .format(num=number,\n                             num_samples=self.samplecount,\n                             plural=samples,\n                             miseqpath=self.miseqpath,\n                             miseqfolder=self.miseqfolder,\n                             destination=self.fastqdestination,\n                             sample_sheet=projectsamplesheet))\n        # Count the number of completed cycles in the run of interest\n        cycles = glob(os.path.join(self.miseqpath, self.miseqfolder, 'Data', 'Intensities', 'BaseCalls', 'L001', 'C*'))\n        while len(cycles) < self.readsneeded:\n            logging.info('Currently at {num_cycles} cycles. Waiting until the MiSeq reaches cycle {target_cycle}'\n                         .format(num_cycles=len(cycles),\n                                 target_cycle=self.readsneeded))\n            sleep(300)\n            cycles = glob(os.path.join(self.miseqpath, self.miseqfolder,\n                                       'Data', 'Intensities', 'BaseCalls', 'L001', 'C*'))\n        # configureBClToFastq requires :self.miseqfolder/Data/Intensities/BaseCalls/config.xml in order to work\n        # When you download runs from BaseSpace, this file is not provided. There is an empty config.xml file that\n        # can be populated with run-specific values and moved to the appropriate folder\n        if not os.path.isfile(os.path.join(self.miseqfolder, 'Data', 'Intensities', 'BaseCalls', 'config.xml')):\n            self.configfilepopulator()\n        if self.debug:\n            # Define the bcl2fastq system call for the unit test\n            bclcall = \"bcl2fastq --input-dir {basecalls} \" \\\n                      \"--output-dir {outdir} --sample-sheet {samplesheet} \" \\\n                      \"--barcode-mismatches 0 -r 1 -p 1 -w 1 -R {runfolder} --use-bases-mask {mask} \" \\\n                      \"--tiles s_1_1101 --minimum-trimmed-read-length 1\" \\\n                .format(basecalls=os.path.join(self.miseqfolder, 'Data', 'Intensities', 'BaseCalls'),\n                        outdir=self.fastqdestination,\n                        samplesheet=projectsamplesheet,\n                        runfolder=self.miseqfolder,\n                        mask=basemask)\n        # elif not self.demultiplex:\n        #     bclcall = \"bcl2fastq --input-dir {basecalls} \" \\\n        #               \"--output-dir {outdir} --sample-sheet {samplesheet} --no-lane-splitting \" \\\n        #               \"-r 1 -p 1 -w 1 -R {runfolder} --use-bases-mask {mask}\"\\\n        #         .format(basecalls=os.path.join(self.miseqfolder, 'Data', 'Intensities', 'BaseCalls'),\n        #                 outdir=self.fastqdestination,\n        #                 samplesheet=projectsamplesheet,\n        #                 runfolder=self.miseqfolder,\n        #                 mask=basemask)\n        else:\n            bclcall = \"bcl2fastq --input-dir {basecalls} \" \\\n                      \"--output-dir {outdir} --sample-sheet {samplesheet} \" \\\n                      \"--barcode-mismatches 1 -r 1 -p 1 -w 1 -R {runfolder} --use-bases-mask {mask}\"\\\n                .format(basecalls=os.path.join(self.miseqfolder, 'Data', 'Intensities', 'BaseCalls'),\n                        outdir=self.fastqdestination,\n                        samplesheet=projectsamplesheet,\n                        runfolder=self.miseqfolder,\n                        mask=basemask)\n        process = False\n        if self.demultiplex:\n            if not os.path.isdir(self.projectpath):\n                process = True\n        else:\n            if not os.path.isfile(os.path.join(self.fastqdestination, 'Undetermined_S0_R1_001.fastq.gz')):\n                process = True\n        if process:\n            # Call bcl2fastq\n            logging.info('Running bcl2fastq')\n            # Run the command\n            out, err = run_subprocess(bclcall)\n            write_to_logfile(bclcall,\n                             bclcall,\n                             self.logfile)\n            write_to_logfile(out,\n                             err,\n                             self.logfile)\n        # Populate the metadata\n        for sample in self.metadata.samples:\n            sample.commands = GenObject()\n            sample.commands.bcl = bclcall\n            sample.run.forwardlength = self.forwardlength\n            sample.run.reverselength = self.reverselength\n        # Copy the fastq files to a central folder so they can be processed\n        self.fastqmover()", "response": "Uses bcl2fastq to create. fastq files from a MiSeqRun"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fastqmover(self):\n        # Create the sequence path if necessary\n        make_path(self.sequencepath)\n        # Iterate through all the sample names\n        for sample in self.metadata.samples:\n            # Make directory variables\n            outputdir = os.path.join(self.sequencepath, sample.name)\n            # Demultiplexed files will be present in the project name subfolder within the fastq destination folder\n            if self.demultiplex:\n                glob_dir = self.projectpath\n            # Undemultiplexed reads (Undetermined_S0_R1_001.fastq.gz) are present in the fastq destination folder\n            else:\n                glob_dir = self.fastqdestination\n            # Sometimes the files are put in self.fastqdestination rather than self.projectpath? Matt has this problem\n            # and I don't understands what's going on\n            if not os.path.isdir(glob_dir):\n                glob_dir = os.path.dirname(glob_dir)\n            # Glob all the .gz files in the subfolders - projectpath/Sample_:sample.name/*.gz\n            for fastq in sorted(glob(os.path.join(glob_dir, '*.gz'))):\n                fastqname = os.path.basename(fastq)\n                # Set the name of the destination file\n                outputfile = os.path.join(self.sequencepath, fastqname)\n                # Link the file if it doesn't already exist\n                if not os.path.isfile(outputfile):\n                    relative_symlink(src_file=fastq,\n                                     output_dir=self.sequencepath)\n            # Repopulate .strainfastqfiles with the freshly-linked/copied files\n            if self.demultiplex:\n                fastqfiles = glob(os.path.join(self.sequencepath, '{}*.fastq*'.format(sample.name)))\n                fastqfiles = sorted([fastq for fastq in fastqfiles if 'trimmed' not in os.path.basename(fastq)])\n            # Undemultiplexed files will not have the sample name in the file name\n            else:\n                fastqfiles = sorted(glob(os.path.join(glob_dir, '*.gz')))\n            # Populate the metadata object with the name/path of the fastq files\n            sample.general.fastqfiles = fastqfiles\n            # Save the outputdir to the metadata object\n            sample.run.outputdirectory = outputdir\n            sample.general.outputdirectory = outputdir\n            sample.general.bestassemblyfile = True\n            sample.general.trimmedcorrectedfastqfiles = sorted(sample.general.fastqfiles)\n            sample.general.logout = os.path.join(sample.general.outputdirectory, 'logout')\n            sample.general.logerr = os.path.join(sample.general.outputdirectory, 'logerr')\n            sample.commands = GenObject()", "response": "Links. fastq files created above to the destination folder"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_markdown(self):\n        if self.type == \"text\":\n            return self.text\n        elif self.type == \"url\" or self.type == \"image\":\n            return \"[\" + self.text + \"](\" + self.attributes[\"ref\"] + \")\"\n        elif self.type == \"title\":\n            return \"#\" * int(self.attributes[\"size\"]) + \" \" + self.text\n\n        return None", "response": "Converts the object to markdown format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the header of the table", "response": "def _get_header(self):\n        \"\"\"Gets header of table\n\n        :return: markdown-formatted header\"\"\"\n        out = self._get_row(self.labels)\n        out += \"\\n\"\n        out += self._get_row([\"---\"] * len(self.labels))  # line below headers\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_markdown(self):\n        out = self._get_header()\n        out += \"\\n\"\n\n        for row in self.table:\n            out += self._get_row(row)\n            out += \"\\n\"\n\n        return out", "response": "Converts to markdown format"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setModel(self, model):\n        self.paramList.setModel(model)\n        model.hintRequested.connect(self.hintRequested)\n        model.rowsInserted.connect(self.updateTitle)\n        model.rowsRemoved.connect(self.updateTitle)\n        self.updateTitle()", "response": "sets the model for the auto parameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the Title of this widget according to how many parameters are currently in the model", "response": "def updateTitle(self):\n        \"\"\"Updates the Title of this widget according to how many parameters are currently in the model\"\"\"\n        title = 'Auto Parameters ({})'.format(self.paramList.model().rowCount())\n        self.titleChange.emit(title)\n        self.setWindowTitle(title)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef closeEvent(self, event):\n        self.visibilityChanged.emit(0)\n        model = self.paramList.model()\n        model.hintRequested.disconnect()\n        model.rowsInserted.disconnect()\n        model.rowsRemoved.disconnect()", "response": "Emits a signal to update start values on components"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authenticate_with_username_password():\n    credentials = Credentials(username='admin', password='password')\n    client = YamcsClient('localhost:8090', credentials=credentials)\n\n    for link in client.list_data_links('simulator'):\n        print(link)", "response": "Authenticate in by directly providing username and password to Yamcs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef authenticate_with_access_token(access_token):\n    credentials = Credentials(access_token=access_token)\n    client = YamcsClient('localhost:8090', credentials=credentials)\n\n    for link in client.list_data_links('simulator'):\n        print(link)", "response": "Authenticate using an existing access token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and creates a pretty table from the data", "response": "def pretty_format_table(labels, data, num_format=\"{:.3f}\", line_separator=\"\\n\"):\n    \"\"\"Parses and creates pretty table\n\n    :param labels: List of labels of data\n    :param data: Matrix of any type\n    :param num_format: Format numbers with this format\n    :param line_separator: Separate each new line with this\n    :return: Pretty formatted table (first row is labels, then actual data)\n    \"\"\"\n    table = SqlTable(labels, data, num_format, line_separator)\n    return table.build()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses value COOKIE and returns parsed value", "response": "def _parse_value(self, raw):\n        \"\"\"Parses value\n\n        :param raw: raw value\n        :return: Parsed value\n        \"\"\"\n        try:\n            if not raw.startswith(\"0\"):\n                val = float(raw)\n                if (val % 1) == 0:  # integer\n                    val = int(raw)\n                    return str(val)\n\n                return self.num_format.format(val)\n            else:\n                raise ValueError(\"Cannot parse int!\")\n        except:\n            return str(raw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_row(self, i):\n        row = self.data[i]\n        for j in range(len(row)):\n            self.data[i][j] = self._parse_value(self.data[i][j])", "response": "Parses the row at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the optimal column widths of columns.", "response": "def _calculate_optimal_column_widths(self):\n        \"\"\"Calculates widths of columns\n\n        :return: Length of longest data in each column (labels and data)\n        \"\"\"\n        columns = len(self.data[0])  # number of columns\n        str_labels = [parse_colorama(str(l)) for l in\n                      self.labels]  # labels as strings\n        str_data = [[parse_colorama(str(col)) for col in row] for row in\n                    self.data]\n        # values as strings\n\n        widths = [0] * columns  # length of longest string in each column\n        for row in str_data:  # calculate max width in each column\n            widths = [max(w, len(c)) for w, c in zip(widths, row)]\n\n        # check if label name is longer than data\n        for col, label in enumerate(str_labels):\n            if len(label) > widths[col]:\n                widths[col] = len(label)\n\n        self.widths = widths"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pretty_row(self, row, filler, splitter):\n        for i, val in enumerate(row):\n            length_diff = self.widths[i] - len(parse_colorama(val))\n            if length_diff > 0:  # value is shorter than foreseen\n                row[i] = str(filler * length_diff) + row[i]  # adjust content\n\n        pretty_row = splitter  # start of row\n        for val in row:\n            pretty_row += filler + val + filler + splitter\n\n        return pretty_row", "response": "Gets a pretty - formatted row of data with the specified columns."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_blank_row(self, filler=\"-\", splitter=\"+\"):\n        return self.get_pretty_row(\n            [\"\" for _ in self.widths],  # blanks\n            filler,  # fill with this\n            splitter,  # split columns with this\n        )", "response": "Gets a blank row of the log file with no meaningful data in it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a pretty - formatted version of the data in the row.", "response": "def pretty_format_row(self, row, filler=\" \", splitter=\"|\"):\n        \"\"\"Gets pretty-formatted row\n\n        :param row: List of data\n        :param filler: Fill empty columns with this char\n        :param splitter: Separate columns with this char\n        :return: Pretty formatted row\n        \"\"\"\n        return self.get_pretty_row(\n            row,\n            filler,\n            splitter\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds pretty - formatted table MimeType", "response": "def build(self):\n        \"\"\"Builds pretty-formatted table\n\n        :return: pretty table\n        \"\"\"\n        self._calculate_optimal_column_widths()\n\n        pretty_table = self.get_blank_row() + self.new_line  # first row\n        pretty_table += self.pretty_format_row(self.labels) + self.new_line\n        pretty_table += self.get_blank_row() + self.new_line\n\n        for row in self.data:  # append each row\n            pretty_table += self.pretty_format_row(row) + self.new_line\n        pretty_table += self.get_blank_row()  # ending line\n\n        return pretty_table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_df(data_frame):\n        labels = data_frame.keys().tolist()\n        data = data_frame.values.tolist()\n        return SqlTable(labels, data, \"{:.3f}\", \"\\n\")", "response": "Parses data and builds an instance of this class\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nedits the headers of SAM files to remove secondary alignments", "response": "def editheaders():\n    \"\"\"Edits the headers of SAM files to remove 'secondary alignments'\"\"\"\n    # Read stdin - this will be the output from samtools view\n    for line in fileinput.input():\n        try:\n            # Get the flag value from the input\n            columns = line.split('\\t')\n            # The FLAG is in the second column\n            flag = int(columns[1])\n            # Subtracts 256 from the flag if the & bitwise operator evaluates to true\n            # See http://www.tutorialspoint.com/python/bitwise_operators_example.htm\n            # For the test case, flags of 256 became 0, and flags of 272 became 16\n            columns[1] = str((flag - 256) if (flag & 256) else flag)\n            # update = [columns[0], str(flag), columns[2:]]\n            sys.stdout.write('\\t'.join(columns))\n        # Don't fail on IOErrors, or ValueErrors, and still print line to stdout\n        except (IOError, ValueError):\n            sys.stdout.write(line)\n            pass\n    # Try except statements to get rid of file closing errors\n    try:\n        sys.stdout.flush()\n        sys.stdout.close()\n    except:\n        pass\n    try:\n        sys.stderr.close()\n    except:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ref_string_matches_ref_sequence(self, ref_sequence):\n        '''Returns true iff the REF string in the record agrees with\n        the given ref_sequence'''\n        # you never know what you're gonna get...\n        if self.POS < 0:\n            return False\n\n        end_pos = self.ref_end_pos()\n        if end_pos >= len(ref_sequence):\n            return False\n\n        return self.REF == ref_sequence[self.POS:end_pos + 1]", "response": "Returns true iff the REF string in the record agrees with\n        the given ref_sequence"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning true iff the CHROM in the the dict of ref_sequences and the REF string matches the REF string", "response": "def ref_string_matches_dict_of_ref_sequences(self, ref_sequences):\n        '''Returns true iff there is a sequence called self.CHROM in the\n        dict of ref_sequences, and the REF string matches'''\n        return self.CHROM in ref_sequences and self.ref_string_matches_ref_sequence(ref_sequences[self.CHROM])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_snp(self):\n        '''Returns true iff this variant is a SNP'''\n        nucleotides = {'A', 'C', 'G', 'T'}\n        return len(self.REF) == 1 and self.REF in nucleotides and set(self.ALT).issubset(nucleotides)", "response": "Returns true iff this variant is a SNP"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true iff this variant has a GT field and is homozygous.", "response": "def is_homozygous(self):\n        '''Returns true iff this variant has a GT field and is homozygous, which here\n        means that the genotype is n/n (where n can be any number).'''\n        if self.FORMAT is None:\n            return False\n        else:\n            genotypes = set(self.FORMAT.get('GT', '0/1').split('/'))\n            return '.' not in genotypes and len(genotypes) == 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new key value pair. Key in column 9 ( FORMAT ) and value in column 10.", "response": "def set_format_key_value(self, key, value):\n        '''Add a new key/value pair. Key in column 9 (FORMAT)\n        and value in column 10. If key already exists, then updates\n        the value to the new given value'''\n        if self.format_keys is None:\n            self.format_keys = []\n            self.FORMAT = {}\n        if key not in self.FORMAT:\n            self.format_keys.append(key)\n        self.FORMAT[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intersects(self, other):\n        '''Returns True iff this record's reference positions overlap\n        the other record reference positions (and are on same chromosome)'''\n        return self.CHROM == other.CHROM and self.POS <= other.ref_end_pos() and other.POS <= self.ref_end_pos()", "response": "Returns True iff this record s reference positions overlap\n        the other record s reference positions and are on same chromosome"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to merge this VcfRecord with another VcfRecord.", "response": "def merge(self, other, reference_seq):\n        '''Tries to merge this VcfRecord with other VcfRecord.\n        Simple example (working in 0-based coords):\n        ref = ACGT\n        var1 = SNP at position 1, C->G\n        var2 = SNP at position 3, T->A\n        then this returns new variant, position=1, REF=CGT, ALT=GGA.\n\n        If there is any kind of conflict, eg two SNPs in same position, then\n        returns None.\n        Also assumes there is only one ALT, otherwise returns None.'''\n        if self.CHROM != other.CHROM or self.intersects(other) or len(self.ALT) != 1 or len(other.ALT) != 1:\n            return None\n\n        ref_start = min(self.POS, other.POS)\n        ref_end = max(self.ref_end_pos(), other.ref_end_pos())\n        ref_seq_for_vcf = reference_seq[ref_start:ref_end + 1]\n        sorted_records = sorted([self, other], key=operator.attrgetter('POS'))\n        alt_seq = []\n        gt_confs = []\n        current_ref_pos = ref_start\n\n        for record in sorted_records:\n            assert record.REF != '.' and record.ALT[0] != '.'\n            alt_seq.append(reference_seq[current_ref_pos:record.POS])\n            alt_seq.append(record.ALT[0])\n            current_ref_pos += len(record.REF)\n            if record.FORMAT is not None and 'GT_CONF' in record.FORMAT:\n                gt_confs.append(record.FORMAT['GT_CONF'])\n\n        gt_conf = 0\n        format = \"GT\"\n        gt_1 = '1/1'\n        if len(gt_confs) > 0:\n            gt_conf = min(gt_confs)\n            format = 'GT:GT_CONF'\n            gt_1 = '1/1:' + str(gt_conf)\n\n        return VcfRecord('\\t'.join([\n            self.CHROM,\n            str(ref_start + 1),\n            '.',\n            ref_seq_for_vcf,\n            ''.join(alt_seq),\n            '.', '.', 'SVTYPE=MERGED',\n            format, gt_1,\n        ]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gt_aware_merge(self, other, reference_seq):\n        '''Tries to merge this VcfRecord with other VcfRecord always using called allele as alt.\n        Simple example (working in 0-based coords):\n        ref = ACGT\n        var1 = SNP at position 1, C->G called alt\n        var2 = SNP at position 3, T->A called ref\n        then this returns new variant, position=1, REF=CGT, ALT=GGT.\n\n        If there is any kind of conflict, eg two SNPs in same position, then\n        returns None.\n        Also assumes there is only one ALT, otherwise returns None.'''\n        if self.CHROM != other.CHROM or self.intersects(other) or len(self.ALT) != 1 or len(other.ALT) != 1:\n            return None\n\n        ref_start = min(self.POS, other.POS)\n        ref_end = max(self.ref_end_pos(), other.ref_end_pos())\n        ref_seq_for_vcf = reference_seq[ref_start:ref_end + 1]\n        sorted_records = sorted([self, other], key=operator.attrgetter('POS'))\n        alt_seq = []\n        all_alt_seq = []\n        gt_confs = []\n        current_ref_pos = ref_start\n\n        for record in sorted_records:\n            assert record.REF != '.' and record.ALT[0] != '.'\n            alt_seq.append(reference_seq[current_ref_pos:record.POS])\n            all_alt_seq.append(reference_seq[current_ref_pos:record.POS])\n            if record.FORMAT is None or 'GT' not in record.FORMAT:\n                return None\n\n            called_alleles = list(set(record.FORMAT['GT'].split('/')))\n            if len(called_alleles) != 1 or '.' in called_alleles:\n                return None\n            gt = int(called_alleles[0])\n            if gt > 0:\n                alt_seq.append(record.ALT[gt-1])\n            else:\n                alt_seq.append(record.REF)\n            all_alt_seq.append(record.ALT[0])\n            current_ref_pos += len(record.REF)\n            if record.FORMAT is not None and 'GT_CONF' in record.FORMAT:\n                gt_confs.append(record.FORMAT['GT_CONF'])\n\n        alt_seq_for_vcf = ''.join(alt_seq)\n        format = \"GT\"\n        gt_0 = '0/0'\n        gt_1 = '1/1'\n        if len(gt_confs) > 0:\n            gt_conf = min(gt_confs)\n            format = 'GT:GT_CONF'\n            gt_0 = '0/0:' + str(gt_conf)\n            gt_1 = '1/1:' + str(gt_conf)\n\n        if ref_seq_for_vcf == alt_seq_for_vcf:\n            return VcfRecord('\\t'.join([\n                self.CHROM,\n                str(ref_start + 1),\n                '.',\n                ref_seq_for_vcf,\n                ''.join(all_alt_seq),\n                '.', '.', 'SVTYPE=MERGED',\n                format, gt_0,\n            ]))\n        else:\n            return VcfRecord('\\t'.join([\n                self.CHROM,\n                str(ref_start + 1),\n                '.',\n                ref_seq_for_vcf,\n                alt_seq_for_vcf,\n                '.', '.', 'SVTYPE=MERGED',\n                format, gt_1,\n            ]))", "response": "Tries to merge this VcfRecord with other VcfRecord always using called allele as alt. Returns None if there is no conflict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_flanking_seqs(self, ref_seq, new_start, new_end):\n        '''Adds new_start many nucleotides at the start, and new_end many nucleotides\n        at the end from the appropriate nucleotides in reference sequence ref_seq.'''\n        if new_start > self.POS or new_end < self.ref_end_pos():\n            raise Error('new start and end positions must not try to shrink VCF record. new_start=' + str(new_start) + ', new_end=' + str(new_end) + '. VCF=' + str(self))\n\n        new_start_nucleotides = ref_seq[new_start:self.POS]\n        new_end_nucleotodes = ref_seq[self.ref_end_pos() + 1:new_end + 1]\n        self.POS = new_start\n        self.REF = new_start_nucleotides + self.REF + new_end_nucleotodes\n        self.ALT = [new_start_nucleotides + x + new_end_nucleotodes for x in self.ALT]", "response": "Adds new_start many nucleotides at the start and new_end many nucleotides at the end from the appropriate nucleotides in reference sequence ref_seq."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_by_adding_new_alts(self, other, ref_seq):\n        '''Adds other VcfRecord to this one, by adding new field(s) in the ALT\n        column. Also adds REF nucleotides if they are needed. eg:\n        ref: ACGT\n        this var: pos=3, REF=T, ALT=A\n        other var: pos=1, REF=C, ALT=T\n        will change this var to be pos=1, REF=CGT, ALT=TGT,CGA'''\n        if self.CHROM != other.CHROM:\n            raise Error('Cannot merge two VCF records that lie on difference chromosomes\\n' + str(self) + '\\n' + str(other))\n\n        new_ref_start = min(self.POS, other.POS)\n        new_ref_end = max(self.ref_end_pos(), other.ref_end_pos())\n        new_ref_end = min(new_ref_end, len(ref_seq) - 1)\n        other = copy.copy(other)\n        self.add_flanking_seqs(ref_seq, new_ref_start, new_ref_end)\n        other.add_flanking_seqs(ref_seq, new_ref_start, new_ref_end)\n\n        for other_alt in other.ALT:\n            if other_alt not in self.ALT:\n                self.ALT.append(other_alt)", "response": "Adds other VcfRecord to this one by adding new nucleotides in the ALT column."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving duplicated nucleotides at the start of REF and ALT.", "response": "def remove_useless_start_nucleotides(self):\n        '''Removes duplicated nucleotides at the start of REF and ALT.\n        But always leaves at least one nucleotide in each of REF and ALT.\n        eg if variant is at position 42, REF=GCTGA, ALT=GCA, then\n        sets position=41, REF=CTGA, ALT=CA.\n        Assumes only one ALT, and does nothing if there is >1 ALT'''\n        if len(self.REF) == 1 or len(self.ALT) != 1:\n            return\n\n        i = 0\n        while i < len(self.REF) and i < len(self.ALT[0]) and self.REF[i] == self.ALT[0][i]:\n            i += 1\n\n        if i > 0:\n            self.REF = self.REF[i - 1:]\n            self.ALT = [self.ALT[0][i - 1:]]\n            self.POS += i - 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn true iff the record is within max_distance of the given position.", "response": "def near_to_position(self, position, max_distance):\n        '''Returns true iff the record is within max_distance of the given position.\n        Note: chromosome name not checked, so that's up to you to do first.'''\n        end = self.ref_end_pos()\n        return self.POS <= position <= end or abs(position - self.POS) <= max_distance or abs(position - end) <= max_distance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inferred_var_seqs_plus_flanks(self, ref_seq, flank_length):\n        '''Returns start position of first flank sequence, plus a list of sequences -\n        the REF, plus one for each ALT.sequence. Order same as in ALT column'''\n        flank_start = max(0, self.POS - flank_length)\n        flank_end = min(len(ref_seq) - 1, self.ref_end_pos() + flank_length)\n        seqs = [ref_seq[flank_start:self.POS] + self.REF + ref_seq[self.ref_end_pos() + 1: flank_end + 1]]\n\n        for alt in self.ALT:\n            seqs.append(ref_seq[flank_start:self.POS] + alt + ref_seq[self.ref_end_pos() + 1: flank_end + 1])\n\n        return flank_start, seqs", "response": "Returns start position of first flank sequence plus a list of sequences -\n        the REF plus one for each ALT. sequence. Order same as in ALT column"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef total_coverage(self):\n        '''Returns the sum of COV data, if present. Otherwise returns None'''\n        if 'COV' in self.FORMAT:\n            return sum([int(x) for x in self.FORMAT['COV'].split(',')])\n        else:\n            return None", "response": "Returns the sum of COV data if present. Otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a set of the REF and ALT strings that were called using GT in FORMAT.", "response": "def called_alts_from_genotype(self):\n        '''Returns a set of the (maybe REF and) ALT strings that were called, using GT in FORMAT.\n        Returns None if GT not in the record'''\n        if 'GT' not in self.FORMAT:\n            return None\n\n        genotype_indexes = set([int(x) for x in self.FORMAT['GT'].split('/')])\n        alts = set()\n\n        for i in genotype_indexes:\n            if i == 0:\n                alts.add(self.REF)\n            else:\n                alts.add(self.ALT[i-1])\n\n        return alts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True iff this record and other_record are the same indel.", "response": "def is_the_same_indel(self, other_record, ref_seq):\n        '''Returns True iff this record and other_record are the \"same\"\n        indel. At repeats, there is more than one way to report the same\n        variant. eg:\n        pos=42, ref=CAAA, alt=CAA\n        pos=43, ref=AAA, alt=AA\n        pos=44, ref=AA, alt=A'''\n        if self.CHROM != other_record.CHROM or len(self.ALT) > 1 or len(other_record.ALT) > 1 or self.is_snp() or other_record.is_snp():\n            return False\n\n        # The number of nuleotides that have been added or removed\n        # is a necessary condition of the indels being the same,\n        # so check that before devling into the actual sequences\n        if (len(self.REF) - len(self.ALT[0])) != (len(other_record.REF) - len(other_record.ALT[0])):\n            return False\n\n        #\u00a0make records that start and end in the same place.\n        # Then compare the REF and ALT sequences\n        record1 = copy.copy(self)\n        record2 = copy.copy(other_record)\n        new_start = min(self.POS, other_record.POS)\n        new_end = max(self.ref_end_pos(), other_record.ref_end_pos())\n        record1.add_flanking_seqs(ref_seq, new_start, new_end)\n        record2.add_flanking_seqs(ref_seq, new_start, new_end)\n        return record1.REF == record2.REF and record1.ALT == record2.ALT"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns list of vcf_records. One per variant in the ALT column. Does not change INFO or FORMAT etc columns which means that they are now broken.", "response": "def to_record_per_alt(self):\n        '''Returns list of vcf_records. One per variant\n        in the ALT column. Does not change INFO/FORMAT etc columns, which\n        means that they are now broken'''\n        record_list = []\n        for alt in self.ALT:\n            record_list.append(copy.copy(self))\n            record_list[-1].ALT = [alt]\n        return record_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn list of vcf_records. Tries to split this record into separate SNPs. eg if REF = ACGT and ALT = AGGA then two SNPs C -> G and T -> A.", "response": "def split_into_snps(self):\n        '''Returns list of vcf_records. Tries to split\n        this record into separate SNPs. eg if\n        REF=ACGT and ALT=AGGA, then two SNPs\n        C->G and T->A. Throws away all information in the\n        INFO and FORMAT fields, except outputs the\n        correct genotype (GT) if present in the input'''\n        allele_lengths = set([len(x) for x in self.ALT])\n        allele_lengths.add(len(self.REF))\n\n        if len(allele_lengths) > 1 or allele_lengths == {1}:\n            return [self]\n\n        if self.FORMAT is not None and 'GT' in self.FORMAT:\n            has_gt = True\n            genotype_alleles = set([int(x) for x in self.FORMAT['GT'].split('/')])\n        else:\n            has_gt = False\n\n        new_snps = {}\n\n        for allele_index, allele in enumerate(self.ALT):\n            for i in range(len(self.REF)):\n                if self.REF[i] != allele[i]:\n                    if i not in new_snps:\n                        new_snps[i] = {'ref': self.REF[i], 'alts': {}}\n                    assert new_snps[i]['ref'] == self.REF[i]\n                    if allele[i] not in new_snps[i]['alts']:\n                        new_snps[i]['alts'][allele[i]] = set()\n\n                    if has_gt:\n                        new_snps[i]['alts'][allele[i]].update(genotype_alleles.intersection({allele_index + 1}))\n\n\n        new_vcfs = []\n\n        for position_in_REF, allele_dict in sorted(new_snps.items()):\n            new_vcfs.append(VcfRecord('\\t'.join([\n                self.CHROM,\n                str(self.POS + position_in_REF + 1),\n                '.',\n                allele_dict['ref'],\n                ','.join(sorted(list(allele_dict['alts'].keys()))),\n                '.',\n                'PASS',\n                'SVTYPE=SNP',\n            ])))\n\n            if has_gt:\n                if genotype_alleles == {0}:\n                    gt = '0/0'\n                else:\n                    x = [len(allele_dict['alts'][x]) for x in sorted(allele_dict['alts'])]\n                    matching_alleles = set([i+1 for i in range(len(x)) if x[i] > 0])\n\n                    if len(matching_alleles) == 0:\n                        gt = '0/0'\n                    elif len(matching_alleles) == 1:\n                        allele = matching_alleles.pop()\n                        if len(genotype_alleles) == 1:\n                            gt = str(allele) + '/' + str(allele)\n                        else:\n                            gt = '0/' + str(allele)\n                    else:\n                        assert len(matching_alleles) == 2\n                        gt = '/'.join(sorted([str(x) for x in matching_alleles]))\n\n                new_vcfs[-1].set_format_key_value('GT', gt)\n\n        return new_vcfs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the blast output file and store necessary data in dictionary in sample object", "response": "def blastparser(self, report, sample):\n        \"\"\"\n        Parse the blast results, and store necessary data in dictionaries in sample object\n        :param report: Name of the blast output report being parsed\n        :param sample: sample object\n        \"\"\"\n        # Open the sequence profile file as a dictionary\n        blastdict = DictReader(open(report), fieldnames=self.fieldnames, dialect='excel-tab')\n        resultdict = dict()\n        # Initialise a dictionary to store all the target sequences\n        sample[self.analysistype].targetsequence = dict()\n        # Go through each BLAST result\n        for row in blastdict:\n            # Calculate the percent identity and extract the bitscore from the row\n            # Percent identity is the (length of the alignment - number of mismatches) / total subject length\n            percentidentity = float('{:0.2f}'.format((float(row['positives']) - float(row['gaps'])) /\n                                                     float(row['subject_length']) * 100))\n            target = row['subject_id']\n            # If the percent identity is greater than the cutoff\n            if percentidentity >= self.cutoff:\n                # Update the dictionary with the target and percent identity\n                resultdict.update({target: percentidentity})\n                # Determine if the orientation of the sequence is reversed compared to the reference\n                if int(row['subject_end']) < int(row['subject_start']):\n                    # Create a sequence object using Biopython\n                    seq = Seq(row['query_sequence'], IUPAC.unambiguous_dna)\n                    # Calculate the reverse complement of the sequence\n                    querysequence = str(seq.reverse_complement())\n                # If the sequence is not reversed, use the sequence as it is in the output\n                else:\n                    querysequence = row['query_sequence']\n                # Add the sequence in the correct orientation to the sample\n                sample[self.analysistype].targetsequence[target] = querysequence\n            # Add the percent identity to the object\n            sample[self.analysistype].blastresults = resultdict\n        # Populate missing results with 'NA' values\n        if len(resultdict) == 0:\n            sample[self.analysistype].blastresults = 'NA'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef uniqueblastparser(self, report, sample):\n        # Encountering the following error: # _csv.Error: field larger than field limit (131072)\n        # According to https://stackoverflow.com/a/15063941, increasing the field limit should fix the issue\n        csv.field_size_limit(sys.maxsize)\n        # Open the sequence profile file as a dictionary\n        blastdict = DictReader(open(report), fieldnames=self.fieldnames, dialect='excel-tab')\n        # Initialise a dictionary to store all the target sequences\n        sample[self.analysistype].targetsequence = dict()\n        sample[self.analysistype].queryranges = dict()\n        sample[self.analysistype].querypercent = dict()\n        sample[self.analysistype].queryscore = dict()\n        sample[self.analysistype].results = dict()\n        # Go through each BLAST result\n        for row in blastdict:\n            print(row)\n            # Calculate the percent identity and extract the bitscore from the row\n            # Percent identity is the (length of the alignment - number of mismatches) / total subject length\n            percentidentity = float('{:0.2f}'.format((float(row['positives'])) /\n                                                     float(row['subject_length']) * 100))\n            target = row['subject_id']\n            contig = row['query_id']\n            high = max([int(row['query_start']), int(row['query_end'])])\n            low = min([int(row['query_start']), int(row['query_end'])])\n            score = row['bit_score']\n            # Create new entries in the blast results dictionaries with the calculated variables\n            row['percentidentity'] = percentidentity\n            row['low'] = low\n            row['high'] = high\n            row['alignment_fraction'] = float('{:0.2f}'.format(float(float(row['alignment_length']) /\n                                                                     float(row['subject_length']) * 100)))\n            # If the percent identity is greater than the cutoff\n            if percentidentity >= self.cutoff:\n                try:\n                    sample[self.analysistype].results[contig].append(row)\n                    # Boolean to store whether the list needs to be updated\n                    append = True\n                    # Iterate through all the ranges in the list - if the new range is different than any of the ranges\n                    # seen before, append it. Otherwise, update the previous ranges with the new, longer range as\n                    # necessary e.g. [2494, 3296] will be updated to [2493, 3296] with [2493, 3293], and\n                    # [2494, 3296] will become [[2493, 3296], [3296, 4132]] with [3296, 4132]\n                    for spot in sample[self.analysistype].queryranges[contig]:\n                        # Update the low value if the new low value is slightly lower than before\n                        if 1 <= (spot[0] - low) <= 100:\n                            # Update the low value\n                            spot[0] = low\n                            # It is not necessary to append\n                            append = False\n                        # Update the previous high value if the new high value is slightly higher than before\n                        elif 1 <= (high - spot[1]) <= 100:\n                            # Update the high value in the list\n                            spot[1] = high\n                            # It is not necessary to append\n                            append = False\n                        # Do not append if the new low is slightly larger than before\n                        elif 1 <= (low - spot[0]) <= 100:\n                            append = False\n                        # Do not append if the new high is slightly smaller than before\n                        elif 1 <= (spot[1] - high) <= 100:\n                            append = False\n                        # Do not append if the high and low are the same as the previously recorded values\n                        elif low == spot[0] and high == spot[1]:\n                            append = False\n                    # If the result appears to be in a new location, add the data to the object\n                    if append:\n                        sample[self.analysistype].queryranges[contig].append([low, high])\n                        sample[self.analysistype].querypercent[contig] = percentidentity\n                        sample[self.analysistype].queryscore[contig] = score\n                # Initialise and populate the dictionary for each contig\n                except KeyError:\n                    sample[self.analysistype].queryranges[contig] = list()\n                    sample[self.analysistype].queryranges[contig].append([low, high])\n                    sample[self.analysistype].querypercent[contig] = percentidentity\n                    sample[self.analysistype].queryscore[contig] = score\n                    sample[self.analysistype].results[contig] = list()\n                    sample[self.analysistype].results[contig].append(row)\n                    sample[self.analysistype].targetsequence[target] = dict()\n                # Determine if the query sequence is in a different frame than the subject, and correct\n                # by setting the query sequence to be the reverse complement\n                if int(row['subject_end']) < int(row['subject_start']):\n                    # Create a sequence object using Biopython\n                    seq = Seq(row['query_sequence'], IUPAC.unambiguous_dna)\n                    # Calculate the reverse complement of the sequence\n                    querysequence = str(seq.reverse_complement())\n                # If the sequence is not reversed, use the sequence as it is in the output\n                else:\n                    querysequence = row['query_sequence']\n                # Add the sequence in the correct orientation to the sample\n                sample[self.analysistype].targetsequence[target] = querysequence", "response": "Parse the unique blast output file and return the unique sequence class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reporter(self):\n        # Create a workbook to store the report. Using xlsxwriter rather than a simple csv format, as I want to be\n        # able to have appropriately sized, multi-line cells\n        workbook = xlsxwriter.Workbook(os.path.join(self.reportpath, '{}.xlsx'.format(self.analysistype)))\n        # New worksheet to store the data\n        worksheet = workbook.add_worksheet()\n        # Add a bold format for header cells. Using a monotype font size 10\n        bold = workbook.add_format({'bold': True, 'font_name': 'Courier New', 'font_size': 10})\n        # Format for data cells. Monotype, size 10, top vertically justified\n        courier = workbook.add_format({'font_name': 'Courier New', 'font_size': 10})\n        courier.set_align('top')\n        # Initialise the position within the worksheet to be (0,0)\n        row = 0\n        # A dictionary to store the column widths for every header\n        columnwidth = dict()\n        for sample in self.metadata:\n            # Reset the column to zero\n            col = 0\n            # Initialise a list to store all the data for each strain\n            data = list()\n            # Initialise a list of all the headers with 'Strain'\n            headers = ['Strain']\n            if sample[self.analysistype].targetnames != 'NA':\n                # Append the sample name to the data list only if the script could find targets\n                data.append(sample.name)\n                if sample[self.analysistype].blastresults != 'NA':\n                    for target in sorted(sample[self.analysistype].targetnames):\n                        # Add the name of the gene to the header\n                        headers.append(target)\n                        try:\n                            # Append the percent identity to the data list\n                            data.append(str(sample[self.analysistype].blastresults[target]))\n                            # Only if the alignment option is selected, for inexact results, add alignments\n                            if self.align and sample[self.analysistype].blastresults[target] != 100.00:\n                                # Align the protein (and nucleotide) sequences to the reference\n                                self.alignprotein(sample, target)\n                                # Add the appropriate headers\n                                headers.extend(['{}_aa_Identity'.format(target),\n                                                '{}_aa_Alignment'.format(target),\n                                                '{}_aa_SNP_location'.format(target),\n                                                '{}_nt_Alignment'.format(target),\n                                                '{}_nt_SNP_location'.format(target)\n                                                ])\n                                # Add the alignment, and the location of mismatches for both nucleotide and amino\n                                # acid sequences\n                                data.extend([sample[self.analysistype].aaidentity[target],\n                                             sample[self.analysistype].aaalign[target],\n                                             sample[self.analysistype].aaindex[target],\n                                             sample[self.analysistype].ntalign[target],\n                                             sample[self.analysistype].ntindex[target],\n                                             ])\n                        # If there are no blast results for the target, add a '-'\n                        except (KeyError, TypeError):\n                            data.append('-')\n                        # If there are no blast results at all, add a '-'\n                        else:\n                            data.append('-')\n            # Write the header to the spreadsheet\n            for header in headers:\n                worksheet.write(row, col, header, bold)\n                # Set the column width based on the longest header\n                try:\n                    columnwidth[col] = len(header)if len(header) > columnwidth[col] else columnwidth[col]\n                except KeyError:\n                    columnwidth[col] = len(header)\n                worksheet.set_column(col, col, columnwidth[col])\n                col += 1\n            # Increment the row and reset the column to zero in preparation of writing results\n            row += 1\n            col = 0\n            # List of the number of lines for each result\n            totallines = list()\n            # Write out the data to the spreadsheet\n            for results in data:\n                worksheet.write(row, col, results, courier)\n                try:\n                    # Counting the length of multi-line strings yields columns that are far too wide, only count\n                    # the length of the string up to the first line break\n                    alignmentcorrect = len(results.split('\\n')[0])\n                    # Count the number of lines for the data\n                    lines = results.count('\\n') if results.count('\\n') >= 1 else 1\n                    # Add the number of lines to the list\n                    totallines.append(lines)\n                # If there are no newline characters, set the width to the length of the string\n                except AttributeError:\n                    alignmentcorrect = len(results)\n                    lines = 1\n                    # Add the number of lines to the list\n                    totallines.append(lines)\n                # Increase the width of the current column, if necessary\n                try:\n                    columnwidth[col] = alignmentcorrect if alignmentcorrect > columnwidth[col] else columnwidth[col]\n                except KeyError:\n                    columnwidth[col] = alignmentcorrect\n                worksheet.set_column(col, col, columnwidth[col])\n                col += 1\n            # Set the width of the row to be the number of lines (number of newline characters) * 12\n            if len(totallines) != 0:\n                worksheet.set_row(row, max(totallines) * 12)\n            else:\n                worksheet.set_row(row, 1)\n            # Increase the row counter for the next strain's data\n            row += 1\n        # Close the workbook\n        workbook.close()", "response": "Creates a. xlsx report using xlsxwriter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alignprotein(self, sample, target):\n        # Initialise dictionaries\n        sample[self.analysistype].dnaseq = dict()\n        sample[self.analysistype].protseq = dict()\n        sample[self.analysistype].ntindex = dict()\n        sample[self.analysistype].aaindex = dict()\n        sample[self.analysistype].ntalign = dict()\n        sample[self.analysistype].aaalign = dict()\n        sample[self.analysistype].aaidentity = dict()\n        # Remove any gaps incorporated into the sequence\n        sample[self.analysistype].targetsequence[target] = \\\n            sample[self.analysistype].targetsequence[target].replace('-', '')\n        # In order to properly translate the nucleotide sequence, BioPython requests that the sequence is a multiple of\n        # three - not partial codons. Trim the sequence accordingly\n        remainder = 0 - len(sample[self.analysistype].targetsequence[target]) % 3\n        seq = sample[self.analysistype].targetsequence[target] if remainder == 0 \\\n            else sample[self.analysistype].targetsequence[target][:remainder]\n        # Set the DNA and protein sequences of the target in the sample\n        sample[self.analysistype].dnaseq[target] = Seq(seq, IUPAC.unambiguous_dna)\n        # Translate the nucleotide sequence\n        sample[self.analysistype].protseq[target] = str(sample[self.analysistype].dnaseq[target].translate())\n        for targetfile in self.targetfiles:\n            # Trim the reference sequence to multiples of three\n            refremainder = 0 - len(self.records[targetfile][target].seq) % 3\n            refseq = str(self.records[targetfile][target].seq) if refremainder % 3 == 0 \\\n                else str(self.records[targetfile][target].seq)[:refremainder]\n            # Translate the nucleotide sequence of the reference sequence\n            refdna = Seq(refseq, IUPAC.unambiguous_dna)\n            refprot = str(refdna.translate())\n            # Use pairwise2 to perform a local alignment with the following parameters:\n            # x     No match parameters. Identical characters have score of 1, otherwise 0.\n            # s     Same open (-1)  and extend (-.1) gap penalties for both sequences\n            ntalignments = pairwise2.align.localxs(seq, refseq, -1, -.1)\n            # Use format_alignment to create a formatted alignment that is subsequently split on newlines e.g.\n            '''\n            ACCGT\n            | ||\n            A-CG-\n            Score=3\n            '''\n            ntformat = (str(format_alignment(*ntalignments[0])).split('\\n'))\n            # Align the nucleotide sequence of the reference (ntalignments[2]) to the sample (ntalignments[0]).\n            # If the corresponding bases match, add a |, otherwise a space\n            ntalignment = ''.join(map(lambda x: '|' if len(set(x)) == 1 else ' ',\n                                      zip(ntformat[0], ntformat[2])))\n            # Create the nucleotide alignment: the sample sequence, the (mis)matches, and the reference sequence\n            sample[self.analysistype].ntalign[target] = self.interleaveblastresults(ntformat[0], ntformat[2])\n            # Regex to determine location of mismatches in the sequences\n            count = 0\n            sample[self.analysistype].ntindex[target] = str()\n            for snp in re.finditer(' ', ntalignment):\n                # If there are many SNPs, then insert line breaks for every 10 SNPs\n                if count <= 10:\n                    sample[self.analysistype].ntindex[target] += str(snp.start()) + ';'\n                else:\n                    sample[self.analysistype].ntindex[target] += '\\n' + str(snp.start()) + ';'\n                    count = 0\n                count += 1\n            # Perform the same steps, except for the amino acid sequence\n            aaalignments = pairwise2.align.localxs(sample[self.analysistype].protseq[target], refprot, -1, -.1)\n            aaformat = (str(format_alignment(*aaalignments[0])).split('\\n'))\n            aaalignment = ''.join(map(lambda x: '|' if len(set(x)) == 1 else ' ',\n                                      zip(aaformat[0], aaformat[2])))\n            sample[self.analysistype].aaidentity[target] = '{:.2f}'\\\n                .format(float(aaalignment.count('|')) / float(len(aaalignment)) * 100)\n            sample[self.analysistype].aaalign[target] = self.interleaveblastresults(aaformat[0], aaformat[2])\n            count = 0\n            sample[self.analysistype].aaindex[target] = str()\n            for snp in re.finditer(' ', aaalignment):\n                if count <= 10:\n                    sample[self.analysistype].aaindex[target] += str(snp.start()) + ';'\n                else:\n                    sample[self.analysistype].aaindex[target] += '\\n' + str(snp.start()) + ';'\n                    count = 0\n                count += 1", "response": "Create the alignment of the sample with the target"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an interleaved string that resembles the BLAST sequence comparisons for the given query and subject.", "response": "def interleaveblastresults(query, subject):\n        \"\"\"\n        Creates an interleaved string that resembles BLAST sequence comparisons\n        :param query: Query sequence\n        :param subject: Subject sequence\n        :return: Properly formatted BLAST-like sequence comparison\n        \"\"\"\n        # Initialise strings to hold the matches, and the final BLAST-formatted string\n        matchstring = ''\n        blaststring = ''\n        # Iterate through the query\n        for i, bp in enumerate(query):\n            # If the current base in the query is identical to the corresponding base in the reference, append a '|'\n            # to the match string, otherwise, append a ' '\n            if bp == subject[i]:\n                matchstring += '|'\n            else:\n                matchstring += ' '\n        # Set a variable to store the progress through the sequence\n        prev = 0\n        # Iterate through the query, from start to finish in steps of 60 bp\n        for j in range(0, len(query), 60):\n            # BLAST results string. The components are: current position (padded to four characters), 'OLC', query\n            # sequence, \\n, matches, \\n, 'ref', subject sequence. Repeated until all the sequence data are present.\n            \"\"\"\n            0000 OLC ATGAAGAAGATATTTGTAGCGGCTTTATTTGCTTTTGTTTCTGTTAATGCAATGGCAGCT\n                     ||||||||||| ||| | |||| ||||||||| || ||||||||||||||||||||||||\n                 ref ATGAAGAAGATGTTTATGGCGGTTTTATTTGCATTAGTTTCTGTTAATGCAATGGCAGCT\n            0060 OLC GATTGTGCAAAAGGTAAAATTGAGTTCTCTAAGTATAATGAGAATGATACATTCACAGTA\n                     ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n                 ref GATTGTGCAAAAGGTAAAATTGAGTTCTCTAAGTATAATGAGAATGATACATTCACAGTA\n            \"\"\"\n            blaststring += '{} OLC {}\\n         {}\\n     ref {}\\n' \\\n                .format('{:04d}'.format(j), query[prev:j + 60], matchstring[prev:j + 60], subject[prev:j + 60])\n            # Update the progress variable\n            prev = j + 60\n        # Return the properly formatted string\n        return blaststring"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef widgetForName(self, name):\n        for iwidget in range(len(self)):\n            if self.widget(iwidget).name() == name:\n                return self.widget(iwidget)", "response": "Gets a widget with the given name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef widgets(self):\n        w = []\n        for i in range(self.count()):\n            w.append(self.widget(i))\n        return w", "response": "Gets all child wigets"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parent(self, refobj):\n        c = cmds.listConnections(\"%s.parent\" % refobj, source=False)\n        return c[0] if c else None", "response": "Return the parent reftrack node of the given reftrack node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the parent of the child reftrack node", "response": "def set_parent(self, child, parent):\n        \"\"\"Set the parent of the child reftrack node\n\n        :param child: the child reftrack node\n        :type child: str\n        :param parent: the parent reftrack node\n        :type parent: str\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        parents = cmds.listConnections(\"%s.parent\" % child, plugs=True, source=True)\n        if parents:\n            # there is only one parent at a time\n            cmds.disconnectAttr(\"%s.parent\" % child, \"%s\" % parents[0])\n        if parent:\n            cmds.connectAttr(\"%s.parent\" % child, \"%s.children\" % parent, force=True, nextAvailable=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the children reftrack nodes of the given reftrack node", "response": "def get_children(self, refobj):\n        \"\"\"Get the children reftrack nodes of the given node\n\n        It is the reverse query of :meth:`RefobjInterface.get_parent`\n\n        :param refobj: the parent reftrack node\n        :type refobj: str\n        :returns: a list with children reftrack nodes\n        :rtype: list\n        :raises: None\n        \"\"\"\n        children = cmds.listConnections(\"%s.children\" % refobj, d=False)\n        if not children:\n            children = []\n        return children"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the entity type of the given reftrack node.", "response": "def get_typ(self, refobj):\n        \"\"\"Return the entity type of the given reftrack node\n\n        See: :data:`MayaRefobjInterface.types`.\n\n        :param refobj: the reftrack node to query\n        :type refobj: str\n        :returns: the entity type\n        :rtype: str\n        :raises: ValueError\n        \"\"\"\n        enum = cmds.getAttr(\"%s.type\" % refobj)\n        try:\n            return JB_ReftrackNode.types[enum]\n        except IndexError:\n            raise ValueError(\"The type on the node %s could not be associated with an available type: %s\" %\n                             (refobj, JB_ReftrackNode.types))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_typ(self, refobj, typ):\n        try:\n            enum = JB_ReftrackNode.types.index(typ)\n        except ValueError:\n            raise ValueError(\"The given type %s could not be found in available types: %\" % (typ, JB_ReftrackNode.types))\n        cmds.setAttr(\"%s.type\" % refobj, enum)", "response": "Set the type of the given reftrack node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_refobj(self, ):\n        n = cmds.createNode(\"jb_reftrack\")\n        cmds.lockNode(n, lock=True)\n        return n", "response": "Create and return a new reftrack node"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef referenced_by(self, refobj):\n        try:\n            ref = cmds.referenceQuery(refobj, referenceNode=True)\n            return ref\n        except RuntimeError as e:\n            if str(e).endswith(\"' is not from a referenced file.\\n\"):\n                return None\n            else:\n                raise e", "response": "Return the reference that holds the given reftrack node. Returns None if the reftrack node is not imported in the current scene."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_refobj(self, refobj):\n        with common.locknode(refobj, lock=False):\n            cmds.delete(refobj)", "response": "Delete the given reftrack node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_current_element(self, ):\n        n = jbscene.get_current_scene_node()\n        if not n:\n            return None\n        tfid = cmds.getAttr(\"%s.taskfile_id\" % n)\n        try:\n            tf = djadapter.taskfiles.get(pk=tfid)\n            return tf.task.element\n        except djadapter.models.TaskFile.DoesNotExist:\n            raise djadapter.models.TaskFile.DoesNotExist(\"Could not find the taskfile that was set on the scene node. Id was %s\" % tfid)", "response": "Returns the currently open Shot or Asset\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect the given reftrack node with the given reference node.", "response": "def set_reference(self, refobj, reference):\n        \"\"\"Connect the given reftrack node with the given refernce node\n\n        :param refobj: the reftrack node to update\n        :type refobj: str\n        :param reference: the reference node\n        :type reference: str\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        refnodeattr = \"%s.referencenode\" % refobj\n        if reference:\n            cmds.connectAttr(\"%s.message\" % reference, refnodeattr, force=True)\n            ns = cmds.referenceQuery(reference, namespace=True)\n            cmds.setAttr(\"%s.namespace\" % refobj, ns, type=\"string\")\n        else:\n            conns = cmds.listConnections(refnodeattr, plugs=True)\n            if not conns:\n                return\n            for c in conns:\n                cmds.disconnectAttr(c, refnodeattr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the reftrack node that the reftrack node is connected to or None if it is imported.", "response": "def get_reference(self, refobj):\n        \"\"\"Return the reference node that the reftrack node is connected to or None if it is imported.\n\n        :param refobj: the reftrack node to query\n        :type refobj: str\n        :returns: the reference node\n        :rtype: str | None\n        :raises: None\n        \"\"\"\n        c = cmds.listConnections(\"%s.referencenode\" % refobj, d=False)\n        return c[0] if c else None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_status(self, refobj):\n        reference = self.get_reference(refobj)\n        return Reftrack.IMPORTED if not reference else Reftrack.LOADED if cmds.referenceQuery(reference, isLoaded=True) else Reftrack.UNLOADED", "response": "Returns the status of the given reftrack node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the taskfile that is loaded and represented by the refobj", "response": "def get_taskfile(self, refobj):\n        \"\"\"Return the taskfile that is loaded and represented by the refobj\n\n        :param refobj: the reftrack node to query\n        :type refobj: str\n        :returns: The taskfile that is loaded in the scene\n        :rtype: :class:`jukeboxcore.djadapter.TaskFile`\n        :raises: None\n        \"\"\"\n        tfid = cmds.getAttr(\"%s.taskfile_id\" % refobj)\n        try:\n            return djadapter.taskfiles.get(pk=tfid)\n        except djadapter.models.TaskFile.DoesNotExist:\n            raise djadapter.models.TaskFile.DoesNotExist(\"Could not find the taskfile that was set on the node %s. Id was %s\" % (refobj, tfid))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect the given reftrack node with the given scene node", "response": "def connect_reftrack_scenenode(self, refobj, scenenode):\n        \"\"\"Connect the given reftrack node with the given scene node\n\n        :param refobj: the reftrack node to connect\n        :type refobj: str\n        :param scenenode: the jb_sceneNode to connect\n        :type scenenode: str\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        conns = [(\"%s.scenenode\" % refobj, \"%s.reftrack\" % scenenode),\n                 (\"%s.taskfile_id\" % scenenode, \"%s.taskfile_id\" % refobj)]\n        for src, dst in conns:\n            if not cmds.isConnected(src, dst):\n                cmds.connectAttr(src, dst, force=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning wheter the given action is restricted for the given reftrack", "response": "def fetch_action_restriction(self, reftrack, action):\n        \"\"\"Return wheter the given action is restricted for the given reftrack\n\n        available actions are:\n\n           ``reference``, ``load``, ``unload``, ``replace``, ``import_reference``, ``import_taskfile``, ``delete``\n\n        If action is not available, True is returned.\n\n        Replace and Delete is always restricted for nested references!\n\n        :param reftrack: the reftrack to query\n        :type reftrack: :class:`Reftrack`\n        :param action: the action to check.\n        :type action: str\n        :returns: True, if the action is restricted\n        :rtype: :class:`bool`\n        :raises: None\n        \"\"\"\n        if action == 'import_reference' and reftrack.status() == Reftrack.UNLOADED:\n            return True\n        if action in ('replace', 'delete', 'import_reference') and reftrack.status() in (Reftrack.LOADED, Reftrack.UNLOADED):\n                tracknode = reftrack.get_refobj()\n                restricted = cmds.referenceQuery(tracknode, isNodeReferenced=True)\n                if restricted:\n                    return True\n        return super(MayaRefobjInterface, self).fetch_action_restriction(reftrack, action)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nframe counts is the core of all the counting operations. It counts on a per - frame basis.", "response": "def frame_counts(self,subsets=None):\n        \"\"\"\n        Frame counts is the core of all the counting operations.  It counts on a per-frame/per-region basis.\n\n        Args:\n            subsets (list): a list of Subset Objects.  if not specified, the phenotypes are used.\n\n        Returns:\n            pandas.DataFrame: A dataframe of count data\n        \"\"\"\n        mergeon = self.cdf.frame_columns+['region_label']\n        if subsets is None:\n            cnts = self.groupby(mergeon+['phenotype_label']).count()[['cell_index']].\\\n                rename(columns={'cell_index':'count'})\n            mr = self.measured_regions\n            mr['_key'] =  1\n            mp = pd.DataFrame({'phenotype_label':self.measured_phenotypes})\n            mp['_key'] = 1\n            mr = mr.merge(mp,on='_key').drop(columns='_key')\n            cnts = mr.merge(cnts,on=mergeon+['phenotype_label'],how='left').fillna(0)\n        else:\n             # Use subsets\n            if isinstance(subsets,SL): subsets=[subsets]\n            cnts = []\n            labels = set([s.label for s in subsets])\n            for x in subsets: \n                if x.label is None: raise ValueError(\"Subsets must be named\")\n            if len(labels) != len(subsets): raise ValueError(\"Subsets must be uniquely named.\")\n            seen_labels = []\n            for sl in subsets:\n                if sl.label in seen_labels: raise ValueError(\"cannot use the same label twice in the subsets list\")\n                seen_labels.append(sl.label)\n\n                df = self.cdf.subset(sl)\n                df = df.groupby(mergeon).count()[['cell_index']].\\\n                    rename(columns={'cell_index':'count'}).reset_index()\n                df = self.measured_regions.merge(df,on=mergeon,how='left').fillna(0)\n                df['phenotype_label'] = sl.label\n                cnts.append(df)\n            cnts = pd.concat(cnts)\n        cnts = cnts[mergeon+['region_area_pixels','phenotype_label','count']]\n        cnts['region_area_mm2'] = cnts.apply(lambda x: \n            (x['region_area_pixels']/1000000)*(self.microns_per_pixel*self.microns_per_pixel),1)\n        cnts['density_mm2'] = cnts.apply(lambda x: np.nan if x['region_area_mm2'] == 0 else x['count']/x['region_area_mm2'],1)\n        # make sure regions of size zero have counts of np.nan\n        cnts.loc[cnts['region_area_pixels']<self.minimum_region_size_pixels,['count','density_mm2']] = np.nan\n        return cnts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_search_page(self, query):\n        query_web_page = Webpage(self.url + self.parse_query(query))\n        query_web_page.get_html_source()  # get html source\n        return query_web_page.source", "response": "Gets HTML source of search page of given query"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the values of the given constants in the current module and object.", "response": "def set(constants):\n    \"\"\"\n    REACH INTO THE MODULES AND OBJECTS TO SET CONSTANTS.\n    THINK OF THIS AS PRIMITIVE DEPENDENCY INJECTION FOR MODULES.\n    USEFUL FOR SETTING DEBUG FLAGS.\n    \"\"\"\n    if not constants:\n        return\n    constants = wrap(constants)\n\n    for k, new_value in constants.leaves():\n        errors = []\n        try:\n            old_value = mo_dots_set_attr(sys.modules, k, new_value)\n            continue\n        except Exception as e:\n            errors.append(e)\n\n        # ONE MODULE IS MISSING, THE CALLING MODULE\n        try:\n            caller_globals = sys._getframe(1).f_globals\n            caller_file = caller_globals[\"__file__\"]\n            if not caller_file.endswith(\".py\"):\n                raise Exception(\"do not know how to handle non-python caller\")\n            caller_module = caller_file[:-3].replace(\"/\", \".\")\n\n            path = split_field(k)\n            for i, p in enumerate(path):\n                if i == 0:\n                    continue\n                prefix = join_field(path[:1])\n                name = join_field(path[i:])\n                if caller_module.endswith(prefix):\n                    old_value = mo_dots_set_attr(caller_globals, name, new_value)\n                    if DEBUG:\n                        from mo_logs import Log\n\n                        Log.note(\n                            \"Changed {{module}}[{{attribute}}] from {{old_value}} to {{new_value}}\",\n                            module=prefix,\n                            attribute=name,\n                            old_value=old_value,\n                            new_value=new_value\n                        )\n                    break\n        except Exception as e:\n            errors.append(e)\n\n        if errors:\n            from mo_logs import Log\n\n            Log.error(\"Can not set constant {{path}}\", path=k, cause=errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef values(self):\n        lower = float(self.lowerSpnbx.value())\n        upper = float(self.upperSpnbx.value())\n        return (lower, upper)", "response": "Gets the user enter max and min values of where the \n        raster points should appear on the y - axis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _delete(self, ):\n        for k in self.keys():\n            try:\n                self[k]._delete()\n            except KeyError:\n                pass\n        if self.__parent is not None:\n            del self.__parent[self.__name]\n            self.__parent = None\n        cmds.deleteUI(self.__menustring)", "response": "Delete the menu and remove it from parent"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a menu or menu item with the given name.", "response": "def create_menu(self, name, parent=None, **kwargs):\n        \"\"\" Creates a maya menu or menu item\n\n        :param name: Used to access a menu via its parent. Unless the nolabel flag is set to True, the name will also become the label of the menu.\n        :type name: str\n        :param parent: Optional - The parent menu. If None, this will create a toplevel menu. If parent menu is a Menu instance, this will create a menu item. Default is None.\n        :type parent: Menu|None\n        :param nolabel: Optional - If nolabel=True, the label flag for the maya command will not be overwritten by name\n        :type nolabel: bool\n        :param kwargs: all keyword arguments used for the cmds.menu/cmds.menuitem command\n        :type kwargs: named arguments\n        :returns: None\n        :rtype: None\n        :raises: errors.MenuExistsError\n        \"\"\"\n        m = Menu(name, parent, **kwargs)\n        if parent is None:\n            self.menus[name] = m\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the specified menu.", "response": "def delete_menu(self, menu):\n        \"\"\" Delete the specified menu\n\n        :param menu:\n        :type menu:\n        :returns:\n        :rtype:\n        :raises:\n        \"\"\"\n        if menu.parent is None:\n            del self.menus[menu.name()]\n        menu._delete()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting all menus managed by this manager.", "response": "def delete_all_menus(self, ):\n        \"\"\" Delete all menues managed by this manager\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        for m in self.menus.itervalues():\n            m._delete()\n        self.menus.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_mismatch(self, entity, *traits):\n        for trait in traits:\n            self.index[trait].add(entity)", "response": "Add a mismatching entity to the index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a matching entity to the index.", "response": "def add_match(self, entity, *traits):\n        \"\"\"\n        Add a matching entity to the index.\n\n        We have to maintain the constraints of the data layout:\n            - `self.mismatch_unknown` must still contain all matched entities\n            - each key of the index must mismatch all known matching entities except those this particular key\n              explicitly includes\n\n        For data layout description, see the class-level docstring.\n\n        :param collections.Hashable entity: an object to be matching the values of `traits_indexed_by`\n        :param list traits: a list of hashable values to index the object with\n        \"\"\"\n        # The index traits of `traits_indexed_by` might have already been used to index some other entities. Those\n        # relations are to be preserved. If the trait was not used to index any entity, we initialize them to mismatch\n        # all matching entities known so far.\n        for trait in traits:\n            if trait not in self.index:\n                self.index[trait] = self.mismatch_unknown.copy()\n\n        # Now each known trait this entity is not matching, will explicitly mismatch currently added entity.\n        for existing_trait in self.index:\n            if existing_trait not in traits:\n                self.index[existing_trait].add(entity)\n\n        # From now on, any new matching or mismatching index will mismatch this entity by default.\n        self.mismatch_unknown.add(entity)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a function that returns the axis associated with a given histogram.", "response": "def hist_axis_func(axis_type: enum.Enum) -> Callable[[Hist], Axis]:\n    \"\"\" Wrapper to retrieve the axis of a given histogram.\n\n    This can be convenient outside of just projections, so it's made available in the API.\n\n    Args:\n        axis_type: The type of axis to retrieve.\n    Returns:\n        Callable to retrieve the specified axis when given a hist.\n    \"\"\"\n    def axis_func(hist: Hist) -> Axis:\n        \"\"\" Retrieve the axis associated with the ``HistAxisRange`` object for a given hist.\n\n        Args:\n            hist: Histogram from which the selected axis should be retrieved.\n            axis_type: Enumeration corresponding to the axis to be restricted. The numerical\n                value of the enum should be axis number (for a THnBase).\n        Returns:\n            ROOT.TAxis: The axis associated with the ``HistAxisRange`` object.\n        \"\"\"\n        # Determine the axis_type value\n        # Use try here instead of checking for a particular type to protect against type changes\n        # (say in the enum)\n        try:\n            # Try to extract the value from an enum\n            hist_axis_type = axis_type.value\n        except AttributeError:\n            # Seems that we received an int, so just use that value\n            hist_axis_type = axis_type\n\n        if hasattr(hist, \"ProjectionND\") and hasattr(hist, \"Projection\"):\n            # THnBase defines ProjectionND and Projection, so we will use those as proxies.\n            # Return the proper THn access\n            #logger.debug(f\"From hist: {hist}, hist_axis_type: {hist_axis_type}, axis: {hist.GetAxis(hist_axis_type.value)}\")\n            return hist.GetAxis(hist_axis_type)\n        else:\n            # If it's not a THn, then it must be a TH1 derived\n            axis_function_map = {\n                TH1AxisType.x_axis.value: hist.GetXaxis,\n                TH1AxisType.y_axis.value: hist.GetYaxis,\n                TH1AxisType.z_axis.value: hist.GetZaxis\n            }\n\n            # Retrieve the axis function and execute it. It is done separately to\n            # clarify any possible errors.\n            return_func = axis_function_map[hist_axis_type]\n            return return_func()\n\n    return axis_func"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef axis(self) -> Callable[[Any], Any]:\n        axis_func = hist_axis_func(\n            axis_type = self.axis_type\n        )\n        return axis_func", "response": "Determine the axis to return based on the hist type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_range_set(self, hist: Hist) -> None:\n        # Do individual assignments to clarify which particular value is causing an error here.\n        axis = self.axis(hist)\n        #logger.debug(f\"axis: {axis}, axis(): {axis.GetName()}\")\n        # Help out mypy\n        assert not isinstance(self.min_val, float)\n        assert not isinstance(self.max_val, float)\n        # Evaluate the functions to determine the values.\n        min_val = self.min_val(axis)\n        max_val = self.max_val(axis)\n        # NOTE: Using SetRangeUser() here was a bug, since I've been passing bin values! In general,\n        #       passing bin values is more flexible, but requires the values to be passed to\n        #       ``apply_func_to_find_bin()`` to be shifted by some small epsilon to get the desired bin.\n        self.axis(hist).SetRange(min_val, max_val)", "response": "Applies the associated range set to the axis of a given histogram."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosuring to determine the bin associated with a value on an axis. It can apply a function to an axis if necessary to determine the proper bin. Otherwise, it can just return a stored value. Note: To properly determine the value, carefully note the information below. In many cases, such as when we want values [2, 5), the values need to be shifted by a small epsilon to retrieve the proper bin. This is done automatically in ``SetRangeUser()``. >>> hist = ROOT.TH1D(\"test\", \"test\", 10, 0, 10) >>> x = 2, y = 5 >>> hist.FindBin(x) 2 >>> hist.FindBin(x+epsilon) 2 >>> hist.FindBin(y) 6 >>> hist.FindBin(y-epsilon) 5 Note that the bin + epsilon on the lower bin is not strictly necessary, but it is used for consistency with the upper bound. Args: func (Callable): Function to apply to the histogram axis. If it is None, the value will be returned. values (int or float): Value to pass to the function. Default: None (in which case, it won't be passed). Returns: Function to be called with an axis to determine the desired bin on that axis.", "response": "def apply_func_to_find_bin(\n        func: Union[None, Callable[..., Union[float, int, Any]]],\n        values: Optional[float] = None\n    ) -> Callable[[Any], Union[float, int]]:\n        \"\"\" Closure to determine the bin associated with a value on an axis.\n\n        It can apply a function to an axis if necessary to determine the proper bin.  Otherwise,\n        it can just return a stored value.\n\n        Note:\n            To properly determine the value, carefully note the information below. In many cases,\n            such as when we want values [2, 5), the values need to be shifted by a small epsilon\n            to retrieve the proper bin. This is done automatically in ``SetRangeUser()``.\n\n            >>> hist = ROOT.TH1D(\"test\", \"test\", 10, 0, 10)\n            >>> x = 2, y = 5\n            >>> hist.FindBin(x)\n            2\n            >>> hist.FindBin(x+epsilon)\n            2\n            >>> hist.FindBin(y)\n            6\n            >>> hist.FindBin(y-epsilon)\n            5\n\n            Note that the bin + epsilon on the lower bin is not strictly necessary, but it is\n            used for consistency with the upper bound.\n\n        Args:\n            func (Callable): Function to apply to the histogram axis. If it is None, the value\n                will be returned.\n            values (int or float): Value to pass to the function. Default: None (in which case,\n                it won't be passed).\n        Returns:\n            Function to be called with an axis to determine the desired bin on that axis.\n        \"\"\"\n        def return_func(axis) -> Any:\n            \"\"\" Apply the stored function and value to a given axis.\n\n            Args:\n                axis (TAxis or similar): Axis to which the function should be applied.\n            Returns:\n                any: The value returned by the function. Often a float or int, but not necessarily.\n            \"\"\"\n            #logger.debug(f\"func: {func}, values: {values}\")\n            if func:\n                if values is not None:\n                    return func(axis, values)\n                else:\n                    return func(axis)\n            else:\n                return values\n\n        return return_func"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call_projection_function(self, hist: Hist) -> Hist:\n        # Restrict projection axis ranges\n        for axis in self.projection_axes:\n            logger.debug(f\"Apply projection axes hist range: {axis.name}\")\n            axis.apply_range_set(hist)\n\n        projected_hist = None\n        if hasattr(hist, \"ProjectionND\") and hasattr(hist, \"Projection\"):\n            # THnBase defines ProjectionND and Projection, so we will use those as proxies.\n            projected_hist = self._project_THn(hist = hist)\n        elif hasattr(hist, \"ProjectionZ\") and hasattr(hist, \"Project3D\"):\n            # TH3 defines ProjectionZ and Project3D, so we will use those as proxies.\n            projected_hist = self._project_TH3(hist = hist)\n        elif hasattr(hist, \"ProjectionX\") and hasattr(hist, \"ProjectionY\"):\n            # TH2 defines ProjectionX and ProjectionY, so we will use those as proxies.\n            projected_hist = self._project_TH2(hist = hist)\n        else:\n            raise TypeError(type(hist), f\"Could not recognize hist {hist} of type {type(hist)}\")\n\n        # Cleanup restricted axes\n        self.cleanup_cuts(hist, cut_axes = self.projection_axes)\n\n        return projected_hist", "response": "Calls the actual projection function for the given histogram."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform the actual THn - > THn - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1 - > THn1", "response": "def _project_THn(self, hist: Hist) -> Any:\n        \"\"\" Perform the actual THn -> THn or TH1 projection.\n\n        This projection could be to 1D, 2D, 3D, or ND.\n\n        Args:\n            hist (ROOT.THnBase): Histogram from which the projections should be performed.\n        Returns:\n            ROOT.THnBase or ROOT.TH1: The projected histogram.\n        \"\"\"\n        # THnBase projections args are given as a list of axes, followed by any possible options.\n        projection_axes = [axis.axis_type.value for axis in self.projection_axes]\n\n        # Handle ROOT THnBase quirk...\n        # 2D projection are called as (y, x, options), so we should reverse the order so it performs\n        # as expected\n        if len(projection_axes) == 2:\n            # Reverses in place\n            projection_axes.reverse()\n\n        # Test calculating errors\n        # Add \"E\" to ensure that errors will be calculated\n        args = projection_axes + [\"E\"]\n        # Do the actual projection\n        logger.debug(f\"hist: {hist.GetName()} args: {args}\")\n\n        if len(projection_axes) > 3:\n            # Project into a THnBase object.\n            projected_hist = hist.ProjectionND(*args)\n        else:\n            # Project a TH1 derived object.\n            projected_hist = hist.Projection(*args)\n\n        return projected_hist"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming the actual TH3 projection.", "response": "def _project_TH3(self, hist: Hist) -> Any:\n        \"\"\" Perform the actual TH3 -> TH1 projection.\n\n        This projection could be to 1D or 2D.\n\n        Args:\n            hist (ROOT.TH3): Histogram from which the projections should be performed.\n        Returns:\n            ROOT.TH1: The projected histogram.\n        \"\"\"\n        # Axis length validation\n        if len(self.projection_axes) < 1 or len(self.projection_axes) > 2:\n            raise ValueError(len(self.projection_axes), \"Invalid number of axes\")\n\n        # Need to concatenate the names of the axes together\n        projection_axis_name = \"\"\n        for axis in self.projection_axes:\n            # Determine the axis name based on the name of the axis type.\n            # [:1] returns just the first letter. For example, we could get \"xy\" if the first axis as\n            # x_axis and the second was y_axis.\n            # NOTE: Careful. This depends on the name of the enumerated values!!! Since this isn't terribly\n            #       safe, we then perform additional validation on the same to ensure that it is one of the\n            #       expected axis names.\n            proj_axis_name = axis.axis_type.name[:1]\n            if proj_axis_name not in [\"x\", \"y\", \"z\"]:\n                raise ValueError(f\"Projection axis name {proj_axis_name} is not 'x', 'y', or 'z'. Please check your configuration.\")\n            projection_axis_name += proj_axis_name\n\n        # Handle ROOT Project3D quirk...\n        # 2D projection are called as (y, x, options), so we should reverse the order so it performs\n        # as expected.\n        # NOTE: This isn't well documented in TH3. It is instead described in THnBase.Projection(...)\n        if len(self.projection_axes) == 2:\n            # Reverse the axes\n            projection_axis_name = projection_axis_name[::-1]\n\n        # Do the actual projection\n        logger.info(f\"Projecting onto axes \\\"{projection_axis_name}\\\" from hist {hist.GetName()}\")\n        projected_hist = hist.Project3D(projection_axis_name)\n\n        return projected_hist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _project_TH2(self, hist: Hist) -> Any:\n        if len(self.projection_axes) != 1:\n            raise ValueError(len(self.projection_axes), \"Invalid number of axes\")\n\n        #logger.debug(f\"self.projection_axes[0].axis: {self.projection_axes[0].axis}, axis range name: {self.projection_axes[0].name}, axis_type: {self.projection_axes[0].axis_type}\")\n        # NOTE: We cannot use TH3.ProjectionZ(...) because it has different semantics than ProjectionX\n        #       and ProjectionY. In particular, it doesn't respect the axis limits of axis onto which it\n        #       is projected.  So we have to separate the projection by histogram type as opposed to axis\n        #       length.\n        projection_func_map = {\n            TH1AxisType.x_axis.value: hist.ProjectionX,\n            TH1AxisType.y_axis.value: hist.ProjectionY\n        }\n\n        # Determine the axis_type value\n        # Use try here instead of checking for a particular type to protect against type changes (say\n        # in the enum)\n        try:\n            # Try to extract the value from an enum\n            axis_type = self.projection_axes[0].axis_type.value\n        except ValueError:\n            # Seems that we received an int, so just use that value\n            axis_type = self.axis_type  # type: ignore\n\n        projection_func = projection_func_map[axis_type]\n\n        # Do the actual projection\n        logger.info(f\"Projecting onto axis range {self.projection_axes[0].name} from hist {hist.GetName()}\")\n        projected_hist = projection_func()\n\n        return projected_hist", "response": "Perform the actual TH2 - > TH1 projection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _project_observable(self, input_key: str,\n                            input_observable: Any,\n                            get_hist_args: Dict[str, Any] = None,\n                            projection_name_args: Dict[str, Any] = None,\n                            **kwargs) -> Hist:\n        \"\"\" Perform a projection for a single observable.\n\n        Note:\n            All cuts on the original histograms will be reset when this function is completed.\n\n        Args:\n            input_key: Key to describe the input observable.\n            input_observable: Observable to project from.\n            get_hist_args: Arguments to pass to ``get_hist(...)``. Made available so the args can be cached\n                to avoid a ``deepcopy`` when looping. Default: None. In this case, they will be retrieved\n                automatically.\n            projection_name_args: Arguments to pass to ``projection_name(...)``. Made available so the args\n                can be cached to avoid a ``deepcopy`` when looping. Default: None. In this case, they will be\n                retrieved automatically.\n            kwargs: Additional named args to be passed to projection_name(...) and output_key_name(...).\n        Returns:\n            The projected histogram.\n        \"\"\"\n        # Validation of other optional arguments.\n        if get_hist_args is None:\n            get_hist_args = copy.deepcopy(kwargs)\n        if projection_name_args is None:\n            projection_name_args = copy.deepcopy(kwargs)\n\n        # Retrieve histogram\n        # We update ``input_observable`` in ``get_hist_args`` every loop, so we don't have to worry\n        # about passing the wrong observable.\n        get_hist_args.update({\"observable\": input_observable})\n        hist = self.get_hist(**get_hist_args)\n\n        # Define projection name\n        projection_name_args.update(self.projection_information)\n        # In principle, we could have overwritten one of the kwargs, so we ensure with one of the other\n        # updates, so we update it again to be certain.\n        projection_name_args.update(kwargs)\n        # Put the values included by default last to ensure nothing overwrites these values\n        projection_name_args.update({  # type: ignore\n            \"input_key\": input_key,\n            \"input_observable\": input_observable,\n            \"input_hist\": hist\n        })\n        projection_name = self.projection_name(**projection_name_args)\n\n        # First apply the cuts\n        # Restricting the range with SetRange(User) works properly for both THn and TH1.\n        logger.debug(f\"hist: {hist}\")\n        for axis in self.additional_axis_cuts:\n            logger.debug(f\"Apply additional axis hist range: {axis.name}\")\n            axis.apply_range_set(hist)\n\n        # We need to ensure that it isn't empty so at least one project occurs\n        if self.projection_dependent_cut_axes == []:\n            self.projection_dependent_cut_axes.append([])\n\n        # Validate the projection dependent cut axes\n        # It is invalid to have PDCA on the same axes as the projection axes.\n        duplicated_axes = [\n            PDCA\n            for PA in self.projection_axes\n            for PDCA_group in self.projection_dependent_cut_axes\n            for PDCA in PDCA_group\n            if PDCA.axis_type == PA.axis_type\n        ]\n        if duplicated_axes:\n            raise ValueError(\n                f\"Axis {duplicated_axes} is in the projection axes and the projection dependent cut axes.\"\n                \" This configuration is not allowed, as the range in the PDCA will be overwritten by the projection axes!\"\n                \" Please revise your configuration.\"\n            )\n\n        # Perform the projections\n        hists = []\n        for i, axes in enumerate(self.projection_dependent_cut_axes):\n            # Projection dependent range set\n            for axis in axes:\n                logger.debug(f\"Apply projection dependent hist range: {axis.name}\")\n                axis.apply_range_set(hist)\n\n            # Do the projection\n            projected_hist = self.call_projection_function(hist)\n            projected_hist.SetName(f\"{projection_name}_{i}\")\n\n            hists.append(projected_hist)\n\n            # Cleanup projection dependent cuts (although they should be set again on the next\n            # iteration of the loop)\n            self.cleanup_cuts(hist, cut_axes = axes)\n\n        # Cleanup the rest of the cuts\n        self.cleanup_cuts(hist, cut_axes = self.additional_axis_cuts)\n\n        # Combine all of the projections together\n        output_hist = hists[0]\n        for temp_hist in hists[1:]:\n            output_hist.Add(temp_hist)\n\n        # Final settings\n        output_hist.SetName(projection_name)\n        # Ensure that the hist doesn't get deleted by ROOT\n        # A reference to the histogram within python may not be enough\n        output_hist.SetDirectory(0)\n\n        return output_hist, projection_name, projection_name_args", "response": "Perform a single projection of a single observable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef project(self, **kwargs: Dict[str, Any]) -> Union[Hist, Dict[str, Hist]]:\n        if self.single_observable_projection:\n            return self._project_single_observable(**kwargs)\n        else:\n            return self._project_dict(**kwargs)", "response": "Perform the requested projections."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefine the projection name for this projector.", "response": "def projection_name(self, **kwargs: Dict[str, Any]) -> str:\n        \"\"\" Define the projection name for this projector.\n\n        Note:\n            This function is just a basic placeholder and likely should be overridden.\n\n        Args:\n            kwargs: Projection information dict combined with additional arguments passed to the\n                projection function.\n        Returns:\n            Projection name string formatted with the passed options. By default, it returns\n                ``projection_name_format`` formatted with the arguments to this function.\n        \"\"\"\n        return self.projection_name_format.format(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_hist(self, observable: Any, **kwargs: Dict[str, Any]) -> Any:\n        return observable", "response": "Returns the ROOT histogram that may be stored in some object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the key under which the output object should be stored. Note: This function is just a basic placeholder which returns the projection name and likely should be overridden. Args: input_key: Key of the input hist in the input dict output_hist: The output histogram projection_name: Projection name for the output histogram kwargs: Projection information dict combined with additional arguments passed to the projection function. Returns: Key under which the output object should be stored. By default, it returns the projection name.", "response": "def output_key_name(self, input_key: str, output_hist: Hist, projection_name: str, **kwargs) -> str:\n        \"\"\" Returns the key under which the output object should be stored.\n\n        Note:\n            This function is just a basic placeholder which returns the projection name\n            and likely should be overridden.\n\n        Args:\n            input_key: Key of the input hist in the input dict\n            output_hist: The output histogram\n            projection_name: Projection name for the output histogram\n            kwargs: Projection information dict combined with additional arguments passed to\n                the projection function.\n        Returns:\n            Key under which the output object should be stored. By default, it returns the\n                projection name.\n        \"\"\"\n        return projection_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef output_hist(self, output_hist: Hist, input_observable: Any, **kwargs: Dict[str, Any]) -> Union[Hist, Any]:\n        return output_hist", "response": "This function returns an output object. It should store the output_hist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_key(soup):\n    '''\n    On each chapter page, a <script> tag is inserted that overrides the\n    encryption key, so we'll need to find that. Only fall back to default key\n    if such a tag is not found.\n    '''\n\n    crypto_tag = soup.find(_crypto_tag)\n    if crypto_tag is None:\n        return _default_key\n\n    pat = re.compile('\\[\"(.+)\"\\]')\n    keys = pat.findall(crypto_tag.text)\n    if len(keys) > 0:\n        unhashed_key = keys[-1].encode().decode('unicode_escape')\n        return _generate_sha(unhashed_key)\n    else:\n        return _default_key", "response": "Get the encryption key for the current page."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving the repos to the new team and add them to the old team", "response": "def run():\n    \"\"\"Move the repos\"\"\"\n    args = parse_args()\n\n    codetools.setup_logging(args.debug)\n\n    global g\n    g = pygithub.login_github(token_path=args.token_path, token=args.token)\n    org = g.get_organization(args.org)\n\n    # only iterate over all teams once\n    try:\n        teams = list(org.get_teams())\n    except github.RateLimitExceededException:\n        raise\n    except github.GithubException as e:\n        msg = 'error getting teams'\n        raise pygithub.CaughtOrganizationError(org, e, msg) from None\n\n    old_team = find_team(teams, args.oldteam)\n    new_team = find_team(teams, args.newteam)\n\n    move_me = args.repos\n    debug(len(move_me), 'repos to be moved')\n\n    added = []\n    removed = []\n    for name in move_me:\n        try:\n            r = org.get_repo(name)\n        except github.RateLimitExceededException:\n            raise\n        except github.GithubException as e:\n            msg = \"error getting repo by name: {r}\".format(r=name)\n            raise pygithub.CaughtOrganizationError(org, e, msg) from None\n\n        # Add team to the repo\n        debug(\"Adding {repo} to '{team}' ...\".format(\n            repo=r.full_name,\n            team=args.newteam\n        ))\n\n        if not args.dry_run:\n            try:\n                new_team.add_to_repos(r)\n                added += r.full_name\n                debug('  ok')\n            except github.RateLimitExceededException:\n                raise\n            except github.GithubException:\n                debug('  FAILED')\n\n        if old_team.name in 'Owners':\n            warn(\"Removing repo {repo} from team 'Owners' is not allowed\"\n                 .format(repo=r.full_name))\n\n        debug(\"Removing {repo} from '{team}' ...\".format(\n            repo=r.full_name,\n            team=args.oldteam\n        ))\n\n        if not args.dry_run:\n            try:\n                old_team.remove_from_repos(r)\n                removed += r.full_name\n                debug('  ok')\n            except github.RateLimitExceededException:\n                raise\n            except github.GithubException:\n                debug('  FAILED')\n\n    info('Added:', added)\n    info('Removed:', removed)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef poll_values():\n    subscription = processor.create_parameter_subscription([\n        '/YSS/SIMULATOR/BatteryVoltage1'\n    ])\n\n    sleep(5)\n    print('Latest value:')\n    print(subscription.get_value('/YSS/SIMULATOR/BatteryVoltage1'))\n\n    sleep(5)\n    print('Latest value:')\n    print(subscription.get_value('/YSS/SIMULATOR/BatteryVoltage1'))", "response": "Shows how to poll values from the subscription."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef receive_callbacks():\n    def print_data(data):\n        for parameter in data.parameters:\n            print(parameter)\n\n    processor.create_parameter_subscription('/YSS/SIMULATOR/BatteryVoltage1',\n                                            on_data=print_data)\n    sleep(5)", "response": "Shows how to receive callbacks on value updates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef manage_subscription():\n    subscription = processor.create_parameter_subscription([\n        '/YSS/SIMULATOR/BatteryVoltage1'\n    ])\n\n    sleep(5)\n\n    print('Adding extra items to the existing subscription...')\n    subscription.add([\n        '/YSS/SIMULATOR/Alpha',\n        '/YSS/SIMULATOR/BatteryVoltage2',\n        'MDB:OPS Name/SIMULATOR_PrimBusVoltage1',\n    ])\n\n    sleep(5)\n\n    print('Shrinking subscription...')\n    subscription.remove('/YSS/SIMULATOR/Alpha')\n\n    print('Cancelling the subscription...')\n    subscription.cancel()\n\n    print('Last values from cache:')\n    print(subscription.get_value('/YSS/SIMULATOR/BatteryVoltage1'))\n    print(subscription.get_value('/YSS/SIMULATOR/BatteryVoltage2'))\n    print(subscription.get_value('/YSS/SIMULATOR/Alpha'))\n    print(subscription.get_value('MDB:OPS Name/SIMULATOR_PrimBusVoltage1'))", "response": "Shows how to interact with a parameter subscription."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eigenvalues_hadamard(matrix1, matrix2):\n\n    matrix1 = array(matrix1)  # as arrays\n    matrix2 = array(matrix2)\n\n    eig_a = eigvalsh(matrix1)  # eigenvalues (optimized for Hermitian matrices)\n    eig_b = eigvalsh(matrix2)\n\n    return eig_a, eig_b", "response": "Computes the Hadamard product of two matrices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the zoom scale of the image in ms.", "response": "def setPixelScale(self, pxms):\n        \"\"\"Sets the zoom scale\n\n        :param pxms: number of pixels per ms\n        :type pxms: int\n        :returns: float -- the miliseconds between grid lines\n        \"\"\"\n        pxms = float(pxms)/2\n        self.pixelsPerms = pxms\n        if pxms*self.gridms < GRID_PIXEL_MIN:\n            self.gridms = self.gridms*2\n        elif pxms*self.gridms > GRID_PIXEL_MAX:\n            self.gridms = self.gridms/2\n        self._viewIsDirty = True\n        self.viewport().update()\n\n        return self.gridms"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setModel(self, model):\n        super(StimulusView, self).setModel(model)\n        self.setSelectionModel(ComponentSelectionModel(model))\n        # initialize nested list to appropriate size\n        self._rects = [[None] * self.model().columnCountForRow(x) for x in range(self.model().rowCount())]\n\n        self._viewIsDirty = True\n        self._calculateRects()", "response": "Sets the model this view represents."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the top left coordinates of the item for the given index", "response": "def indexXY(self, index):\n        \"\"\"Returns the top left coordinates of the item for the given index\n\n        :param index: index for the item\n        :type index: :qtdoc:`QModelIndex`\n        :returns: (int, int) -- (x, y) view coordinates of item\n        \"\"\"\n        rect = self.visualRect(index)\n        return rect.x(), rect.y()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the index of the component at point relative to view coordinates.", "response": "def indexAt(self, point):\n        \"\"\"Returns the index of the component at *point* relative to view coordinates.\n        If there is None, and empty index is returned. :qtdoc:`Re-implemented<QAbstractItemView.indexAt>`\n\n        :param point: the point, in view coordinates, to find an index for\n        :type point: :qtdoc:`QPoint`\n        :returns: :qtdoc:`QModelIndex`\n        \"\"\"\n        # Transform the view coordinates into contents widget coordinates.\n        wx = point.x() + self.horizontalScrollBar().value()\n        wy = point.y() + self.verticalScrollBar().value()\n        self._calculateRects()\n        # naive search\n        for row in range(self.model().rowCount(self.rootIndex())):\n            for col in range(self.model().columnCountForRow(row)):\n                if self._rects[row][col].contains(wx, wy):\n                    return self.model().index(row, col, self.rootIndex())\n\n        return QtCore.QModelIndex()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the nearest index to point in the view and returns the row and column of the item.", "response": "def splitAt(self, point):\n        \"\"\"Gets the nearest index to *point*, *point* does not have to be over \n        an item. index can be +1 more in row and/or column than existing items\n\n        :param point: any point within the view, in view coordinates\n        :type point: :qtdoc:`QPoint`\n        :returns: (int, int) -- (row, column) of the nearest index\n        \"\"\"\n        wx = point.x() + self.horizontalScrollBar().value()\n        wy = point.y() + self.verticalScrollBar().value()\n\n        row = wy/(ROW_HEIGHT + ROW_SPACE)\n        if row > self.model().rowCount(self.rootIndex()) - 1:\n            row = self.model().rowCount(self.rootIndex())\n        for col in range(self.model().columnCountForRow(row)):\n            if self._rects[row][col].contains(wx, wy):\n                return (row, col)\n        return row, self.model().columnCountForRow(row)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the rectangle that is the bounding box of the item at index.", "response": "def visualRect(self, index):\n        \"\"\"The rectangle for the bounds of the item at *index*. :qtdoc:`Re-implemented<QAbstractItemView.visualRect>`\n\n        :param index: index for the rect you want\n        :type index: :qtdoc:`QModelIndex`\n        :returns: :qtdoc:`QRect` -- rectangle of the borders of the item\n        \"\"\"\n        if len(self._rects[index.row()]) -1 < index.column() or index.row() == -1:\n            #Er, so I don't know why this was getting called with index -1\n            return QtCore.QRect()\n    \n        return self.visualRectRC(index.row(),index.column())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the rectangle that is used to visualize the item at row column", "response": "def visualRectRC(self, row, column):\n        \"\"\"The rectangle for the bounds of the item at *row*, *column*\n\n        :param row: row of the item\n        :type row: int\n        :param column: column of the item\n        :type column: int\n        :returns: :qtdoc:`QRect` -- rectangle of the borders of the item\n        \"\"\"\n        rect = self._rects[row][column]\n        if rect.isValid():\n            return QtCore.QRect(rect.x() - self.horizontalScrollBar().value(),\n                         rect.y() - self.verticalScrollBar().value(),\n                         rect.width(), rect.height())\n        else:\n            return rect"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmark view for repaint.", "response": "def dataChanged(self, topleft, bottomright):\n        \"\"\"Marks view for repaint. :qtdoc:`Re-implemented<QAbstractItemView.dataChanged>`\"\"\"\n        self._viewIsDirty = True\n        super(StimulusView, self).dataChanged(topleft, bottomright)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmark view for repaint.", "response": "def rowsAboutToBeRemoved(self, parent, start, end):\n        \"\"\"Marks view for repaint. :qtdoc:`Re-implemented<QAbstractItemView.rowsAboutToBeRemoved>`\"\"\"\n        self._viewIsDirty = True\n        super(StimulusView, self).rowsAboutToBeRemoved(parent, start, end)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscroll the mouse to the specified index.", "response": "def scrollTo(self, index, ScrollHint):\n        \"\"\":qtdoc:`Re-implemented<QAbstractItemView.scrollTo>`\"\"\"\n        # copied verbatim from chart example\n        area = self.viewport().rect()\n        rect = self.visualRect(index)\n\n        if rect.left() < area.left():\n            self.horizontalScrollBar().setValue(\n                self.horizontalScrollBar().value() + rect.left() - area.left())\n        elif rect.right() > area.right():\n            self.horizontalScrollBar().setValue(\n                self.horizontalScrollBar().value() + min(\n                    rect.right() - area.right(), rect.left() - area.left()))\n\n        if rect.top() < area.top():\n            self.verticalScrollBar().setValue(\n                self.verticalScrollBar().value() + rect.top() - area.top())\n        elif rect.bottom() > area.bottom():\n            self.verticalScrollBar().setValue(\n                self.verticalScrollBar().value() + min(\n                    rect.bottom() - area.bottom(), rect.top() - area.top()))\n\n        self.viewport().update()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef paintEvent(self, event):\n        selections = self.selectionModel()\n        option = self.viewOptions()\n        state = option.state\n\n        if self.parentWidget() is not None:\n            background = self.parentWidget().palette().color(1)\n        else:\n            background = option.palette.base()\n\n        foreground = QtGui.QPen(option.palette.color(QtGui.QPalette.WindowText))\n        textPen = QtGui.QPen(option.palette.color(QtGui.QPalette.Text))\n        highlightedPen = QtGui.QPen(option.palette.color(QtGui.QPalette.HighlightedText))\n\n        painter = QtGui.QPainter(self.viewport())\n        painter.setRenderHint(QtGui.QPainter.Antialiasing)\n\n        self._calculateRects()\n\n        viewrect = event.rect()\n        painter.fillRect(viewrect, background)\n        painter.setPen(foreground)\n\n        fontsz = self.font().pointSize()  \n\n        # draw grid lines\n        wid = int(max(viewrect.width(), self._width))\n        nlines = int((wid/self.pixelsPerms)/self.gridms)\n        y0 = viewrect.y()\n        y1 = viewrect.y() + viewrect.height()\n        for iline in range(1, nlines + 1):\n            x = (iline * self.gridms * self.pixelsPerms) - self.horizontalScrollBar().value()\n            painter.drawLine(x, y0+fontsz+2, x, y1)\n            painter.drawText(x-5, y0+fontsz+1, str(iline*self.gridms))\n\n        # painting of components\n        for row in range(self.model().rowCount(self.rootIndex())):\n            for col in range(self.model().columnCountForRow(row)):\n                index = self.model().index(row, col, self.rootIndex())\n                component = self.model().data(index, QtCore.Qt.UserRole)\n                if component is not None:\n                    option = self.viewOptions()\n                    option.rect = self.visualRectRC(row, col)\n                    self.itemDelegate().paint(painter, option, index)\n\n        # highlight selected components\n        region = self.visualRegionForSelection(self.selectionModel().selection())\n\n        painter.save()\n        painter.setClipRegion(region)\n        painter.setOpacity(0.5)\n        painter.fillRect(viewrect, QtCore.Qt.blue)\n        painter.restore()\n\n        if self.dragline is not None:\n            pen = QtGui.QPen(QtCore.Qt.blue)\n            pen.setWidth(3)\n            painter.setPen(pen)\n            painter.drawLine(self.dragline)", "response": "Custom painting draws the entire view."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mouseDoubleClickEvent(self, event):\n        if self.mode == BuildMode:\n            if event.button() == QtCore.Qt.LeftButton:\n                index = self.indexAt(event.pos())\n                self.edit(index)", "response": "Launches an editor for the component if the mouse cursor is over an item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grabImage(self, index):\n        # rect = self._rects[index.row()][index.column()]\n        rect = self.visualRect(index)\n        pixmap = QtGui.QPixmap()\n        pixmap = pixmap.grabWidget(self, rect)     \n        return pixmap", "response": "Grabs an image of the item at index"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mousePressEvent(self, event):\n        if self.mode == BuildMode:\n            super(StimulusView, self).mousePressEvent(event)\n        else:\n            # select and de-select components\n            index = self.indexAt(event.pos())\n            if index.isValid():\n                self.selectionModel().select(index, QtGui.QItemSelectionModel.Toggle)\n                comp = self.model().data(index, AbstractDragView.DragRole)\n                self.componentSelected.emit(comp)\n                self.hintRequested.emit('Click components to toggle more members of auto-parameter\\n\\n-or-\\n\\nEdit fields of auto-parameter (parameter type should be selected first)')", "response": "In Auto - parameter selection mode mouse press over an item emits\n        componentSelected"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emptySelection(self, empty):\n        self.setEnabled(not empty)\n        if empty:\n            # self.clearSelection()\n            # Clear selection doesn't work? But removing individually does\n            m = self.selectionModel()\n            for index in m.selectedIndexes():\n                m.select(index, QtGui.QItemSelectionModel.Deselect)\n            self.hintRequested.emit('To add a parameter, Drag \"Add\" onto empty auto-parameter table')", "response": "Enables the view if not empty clears the current selection and disables the view if is emtpy*"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the current selection model and updates the viewport with the given list of components.", "response": "def updateSelectionModel(self, components):\n        \"\"\"Creates a new selection model and adds *components* to it\n\n        :param components: components in this view to add to the selection\n        :type components: list<:class:`AbstractStimulusComponent<sparkle.stim.abstract_component.AbstractStimulusComponent>`\n        \"\"\"\n        # selmodel = self.selectionModel()\n        # selmodel.clearSelection()\n        selmodel = ComponentSelectionModel(self.model())\n        self.setSelectionModel(selmodel)\n        for comp in components:\n            selmodel.selectComponent(comp)\n        self.viewport().update()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cursor(self, pos):\n        index = self.splitAt(pos)\n\n        if len(self._rects[index[0]])-1 < index[1]:\n            if index[1] == 0:\n                # empty row\n                x = 0\n            else:\n                rect = self._rects[index[0]][index[1]-1]\n                x = rect.x() + rect.width()\n        else:\n            rect = self._rects[index[0]][index[1]]\n            x = rect.x()\n\n        y0 = index[0]*(ROW_HEIGHT + ROW_SPACE) + ROW_SPACE\n        y1 = y0 + ROW_HEIGHT\n\n        # adjust for scrolled viewport\n        x -= self.horizontalScrollBar().value()\n        y0 -= self.verticalScrollBar().value()\n        y1 -= self.verticalScrollBar().value()\n        \n        return QtCore.QLine(x,y0,x,y1)", "response": "Returns a line for the cursor at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the dropped component into the model.", "response": "def dropped(self, component, event):\n        \"\"\"Adds the dropped *component* into the model. \n        :meth:`Re-implemented<sparkle.gui.abstract_drag_view.AbstractDragView.dropped>`\n        \"\"\"\n        if isinstance(component, AbstractStimulusComponent):\n            row, col = self.splitAt(event.pos())\n            index = self.model().createIndex(row, col, component)\n            \n            self.model().insertComponent(index, component)\n\n            if isinstance(event.source(), DragLabel):\n                if component.__class__.__name__ in self._componentDefaults:\n                    component.loadState(self._componentDefaults[component.__class__.__name__])\n                self.edit(index)\n\n            self._viewIsDirty = True\n            self.viewport().update()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setMode(self, mode):\n        self.mode = mode\n        if mode == BuildMode:\n            self.setSelectionMode(QtGui.QAbstractItemView.SingleSelection)\n            self.setSelectionModel(QtGui.QItemSelectionModel(self.model()))\n            self.setEnabled(True)\n            self.model().updateComponentStartVals()\n        else:\n            self.model().purgeAutoSelected()\n            self.setSelectionModel(ComponentSelectionModel(self.model()))\n            self.setSelectionMode(QtGui.QAbstractItemView.MultiSelection)", "response": "Sets the mode for this view."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visualRegionForSelection(self, selection):\n        region = QtGui.QRegion()\n        for index in selection.indexes():\n            region = region.united(self._rects[index.row()][index.column()])\n\n        return region", "response": "Gets the region of all of the components in selection"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmark repaint needed. :qtdoc:`Re-implemented<QWidget.resizeEvent>`", "response": "def resizeEvent(self, event):\n        \"\"\"Mark repaint needed. :qtdoc:`Re-implemented<QWidget.resizeEvent>`\"\"\"\n        self._viewIsDirty = True\n        super(StimulusView, self).resizeEvent(event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef updateVocalAuto(self, component, files):\n        auto_model = self.model().autoParams()\n        row = auto_model.fileParameter(component)\n        if len(files) > 1:\n            clean_component = self.model().data(self.model().indexByComponent(component), AbstractDragView.DragRole)\n            p = {'parameter' : 'filename',\n                 'names' : files,\n                 'selection' : [clean_component]\n            }\n            if row is None:\n                auto_model.insertItem(auto_model.index(0,0), p)\n            else:\n                auto_model.setData(auto_model.index(row,0),p)\n        elif row is not None:\n            # remove the autoparameter\n            auto_model.removeRow(row)\n        # if row is none and len(files) == 1 then we don't need to do anything\n        self.countChanged.emit()", "response": "Updates the auto - parameter with selected component to have\n        files*. Adds auto - parameter if not already present."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the contents of the item at the given index.", "response": "def paint(self, painter, option, index):\n        \"\"\"Uses the :meth:`paint<sparkle.gui.stim.components.qcomponents.QStimulusComponent.paint>` \n        method of the component it represents to fill in an appropriately \n        sized rectange. :qtdoc:`Re-implemented<QStyledItemDelegate.paint>`\"\"\"\n        component = index.model().data(index, role=QtCore.Qt.UserRole)\n        painter.drawRect(option.rect)\n\n        component.paint(painter, option.rect, option.palette)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsizing based on component duration and a fixed height", "response": "def sizeHint(self, option, index):\n        \"\"\"Size based on component duration and a fixed height\"\"\"\n        # calculate size by data component\n        component = index.internalPointer()\n        width = self.component.duration() * self.pixelsPerms*1000\n        return QtCore.QSize(width, 50)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an editor in a separate window specific for the component parameters that this delegate represents.", "response": "def createEditor(self, parent, option, index):\n        \"\"\"Creates an editor in a separate window, specific for the component\n        type this delegate represents. :qtdoc:`Re-implemented<QStyledItemDelegate.createEditor>`\"\"\"\n        # bring up separate window for component parameters\n        view = parent.parentWidget()\n        component = view.model().data(index)\n\n        if component is not None:\n            editor = component.showEditor()\n        else:\n            print 'delegate data type', type(component)\n            raise Exception('UnknownDelegateType')\n\n        # connect editor to update defaults\n        editor.attributesSaved.connect(view.updateDefaults)\n        editor.attributesSaved.connect(view.somethingChanged)\n\n        if component.name == 'Vocalization':\n            # find any associated file auto-parameters\n            files = view.model().autoParams().findFileParam(component)\n            if files is not None:\n                editor.selectMany(files)\n            editor.vocalFilesChanged.connect(view.updateVocalAuto)\n\n        return editor"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the input from the editor widget to the model component.", "response": "def setModelData(self, editor, model, index):\n        \"\"\"Saves the input from the editor widget to the model component.\n        :qtdoc:`Re-implemented<QStyledItemDelegate.setModelData>`\"\"\"\n        editor.saveToObject()\n        # need to save over component object in stimulus model\n        model.dataEdited()\n\n        # clean up\n        editor.attributesSaved.disconnect()\n        if hasattr(editor, 'vocalFilesChanged'):\n            editor.vocalFilesChanged.disconnect()\n        editor.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef updateEditorGeometry(self, editor, option, index):\n        qr = editor.frameGeometry()\n        cp = QtGui.QDesktopWidget().availableGeometry().center()\n        qr.moveCenter(cp)\n\n        editor.move(qr.topLeft())", "response": "centers the editor widget."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset focus to the editor.", "response": "def eventFilter(self, editor, event):\n        \"\"\"Sets focus to the editor. :qtdoc:`Re-implemented<QStyledItemDelegate.eventFilter>`\"\"\"\n        if event.type() == QtCore.QEvent.FocusIn:\n            editor.setContentFocus()\n            return True\n\n        return super(ComponentDelegate, self).eventFilter(editor, event)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a suitable name for the taskfileinfo", "response": "def get_namespace(taskfileinfo):\n    \"\"\"Return a suitable name for a namespace for the taskfileinfo\n\n    Returns the name of the shot/asset with a \"_1\" suffix.\n    When you create the namespace the number will automatically be incremented by Maya.\n\n    :param taskfileinfo: the taskfile info for the file that needs a namespace\n    :type taskfileinfo: :class:`jukeboxcore.filesys.TaskFileInfo`\n    :returns: a namespace suggestion\n    :rtype: str\n    :raises: None\n    \"\"\"\n    element = taskfileinfo.task.element\n    name = element.name\n    return name + \"_1\""}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a suitable name for a groupname for the given taskfileinfo.", "response": "def get_groupname(taskfileinfo):\n    \"\"\"Return a suitable name for a groupname for the given taskfileinfo.\n\n    :param taskfileinfo: the taskfile info for the file that needs a group when importing/referencing\n    :type taskfileinfo: :class:`jukeboxcore.filesys.TaskFileInfo`\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    element = taskfileinfo.task.element\n    name = element.name\n    return name + \"_grp\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngroup the given content in the given namespace under a node of type grpnodetype with the given name", "response": "def group_content(content, namespace, grpname, grpnodetype):\n    \"\"\"Group the given content in the given namespace under a node of type\n    grpnodetype with the name grpname\n\n    :param content: the nodes to group\n    :type content: :class:`list`\n    :param namespace: the namespace to use\n    :type namespace: str | None\n    :param grpname: the name of the new grpnode\n    :type grpname: str\n    :param grpnodetype: the nodetype for the grpnode\n    :type grpnodetype: str\n    :returns: the created group node\n    :rtype: str\n    :raises: None\n    \"\"\"\n    with common.preserve_namespace(namespace):\n        grpnode = cmds.createNode(grpnodetype, name=grpname) # create grp node\n        cmds.group(content, uag=grpnode) # group content\n    return grpnode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a label widget by it component name", "response": "def getLabelByName(self, name):\n        \"\"\"Gets a label widget by it component name\n\n        :param name: name of the AbstractStimulusComponent which this label is named after\n        :type name: str\n        :returns: :class:`DragLabel<sparkle.gui.drag_label.DragLabel>`\n        \"\"\"\n        name = name.lower()\n        if name in self.stimLabels:\n            return self.stimLabels[name]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the raw queue.", "response": "def process_rawq(self):\n        \"\"\"Transfer from raw queue to cooked queue.\n\n        Set self.eof when connection is closed.\n        \"\"\"\n        buf = [b'', b'']\n        try:\n            while self.rawq:\n                c = yield from self.rawq_getchar()\n                if not self.iacseq:\n                    if self.sb == 0 and c == theNULL:\n                        continue\n                    if self.sb == 0 and c == b\"\\021\":\n                        continue\n                    if c != IAC:\n                        buf[self.sb] = buf[self.sb] + c\n                        continue\n                    else:\n                        self.iacseq += c\n                elif len(self.iacseq) == 1:\n                    # 'IAC: IAC CMD [OPTION only for WILL/WONT/DO/DONT]'\n                    if c in (DO, DONT, WILL, WONT):\n                        self.iacseq += c\n                        continue\n\n                    self.iacseq = b''\n                    if c == IAC:\n                        buf[self.sb] = buf[self.sb] + c\n                    else:\n                        if c == SB:  # SB ... SE start.\n                            self.sb = 1\n                            self.sbdataq = b''\n                        elif c == SE:\n                            self.sb = 0\n                            self.sbdataq = self.sbdataq + buf[1]\n                            buf[1] = b''\n                        yield from self._opt_handler(c, NOOPT,\n                                                     data=self.sbdataq)\n                elif len(self.iacseq) == 2:\n                    cmd = self.iacseq[1:2]\n                    self.iacseq = b''\n                    opt = c\n                    if cmd in (DO, DONT):\n                        yield from self._opt_handler(cmd, opt)\n                    elif cmd in (WILL, WONT):\n                        yield from self._opt_handler(cmd, opt)\n        except EOFError:  # raised by self.rawq_getchar()\n            self.iacseq = b''  # Reset on EOF\n            self.sb = 0\n            pass\n        self.cookedq = self.cookedq + buf[0]\n        self.sbdataq = self.sbdataq + buf[1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fill_rawq(self):\n        if self.irawq >= len(self.rawq):\n            self.rawq = b''\n            self.irawq = 0\n        # The buffer size should be fairly small so as to avoid quadratic\n        # behavior in process_rawq() above\n        buf = yield from self._reader.read(50)\n        self.eof = (not buf)\n        self.rawq = self.rawq + buf", "response": "Fill the raw queue from exactly one recv() system call. Set self. eof when connection is closed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_some(self):\n        yield from self.process_rawq()\n        while not self.cookedq and not self.eof:\n            yield from self.fill_rawq()\n            yield from self.process_rawq()\n        buf = self.cookedq\n        self.cookedq = b''\n        return buf", "response": "Read at least one byte of cooked data until EOF is hit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_byte(self):\n        buf = b''\n        if len(self.cookedq) > 0:\n            buf = bytes([self.cookedq[0]])\n            self.cookedq = self.cookedq[1:]\n        else:\n            yield from self.process_rawq()\n            if not self.eof:\n                yield from self.fill_rawq()\n                yield from self.process_rawq()\n                # There now should be data so lets read again\n                buf = yield from self.read_byte()\n\n        return buf", "response": "Read one byte of cooked data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_line(self):\n        buf = b''\n        while not self.eof and buf.endswith(b'\\n') is False:\n            buf += yield from self.read_byte()\n\n        if self.eof:\n            buf = b''\n\n        # Remove \\n character\n        buf = buf.replace(b'\\n', b'')\n\n        return buf", "response": "Read a line of data until \\ n is found."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the tzid if it exists or None.", "response": "def getTzid(tzid, smart=True):\n    \"\"\"Return the tzid if it exists, or None.\"\"\"\n    tz = __tzidMap.get(toUnicode(tzid), None)\n    if smart and tzid and not tz:\n        try:\n            from pytz import timezone, UnknownTimeZoneError\n            try:\n                tz = timezone(tzid)\n                registerTzid(toUnicode(tzid), tz)\n            except UnknownTimeZoneError:\n                pass\n        except ImportError:\n            pass\n    return tz"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a datetime. datetime object to a string.", "response": "def dateTimeToString(dateTime, convertToUTC=False):\n    \"\"\"\n    Ignore tzinfo unless convertToUTC.  Output string.\n    \"\"\"\n    if dateTime.tzinfo and convertToUTC:\n        dateTime = dateTime.astimezone(utc)\n\n    datestr = \"{}{}{}T{}{}{}\".format(\n        numToDigits(dateTime.year, 4),\n        numToDigits(dateTime.month, 2),\n        numToDigits(dateTime.day, 2),\n        numToDigits(dateTime.hour, 2),\n        numToDigits(dateTime.minute, 2),\n        numToDigits(dateTime.second, 2),\n    )\n    if tzinfo_eq(dateTime.tzinfo, utc):\n        datestr += \"Z\"\n    return datestr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stringToDateTime(s, tzinfo=None):\n    try:\n        year = int(s[0:4])\n        month = int(s[4:6])\n        day = int(s[6:8])\n        hour = int(s[9:11])\n        minute = int(s[11:13])\n        second = int(s[13:15])\n        if len(s) > 15:\n            if s[15] == 'Z':\n                tzinfo = utc\n    except:\n        raise ParseError(\"'%s' is not a valid DATE-TIME\" % s)\n    year = year and year or 2000\n    return datetime.datetime(year, month, day, hour, minute, second, 0, tzinfo)", "response": "Returns a datetime object from a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an iCalendar rruleset for this iCalendar object.", "response": "def getrruleset(self, addRDate=False):\n        \"\"\"\n        Get an rruleset created from self.\n\n        If addRDate is True, add an RDATE for dtstart if it's not included in\n        an RRULE, and count is decremented if it exists.\n\n        Note that for rules which don't match DTSTART, DTSTART may not appear\n        in list(rruleset), although it should.  By default, an RDATE is not\n        created in these cases, and count isn't updated, so dateutil may list\n        a spurious occurrence.\n\n        \"\"\"\n        rruleset = None\n        for name in DATESANDRULES:\n            addfunc = None\n            for line in self.contents.get(name, ()):\n                # don't bother creating a rruleset unless there's a rule\n                if rruleset is None:\n                    rruleset = rrule.rruleset()\n                if addfunc is None:\n                    addfunc = getattr(rruleset, name)\n\n                if name in DATENAMES:\n                    if type(line.value[0]) == datetime.datetime:\n                        map(addfunc, line.value)\n                    elif type(line.value[0]) == datetime.date:\n                        for dt in line.value:\n                            addfunc(datetime.datetime(dt.year, dt.month, dt.day))\n                    else:\n                        # ignore RDATEs with PERIOD values for now\n                        pass\n                elif name in RULENAMES:\n                    try:\n                        dtstart = self.dtstart.value\n                    except (AttributeError, KeyError):\n                        # Special for VTODO - try DUE property instead\n                        try:\n                            if self.name == \"VTODO\":\n                                dtstart = self.due.value\n                            else:\n                                # if there's no dtstart, just return None\n                                print('failed to get dtstart with VTODO')\n                                return None\n                        except (AttributeError, KeyError):\n                            # if there's no due, just return None\n                            print('failed to find DUE at all.')\n                            return None\n\n                    # a Ruby iCalendar library escapes semi-colons in rrules,\n                    # so also remove any backslashes\n                    value = str_(line.value).replace('\\\\', '')\n                    rule = rrule.rrulestr(value, dtstart=dtstart)\n                    until = rule._until\n\n                    if until is not None and isinstance(dtstart, datetime.datetime) and \\\n                       (until.tzinfo != dtstart.tzinfo):\n                        # dateutil converts the UNTIL date to a datetime,\n                        # check to see if the UNTIL parameter value was a date\n                        vals = dict(pair.split('=') for pair in\n                                    line.value.upper().split(';'))\n                        if len(vals.get('UNTIL', '')) == 8:\n                            until = datetime.datetime.combine(until.date(), dtstart.time())\n                        # While RFC2445 says UNTIL MUST be UTC, Chandler allows\n                        # floating recurring events, and uses floating UNTIL values.\n                        # Also, some odd floating UNTIL but timezoned DTSTART values\n                        # have shown up in the wild, so put floating UNTIL values\n                        # DTSTART's timezone\n                        if until.tzinfo is None:\n                            until = until.replace(tzinfo=dtstart.tzinfo)\n\n                        if dtstart.tzinfo is not None:\n                            until = until.astimezone(dtstart.tzinfo)\n\n                        # RFC2445 actually states that UNTIL must be a UTC value. Whilst the\n                        # changes above work OK, one problem case is if DTSTART is floating but\n                        # UNTIL is properly specified as UTC (or with a TZID). In that case dateutil\n                        # will fail datetime comparisons. There is no easy solution to this as\n                        # there is no obvious timezone (at this point) to do proper floating time\n                        # offset compisons. The best we can do is treat the UNTIL value as floating.\n                        # This could mean incorrect determination of the last instance. The better\n                        # solution here is to encourage clients to use COUNT rather than UNTIL\n                        # when DTSTART is floating.\n                        if dtstart.tzinfo is None:\n                            until = until.replace(tzinfo=None)\n\n                        rule._until = until\n\n                    # add the rrule or exrule to the rruleset\n                    addfunc(rule)\n\n                    if name == 'rrule' and addRDate:\n                        try:\n                            # dateutils does not work with all-day (datetime.date) items\n                            # so we need to convert to a datetime.datetime\n                            # (which is what dateutils does internally)\n                            if not isinstance(dtstart, datetime.datetime):\n                                adddtstart = datetime.datetime.fromordinal(dtstart.toordinal())\n                            else:\n                                adddtstart = dtstart\n                            if rruleset._rrule[-1][0] != adddtstart:\n                                rruleset.rdate(adddtstart)\n                                added = True\n                            else:\n                                added = False\n                        except IndexError:\n                            # it's conceivable that an rrule might have 0 datetimes\n                            added = False\n                        if added and rruleset._rrule[-1]._count is not None:\n                            rruleset._rrule[-1]._count -= 1\n        return rruleset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(cls, line):\n        if line.encoded:\n            encoding = getattr(line, 'encoding_param', None)\n            if encoding and encoding.upper() == cls.base64string:\n                line.value = b64decode(line.value)\n            else:\n                line.value = stringToTextValues(line.value)[0]\n            line.encoded = False", "response": "Remove backslash escaping from line. value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transformToNative(obj):\n        if obj.isNative:\n            return obj\n        obj.isNative = True\n        if obj.value == '':\n            return obj\n        obj.value = obj.value\n        #we're cheating a little here, parseDtstart allows DATE\n        obj.value = parseDtstart(obj)\n        if obj.value.tzinfo is None:\n            obj.params['X-VOBJ-FLOATINGTIME-ALLOWED'] = ['TRUE']\n        if obj.params.get('TZID'):\n            # Keep a copy of the original TZID around\n            obj.params['X-VOBJ-ORIGINAL-TZID'] = [obj.params['TZID']]\n            del obj.params['TZID']\n        return obj", "response": "Turn obj. value into a datetime."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces the datetime in obj. value with an ISO 8601 string.", "response": "def transformFromNative(cls, obj):\n        \"\"\"Replace the datetime in obj.value with an ISO 8601 string.\"\"\"\n        # print('transforming from native')\n        if obj.isNative:\n            obj.isNative = False\n            tzid = TimezoneComponent.registerTzinfo(obj.value.tzinfo)\n            obj.value = dateTimeToString(obj.value, cls.forceUTC)\n            if not cls.forceUTC and tzid is not None:\n                obj.tzid_param = tzid\n            if obj.params.get('X-VOBJ-ORIGINAL-TZID'):\n                if not hasattr(obj, 'tzid_param'):\n                    obj.tzid_param = obj.x_vobj_original_tzid_param\n                del obj.params['X-VOBJ-ORIGINAL-TZID']\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets META information about currencies", "response": "def get_currencies_info() -> Element:\n    \"\"\"Get META information about currencies\n\n    url: http://www.cbr.ru/scripts/XML_val.asp\n\n    :return: :class: `Element <Element 'Valuta'>` object\n    :rtype: ElementTree.Element\n    \"\"\"\n    response = requests.get(const.CBRF_API_URLS['info'])\n\n    return XML(response.text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_daily_rates(date_req: datetime.datetime = None, lang: str = 'rus') -> Element:\n    if lang not in ['rus', 'eng']:\n        raise ValueError('\"lang\" must be string. \"rus\" or \"eng\"')\n\n    base_url = const.CBRF_API_URLS['daily_rus'] if lang == 'rus' \\\n        else const.CBRF_API_URLS['daily_eng']\n\n    url = base_url + 'date_req=' + utils.date_to_str(date_req) if date_req else base_url\n\n    response = requests.get(url=url)\n\n    return XML(response.text)", "response": "Get daily rates for current day."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mangleIR(data, ignore_errors=False):\n    try:\n        # Packet mangling algorithm inspired by Rex Becket's kirarx vera plugin\n        # Determine a median value for the timing packets and categorize each\n        # timing as longer or shorter than that. This will always work for signals\n        # that use pulse width modulation (since varying by long-short is basically\n        # the definition of what PWM is). By lucky coincidence this also works with\n        # the RC-5/RC-6 encodings used by Phillips (manchester encoding)\n        # because time variations of opposite-phase/same-phase are either N or 2*N\n        if isinstance(data, bytes):\n            data = data.decode('ascii')\n        data = data.strip()\n        times = [int(x, 16) for x in data.split()[2:]]\n        minTime = min(times[2:-1])\n        maxTime = max(times[2:-1])\n        margin = (maxTime - minTime) / 2 + minTime\n        return ''.join([(x < margin and 'S' or 'L') for x in times])\n    except:\n        # Probably a mangled packet.\n        if not ignore_errors:\n            raise", "response": "Mangle a raw Kira data packet into a shorthand"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pronto2kira(data):\n    octets = [int(x, 16) for x in data.split()]\n    preamble = octets[:4]\n    convert = lambda x: 1000.0 / (x * 0.241246)\n    freq = convert(preamble[1])\n    period = 1000000.0 / (freq * 1000.0)\n    dataLen = preamble[2]\n    res = \"K %02X%02X \" %(freq, dataLen)\n    res += \" \".join([\"%0.4X\" % min(0x2000, (period * x)) for x in octets[4: 4+(2*dataLen)]])\n    return res", "response": "Convert a pronto code to a discrete Kira code"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts NEC code to shorthand notation", "response": "def mangleNec(code, freq=40):\n    \"\"\"Convert NEC code to shorthand notation\"\"\"\n    # base time is 550 microseconds\n    # unit of burst time\n    # lead in pattern:   214d 10b3\n    # \"1\" burst pattern: 0226 0960\n    # \"0\" burst pattern: 0226 0258\n    # lead out pattern:  0226 2000\n    # there's large disagreement between devices as to a common preamble\n    # or the \"long\" off period for the representation of a binary 1\n    # thus we can't construct a code suitable for transmission\n    # without more information--but it's good enough for creating\n    # a shorthand representaiton for use with recv\n    timings = []\n    for octet in binascii.unhexlify(code.replace(\" \", \"\")):\n        burst = lambda x: x and \"0226 06AD\" or \"0226 0258\"\n        for bit in reversed(\"%08d\" % int(bin(ord(octet))[2:])):\n            bit = int(bit)\n            timings.append(burst(bit))\n    return mangleIR(\"K %0X22 214d 10b3 \" % freq + \" \".join(timings) + \" 0226 2000\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a limit to any query and allow a return as pandas. DataFrame", "response": "def _limit_and_df(self, query, limit, as_df=False):\n        \"\"\"adds a limit (limit==None := no limit) to any query and allow a return as pandas.DataFrame\n\n        :param bool as_df: if is set to True results return as pandas.DataFrame\n        :param `sqlalchemy.orm.query.Query` query: SQL Alchemy query\n        :param int,tuple limit: maximum number of results\n        :return: query result of pyhgnc.manager.models.XY objects\n        \"\"\"\n        if limit:\n\n            if isinstance(limit, int):\n                query = query.limit(limit)\n\n            if isinstance(limit, Iterable) and len(limit) == 2 and [int, int] == [type(x) for x in limit]:\n                page, page_size = limit\n                query = query.limit(page_size)\n                query = query.offset(page * page_size)\n\n        if as_df:\n            results = read_sql(query.statement, self.engine)\n\n        else:\n            try:\n                results = query.all()\n            except:\n                query.session.rollback()\n                results = query.all()\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this if you are searching for a field in the same model", "response": "def get_model_queries(self, query_obj, model_queries_config):\n        \"\"\"use this if your are searching for a field in the same model\"\"\"\n        for search4, model_attrib in model_queries_config:\n\n            if search4 is not None:\n                query_obj = self._model_query(query_obj, search4, model_attrib)\n        return query_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _one_to_many_query(cls, query_obj, search4, model_attrib):\n        model = model_attrib.parent.class_\n\n        already_joined_tables = [mapper.class_ for mapper in query_obj._join_entities]\n\n        if isinstance(search4, (str, int, Iterable)) and model not in already_joined_tables:\n            query_obj = query_obj.join(model)\n\n        if isinstance(search4, str):\n            query_obj = query_obj.filter(model_attrib.like(search4))\n\n        elif isinstance(search4, int):\n            query_obj = query_obj.filter(model_attrib == search4)\n\n        elif isinstance(search4, Iterable):\n            query_obj = query_obj.filter(model_attrib.in_(search4))\n\n        return query_obj", "response": "extends and returns a SQLAlchemy query object to allow one - to - many queries"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef alias_symbol(self,\n                     alias_symbol=None,\n                     is_previous_symbol=None,\n                     hgnc_symbol=None,\n                     hgnc_identifier=None,\n                     limit=None,\n                     as_df=False):\n        \"\"\"Method to query :class:`.models.AliasSymbol` objects in database\n\n        :param alias_symbol: alias symbol(s)\n        :type alias_symbol: str or tuple(str) or None\n\n        :param is_previous_symbol: flag for 'is previous'\n        :type is_previous_symbol: bool or tuple(bool) or None\n\n        :param hgnc_symbol: HGNC symbol(s)\n        :type hgnc_symbol: str or tuple(str) or None\n\n        :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC`\n        :type hgnc_identifier: int or tuple(int) or None\n\n        :param limit:\n            - if `isinstance(limit,int)==True` -> limit\n            - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page)\n            - if limit == None -> all results\n        :type limit: int or tuple(int) or None\n\n        :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame`\n\n        :return:\n            - if `as_df == False` -> list(:class:`.models.AliasSymbol`)\n            - if `as_df == True`  -> :class:`pandas.DataFrame`\n        :rtype: list(:class:`.models.AliasSymbol`) or :class:`pandas.DataFrame`\n\n        \"\"\"\n        q = self.session.query(models.AliasSymbol)\n\n        model_queries_config = (\n            (alias_symbol, models.AliasSymbol.alias_symbol),\n            (is_previous_symbol, models.AliasSymbol.is_previous_symbol),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        one_to_many_queries_config = (\n            (hgnc_symbol, models.HGNC.symbol),\n            (hgnc_identifier, models.HGNC.identifier)\n        )\n        q = self.get_one_to_many_queries(q, one_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the alias symbol in database and return a Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alias_name(self,\n                   alias_name=None,\n                   is_previous_name=None,\n                   hgnc_symbol=None,\n                   hgnc_identifier=None,\n                   limit=None,\n                   as_df=False):\n        \"\"\"Method to query :class:`.models.AliasName` objects in database\n\n        :param alias_name: alias name(s)\n        :type alias_name: str or tuple(str) or None\n\n        :param is_previous_name: flag for 'is previous'\n        :type is_previous_name: bool or tuple(bool) or None\n\n        :param hgnc_symbol: HGNC symbol(s)\n        :type hgnc_symbol: str or tuple(str) or None\n\n        :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC`\n        :type hgnc_identifier: int or tuple(int) or None\n\n        :param limit:\n            - if `isinstance(limit,int)==True` -> limit\n            - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page)\n            - if limit == None -> all results\n        :type limit: int or tuple(int) or None\n\n        :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame`\n\n        :return:\n            - if `as_df == False` -> list(:class:`.models.AliasSymbol`)\n            - if `as_df == True`  -> :class:`pandas.DataFrame`\n        :rtype: list(:class:`.models.AliasSymbol`) or :class:`pandas.DataFrame`\n\n        \"\"\"\n        q = self.session.query(models.AliasName)\n\n        model_queries_config = (\n            (alias_name, models.AliasName.alias_name),\n            (is_previous_name, models.AliasName.is_previous_name),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        one_to_many_queries_config = (\n            (hgnc_symbol, models.HGNC.symbol),\n            (hgnc_identifier, models.HGNC.identifier)\n        )\n        q = self.get_one_to_many_queries(q, one_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the alias name of a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ref_seq(self, accession=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):\n        q = self.session.query(models.RefSeq)\n\n        model_queries_config = (\n            (accession, models.RefSeq.accession),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        many_to_many_queries_config = (\n            (hgnc_symbol, models.RefSeq.hgncs, models.HGNC.symbol),\n            (hgnc_identifier, models.RefSeq.hgncs, models.HGNC.identifier),\n        )\n        q = self.get_many_to_many_queries(q, many_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the database for the specified refseq."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rgd(self, rgdid=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):\n        q = self.session.query(models.RGD)\n\n        model_queries_config = (\n            (rgdid, models.RGD.rgdid),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        many_to_many_queries_config = (\n            (hgnc_symbol, models.RGD.hgncs, models.HGNC.symbol),\n            (hgnc_identifier, models.RGD.hgncs, models.HGNC.identifier),\n        )\n        q = self.get_many_to_many_queries(q, many_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the RGD database for a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mgd(self, mgdid=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):\n        q = self.session.query(models.MGD)\n\n        model_queries_config = (\n            (mgdid, models.MGD.mgdid),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        many_to_many_queries_config = (\n            (hgnc_symbol, models.MGD.hgncs, models.HGNC.symbol),\n            (hgnc_identifier, models.MGD.hgncs, models.HGNC.identifier),\n        )\n        q = self.get_many_to_many_queries(q, many_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the MGD objects in database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ccds(self, ccdsid=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):\n        q = self.session.query(models.CCDS)\n\n        model_queries_config = (\n            (ccdsid, models.CCDS.ccdsid),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        one_to_many_queries_config = (\n            (hgnc_symbol, models.HGNC.symbol),\n            (hgnc_identifier, models.HGNC.identifier)\n        )\n        q = self.get_one_to_many_queries(q, one_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the Consensus CDS objects in database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enzyme(self, ec_number=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):\n        q = self.session.query(models.Enzyme)\n\n        model_queries_config = (\n            (ec_number, models.Enzyme.ec_number),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        many_to_many_queries_config = (\n            (hgnc_symbol, models.Enzyme.hgncs, models.HGNC.symbol),\n            (hgnc_identifier, models.Enzyme.hgncs, models.HGNC.identifier),\n        )\n        q = self.get_many_to_many_queries(q, many_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)", "response": "Method to query the database for the specified enzyme."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authenticated(function):\n    def wrapped(*args):\n        \"\"\"Wrap function.\"\"\"\n        try:\n            return function(*args)\n        except UPSError:\n            _login(*args)\n            return function(*args)\n    return wrapped", "response": "Wrap function to re - authenticate if session expired."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_packages(session):\n    resp = session.get(DELIVERIES_URL, params=_get_params(session.auth.locale))\n    parsed = BeautifulSoup(resp.text, HTML_PARSER)\n    token_elem = parsed.find(TOKEN_FIND_TAG, TOKEN_FIND_ATTR)\n    tid_elem = parsed.find(TID_FIND_TAG, TID_FIND_ATTR)\n    if not token_elem or not tid_elem:\n        raise UPSError('failed to find token or tid')\n    token = token_elem.get(VALUE_ATTR)\n    tid = tid_elem.get(VALUE_ATTR)\n    resp = session.post(SERVICE_URL, {\n        'token': token,\n        'uid': session.auth.username,\n        'callType': 'allShipments',\n        'tid': tid,\n        'loc': session.auth.locale\n    })\n    try:\n        packages = []\n        data = json.loads(resp.text[UPS_JSON_PREAMBLE_SIZE:])\n        shipments = data['shipmentContainer']['inboundShipments'] + \\\n            data['shipmentContainer']['historyShipments']\n        for shipment in shipments:\n            from_location = '{}, {}, {}'.format(shipment['sfc'],\n                                                shipment['sfs'],\n                                                shipment['sfcn'])\n            estimated_date = _parsed_date(shipment['sddfd'])\n            actual_date = _parsed_date(shipment['dd'])\n            packages.append({\n                'tracking_number': shipment['tn'],\n                'status': shipment['sts'],\n                'from': shipment['sfn'],\n                'from_location': from_location,\n                'estimated_delivery_date': estimated_date,\n                'estimated_delivery_timeframe': shipment['sdtfd'],\n                'delivery_date': actual_date\n            })\n        return packages\n    except JSONDecodeError:\n        raise UPSError('failed to parse json')", "response": "Get deliveries in progress and completed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_session(username, password, locale=DEFAULT_LOCALE,\n                cookie_path=COOKIE_PATH):\n    \"\"\"Get UPS HTTP session.\"\"\"\n    class UPSAuth(AuthBase):  # pylint: disable=too-few-public-methods\n        \"\"\"UPS authorization storage.\"\"\"\n\n        def __init__(self, username, password, locale, cookie_path):\n            \"\"\"Init.\"\"\"\n            self.username = username\n            self.password = password\n            self.locale = locale\n            self.cookie_path = cookie_path\n\n        def __call__(self, r):\n            \"\"\"Call is no-op.\"\"\"\n            return r\n\n    session = requests.session()\n    session.auth = UPSAuth(username, password, locale, cookie_path)\n    if os.path.exists(cookie_path):\n        session.cookies = _load_cookies(cookie_path)\n    else:\n        _login(session)\n    return session", "response": "Get UPS HTTP session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hide(self, event):\n        if self.content.isHidden():\n            self.content.show()\n            self.hideBtn.setIcon(self.hideIcon)\n            self.setMaximumHeight(16777215)\n        else:\n            self.content.hide()\n            self.hideBtn.setIcon(self.showIcon)\n            self.setFixedHeight(30)", "response": "Toggles the visiblity of the content widget"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match_lines(original_text, things_to_match):\n    '''\n    :param original_text: ``str``/``Unicode`` containing the original text to get offsets within\n    :param things_to_match: ``list(words/phrases)`` whose offsets we wish to find within ``original_text``.\n\n    If ``things_to_match`` is a list of tokenized strings, each element of ``things_to_match`` is expected to be a ``list`` of tokens.\n    For example::\n\n        [[\"Hello\", \",\", \"world\", \"!\"], [\"That\", \"was\", \"the\", \"first\", \"sentence\", \";\", \"here\", \"is\", \"the\", \"second\", \".\"]]\n\n    ``things_to_match`` could also be::\n\n        [\"cat\", \"dog\", \"octopus\"]\n\n    or even a mix of the two.  This function will call :mod:`sourcerater.util.Match.match()` on each element of ``things_to_match``.\n\n    :returns: ``sorted(set([(start, end, word/phrase) for word/phrase in things_to_match]))`` for ALL occurrences of each word/phrase in ``things_to_match``.\n    '''\n    matched_lines = []\n    unique_things_to_match = (set(things_to_match) if type(things_to_match[0]) is not list else things_to_match)\n    \n    without_smart_quotes = _cleanup_text(original_text)\n\n    for thing in unique_things_to_match:\n        if len(thing) == 0:\n            continue\n        matches = match(original_text, thing, clean_text=without_smart_quotes)\n        matched_lines += matches\n\n    return sorted(set(matched_lines))", "response": "Match the original text with the given list of words and phrases."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match(original_text, word_or_token_list_to_match, clean_text=None):\n    '''\n    :param original_text: ``str``/``Unicode`` containing the original text to get offsets within\n    :param word_or_token_list_to_match: Either a single ``str``/``Unicode`` corresponding to a single token we want offsets for, or a ``list(str)`` corresponding to a phrase/sentence we want offsets for.\n    :param clean_text: If ``None``, this function will do some preliminary cleaning of ``original_text`` to better facilitate the matching process (replacing strange quotation marks with ASCII ones, etc.).  This is a one-to-one process; a single character simply becomes a different single character, so as to not throw off the offsets of ``word_or_token_list_to_match`` within ``original_text``.  It's just that ``word_or_token_to_match`` will probably not match against a text with non-ASCII characters in it, especially if those characters are Windows \"smart\" quotes.\n\n    :returns: ``sorted([(start, end, word/phrase)])`` where each ``tuple`` contains a unique occurrence of ``word_or_token_list_to_match`` in ``original_text``, and the word/phrase is what is contained within ``original_text`` at that ``start``/``end`` offset pair.\n\n    **How This Works**\n\n    Much of this hinges on the existance of ``finditer()`` in Python's ``regex`` module.  We use a list \n    comprehension to generate a ``list`` of ``(m.start(), m.end(), original_text[m.start():m.end()])``\n    tuples for each match ``m`` returned by ``finditer()``.\n\n    When where we're interested in all occurrences of a single token, for example \"dog\", we\n    simply call ``regex.finditer(r'\\bdog\\b', original_text, regex.U | regex.I)``.  When where \n    we're interested in a sentence or phrase like \"Dogs make great pets.\", things are not so \n    straightforward.\n\n    In this case, it is useful to perform some cleanup of ``original_text``, as described above.  This is very\n    helpful because typical tokenization normalizes any non-ASCII characters to ASCII equivalents.  We do the same here,\n    being careful not to expand any single character into multiple ones; that will throw off the offsets, since\n    we are not interested in the offsets from ``clean_text``, which the user will never see, but in ``original_text``,\n    which the user submitted.\n\n    Through thorough testing, I determined that the following two strategies are necessary in order to get\n    the offsets for any tokenized string.\n\n    **First**, we simply reverse tokenization on the string ``\" \".join([\"Dogs\", \"make\", \"great\", \"pets\", \".\"])``\n    and try to find it directly in ``original_text`` using ``finditer()``.  For my test set, this returned\n    the correct match for over 91% of the cases.\n\n    For the remaining test cases, a **second** and slightly more time-consuming method is necessary.\n\n    1. Use ``finditer()`` on ``\"\\s*\".join([\"Dogs\", \"make\", \"great\", \"pets\", \".\"])``.  This works particularly well for essays because many candidates accidentally insert more whitespace characters than necessary, and toksent and expunct do an excellent job of removing the excess spaces.  However, our offsets need to include those spaces.  Should this fail:\n    2. Use Levenshtein edit distance, as implemented in ``nltk.metrics.distance.edit_distance()``.  For each attempt with edit distance, we extract all strings that start with the same token as the one we're looking for (\"Dogs\" in this example) that have the same number of characters (as \"Dogs make great pets.\"), and return the one with the lowest edit distance to the search string.\n\n    We'll try (2) against the original text and our untokenized version from before, and if that fails, \n    we try edit distance again against the original text and ``\" \".join([\"Dogs\", \"make\", \"great\", \"pets\", \".\"])``.\n    Even if the search string is not in the original text, we'll return the offsets of the string\n    with the lowest edit distance within the original text.\n    '''\n\n    regex_flags = re.U | re.I\n\n    if len(word_or_token_list_to_match) == 0:\n        return []\n\n    if not(clean_text):\n        clean_text = _cleanup_text(original_text)\n\n    if type(word_or_token_list_to_match) is list:\n        to_match = untokenize(\" \".join(word_or_token_list_to_match).strip())\n        matches = [(m.start(), m.end(), original_text[m.start():m.end()]) for m in re.finditer(re.escape(to_match), clean_text, regex_flags)]\n        if len(matches) == 0:\n            matches = [(m.start(), m.end(), original_text[m.start():m.end()]) \n                       for m in re.finditer(\"\\s*\".join(re.escape(w) for w in word_or_token_list_to_match), original_text, regex_flags)]\n            if len(matches) == 0:\n                edit_distance_match = _match_by_edit_distance(clean_text, re.sub(r'\\\\s[\\*\\+]', r' ', to_match))\n                matches = [(m.start(), m.end(), original_text[m.start():m.end()]) \n                           for m in re.finditer(re.escape(edit_distance_match), clean_text, regex_flags)]\n                if len(matches) == 0:\n                    edit_distance_match = _match_by_edit_distance(original_text, re.sub(r'\\\\s[\\*\\+]', r' ', to_match))\n                    matches = [(m.start(), m.end(), original_text[m.start():m.end()]) \n                               for m in re.finditer(re.escape(edit_distance_match), original_text, regex_flags)]\n                    if len(matches) == 0:\n                        edit_distance_match = _match_by_edit_distance(original_text, \" \".join(word_or_token_list_to_match))\n                        matches = [(m.start(), m.end(), original_text[m.start():m.end()]) \n                                   for m in re.finditer(re.escape(edit_distance_match), original_text, regex_flags)]\n                        if len(matches) == 0:\n                            return []\n    else:\n        matches = [(m.start(), m.end(), original_text[m.start():m.end()]) for m in re.finditer(r'\\b' + re.escape(word_or_token_list_to_match) + r'\\b', clean_text, regex_flags)]\n\n    return sorted(matches)", "response": "Match a text string with a list of words or tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef untokenize(text):\n    '''\n    Based on https://github.com/commonsense/simplenlp/blob/master/simplenlp/euro.py#L132\n    \n    :param text: A single ``str``/``Unicode`` containing the sentence (well, might work on any arbitrary text) you'd like to untokenize.\n    :returns: A UTF8-encoded, regular expression-friendly, untokenized version of ``text``.\n\n    .. seealso:: https://github.com/EducationalTestingService/stanford-thrift/blob/master/README_tokenizer.md which isn't actually used here (much slower than this approach)\n    '''\n    text = text.encode('utf8')\n\n    step1 = re.sub(r'([\\*\\?])', r'\\\\\\\\\\1', text.decode(\"utf8\"), re.U)\n \n    step2 = step1.replace(\"`` \", '\"\\s*').replace(\" ''\", '\"\\s*')\n    step2 = step2.replace(\" -LRB- \", \" [\\[\\(]\")\n    step2 = re.sub(r' -RRB- ?', r\"[\\]\\)] \", step2)\n\n    step2a = re.sub(r'\\.\\.\\. *', r'[\\.\u2026]{1,3}', step2, re.U)\n\n    step3 = re.sub(r' \\\\*([.,:;?!%]+)([ \\'\"`\\*])', r\"\\1\\2\", step2a)\n    step4 = re.sub(r' \\\\*([,:;?!%]+)$', r'\\s*\\\\' + r\"\\1\", step3)\n\n    step5 = re.sub(r\" '\", r\"'\", step4)\n    step5 = re.sub(r\" n't\", r\"n't\", step5)\n    step5 = step5.replace(\"can not\", \"cannot\")\n\n    step6 = re.sub(r'( *)` ', r\"\\1'\", step5)\n\n    step7 = step6.strip()\n\n    step8 = re.sub(r' \\*$', r'', step7)\n    \n    step9 = re.sub(r' ([^\\\\\\*\\+])', r'\\s+\\1', step8)\n    step9 = re.sub(r'\\\\s[\\+\\*]$', r'', step9)\n\n    return step9", "response": "Untokenizes a single text sentence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rotated_files(path):\n    for globbed_path in iglob(path + FILE_NAME_GLOB):\n        match = re.search(FILE_NAME_REGEX, globbed_path)\n        if match:\n            yield globbed_path, int(match.group('rotation_id'))", "response": "Generator. Yields the next rotated file as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving the list of generated files in the output directory returns the rotation_id that will be given to the current file.", "response": "def _next_rotation_id(rotated_files):\n    \"\"\"Given the hanoi_rotator generated files in the output directory,\n    returns the rotation_id that will be given to the current file. If there\n    are no existing rotated files, return 0.\n    \"\"\"\n    if not rotated_files:\n        return 0\n    else:\n        highest_rotated_file = max(rotated_files, key=lambda x: x[1])\n        return highest_rotated_file[1] + 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _locate_files_to_delete(algorithm, rotated_files, next_rotation_id):\n    rotation_slot = algorithm.id_to_slot(next_rotation_id)\n    for a_path, a_rotation_id in rotated_files:\n        if rotation_slot == algorithm.id_to_slot(a_rotation_id):\n            yield a_path", "response": "Locates all generated files that occupy the same slot\n    that will be given to next_rotation_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rotate(algorithm, path, ext=\"\", destination_dir=None, verbose=False):\n    paths = Paths(path, ext, destination_dir)\n    _move_files(algorithm, paths, verbose)", "response": "Programmatic access to the archive rotator\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the election to the database.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        **uid**: :code:`{race.uid}_election:{election_day}-{party}`\n        \"\"\"\n        if self.party:\n            self.uid = \"{}_election:{}-{}\".format(\n                self.race.uid,\n                self.election_day.date,\n                slugify(self.party.ap_code),\n            )\n        else:\n            self.uid = \"{}_election:{}\".format(\n                self.race.uid, self.election_day.date\n            )\n        super(Election, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all CandidateElections for this election.", "response": "def get_candidates(self):\n        \"\"\"Get all CandidateElections for this election.\"\"\"\n        candidate_elections = CandidateElection.objects.filter(election=self)\n\n        return [ce.candidate for ce in candidate_elections]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget CandidateElections serialized into an object with party - slug keys.", "response": "def get_candidates_by_party(self):\n        \"\"\"\n        Get CandidateElections serialized into an object with\n        party-slug keys.\n        \"\"\"\n        candidate_elections = CandidateElection.objects.filter(election=self)\n\n        return {\n            ce.candidate.party.slug: ce.candidate for ce in candidate_elections\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a CandidateElection for a Candidate in this election.", "response": "def get_candidate_election(self, candidate):\n        \"\"\"Get CandidateElection for a Candidate in this election.\"\"\"\n        return CandidateElection.objects.get(\n            candidate=candidate, election=self\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all the votes attached to a CandidateElection for a Candidate in this election.", "response": "def get_candidate_votes(self, candidate):\n        \"\"\"\n        Get all votes attached to a CandidateElection for a Candidate in\n        this election.\n        \"\"\"\n        candidate_election = CandidateElection.objects.get(\n            candidate=candidate, election=self\n        )\n\n        return candidate_election.votes.all()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_votes(self):\n        candidate_elections = CandidateElection.objects.filter(election=self)\n\n        votes = None\n        for ce in candidate_elections:\n            votes = votes | ce.votes.all()\n\n        return votes", "response": "Get all votes for this election."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_candidate_electoral_votes(self, candidate):\n        candidate_election = CandidateElection.objects.get(\n            candidate=candidate, election=self\n        )\n\n        return candidate_election.electoral_votes.all()", "response": "Get all electoral votes for a candidate in this election."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_electoral_votes(self):\n        candidate_elections = CandidateElection.objects.filter(election=self)\n\n        electoral_votes = None\n        for ce in candidate_elections:\n            electoral_votes = electoral_votes | ce.electoral_votes.all()\n\n        return electoral_votes", "response": "Get all electoral votes for all candidates in this election."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all pledged delegates for a candidate in this election.", "response": "def get_candidate_delegates(self, candidate):\n        \"\"\"\n        Get all pledged delegates for a candidate in this election.\n        \"\"\"\n        candidate_election = CandidateElection.objects.get(\n            candidate=candidate, election=self\n        )\n\n        return candidate_election.delegates.all()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_delegates(self):\n        candidate_elections = CandidateElection.objects.filter(election=self)\n\n        delegates = None\n        for ce in candidate_elections:\n            delegates = delegates | ce.delegates.all()\n\n        return delegates", "response": "Get all pledged delegates for any candidate in this election."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self,limit,start_date=None,end_date=None,clipper=None):\n\n        search_string = self._query_builder(start_date,\n                                            end_date,\n                                            clipper\n                                            )\n\n        # Have to manually build the URI to bypass requests URI encoding\n        # The api server doesn't accept encoded URIs\n        #r = requests.get('%s?%s&&maxRecords=%s' % (self.api_url,\n        #                                            search_string,\n        #                                            limit))\n        \n        try:\n            r = requests.get('%s?%s&&maxRecords=%s' % (self.api_url,\n                                                    search_string,\n                                                    limit)) \n            r.raise_for_status()\n        except requests.HTTPError, e:\n            exit (\"site is not available\")\n            \n        r_dict = json.loads(r.text)\n        \n        result={}\n        \n        if  (r_dict['features'] == 0):\n            result['status'] = u'error'\n            result['message'] = \"error while loading datas\"\n\n        else:\n            result['status'] = u'SUCCESS'\n            result['total'] = len(r_dict['features'])\n            result['limit'] = limit\n            result['ID']=[i['id'] for i in r_dict['features']]\n            result['downloads']=[{\"download\" : i['properties']['services']['download']['url'],\n                                 \"id\" : i['id']}\n                                  for i in r_dict['features']]\n            result['results'] = {\n                                \"features\": [{\n                                  'properties':{'sceneID': i['id'],\n                                  'sat_type': i['properties']['platform'],\n                                  'thumbnail': i['properties']['thumbnail'],\n                                  'date': i['properties']['completionDate'],\n                                  'download': i['properties']['services']['download']['url']}\n                                  ,\n                                  'geometry': i['geometry'],\n                                  \"type\": \"Feature\"}\n                                 for i in r_dict['features']],\n                                 \"type\": \"FeatureCollection\"\n                                 }\n\n\n        \n        return result", "response": "This method searches tTheia Landsat API for the specified date range and returns a dictionary containing the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the proper search syntax for the Landsat theia API.", "response": "def _query_builder(self,\n                       start_date=None,\n                       end_date=None,\n                       clipper=None\n                        ):\n        \"\"\" Builds the proper search syntax (query) for Landsat theia API \"\"\"\n        search_string='format=json&lang=fr&q='\n        if (start_date is not None):\n            if(end_date is not None):\n                search_string+='entre+'+start_date+'+et+'+end_date\n            else:\n                search_string+=start_date\n        elif(end_date is not None): \n            search_string+=end_date\n        \n        if(clipper.query is not None):\n            query=clipper.query.replace(' ','+')\n            search_string+='+'+query\n        if(clipper.bbox is not None):\n            search_string+='&box='+clipper.bbox\n\n        return search_string"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps an optional user callback to parse StreamData from a WebSocket data message.", "response": "def _wrap_callback_parse_stream_data(subscription, on_data, message):\n    \"\"\"\n    Wraps an (optional) user callback to parse StreamData\n    from a WebSocket data message\n    \"\"\"\n    if (message.type == message.DATA and\n            message.data.type == yamcs_pb2.STREAM_DATA):\n        stream_data = getattr(message.data, 'streamData')\n        on_data(StreamData(stream_data))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the existing packet names.", "response": "def list_packet_names(self):\n        \"\"\"\n        Returns the existing packet names.\n\n        :rtype: ~collections.Iterable[str]\n        \"\"\"\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        path = '/archive/{}/packet-names'.format(self._instance)\n        response = self._client.get_proto(path=path)\n        message = archive_pb2.GetPacketNamesResponse()\n        message.ParseFromString(response.content)\n        names = getattr(message, 'name')\n        return iter(names)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the existing parameter groups.", "response": "def list_processed_parameter_groups(self):\n        \"\"\"\n        Returns the existing parameter groups.\n\n        :rtype: ~collections.Iterable[str]\n        \"\"\"\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        path = '/archive/{}/parameter-groups'.format(self._instance)\n        response = self._client.get_proto(path=path)\n        message = archive_pb2.ParameterGroupInfo()\n        message.ParseFromString(response.content)\n        groups = getattr(message, 'group')\n        return iter(groups)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_processed_parameter_group_histogram(self, group=None, start=None, stop=None, merge_time=20):\n        params = {}\n        if group is not None:\n            params['group'] = group\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n        if merge_time is not None:\n            params['mergeTime'] = int(merge_time * 1000)\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/archive/{}/parameter-index'.format(self._instance),\n            params=params,\n            response_class=archive_pb2.IndexResponse,\n            items_key='group',\n            item_mapper=IndexGroup,\n        )", "response": "Reads index records related to processed parameter groups between the specified start and stop time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_event_sources(self):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        path = '/archive/{}/events/sources'.format(self._instance)\n        response = self._client.get_proto(path=path)\n        message = archive_pb2.EventSourceInfo()\n        message.ParseFromString(response.content)\n        sources = getattr(message, 'source')\n        return iter(sources)", "response": "Returns the existing event sources."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_completeness_index(self, start=None, stop=None):\n        params = {}\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/archive/{}/completeness-index'.format(self._instance),\n            params=params,\n            response_class=archive_pb2.IndexResponse,\n            items_key='group',\n            item_mapper=IndexGroup,\n        )", "response": "Reads the completeness index records between the specified start and stop time."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads packet information between the specified start and stop time.", "response": "def list_packets(self, name=None, start=None, stop=None, page_size=500, descending=False):\n        \"\"\"\n        Reads packet information between the specified start and stop\n        time.\n\n        Packets are sorted by generation time and sequence number.\n\n        :param ~datetime.datetime start: Minimum generation time of the returned\n                                         packets (inclusive)\n        :param ~datetime.datetime stop: Maximum genreation time of the returned\n                                        packets (exclusive)\n        :param int page_size: Page size of underlying requests. Higher values imply\n                              less overhead, but risk hitting the maximum message size limit.\n        :param bool descending: If set to ``True`` packets are fetched in reverse\n                                order (most recent first).\n        :rtype: ~collections.Iterable[.Packet]\n        \"\"\"\n        params = {\n            'order': 'desc' if descending else 'asc',\n        }\n        if name is not None:\n            params['name'] = name\n        if page_size is not None:\n            params['limit'] = page_size\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/archive/{}/packets'.format(self._instance),\n            params=params,\n            response_class=rest_pb2.ListPacketsResponse,\n            items_key='packet',\n            item_mapper=Packet,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_packet(self, generation_time, sequence_number):\n        url = '/archive/{}/packets/{}/{}'.format(\n            self._instance, to_isostring(generation_time), sequence_number)\n        response = self._client.get_proto(url)\n        message = yamcs_pb2.TmPacketData()\n        message.ParseFromString(response.content)\n        return Packet(message)", "response": "Gets a single packet by its unique key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_events(self, source=None, severity=None, text_filter=None,\n                    start=None, stop=None, page_size=500, descending=False):\n        \"\"\"\n        Reads events between the specified start and stop time.\n\n        Events are sorted by generation time, source, then sequence number.\n\n        :param str source: The source of the returned events.\n        :param str severity: The minimum severity level of the returned events.\n                             One of ``INFO``, ``WATCH``, ``WARNING``, ``DISTRESS``,\n                             ``CRITICAL`` or ``SEVERE``.\n        :param str text_filter: Filter the text message of the returned events\n        :param ~datetime.datetime start: Minimum start date of the returned events (inclusive)\n        :param ~datetime.datetime stop: Maximum start date of the returned events (exclusive)\n        :param int page_size: Page size of underlying requests. Higher values imply\n                              less overhead, but risk hitting the maximum message size limit.\n        :param bool descending: If set to ``True`` events are fetched in reverse\n                                order (most recent first).\n        :rtype: ~collections.Iterable[.Event]\n        \"\"\"\n        params = {\n            'order': 'desc' if descending else 'asc',\n        }\n        if source is not None:\n            params['source'] = source\n        if page_size is not None:\n            params['limit'] = page_size\n        if severity is not None:\n            params['severity'] = severity\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n        if text_filter is not None:\n            params['q'] = text_filter\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/archive/{}/events'.format(self._instance),\n            params=params,\n            response_class=rest_pb2.ListEventsResponse,\n            items_key='event',\n            item_mapper=Event,\n        )", "response": "Reads events between the specified start and stop time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_parameter_values(self, parameter, start=None, stop=None,\n                                sample_count=500, parameter_cache='realtime',\n                                source='ParameterArchive'):\n        \"\"\"\n        Returns parameter samples.\n\n        The query range is split in sample intervals of equal length. For\n        each interval a :class:`.Sample` is returned which describes the\n        min, max, count and avg during that interval.\n\n        Note that sample times are determined without considering the\n        actual parameter values. Two separate queries with equal start/stop\n        arguments will always return the same number of samples with the\n        same timestamps. This is done to ease merging of multiple sample\n        series. You should always be explicit about the ``start`` and ``stop``\n        times when relying on this property.\n\n        :param str parameter: Either a fully-qualified XTCE name or an alias in the\n                              format ``NAMESPACE/NAME``.\n        :param ~datetime.datetime start: Minimum generation time of the sampled\n                                         parameter values (inclusive). If not set\n                                         this defaults to one hour ago.\n        :param ~datetime.datetime stop: Maximum generation time of the sampled\n                                        parameter values (exclusive). If not set\n                                        this defaults to the current time.\n        :param int sample_count: The number of returned samples.\n        :param str parameter_cache: Specify the name of the processor who's\n                                    parameter cache is merged with already\n                                    archived values. To disable results from\n                                    the parameter cache, set this to ``None``.\n        :param str source: Specify how to retrieve parameter values. By\n                           default this uses the ``ParameterArchive`` which\n                           is optimized for retrieval. For Yamcs instances\n                           that do not enable the ``ParameterArchive``, you can\n                           still get results by specifying ``replay`` as the\n                           source. Replay requests take longer to return because\n                           the data needs to be reprocessed.\n        :rtype: .Sample[]\n        \"\"\"\n        path = '/archive/{}/parameters{}/samples'.format(\n            self._instance, parameter)\n        now = datetime.utcnow()\n        params = {\n            'count': sample_count,\n            'source': source,\n            'start': to_isostring(now - timedelta(hours=1)),\n            'stop': to_isostring(now),\n        }\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n\n        if parameter_cache:\n            params['processor'] = parameter_cache\n        else:\n            params['norealtime'] = True\n\n        response = self._client.get_proto(path=path, params=params)\n        message = pvalue_pb2.TimeSeries()\n        message.ParseFromString(response.content)\n        samples = getattr(message, 'sample')\n        return [Sample(s) for s in samples]", "response": "Returns a list of samples for the specified parameter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_parameter_ranges(self, parameter, start=None, stop=None,\n                              min_gap=None, max_gap=None,\n                              parameter_cache='realtime'):\n        \"\"\"\n        Returns parameter ranges between the specified start and stop time.\n\n        Each range indicates an interval during which this parameter's\n        value was uninterrupted and unchanged.\n\n        Ranges are a good fit for retrieving the value of a parameter\n        that does not change frequently. For example an on/off indicator\n        or some operational status. Querying ranges will then induce\n        much less overhead than manually processing the output of\n        :meth:`list_parameter_values` would.\n\n        The maximum number of returned ranges is limited to 500.\n\n        :param str parameter: Either a fully-qualified XTCE name or an alias in the\n                              format ``NAMESPACE/NAME``.\n        :param ~datetime.datetime start: Minimum generation time of the considered\n                                         values (inclusive)\n        :param ~datetime.datetime stop: Maximum generation time of the considered\n                                        values (exclusive)\n        :param float min_gap: Time in seconds. Any gap (detected based on parameter\n                              expiration) smaller than this will be ignored.\n                              However if the parameter changes value, the ranges\n                              will still be split.\n        :param float max_gap: Time in seconds. If the distance between two\n                              subsequent parameter values is bigger than\n                              this value (but smaller than the parameter\n                              expiration), then an artificial gap is\n                              created. This also applies if there is no\n                              expiration defined for the parameter.\n        :param str parameter_cache: Specify the name of the processor who's\n                                    parameter cache is merged with already\n                                    archived values. To disable results from\n                                    the parameter cache, set this to ``None``.\n        :rtype: .ParameterRange[]\n        \"\"\"\n        path = '/archive/{}/parameters{}/ranges'.format(\n            self._instance, parameter)\n        params = {}\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n        if min_gap is not None:\n            params['minGap'] = int(min_gap * 1000)\n        if max_gap is not None:\n            params['maxGap'] = int(max_gap * 1000)\n\n        if parameter_cache:\n            params['processor'] = parameter_cache\n        else:\n            params['norealtime'] = True\n\n        response = self._client.get_proto(path=path, params=params)\n        message = pvalue_pb2.Ranges()\n        message.ParseFromString(response.content)\n        ranges = getattr(message, 'range')\n        return [ParameterRange(r) for r in ranges]", "response": "This method returns a list of parameter ranges between the specified start and stop time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the values of a specific parameter in the specified start and stop time.", "response": "def list_parameter_values(self, parameter, start=None, stop=None,\n                              page_size=500, descending=False,\n                              parameter_cache='realtime',\n                              source='ParameterArchive'):\n        \"\"\"\n        Reads parameter values between the specified start and stop time.\n\n        :param str parameter: Either a fully-qualified XTCE name or an alias in the\n                              format ``NAMESPACE/NAME``.\n        :param ~datetime.datetime start: Minimum generation time of the returned\n                                         values (inclusive)\n        :param ~datetime.datetime stop: Maximum generation time of the returned\n                                        values (exclusive)\n        :param int page_size: Page size of underlying requests. Higher values imply\n                              less overhead, but risk hitting the maximum message size limit.\n        :param bool descending: If set to ``True`` values are fetched in reverse\n                                order (most recent first).\n        :param str parameter_cache: Specify the name of the processor who's\n                                    parameter cache is merged with already\n                                    archived values. To disable results from\n                                    the parameter cache, set this to ``None``.\n        :param str source: Specify how to retrieve parameter values. By\n                           default this uses the ``ParameterArchive`` which\n                           is optimized for retrieval. For Yamcs instances\n                           that do not enable the ``ParameterArchive``, you can\n                           still get results by specifying ``replay`` as the\n                           source. Replay requests take longer to return because\n                           the data needs to be reprocessed.\n        :rtype: ~collections.Iterable[.ParameterValue]\n        \"\"\"\n        params = {\n            'source': source,\n            'order': 'desc' if descending else 'asc',\n        }\n        if page_size is not None:\n            params['limit'] = page_size\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n\n        if parameter_cache:\n            params['processor'] = parameter_cache\n        else:\n            params['norealtime'] = True\n\n        return pagination.Iterator(\n            client=self._client,\n            path='/archive/{}/parameters{}'.format(self._instance, parameter),\n            params=params,\n            response_class=rest_pb2.ListParameterValuesResponse,\n            items_key='parameter',\n            item_mapper=ParameterValue,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread command history entries between the specified start and stop time.", "response": "def list_command_history(self, command=None, start=None, stop=None,\n                             page_size=500, descending=False):\n        \"\"\"\n        Reads command history entries between the specified start and stop time.\n\n        :param str command: Either a fully-qualified XTCE name or an alias in the\n                            format ``NAMESPACE/NAME``.\n        :param ~datetime.datetime start: Minimum generation time of the returned\n                                         command history entries (inclusive)\n        :param ~datetime.datetime stop: Maximum generation time of the returned\n                                        command history entries (exclusive)\n        :param int page_size: Page size of underlying requests. Higher values imply\n                              less overhead, but risk hitting the maximum message size limit.\n        :param bool descending: If set to ``True`` results are fetched in reverse\n                                order (most recent first).\n        :rtype: ~collections.Iterable[.CommandHistory]\n        \"\"\"\n        params = {\n            'order': 'desc' if descending else 'asc',\n        }\n        if page_size is not None:\n            params['limit'] = page_size\n        if start is not None:\n            params['start'] = to_isostring(start)\n        if stop is not None:\n            params['stop'] = to_isostring(stop)\n\n        if command:\n            path = '/archive/{}/commands{}'.format(self._instance, command)\n        else:\n            path = '/archive/{}/commands'.format(self._instance)\n\n        return pagination.Iterator(\n            client=self._client,\n            path=path,\n            params=params,\n            response_class=rest_pb2.ListCommandsResponse,\n            items_key='entry',\n            item_mapper=CommandHistory,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_tables(self):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        path = '/archive/{}/tables'.format(self._instance)\n        response = self._client.get_proto(path=path)\n        message = rest_pb2.ListTablesResponse()\n        message.ParseFromString(response.content)\n        tables = getattr(message, 'table')\n        return iter([Table(table) for table in tables])", "response": "Returns the existing tables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a single table.", "response": "def get_table(self, table):\n        \"\"\"\n        Gets a single table.\n\n        :param str table: The name of the table.\n        :rtype: .Table\n        \"\"\"\n        path = '/archive/{}/tables/{}'.format(self._instance, table)\n        response = self._client.get_proto(path=path)\n        message = archive_pb2.TableInfo()\n        message.ParseFromString(response.content)\n        return Table(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_streams(self):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        path = '/archive/{}/streams'.format(self._instance)\n        response = self._client.get_proto(path=path)\n        message = rest_pb2.ListStreamsResponse()\n        message.ParseFromString(response.content)\n        streams = getattr(message, 'stream')\n        return iter([Stream(stream) for stream in streams])", "response": "Returns the existing streams."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_stream(self, stream):\n        path = '/archive/{}/streams/{}'.format(self._instance, stream)\n        response = self._client.get_proto(path=path)\n        message = archive_pb2.StreamInfo()\n        message.ParseFromString(response.content)\n        return Stream(message)", "response": "Gets a single stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_stream_subscription(self, stream, on_data, timeout=60):\n        options = rest_pb2.StreamSubscribeRequest()\n        options.stream = stream\n\n        manager = WebSocketSubscriptionManager(\n            self._client, resource='stream', options=options)\n\n        # Represent subscription as a future\n        subscription = WebSocketSubscriptionFuture(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_stream_data, subscription, on_data)\n\n        manager.open(wrapped_callback, instance=self._instance)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription", "response": "Create a new subscription."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_sql(self, statement):\n        path = '/archive/{}/sql'.format(self._instance)\n        req = archive_pb2.ExecuteSqlRequest()\n        req.statement = statement\n\n        response = self._client.post_proto(path=path,\n                                           data=req.SerializeToString())\n        message = archive_pb2.ExecuteSqlResponse()\n        message.ParseFromString(response.content)\n        if message.HasField('result'):\n            return message.result\n        return None", "response": "Executes a single SQL statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies a file from source to destination", "response": "def copy_file(source, destination, unique=False, sort=False, case_sensitive=True, create_path=False):\n    \"\"\"\n    Python utility to create file\n\n    Args:\n        source: absolute/relative path of source file\n        destination: absolute/relative path of destination file.\n                     Use same as source for replacing the content of existing file.\n        unique: Copy only unique lines from file\n        sort: Sort the content of file\n        case_sensitive: unique/sort operations to be performed case-sensitive string\n        create_path: Recursively create the path to destination directory in case not found\n\n    Returns: None\n\n    \"\"\"\n    _File.copy(source, destination, unique, sort, case_sensitive, create_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_details(self):\n        title = str(self.get_title()).strip()\n        artist = str(self.get_artist()).strip()\n        album = str(self.get_album()).strip()\n        year = str(self.get_year()).strip()\n\n        return {\n            \"title\": title,\n            \"artist\": artist,\n            \"album\": album,\n            \"year\": year\n        }", "response": "Finds songs details\n\n        :return: Dictionary with songs details about title, artist, album and\n            year"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the attribute of song", "response": "def _set_attr(self, attribute):\n        \"\"\"Sets attribute of song\n\n        :param attribute: Attribute to save\n        :return: True iff operation completed\n        \"\"\"\n\n        self.tags.add(attribute)\n        self.song.save()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_title(self, name):\n        self._set_attr(TIT2(encoding=3, text=name.decode('utf-8')))", "response": "Sets song s title"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_artist(self, artist):\n        self._set_attr(TPE1(encoding=3, text=artist.decode('utf-8')))", "response": "Sets song s artist"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting song s album", "response": "def set_album(self, album):\n        \"\"\"Sets song's album\n\n        :param album: album\n        \"\"\"\n        self._set_attr(TALB(encoding=3, text=album.decode('utf-8')))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_nr_track(self, nr_track):\n        self._set_attr(TRCK(encoding=3, text=str(nr_track)))", "response": "Sets song s track numb\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_year(self, year):\n        self._set_attr(TDRC(encoding=3, text=str(year)))", "response": "Sets the song s year\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset song s genre attribute.", "response": "def set_genre(self, genre):\n        \"\"\"Sets song's genre\n\n        :param genre: genre\n        \"\"\"\n        self._set_attr(TCON(encoding=3, text=str(genre)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the QStimulusModel for the StimulusView and sets the QStimulusModel in the Controls and Controls widgets for the StimulusView.", "response": "def setModel(self, model):\n        \"\"\"Sets the QStimulusModel *model* for the StimulusView\"\"\"\n        # disconnect old signals\n        try:\n            self.ui.parametizer.randomizeCkbx.toggled.disconnect()\n            self.ui.parametizer.randomizeCkbx.disconnect()\n        except TypeError:\n            # disconnecting without any current connections throws error\n            pass\n\n        self.ui.trackview.setModel(model)\n        self.ui.nrepsSpnbx.setValue(model.repCount())\n\n        self.ui.parametizer.randomizeCkbx.toggled.connect(model.randomToggle)\n        self.ui.parametizer.randomizeCkbx.setChecked(bool(model.reorder()))\n\n        # extract the QAutoParameterModel from the QStimulusModel and \n        # set in the AutoParameterView\n        autoParamModel = model.autoParams()\n        self.ui.parametizer.setModel(autoParamModel)\n        # whether the auto parameters are emtpy \n        # affects the enable-ness of the StimlusView\n        autoParamModel.emptied.connect(self.ui.trackview.emptySelection)\n        autoParamModel.countChanged.connect(self.updateTraceCount)\n        self.updateTraceCount()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setRepCount(self, count):\n        self._rep_default_cache[0] = count\n        self.ui.trackview.model().setRepCount(count)", "response": "Sets the repetition count for the stimulus model"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the trace count label with the data from the model", "response": "def updateTraceCount(self):\n        \"\"\"Updates the trace count label with the data from the model\"\"\"\n        self.ui.ntracesLbl.setNum(self.ui.trackview.model().traceCount())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preview(self):\n        msg = self.ui.trackview.model().verify()\n        if msg:\n            answer = QtGui.QMessageBox.warning(self, \"Bummer\", 'Problem: {}.'.format(msg))\n            return\n        stim_signal, atten, ovld = self.ui.trackview.model().signal()\n        fig = SpecWidget()\n        fig.setWindowModality(2) # application modal\n        fig.updateData(stim_signal, self.ui.trackview.model().samplerate())\n        fig.setTitle('Stimulus Preview')\n        fig.show()\n        self.previewFig = fig", "response": "Assemble the current components in the QStimulusModel and generate a spectrogram \n        plot in a separate window"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that the arguments are at least mostly valid.", "response": "def assertpathsandfiles(self):\n        \"\"\"Assertions to make sure that arguments are at least mostly valid\"\"\"\n        # Assertion to ensure that the MiSeq path exists\n        assert os.path.isdir(self.miseqpath), u'MiSeqPath is not a valid directory {0!r:s}'.format(self.miseqpath)\n        # If the miseq folder name is not provided, the default of the most recent run will be used\n        if not self.miseqfolder:\n            # Get a list of folders\n            miseqfolders = glob('{}*/'.format(self.miseqpath))\n            self.miseqfolder = sorted(miseqfolders)[-1]\n            # Create :miseqfoldername to store the name of this folder by splitting the path and taking the second\n            # last piece (it's not the last piece because the folder has a trailing slash)\n            self.miseqfoldername = self.miseqfolder.split(\"/\")[-2]\n        # Otherwise add the folder to the miseq path to yield the destination folder\n        else:\n            # Set the folder name before adding the path to the miseq path\n            self.miseqfoldername = self.miseqfolder\n            self.miseqfolder = self.miseqpath + self.miseqfolder + \"/\"\n            # Assert to ensure that the folder exists\n            assert os.path.isdir(self.miseqfolder), u'MiSeqFolder is not a valid directory {0!r:s}'\\\n                .format(self.miseqfolder)\n        # Pull the data from the SampleSheet.csv\n        if self.customsamplesheet:\n            self.samplesheet = self.customsamplesheet\n            assert os.path.isfile(self.customsamplesheet), u'Could not find CustomSampleSheet as entered: {0!r:s}'\\\n                .format(self.customsamplesheet)\n        # Otherwise use the SampleSheet.csv located in :self.miseqfolder\n        else:\n            self.samplesheet = self.miseqfolder + \"SampleSheet.csv\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncounting the number of samples in the samplesheet", "response": "def numberofsamples(self):\n        \"\"\"Count the number of samples is the samplesheet\"\"\"\n        # Initialise variables to store line data\n        idline = 0\n        linenumber = 0\n        # Parse the sample sheet to find the number of samples\n        with open(self.samplesheet, \"rb\") as ssheet:\n            # Use enumerate to iterate through the lines in the sample sheet to retrieve the line number and the data\n            for linenumber, entry in enumerate(ssheet):\n                # Once Sample_ID is encountered\n                if \"Sample_ID\" in entry:\n                    # Set the id line as the current line number\n                    idline = linenumber\n        # :samplecount is the last line number in the file minus the line number of Sample_ID\n        self.samplecount = linenumber - idline\n        printtime('There are {} samples in this run. '\n                  'Running off-hours module with the following parameters:\\n'\n                  'MiSeqPath: {},\\n'\n                  'MiSeqFolder: {},\\n'\n                  'SampleSheet: {}'.format(self.samplecount, self.miseqpath, self.miseqfolder, self.samplesheet),\n                  self.start)\n        # Run the fastqmover module now that the number of sequences is known\n        self.fastqlinker()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_packet_range():\n    first_packet = next(iter(archive.list_packets()))\n    last_packet = next(iter(archive.list_packets(descending=True)))\n    print('First packet:', first_packet)\n    print('Last packet:', last_packet)\n\n    td = last_packet.generation_time - first_packet.generation_time\n    print('Timespan:', td)", "response": "Print the range of archived packets."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncounts the number of packets in a specific range.", "response": "def iterate_specific_packet_range():\n    \"\"\"Count the number of packets in a specific range.\"\"\"\n    now = datetime.utcnow()\n    start = now - timedelta(hours=1)\n\n    total = 0\n    for packet in archive.list_packets(start=start, stop=now):\n        total += 1\n        # print(packet)\n    print('Found', total, 'packets in range')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncounts the number of events in a specific range.", "response": "def iterate_specific_event_range():\n    \"\"\"Count the number of events in a specific range.\"\"\"\n    now = datetime.utcnow()\n    start = now - timedelta(hours=1)\n\n    total = 0\n    for event in archive.list_events(start=start, stop=now):\n        total += 1\n        # print(event)\n    print('Found', total, 'events in range')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting the last 10 values.", "response": "def print_last_values():\n    \"\"\"Print the last 10 values.\"\"\"\n    iterable = archive.list_parameter_values('/YSS/SIMULATOR/BatteryVoltage1',\n                                             descending=True)\n    for pval in islice(iterable, 0, 10):\n        print(pval)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount the number of parameter values in a specific range.", "response": "def iterate_specific_parameter_range():\n    \"\"\"Count the number of parameter values in a specific range.\"\"\"\n    now = datetime.utcnow()\n    start = now - timedelta(hours=1)\n\n    total = 0\n    for pval in archive.list_parameter_values(\n            '/YSS/SIMULATOR/BatteryVoltage1', start=start, stop=now):\n        total += 1\n        # print(pval)\n    print('Found', total, 'parameter values in range')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the last 10 commands.", "response": "def print_last_commands():\n    \"\"\"Print the last 10 commands.\"\"\"\n    iterable = archive.list_command_history(descending=True)\n    for entry in islice(iterable, 0, 10):\n        print(entry)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transmogrify(l):\n  d = {l[0]: {}}\n  tmp = d\n  for c in l:\n    tmp[c] = {}\n    tmp = tmp[c]\n  return d", "response": "Fit a flat list into a treeable object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint a pretty tree with root node.", "response": "def tree(node, formatter=None, prefix=None, postfix=None, _depth=1):\n  \"\"\"Print a tree.\n\n  Sometimes it's useful to print datastructures as a tree. This function prints\n  out a pretty tree with root `node`. A tree is represented as a :class:`dict`,\n  whose keys are node names and values are :class:`dict` objects for sub-trees\n  and :class:`None` for terminals.\n\n  :param dict node: The root of the tree to print.\n  :param callable formatter: A callable that takes a single argument, the key,\n    that formats the key in the tree.\n  :param callable prefix: A callable that takes a single argument, the key,\n    that adds any additional text before the formatted key.\n  :param callable postfix: A callable that takes a single argument, the key,\n    that adds any additional text after the formatted key.\n\n  \"\"\"\n  current = 0\n  length = len(node.keys())\n  tee_joint = '\\xe2\\x94\\x9c\\xe2\\x94\\x80\\xe2\\x94\\x80'\n  elbow_joint = '\\xe2\\x94\\x94\\xe2\\x94\\x80\\xe2\\x94\\x80'\n  for key, value in node.iteritems():\n    current += 1\n    k = formatter(key) if formatter else key\n    pre = prefix(key) if prefix else ''\n    post = postfix(key) if postfix else ''\n    space = elbow_joint if current == length else tee_joint\n    yield ' {space} {prefix}{key}{postfix}'.format(space=space, key=k, prefix=pre, postfix=post)\n    if value:\n      for e in tree(value, formatter=formatter, prefix=prefix, postfix=postfix, _depth=_depth + 1):\n        yield (' |  ' if current != length else '    ') + e"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assemble_chain(leaf, store):\n  store_dict = {}\n  for cert in store:\n    store_dict[cert.get_subject().CN] = cert\n\n  chain = [leaf]\n\n  current = leaf\n  try:\n    while current.get_issuer().CN != current.get_subject().CN:\n      chain.append(store_dict[current.get_issuer().CN])\n      current = store_dict[current.get_issuer().CN]\n  except KeyError:\n    invalid = crypto.X509()\n    patch_certificate(invalid)\n    invalid.set_subject(current.get_issuer())\n    chain.append(invalid)\n\n  chain.reverse()\n  return chain", "response": "Assemble the trust chain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, *args, **kwargs):\n        if self.win:\n            self.win.deleteLater()\n        mayawin = maya_main_window()\n        self.win = ReftrackWin(self.inter, parent=mayawin)\n        self.win.destroyed.connect(self.win_destroyed)\n        self.win.show()\n        self.win.wrap_scene()", "response": "Start genesis\n\n        :returns: None\n        :rtype: None\n        :raises: None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_api_content(self):\n\n        if GITHUB_TOKEN is not None:\n            self.add_params_to_url({\n                \"access_token\": GITHUB_TOKEN\n            })\n\n        api_content_response = requests.get(self.api_url)\n        self.api_content = json.loads(\n            api_content_response.text\n        )", "response": "Updates class api content by calling Github api and storing result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching repos in Trending Daily Github section", "response": "def get_trending_daily(lang=\"\"):\n        \"\"\"Fetches repos in \"Trending Daily\" Github section\n\n        :param lang: Coding language\n        :return: List of GithubUserRepository\n        \"\"\"\n        url = \"https://github.com/trending/\"\n        url += str(lang).lower().replace(\" \", \"\") + \"?since=daily\"\n        api_content_request = urllib.request.Request(url)\n        api_content_response = urllib.request.urlopen(\n            api_content_request).read().decode(\"utf-8\")  # parse response\n        soup = BeautifulSoup(api_content_response, \"lxml\")  # html parser\n        raw_repo_list = soup.find(\n            \"ol\", {\"class\": \"repo-list\"}\n        ).find_all(\"li\")\n\n        repos_list = []\n        for repo in raw_repo_list:\n            details = repo.find_all(\"div\")[0].a.text.split(\"/\")\n            repo_owner = details[0].strip()\n            repo_name = details[1].strip()\n            repos_list.append(GithubUserRepository(repo_owner, repo_name))\n        return repos_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_email(self):\n        api_url = self.api_url + \"/events/public\"\n        api_content = GithubRawApi(\n            api_url,\n            get_api_content_now=True\n        ).api_content\n\n        for event in api_content:\n            if event[\"type\"] == \"PushEvent\":\n                return event[\"payload\"][\"commits\"][0][\"author\"][\"email\"]", "response": "Gets email of user\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets repos in given url", "response": "def _get_repos(url):\n        \"\"\"Gets repos in url\n\n        :param url: Url\n        :return: List of repositories in given url\n        \"\"\"\n        current_page = 1\n        there_is_something_left = True\n        repos_list = []\n\n        while there_is_something_left:\n            api_driver = GithubRawApi(\n                url,\n                url_params={\"page\": current_page},\n                get_api_content_now=True\n            )  # driver to parse API content\n\n            for repo in api_driver.api_content:  # list of raw repository\n                repo_name = repo[\"name\"]\n                repo_user = repo[\"owner\"][\"login\"]\n                repos_list.append(\n                    GithubUserRepository(repo_user, repo_name))\n\n            there_is_something_left = bool(api_driver.api_content)\n            current_page += 1\n\n        return repos_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all user repositories", "response": "def get_all_repos(self):\n        \"\"\"Gets user repos\n        :return: List of all user repositories (public, orgs and private)\n        \"\"\"\n        url = \"https://api.github.com/user/repos\"\n        params = {\n            \"access_token\": GITHUB_TOKEN\n        }  # add auth params\n        url = add_params_to_url(url, params)\n        return self._get_repos(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_starred_repos(self):\n        starred_url = self.api_url + \"/starred\"\n        keep_finding = True  # False when there are no more stars to find\n        current_page = 1\n        repos_list = []\n\n        while keep_finding:\n            api_url = starred_url + \"?page=\" + str(\n                current_page)  # request starred list url with page number\n            api_driver = GithubRawApi(\n                api_url,\n                True\n            )  # driver to parse API content\n            for repo in api_driver:\n                repo_username = repo[\"owner\"][\"login\"]\n                repo_name = repo[\"name\"]\n                repos_list.append(\n                    GithubUserRepository(repo_username, repo_name))\n\n            if len(api_driver.api_content) < 1:  # no more repo to find\n                keep_finding = False\n            current_page += 1  # increase page counter\n\n        return repos_list", "response": "Gets repos starred by user\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget trending repositories which are not starred by user", "response": "def get_trending_daily_not_starred(self):\n        \"\"\"Gets trending repositories NOT starred by user\n        :return: List of daily-trending repositories which are not starred\n        \"\"\"\n        trending_daily = self.get_trending_daily()  # repos trending daily\n        starred_repos = self.get_starred_repos()  # repos starred by user\n        repos_list = []\n\n        for repo in trending_daily:\n            if repo not in starred_repos:\n                repos_list.append(repo)\n\n        return repos_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dict_of_vars_to_vcf_file(variants, outfile):\n    '''Input is dict made by vcf_file_read.vcf_file_to_dict_of_vars or\n    vcf_file_read.vcf_file_to_dict_of_vars. Output is bare-bones VCF\n    file (columns empty wherever possible'''\n    header_lines = [\n        '##fileformat=VCFv4.2',\n        '##source=cluster_vcf_records, version ' + cluster_vcf_records_version,\n        '##fileDate=' + str(datetime.date.today()),\n        '\\t'.join(['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO'])\n    ]\n\n    with open(outfile, 'w') as f:\n        print(*header_lines, sep='\\n', file=f)\n\n        for ref_name in sorted(variants):\n            for pos in sorted(variants[ref_name]):\n                for ref_string in sorted(variants[ref_name][pos]):\n                    alts = sorted(list(variants[ref_name][pos][ref_string]))\n                    print(ref_name, pos + 1, '.', ref_string, ','.join(alts), '.', 'PASS', 'SVTYPE=MERGED', sep='\\t', file=f)", "response": "This function takes a dict made by vcf_file_read. vcf_file_to_dict_of_vars and outputs a VCF\n    file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges VCF files into a single file.", "response": "def merge_vcf_files(infiles, ref_seqs, outfile, threads=1):\n    '''infiles: list of input VCF file to be merge.\n    outfile: name of output VCF file.\n    threads: number of input files to read in parallel'''\n    vars_dict = vcf_file_read.vcf_files_to_dict_of_vars(infiles, ref_seqs, threads=threads)\n    _dict_of_vars_to_vcf_file(vars_dict, outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collect_commands(package_name=None, in_place=False, level=1):\n    commands = {}\n\n    frame = inspect.stack()[level][0]\n    f_globals = frame.f_globals\n\n    if package_name is None:\n        # Collect from package containing module of call site\n        package_name = f_globals['__name__'].rsplit('.', 1)[0]\n        package_paths = [os.path.dirname(f_globals['__file__'])]\n    else:\n        # Collect from named package\n        package = importlib.import_module(package_name)\n        package_name = package.__name__\n        package_paths = package.__path__\n\n    for package_path in package_paths:\n        package_path = pathlib.Path(package_path)\n        for file in package_path.rglob('*.py'):\n            rel_path = str(file.relative_to(package_path))\n            rel_path = rel_path[:-3]\n            module_name = rel_path.replace(os.sep, '.')\n            module_name = '.'.join((package_name, module_name))\n            module = importlib.import_module(module_name)\n            module_commands = get_commands_in_namespace(module)\n            commands.update(module_commands)\n\n    commands = OrderedDict((name, commands[name]) for name in sorted(commands))\n\n    if in_place:\n        f_globals.update(commands)\n\n    return commands", "response": "Collect commands from a package and its subpackages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all commands in a namespace.", "response": "def get_commands_in_namespace(namespace=None, level=1):\n    \"\"\"Get commands in namespace.\n\n    Args:\n        namespace (dict|module): Typically a module. If not passed, the\n            globals from the call site will be used.\n        level (int): If not called from the global scope, set this\n            appropriately to account for the call stack.\n\n    Returns:\n        OrderedDict: The commands found in the namespace, ordered by\n            name.\n\n    Can be used to create ``__all__`` lists::\n\n        __all__ = list(get_commands_in_namespace())\n\n    \"\"\"\n    from ..command import Command  # noqa: Avoid circular import\n    commands = {}\n    if namespace is None:\n        frame = inspect.stack()[level][0]\n        namespace = frame.f_globals\n    elif inspect.ismodule(namespace):\n        namespace = vars(namespace)\n    for name in namespace:\n        obj = namespace[name]\n        if isinstance(obj, Command):\n            commands[name] = obj\n    return OrderedDict((name, commands[name]) for name in sorted(commands))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nchange the inclusion of the given index in the selection model", "response": "def select(self, index, command=QtGui.QItemSelectionModel.Toggle):\n        \"\"\"Changes the inclusion of the given *index* in the selection model\"\"\"\n        component = self.model().data(index, QtCore.Qt.UserRole)\n        self.selectComponent(component, index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting the given component according to the given command.", "response": "def selectComponent(self, component, index=None, command=QtGui.QItemSelectionModel.Toggle):\n        \"\"\"Selects the given *component* according to *command* policy\n        (actually, only toggle supported). If index is None, looks up index\n        of component in model\"\"\"\n        if command == QtGui.QItemSelectionModel.Toggle:\n            if component in self._selectedComponents:\n                self._selectedComponents.remove(component)\n                if index is None:\n                    index = self.model().indexByComponent(component)\n                self.selectionChanged.emit(self.selection(), QtGui.QItemSelection(index, index))\n                if len(self._selectedComponents) == 0:\n                    self.hintRequested.emit('Select Components in view to modify')\n            else:\n                self._selectedComponents.append(component)\n                self.selectionChanged.emit(self.selection(), QtGui.QItemSelection())\n                self.hintRequested.emit('Select more components, or click again to toggle inclusion. To edit parameter type or bounds, select parameter field in table')\n        else:\n            raise Exception(\"Selection command not supported\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of QModelIndex currently in the model", "response": "def selectedIndexes(self):\n        \"\"\"Returns a list of QModelIndex currently in the model\"\"\"\n        model = self.model()\n        indexes = []\n        for comp in self._selectedComponents:\n            index = model.indexByComponent(comp)\n            if index is None:\n                # must have been removed from model, discard\n                self._selectedComponents.remove(comp)\n            else:\n                indexes.append(index)\n        return indexes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef selection(self):\n        sel = QtGui.QItemSelection()\n        for index in self.selectedIndexes():\n            sel.select(index, index)\n        return sel", "response": "Returns items in selection as a QItemSelection object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef selectionComponents(self):\n        comps = []\n        model = self.model()\n        for comp in self._selectedComponents:\n            index = model.indexByComponent(comp)\n            if index is not None:\n                comps.append(comp)\n        return comps", "response": "Returns the names of the component types in this selection"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides the default save method.", "response": "def save(self, *args, **kwargs):\n        \"\"\"Override the default ``save`` method.\"\"\"\n        if not self.status:\n            self.status = self.DRAFT\n        # Published pages should always have a publication date\n        if self.publication_date is None and self.status == self.PUBLISHED:\n            self.publication_date = now_utc()\n        # Drafts should not, unless they have been set to the future\n        if self.status == self.DRAFT:\n            if settings.PAGE_SHOW_START_DATE:\n                if (self.publication_date and\n                        self.publication_date <= now_utc()):\n                    self.publication_date = None\n            else:\n                self.publication_date = None\n        self.last_modification_date = now_utc()\n        # let's assume there is no more broken links after a save\n        cache.delete(self.PAGE_BROKEN_LINK_KEY % self.id)\n        super(Page, self).save(*args, **kwargs)\n        # fix sites many-to-many link when the're hidden from the form\n        if settings.PAGE_HIDE_SITES and self.sites.count() == 0:\n            self.sites.add(Site.objects.get(pk=settings.SITE_ID))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninvalidates the cache for this page.", "response": "def invalidate(self):\n        \"\"\"Invalidate cached data for this page.\"\"\"\n\n        cache.delete(self.PAGE_LANGUAGES_KEY % (self.id))\n        cache.delete('PAGE_FIRST_ROOT_ID')\n        self._languages = None\n        self._complete_slug = None\n        self._content_dict = dict()\n\n        p_names = [p.name for p in get_placeholders(self.get_template())]\n        if 'slug' not in p_names:\n            p_names.append('slug')\n        if 'title' not in p_names:\n            p_names.append('title')\n        # delete content cache, frozen or not\n        for name in p_names:\n            # frozen\n            cache.delete(PAGE_CONTENT_DICT_KEY % (self.id, name, 1))\n            # not frozen\n            cache.delete(PAGE_CONTENT_DICT_KEY % (self.id, name, 0))\n\n        cache.delete(self.PAGE_URL_KEY % (self.id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all used languages for this page.", "response": "def get_languages(self):\n        \"\"\"\n        Return a list of all used languages for this page.\n        \"\"\"\n        if self._languages:\n            return self._languages\n        self._languages = cache.get(self.PAGE_LANGUAGES_KEY % (self.id))\n        if self._languages is not None:\n            return self._languages\n\n        languages = [c['language'] for\n                            c in Content.objects.filter(page=self,\n                            type=\"slug\").values('language')]\n        # remove duplicates\n        languages = list(set(languages))\n        languages.sort()\n        cache.set(self.PAGE_LANGUAGES_KEY % (self.id), languages)\n        self._languages = languages\n        return languages"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_first_root(self):\n        if self.parent:\n            return False\n        if self._is_first_root is not None:\n            return self._is_first_root\n        first_root_id = cache.get('PAGE_FIRST_ROOT_ID')\n        if first_root_id is not None:\n            self._is_first_root = first_root_id == self.id\n            return self._is_first_root\n        try:\n            first_root_id = Page.objects.root().values('id')[0]['id']\n        except IndexError:\n            first_root_id = None\n        if first_root_id is not None:\n            cache.set('PAGE_FIRST_ROOT_ID', first_root_id)\n        self._is_first_root = self.id == first_root_id\n        return self._is_first_root", "response": "Return True if this page is the first root pages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the complete slug of this page by concatenating all parent s slugs.", "response": "def get_complete_slug(self, language=None, hideroot=True):\n        \"\"\"Return the complete slug of this page by concatenating\n        all parent's slugs.\n\n        :param language: the wanted slug language.\"\"\"\n        if not language:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n\n        if self._complete_slug and language in self._complete_slug:\n            return self._complete_slug[language]\n\n        self._complete_slug = cache.get(self.PAGE_URL_KEY % (self.id))\n        if self._complete_slug is None:\n            self._complete_slug = {}\n        elif language in self._complete_slug:\n            return self._complete_slug[language]\n\n        if hideroot and settings.PAGE_HIDE_ROOT_SLUG and self.is_first_root():\n            url = u''\n        else:\n            url = u'%s' % self.slug(language)\n        for ancestor in self.get_ancestors(ascending=True):\n            url = ancestor.slug(language) + u'/' + url\n\n        self._complete_slug[language] = url\n        cache.set(self.PAGE_URL_KEY % (self.id), self._complete_slug)\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the slug of the page depending on the language.", "response": "def slug(self, language=None, fallback=True):\n        \"\"\"\n        Return the slug of the page depending on the given language.\n\n        :param language: wanted language, if not defined default is used.\n        :param fallback: if ``True``, the slug will also be searched in other \\\n        languages.\n        \"\"\"\n\n        slug = self.get_content(language, 'slug', language_fallback=fallback)\n\n        return slug"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expose_content(self):\n        placeholders = get_placeholders(self.get_template())\n        exposed_content = []\n        for lang in self.get_languages():\n            for ctype in [p.name for p in placeholders]:\n                content = self.get_content(lang, ctype, False)\n                if content:\n                    exposed_content.append(content)\n        return u\"\\r\\n\".join(exposed_content)", "response": "Return all the current content of this page into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef content_by_language(self, language):\n        placeholders = get_placeholders(self.get_template())\n        content_list = []\n        for ctype in [p.name for p in placeholders]:\n            try:\n                content = Content.objects.get_content_object(self,\n                    language, ctype)\n                content_list.append(content)\n            except Content.DoesNotExist:\n                pass\n        return content_list", "response": "Return a list of latest published content for a particluar language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndump the page s content as a python dict.", "response": "def dump_json_data(self, get_children=False):\n        \"\"\"\n        Return a python dict representation of this page for use as part of\n        a JSON export.\n        \"\"\"\n        def content_langs_ordered():\n            \"\"\"\n            Return a list of languages ordered by the page content\n            with the latest creation date in each.  This will be used\n            to maintain the state of the language_up_to_date template\n            tag when a page is restored or imported into another site.\n            \"\"\"\n            params = {'page': self}\n            if self.freeze_date:\n                params['creation_date__lte'] = self.freeze_date\n            cqs = Content.objects.filter(**params)\n            cqs = cqs.values('language').annotate(latest=Max('creation_date'))\n            return [c['language'] for c in cqs.order_by('latest')]\n        languages = content_langs_ordered()\n\n        def language_content(ctype):\n            return dict(\n                (lang, self.get_content(lang, ctype, language_fallback=False))\n                for lang in languages)\n\n        def placeholder_content():\n            \"\"\"Return content of each placeholder in each language.\"\"\"\n            out = {}\n            for p in get_placeholders(self.get_template()):\n                if p.name in ('title', 'slug'):\n                    continue  # these were already included\n                out[p.name] = language_content(p.name)\n\n            for p in Content.objects.filter(type__in=['meta_title', 'meta_description', 'meta_keywords', 'meta_author', 'fb_page_type', 'fb_image']):\n                out[p.type] = language_content(p.type)\n            return out\n\n        def isoformat(d):\n            return None if d is None else d.strftime(ISODATE_FORMAT)\n\n        def custom_email(user):\n            \"\"\"Allow a user's profile to return an email for the user.\"\"\"\n            try:\n                profile = user.get_profile()\n            except (ObjectDoesNotExist, AttributeError):\n                return user.email\n            get_email = getattr(profile, 'get_email', None)\n            return get_email() if get_email else user.email\n\n        tags = [tag.name for tag in self.tags.all()]\n\n        children = []\n        if get_children:\n            for c in self.children.filter(status__in=[self.PUBLISHED, self.HIDDEN]):\n                children.append(c.dump_json_data())\n\n        return {\n            'complete_slug': dict(\n                (lang, self.get_complete_slug(lang, hideroot=False))\n                for lang in languages),\n            'title': language_content('title'),\n            'author_email': custom_email(self.author),\n            'creation_date': isoformat(self.creation_date),\n            'publication_date': isoformat(self.publication_date),\n            'publication_end_date': isoformat(self.publication_end_date),\n            'last_modification_date': isoformat(self.last_modification_date),\n            'status': {\n                Page.PUBLISHED: 'published',\n                Page.HIDDEN: 'hidden',\n                Page.DRAFT: 'draft'}[self.status],\n            'template': self.template,\n            'sites': (\n                [site.domain for site in self.sites.all()]\n                if settings.PAGE_USE_SITE_ID else []),\n            'redirect_to_url': self.redirect_to_url,\n            'redirect_to_complete_slug': dict(\n                (lang, self.redirect_to.get_complete_slug(\n                    lang, hideroot=False))\n                for lang in self.redirect_to.get_languages()\n                ) if self.redirect_to is not None else None,\n            'content': placeholder_content(),\n            'content_language_updated_order': languages,\n            'tags': tags,\n            'children': children\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads HTTP header from socket return header and rest of data.", "response": "def read_http_header(sock):\n    \"\"\"Read HTTP header from socket, return header and rest of data.\"\"\"\n    buf = []\n    hdr_end = '\\r\\n\\r\\n'\n\n    while True:\n        buf.append(sock.recv(bufsize).decode('utf-8'))\n        data = ''.join(buf)\n        i = data.find(hdr_end)\n        if i == -1:\n            continue\n        return data[:i], data[i + len(hdr_end):]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef header_status(header):\n    status_line = header[:header.find('\\r')]\n    # 'HTTP/1.1 200 OK' -> (200, 'OK')\n    fields = status_line.split(None, 2)\n    return int(fields[1]), fields[2]", "response": "Parse HTTP status line and return status and reason."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconnects to UNIX or TCP socket.", "response": "def connect(url):\n    \"\"\"Connect to UNIX or TCP socket.\n\n        url can be either tcp://<host>:port or ipc://<path>\n    \"\"\"\n    url = urlparse(url)\n    if url.scheme == 'tcp':\n        sock = socket()\n        netloc = tuple(url.netloc.rsplit(':', 1))\n        hostname = socket.gethostname()\n    elif url.scheme == 'ipc':\n        sock = socket(AF_UNIX)\n        netloc = url.path\n        hostname = 'localhost'\n    else:\n        raise ValueError('unknown socket type: %s' % url.scheme)\n\n    sock.connect(netloc)\n    return sock, hostname"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watch(callback, url=default_sock_url):\n    sock, hostname = connect(url)\n    request = 'GET /events HTTP/1.1\\nHost: %s\\n\\n' % hostname\n    request = request.encode('utf-8')\n\n    with closing(sock):\n        sock.sendall(request)\n        header, payload = read_http_header(sock)\n        status, reason = header_status(header)\n        if status != HTTP_OK:\n            raise DockermonError('bad HTTP status: %s %s' % (status, reason))\n\n        # Messages are \\r\\n<size in hex><JSON payload>\\r\\n\n        buf = [payload]\n        while True:\n            chunk = sock.recv(bufsize)\n            if not chunk:\n                raise EOFError('socket closed')\n            buf.append(chunk.decode('utf-8'))\n            data = ''.join(buf)\n            i = data.find('\\r\\n')\n            if i == -1:\n                continue\n\n            size = int(data[:i], 16)\n            start = i + 2  # Skip initial \\r\\n\n\n            if len(data) < start + size + 2:\n                continue\n            payload = data[start:start+size]\n            callback(json.loads(payload))\n            buf = [data[start+size+2:]]", "response": "Watch docker events. Will call callback with each new event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting callback prints message to stdout as JSON in one line.", "response": "def print_callback(msg):\n    \"\"\"Print callback, prints message to stdout as JSON in one line.\"\"\"\n    json.dump(msg, stdout)\n    stdout.write('\\n')\n    stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprograming callback calls prog with message in stdin", "response": "def prog_callback(prog, msg):\n    \"\"\"Program callback, calls prog with message in stdin\"\"\"\n    pipe = Popen(prog, stdin=PIPE)\n    data = json.dumps(msg)\n    pipe.stdin.write(data.encode('utf-8'))\n    pipe.stdin.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a git tag to an acceptable eups tag", "response": "def git_tag2eups_tag(git_tag):\n    \"\"\"Convert git tag to an acceptable eups tag format\n\n    I.e., eups no likey semantic versioning markup, wants underscores\n\n    Parameters\n    ----------\n    git_tag: str\n        literal git tag string\n\n    Returns\n    -------\n    eups_tag: string\n        A string suitable for use as an eups tag name\n    \"\"\"\n    eups_tag = git_tag\n\n    # eups tags should not start with a numeric value -- prefix `v` if\n    # it does\n    if re.match(r'\\d', eups_tag):\n        eups_tag = \"v{eups_tag}\".format(eups_tag=eups_tag)\n\n    # convert '.'s and '-'s to '_'s\n    eups_tag = eups_tag.translate(str.maketrans('.-', '__'))\n\n    return eups_tag"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a non - negative integer n return a pair a b c where a b is a square - free integer and b is a square - free integer.", "response": "def isqrt(n):\n    ''' given a non-negative integer n, return a pair (a,b) such that n = a * a * b\n        where b is a square-free integer.\n\n        If n is a perfect square, then a is its square root and b is one.\n    '''\n    # TODO: replace with a more efficient implementation\n\n    if n == 0:\n        return n, 1\n    if n < 0:\n        raise ValueError('math domain error')\n\n    a, b, c = 1, n, 1\n\n    def divisors():\n        yield 2\n        yield 3\n        k = 5\n        while k * k <= b:\n            yield k\n            k += 2\n            yield k\n            k += 4\n            \n    for k in divisors():\n        d, m = divmod(b, k * k)\n        while m == 0:\n            a *= k\n            b = d\n            d, m = divmod(b, k * k)\n        if b % k == 0:\n            b //= k\n            c *= k\n\n        k += 1\n    return a, b*c"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fsqrt(q):\n    ''' given a non-negative fraction q, return a pair (a,b) such that q = a * a * b\n        where b is a square-free integer.\n\n        if q is a perfect square, a is its square root and b is one.\n    '''\n\n    if q == 0:\n        return q, 1\n    if q < 0:\n        raise ValueError('math domain error %s' % q)\n\n    a, b = isqrt(q.numerator)\n    c, d = isqrt(q.denominator)\n\n    # q == (a/c)**2 * (b/d) == (a/(c*d))**2 * b*d\n    return Fraction(a, c * d), b * d", "response": "given a non - negative fraction q return a pair of a pair of a pair of a and b where a b is a square - free integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sqrt(n):\n    '''return the square root of n in an exact representation'''\n    if isinstance(n, Rational):\n        n = Constructible(n)\n    elif not isinstance(n, Constructible):\n        raise ValueError('the square root is not implemented for the type %s' % type(n))\n\n    r = n._try_sqrt()  # pylint: disable=protected-access\n    if r is not None:\n        return r\n    return Constructible(Constructible.lift_rational_field(0, n.field),\n                         Constructible.lift_rational_field(1, n.field),\n                         (n, n.field))", "response": "return the square root of n in an exact representation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tuple of self and other such that self and other are the same size.", "response": "def join(self, other):\n        '''return a tuple (new_self, new_other) such that\n        new_self == self, new_other == other, and new_self.field == new_other.field '''\n        if self.field == other.field:\n            return self, other\n\n        _, f1, f2 = Constructible.join_fields(self.field, other.field)\n        return f1(self), f2(other)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to compute the square root in the field itself. return None if not found.", "response": "def _try_sqrt(self):\n        ''' try to compute the square root in the field itself.\n\n        if there is no square root in the field return None.\n        '''\n        if not self.field:\n            assert self.b == 0\n            root, remainder = fsqrt(self.a)\n            if remainder == 1:\n                return Constructible(root)\n            else:\n                return None\n\n        if self._sign() < 0:\n            raise ValueError('math domain error %s' % self)\n\n        nn = self.a * self.a - self.b * self.b * self.r\n        if nn._sign() < 0:\n            return None\n\n        n = nn._try_sqrt()\n        if n is None:\n            return None\n\n        a = ((self.a + n) * Fraction(1, 2))._try_sqrt()\n        if a is not None:\n            result = Constructible(a, self.b / a * Fraction(1, 2), self.field)\n            assert result.field == self.field\n            return result\n\n        b = ((self.a + n) / self.r * Fraction(1, 2))._try_sqrt()\n        if b is not None:\n            result = Constructible(self.b / b * Fraction(1, 2), b, self.field)\n            assert result.field == self.field\n            return result\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the most up - to - date rMLST profile and alleles and set the path to the new folder containing the new alleles and profile and alleles.", "response": "def getrmlsthelper(self):\n        \"\"\"\n        Makes a system call to rest_auth.py, a Python script modified from\n        https://github.com/kjolley/BIGSdb/tree/develop/scripts/test\n        And downloads the most up-to-date rMLST profile and alleles\n        \"\"\"\n        # Set the path/name of the folder to contain the new alleles and profile\n        newfolder = os.path.join(self.path, self.analysistype)\n        # Create the path\n        make_path(newfolder)\n        # Create arguments to feed into the rest_auth_class script\n        args = ArgumentParser\n        args.secret_file = os.path.join(self.credentials, 'secret.txt')\n        args.file_path = os.path.join(self.credentials)\n        args.output_path = newfolder\n        args.logging = self.logging\n        rmlst = rest_auth_class.REST(args)\n        rmlst.secret_finder()\n        rmlst.get_request_token()\n        rmlst.get_access_token()\n        # Download the profile and alleles\n        rmlst.main()\n        # Get the new alleles into a list, and create the combinedAlleles file\n        alleles = glob(os.path.join(newfolder, '*.tfa'))\n        self.combinealleles(newfolder, alleles)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads all the vcf files from filename_list and returns a tuple of tuples.", "response": "def _load_vcf_files(cls, filename_list, reference_seqs, homozygous_only=False, max_REF_len=None, min_SNP_qual=None, min_dp4=None, min_GT_conf=None):\n        '''Loads all the vcf files from filename_list. Returns tuple of:\n        1. Sample name. If more than one sample name found, uses the first one\n        and warns to stderr\n        2. Dictionary. filename => list of header lines for that file\n        3. Dictionary. ref name => list of VcfRecords sorted by position.\n\n        reference_seqs should be a dictionary of sequence name -> sequence.\n        This causes all records from the VCF to be sanity checked against the reference sequence,\n        and any records where the REF seq does not match the expected sequence is removed.'''\n        headers = {}\n        vcf_records = None\n        sample_name = None\n\n        for filename in filename_list:\n            headers[filename], new_records = vcf_file_read.vcf_file_to_dict(filename, homozygous_only=homozygous_only, remove_asterisk_alts=True, max_REF_len=max_REF_len, remove_useless_start_nucleotides=True, min_SNP_qual=min_SNP_qual, min_dp4=min_dp4, min_GT_conf=min_GT_conf, reference_seqs=reference_seqs)\n\n            new_sample_name = vcf_file_read.get_sample_name_from_vcf_header_lines(headers[filename])\n            if sample_name is None and new_sample_name is not None:\n                sample_name = new_sample_name\n            elif new_sample_name != sample_name:\n                logging.warning('Using first sample name found \"' + str(sample_name) + '\". Found a different (or no) sample name \"' + str(new_sample_name) + '\", which will not be used')\n\n            if vcf_records is None:\n                vcf_records = new_records\n            else:\n                for ref_name, record_list in new_records.items():\n                    if ref_name not in vcf_records:\n                        vcf_records[ref_name] = record_list\n                    else:\n                        vcf_records[ref_name].extend(record_list)\n\n        for record_list in vcf_records.values():\n            record_list.sort(key=operator.attrgetter('POS'))\n\n        if sample_name is None:\n            logging.warning('No sample name found in VCF files. Going to use \"sample\"')\n            sample_name = 'sample'\n\n        return sample_name, headers, vcf_records"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninput : list of vcf records. Returns new list where any records with > ALT is replaced with one vcf record per ALT.", "response": "def _expand_alts_in_vcf_record_list(cls, vcf_records):\n        '''Input: list of vcf_records. Returns new list, where\n        any records with >ALT is replaced with one vcf record per ALT.\n        This doesn't change FORMAT or INFO columns, which means they\n        are now broken for those records'''\n        new_vcf_records = []\n        for record in vcf_records:\n            new_vcf_records.extend(record.to_record_per_alt())\n        return new_vcf_records"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexpands any record in the list that has >ALT into the new VCF records and removes duplicated records.", "response": "def _expand_alts_and_remove_duplicates_in_list(cls, vcf_records, ref_seq, indel_gap=100):\n        '''Input: list of VCF records, all from the same CHROM. ref_seq = sequence\n        of that CHROM. Expands any record in the list that has >ALT, into\n        one record per ALT. Removes duplicated records, where REF and ALT\n        are the same (at the same position!), or where there is the same\n        indel more than once, but written in a different way (eg indel in\n        homopolymer run can be put in >1 way in a VCF. Checks indels\n        are the same within indel_gap nucleotides of each other'''\n        expanded_vcf_records = VcfClusterer._expand_alts_in_vcf_record_list(vcf_records)\n        new_vcf_records = [x for x in expanded_vcf_records if not x.is_snp()]\n\n\n        for i in range(len(new_vcf_records) - 1):\n            j = i + 1\n            while j < len(new_vcf_records) and new_vcf_records[i].ref_end_pos() + indel_gap > new_vcf_records[j].POS:\n                if new_vcf_records[i].is_the_same_indel(new_vcf_records[j], ref_seq):\n                    new_vcf_records.pop(j)\n                else:\n                    j += 1\n\n        new_vcf_records.extend([x for x in expanded_vcf_records if x.is_snp()])\n        new_vcf_records.sort(key=operator.attrgetter('POS'))\n        return new_vcf_records"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the number of packets grouped by packet name.", "response": "def print_packet_count():\n    \"\"\"Print the number of packets grouped by packet name.\"\"\"\n    for name in archive.list_packet_names():\n        packet_count = 0\n        for group in archive.list_packet_histogram(name):\n            for rec in group.records:\n                packet_count += rec.count\n        print('  {: <40} {: >20}'.format(name, packet_count))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting the number of processed parameter frames by group name.", "response": "def print_pp_groups():\n    \"\"\"Print the number of processed parameter frames by group name.\"\"\"\n    for group in archive.list_processed_parameter_groups():\n        frame_count = 0\n        for pp_group in archive.list_processed_parameter_group_histogram(group):\n            for rec in pp_group.records:\n                frame_count += rec.count\n        print('  {: <40} {: >20}'.format(group, frame_count))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the number of events grouped by source.", "response": "def print_event_count():\n    \"\"\"Print the number of events grouped by source.\"\"\"\n    for source in archive.list_event_sources():\n        event_count = 0\n        for group in archive.list_event_histogram(source):\n            for rec in group.records:\n                event_count += rec.count\n        print('  {: <40} {: >20}'.format(source, event_count))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the number of commands grouped by name.", "response": "def print_command_count():\n    \"\"\"Print the number of commands grouped by name.\"\"\"\n    mdb = client.get_mdb(instance='simulator')\n    for command in mdb.list_commands():\n        total = 0\n        for group in archive.list_command_histogram(command.qualified_name):\n            for rec in group.records:\n                total += rec.count\n        print('  {: <40} {: >20}'.format(command, total))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_args():\n    prog = 'github-tag-release'\n\n    parser = argparse.ArgumentParser(\n        prog=prog,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=textwrap.dedent(\"\"\"\n            Tag git repositories, in a GitHub org, that correspond to the\n            products in a published eups distrib tag.\n\n            Examples:\n\n                # eups tag is derived from git tag\n                {prog} \\\\\n                    --dry-run \\\\\n                    --debug \\\\\n                    --limit 10 \\\\\n                    --org lsst \\\\\n                    --allow-team 'Data Management' \\\\\n                    --allow-team 'DM Externals' \\\\\n                    'w.2018.18' 'b3595'\n\n                # explicit eups tag and git tag\n                {prog} \\\\\n                    --dry-run \\\\\n                    --debug \\\\\n                    --limit 10 \\\\\n                    --org lsst \\\\\n                    --allow-team 'Data Management' \\\\\n                    --allow-team 'DM Externals' \\\\\n                    --external-team 'DM Externals' \\\\\n                    --eups-tag v11_0_rc2 \\\\\n                    11.0.rc2 b1679\n\n                # verify a past eups tag + git tag release\n                {prog} \\\\\n                    --verify \\\\\n                    --debug \\\\\n                    --limit 10 \\\\\n                    --org 'lsst' \\\\\n                    --allow-team 'Data Management' \\\\\n                    --allow-team 'DM Externals' \\\\\n                    --external-team 'DM Externals' \\\\\n                    --deny-team 'DM Auxilliaries' \\\\\n                    --email 'sqre-admin@lists.lsst.org' \\\\\n                    --user 'sqreadmin' \\\\\n                    --token \"$GITHUB_TOKEN\" \\\\\n                    --ignore-git-tagger \\\\\n                    --ignore-git-message \\\\\n                    --manifest 'b3595' \\\\\n                    'w.2018.18'\n\n                # tag a git release from a manifest *without* a pre-existing\n                # eups tag.\n                {prog} \\\\\n                    --dry-run \\\\\n                    --debug \\\\\n                    --limit 10 \\\\\n                    --org 'lsst' \\\\\n                    --allow-team 'Data Management' \\\\\n                    --allow-team 'DM Externals' \\\\\n                    --external-team 'DM Externals' \\\\\n                    --deny-team 'DM Auxilliaries' \\\\\n                    --email 'sqre-admin@lists.lsst.org' \\\\\n                    --user 'sqreadmin' \\\\\n                    --token \"$GITHUB_TOKEN\" \\\\\n                    --ignore-git-tagger \\\\\n                    --ignore-git-message \\\\\n                    --manifest 'b3595' \\\\\n                    --manifest-only \\\\\n                    'w.2018.18'\n\n            Note that the access token must have access to these oauth scopes:\n                * read:org\n                * repo\n\n            The token generated by `github-auth --user` should have sufficient\n            permissions.\n        \"\"\").format(prog=prog),\n        epilog='Part of codekit: https://github.com/lsst-sqre/sqre-codekit'\n    )\n\n    parser.add_argument(\n        '--manifest',\n        required=True,\n        help='Name of versiondb manifest for git repo sha resolution'\n             ' AKA bNNNN')\n    parser.add_argument(\n        '--org',\n        required=True,\n        help='Github organization')\n    parser.add_argument(\n        '--allow-team',\n        action='append',\n        required=True,\n        help='git repos to be tagged MUST be a member of ONE or more of'\n             ' these teams (can specify several times)')\n    parser.add_argument(\n        '--external-team',\n        action='append',\n        help='git repos in this team MUST not have tags that start with a'\n             ' number. Any requested tag that violates this policy will be'\n             ' prefixed with \\'v\\' (can specify several times)')\n    parser.add_argument(\n        '--deny-team',\n        action='append',\n        help='git repos to be tagged MUST NOT be a member of ANY of'\n             ' these teams (can specify several times)')\n    parser.add_argument(\n        '--user',\n        help='Name of person making the tag - defaults to gitconfig value')\n    parser.add_argument(\n        '--email',\n        help='Email address of tagger - defaults to gitconfig value')\n    parser.add_argument(\n        '--token-path',\n        default='~/.sq_github_token_delete',\n        help='Use a token (made with github-auth) in a non-standard location')\n    parser.add_argument(\n        '--token',\n        default=None,\n        help='Literal github personal access token string')\n    parser.add_argument(\n        '--versiondb-base-url',\n        default=os.getenv('LSST_VERSIONDB_BASE_URL'),\n        help='Override the default versiondb base url')\n    parser.add_argument(\n        '--eupstag-base-url',\n        default=os.getenv('LSST_EUPSTAG_BASE_URL'),\n        help='Override the default eupstag base url')\n    parser.add_argument(\n        '--force-tag',\n        action='store_true',\n        help='Force moving pre-existing annotated git tags.')\n    parser.add_argument(\n        '--ignore-manifest-versions',\n        action='store_true',\n        help='Ignore manifest version strings'\n             ' when cross referencing eups tag and manifest data.')\n    parser.add_argument(\n        '--ignore-git-message',\n        action='store_true',\n        help='Ignore git tag message when verifying an existing tag.')\n    parser.add_argument(\n        '--ignore-git-tagger',\n        action='store_true',\n        help='Ignore git tag \"tagger\"/author when verifying an existing tag.')\n    parser.add_argument(\n        '--limit',\n        default=None,\n        type=int,\n        help='Maximum number of products/repos to tags. (useful for testing)')\n    parser.add_argument(\n        '--fail-fast',\n        action='store_true',\n        help='Fail immediately on github API error.')\n    parser.add_argument(\n        '--no-fail-fast',\n        action='store_const',\n        const=False,\n        dest='fail_fast',\n        help='DO NOT Fail immediately on github API error(s). (default)')\n    parser.add_argument(\n        '-d', '--debug',\n        action='count',\n        default=codetools.debug_lvl_from_env(),\n        help='Debug mode (can specify several times)')\n    parser.add_argument('-v', '--version', action=codetools.ScmVersionAction)\n    parser.add_argument('tag')\n\n    manifest_group = parser.add_mutually_exclusive_group()\n    manifest_group.add_argument(\n        '--eups-tag',\n        help='(mutually exclusive with --manifest-only)')\n    manifest_group.add_argument(\n        '--manifest-only',\n        action='store_true',\n        help='Do not cross reference a published eups tag with the manifest'\n             ' -- use only the metadata from the manifest to determine'\n             ' git tag location.'\n             ' This allows a git tag to be created without a prior eups tag.'\n             ' (mutually exclusive with --eups-tag)')\n\n    dryrun_group = parser.add_mutually_exclusive_group()\n    dryrun_group.add_argument(\n        '--dry-run',\n        action='store_true',\n        help='Do not create/update tag(s) or modify any state.'\n             ' (mutually exclusive with --verify)')\n    dryrun_group.add_argument(\n        '--verify',\n        action='store_true',\n        help='Verify that all git tags for a release are present and correct.'\n             ' will not create/update tag(s) or modify any state.'\n             ' (mutually exclusive with --dry-run)')\n\n    return parser.parse_args()", "response": "Parse command - line arguments and return a new instance of\nInviteable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cmp_dict(d1, d2, ignore_keys=[]):\n    # https://stackoverflow.com/questions/10480806/compare-dictionaries-ignoring-specific-keys\n    return {k: v for k, v in d1.items() if k not in ignore_keys} \\\n        == {k: v for k, v in d2.items() if k not in ignore_keys}", "response": "Compare dicts ignoring select keys"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cross_reference_products(\n    eups_products,\n    manifest_products,\n    ignore_manifest_versions=False,\n    fail_fast=False,\n):\n    \"\"\"\n    Cross reference EupsTag and Manifest data and return a merged result\n\n    Parameters\n    ----------\n    eups_products:\n    manifest:\n    fail_fast: bool\n    ignore_manifest_versions: bool\n\n    Returns\n    -------\n    products: dict\n\n    Raises\n    ------\n    RuntimeError\n        Upon error if `fail_fast` is `True`.\n    \"\"\"\n    products = {}\n\n    problems = []\n    for name, eups_data in eups_products.items():\n        try:\n            manifest_data = manifest_products[name]\n        except KeyError:\n            yikes = RuntimeError(textwrap.dedent(\"\"\"\\\n                failed to find record in manifest for:\n                  {product} {eups_version}\\\n                \"\"\").format(\n                product=name,\n                eups_version=eups_data['eups_version'],\n            ))\n            if fail_fast:\n                raise yikes from None\n            problems.append(yikes)\n            error(yikes)\n\n        if ignore_manifest_versions:\n            # ignore the manifest eups_version string by simply setting it to\n            # the eups tag value.  This ensures that the eups tag value will be\n            # passed though.\n            manifest_data = manifest_data.copy()\n            manifest_data['eups_version'] = eups_data['eups_version']\n\n        if eups_data['eups_version'] != manifest_data['eups_version']:\n            yikes = RuntimeError(textwrap.dedent(\"\"\"\\\n                eups version string mismatch:\n                  eups tag: {product} {eups_eups_version}\n                  manifest: {product} {manifest_eups_version}\\\n                \"\"\").format(\n                product=name,\n                eups_eups_version=eups_data['eups_version'],\n                manifest_eups_version=manifest_data['eups_version'],\n            ))\n            if fail_fast:\n                raise yikes\n            problems.append(yikes)\n            error(yikes)\n\n        products[name] = eups_data.copy()\n        products[name].update(manifest_data)\n\n    if problems:\n        error(\"{n} product(s) have error(s)\".format(n=len(problems)))\n\n    return products, problems", "response": "Returns a new n - grams tree that is merged with the eups tag and manifest data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an author object to a dict.", "response": "def author_to_dict(obj):\n    \"\"\"Who needs a switch/case statement when you can instead use this easy to\n    comprehend drivel?\n    \"\"\"\n    def default():\n        raise RuntimeError(\"unsupported type {t}\".format(t=type(obj).__name__))\n\n    # a more pythonic way to handle this would be several try blocks to catch\n    # missing attributes\n    return {\n        # GitAuthor has name,email,date properties\n        'GitAuthor': lambda x: {'name': x.name, 'email': x.email},\n        # InputGitAuthor only has _identity, which returns a dict\n        # XXX consider trying to rationalize this upstream...\n        'InputGitAuthor': lambda x: x._identity,\n    }.get(type(obj).__name__, lambda x: default())(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a pre - existng tag in the github repo.", "response": "def check_existing_git_tag(repo, t_tag, **kwargs):\n    \"\"\"\n    Check for a pre-existng tag in the github repo.\n\n    Parameters\n    ----------\n    repo : github.Repository.Repository\n        repo to inspect for an existing tagsdf\n    t_tag: codekit.pygithub.TargetTag\n        dict repesenting a target git tag\n\n    Returns\n    -------\n    insync : `bool`\n        True if tag exists and is in sync. False if tag does not exist.\n\n    Raises\n    ------\n    GitTagExistsError\n        If tag exists but is not in sync.\n    \"\"\"\n\n    assert isinstance(repo, github.Repository.Repository), type(repo)\n    assert isinstance(t_tag, codekit.pygithub.TargetTag), type(t_tag)\n\n    debug(\"looking for existing tag: {tag} in repo: {repo}\".format(\n        repo=repo.full_name,\n        tag=t_tag.name,\n    ))\n\n    # find ref/tag by name\n    e_ref = pygithub.find_tag_by_name(repo, t_tag.name)\n    if not e_ref:\n        debug(\"  not found: {tag}\".format(tag=t_tag.name))\n        return False\n\n    # find tag object pointed to by the ref\n    try:\n        e_tag = repo.get_git_tag(e_ref.object.sha)\n    except github.RateLimitExceededException:\n        raise\n    except github.GithubException as e:\n        msg = \"error getting tag: {tag} [{sha}]\".format(\n            tag=e_tag.tag,\n            sha=e_tag.sha,\n        )\n        raise pygithub.CaughtRepositoryError(repo, e, msg) from None\n\n    debug(\"  found existing: {tag} [{sha}]\".format(\n        tag=e_tag.tag,\n        sha=e_tag.sha,\n    ))\n\n    if cmp_existing_git_tag(t_tag, e_tag, **kwargs):\n        return True\n\n    yikes = GitTagExistsError(textwrap.dedent(\"\"\"\\\n        tag: {tag} already exists in repo: {repo}\n        with conflicting values:\n          existing:\n            sha: {e_sha}\n            message: {e_message}\n            tagger: {e_tagger}\n          target:\n            sha: {t_sha}\n            message: {t_message}\n            tagger: {t_tagger}\\\n    \"\"\").format(\n        tag=t_tag.name,\n        repo=repo.full_name,\n        e_sha=e_tag.object.sha,\n        e_message=e_tag.message,\n        e_tagger=e_tag.tagger,\n        t_sha=t_tag.sha,\n        t_message=t_tag.message,\n        t_tagger=t_tag.tagger,\n    ))\n\n    raise yikes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef can_proceed(self):\n        now = datetime.datetime.now()\n        delta = datetime.timedelta(days=self.update_interval)\n        return now >= self.last_update + delta", "response": "Checks whether the app can proceed with the current update interval."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the app lock file and updates the last update datetime.", "response": "def parse_lock(self):\n        \"\"\"Parses app lock file\n\n        :return: Details about last update\n        \"\"\"\n        try:\n            with open(self.lock_file, \"r\") as reader:\n                data = json.loads(reader.read())\n                self.last_update = datetime.datetime.strptime(\n                    data[\"last_update\"],\n                    AppCronLock.DATETIME_FORMAT\n                )\n        except:  # malformed lock file\n            self.write_lock(last_update=datetime.datetime.fromtimestamp(0))\n            self.parse_lock()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_lock(self, last_update=datetime.datetime.now()):\n        data = {\n            \"last_update\": last_update.strftime(AppCronLock.DATETIME_FORMAT)\n        }\n\n        with open(self.lock_file, \"w\") as writer:\n            json.dump(data, writer)", "response": "Writes the lock file to disk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the FDB by ID to analyze if the corresponding number may mutate by assuming the composite is of the given forms where forms is a list of forms used by the mfaliquot. composite_tau_lte function. The optional n and guide arguments are for error checking purposes.", "response": "def examine_seq(id, forms=None, n=None, guide=None, seq=None):\n     '''Query the FDB by ID to analyze if the corresponding number may mutate by assuming\n     the composite is of the given `forms`, where `forms` is a list of `form`s as used by\n     the mfaliquot.aliquot.composite_tau_lte function. The optional n and guide arguments\n     are for error checking purposes.'''\n     primes, comps = get_id_info(id)\n     if len(comps) == 0:\n          return None # json data for this seq is out of date\n     if len(comps) > 1 or list(comps.values()) != [1]:\n          raise ValueError(\"Wtf?!? two composites or composite to a power? seq {}, id {}\".format(seq.seq, seq.id))\n     c = int(list(comps.keys())[0])\n     guideprime, s, t = aq.canonical_form(primes)\n\n     # We do a cross check that the fdb and data file agree: to do this,\n     # we cut primes >9 digits from the fdb data\n     nprime = {p: a for p, a in primes.items() if len(str(p)) <= 9}\n     if (n is not None and nprime != n) or (guide is not None and guideprime != guide):\n          #raise ValueError(\"Disagreement between local file and fdb: {} {}\".format(n, nprime))\n          print(\"Weird! Seq {} apparently is bad info in the data file.\".format(seq.seq if seq else None))\n          return None\n\n     return aq.mutation_possible(primes, c, forms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_seq(seq):\n     '''Examines unreserved sequences to see if they are prone to mutation. This\n     currently ignores solely-power-of-2 guides with b > 3'''\n     if seq.res:\n          return None\n     n = nt.Factors(seq.factors)\n     guide, s, t = aq.canonical_form(n)\n     seq.guide = guide\n     # The target_tau for the composite is at most the class minus extant prime factor count\n     cls = aq.get_class(guide=guide)\n     num_larges = seq.factors.count('P')\n     upper_bound_tau = cls - num_larges - len(t)\n\n     if cls < 2 or upper_bound_tau < 2: # Cheap tests to eliminate almost all sequences\n          return None\n\n     # Next we ignore sequences whose guide is solely a power of 2 greater than 3\n     v = nt.Factors({p: a for p, a in guide.items() if p != 2 and a > 0})\n     if int(v) == 1 and cls > 3:\n          return None\n     # This condition greatly reduces fdb load, but excludes a lot of sequences\n     if not aq.is_driver(guide=guide):\n          return None\n\n     return n, guide", "response": "Examines unreserved sequences to see if they are prone to mutation. This is a helper function that returns None if the sequence is not prone to mutation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_token(self, appname, username, password):\n        ext_exception = TouchWorksException(\n            TouchWorksErrorMessages.GET_TOKEN_FAILED_ERROR)\n        data = {'Username': username,\n                'Password': password}\n        resp = self._http_request(TouchWorksEndPoints.GET_TOKEN, data)\n        try:\n            logger.debug('token : %s' % resp)\n            if not resp.text:\n                raise ext_exception\n            try:\n                uuid.UUID(resp.text, version=4)\n                return SecurityToken(resp.text)\n            except ValueError:\n                logger.error('response was not valid uuid string. %s' % resp.text)\n                raise ext_exception\n\n        except Exception as ex:\n            logger.exception(ex)\n            raise ext_exception", "response": "get the security token by connecting to TouchWorks API"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the token is valid by comparing the time token was created with current time", "response": "def _token_valid(self):\n        \"\"\"\n        checks if the token cached is valid or has expired by comparing\n        the time token was created with current time\n        :return: True if token has not expired yet and False is token is empty or\n                it has expired\n        \"\"\"\n        if not self._cache_token:\n            return False\n        now = time.time()\n        if now - self._token.acquired_time > self._token_timeout:\n            logger.debug('token needs to be reset')\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninvoke TouchWorksMagicConstants.ACTION_SAVE_NOTE action :return: JSON response", "response": "def save_note(self, note_text, patient_id,\n                  document_type,\n                  document_status='Unsigned', wrapped_in_rtf='N'):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_SAVE_NOTE action\n        :return: JSON response\n        \"\"\"\n        allowed_document_status = ['Unsigned', 'Final']\n        if document_status not in ['Unsigned', 'Final']:\n            raise ValueError('document_status was invalid. allowed values are %s' %\n                             allowed_document_status)\n        magic = self._magic_json(action=TouchWorksMagicConstants.ACTION_SAVE_NOTE,\n                                 patient_id=patient_id,\n                                 parameter1=note_text,\n                                 parameter2=document_type,\n                                 parameter3=document_status,\n                                 parameter4=wrapped_in_rtf)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SAVE_NOTE)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_patients(self, search_criteria,\n                        include_picture='N', organization_id=None):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_SEARCH_PATIENTS action\n        :return: JSON response\n        \"\"\"\n        include_picture = include_picture or ''\n        organization_id = organization_id or ''\n        magic = self._magic_json(action=TouchWorksMagicConstants.ACTION_SEARCH_PATIENTS,\n                                 app_name=self._app_name,\n                                 token=self._token.token,\n                                 parameter1=search_criteria,\n                                 parameter2=include_picture,\n                                 parameter3=organization_id)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SEARCH_PATIENTS)\n        return result", "response": "This method searches for patients in the TouchWorks master."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_document_type(self, ehr_username, doc_type):\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_DOCUMENT_TYPE,\n            app_name=self._app_name,\n            user_id=ehr_username,\n            token=self._token.token,\n            parameter1=doc_type\n        )\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_DOCUMENT_TYPE)\n        return result", "response": "This method gets the document type from the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking TouchWorksMagicConstants.ACTION_GET_PATIENT_INFO action :return: JSON response", "response": "def get_patient(self, ehr_username, patient_id):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_PATIENT_INFO action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PATIENT_INFO,\n            app_name=self._app_name,\n            user_id=ehr_username,\n            token=self._token.token,\n            patient_id=patient_id\n        )\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PATIENT_INFO)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_encounter(self, ehr_username, patient_id):\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_ENCOUNTER,\n            app_name=self._app_name,\n            user_id=ehr_username,\n            token=self._token.token,\n            patient_id=patient_id\n        )\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_ENCOUNTER)\n        return result", "response": "This method is used to get the ISO ISO."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch document types by name and active", "response": "def find_document_type_by_name(self, entity_name, active='Y',\n                                   match_case=True):\n        \"\"\"\n        search document types by name and active(Y/N) status\n        :param entity_name: entity name\n        :return:\n        \"\"\"\n        all_types = self.get_dictionary('Document_Type_DE')\n        if match_case:\n            filtered = filter(\n                lambda x: x['Active'] == active and x['EntryName'].find(entity_name) >= 0,\n                all_types)\n        else:\n            token = entity_name.lower()\n            filtered = filter(\n                lambda x: x['Active'] == active and x['EntryName'].lower().find(token) >= 0,\n                all_types)\n        return filtered"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninvoke TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_encounter_list_for_patient(self, patient_id):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT,\n            app_name=self._app_name,\n            token=self._token.token,\n            patient_id=patient_id)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_ENCOUNTER_LIST_FOR_PATIENT)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninvokes TouchWorksMagicConstants.ACTION_SAVE_UNSTRUCTURED_DATA action :return: JSON response", "response": "def save_unstructured_document(self, ehr_username,\n                                   patient_id,\n                                   encounter_id,\n                                   document_content):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_SAVE_UNSTRUCTURED_DATA action\n        :return: JSON response\n        \"\"\"\n        doc_xml = \"<docParams><item name='documentCommand' value='I'/>\" + \\\n                  \"<item name='documentType'  value='Chart'/>\" + \\\n                  \"<item name='authorCode' value='ResLet'/>\" + \\\n                  \"<item name='ahsEncounterID' value='@@ENCOUNTERID@@'/>\" + \\\n                  \"<item name='OrganizationID' value=''/>\" + \\\n                  \"<item name='accessionValue' value=''/>\" + \\\n                  \"<item name='appGroup' value='TouchWorks'/></docParams>\"\n        doc_xml = doc_xml.replace(\"@@ENCOUNTERID@@\", str(encounter_id))\n        print(doc_xml)\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SAVE_UNSTRUCTURED_DATA,\n            patient_id=patient_id,\n            user_id=ehr_username,\n            parameter1=doc_xml,\n            parameter2=document_content)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SAVE_UNSTRUCTURED_DATA)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_patient_location_and_status(self, patient_id,\n                                        encounter_status,\n                                        patient_location):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_SET_PATIENT_LOCATION_AND_STATUS action\n        :param encounter_status - EntryName from the Encounter_Status_DE dictionary.\n            The desired entryname can be looked up with the GetDictionary action.\n        :param patient_location - EntryName from the Site_Location_DE dictionary.\n            The desired entryname can be looked up with the GetDictionary action.\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SET_PATIENT_LOCATION_AND_STATUS,\n            patient_id=patient_id,\n            parameter1=encounter_status,\n            parameter2=patient_location)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SET_PATIENT_LOCATION_AND_STATUS)\n        return result", "response": "This method is used to set the patient location and status of a specific entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_clinical_summary(self, patient_id,\n                             section,\n                             encounter_id_identifer,\n                             verbose=''):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_CLINICAL_SUMMARY action\n        :param patient_id:\n        :param section - if one of the following values is specified, Section indicates\n            which section of clinical data to return. If no Section is specified,\n            all sections with data are returned. You can specify multiple sections\n            using a pipe-delimited list. For example, \"Vitals|Results.\"\n                List\n                ChiefComplaint\n                Vitals\n                Activities\n                Alerts\n                Problems\n                Results\n                History\n                Medications\n                Allergies\n                Immunizations\n                Orders\n        :param encounter_id_identifer - identifier for the encounter. Used in conjunction with\n            the \"ChiefComplaint\" when called in Parameter1. EncounterID can be acquired\n            with the Unity call GetEncounterList.\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_CLINICAL_SUMMARY,\n            patient_id=patient_id,\n            parameter1=section,\n            parameter2=encounter_id_identifer,\n            parameter3=verbose)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_CLINICAL_SUMMARY)\n        return result", "response": "This method returns the clinical summary of the specified clinical entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninvoke TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_patient_activity(self, patient_id, since=''):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PATIENT_ACTIVITY,\n            patient_id=patient_id,\n            parameter1=since)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PATIENT_ACTIVITY)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_patient_medhx_flag(self, patient_id,\n                               medhx_status):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :param patient_id\n        :param medhx_status - \tField in EEHR expects U, G, or D. SP defaults to Null and\n            errors out if included.\n                U=Unknown\n                G=Granted\n                D=Declined\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SET_PATIENT_MEDHX_FLAG,\n            patient_id=patient_id,\n            parameter1=medhx_status\n        )\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SET_PATIENT_MEDHX_FLAG)\n        return result", "response": "This method is used to set the patient_id of a specific MEDHX flag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_changes_patients(self, patient_id,\n                             since,\n                             clinical_data_only='Y',\n                             verbose='Y',\n                             quick_scan='Y',\n                             which_field='',\n                             what_value=''):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_CHANGED_PATIENTS,\n            patient_id=patient_id,\n            parameter1=since,\n            parameter2=clinical_data_only,\n            parameter3=verbose,\n            parameter4=quick_scan,\n            parameter5=which_field,\n            parameter6=what_value\n        )\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_CHANGED_PATIENTS)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvoke TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_patients_locations(self, patient_id):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        doc_xml = \"<docParams><item name='User' value='@@USER@@'/></docParams>\"\n        doc_xml = doc_xml.replace(\"@@USER@@\", str(patient_id))\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PATIENT_LOCATIONS,\n            parameter1=doc_xml)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PATIENT_LOCATIONS)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_patient_pharmacies(self, patient_id,\n                               patients_favorite_only='N'):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PATIENT_PHARAMCIES,\n            patient_id=patient_id,\n            parameter1=patients_favorite_only)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PATIENT_PHARAMCIES)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_id(self):\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_USER_ID)\n\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_USER_ID)\n        return result", "response": "This method gets the user id from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_provider(self, provider_id, provider_username=''):\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PROVIDER,\n            parameter1=provider_id,\n            parameter2=provider_username)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PROVIDER)\n        return result", "response": "This method is used to get information about a specific provider."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninvoking TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_provider_info(self, sought_user):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PROVIDER_INFO,\n            app_name=self._app_name,\n            token=self._token.token,\n            parameter1=sought_user)\n\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PROVIDER_INFO)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninvokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :param security_filter - This is the EntryCode of the Security_Code_DE dictionary for the providers being sought. A list of valid security codes can be obtained from GetDictionary on the Security_Code_DE dictionary. :param name_filter :param only_providers_flag :param internal_external :param ordering_authority :param real_provider :return: JSON response", "response": "def get_providers(self, security_filter,\n                      name_filter='%',\n                      only_providers_flag='Y',\n                      internal_external='I',\n                      ordering_authority='',\n                      real_provider='N'):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :param security_filter - This is the EntryCode of the Security_Code_DE dictionary\n            for the providers being sought. A list of valid security codes can be obtained from\n            GetDictionary on the Security_Code_DE dictionary.\n        :param name_filter\n        :param only_providers_flag\n        :param internal_external\n        :param ordering_authority\n        :param real_provider\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_PROVIDERS,\n            parameter1=security_filter,\n            parameter2=name_filter,\n            parameter3=only_providers_flag,\n            parameter4=internal_external,\n            parameter5=ordering_authority,\n            parameter6=real_provider)\n\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_PROVIDERS)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninvoking TouchWorksMagicConstants.ACTION_GET_TASK_LIST action :param since - If given a datetime, retrieves only tasks created (or last modified) after that date and time. Defaults to 1/1/1900. :param task_status - Optional list of pipe-delimited task status names. For example, \"Active|In Progress|Complete\". :param task_types - Optional list of pipe-delimited task type names. For example, \"Sign Note|Verify Result|MedRenewal\" :return: JSON response", "response": "def get_task_list(self, since='', task_types='', task_status=''):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_TASK_LIST action\n        :param since - If given a datetime, retrieves only tasks created (or last modified)\n            after that date and time. Defaults to 1/1/1900.\n        :param task_status - Optional list of pipe-delimited task status names.\n            For example, \"Active|In Progress|Complete\".\n        :param task_types - Optional list of pipe-delimited task type names.\n            For example, \"Sign Note|Verify Result|MedRenewal\"\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_TASK_LIST,\n            parameter1=since,\n            parameter2=task_types,\n            parameter3=task_status)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_ENCOUNTER_LIST_FOR_PATIENT)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninvoking TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :param :param message :param sent_date :param transaction_type - type To register a patient with the portal, this should be 'Register Patient Request.' Valid types are stored in iHealth_TransCode_DE. Approve Online Consultation Custom Form Submitted Decline Online Consultation Deny Patient Registration Form Requested Health Remiders Register Patient Register Patient Request RenewRx Seek Appointment Seek Online Consultation Send Clinical Document Send General Message Send Notification Message Unregister Patient :return: JSON response", "response": "def save_message_from_pat_portal(self, patient_id,\n                                     p_vendor_name,\n                                     p_message_id,\n                                     p_practice_id,\n                                     message,\n                                     sent_date,\n                                     transaction_type\n                                     ):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :param\n        :param message\n        :param sent_date\n        :param transaction_type -   type\tTo register a patient with the portal,\n            this should be 'Register Patient Request.'\n                Valid types are stored in iHealth_TransCode_DE.\n                Approve Online Consultation\n                Custom Form Submitted\n                Decline Online Consultation\n                Deny Patient Registration\n                Form Requested\n                Health Remiders\n                Register Patient\n                Register Patient Request\n                RenewRx\n                Seek Appointment\n                Seek Online Consultation\n                Send Clinical Document\n                Send General Message\n                Send Notification Message\n                Unregister Patient\n        :return: JSON response\n        \"\"\"\n        portal_info_xml = '<msg>' + \\\n                          '<ppvendor value=\"@@VENDOR@@\" />' + \\\n                          '<ppmsgid value=\"@@MESSAGEID@@\" />' + \\\n                          '<pppractice value=\"@@PRACTICE@@\" />' + \\\n                          '</msg>'\n        portal_info_xml = portal_info_xml.replace(\n            '@@VENDOR@@', p_vendor_name).replace(\n            '@@MESSAGEID@@', p_message_id).replace(\n            '@@PRACTICE@@', p_practice_id)\n\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SAVE_MSG_FROM_PAT_PORTAL,\n            patient_id=patient_id,\n            parameter1=portal_info_xml,\n            parameter2=self._ehr_username,\n            parameter3=message,\n            parameter4=sent_date,\n            parameter5=transaction_type)\n\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SAVE_MSG_FROM_PAT_PORTAL)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninvoking TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def save_task_comment(self, task_id, task_comment):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SAVE_TASK_COMMENT,\n            parameter1=task_id,\n            parameter6=task_comment)\n\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SAVE_TASK_COMMENT)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninvokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_task(self, patient_id, task_id):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_TASK,\n            patient_id=patient_id,\n            parameter1=task_id)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_TASK)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_task_status(self, task_id,\n                         task_action,\n                         comment,\n                         delegate_id=''):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_SAVE_TASK_STATUS action\n        :param task_action - Task action, such as Approve, Complete, or Deny.\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SAVE_TASK_STATUS,\n            parameter1=task_id,\n            parameter2=task_action,\n            parameter3=delegate_id,\n            parameter4=comment)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SAVE_TASK_STATUS)\n        return result", "response": "This method is used to save the status of a task."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_task_views(self, user, search_string):\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SEARCH_TASK_VIEWS,\n            parameter1=user,\n            parameter2=search_string)\n\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_SEARCH_TASK_VIEWS)\n        return result", "response": "This method searches for the task views that match the search string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninvoke TouchWorksMagicConstants.ACTION_SAVE_TASK action :param patient_id :param task_type - EntryMnemonic value from IDX_TASK_ACTION_DE. Dictionary values can be looked up using the GetDictionary action. :param target_user - TargetUser Pass in the username of the individual who will be assigned the task. Typical delegates can be found by calling GetDelegates. It is also possible to assign a task to a team by passing in 'Team'+the ID of the corresponding team from the Team_DE dictionary. The team can be looked up using the GetDictionary action. If the LoginUser is the same as the TargetUser, the task will be marked as delegated (and therefore no longer available in GetTask for that LoginUser). :param work_object_id - The ID of the item to link to the task, such as the medication or note ID. If not needed, 0 can be passed instead. :param comments - A comment to set for the task. :return: JSON response", "response": "def save_task(self, patient_id,\n                  task_type,\n                  target_user,\n                  work_object_id,\n                  comments,\n                  subject):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_SAVE_TASK action\n        :param patient_id\n        :param task_type - EntryMnemonic value from IDX_TASK_ACTION_DE. Dictionary\n            values can be looked up using the GetDictionary action.\n        :param target_user - TargetUser\tPass in the username of the individual who\n            will be assigned the task. Typical delegates can be found by calling GetDelegates.\n                It is also possible to assign a task to a team by passing in 'Team'+the ID\n                of the corresponding team from the Team_DE dictionary.\n                The team can be looked up using the GetDictionary action.\n                If the LoginUser is the same as the TargetUser, the task will be marked as\n                delegated (and therefore no longer available in GetTask for that LoginUser).\n        :param work_object_id - The ID of the item to link to the task,\n                such as the medication or note ID. If not needed, 0 can be passed instead.\n        :param comments - A comment to set for the task.\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_SAVE_TASK,\n            patient_id=patient_id,\n            parameter1=task_type,\n            parameter2=target_user,\n            parameter3=work_object_id,\n            parameter4=comments,\n            parameter5=subject)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_ENCOUNTER_LIST_FOR_PATIENT)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvoke TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_task_comments(self, patient_id, task_id):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_TASK_COMMENTS,\n            patient_id=patient_id,\n            parameter1=task_id)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_TASK_COMMENTS)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_delegates(self, patient_id):\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_DELEGATES,\n            app_name=self._app_name,\n            token=self._token.token,\n            patient_id=patient_id)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_DELEGATES)\n        return result", "response": "This method returns a JSON response of the Delegates API call."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvoke TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action :return: JSON response", "response": "def get_task_list_by_view(self, patient_id, task_view_id, org_id=''):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_ENCOUNTER_LIST_FOR_PATIENT action\n        :return: JSON response\n        \"\"\"\n        magic = self._magic_json(\n            action=TouchWorksMagicConstants.ACTION_GET_TASKLIST_BY_VIEW,\n            patient_id=patient_id,\n            parameter1=task_view_id,\n            parameter2=org_id)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_TASKLISTBY_VIEW)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_schedule(self, ehr_username, start_date,\n                     changed_since, include_pix, other_user='All',\n                     end_date='',\n                     appointment_types=None, status_filter='All'):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_SCHEDULE action\n        :return: JSON response\n        \"\"\"\n        if not start_date:\n            raise ValueError('start_date can not be null')\n        if end_date:\n            start_date = '%s|%s' % (start_date, end_date)\n        if not changed_since:\n            changed_since = ''\n        magic = self._magic_json(action=TouchWorksMagicConstants.ACTION_GET_SCHEDULE,\n                                 app_name=self._app_name,\n                                 user_id=ehr_username, token=self._token.token,\n                                 parameter1=start_date,\n                                 parameter2=changed_since,\n                                 parameter3=include_pix,\n                                 parameter4=other_user,\n                                 parameter5=appointment_types,\n                                 parameter6=status_filter)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_SCHEDULE)\n        return result", "response": "This method returns a JSON object that represents a single entry in the schedule of a user."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_documents(self, ehr_username, patient_id, start_date=None,\n                      end_date=None, document_id=None, doc_type=None,\n                      newest_document='N'):\n        \"\"\"\n        invokes TouchWorksMagicConstants.ACTION_GET_DOCUMENTS action\n        :return: JSON response\n        \"\"\"\n\n        if not start_date:\n            start_date = ''\n        if not end_date:\n            end_date = ''\n        if not doc_type:\n            doc_type = ''\n        magic = self._magic_json(action=TouchWorksMagicConstants.ACTION_GET_DOCUMENTS,\n                                 user_id=ehr_username, token=self._token.token,\n                                 patient_id=patient_id,\n                                 app_name=self._app_name,\n                                 parameter1=start_date,\n                                 parameter2=end_date,\n                                 parameter3=document_id,\n                                 parameter4=doc_type,\n                                 parameter5=newest_document)\n        response = self._http_request(TouchWorksEndPoints.MAGIC_JSON, data=magic)\n        result = self._get_results_or_raise_if_magic_invalid(\n            magic,\n            response,\n            TouchWorksMagicConstants.RESULT_GET_DOCUMENTS)\n        return result", "response": "This method is used to get a list of all the documents in a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _magic_json(self, action='', user_id='', app_name='', patient_id='',\n                    token='', parameter1='', parameter2='',\n                    parameter3='', parameter4='', parameter5='',\n                    parameter6='', data=''):\n        \"\"\"\n        utility method to create a magic json object needed to invoke TouchWorks APIs\n        :return: magic json\n        \"\"\"\n        if not token:\n            token = self._token.token\n        if not app_name:\n            app_name = self._app_name\n        if not user_id:\n            if self._ehr_username:\n                user_id = self._ehr_username\n\n        return {\n            'Action': action,\n            'AppUserID': user_id,\n            'Appname': app_name,\n            'PatientID': patient_id,\n            'Token': token,\n            'Parameter1': parameter1,\n            'Parameter2': parameter2,\n            'Parameter3': parameter3,\n            'Parameter4': parameter4,\n            'Parameter5': parameter5,\n            'Parameter6': parameter6,\n            'Data': data\n        }", "response": "This method creates a magic json object needed to invoke TouchWorks APIs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery items based on system call number or name.", "response": "def query_item(self, key, abis):\n        \"\"\"Query items based on system call number or name.\"\"\"\n        try:\n            key = int(key)\n            field = 'number'\n        except ValueError:\n            try:\n                key = int(key, 16)\n                field = 'number'\n            except ValueError:\n                field = 'name'\n        arg = and_(getattr(Item, field) == key,\n                   or_(Item.abi == abi for abi in abis))\n        return self.session.query(Item).filter(arg).all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds data to the database.", "response": "def add_data(self, filenames):\n        \"\"\"Add data.\"\"\"\n        def _parse_table(table):\n            def _parse_line(line):\n                return line.split('\\t')\n            lines = (_parse_line(one) for one in table.splitlines()\n                     if re.match(r'^\\d', one))\n            return (remove_false(one) for one in lines)\n\n        def _parse_decl(decl):\n            index = len('SYSCALL_DEFINE')\n            argc = decl[index]\n            rest = decl[index + 1:][1:-1].split(',')\n            name = rest[0]\n            # args = [one.strip() for one in rest[1:]]\n            args = ','.join(rest[1:])\n            return name, argc, args\n\n        def _parse_line(line):\n            index = line.find(':')\n            if index == -1:\n                raise RuntimeError('This is unexpected: %s', line)\n            filename = line[:index]\n            decl = line[index + 1:]\n            return filename, _parse_decl(decl)\n\n        def _split_into_lines(grep_output):\n            lines = grep_output.replace('\\n\\n', '\\n')\n            lines = lines.replace('\\n\\t', '').replace('\\t', ' ')\n            return lines.strip().splitlines()\n\n        for one in filenames:\n            if one.name.endswith('.tbl'):\n                for item in _parse_table(one.read()):\n                    args = list(item)\n                    if len(args) != 5:\n                        args += [''] * (5 - len(args))\n                    self.session.add(\n                        Item(name=args[2], abi=args[1],\n                             number=args[0], entry=args[3],\n                             compat=args[4]))\n            else:\n                for line in _split_into_lines(one.read()):\n                    filename, rest = (_parse_line(line))\n                    self.session.add(\n                        Decl(name=rest[0], filename=filename,\n                             argc=rest[1], args=rest[2]))\n        self.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run():\n    args = parse_args()\n\n    appname = sys.argv[0]\n    hostname = platform.node()\n\n    codetools.setup_logging(args.debug)\n\n    password = ''\n\n    if args.token_path is None and args.delete_role is True:\n        cred_path = os.path.expanduser('~/.sq_github_token_delete')\n    elif args.token_path is None and args.delete_role is False:\n        cred_path = os.path.expanduser('~/.sq_github_token')\n    else:\n        cred_path = os.path.expandvars(os.path.expanduser(args.token_path))\n\n    if not os.path.isfile(cred_path):\n        print(\"\"\"\n        Type in your password to get an auth token from github\n        It will be stored in {0}\n        and used in subsequent occasions.\n        \"\"\".format(cred_path))\n\n        while not password:\n            password = getpass('Password for {0}: '.format(args.user))\n\n        note = textwrap.dedent(\"\"\"\\\n            {app} via bored^H^H^H^H^H terrified opossums[1]\n            on {host}\n            by {user} {creds}\n            [1] https://youtu.be/ZtLrn2zPTxQ?t=1m10s\n            \"\"\").format(\n            app=appname,\n            host=hostname,\n            user=args.user,\n            creds=cred_path\n        )\n        note_url = 'https://www.youtube.com/watch?v=cFvijBpzD_Y'\n\n        if args.delete_role:\n            scopes = ['repo', 'user', 'delete_repo', 'admin:org']\n        else:\n            scopes = ['repo', 'user']\n\n        global g\n        g = github.Github(args.user, password)\n        u = g.get_user()\n\n        try:\n            auth = u.create_authorization(\n                scopes=scopes,\n                note=note,\n                note_url=note_url,\n            )\n        except github.TwoFactorException:\n            auth = u.create_authorization(\n                scopes=scopes,\n                note=note,\n                note_url=note_url,\n                # not a callback\n                onetime_password=codetools.github_2fa_callback()\n            )\n        g = github.Github(auth.token)\n\n        with open(cred_path, 'w') as fdo:\n            fdo.write(auth.token + '\\n')\n            fdo.write(str(auth.id))\n\n        print('Token written to {0}'.format(cred_path))\n\n    else:\n        print(\"You already have an auth file: {0} \".format(cred_path))\n        print(\"Delete it if you want a new one and run again\")\n        print(\"Remember to also remove the corresponding token on Github\")", "response": "Log in and store credentials"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start(self, on_exit_callback=None):\n        # TODO: Support params for services by mapping {servicename: {class,\n        # params}}?\n        for service in self.services.keys():\n            self.services[service] = self.services[service]()\n\n        self.server.start(on_exit_callback)", "response": "Start the Engel application by initializing all registered services and starting an Autobahn IOLoop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering an event that you want to monitor.", "response": "def register(self, event, callback, selector=None):\n        \"\"\"\n        Resister an event that you want to monitor.\n\n        :param event: Name of the event to monitor\n        :param callback: Callback function for when the event is received (Params: event, interface).\n        :param selector: `(Optional)` CSS selector for the element(s) you want to monitor.\n        \"\"\"\n        self.processor.register(event, callback, selector)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister an event handler for the view.", "response": "def on(self, event, callback, selector=None):\n        \"\"\"\n        Wrapper around :meth:`~.application.Application.register`.\n        If :meth:`~.application.View.on` is called, for instance, during :meth:`~.application.View.build`,\n        the event handlers will be enqueued and registered when the view is loaded. Similarly,\n        if :meth:`~.application.View.on` is called once the view is loaded (for example, in a button callback),\n        the event handler will be registered immediately.\n\n        :param event: Name of the event to monitor\n        :param callback: Callback function for when the event is received (Params: event, interface).\n        :param selector: `(Optional)` CSS selector for the element(s) you want to monitor\n        \"\"\"\n        cbk = asyncio.coroutine(callback)\n        self._event_cache.append(\n            {'event': event, 'callback': cbk, 'selector': selector})\n        if self.is_loaded:\n            self.context.register(event, cbk, selector)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunload the view from the event cache.", "response": "def unload(self):\n        \"\"\"\n        Overridable method called when a view is unloaded (either on view change or on application shutdown).\n        Handles by default the unregistering of all event handlers previously registered by\n        the view.\n        \"\"\"\n        self.is_loaded = False\n        for evt in self._event_cache:\n            self.context.unregister(\n                evt['event'], evt['callback'], evt['selector'])\n        self._event_cache = {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_api(cls, api):\n        ux = TodoUX(api)\n        from .pseudorpc import PseudoRpc\n\n        rpc = PseudoRpc(api)\n\n        return cls({ViaAPI: api, ViaUX: ux, ViaRPC: rpc})", "response": "Create an application description for the todo app based on the api"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists of namespace - > name pairs as 2 - tuples", "response": "def aliases(self):\n        \"\"\"List of (namespace, name) pairs, as 2-tuples\"\"\"\n        return {alias.namespace: alias.name for alias in self._proto.alias}.items()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef consequence_level(self):\n        if self._proto.HasField('consequenceLevel'):\n            return mdb_pb2.SignificanceInfo.SignificanceLevelType.Name(self._proto.consequenceLevel)\n        return None", "response": "Return the consequence level of the message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data_source(self):\n        if self._proto.HasField('dataSource'):\n            return mdb_pb2.DataSourceType.Name(self._proto.dataSource)\n        return None", "response": "Specifies the data source of this parameter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts documents in database", "response": "def get_documents_count(self):\n        \"\"\"Counts documents in database\n\n        :return: Number of documents in db\n        \"\"\"\n        db_collections = [\n            self.database[c] for c in self.get_collection_names()\n        ]  # list of all collections in database\n        return sum([c.count() for c in db_collections])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all documents in collection", "response": "def get_documents_in_collection(self, collection_name, with_id=True):\n        \"\"\"Gets all documents in collection\n\n        :param collection_name: Name of collection\n        :param with_id: True iff each document should also come with its id\n        :return: List of documents in collection in self.db\n        \"\"\"\n        documents_iterator = self.database[collection_name].find()  # anything\n        documents = [\n            d for d in documents_iterator\n        ]  # list of all documents in collection in database\n\n        if not with_id:\n            for doc in documents:\n                doc.pop(\"_id\")  # remove id key\n\n        return documents"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all documents in database", "response": "def get_documents_in_database(self, with_id=True):\n        \"\"\"Gets all documents in database\n\n        :param with_id: True iff each document should also come with its id\n        :return: List of documents in collection in database\n        \"\"\"\n        documents = []\n        for coll in self.get_collection_names():\n            documents += self.get_documents_in_collection(\n                coll,\n                with_id=with_id\n            )\n\n        return documents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_tags(repos, tags, ignore_existing=False, fail_fast=False):\n\n    debug(\"looking for {n} tag(s):\".format(n=len(tags)))\n    [debug(\"  {t}\".format(t=t)) for t in tags]\n    debug(\"in {n} repo(s):\".format(n=len(repos)))\n    [debug(\"  {r}\".format(r=r.full_name)) for r in repos]\n\n    # present/missing tags by repo name\n    present_tags = {}\n    absent_tags = {}\n\n    problems = []\n    for r in repos:\n        has_tags = find_tags_in_repo(r, tags)\n        if has_tags:\n            if not ignore_existing:\n                yikes = GitTagExistsError(\n                    \"tag(s) {tag} already exists in repos {r}\".format(\n                        tag=list(has_tags.keys()),\n                        r=r.full_name\n                    ))\n                if fail_fast:\n                    raise yikes\n                problems.append(yikes)\n                error(yikes)\n\n            present_tags[r.full_name] = {\n                'repo': r,\n                'tags': list(has_tags.values()),\n            }\n\n        missing_tags = [x for x in tags if x not in has_tags]\n        if missing_tags:\n            absent_tags[r.full_name] = {\n                'repo': r,\n                'need_tags': missing_tags,\n            }\n\n    debug(textwrap.dedent(\"\"\"\\\n        found:\n          {n_with:>4} repos with tag(s)\n          {n_none:>4} repos with no tag(s)\n          {errors:>4} repos with error(s)\\\n        \"\"\").format(\n        n_with=len(present_tags),\n        n_none=len(absent_tags),\n        errors=len(problems),\n    ))\n\n    return present_tags, absent_tags, problems", "response": "check if tags already exist in repos"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_refs(repo, refs, dry_run=False):\n\n    assert isinstance(repo, github.Repository.Repository), type(repo)\n\n    debug(\"removing {n} refs from {repo}\".format(\n        n=len(refs),\n        repo=repo.full_name)\n    )\n\n    for r in refs:\n        debug(\"  deleting {ref}\".format(ref=r.ref))\n        if dry_run:\n            debug('    (noop)')\n            continue\n\n        r.delete()", "response": "Delete the given refs from the tag\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a todo list ux by name", "response": "def get_by(self, name):\n        \"\"\"get a todo list ux by name\n\n        :rtype: TodoListUX\n        \"\"\"\n        item = self.app.get_by(name)\n        return TodoListUX(ux=self, controlled_list=item)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_item(self, name):\n        item = self.app.create_item(name)\n\n        return TodoListUX(ux=self, controlled_list=item)", "response": "create a new named todo list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a todo list element by name", "response": "def get_by(self, name):\n        \"\"\"\n        find a todo list element by name\n        \"\"\"\n        item = self.controlled_list.get_by(name)\n        if item:\n            return TodoElementUX(parent=self, controlled_element=item)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new todo list item", "response": "def create_item(self, name):\n        \"\"\"\n        create a new todo list item\n        \"\"\"\n        elem = self.controlled_list.create_item(name)\n        if elem:\n            return TodoElementUX(parent=self, controlled_element=elem)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def add(self, setname, ip, timeout):\n        # We have to double-quote the '{' '}' at both ends for `format` to work.\n        if timeout > 0:\n            to_ban = \"{{ {0} timeout {1}s }}\".format(ip, timeout)\n        else:\n            to_ban = \"{{\u00a0{0} }}\".format(ip)\n\n        args = ['add', 'element', self.table_family, self.table_name, setname, to_ban]\n\n        return await self.start(__class__.CMD, *args)", "response": "Add an entry to the set with the given IP address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chose_blacklist(self, ip):\n        blacklist = 'ellis_blacklist{0}'\n\n        try:\n            address = ipaddress.ip_address(ip)\n        except ipaddress.AddressValueError:\n            raise\n        else:\n            if address.version is 6:\n                # We don't ban private IPv6:\n                if address.is_private:\n                    msg = \"We don't ban private addresses ({0} given).\" \\\n                          .format(address)\n                    raise ipaddress.AddressValueError(msg)\n                else:\n                    # Do we have an embedded IPv4 ?\n                    if address.ipv4_mapped is not None:\n                        address = address.ipv4_mapped\n                    elif address.sixtofour is not None:\n                        address = address.sixtofour\n\n        blacklist = blacklist.format(address.version)\n\n        return (address, blacklist)", "response": "Given an IP address figure out the set we have to use."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef under_attack(col, queens):\n        left = right = col\n        for _, column in reversed(queens):\n            left, right = left - 1, right + 1\n            if column in (left, col, right):\n                return True\n        return False", "response": "Checks if a queen is under attack"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve(self, table_size):\n        if table_size == 0:\n            return [[]]\n\n        smaller_solutions = self.solve(table_size - 1)\n        solutions = []\n        for solution in smaller_solutions:\n            for column in range(1, self.board_size + 1):\n                # try adding a new queen to row = n, column = column\n                if not self.under_attack(column, solution):\n                    solutions.append(solution + [(table_size, column)])\n        return solutions", "response": "Solves problem\n\n        :param table_size: Size of table\n        :return: List of possible solutions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_bundles_by_type(self, type):\n        bundles = {}\n        bundle_definitions = self.config.get(type)\n        if bundle_definitions is None:\n            return bundles\n        # bundle name: common\n        for bundle_name, paths in bundle_definitions.items():\n            bundle_files = []\n            # path: static/js/vendor/*.js\n            for path in paths:\n                # pattern: /tmp/static/js/vendor/*.js\n                pattern = abspath = os.path.join(self.basedir, path)\n                # assetdir: /tmp/static/js/vendor\n                # assetdir contents:\n                #  - /tmp/static/js/vendor/t1.js\n                #  - /tmp/static/js/vendor/t2.js\n                #  - /tmp/static/js/vendor/index.html\n                assetdir = os.path.dirname(abspath)\n                # expanded_fnames after filtering using the pattern:\n                #  - /tmp/static/js/vendor/t1.js\n                #  - /tmp/static/js/vendor/t2.js\n                fnames = [os.path.join(assetdir, fname)\n                          for fname in os.listdir(assetdir)]\n                expanded_fnames = fnmatch.filter(fnames, pattern)\n                bundle_files.extend(sorted(expanded_fnames))\n            bundles[bundle_name] = bundle_files\n\n        return bundles", "response": "Get a dictionary of bundles for requested type."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a system call to rest_auth.pl, a Perl script modified from https://github.com/kjolley/BIGSdb/tree/develop/scripts/test And downloads the most up-to-date rMLST profile and alleles", "response": "def getrmlsthelper(referencefilepath, update, start):\n    \"\"\"\n    Makes a system call to rest_auth.pl, a Perl script modified from\n    https://github.com/kjolley/BIGSdb/tree/develop/scripts/test\n    And downloads the most up-to-date rMLST profile and alleles\n    \"\"\"\n    from subprocess import call\n    analysistype = 'rMLST'\n    # Folders are named based on the download date e.g 2016-04-26\n    # Find all folders (with the trailing / in the glob search) and remove the trailing /\n    lastfolder = sorted(glob('{}{}/*/'.format(referencefilepath, analysistype)))[-1].rstrip('/')\n    # lastfolder = os.path.join(referencefilepath, analysistype)\n    delta, foldersize, d1 = schemedate(lastfolder)\n    # Extract the path of the current script from the full path + file name\n    homepath = os.path.split(os.path.abspath(__file__))[0]\n    # Set the path/name of the folder to contain the new alleles and profile\n    newfolder = os.path.join(referencefilepath, analysistype, str(d1))\n    # System call\n    rmlstupdatecall = 'cd {} && perl {}/rest_auth.pl -a {}/secret.txt'.format(newfolder, homepath, homepath)\n    if update:\n        if delta.days > 7 or foldersize < 100:\n            printtime(\"Last update of rMLST profile and alleles was {} days ago. Updating\".format(str(delta.days)),\n                      start)\n            # Create the path\n            make_path(newfolder)\n            # Copy over the access token to be used in the authentication\n            shutil.copyfile('{}/access_token'.format(homepath), '{}/access_token'.format(newfolder))\n            # Run rest_auth.pl\n            call(rmlstupdatecall, shell=True)\n            # Get the new alleles into a list, and create the combinedAlleles file\n            alleles = glob('{}/*.tfa'.format(newfolder))\n            combinealleles(start, newfolder, alleles)\n        # If the profile and alleles are up-to-date, set :newfolder to :lastfolder\n        else:\n            newfolder = lastfolder\n        # Ensure that the profile/alleles updated successfully\n        # Calculate the size of the folder by adding the sizes of all the files within the folder together\n        newfoldersize = sum(os.path.getsize('{}/{}'.format(newfolder, f)) for f in os.listdir(newfolder)\n                            if os.path.isfile('{}/{}'.format(newfolder, f)))\n        # If the profile/allele failed, remove the folder, and use the most recent update\n        if newfoldersize < 100:\n            shutil.rmtree(newfolder)\n            newfolder = sorted(glob('{}{}/*/'.format(referencefilepath, analysistype)))[-1].rstrip('/')\n    # Don't update the profile/alleles if not requested\n    else:\n        newfolder = lastfolder\n    # Return the system call and the folder containing the profile and alleles\n    return rmlstupdatecall, newfolder"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getmlsthelper(referencefilepath, start, organism, update):\n    from accessoryFunctions.accessoryFunctions import GenObject\n    # Initialise a set to for the organism(s) for which new alleles and profiles are desired\n    organismset = set()\n    # Allow for Shigella to use the Escherichia MLST profile/alleles\n    organism = organism if organism != 'Shigella' else 'Escherichia'\n    # As there are multiple profiles for certain organisms, this dictionary has the schemes I use as values\n    organismdictionary = {'Escherichia': 'Escherichia coli#1',\n                          'Shigella': 'Escherichia coli#1',\n                          'Vibrio': 'Vibrio parahaemolyticus',\n                          'Campylobacter': 'Campylobacter jejuni',\n                          'Listeria': 'Listeria monocytogenes',\n                          'Bacillus': 'Bacillus cereus',\n                          'Klebsiella': 'Klebsiella pneumoniae'}\n    # Allow for a genus not in the dictionary being specified\n    try:\n        organismset.add(organismdictionary[organism])\n    except KeyError:\n        # Add the organism to the set\n        organismset.add(organism)\n    for scheme in organismset:\n        organismpath = os.path.join(referencefilepath, 'MLST', organism)\n        # Find all folders (with the trailing / in the glob search) and remove the trailing /\n        try:\n            lastfolder = sorted(glob('{}/*/'.format(organismpath)))[-1].rstrip('/')\n        except IndexError:\n            lastfolder = []\n        # Run the method to determine the most recent folder, and how recently it was updated\n        delta, foldersize, d1 = schemedate(lastfolder)\n        # Set the path/name of the folder to contain the new alleles and profile\n        newfolder = '{}/{}'.format(organismpath, d1)\n        if update:\n            if delta.days > 7 or foldersize < 100:\n                printtime('Downloading {} MLST scheme from pubmlst.org'.format(organism), start)\n                # Create the object to store the argument attributes to feed to getmlst\n                getmlstargs = GenObject()\n                getmlstargs.species = scheme\n                getmlstargs.repository_url = 'http://pubmlst.org/data/dbases.xml'\n                getmlstargs.force_scheme_name = False\n                getmlstargs.path = newfolder\n                # Create the path to store the downloaded\n                make_path(getmlstargs.path)\n                getmlst.main(getmlstargs)\n                # Even if there is an issue contacting the database, files are created, however, they are populated\n                # with XML strings indicating that the download failed\n                # Read the first character in the file\n                try:\n                    profilestart = open(glob('{}/*.txt'.format(newfolder))[0]).readline()\n                except IndexError:\n                    profilestart = []\n                # If it is a <, then the download failed\n                if not profilestart or profilestart[0] == '<':\n                    # Delete the folder, and use the previous definitions instead\n                    shutil.rmtree(newfolder)\n                    newfolder = lastfolder\n            # If the profile and alleles are up-to-date, set :newfolder to :lastfolder\n            else:\n                newfolder = lastfolder\n        # If update isn't specified, don't update\n        else:\n            newfolder = lastfolder\n            # Ensure that the profile/alleles updated successfully\n            # Calculate the size of the folder by adding the sizes of all the files within the folder together\n        try:\n            newfoldersize = sum(os.path.getsize('{}/{}'.format(newfolder, f)) for f in os.listdir(newfolder)\n                                if os.path.isfile('{}/{}'.format(newfolder, f)))\n        except (OSError, TypeError):\n            newfoldersize = 100\n        # If the profile/allele failed, remove the folder, and use the most recent update\n        if newfoldersize < 100:\n            shutil.rmtree(newfolder)\n            try:\n                newfolder = sorted(glob('{}/*/'.format(organismpath)))[-1].rstrip('/')\n            except IndexError:\n                newfolder = organismpath\n        # Return the name/path of the allele-containing folder\n        return newfolder", "response": "Prepares to run the getmlst. py script provided in SRST2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a dictionary of unique sequence - related information from the profile scheme of the samples in the metadata file.", "response": "def profiler(self):\n        \"\"\"Creates a dictionary from the profile scheme(s)\"\"\"\n        # Initialise variables\n        profiledata = defaultdict(make_dict)\n        profileset = set()\n        # supplementalset = ''\n        genedict = {}\n        # Find all the unique profiles to use with a set\n        for sample in self.metadata:\n            if sample[self.analysistype].profile != 'NA':\n                profileset.add(sample[self.analysistype].profile[0])\n                # if self.analysistype == 'rmlst':\n                #     supplementalset = sample[self.analysistype].supplementalprofile\n        # Extract the profiles for each set\n        for sequenceprofile in profileset:\n            # Clear the list of genes\n            genelist = []\n            for sample in self.metadata:\n                if sequenceprofile == sample[self.analysistype].profile[0]:\n                    # genelist = [os.path.split(x)[1].split('.')[0] for x in sample[self.analysistype].alleles]\n                    genelist = sample[self.analysistype].allelenames\n            try:\n                # Open the sequence profile file as a dictionary\n                profile = DictReader(open(sequenceprofile), dialect='excel-tab')\n            # Revert to standard comma separated values\n            except KeyError:\n                # Open the sequence profile file as a dictionary\n                profile = DictReader(open(sequenceprofile))\n            # Iterate through the rows\n            for row in profile:\n                # Iterate through the genes\n                for gene in genelist:\n                    # Add the sequence profile, and type, the gene name and the allele number to the dictionary\n                    try:\n                        profiledata[sequenceprofile][row['ST']][gene] = row[gene]\n                    except KeyError:\n                        try:\n                            profiledata[sequenceprofile][row['rST']][gene] = row[gene]\n                        except KeyError:\n                            raise\n\n            # # Load the supplemental profile definitions\n            # if self.analysistype == 'rmlst':\n            #     supplementalprofile = DictReader(open(supplementalset), dialect='excel-tab')\n            #     # Do the same with the supplemental profile\n            #     for row in supplementalprofile:\n            #         # Iterate through the genes\n            #         for gene in genelist:\n            #             # Add the sequence profile, and type, the gene name and the allele number to the dictionary\n            #             profiledata[sequenceprofile][row['rST']][gene] = row[gene]\n            # Add the gene list to a dictionary\n            genedict[sequenceprofile] = sorted(genelist)\n            # Add the profile data, and gene list to each sample\n            for sample in self.metadata:\n                if sample.general.bestassemblyfile != 'NA':\n                    if sequenceprofile == sample[self.analysistype].profile[0]:\n                        # Populate the metadata with the profile data\n                        sample[self.analysistype].profiledata = profiledata[sample[self.analysistype].profile[0]]\n                        # Add the allele directory to a list of directories used in this analysis\n                        self.allelefolders.add(sample[self.analysistype].alleledir)\n                        dotter()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes blast database files from targets as necessary", "response": "def makeblastdb(self):\n        \"\"\"Makes blast database files from targets as necessary\"\"\"\n        while True:  # while daemon\n            fastapath = self.dqueue.get()  # grabs fastapath from dqueue\n            # remove the path and the file extension for easier future globbing\n            db = fastapath.split('.')[0]\n            nhr = '{}.nhr'.format(db)  # add nhr for searching\n            if not os.path.isfile(str(nhr)):  # if check for already existing dbs\n                # Create the databases\n                # TODO use MakeBLASTdb class\n                subprocess.call(shlex.split('makeblastdb -in {} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {}'\n                                            .format(fastapath, db)), stdout=self.fnull, stderr=self.fnull)\n            dotter()\n            self.dqueue.task_done()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the BLAST analyses and save the results in the metadata file", "response": "def runblast(self, assembly, allele, sample):\n        \"\"\"\n        Run the BLAST analyses\n        :param assembly: assembly path/file\n        :param allele: combined allele file\n        :param sample: sample object\n        :return:\n        \"\"\"\n        genome = os.path.split(assembly)[1].split('.')[0]\n        # Run the BioPython BLASTn module with the genome as query, fasta(target gene) as db.\n        # Do not re-perform the BLAST search each time\n        make_path(sample[self.analysistype].reportdir)\n        try:\n            report = glob('{}{}*rawresults*'.format(sample[self.analysistype].reportdir, genome))[0]\n            size = os.path.getsize(report)\n            if size == 0:\n                os.remove(report)\n                report = '{}{}_rawresults_{:}.csv'.format(sample[self.analysistype].reportdir, genome,\n                                                          time.strftime(\"%Y.%m.%d.%H.%M.%S\"))\n        except IndexError:\n            report = '{}{}_rawresults_{:}.csv'.format(sample[self.analysistype].reportdir, genome,\n                                                      time.strftime(\"%Y.%m.%d.%H.%M.%S\"))\n        db = allele.split('.')[0]\n        # BLAST command line call. Note the mildly restrictive evalue, and the high number of alignments.\n        # Due to the fact that all the targets are combined into one database, this is to ensure that all potential\n        # alignments are reported. Also note the custom outfmt: the doubled quotes are necessary to get it work\n        blastn = NcbiblastnCommandline(query=assembly, db=db, evalue='1E-20', num_alignments=1000000,\n                                       num_threads=12,\n                                       outfmt=\"'6 qseqid sseqid positive mismatch gaps \"\n                                              \"evalue bitscore slen length qstart qend qseq sstart send'\",\n                                       out=report)\n        # Save the blast command in the metadata\n        sample[self.analysistype].blastcommand = str(blastn)\n        sample[self.analysistype].blastreport = report\n        if not os.path.isfile(report):\n            # Run BLAST\n            blastn()\n        # Run the blast parsing module\n        self.blastparser(report, sample)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef alleleupdater(self, sample, gene, targetallele):\n        from Bio.Seq import Seq\n        from Bio.Alphabet import IUPAC\n        from Bio.SeqRecord import SeqRecord\n        # As there is some discrepancy with the capitalisation of terms, make sure it is consistent\n        analysistype = 'rMLST' if self.analysistype.lower() == 'rmlst' else 'MLST'\n\n        # Set the directory containing the profile and alleles\n        alleledir = self.referencefilepath + analysistype + '/local' if self.pipeline else self.referencefilepath\n        allelefile = glob('{}/{}.fa'.format(alleledir, gene))[0]\n        # Create a string to store the last local allele number\n        nextallele = str()\n        # Check the local allele file to see if this allele has already been recorded\n        for record in SeqIO.parse(allelefile, 'fasta'):\n            # String match of the sequence to the sequence of the alleles - if they match then set the name of the\n            # matching allele to this local allele\n            if sample[self.analysistype].queryseq[gene] == str(record.seq):\n\n                allelenumber = record.id.split('_')[-1]\n                # Return allelenumber, percent identity (100.0%), hsp.score - 100?\n                return '', '', ''\n            # Record the allele number + 1; following the last record, this number will represent the next allele number\n            nextallele = int(record.id.split('_')[-1]) + 1\n        # Translate the nucleotide sequence to determine if there are any internal stop codons\n        dnaseq = Seq(sample[self.analysistype].queryseq[gene], IUPAC.unambiguous_dna)\n        protseq = str(dnaseq.translate())\n        # There should be only one stop codon per sequence. In sequences with more than one stop codon, this additional\n        # stop codon must be internal\n        internalstop = True if protseq.count('*') > 1 else False\n        if not internalstop:\n\n            # If the alignment length is less than the subject length, then the query sequence is truncated.\n            # This may not be an issue, but it must still be flagged\n            truncated = True if sample[self.analysistype].alignmentlength[gene] < sample[self.analysistype].subjectlength[gene] else False\n            if not truncated:\n                print('full length', sample.name, gene, nextallele, targetallele, alleledir, allelefile, protseq)\n                # The header will be >BACT00001_1000000\n                definitionline = '{}_{} {} NT from allele {}### no internal stop codons'\\\n                    .format(gene, nextallele, sample[self.analysistype].mismatches[gene], sample[self.analysistype].closealleles[gene])\n                # Create a sequence record using BioPython\n                fasta = SeqRecord(dnaseq,\n                                  # Without this, the header will be improperly formatted\n                                  description='',\n                                  # Use >:definitionline as the header\n                                  id=definitionline)\n                print(definitionline)\n                # with open(sample[self.analysistype].supplementalalleles) as supplemental:\n                #     # Use the SeqIO module to properly format the new sequence record\n                #     SeqIO.write(fasta, supplemental, 'fasta')\n                # with open(allelefile) as localalleles:\n                #     # Use the SeqIO module to properly format the new sequence record\n                #     SeqIO.write(fasta, localalleles, 'fasta')\n            else:\n                print('truncated', sample.name, gene, nextallele, targetallele, alleledir, allelefile, protseq)\n\n        else:\n            print('internal stop', sample.name, gene, nextallele, targetallele, alleledir, allelefile, protseq)\n\n        return '', '', ''", "response": "Update the file of alleles if the new allele passes length and identity checks\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef alleleupdater1(self, sample, gene, targetallele):\n        from io import StringIO\n        from Bio.Blast import NCBIXML\n        from Bio.SeqRecord import SeqRecord\n        from Bio.Seq import Seq\n        from Bio.Alphabet import generic_dna\n        # Find the name of the allele file based on whether the gene name is found in the file name\n        genefile = [x for x in sample[self.analysistype].alleles if gene in x][0]\n        # Remove the extension of the gene file for use in makeblastdb and blastn\n        genefilenoext = genefile.split(\".\")[0]\n        # Create the blast databases every time the method is called\n        subprocess.call(shlex.split('makeblastdb -in {} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {}'\n                                    .format(genefile, genefilenoext)), stdout=self.fnull, stderr=self.fnull)\n        # Re-perform BLAST analyses, but using an XML output that stores the alignment\n        blastn = NcbiblastnCommandline(query=sample.general.bestassemblyfile, db=genefilenoext, evalue=0.1, outfmt=5)\n        # Note that there is no output file specified -  the search results are currently stored in stdout\n        stdout, stderr = blastn()\n        if stdout.find('Hsp') != -1:\n            # Map the blast record to memory\n            blast_handle = StringIO(stdout)\n            # Open record from memory-mapped file\n            records = NCBIXML.parse(blast_handle)\n            # Iterate through the records in the blast results\n            for record in records:  # This process is just to retrieve HSPs from xml files\n                for alignment in record.alignments:\n                    for hsp in alignment.hsps:\n                        # Extract the allele name from the blast result\n                        allele = str(alignment.accession.split(\"_\")[-1]) if \"_\" in alignment.accession else \\\n                            str(alignment.accession.split(\"-\")[-1])\n                        # The point of this blast is to extract the sequence of the allele\n                        # If the allele name matches the target allele (the closest match e.g. BACT000063_20 -> 20)\n                        if allele == str(targetallele):\n                            # As there is some discrepancy with the capitalisation of terms, make sure it is consistent\n                            analysistype = 'rMLST' if self.analysistype.lower() == 'rmlst' else 'MLST'\n                            # Set the directory containing the profile and alleles\n                            alleledir = self.referencefilepath + analysistype if self.pipeline else \\\n                                self.referencefilepath\n                            # The name of the supplemental allele file (without and with the .fa extension)\n                            allelefilenoext = '{}/OLC_{}_alleles'.format(alleledir, analysistype) if self.pipeline \\\n                                else '{}/rMLST_combined'.format(alleledir)\n                            allelefile = allelefilenoext + '.fa' if self.pipeline else allelefilenoext + '.fasta'\n                            # Create the file if it doesn't exist\n                            open(allelefile, 'a').close()\n                            # Create a list of all the blast database files in the folder\n                            dbfiles = glob('{}.n*'.format(allelefilenoext))\n                            # Remove the database files\n                            map(lambda y: os.remove(y), dbfiles)\n                            # Create the necessary blast database files\n                            subprocess.call(\n                                shlex.split('makeblastdb -in {} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {}'\n                                            .format(allelefile, allelefilenoext)), stdout=self.fnull, stderr=self.fnull)\n\n                            # Perform BLAST analysis using the supplemental allele file as the database\n                            nestedblastn = NcbiblastnCommandline(db=allelefilenoext, evalue=0.1, outfmt=5)\n                            # There is no output file specified; the search results are currently stored in stdout\n                            # Additionally, the sequence from the previous BLAST query is used as stdin in this BLAST\n                            import Bio.Application\n                            try:\n                                nestedstdout, nestedstderr = nestedblastn(stdin=hsp.query)\n                            except Bio.Application.ApplicationError:\n                                nestedstdout = ''\n                                pass\n                            # Search stdout for matches - if the term Hsp appears (the .find function will NOT\n                            # return -1), a match has been found, and stdout is written to file\n                            if nestedstdout.find('Hsp') != -1:\n                                nested_blast_handle = StringIO(nestedstdout)\n                                # Open record from memory-mapped file\n                                nestedrecords = NCBIXML.parse(nested_blast_handle)\n                                # Initialise variables\n                                previousallele = ''\n                                bitscore = ''\n                                # Iterate through the records\n                                for nestedrecord in nestedrecords:\n                                    for nestedalignment in nestedrecord.alignments:\n                                        for nestedhsp in nestedalignment.hsps:\n                                            # Calculate the percent identity\n                                            percentidentity = float(\"%.2f\" % float(float(nestedhsp.identities) /\n                                                                                   float(nestedalignment.length) * 100))\n                                            # Only set the previous allele and the bitscore if the % identity is 100%\n                                            if percentidentity == 100:\n                                                # Get the allele number\n                                                previousallele = nestedalignment.accession.split(\"_\")[-1]\n                                                bitscore = nestedhsp.score\n\n                                # If this allele has already been found in the supplemental file, return these data\n                                if previousallele:\n                                    return gene, previousallele, 100.0, bitscore\n                                # Otherwise, get the allele sequence into the supplemental file\n                                else:\n                                    # Initialise variables\n                                    allelelist = []\n                                    allelenumber = 1000000\n                                    # Open the allele file to append\n                                    with open(allelefile, 'ab+') as supplemental:\n                                        for line in supplemental:\n                                            # As there are multiple genes (e.g. BACT000001, BACT000063, etc.) check to\n                                            # see if the gene of interest is in the line\n                                            if gene in line:\n                                                # Append the header to the list\n                                                allelelist.append(line)\n                                        # Find the last allele number from the header\n                                        try:\n                                            allelenumber = int(allelelist[-1].split(\"_\")[1]) + 1\n                                        # If there are no alleles for the gene of interest, then pass\n                                        except IndexError:\n                                            pass\n                                        # Puts the HSP in the correct order -  hits to the negative strand will be\n                                        # reversed compared to what we're looking for\n                                        allelesequence = Seq(hsp.query, generic_dna)\n                                        if hsp.sbjct_start < hsp.sbjct_end:\n                                            end = hsp.sbjct_end\n                                        else:\n                                            end = hsp.sbjct_start\n                                            allelesequence = allelesequence.reverse_complement()\n                                        # Screen out hits that are shorter than the targets\n                                        # Keeping this format though this statement could be re-written more efficiently\n                                        if end < alignment.length:\n                                            pass\n                                        # The header will be >BACT00001_1000000\n                                        definitionline = '{}_{}'.format(gene, allelenumber)\n                                        # Create a sequence record using BioPython\n                                        fasta = SeqRecord(allelesequence,\n                                                          # Without this, the header will be improperly formatted\n                                                          description='',\n                                                          # Use >:definitionline as the header\n                                                          id=definitionline)\n                                        # Use the SeqIO module to properly format the new sequence record\n                                        SeqIO.write(fasta, supplemental, \"fasta\")\n                                        # Return the necessary values\n                                        return gene, allelenumber, 100.0, hsp.score\n                            # If there are hits in the supplemental allele file\n                            else:\n                                # Initialise variables\n                                allelelist = []\n                                allelenumber = 1000000\n                                # Open the allele file to find the last allele associated with the gene of interest\n                                with open(allelefile, 'a+') as supplemental:\n                                    for line in supplemental:\n                                        if gene in line:\n                                            allelelist.append(line)\n                                    try:\n                                        allelenumber = int(allelelist[-1].split(\"_\")[1]) + 1\n                                    except IndexError:\n                                        pass\n                                    # Puts the HSP in the correct order -  hits to the negative strand will be\n                                    # reversed compared to what we're looking for\n                                    allelesequence = Seq(hsp.query, generic_dna)\n                                    if hsp.sbjct_start < hsp.sbjct_end:\n                                        end = hsp.sbjct_end\n                                    else:\n                                        end = hsp.sbjct_start\n                                        allelesequence = allelesequence.reverse_complement()\n                                    # Screen out hits that are shorter than the targets\n                                    # Keeping this format, though this if statement could be re-written more efficiently\n                                    if end < alignment.length:\n                                        pass\n                                    definitionline = '{}_{}'.format(gene, allelenumber)\n                                    # Create a sequence record using BioPython\n                                    fasta = SeqRecord(allelesequence,\n                                                      # If this is not added, the header will not be formatted properly\n                                                      description='',\n                                                      # Use >:definitionline as the header\n                                                      id=definitionline)\n                                    # Use the SeqIO module to properly format the new sequence record\n                                    SeqIO.write(fasta, supplemental, \"fasta\")\n                                    # Return the appropriate information\n                                    return gene, allelenumber, 100.0, hsp.score", "response": "Update file of alleles if the new allele passes length and identity checks"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the sequence type of each strain based on comparisons to sequence type profiles", "response": "def sequencetyper(self):\n        \"\"\"Determines the sequence type of each strain based on comparisons to sequence type profiles\"\"\"\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                if type(sample[self.analysistype].allelenames) == list:\n                    # Initialise variables\n                    header = 0\n                    # Iterate through the genomes\n                    # for sample in self.metadata:\n                    genome = sample.name\n                    # Initialise self.bestmatch[genome] with an int that will eventually be replaced by the # of matches\n                    self.bestmatch[genome] = defaultdict(int)\n                    if sample[self.analysistype].profile != 'NA':\n                        # Create the profiledata variable to avoid writing self.profiledata[self.analysistype]\n                        profiledata = sample[self.analysistype].profiledata\n                        # For each gene in plusdict[genome]\n                        for gene in sample[self.analysistype].allelenames:\n                            # Clear the appropriate count and lists\n                            multiallele = []\n                            multipercent = []\n                            # Go through the alleles in plusdict\n                            for allele in self.plusdict[genome][gene]:\n                                percentid = list(self.plusdict[genome][gene][allele].keys())[0]\n                                # \"N\" alleles screw up the allele splitter function\n                                if allele != \"N\":\n                                    # Use the alleleSplitter function to get the allele number\n                                    # allelenumber, alleleprenumber = allelesplitter(allele)\n                                    # Append as appropriate - alleleNumber is treated as an integer for proper sorting\n                                    multiallele.append(int(allele))\n                                    multipercent.append(percentid)\n                                # If the allele is \"N\"\n                                else:\n                                    # Append \"N\" and a percent identity of 0\n                                    multiallele.append(\"N\")\n                                    multipercent.append(0)\n                                if not multiallele:\n                                    multiallele.append(\"N\")\n                                    multipercent.append(0)\n                            # if self.analysistype == 'rmlst':\n                            #     # For whatever reason, the rMLST profile scheme treat multiple allele hits as 'N's.\n                            #     multiallele = multiallele if len(multiallele) == 1 else ['N']\n                            #     if multipercent:\n                            #         multipercent = multipercent if len(multiallele) == 1 else [0, 0]\n                            #     else:\n                            #         multipercent = [0]\n                            # Populate self.bestdict with genome, gene, alleles joined with a space (this was made like\n                            # this because allele is a list generated by the .iteritems() above\n                            self.bestdict[genome][gene][\" \".join(str(allele)\n                                                                 for allele in sorted(multiallele))] = multipercent[0]\n                            # Find the profile with the most alleles in common with the query genome\n                            for sequencetype in profiledata:\n                                # The number of genes in the analysis\n                                header = len(profiledata[sequencetype])\n                                # refallele is the allele number of the sequence type\n                                refallele = profiledata[sequencetype][gene]\n                                # If there are multiple allele matches for a gene in the reference profile e.g. 10 692\n                                if len(refallele.split(\" \")) > 1:\n                                    # Map the split (on a space) alleles as integers - if they are treated as integers,\n                                    # the alleles will sort properly\n                                    intrefallele = map(int, refallele.split(\" \"))\n                                    # Create a string of the joined, sorted alleles\n                                    sortedrefallele = \" \".join(str(allele) for allele in sorted(intrefallele))\n                                else:\n                                    # Use the reference allele as the sortedRefAllele\n                                    sortedrefallele = refallele\n                                for allele, percentid in self.bestdict[genome][gene].items():\n                                    # If the allele in the query genome matches the allele in the reference profile, add\n                                    # the result to the bestmatch dictionary. Genes with multiple alleles were sorted\n                                    # the same, strings with multiple alleles will match: 10 692 will never be 692 10\n                                    if allele == sortedrefallele and float(percentid) == 100.00:\n                                        # Increment the number of matches to each profile\n                                        self.bestmatch[genome][sequencetype] += 1\n                                    # Special handling of BACT000060 and BACT000065 genes. When the reference profile\n                                    # has an allele of 'N', and the query allele doesn't, set the allele to 'N', and\n                                    # count it as a match\n                                    elif gene == 'BACT000060' or gene == 'BACT000065':\n                                        if sortedrefallele == 'N' and allele != 'N':\n                                            # Increment the number of matches to each profile\n                                            self.bestmatch[genome][sequencetype] += 1\n                                    elif allele == sortedrefallele and sortedrefallele == 'N':\n                                        # Increment the number of matches to each profile\n                                        self.bestmatch[genome][sequencetype] += 1\n                        # Get the best number of matches\n                        # From: https://stackoverflow.com/questions/613183/sort-a-python-dictionary-by-value\n                        try:\n                            sortedmatches = sorted(self.bestmatch[genome].items(), key=operator.itemgetter(1),\n                                                   reverse=True)[0][1]\n                        # If there are no matches, set :sortedmatches to zero\n                        except IndexError:\n                            sortedmatches = 0\n                        # Otherwise, the query profile matches the reference profile\n                        if int(sortedmatches) == header:\n                            # Iterate through best match\n                            for sequencetype, matches in self.bestmatch[genome].items():\n                                if matches == sortedmatches:\n                                    for gene in profiledata[sequencetype]:\n                                        # Populate resultProfile with the genome, best match to profile, # of matches\n                                        # to the profile, gene, query allele(s), reference allele(s), and % identity\n                                        self.resultprofile[genome][sequencetype][sortedmatches][gene][\n                                            list(self.bestdict[genome][gene]\n                                                .keys())[0]] = str(list(self.bestdict[genome][gene].values())[0])\n                                    sample[self.analysistype].sequencetype = sequencetype\n                                    sample[self.analysistype].matchestosequencetype = matches\n                        # If there are fewer matches than the total number of genes in the typing scheme\n                        elif 0 < int(sortedmatches) < header:\n                            mismatches = []\n                            # Iterate through the sequence types and the number of matches in bestDict for each genome\n                            for sequencetype, matches in self.bestmatch[genome].items():\n                                # If the number of matches for a profile matches the best number of matches\n                                if matches == sortedmatches:\n                                    # Iterate through the gene in the analysis\n                                    for gene in profiledata[sequencetype]:\n                                        # Get the reference allele as above\n                                        refallele = profiledata[sequencetype][gene]\n                                        # As above get the reference allele split and ordered as necessary\n                                        if len(refallele.split(\" \")) > 1:\n                                            intrefallele = map(int, refallele.split(\" \"))\n                                            sortedrefallele = \" \".join(str(allele) for allele in sorted(intrefallele))\n                                        else:\n                                            sortedrefallele = refallele\n                                        # Populate self.mlstseqtype with the genome, best match to profile, # of matches\n                                        # to the profile, gene, query allele(s), reference allele(s), and % identity\n                                        if self.updateprofile:\n                                            self.mlstseqtype[genome][sequencetype][sortedmatches][gene][\n                                                str(list(self.bestdict[genome][gene]\n                                                    .keys())[0])][sortedrefallele] = str(list(self.bestdict[genome][gene]\n                                                                                        .values())[0])\n                                        else:\n                                            self.resultprofile[genome][sequencetype][sortedmatches][gene][\n                                                list(self.bestdict[genome][gene].keys())[0]] \\\n                                                = str(list(self.bestdict[genome][gene])[0])\n                                                #= str(list(self.bestdict[genome][gene].values())[0])\n                                            if sortedrefallele != list(self.bestdict[sample.name][gene].keys())[0]:\n                                                mismatches.append(\n                                                    ({gene: ('{} ({})'.format(list(self.bestdict[sample.name][gene]\n                                                                              .keys())[0], sortedrefallele))}))\n                                        if not self.updateprofile or self.analysistype == 'mlst':\n                                            self.resultprofile[genome][sequencetype][sortedmatches][gene][\n                                                list(self.bestdict[genome][gene]\n                                                    .keys())[0]] = str(list(self.bestdict[genome][gene].values())[0])\n                                            sample[self.analysistype].mismatchestosequencetype = mismatches\n                                            sample[self.analysistype].sequencetype = sequencetype\n                                            sample[self.analysistype].matchestosequencetype = matches\n                            # Add the new profile to the profile file (if the option is enabled)\n                            if self.updateprofile and self.analysistype != 'mlst':\n                                self.reprofiler(int(header), genome, sample)\n                        elif sortedmatches == 0:\n                            for gene in sample[self.analysistype].allelenames:\n                                # Populate the results profile with negative values for sequence type and sorted matches\n                                self.resultprofile[genome]['NA'][sortedmatches][gene]['NA'] = 0\n                            # Add the new profile to the profile file (if the option is enabled)\n                            if self.updateprofile:\n                                self.reprofiler(int(header), genome, sample)\n                            sample[self.analysistype].sequencetype = 'NA'\n                            sample[self.analysistype].matchestosequencetype = 'NA'\n                            sample[self.analysistype].mismatchestosequencetype = 'NA'\n                        else:\n                            sample[self.analysistype].matchestosequencetype = 'NA'\n                            sample[self.analysistype].mismatchestosequencetype = 'NA'\n                            sample[self.analysistype].sequencetype = 'NA'\n                        dotter()\n                else:\n                    sample[self.analysistype].matchestosequencetype = 'NA'\n                    sample[self.analysistype].mismatchestosequencetype = 'NA'\n                    sample[self.analysistype].sequencetype = 'NA'\n\n            else:\n                sample[self.analysistype].matchestosequencetype = 'NA'\n                sample[self.analysistype].mismatchestosequencetype = 'NA'\n                sample[self.analysistype].sequencetype = 'NA'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reprofiler(self, header, genome, sample):\n        # Iterate through mlstseqtype - it contains genomes with partial matches to current reference profiles\n        # Reset :newprofile\n        newprofile = \"\"\n        # Find the last profile entry in the dictionary of profiles\n        # Opens uses the command line tool 'tail' to look at the last line of the file (-1). This last line\n        # is split on tabs, and only the first entry (the sequence type number) is captured\n        if sample[self.analysistype].supplementalprofile != 'NA':\n            if os.path.isfile(sample[self.analysistype].supplementalprofile):\n                try:\n                    lastentry = int(\n                        subprocess.check_output(['tail', '-1', sample[self.analysistype].supplementalprofile])\n                        .split(\"\\t\")[0]) + 1\n                except ValueError:\n                    lastentry = 1000000\n            else:\n                open(sample[self.analysistype].supplementalprofile, 'w').close()\n                lastentry = 1000000\n            # As there can be multiple profiles in MLSTSeqType, this loop only needs to be performed once.\n            seqcount = 0\n            # Go through the sequence types\n            try:\n                sequencetype = list(self.mlstseqtype[genome].keys())[0]\n            except IndexError:\n                sequencetype = ''\n                seqcount = 1\n            # Only do this once\n            if seqcount == 0:\n                # Set the :newprofile string to start with the new profile name (e.g. 1000000_CFIA)\n                newprofile = str(lastentry)\n                # The number of matches to the reference profile\n                nummatches = list(self.mlstseqtype[genome][sequencetype].keys())[0]\n                # The genes in geneList - should be in the correct order\n                for gene in sorted(sample[self.analysistype].allelenames):\n                    # The allele for each gene in the query genome\n                    allele = list(self.mlstseqtype[genome][sequencetype][nummatches][gene].keys())[0]\n                    # Append the allele to newprofile\n                    newprofile += '\\t{}'.format(allele)\n                    # Add the MLST results for the query genome as well as the new profile data\n                    # to resultProfile\n                    self.resultprofile[genome]['{}(new)'.format(str(lastentry))][header][gene][allele] = \\\n                        list(self.mlstseqtype[genome][sequencetype][nummatches][gene][allele].values())[0]\n                seqcount += 1\n                sample[self.analysistype].mismatchestosequencetype = 'NA'\n                sample[self.analysistype].matchestosequencetype = header\n            # Only perform the next loop if :newprofile exists\n            if newprofile:\n                # Open the profile file to append\n                with open(sample[self.analysistype].supplementalprofile, 'a') as appendfile:\n                    # Append the new profile to the end of the profile file\n                    appendfile.write('{}\\n'.format(newprofile))\n                # Re-run profiler with the updated files\n                self.profiler()\n        else:\n            sample[self.analysistype].mismatchestosequencetype = 'NA'\n            sample[self.analysistype].matchestosequencetype = 'NA'", "response": "Creates and appends new profiles as required\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reporter(self):\n        # Initialise variables\n        combinedrow = ''\n        reportdirset = set()\n        # Populate a set of all the report directories to use. A standard analysis will only have a single report\n        # directory, while pipeline analyses will have as many report directories as there are assembled samples\n        for sample in self.metadata:\n            # Ignore samples that lack a populated reportdir attribute\n            if sample[self.analysistype].reportdir != 'NA':\n                make_path(sample[self.analysistype].reportdir)\n                # Add to the set - I probably could have used a counter here, but I decided against it\n                reportdirset.add(sample[self.analysistype].reportdir)\n        # Create a report for each sample from :self.resultprofile\n        for sample in self.metadata:\n            if sample[self.analysistype].reportdir != 'NA':\n                if type(sample[self.analysistype].allelenames) == list:\n                    # Populate the header with the appropriate data, including all the genes in the list of targets\n                    row = 'Strain,Genus,SequenceType,Matches,{},\\n' \\\n                        .format(','.join(sorted(sample[self.analysistype].allelenames)))\n                    # Set the sequence counter to 0. This will be used when a sample has multiple best sequence types.\n                    # The name of the sample will not be written on subsequent rows in order to make the report clearer\n                    seqcount = 0\n                    # Iterate through the best sequence types for the sample (only occurs if update profile is disabled)\n                    for seqtype in self.resultprofile[sample.name]:\n                        \"\"\"\n                        {\n                            \"OLF15230-1_2015-SEQ-0783\": {\n                                \"1000004_CFIA\": {\n                                    \"7\": {\n                                        \"dnaE\": {\n                                            \"47\": \"100.00\"\n                                        },\n                                        \"dtdS\": {\n                                            \"19\": \"100.00\"\n                                        },\n                                        \"gyrB\": {\n                                            \"359\": \"100.00\"\n                                        },\n                                        \"pntA\": {\n                                            \"50\": \"100.00\"\n                                        },\n                                        \"pyrC\": {\n                                            \"143\": \"100.00\"\n                                        },\n                                        \"recA\": {\n                                            \"31\": \"100.00\"\n                                        },\n                                        \"tnaA\": {\n                                            \"26\": \"100.00\"\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                        \"\"\"\n                        # Becomes\n                        \"\"\"\n                        Strain,SequenceType,Matches,dnaE,gyrB,recA,dtdS,pntA,pyrC,tnaA\n                        OLF15230-1_2015-SEQ-0783,1000004_CFIA,7,26 (100.00%),359 (100.00%),31 (100.00%),50 (100.00%),\n                            19 (100.00%),47 (100.00%),143 (100.00%)\n                        \"\"\"\n                        sample[self.analysistype].sequencetype = seqtype\n                        # The number of matches to the profile\n                        matches = list(self.resultprofile[sample.name][seqtype].keys())[0]\n                        # If this is the first of one or more sequence types, include the sample name\n                        if seqcount == 0:\n                            row += '{},{},{},{},'.format(sample.name, sample.general.referencegenus, seqtype, matches)\n                        # Otherwise, skip the sample name\n                        else:\n                            row += ',,{},{},'.format(seqtype, matches)\n                        # Iterate through all the genes present in the analyses for the sample\n                        for gene in sorted(sample[self.analysistype].allelenames):\n                            # refallele = self.profiledata[self.analysistype][seqtype][gene]\n                            refallele = sample[self.analysistype].profiledata[seqtype][gene]\n                            # Set the allele and percent id from the dictionary's keys and values, respectively\n                            allele = list(self.resultprofile[sample.name][seqtype][matches][gene].keys())[0]\n                            percentid = list(self.resultprofile[sample.name][seqtype][matches][gene].values())[0]\n                            if refallele and refallele != allele:\n                                if 0 < float(percentid) < 100:\n                                    row += '{} ({:.2f}%),'.format(allele, float(percentid))\n                                else:\n                                    row += '{} ({}),'.format(allele, refallele)\n                            else:\n                                # Add the allele and % id to the row (only add the percent identity if it is not 100%)\n                                if 0 < float(percentid) < 100:\n                                    row += '{} ({:.2f}%),'.format(allele, float(percentid))\n                                else:\n                                    row += '{},'.format(allele)\n                            self.referenceprofile[sample.name][gene] = allele\n                        # Add a newline\n                        row += '\\n'\n                        # Increment the number of sequence types observed for the sample\n                        seqcount += 1\n                    combinedrow += row\n                    # If the length of the # of report directories is greater than 1 (script is being run as part of\n                    # the assembly pipeline) make a report for each sample\n                    if self.pipeline:\n                        # Open the report\n                        with open('{}{}_{}.csv'.format(sample[self.analysistype].reportdir, sample.name,\n                                                       self.analysistype), 'w') as report:\n                            # Write the row to the report\n                            report.write(row)\n                dotter()\n            # Create the report folder\n            make_path(self.reportpath)\n            # Create the report containing all the data from all samples\n            if self.pipeline:\n                with open('{}{}.csv'.format(self.reportpath, self.analysistype), 'w') \\\n                        as combinedreport:\n                    # Write the results to this report\n                    combinedreport.write(combinedrow)\n            else:\n                with open('{}{}_{:}.csv'.format(self.reportpath, self.analysistype, time.strftime(\"%Y.%m.%d.%H.%M.%S\")),\n                          'w') as combinedreport:\n                    # Write the results to this report\n                    combinedreport.write(combinedrow)\n            # Remove the raw results csv\n            [os.remove(rawresults) for rawresults in glob('{}*rawresults*'.format(self.reportpath))]", "response": "Parse the results into a report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dumper(self):\n        with open('{}{}_referenceprofile.json'.format(self.reportpath, self.analysistype, ), 'w') as referenceprofile:\n            referenceprofile.write(json.dumps(self.referenceprofile, sort_keys=True, indent=4, separators=(',', ': ')))", "response": "Write self. referenceprofile to file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef referencegenomefinder(self):\n        # Initialise dictionaries\n        referencematch = defaultdict(make_dict)\n        referencehits = defaultdict(make_dict)\n        # Set the name of the reference profile file\n        referencegenomeprofile = '{}rMLST_referenceprofile.json'.format(self.referenceprofilepath)\n        # Open the reference profile and load the profile into memory\n        with open(referencegenomeprofile) as referencefile:\n            referencetypes = json.load(referencefile)\n        # Iterate through the samples\n        for sample in self.metadata:\n            if sample[self.analysistype].reportdir != 'NA':\n                # Iterate through the reference genomes in the profile\n                for genome in referencetypes:\n                    # Initialise the number of identical alleles between the assembly of interest and the\n                    # reference genome to 0\n                    referencehits[sample.name][genome] = 0\n                    # Iterate through all the genes in the analysis\n                    for gene in self.bestdict[sample.name]:\n                        # If the alleles match between the assembly of interest and the reference genome, increment\n                        # the number of matches by 1\n                        if list(self.bestdict[sample.name][gene].keys())[0] == referencetypes[genome][gene]:\n                            referencematch[sample.name][genome][gene] = 1\n                            referencehits[sample.name][genome] += 1\n                        else:\n                            referencematch[sample.name][genome][gene] = 0\n        #\n        for sample in self.metadata:\n            if sample[self.analysistype].reportdir != 'NA':\n                # Get the best number of matches\n                # From: https://stackoverflow.com/questions/613183/sort-a-python-dictionary-by-value\n                try:\n                    matches = sorted(referencehits[sample.name].items(),\n                                     key=operator.itemgetter(1), reverse=True)\n                    most_matches = matches[0][1]\n                    i = 0\n                    match_list = list()\n                    while matches[i][1] == most_matches:\n                        match_list.append(matches[i])\n                        i += 1\n                    sorted_list = sorted(match_list)\n                    sortedmatches = sorted_list[0]\n                except IndexError:\n                    sortedmatches = (0, 0)\n                # If there are fewer matches than the total number of genes in the typing scheme\n                if 0 < int(sortedmatches[1]) < len(sample[self.analysistype].allelenames):\n                    mismatches = []\n                    # Iterate through the gene in the analysis\n                    for gene, allele in referencetypes[sortedmatches[0]].items():\n                        # Populate :self.referencegenome with the genome name, best reference match, number of matches,\n                        # gene, query allele(s), and percent identity\n                        percentidentity = '{:.2f}'.format(list(self.bestdict[sample.name][gene].values())[0])\n                        self.referencegenome[sample.name][sortedmatches[0]][sortedmatches[1]][gene][list(self.bestdict[\n                            sample.name][gene].keys())[0]] = percentidentity\n                        if list(self.bestdict[sample.name][gene].keys())[0] != allele:\n                            sample[self.analysistype].referencegenome = sortedmatches[0]\n                            sample.general.referencegenus = sortedmatches[0].split('_')[0]\n                            sample[self.analysistype].referencegenomepath = '{}{}.fa' \\\n                                .format(self.referenceprofilepath, sortedmatches[0])\n                            sample[self.analysistype].matchestoreferencegenome = sortedmatches[1]\n                            mismatches.append(({gene: ('{} ({})'.format(list(self.bestdict[sample.name][gene]\n                                                                        .keys())[0], allele))}))\n                        # sample[self.analysistype].mismatchestoreferencegenome = sorted(mismatches)\n                        sample[self.analysistype].mismatchestoreferencegenome = mismatches\n                elif sortedmatches == 0:\n                    for gene in sample[self.analysistype].allelenames:\n                        # Populate the profile of results with 'negative' values for sequence type and sorted matches\n                        self.referencegenome[sample.name][sortedmatches[0]][0][gene]['NA'] = 0\n                        sample[self.analysistype].referencegenome = 'NA'\n                        sample.general.referencegenus = 'NA'\n                        sample[self.analysistype].referencegenomepath = 'NA'\n                        sample[self.analysistype].matchestoreferencegenome = 0\n                        sample[self.analysistype].mismatchestoreferencegenome = [0]\n                # Otherwise, the query profile matches the reference profile\n                else:\n                    for gene in referencetypes[sortedmatches[0]]:\n                        # Populate self.referencegenome as above\n                        self.referencegenome[sample.name][sortedmatches[0]][sortedmatches[1]][gene][list(self.bestdict[\n                            sample.name][gene].keys())[0]] = '{:.2f}'.format(list(self.bestdict[\n                                                                                sample.name][gene].values())[0])\n                        sample[self.analysistype].referencegenome = sortedmatches[0]\n                        sample[self.analysistype].referencegenomepath = '{}{}.fa' \\\n                            .format(self.referenceprofilepath, sortedmatches[0])\n                        sample.general.referencegenus = sortedmatches[0].split('_')[0]\n                        sample[self.analysistype].matchestoreferencegenome = sortedmatches[1]\n                        sample[self.analysistype].mismatchestoreferencegenome = [0]\n        # Print the results to file\n        make_path(self.reportpath)\n        with open('{}referencegenomes.csv'.format(self.reportpath), 'w') as referencegenomereport:\n            row = 'Strain,referencegenome,numberofmatches\\n'\n            for sample in self.metadata:\n                if sample[self.analysistype].reportdir != 'NA':\n                    row += '{},{},{}\\n'.format(sample.name, sample[self.analysistype].referencegenome,\n                                               sample[self.analysistype].matchestoreferencegenome)\n            referencegenomereport.write(row)\n            dotter()", "response": "Find the closest reference genome to the profile of interest\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strainer(self):\n        # Initialise a variable to store whether the analyses need to be performed\n        analyse = list()\n        for sample in self.runmetadata.samples:\n            if sample.general.bestassemblyfile != 'NA':\n                try:\n                    # Try to open the final report from the analyses. If it exists, then the analyses don't need to be\n                    # performed again.\n                    if os.path.isfile('{}{}_{}.csv'.format(sample[self.analysistype].reportdir, sample.name,\n                                                           self.analysistype)):\n                        if self.analysistype == 'rmlst':\n                            # Run the allele updater method\n                            updatecall, allelefolder = getrmlsthelper(self.referencefilepath, self.updatedatabases,\n                                                                      self.start)\n                        else:\n                            # referencefilepath, start, organism, update\n                            allelefolder = getmlsthelper(self.referencefilepath, self.start,\n                                                                     sample.general.referencegenus, self.updatedatabases)\n                        # Alleles have a .tfa extension\n                        self.alleles = glob('{}/*.tfa'.format(allelefolder))\n                        sample[self.analysistype].alleles = self.alleles\n                        sample[self.analysistype].allelenames = [os.path.split(x)[1].split('.')[0] for x in\n                                                                 self.alleles]\n                        # The analyses have already been successfully completed\n                        analyse.append(False)\n                    # Otherwise run the analyses\n                    else:\n                        self.populator(sample)\n                        analyse.append(True)\n                # If the attribute doesn't exist, then the analyses haven't been performed yet.\n                except (KeyError, AttributeError):\n                    self.populator(sample)\n                    analyse.append(True)\n            else:\n                self.populator(sample)\n                analyse.append(False)\n        # Only run the analyses if they have not completed successfully before\n        # if any(analyse):\n        # Run the MLST analyses\n        MLST(self)", "response": "This method is called by the MLST class when the sample is finished."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef populator(self, sample):\n        from accessoryFunctions.accessoryFunctions import GenObject\n        if sample.general.bestassemblyfile != 'NA':\n            profile = ''\n            setattr(sample, self.analysistype, GenObject())\n            sample[self.analysistype].analyse = True\n            if self.analysistype.lower() == 'rmlst':\n                # Run the allele updater method\n                updatecall, allelefolder = getrmlsthelper(self.referencefilepath, self.updatedatabases, self.start)\n                # Alleles have a .tfa extension\n                self.alleles = glob('{}/*.tfa'.format(allelefolder))\n                # Get the profile file into a list\n                profile = glob('{}/*.txt'.format(allelefolder))\n                self.supplementalprofile = '{}rMLST/OLC_rMLST_profiles.txt'.format(self.referencefilepath)\n                self.supplementalalleles = '{}rMLST/OLC_rMLST_alleles.fa'.format(self.referencefilepath)\n                self.combinedalleles = glob('{}/*.fasta'.format(allelefolder))\n                # Set the metadata file appropriately\n                sample[self.analysistype].alleledir = allelefolder\n                sample[self.analysistype].updatecall = updatecall\n            else:\n                # Use the getmlsthelper module to download databases as required\n                schemefolder = getmlsthelper(self.referencefilepath, self.start, sample.general.referencegenus,\n                                             self.updatedatabases)\n                # If there is no database folder, do not perform the MLST analyses on this sample\n                if not schemefolder:\n                    sample[self.analysistype].analyse = False\n                    # Set the metadata file appropriately\n                    sample[self.analysistype].alleles = 'NA'\n                    sample[self.analysistype].allelenames = 'NA'\n                    sample[self.analysistype].profile = 'NA'\n                    sample[self.analysistype].analysistype = 'NA'\n                    sample[self.analysistype].reportdir = 'NA'\n                    sample[self.analysistype].combinedalleles = 'NA'\n                    sample[self.analysistype].supplementalprofile = 'NA'\n                    sample[self.analysistype].supplementalalleles = 'NA'\n                    sample[self.analysistype].alleledir = 'NA'\n                else:\n                    self.alleles = glob('{}/*.tfa'.format(schemefolder))\n                    profile = glob('{}/*.txt'.format(schemefolder))\n                    self.combinedalleles = glob('{}/*.fasta'.format(schemefolder))\n                    sample[self.analysistype].alleledir = schemefolder\n            # Only perform analyses on samples with target databases\n            if sample[self.analysistype].analyse:\n                sample[self.analysistype].alleles = self.alleles\n                sample[self.analysistype].allelenames = [os.path.split(x)[1].split('.')[0] for x in self.alleles]\n                sample[self.analysistype].profile = profile if profile else 'NA'\n                sample[self.analysistype].analysistype = self.analysistype\n                sample[self.analysistype].reportdir = '{}/{}/'.format(sample.general.outputdirectory,\n                                                                      self.analysistype)\n                sample[self.analysistype].combinedalleles = self.combinedalleles\n                sample[self.analysistype].supplementalprofile = self.supplementalprofile \\\n                    if self.supplementalprofile else 'NA'\n                sample[self.analysistype].supplementalalleles = self.supplementalalleles \\\n                    if self.supplementalalleles else 'NA'\n                print(sample[self.analysistype].datastore)\n\n        else:\n            setattr(sample, self.analysistype, GenObject())\n            # Set the metadata file appropriately\n            sample[self.analysistype].alleledir = 'NA'\n            sample[self.analysistype].alleles = 'NA'\n            sample[self.analysistype].allelenames = 'NA'\n            sample[self.analysistype].profile = 'NA'\n            sample[self.analysistype].analysistype = 'NA'\n            sample[self.analysistype].reportdir = 'NA'\n            sample[self.analysistype].combinedalleles = 'NA'\n            sample[self.analysistype].supplementalprofile = 'NA'\n            sample[self.analysistype].supplementalalleles = 'NA'", "response": "Populates the object with the necessary attributes and the metadata files."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses raw time and returns a Time object", "response": "def parse_hh_mm_ss(self):\n        \"\"\"Parses raw time\n\n        :return: Time parsed\n        \"\"\"\n        split_count = self.raw.count(\":\")\n\n        if split_count == 2:  # hh:mm:ss\n            return datetime.strptime(str(self.raw).strip(), \"%H:%M:%S\").time()\n        elif split_count == 1:  # mm:ss\n            return datetime.strptime(str(self.raw).strip(), \"%M:%S\").time()\n\n        return datetime.strptime(str(self.raw).strip(), \"%S\").time()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting seconds from raw time", "response": "def get_seconds(self):\n        \"\"\"Gets seconds from raw time\n\n        :return: Seconds in time\n        \"\"\"\n        parsed = self.parse_hh_mm_ss()  # get times\n        total_seconds = parsed.second\n        total_seconds += parsed.minute * 60.0\n        total_seconds += parsed.hour * 60.0 * 60.0\n        return total_seconds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing raw time into a Time object", "response": "def parse_hh_mm(self):\n        \"\"\"Parses raw time\n\n        :return: Time parsed\n        \"\"\"\n        split_count = self.raw.count(\":\")\n        if split_count == 1:  # hh:mm\n            return datetime.strptime(self.raw, \"%H:%M\").time()\n\n        return datetime.strptime(self.raw, \"%M\").time()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the email content in file", "response": "def get_email_content(file_path):\n    \"\"\"Email content in file\n\n    :param file_path: Path to file with email text\n    :return: Email text (html formatted)\n    \"\"\"\n    with open(file_path, \"r\") as in_file:\n        text = str(in_file.read())\n        return text.replace(\"\\n\\n\", \"<br>\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot function for the given function.", "response": "def plot_type(self, func, mins, maxs, precision, kind):\n        \"\"\"Plots function\n\n        :param func: function to plot\n        :param mins: minimum of values (x, y ...)\n        :param maxs: maximum of values (x, y ...)\n        :param precision: precision to plot\n        :param kind: kind of plot, \"slice\", \"countour\"\n        \"\"\"\n\n        min_x, min_y, min_z = mins[0], mins[1], mins[2]\n        max_x, max_y, max_z = maxs[0], maxs[1], maxs[2]\n\n        def set_labels(graph, label_x, label_y, label_z):\n            \"\"\"Sets given labels to axes of graph\n\n            :param graph: plot\n            :param label_x: new label on x axis\n            :param label_y: new label on y axis\n            :param label_z: new label on z axis\n            \"\"\"\n            graph.set_xlabel(label_x)\n            graph.set_ylabel(label_y)\n            graph.set_zlabel(label_z)\n\n        def set_limits(graph):\n            \"\"\"Set chart limits to axes of graph\n\n            :param graph: plot\n            \"\"\"\n            graph.set_xlim(min_x, max_x)\n            graph.set_ylim(min_y, max_y)\n            graph.set_zlim(min_z, max_z)\n\n        def get_precision(min_val, max_val):\n            \"\"\"Calculates precision\n\n            :param min_val: minimum\n            :param max_val: maximum\n            :return: precision: prevision of values\n            \"\"\"\n            return int((max_val - min_val) * (1 + precision))\n\n        def get_precision_delta(min_val, max_val):\n            \"\"\"Calculates precision delta\n\n            :param min_val: minimum\n            :param max_val: maximum\n            :return: delta: Precision delta\n            \"\"\"\n            return float(max_val - min_val) / float(10 * precision)\n\n        def plot_slice():\n            \"\"\" Plots slice\n\n            :return: shows plot\n            \"\"\"\n            chart = plt.axes(projection=\"3d\")  # general settings\n            points_x = get_precision(min_x, max_x)\n            points_y = get_precision(min_y, max_z)\n\n            x_axis = numpy.outer(linspace(min_x, max_x, points_x), points_x)\n            y_axis = numpy.outer(\n                linspace(min_y, max_y, points_y).flatten(), points_y\n            ).T\n\n            def update(val):\n                \"\"\"Updates chart with value\n\n                :param val: value\n                \"\"\"\n                chart.clear()\n                x_const = slider.val\n                z_axis = func(x_const, x_axis, y_axis)\n                chart.plot_surface(\n                    x_axis, y_axis, z_axis, alpha=0.3, linewidth=2.0\n                )\n                set_labels(chart, \"y\", \"z\", \"w\")\n\n            # slider\n            axis_slider = plt.axes([0.12, 0.03, 0.78, 0.03], axisbg=\"white\")\n            slider = Slider(axis_slider, \"x\", min_x, max_x, valinit=min_x)\n\n            slider.on_changed(update)\n            set_limits(chart)\n            self.show_plot()\n\n            slider.on_changed(update)\n            set_labels(chart, \"y\", \"z\", \"w\")\n\n        def plot_countour():\n            \"\"\"Plots countour\n            \"\"\"\n            # general settings\n            fig = plt.figure()\n            chart = fig.gca(projection=\"3d\")\n\n            # create axes\n            x_axis = numpy.arange(min_x, max_x, get_precision_delta(\n                min_x, max_x)).tolist()\n            y_axis = numpy.arange(min_y, max_y, get_precision_delta(\n                min_y, max_y)).tolist()\n            x_axis, y_axis = numpy.meshgrid(x_axis, y_axis)\n\n            def update(val):\n                \"\"\"Updates chart with value\n\n                :param val: value\n                \"\"\"\n                chart.clear()  # re-plot\n                x_const = slider.val\n                z_axis = []\n\n                # add new points\n                for i, _ in enumerate(x_axis):\n                    z_axis.append(func(x_const, x_axis[i], y_axis[i]))\n\n                # show\n                chart.contour(\n                    x_axis, y_axis, z_axis, zdir=\"x\", offset=min_x\n                )\n                chart.contour(\n                    x_axis, y_axis, z_axis, zdir=\"y\", offset=min_y\n                )\n                chart.contour(\n                    x_axis, y_axis, z_axis, zdir=\"z\", offset=min_z\n                )\n                chart.contour(x_axis, y_axis, z_axis, extend3d=True)\n                set_labels(chart, \"y\", \"z\", \"w\")\n\n            # slider\n            axis_slider = plt.axes([0.12, 0.03, 0.78, 0.03], axisbg=\"white\")\n            slider = Slider(axis_slider, \"x\", min_x, max_x, valinit=min_x)\n\n            slider.on_changed(update)\n            set_limits(chart)\n\n        if kind == \"slice\":\n            plot_slice()\n        elif kind == \"countour\":\n            plot_countour()\n\n        self.show_plot()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring matplotlib according to my seaborn specification.", "response": "def configure() -> None:\n    \"\"\" Configure matplotlib according to my (biased) specification.\n\n    As a high level summary, this is a combination of a number of seaborn settings, along with\n    my own tweaks. By calling this function, the matplotlilb ``rcParams`` will be modified according\n    to these settings.\n\n    Up to this point, the settings have been configured by importing the `jet_hadron.plot.base`\n    module, which set a variety of parameters on import. This included some options which were set\n    by seaborn. Additional modifications were made to the fonts to ensure that they are the same in\n    labels and latex. Lastly, it tweaked smaller visual settings. The differences between the default\n    matplotlib and these settings are:\n\n    .. code-block:: python\n        >>> pprint.pprint(diff)\n        {'axes.axisbelow': 'original: line, new: True',\n         'axes.edgecolor': 'original: black, new: .15',\n         'axes.labelcolor': 'original: black, new: .15',\n         'axes.labelsize': 'original: medium, new: 12.0',\n         'axes.linewidth': 'original: 0.8, new: 1.25',\n         'axes.prop_cycle': \"original: cycler('color', ['#1f77b4', '#ff7f0e', \"\n                            \"'#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', \"\n                            \"'#7f7f7f', '#bcbd22', '#17becf']), new: cycler('color', \"\n                            '[(0.2980392156862745, 0.4470588235294118, '\n                            '0.6901960784313725), (0.8666666666666667, '\n                            '0.5176470588235295, 0.3215686274509804), '\n                            '(0.3333333333333333, 0.6588235294117647, '\n                            '0.40784313725490196), (0.7686274509803922, '\n                            '0.3058823529411765, 0.3215686274509804), '\n                            '(0.5058823529411764, 0.4470588235294118, '\n                            '0.7019607843137254), (0.5764705882352941, '\n                            '0.47058823529411764, 0.3764705882352941), '\n                            '(0.8549019607843137, 0.5450980392156862, '\n                            '0.7647058823529411), (0.5490196078431373, '\n                            '0.5490196078431373, 0.5490196078431373), (0.8, '\n                            '0.7254901960784313, 0.4549019607843137), '\n                            '(0.39215686274509803, 0.7098039215686275, '\n                            '0.803921568627451)])',\n         'axes.titlesize': 'original: large, new: 12.0',\n         'font.sans-serif': \"original: ['DejaVu Sans', 'Bitstream Vera Sans', \"\n                            \"'Computer Modern Sans Serif', 'Lucida Grande', 'Verdana', \"\n                            \"'Geneva', 'Lucid', 'Arial', 'Helvetica', 'Avant Garde', \"\n                            \"'sans-serif'], new: ['Arial', 'DejaVu Sans', 'Liberation \"\n                            \"Sans', 'Bitstream Vera Sans', 'sans-serif']\",\n         'font.size': 'original: 10.0, new: 12.0',\n         'grid.color': 'original: #b0b0b0, new: .8',\n         'grid.linewidth': 'original: 0.8, new: 1.0',\n         'image.cmap': 'original: viridis, new: rocket',\n         'legend.fontsize': 'original: medium, new: 11.0',\n         'lines.solid_capstyle': 'original: projecting, new: round',\n         'mathtext.bf': 'original: sans:bold, new: Bitstream Vera Sans:bold',\n         'mathtext.fontset': 'original: dejavusans, new: custom',\n         'mathtext.it': 'original: sans:italic, new: Bitstream Vera Sans:italic',\n         'mathtext.rm': 'original: sans, new: Bitstream Vera Sans',\n         'patch.edgecolor': 'original: black, new: w',\n         'patch.facecolor': 'original: C0, new: (0.2980392156862745, '\n                            '0.4470588235294118, 0.6901960784313725)',\n         'patch.force_edgecolor': 'original: False, new: True',\n         'text.color': 'original: black, new: .15',\n         'text.usetex': 'original: False, new: True',\n         'xtick.color': 'original: black, new: .15',\n         'xtick.direction': 'original: out, new: in',\n         'xtick.labelsize': 'original: medium, new: 11.0',\n         'xtick.major.size': 'original: 3.5, new: 6.0',\n         'xtick.major.width': 'original: 0.8, new: 1.25',\n         'xtick.minor.size': 'original: 2.0, new: 4.0',\n         'xtick.minor.top': 'original: True, new: False',\n         'xtick.minor.visible': 'original: False, new: True',\n         'xtick.minor.width': 'original: 0.6, new: 1.0',\n         'ytick.color': 'original: black, new: .15',\n         'ytick.direction': 'original: out, new: in',\n         'ytick.labelsize': 'original: medium, new: 11.0',\n         'ytick.major.size': 'original: 3.5, new: 6.0',\n         'ytick.major.width': 'original: 0.8, new: 1.25',\n         'ytick.minor.right': 'original: True, new: False',\n         'ytick.minor.size': 'original: 2.0, new: 4.0',\n         'ytick.minor.visible': 'original: False, new: True',\n         'ytick.minor.width': 'original: 0.6, new: 1.0'}\n\n    I implemented most of these below (although I left out a few color options).\n\n    Args:\n        None.\n    Returns:\n        None. The current matplotlib ``rcParams`` are modified.\n    \"\"\"\n    # Color definitions from seaborn\n    # NOTE: They need to be strings rather than raw floats.\n    light_grey = \".8\"\n    # NOTE: I elect not to label with dark grey instead of black. It's not clear to me\n    #       why that might be preferable here.\n\n    # Setup the LaTeX preamble\n    # Enable AMS math package (for among other things, \"\\text\")\n    matplotlib.rcParams[\"text.latex.preamble\"].append(r\"\\usepackage{amsmath}\")\n    # Add fonts that will be used below. See the `mathtext` fonts set below for further info.\n    matplotlib.rcParams[\"text.latex.preamble\"].append(r\"\\usepackage{sfmath}\")\n    params = {\n        # Enable latex\n        \"text.usetex\": True,\n        # Enable axis ticks (after they can be disabled by seaborn)\n        \"xtick.bottom\": True,\n        \"ytick.left\": True,\n        # Make minor axis ticks visible (but only on left and bottom)\n        \"xtick.minor.visible\": True,\n        \"ytick.minor.visible\": True,\n        \"xtick.minor.top\": False,\n        \"ytick.minor.right\": False,\n        # Ensure that axis ticks go inward instead of outward\n        \"xtick.direction\": \"in\",\n        \"ytick.direction\": \"in\",\n        # Below, we set the LaTeX fonts to be the same fonts as those used in matplotlib.\n        # For sans serif fonts in LaTeX (required for setting the fonts below), see: https://stackoverflow.com/a/11612347\n        # To set the latex fonts to be the same as the normal matplotlib fonts, see: https://stackoverflow.com/a/27697390\n        \"mathtext.fontset\": \"custom\",\n        \"mathtext.rm\": \"Bitstream Vera Sans\",\n        \"mathtext.it\": \"Bitstream Vera Sans:italic\",\n        \"mathtext.bf\": \"Bitstream Vera Sans:bold\",\n        ##### Extracted from seaborn\n        # Plot axis underneath points\n        \"axes.axisbelow\": True,\n        # Modify label sizes.\n        \"axes.labelsize\": 12.0,\n        \"axes.linewidth\": 1.25,\n        \"axes.titlesize\": 12.0,\n        \"font.size\": 12.0,\n        \"legend.fontsize\": 11.0,\n        # Set the possible sans serif fonts. These are the ones made available in seaborn.\n        \"font.sans-serif\": [\"Arial\", \"DejaVu Sans\", \"Liberation \" \"Sans\",\n                            \"Bitstream Vera Sans\", \"sans-serif\"],\n        # Make the grid lines light grey and slightly larger.\n        \"grid.color\": light_grey,\n        \"grid.linewidth\": 1.0,\n        # End a line in a rounded style.\n        \"lines.solid_capstyle\": \"round\",\n        # This will disable lines connecting data points.\n        # NOTE: This is disabled because if you forget to set the marker, then nothing will show up,\n        #       which is a very frustrating user experience. Better to instead just disable it for a\n        #       given plot.\n        #\"lines.linestyle\": \"none\",\n        # Set the edge color to white.\n        \"patch.edgecolor\": \"none\",\n        # Apparently this has to be enabled just for setting the edge color to be possible.\n        \"patch.force_edgecolor\": True,\n        # Tick label size.\n        \"xtick.labelsize\": 11.0,\n        \"ytick.labelsize\": 11.0,\n        # Major tick settings\n        \"xtick.major.size\": 6.0,\n        \"ytick.major.size\": 6.0,\n        \"xtick.major.width\": 1.25,\n        \"ytick.major.width\": 1.25,\n        # Minor tick settings\n        \"xtick.minor.size\": 4.0,\n        \"ytick.minor.size\": 4.0,\n        \"xtick.minor.width\": 1.0,\n        \"ytick.minor.width\": 1.0,\n    }\n\n    # Apply the updated settings.\n    matplotlib.rcParams.update(params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set(self, **kwargs):\n        self.player_lock.acquire()\n        if 'acqtime' in kwargs:\n            self.player.set_aidur(kwargs['acqtime'])\n        if 'aifs' in kwargs:\n            self.player.set_aifs(kwargs['aifs'])\n            self.aifs = kwargs['aifs']\n        if 'aifs' in kwargs or 'acqtime' in kwargs:\n            t = kwargs.get('acqtime', self.player.get_aidur())\n            npoints = t*float(kwargs.get('aifs', self.player.get_aifs()))\n            self.aitimes = np.linspace(0, t, npoints)\n        if 'trigger' in kwargs:\n            self.player.set_trigger(kwargs['trigger'])\n        self.player_lock.release()\n\n        if 'aochan' in kwargs:\n            self.aochan = kwargs['aochan']\n        if 'aichan' in kwargs:\n            self.aichan = kwargs['aichan']\n        if 'binsz' in kwargs:\n            self.binsz = kwargs['binsz']\n        if 'save' in kwargs:\n            self.save_data = kwargs['save']\n        if 'caldb' in kwargs:\n            self.caldb = kwargs['caldb']\n        if 'calv' in kwargs:\n            self.calv = kwargs['calv']\n        if 'calf' in kwargs:\n            self.calf = kwargs['calf']\n        if 'caldb' in kwargs or 'calv' in kwargs:\n            self.update_reference_voltage()\n        if 'datafile' in kwargs:\n            self.datafile = kwargs['datafile']\n        if 'reprate' in kwargs:\n            self.reprate = kwargs['reprate']\n        if 'save' in kwargs:\n            self.save_data = kwargs['save']\n        if 'average' in kwargs:\n            self.average = kwargs['average']\n        if 'reject' in kwargs:\n            self.reject = kwargs['reject']\n        if 'rejectrate' in kwargs:\n            self.rejectrate = kwargs['rejectrate']", "response": "Set an internal setting for the acquistion."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interval_wait(self):\n        # calculate time since last interation and wait to acheive desired interval\n        now = time.time()\n        elapsed = (now - self.last_tick)*1000\n        # print(\"interval %d, time from start %d \\n\" % (elapsed, (now - self.start_time)*1000))\n        if elapsed < self.interval:\n            # print('sleep ', (self.interval-elapsed))\n            # self.signals.warning.emit('') # clear previous warning\n            time.sleep((self.interval-elapsed)/1000)\n            now = time.time()\n        elif elapsed > self.interval:\n            pass\n            # self.signals.warning.emit(\"WARNING: PROVIDED INTERVAL EXCEEDED, ELAPSED TIME %d\" % (elapsed))\n        self.last_tick = now", "response": "Pauses the correct amount of time according to this \n        acquisition object s interval setting and the last time this \n        function was called"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef putnotify(self, name, *args):\n        # self.signals[name][0].send(*args)\n        self.queues[name][0].put(*args)\n        self.queues[name][1].set()", "response": "Puts data into queue and alerts listeners"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the text to msg *", "response": "def setComment(self, msg):\n        \"\"\"Sets the widget text to *msg*\n\n        :param msg: overwrites any existing text with *msg*\n        :type msg: str\n        \"\"\"\n        self.ui.commentTxtedt.setPlainText(msg)\n        # move text cursor to end\n        self.ui.commentTxtedt.moveCursor(QtGui.QTextCursor.End)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the taxonomic assignments for each read", "response": "def loadassignment(self):\n        \"\"\"Load the taxonomic assignment for each read\"\"\"\n        printtime('Finding taxonomic assignments', self.start)\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.assignmentload, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.runmetadata.samples:\n            self.loadqueue.put(sample)\n        self.loadqueue.join()\n        # Filter the .fastq files\n        self.readlist()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsorts the reads and create lists to be used in creating sorted. fastq files", "response": "def readlist(self):\n        \"\"\"Sort the reads, and create lists to be used in creating sorted .fastq files\"\"\"\n        printtime('Sorting reads', self.start)\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.listread, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.runmetadata.samples:\n            self.listqueue.put(sample)\n        self.listqueue.join()\n        # Create\n        self.fastqfilter()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiltering the reads into separate files based on taxonomic assignment", "response": "def fastqfilter(self):\n        \"\"\"Filter the reads into separate files based on taxonomic assignment\"\"\"\n        printtime('Creating filtered .fastqfiles', self.start)\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.filterfastq, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.runmetadata.samples:\n            self.filterqueue.put(sample)\n        self.filterqueue.join()\n        # Print the metadata to file\n        metadataprinter.MetadataPrinter(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_escapes(self):\n        chars = []\n\n        i = 0\n        while i < len(self.string):\n            char = self.string[i]\n            if char == \"\\\\\":\n                i += 1\n            else:\n                chars.append(char)\n\n            i += 1\n\n        return \"\".join(chars)", "response": "Removes everything except number and letters from string\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove accents from text", "response": "def convert_accents(self):\n        \"\"\"Removes accents from text\n\n        :return: input with converted accents chars\n        \"\"\"\n        nkfd_form = unicodedata.normalize('NFKD', self.string)\n        return \"\".join([\n            char\n            for char in nkfd_form\n            if not unicodedata.combining(char)\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_control_chars(self):\n        esc_key = Literal('\\x1b')\n        integer = Word(nums)\n        escape_seq = Combine(\n            esc_key + '[' + Optional(delimitedList(integer, ';')) +\n            oneOf(list(alphas)))\n\n        return Suppress(escape_seq).transformString(self.string)", "response": "Removes controls chars from text\n    \n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if string is well formatted and returns True iff it is well formatted", "response": "def is_well_formatted(self):\n        \"\"\"Checks if string is good formatted\n    \n        :return: True iff string is good formatted\n        \"\"\"\n        # False iff there are at least \\n, \\r, \\t,\"  \"\n        is_bad_formatted = \":\" in self.string or \\\n                           \"\\\\'\" in self.string or \\\n                           \"\\n\" in self.string or \\\n                           \"\\r\" in self.string or \\\n                           \"\\t\" in self.string or \\\n                           \"\\\\n\" in self.string or \\\n                           \"\\\\r\" in self.string or \\\n                           \"\\\\t\" in self.string or \\\n                           \"  \" in self.string\n        return not is_bad_formatted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstripping string of all HTML elements in the given string with raw HTML elements removed.", "response": "def strip_bad_html(self):\n        \"\"\"Strips string of all HTML elements\n    \n        :return: Given string with raw HTML elements removed\n        \"\"\"\n        out = self.string\n        while not String(out).is_well_formatted():\n            out = out.replace(\":\", \"\") \\\n                .replace(\"\\\\'\", \"\\'\") \\\n                .replace(\"\\\\n\", \"\") \\\n                .replace(\"\\\\r\", \"\") \\\n                .replace(\"\\\\t\", \"\") \\\n                .replace(\"\\n\", \"\") \\\n                .replace(\"\\r\", \"\") \\\n                .replace(\"\\t\", \"\") \\\n                .replace(\"  \", \" \") \\\n                .strip()\n\n        return str(out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_all(self, token):\n\n        out = self.string.replace(\" \", token)  # replace tokens\n        while out.find(token + token) >= 0:  # while there are tokens\n            out = out.replace(token + token, token)\n        return out", "response": "Removes all occurrences of token from the input string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_logging():\n    with open(os.path.join(os.path.dirname(__file__),'logging.conf'), 'r') as yf:\n        config = yaml.load(yf)\n    logging.config.dictConfig(config)", "response": "Initialize a logger from a configuration file to use throughout the project"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsee if this slug exists already", "response": "def _clean_page_unique_slug_required(self, slug):\n        \"\"\"See if this slug exists already\"\"\"\n\n        if hasattr(self, 'instance') and self.instance.id:\n            if Content.objects.exclude(page=self.instance).filter(\n                    body=slug, type=\"slug\").count():\n                raise forms.ValidationError(self.err_dict['another_page_error'])\n        elif Content.objects.filter(body=slug, type=\"slug\").count():\n            raise forms.ValidationError(self.err_dict['another_page_error'])\n        return slug"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the raw traceback from the current stack frame.", "response": "def extract_stack(start=0):\n    \"\"\"\n    SNAGGED FROM traceback.py\n    Altered to return Data\n\n    Extract the raw traceback from the current stack frame.\n\n    Each item in the returned list is a quadruple (filename,\n    line number, function name, text), and the entries are in order\n    from newest to oldest\n    \"\"\"\n    try:\n        raise ZeroDivisionError\n    except ZeroDivisionError:\n        trace = sys.exc_info()[2]\n        f = trace.tb_frame.f_back\n\n    for i in range(start):\n        f = f.f_back\n\n    stack = []\n    while f is not None:\n        stack.append({\n            \"line\": f.f_lineno,\n            \"file\": f.f_code.co_filename,\n            \"method\": f.f_code.co_name\n        })\n        f = f.f_back\n    return stack"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extract_traceback(start):\n    tb = sys.exc_info()[2]\n    for i in range(start):\n        tb = tb.tb_next\n    return _parse_traceback(tb)", "response": "Extract a traceback from sys. exc_info and return a list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps an exception into a Except object.", "response": "def wrap(cls, e, stack_depth=0):\n        \"\"\"\n        ENSURE THE STACKTRACE AND CAUSAL CHAIN IS CAPTURED, PLUS ADD FEATURES OF Except\n\n        :param e: AN EXCEPTION OF ANY TYPE\n        :param stack_depth: HOW MANY CALLS TO TAKE OFF THE TOP OF THE STACK TRACE\n        :return: A Except OBJECT OF THE SAME\n        \"\"\"\n        if e == None:\n            return Null\n        elif isinstance(e, (list, Except)):\n            return e\n        elif is_data(e):\n            e.cause = unwraplist([Except.wrap(c) for c in listwrap(e.cause)])\n            return Except(**e)\n        else:\n            tb = getattr(e, '__traceback__', None)\n            if tb is not None:\n                trace = _parse_traceback(tb)\n            else:\n                trace = _extract_traceback(0)\n\n            cause = Except.wrap(getattr(e, '__cause__', None))\n            if hasattr(e, \"message\") and e.message:\n                output = Except(context=ERROR, template=text_type(e.message), trace=trace, cause=cause)\n            else:\n                output = Except(context=ERROR, template=text_type(e), trace=trace, cause=cause)\n\n            trace = extract_stack(stack_depth + 2)  # +2 = to remove the caller, and it's call to this' Except.wrap()\n            output.trace.extend(trace)\n            return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a new entry to the cache.", "response": "async def add(self, rule, kwargs=None):\n        \"\"\"\n        Increments the counter for the given *rule* and *kwargs*.\n\n        If this pair of *rule* and *kwargs* doesn't already have a counter, it\n        is created.\n\n        *rule* is the :class:`rule.Rule` instance that got the match.\n\n        *kwargs* is an optional dict of vars captured by the\n        :class:`filter.Filter` that match the log entry.\n        \"\"\"\n        index = self[rule.name].increment(kwargs)\n\n        if self[rule.name][index] >= rule.limit:\n            await rule.action.run(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincrement the counter for the given kwargs.", "response": "def increment(self, kwargs):\n        \"\"\"\n        Increments the counter for the given *kwargs*.\n\n        The counter index is computed from *kwargs*.\n\n        *kwargs* is an optional dict of vars captured by the\n        :class:`filter.Filter` that match the log entry. An immutable version\n        of *kwargs* is used as an index to keep track of several counters for\n        the same :class:`rule.Rule`. It can be `None`.\n\n        Returns the index of the updated counter.\n        \"\"\"\n        index = None\n\n        if kwargs:\n            # index = hash(tuple(sorted(kwargs.items())))\n            # Better keep something readable so we can output it.\n            index = tuple(sorted(kwargs.items()))\n\n        self[index] += 1\n\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving a tie between candidates", "response": "def resolve_exclusion_tie(self, candidates):\n        \"\"\"\n        call callback to resolve a tie between candidates\n        \"\"\"\n        sorted_candidate_ids = list(sorted(candidates, key=self.candidate_order_fn))\n        return sorted_candidate_ids[self.exclusion_tie_cb(candidates)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving a tie between candidates", "response": "def resolve_election_tie(self, candidates):\n        \"\"\"\n        call callback to resolve a tie between candidates\n        \"\"\"\n        sorted_candidate_ids = list(sorted(candidates, key=self.candidate_order_fn))\n        return sorted_candidate_ids[self.election_tie_cb(candidates)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef determine_elected_candidates_in_order(self, candidate_votes):\n        eligible_by_vote = defaultdict(list)\n        for candidate_id, votes in candidate_votes.candidate_votes_iter():\n            if candidate_id in self.candidates_elected:\n                continue\n            if votes < self.quota:\n                continue\n            eligible_by_vote[votes].append(candidate_id)\n\n        elected = []\n        for votes in reversed(sorted(eligible_by_vote)):\n            candidate_ids = eligible_by_vote[votes]\n            # we sort here to ensure stability, so external callers can hard-coded their response\n            candidate_ids.sort(key=self.candidate_order_fn)\n            if len(candidate_ids) == 1:\n                elected.append(candidate_ids[0])\n            else:\n                tie_breaker_round = self.find_tie_breaker(candidate_ids)\n                if tie_breaker_round is not None:\n                    self.results.provision_used(\n                        ActProvision(\"Multiple candidates elected with %d votes. Tie broken from previous totals.\" % (votes)))\n                    for candidate_id in reversed(sorted(candidate_ids, key=tie_breaker_round.get_vote_count)):\n                        elected.append(candidate_id)\n                else:\n                    self.results.provision_used(\n                        ActProvision(\"Multiple candidates elected with %d votes. Input required from Australian Electoral Officer.\" % (votes)))\n                    permutations = list(itertools.permutations(candidate_ids))\n                    permutations.sort()\n                    choice = self.resolve_election_order(permutations)\n                    for candidate_id in permutations[choice]:\n                        elected.append(candidate_id)\n        return elected", "response": "determine all candidates with at least a quota of votes in candidate_votes. returns results in order of decreasing vote count."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_initial_totals(self):\n        \"determine the initial total for each candidate. only call this at the start of round 1\"\n        candidate_votes = {}\n        # initialise to zero for every individual candidate\n        for candidate_id in self.candidate_ids:\n            candidate_votes[candidate_id] = 0\n        for candidate_id in self.candidate_ids:\n            candidate_votes[candidate_id] = self.candidate_bundle_transactions.get_paper_count(candidate_id)\n        for candidate_id in candidate_votes:\n            candidate_votes[candidate_id] = int(candidate_votes[candidate_id])\n        return candidate_votes, 0, 0", "response": "determine the initial total for each candidate. only call this at the start of round 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bundle_to_next_candidate(self, bundle):\n        ticket_state = bundle.ticket_state\n        while True:\n            ticket_state = TicketState(ticket_state.preferences, ticket_state.up_to + 1)\n            candidate_id = get_preference(ticket_state)\n            # if the preference passes through an elected or excluded candidate, we\n            # skip over it\n            if candidate_id in self.candidates_elected or candidate_id in self.candidates_excluded:\n                continue\n            return candidate_id, ticket_state", "response": "Returns the next candidate_id and ticket_state for the next preference in the bundle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distribute_bundle_transactions(self, candidate_votes, bundle_transactions_to_distribute, transfer_value):\n        \"bundle_transactions_to_distribute is an array of tuples, [(CandidateFrom, BundleTransactions)]\"\n\n        # figure out how many net votes, and which ticket_tpls, go where\n        incoming_tickets = defaultdict(list)\n        exhausted_papers = 0\n\n        # determine how many papers go to each candidate; effectively we unpack each transaction bundle into\n        # preference flows, split as needed, and then repack into new bundles\n        for from_candidate_id, bundle_transactions in bundle_transactions_to_distribute:\n            for bundle_transaction in bundle_transactions:\n                # take the bundle away from the candidate\n                candidate_votes[from_candidate_id] -= bundle_transaction.votes\n                self.candidate_bundle_transactions.transfer_from(from_candidate_id, bundle_transaction)\n\n                for bundle in bundle_transaction.bundles:\n                    # determine the candidate(s) that this bundle of papers flows to\n                    to_candidate, next_ticket_state = self.bundle_to_next_candidate(bundle)\n                    if to_candidate is None:\n                        exhausted_papers += bundle.size\n                        continue\n                    incoming_tickets[to_candidate].append((next_ticket_state, bundle.size))\n\n        for candidate_id in sorted(incoming_tickets, key=self.candidate_order_fn):\n            bundles_moving = []\n            for ticket, new_count in incoming_tickets[candidate_id]:\n                bundles_moving.append(PaperBundle(ticket, new_count))\n            bundle_transaction = make_bundle_transaction(bundles_moving, transfer_value)\n            self.candidate_bundle_transactions.transfer_to(candidate_id, bundle_transaction)\n            candidate_votes[candidate_id] += bundle_transaction.votes\n\n        exhausted_votes = int(exhausted_papers * transfer_value)\n        return exhausted_votes, exhausted_papers", "response": "This function takes a list of tuples from the candidate_from and bundle_transactions and attempts to distribute them into the new bundle_transactions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef elect(self, candidate_aggregates, candidate_id):\n\n        # somewhat paranoid cross-check, but we've had this bug before..\n        assert(candidate_id not in self.candidates_elected)\n\n        elected_no = len(self.candidates_elected) + 1\n        self.candidates_elected[candidate_id] = True\n\n        transfer_value = 0\n        excess_votes = paper_count = None\n        if len(self.candidates_elected) != self.vacancies:\n            excess_votes = max(candidate_aggregates.get_vote_count(candidate_id) - self.quota, 0)\n            assert(excess_votes >= 0)\n            paper_count = self.candidate_bundle_transactions.get_paper_count(candidate_id)\n            if paper_count > 0:\n                transfer_value = fractions.Fraction(excess_votes, paper_count)\n            assert(transfer_value >= 0)\n            self.election_distributions_pending.append((candidate_id, transfer_value, excess_votes))\n        self.results.candidate_elected(\n            CandidateElected(\n                candidate_id=candidate_id,\n                order=elected_no,\n                excess_votes=excess_votes,\n                paper_count=paper_count,\n                transfer_value=transfer_value))", "response": "Elect a candidate updating internal state to track this."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exclude_candidates(self, candidates, reason):\n\n        # put some paranoia around exclusion: we want to make sure that\n        # `candidates` is unique, and that none of these candidates have\n        # been previously excluded\n        for candidate_id in candidates:\n            assert(candidate_id not in self.candidates_excluded)\n        assert(len(set(candidates)) == len(candidates))\n\n        # determine the paper transfers to be run, and the candidates\n        # holding papers which are distributed in each transfer\n        transfers_applicable = defaultdict(set)\n        for candidate_id in candidates:\n            self.candidates_excluded[candidate_id] = True\n            for bundle_transaction in self.candidate_bundle_transactions.get(candidate_id):\n                value = bundle_transaction.transfer_value\n                transfers_applicable[value].add(candidate_id)\n\n        transfer_values = list(reversed(sorted(transfers_applicable)))\n        self.results.candidates_excluded(\n            CandidatesExcluded(\n                candidates=candidates,\n                transfer_values=transfer_values,\n                reason=reason))\n\n        for transfer_value in transfer_values:\n            self.exclusion_distributions_pending.append((list(transfers_applicable[transfer_value]), transfer_value))", "response": "Mark one or more candidates as excluded from the count\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind a tie breaker in which the candidate_ids each had different vote counts returns None", "response": "def find_tie_breaker(self, candidate_ids):\n        \"\"\"\n        finds a round in the count history in which the candidate_ids each had different vote counts\n        if no such round exists, returns None\n        \"\"\"\n        for candidate_aggregates in reversed(self.round_candidate_aggregates):\n            candidates_on_vote = defaultdict(int)\n            for candidate_id in candidate_ids:\n                votes = candidate_aggregates.get_vote_count(candidate_id)\n                candidates_on_vote[votes] += 1\n            if max(candidates_on_vote.values()) == 1:\n                return candidate_aggregates"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_candidate_notional_votes(self, candidate_aggregates, adjustment):\n        \"aggregate of vote received by each candidate, and the votes received by any candidate lower in the poll\"\n        continuing = self.get_continuing_candidates(candidate_aggregates)\n        candidates_notional = {}\n        by_votes = self.get_votes_to_candidates(continuing, candidate_aggregates)\n        total = adjustment\n        for votes, candidates in sorted(by_votes.items(), key=lambda x: x[0]):\n            for candidate_id in candidates:\n                candidates_notional[candidate_id] = total + votes\n            total += votes * len(candidates)\n        return candidates_notional", "response": "aggregate of vote received by each candidate and the votes received by any candidate lower in the poll"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef determine_bulk_exclusions(self, candidate_aggregates):\n        \"determine candidates who may be bulk excluded, under 273(13)\"\n        # adjustment as under (13C) - seems to only apply if more than one candidate was elected in a round\n        continuing = self.get_continuing_candidates(candidate_aggregates)\n        candidate_votes = candidate_aggregates.get_candidate_votes()\n        by_votes = self.get_votes_to_candidates(continuing, candidate_aggregates)\n        adjustment = sum(excess_votes for _, _, excess_votes in self.election_distributions_pending)\n        candidate_notional_votes = self.get_candidate_notional_votes(candidate_aggregates, adjustment)\n        leading_shortfall, vacancy_shortfall = self.get_leading_and_vacancy_shortfall(candidate_aggregates)\n\n        def determine_candidate_A():\n            # notional votes >= vacancy shortfall\n            eligible = [candidate_id for candidate_id, notional in candidate_notional_votes.items() if notional >= vacancy_shortfall]\n            if len(eligible) == 0:\n                return None\n            # lowest in the poll: tie is irrelevant\n            eligible.sort(key=lambda candidate_id: candidate_votes[candidate_id])\n            return eligible[0]\n\n        sorted_votes = list(sorted(by_votes.keys()))\n\n        def notional_lower_than_higher(candidate_id):\n            \"check notional votes of candidate is lower than the number of votes of the candidate standing immediately higher\"\n            votes = candidate_votes[candidate_id]\n            votes_idx = sorted_votes.index(votes)\n            # no higher candidate\n            if votes_idx == len(sorted_votes) - 1:\n                return False\n            # legislation ambiguous, but if there's a tie above us let's check all the candidates on that count\n            notional = candidate_notional_votes[candidate_id]\n            higher_votes = sorted_votes[votes_idx + 1]\n            acceptable = all(notional < candidate_votes[t] for t in by_votes[higher_votes])\n            return acceptable\n\n        def highest(eligible):\n            \"return the highest ranked candidate in candidates, by vote. if a tie, or list empty, return None\"\n            if not eligible:\n                return\n            binned = self.get_votes_to_candidates(eligible, candidate_aggregates)\n            possible = binned[max(binned)]\n            if len(possible) == 1:\n                return possible[0]\n\n        def determine_candidate_B(candidate_A):\n            if candidate_A is not None:\n                A_votes = candidate_votes[candidate_A]\n                eligible = [candidate_id for candidate_id in continuing if candidate_votes[candidate_id] < A_votes]\n            else:\n                eligible = [candidate_id for candidate_id in continuing if candidate_notional_votes[candidate_id] < vacancy_shortfall]\n            eligible = [candidate_id for candidate_id in eligible if notional_lower_than_higher(candidate_id)]\n            return highest(eligible)\n\n        def determine_candidate_C():\n            eligible = [candidate_id for candidate_id in continuing if candidate_notional_votes[candidate_id] < leading_shortfall]\n            # the candidate of those eligible which stands highest in the poll\n            eligible.sort(key=lambda candidate_id: candidate_votes[candidate_id])\n            return highest(eligible)\n\n        def candidates_lte(candidate_id):\n            votes = candidate_votes[candidate_id]\n            lower_votes = [t for t in by_votes.keys() if t < votes]\n            to_exclude = [candidate_id]\n            for vote in lower_votes:\n                to_exclude += [candidate_id for candidate_id in by_votes[vote] if candidate_id in continuing]\n            return to_exclude\n\n        # candidate A, B, C as under (13A)(a)\n        to_exclude = None\n        candidate_A = determine_candidate_A()\n        candidate_B = determine_candidate_B(candidate_A)\n        candidate_C = None\n        if candidate_B:\n            candidate_B_votes = candidate_votes[candidate_B]\n            if candidate_B_votes < leading_shortfall:\n                to_exclude = candidates_lte(candidate_B)\n            else:\n                candidate_C = determine_candidate_C()\n                if candidate_C:\n                    to_exclude = candidates_lte(candidate_C)\n        if to_exclude and len(to_exclude) == 1:\n            to_exclude = None\n        return to_exclude, ExclusionReason(\"bulk\", {\n            \"candidate_A\": candidate_A,\n            \"candidate_B\": candidate_B,\n            \"candidate_C\": candidate_C\n        })", "response": "determine candidates who may be bulk excluded under 273 ( 13 )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the current user has permission on the page.", "response": "def check(self, action, page=None, lang=None, method=None):\n        \"\"\"Return ``True`` if the current user has permission on the page.\"\"\"\n        if self.user.is_superuser:\n            return True\n\n        if action == 'change':\n            return self.has_change_permission(page, lang, method)\n\n        if action == 'delete':\n            if not self.delete_page():\n                return False\n            return True\n        if action == 'add':\n            if not self.add_page():\n                return False\n            return True\n        if action == 'freeze':\n            perm = self.user.has_perm('pages.can_freeze')\n            if perm:\n                return True\n            return False\n        if action == 'publish':\n            perm = self.user.has_perm('pages.can_publish')\n            if perm:\n                return True\n            return False\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the current user has permission to change the page.", "response": "def has_change_permission(self, page, lang, method=None):\n        \"\"\"Return ``True`` if the current user has permission to\n        change the page.\"\"\"\n\n        # the user has always the right to look at a page content\n        # if he doesn't try to modify it.\n        if method != 'POST':\n            return True\n\n        # right to change all the pages\n        if self.change_page():\n            return True\n        if lang:\n            # try the global language permission first\n            perm = self.user.has_perm(\n                'pages.can_manage_%s' % lang.replace('-', '_')\n            )\n            if perm:\n                return True\n            # then per object permission\n            perm_func = getattr(self, 'manage (%s)_page' % lang)\n            if perm_func(page):\n                return True\n        # last hierarchic permissions because it's more expensive\n        perm_func = getattr(self, 'manage hierarchy_page')\n        if perm_func(page):\n            return True\n        else:\n            for ancestor in page.get_ancestors():\n                if perm_func(ancestor):\n                    return True\n\n        # everything else failed, no permissions\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\njoining lines of text into a single string", "response": "def _join_lines(txt):\n    \"\"\" Remove whitespace from XML input \"\"\"\n    txt = txt or ''  # Handle NoneType input values\n    val = ''\n    lines = txt.split('\\n')\n    for line in lines:\n        stripped = line.strip()\n        if len(stripped) == 0:\n            continue\n        val += stripped + ' '\n    return val.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_desc(node):\n    desc = ''\n    if len(node) == 0:\n        return '<p>' + node.text + '</p>'\n    for n in node:\n        if n.tag == 'p':\n            desc += '<p>' + _join_lines(n.text) + '</p>'\n        elif n.tag == 'ol' or n.tag == 'ul':\n            desc += '<ul>'\n            for c in n:\n                if c.tag == 'li':\n                    desc += '<li>' + _join_lines(c.text) + '</li>'\n                else:\n                    raise ParseError('Expected <li> in <%s>, got <%s>' % (n.tag, c.tag))\n            desc += '</ul>'\n        else:\n            raise ParseError('Expected <p>, <ul>, <ol> in <%s>, got <%s>' % (node.tag, n.tag))\n    return desc", "response": "A quick n'dirty description parser"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_description(xml_data):\n    try:\n        root = ET.fromstring('<document>' + xml_data + '</document>')\n    except StdlibParseError as e:\n        raise ParseError(str(e))\n    return _parse_desc(root)", "response": "Validate the description for validity"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_description(text):\n    xml = ''\n    is_in_ul = False\n    for line in text.split('\\n'):\n\n        # don't include whitespace\n        line = line.strip()\n        if len(line) == 0:\n            continue\n\n        # detected as a list element?\n        line_li = _import_description_to_list_element(line)\n        if line_li:\n            # first list element\n            if not is_in_ul:\n                xml += '<ul>\\n'\n                is_in_ul = True\n            xml += '<li>' + _import_description_sentence_case(line_li) + '</li>\\n'\n            continue\n\n        # done with the list\n        if is_in_ul:\n            xml += '</ul>\\n'\n            is_in_ul = False\n\n        # regular paragraph\n        xml += '<p>' + _import_description_sentence_case(line) + '</p>\\n'\n\n    # no trailing paragraph\n    if is_in_ul:\n        xml += '</ul>\\n'\n\n    return xml", "response": "Convert ASCII text to AppStream markup format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef image_exists(self, image_name, tag='latest'):\r\n        code, image = self.image_tags(image_name)\r\n        if code != httplib.OK:\r\n            return False\r\n        tag = tag.lower()\r\n        return any(x.lower() == tag for x in image.tags)", "response": "Returns True if the image_name exists in docker. neg."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill_form_field(self, field_name, field_value):\n        self.browser.execute_script(\n            \"document.getElementsByName(\\\"\" + str(\n                field_name) + \"\\\")[0].value = \\\"\" + str(field_value) + \"\\\"\")", "response": "Fills given field with given value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfill form with login info", "response": "def fill_login_form(self, username, username_field, user_password,\n                        user_password_field):\n        \"\"\"Fills form with login info\n\n        :param username: user login\n        :param username_field: name of field to fill with username\n        :param user_password: login password\n        :param user_password_field: name of field to fill with user password\n        \"\"\"\n        self.fill_form_field(username_field, username)  # set username\n        self.fill_form_field(user_password_field, user_password)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening the given JB_File and returns the mayafile object.", "response": "def open_scene(f, kwargs=None):\n    \"\"\"Opens the given JB_File\n\n    :param f: the file to open\n    :type f: :class:`jukeboxcore.filesys.JB_File`\n    :param kwargs: keyword arguments for the command maya.cmds file.\n                   defaultflags that are always used:\n\n                     :open: ``True``\n\n                   e.g. to force the open command use ``{'force'=True}``.\n    :type kwargs: dict|None\n    :returns: An action status. The returnvalue of the actionstatus is the opened mayafile\n    :rtype: :class:`ActionStatus`\n    :raises: None\n    \"\"\"\n    defaultkwargs = {'open':True}\n    if kwargs is None:\n        kwargs = {}\n    kwargs.update(defaultkwargs)\n    fp = f.get_fullpath()\n    mayafile = cmds.file(fp, **kwargs)\n    msg = \"Successfully opened file %s with arguments: %s\" % (fp, kwargs)\n    return ActionStatus(ActionStatus.SUCCESS, msg, returnvalue=mayafile)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting all references in the currently open scene.", "response": "def import_all_references(arg, kwargs=None):\n    \"\"\"Import all references in the currently open scene\n\n    :param arg: this argument is ignored. But thisway you can use this function in an ActionUnit more easily.\n    :param kwargs: keyword arguments for the command maya.cmds file.\n                   defaultflags that are always used:\n\n                     :importReference: ``True``\n\n    :type kwargs: dict|None\n    :returns: An action status. The returnvalue of the actionstatus are the imported references.\n    :rtype: :class:`ActionStatus`\n    :raises: None\n    \"\"\"\n    defaultkwargs = {'importReference':True}\n    if kwargs is None:\n        kwargs = {}\n    kwargs.update(defaultkwargs)\n    imported = []\n\n    # list all reference files\n    refs = cmds.file(query=True, reference=True)\n    while refs:\n        for rfile in refs:\n            cmds.file(rfile, **kwargs)\n            imported.append(rfile)\n        refs = cmds.file(query=True, reference=True)\n    msg = \"Successfully imported references %s with arguments: %s\" % (imported, kwargs)\n    return ActionStatus(ActionStatus.SUCCESS, msg, returnvalue=imported)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_scenenode(f):\n    n = get_current_scene_node()\n    if not n:\n        msg = \"Could not find a scene node.\"\n        return ActionStatus(ActionStatus.FAILURE, msg)\n    # get dbentry for for the given jbfile\n    tfi = f.get_obj()\n    assert tfi\n    tf = dj.taskfiles.get(task=tfi.task,\n                          releasetype=tfi.releasetype,\n                          version=tfi.version,\n                          descriptor=tfi.descriptor,\n                          typ=tfi.typ)\n\n    cmds.setAttr('%s.taskfile_id' % n, lock=False)\n    cmds.setAttr('%s.taskfile_id' % n, tf.pk)\n    cmds.setAttr('%s.taskfile_id' % n, lock=True)\n    msg = \"Successfully updated scene node to %s\" % tf.id\n    return ActionStatus(ActionStatus.SUCCESS, msg)", "response": "Update the id of the current scene node for the given file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall the given arguments in a seperate process and returns the contents of standard out.", "response": "def call(args, stdout=PIPE, stderr=PIPE):\n    \"\"\" Calls the given arguments in a seperate process\n    and returns the contents of standard out.\n    \"\"\"\n    p = Popen(args, stdout=stdout, stderr=stderr)\n    out, err = p.communicate()\n\n    try:\n        return out.decode(sys.stdout.encoding), err.decode(sys.stdout.encoding)\n    except Exception:\n        return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make(self):\n        eval = self.command.eval()\n        with open(self.filename, 'w') as f:\n            f.write(eval)", "response": "Evaluate the command and write it to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the given attributes on all commands in collection.", "response": "def set_attrs(self, **attrs):\n        \"\"\"Set the given attributes on *all* commands in collection.\"\"\"\n        commands = tuple(self.values())\n        for name, value in attrs.items():\n            for command in commands:\n                setattr(command, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_default_args(self, default_args):\n        for name, args in default_args.items():\n            command = self[name]\n            command.default_args = default_args.get(command.name) or {}", "response": "Set the default args for commands in collection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract data required to classify entity.", "response": "def extract_traits(self, entity):\n        \"\"\"\n        Extract data required to classify entity.\n\n        :param object entity:\n        :return: namedtuple consisting of characteristic traits and match flag\n        :rtype: matchbox.box.Trait\n        \"\"\"\n        traits = getattr(entity, self._characteristic)\n        if traits is not None and isinstance(traits, Hashable):\n            traits = [traits]\n        return Trait(\n            traits,\n            getattr(entity, self._characteristic + '_match', True)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd entity to index.", "response": "def add(self, entity):\n        \"\"\"\n        Add entity to index.\n\n        :param object entity: single object to add to box's index\n        \"\"\"\n        characteristic = self.extract_traits(entity)\n        if not characteristic.traits:\n            return\n\n        if characteristic.is_matching:\n            self.add_match(entity, *characteristic.traits)\n        else:\n            self.add_mismatch(entity, *characteristic.traits)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, entity):\n        empty_traits = set()\n        self.mismatch_unknown.discard(entity)\n        for trait, entities in self.index.items():\n            entities.discard(entity)\n            if not entities:\n                empty_traits.add(trait)\n\n        for empty_trait in empty_traits:\n            del self.index[empty_trait]", "response": "Remove entity from the MatchBox."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_host_certificate(host, port=443):\n  ip_addr = socket.gethostbyname(host)\n  sock = socket.socket()\n  context = SSL.Context(SSL.TLSv1_METHOD)\n  context.set_options(SSL.OP_NO_SSLv2)\n  context.load_verify_locations(certifi.where(), None)\n  ssl_sock = SSL.Connection(context, sock)\n  ssl_sock.connect((ip_addr, port))\n  ssl_sock.do_handshake()\n  return ssl_sock.get_peer_certificate()", "response": "Get a host s X.509 certificate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating how much similar values of dictionaries are", "response": "def how_similar_dicts(dict1, dict2):\n    \"\"\"Calculates similarity\n\n    :param dict1: Dictionary\n    :param dict2: Dictionary\n    :return: measure of how much similar values of dictionaries are\n    \"\"\"\n    values = []\n    for k in dict1:  # iterate keys\n        if k in dict2 and dict1[k] and dict2[k]:\n            values.append(\n                how_similar_are(str(dict1[k]), str(dict2[k]))\n            )\n    return np.mean(values)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_inner_keys(dictionary):\n\n    keys = []\n\n    for key in dictionary.keys():\n        inner_keys = dictionary[key].keys()\n        keys += [\n            key + \" \" + inner_key  # concatenate\n            for inner_key in inner_keys\n        ]\n\n    return keys", "response": "Gets 2nd - level dictionary keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget 2nd - level data into 1st - level dictionary", "response": "def get_inner_data(dictionary):\n    \"\"\"Gets 2nd-level data into 1st-level dictionary\n\n    :param dictionary: dict\n    :return: with 2nd-level data\n    \"\"\"\n\n    out = {}\n\n    for key in dictionary.keys():\n        inner_keys = dictionary[key].keys()\n        for inner_key in inner_keys:\n            new_key = key + \" \" + inner_key  # concatenate\n            out[new_key] = dictionary[key][inner_key]\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse another instance provided as argument.", "response": "def do_use(self, args):\n        \"\"\"Use another instance, provided as argument.\"\"\"\n        self.instance = args\n        self.prompt = self.instance + '> '\n\n        archive = self._client.get_archive(self.instance)\n        self.streams = [s.name for s in archive.list_streams()]\n        self.tables = [t.name for t in archive.list_tables()]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nedit a command with the editor.", "response": "def do_edit(self, args):\n        \"\"\"Edit a command with $EDITOR.\"\"\"\n        if 'EDITOR' not in os.environ:\n            print('*** $EDITOR not set')\n        else:\n            path = os.path.join(utils.CONFIG_DIR, 'sql')\n            cmd = os.environ['EDITOR']\n            try:\n                os.system(cmd + ' ' + path)\n                if os.path.exists(path):\n                    with open(path, 'r') as f:\n                        sql = f.read()\n                        if sql:\n                            self.default(sql)\n            finally:\n                if os.path.exists(path):\n                    os.remove(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the text on the accept button to reflect if the name of the data file will result in opening an existing file or creating a new one", "response": "def update_label(self):\n        \"\"\"Updates the text on the accept button, to reflect if the \n        name of the data file will result in opening an existing file,\n        or creating a new one\"\"\"\n        current_file = str(self.selectedFiles()[0])\n        if not '.' in current_file.split(os.path.sep)[-1]:\n            # add hdf5 extention if none given\n            current_file += '.hdf5'\n        if os.path.isfile(current_file):\n            self.setLabelText(QtGui.QFileDialog.Accept, 'Reload')\n        elif os.path.isdir(current_file):\n            self.setLabelText(QtGui.QFileDialog.Accept, 'Open')\n        else:\n            self.setLabelText(QtGui.QFileDialog.Accept, 'Create')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getfile(self):\n        current_file = str(self.selectedFiles()[0])\n        if os.path.isfile(current_file):\n            print 'current_file', current_file\n            if current_file.endswith('.raw') or current_file.endswith('.pst'):\n                fmode = 'r'\n            else:\n                fmode = 'a'\n        else:\n            if not current_file.endswith('.hdf5') and not current_file.endswith('.h5'):\n                current_file += '.hdf5'\n            fmode = 'w-'\n        return current_file, fmode", "response": "Gets the full file path of the entered file containing the data file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting absolute path for path.", "response": "def abs_path(path, format_kwargs={}, relative_to=None, keep_slash=False):\n    \"\"\"Get abs. path for ``path``.\n\n    ``path`` may be a relative or absolute file system path or an asset\n    path. If ``path`` is already an abs. path, it will be returned as\n    is. Otherwise, it will be converted into a normalized abs. path.\n\n    If ``relative_to`` is passed *and* ``path`` is not absolute, the\n    path will be joined to the specified prefix before it's made\n    absolute.\n\n    If ``path`` ends with a slash, it will be stripped unless\n    ``keep_slash`` is set (for use with ``rsync``, for example).\n\n    >>> file_path = os.path.normpath(__file__)\n    >>> dir_name = os.path.dirname(file_path)\n    >>> file_name = os.path.basename(file_path)\n    >>> os.chdir(dir_name)\n    >>>\n    >>> abs_path(file_name) == file_path\n    True\n    >>> abs_path('runcommands.util:') == dir_name\n    True\n    >>> abs_path('runcommands.util:path.py') == file_path\n    True\n    >>> abs_path('/{xyz}', format_kwargs={'xyz': 'abc'})\n    '/abc'\n    >>> abs_path('banana', relative_to='/usr')\n    '/usr/banana'\n    >>> abs_path('/usr/banana/')\n    '/usr/banana'\n    >>> abs_path('banana/', relative_to='/usr', keep_slash=True)\n    '/usr/banana/'\n    >>> abs_path('runcommands.util:banana/', keep_slash=True) == (dir_name + '/banana/')\n    True\n\n    \"\"\"\n    if format_kwargs:\n        path = path.format_map(format_kwargs)\n\n    has_slash = path.endswith(os.sep)\n\n    if os.path.isabs(path):\n        path = os.path.normpath(path)\n    elif ':' in path:\n        path = asset_path(path, keep_slash=False)\n    else:\n        path = os.path.expanduser(path)\n        if relative_to:\n            path = os.path.join(relative_to, path)\n        path = os.path.abspath(path)\n        path = os.path.normpath(path)\n\n    if has_slash and keep_slash:\n        path = '{path}{slash}'.format(path=path, slash=os.sep)\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef asset_path(path, format_kwargs={}, keep_slash=False):\n    if format_kwargs:\n        path = path.format_map(format_kwargs)\n\n    has_slash = path.endswith(os.sep)\n\n    if ':' in path:\n        package_name, *rel_path = path.split(':', 1)\n    else:\n        package_name, rel_path = path, ()\n\n    try:\n        package = importlib.import_module(package_name)\n    except ImportError:\n        raise ValueError(\n            'Could not get asset path for {path}; could not import package: {package_name}'\n            .format_map(locals()))\n\n    if not hasattr(package, '__file__'):\n        raise ValueError(\"Can't compute path relative to namespace package\")\n\n    package_path = os.path.dirname(package.__file__)\n    path = os.path.join(package_path, *rel_path)\n    path = os.path.normpath(path)\n\n    if has_slash and keep_slash:\n        path = '{path}{slash}'.format(path=path, slash=os.sep)\n\n    return path", "response": "Get absolute path to asset in package."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef paths_to_str(paths, format_kwargs={}, delimiter=os.pathsep, asset_paths=False,\n                 check_paths=False):\n    \"\"\"Convert ``paths`` to a single string.\n\n    Args:\n        paths (str|list): A string like \"/a/path:/another/path\" or\n            a list of paths; may include absolute paths and/or asset\n            paths; paths that are relative will be left relative\n        format_kwargs (dict): Will be injected into each path\n        delimiter (str): The string used to separate paths\n        asset_paths (bool): Whether paths that look like asset paths\n            will be converted to absolute paths\n        check_paths (bool): Whether paths should be checked to ensure\n            they exist\n\n    \"\"\"\n    if not paths:\n        return ''\n    if isinstance(paths, str):\n        paths = paths.split(delimiter)\n    processed_paths = []\n    for path in paths:\n        original = path\n        path = path.format_map(format_kwargs)\n        if not os.path.isabs(path):\n            if asset_paths and ':' in path:\n                try:\n                    path = asset_path(path)\n                except ValueError:\n                    path = None\n        if path is not None and os.path.isdir(path):\n            processed_paths.append(path)\n        elif check_paths:\n            f = locals()\n            printer.warning('Path does not exist: {path} (from {original})'.format_map(f))\n    return delimiter.join(processed_paths)", "response": "Convert a list of paths into a single string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index():\n    # Reset current index values when the page is refreshed\n    for k, v in current_index.items():\n        current_index[k] = 0\n\n    logging.info(\"Dashboard refreshed\")\n\n    # render the template (below) that will use JavaScript to read the stream\n    return render_template(\"crystal_dashboard.html\")", "response": "Renders the dashboard when the server is initially run."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update():\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == 'POST':\n        # Get figure stats\n        selected_run = request.form['selected_run']\n        variable_names = utils.get_variables(selected_run).items()\n\n        if len(current_index) < 1:\n            for _, v_n in variable_names:\n                current_index[v_n] = 0\n\n        logging.info(\"Current index: {}\".format(current_index))\n        data = utils.get_variable_update_dicts(current_index, variable_names, selected_run)\n\n        return jsonify(data)", "response": "This function updates the current index of the current variable in the database and returns all the newly added values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a dictionary of projects that are available on the database. Usage description: This function is usually called to get and display the list of projects available in the database. :return: JSON, {<int_keys>: <project_name>}", "response": "def get_projects():\n    \"\"\"\n    Send a dictionary of projects that are available on the database.\n\n    Usage description:\n    This function is usually called to get and display the list of projects available in the database.\n\n    :return: JSON, {<int_keys>: <project_name>}\n    \"\"\"\n    assert request.method == \"GET\", \"GET request expected received {}\".format(request.method)\n    try:\n        if request.method == 'GET':\n            projects = utils.get_projects()\n\n            return jsonify(projects)\n    except Exception as e:\n        logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a dictionary of runs associated with the selected project. Usage description: This function is usually called to get and display the list of runs associated with a selected project available in the database. :return: JSON, {<int_keys>: <run_name>}", "response": "def get_runs():\n    \"\"\"\n    Send a dictionary of runs associated with the selected project.\n\n    Usage description:\n    This function is usually called to get and display the list of runs associated with a selected project available\n    in the database.\n\n    :return: JSON, {<int_keys>: <run_name>}\n    \"\"\"\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == \"POST\":\n        try:\n            selected_project = request.form[\"selected_project\"]\n            runs = utils.get_runs(selected_project)\n\n            return jsonify(runs)\n        except Exception as e:\n            logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a dictionary of variables associated with the selected run. Usage description: This function is usually called to get and display the list of runs associated with a selected project available in the database for the user to view. :return: JSON, {<int_keys>: <run_name>}", "response": "def get_variables():\n    \"\"\"\n    Send a dictionary of variables associated with the selected run.\n\n    Usage description:\n    This function is usually called to get and display the list of runs associated with a selected project available\n    in the database for the user to view.\n\n    :return: JSON, {<int_keys>: <run_name>}\n    \"\"\"\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == \"POST\":\n        try:\n            selected_run = request.form[\"selected_run\"]\n            variables = utils.get_variables(selected_run)\n\n            # Reset current_index when you select a new run\n            variable_names = variables.items()\n            global current_index\n            current_index = {}\n            if len(current_index) < 1:\n                for _, v_n in variable_names:\n                    current_index[\"{}\".format(v_n)] = 0\n\n            return jsonify(variables)\n        except Exception as e:\n            logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dialog box that allows the user to download a graph s data as a CSV file.", "response": "def get_graph_csv():\n    \"\"\"\n    Allows the user to download a graph's data as a CSV file.\n    :return: show a dialog box that allows the user to download the CSV file.\n    \"\"\"\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == \"POST\":\n        try:\n            selected_variable_table = request.form[\"selected_variable_table\"]\n            filename = utils.generate_graph_csv(selected_variable_table)\n            return send_file(filename, as_attachment=True, attachment_filename='{}.csv'.format(selected_variable_table))\n        except Exception as e:\n            logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_run():\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == \"POST\":\n        try:\n            selections = json.loads(request.form[\"selections\"])\n            utils.drop_run(selections[\"project\"], selections[\"run\"])\n            return jsonify({\"response\": \"deleted {}\".format(selections[\"run\"])})\n        except Exception as e:\n            logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})", "response": "Delete the selected run from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_project():\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == \"POST\":\n        try:\n            selections = json.loads(request.form[\"selections\"])\n            utils.drop_project(selections[\"project\"])\n            return jsonify({\"response\": \"deleted {}\".format(selections[\"project\"])})\n        except Exception as e:\n            logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})", "response": "Delete the selected run from the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef install_completion(\n        shell: arg(choices=('bash', 'fish'), help='Shell to install completion for'),\n        to: arg(help='~/.bashrc.d/runcommands.rc or ~/.config/fish/runcommands.fish') = None,\n        overwrite: 'Overwrite if exists' = False):\n    \"\"\"Install command line completion script.\n\n    Currently, bash and fish are supported. The corresponding script\n    will be copied to an appropriate directory. If the script already\n    exists at that location, it will be overwritten by default.\n\n    \"\"\"\n    if shell == 'bash':\n        source = 'runcommands:completion/bash/runcommands.rc'\n        to = to or '~/.bashrc.d'\n    elif shell == 'fish':\n        source = 'runcommands:completion/fish/runcommands.fish'\n        to = to or '~/.config/fish/runcommands.fish'\n\n    source = asset_path(source)\n    destination = os.path.expanduser(to)\n\n    if os.path.isdir(destination):\n        destination = os.path.join(destination, os.path.basename(source))\n\n    printer.info('Installing', shell, 'completion script to:\\n    ', destination)\n\n    if os.path.exists(destination):\n        if overwrite:\n            printer.info('Overwriting:\\n    {destination}'.format_map(locals()))\n        else:\n            message = 'File exists. Overwrite?'.format_map(locals())\n            overwrite = confirm(message, abort_on_unconfirmed=True)\n\n    copy_file(source, destination)\n    printer.info('Installed; remember to:\\n    source {destination}'.format_map(locals()))", "response": "Install command line completion script."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans up the cache.", "response": "def clean(verbose=False):\n    \"\"\"Clean up.\n\n    Removes:\n\n        - ./build/\n        - ./dist/\n        - **/__pycache__\n        - **/*.py[co]\n\n    Skips hidden directories.\n\n    \"\"\"\n    def rm(name):\n        if os.path.isfile(name):\n            os.remove(name)\n            if verbose:\n                printer.info('Removed file:', name)\n        else:\n            if verbose:\n                printer.info('File not present:', name)\n\n    def rmdir(name):\n        if os.path.isdir(name):\n            shutil.rmtree(name)\n            if verbose:\n                printer.info('Removed directory:', name)\n        else:\n            if verbose:\n                printer.info('Directory not present:', name)\n\n    root = os.getcwd()\n\n    rmdir('build')\n    rmdir('dist')\n\n    for path, dirs, files in os.walk(root):\n        rel_path = os.path.relpath(path, root)\n\n        if rel_path == '.':\n            rel_path = ''\n\n        if rel_path.startswith('.'):\n            continue\n\n        for d in dirs:\n            if d == '__pycache__':\n                rmdir(os.path.join(rel_path, d))\n\n        for f in files:\n            if f.endswith('.pyc') or f.endswith('.pyo'):\n                rm(os.path.join(rel_path, f))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the final source code for all modules.", "response": "def make_source(self, groups, code_opts, gen_opts):\n        \"\"\"Build the final source code for all modules.\"\"\"\n        modules = self.make_modules(groups, code_opts)\n        var_decls = modules.var_decls\n        relocs = AttrsGetter(modules.relocs)\n        x86, x64 = relocs.get_attrs('x86', 'x64')\n        if code_opts.windll:\n            structs, x86_reloc, x64_reloc = make_windll(\n                modules.structs\n            )\n            x86 += x86_reloc\n            x64 += x64_reloc\n        else:\n            structs = ''.join(modules.structs)\n        c_relocs = reloc_both(relocs.strings + x86, x64)\n        data = var_decls.strip()\n        c_header = make_c_header(\n            gen_opts.filename, 'NOTICE',\n            modules.typedefs + structs + data\n        )\n        c_source = make_init(\n            modules.hashes + c_relocs + modules.libprocs,\n            callable(code_opts.hash_func)\n        )\n        return [c_header, c_source]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild shellcoding files for the modules.", "response": "def make_modules(self, groups, code_opts):\n        \"\"\"Build shellcoding files for the module.\"\"\"\n        modules = []\n        for raw_module, raw_funcs in groups:\n            module = raw_module[0].strip().strip(string.punctuation)\n            funcs = [func.strip() for func in raw_funcs]\n            args = [self.database.query_args(func, raw=True)\n                    for func in funcs]\n            if self.generic:\n                args = [arg if arg else ('VOID *', [])\n                        for arg in args]\n            else:\n                args = [arg for arg in args if arg]\n            if not args:\n                logging.info(_('%s not found.'), module)\n                continue\n            logging.debug(module)\n            module = ModuleSource(module, zip(funcs, args),\n                                  code_opts)\n            modules.append(module.c_source())\n        return AttrsGetter(modules)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the typedefs of the module.", "response": "def c_typedefs(self):\n        \"\"\"Get the typedefs of the module.\"\"\"\n        defs = []\n        attrs = self.opts.attrs + '\\n' if self.opts.attrs else ''\n        for name, args in self.funcs:\n            logging.debug('name: %s args: %s', name, args)\n            defs.append(\n                'typedef\\n{}\\n{}{}({});\\n'.format(\n                    args[0], attrs,\n                    self._c_type_name(name), make_c_args(args[2])\n                )\n            )\n        return defs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef c_struct(self):\n        member = '\\n'.join(self.c_member_funcs(True))\n        if self.opts.windll:\n            return 'struct {{\\n{}{} }} {};\\n'.format(\n                self._c_dll_base(), member, self.name\n            )\n        return 'typedef\\nstruct {2} {{\\n{0}\\n{1}}}\\n{3};\\n'.format(\n            self._c_dll_base(), member, *self._c_struct_names()\n        )", "response": "Get the struct of the module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the hashes of the module including functions and DLLs.", "response": "def c_hashes(self):\n        \"\"\"Get the hashes of the module including functions and DLLs.\n        \"\"\"\n        if callable(self.opts.hash_func):\n            hashes = [\n                '# define {}{} {}\\n'.format(\n                    self.opts.prefix, name, self.opts.hash_func(name)\n                ) for name, dummy_args in self.funcs\n            ]\n        else:\n            hashes = [\n                make_c_str(self.opts.prefix + name, name)\n                for name, dummy_args in self.funcs\n            ]\n        if self.name != 'kernel32':\n            hashes = [\n                make_c_str(self.opts.prefix + self.name, self.name)\n            ] + hashes\n        return hashes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef c_self_relocs(self):\n        relocs = []\n        if not callable(self.opts.hash_func):\n            relocs = [\n                reloc_ptr(\n                    self.opts.prefix + name, self.opts.reloc_delta,\n                    'char *'\n                )\n                for name, dummy_args in self.funcs\n            ]\n        if self.name != 'kernel32':\n            relocs = [\n                reloc_ptr(\n                    self.opts.prefix + self.name,\n                    self.opts.reloc_delta, 'char *'\n                )\n            ] + relocs\n        return relocs", "response": "Build relocation for strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the needed variable definitions.", "response": "def c_var_decls(self):\n        \"\"\"Get the needed variable definitions.\"\"\"\n        if self.opts.no_structs:\n            mod_decl = 'HMODULE {} = NULL;\\n'.format(self.name)\n            return [mod_decl] + [\n                '{} *{} = NULL;\\n'.format(\n                    self._c_type_name(name), name\n                )\n                for name, dummy_args in self.funcs\n            ]\n        if self.opts.windll:\n            return ''\n        return [\n            '{} _{} = {{ 0 }};\\n'.format(\n                self._c_struct_names()[1], self.name\n            )\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef c_module_relocs(self):\n        if self.opts.no_structs or self.opts.windll:\n            return '', ''\n        x86 = reloc_var(\n            self.name, self._c_struct_names()[1],\n            self.opts.reloc_delta,\n            self._c_uses_pointer()\n        )\n        x64 = '{0} *{1} = &_{1};\\n'.format(\n            self._c_struct_names()[1], self.name\n        ) if self._c_uses_pointer() else ''\n        return x86, x64", "response": "Build the relocation for the module variable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef c_loadlib(self):\n        name = self._c_base_var()\n        kernel32 = 'windll->kernel32.'\n        if self.name == 'kernel32':\n            loadlib = '{} = get_kernel32_base();\\n'.format(\n                'kernel32' if self.opts.no_structs\n                else kernel32 + self.opts.base\n            )\n        else:\n            loadlib = '{} = {}LoadLibraryA({}{});\\n'.format(\n                name,\n                '' if self.opts.no_structs else kernel32,\n                self.opts.prefix, self.name\n            )\n        return loadlib + self._c_null_check(name)", "response": "Get the loadlib of the module."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the getprocs of the module.", "response": "def c_getprocs(self):\n        \"\"\"Get the getprocs of the module.\"\"\"\n        getprocs = []\n        for name, dummy_args in self.funcs:\n            if name == 'GetProcAddress':\n                if callable(self.opts.hash_func):\n                    continue\n                getter = 'get_proc_by_string'\n            elif self.opts.no_structs:\n                getter = 'GetProcAddress'\n            else:\n                getter = 'windll->kernel32.GetProcAddress'\n            if callable(self.opts.hash_func):\n                getter = 'get_proc_by_hash'\n            if self.opts.no_structs:\n                var = name\n            else:\n                var = 'windll->{}.{}'.format(self.name, name)\n            getproc = '{} = ({} *){}({}, {}{});\\n'.format(\n                var,\n                self._c_type_name(name),\n                getter,\n                self._c_base_var(),\n                self.opts.prefix, name\n            )\n            getprocs.append(getproc + self._c_null_check(var))\n        return getprocs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef c_member_funcs(self, for_struct=False):\n        decls = [\n            '{} *{};'.format(self._c_type_name(name), name)\n            for name, dummy_args in self.funcs\n        ]\n        if for_struct:\n            return decls\n        return [self._c_mod_decl()] + decls", "response": "Get the decls of the module."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the name of the module base variable.", "response": "def _c_base_var(self):\n        \"\"\"Return the name of the module base variable.\"\"\"\n        if self.opts.no_structs:\n            return self.name\n        return 'windll->{}.{}'.format(\n            self.name, self.opts.base\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bbduk_trim(forward_in, forward_out, reverse_in='NA', reverse_out='NA',\n               trimq=20, k=25, minlength=50, forcetrimleft=15, hdist=1, returncmd=False, **kwargs):\n    \"\"\"\n    Wrapper for using bbduk to quality trim reads. Contains arguments used in OLC Assembly Pipeline, but these can\n    be overwritten by using keyword parameters.\n    :param forward_in: Forward reads you want to quality trim.\n    :param returncmd: If set to true, function will return the cmd string passed to subprocess as a third value.\n    :param forward_out: Output forward reads.\n    :param reverse_in: Reverse input reads. Don't need to be specified if _R1/_R2 naming convention is used.\n    :param reverse_out: Reverse output reads. Don't need to be specified if _R1/_R2 convention is used.\n    :param kwargs: Other arguments to give to bbduk in parameter=argument format. See bbduk documentation for full list.\n    :return: out and err: stdout string and stderr string from running bbduk.\n    \"\"\"\n    options = kwargs_to_string(kwargs)\n    cmd = 'which bbduk.sh'\n    try:\n        subprocess.check_output(cmd.split()).decode('utf-8')\n    except subprocess.CalledProcessError:\n        print('ERROR: Could not find bbduk. Plase check that the bbtools package is installed and on your $PATH.\\n\\n')\n        raise FileNotFoundError\n    if os.path.isfile(forward_in.replace('_R1', '_R2')) and reverse_in == 'NA' and '_R1' in forward_in:\n        reverse_in = forward_in.replace('_R1', '_R2')\n        if reverse_out == 'NA':\n            if '_R1' in forward_out:\n                reverse_out = forward_out.replace('_R1', '_R2')\n            else:\n                raise ValueError('If you do not specify reverse_out, forward_out must contain R1.\\n\\n')\n        cmd = 'bbduk.sh in1={f_in} in2={r_in} out1={f_out} out2={r_out} qtrim=w trimq={trimq} k={k} ' \\\n              'minlength={minlength} forcetrimleft={forcetrimleft} ref=adapters overwrite hdist={hdist} tpe tbo{optn}'\\\n            .format(f_in=forward_in,\n                    r_in=reverse_in,\n                    f_out=forward_out,\n                    r_out=reverse_out,\n                    trimq=trimq,\n                    k=k,\n                    minlength=minlength,\n                    forcetrimleft=forcetrimleft,\n                    hdist=hdist,\n                    optn=options)\n    elif reverse_in == 'NA' or reverse_in is None:\n        cmd = 'bbduk.sh in={f_in} out={f_out} qtrim=w trimq={trimq} k={k} minlength={minlength} ' \\\n              'forcetrimleft={forcetrimleft} ref=adapters overwrite hdist={hdist} tpe tbo{optn}'\\\n            .format(f_in=forward_in,\n                    f_out=forward_out,\n                    trimq=trimq,\n                    k=k,\n                    minlength=minlength,\n                    forcetrimleft=forcetrimleft,\n                    hdist=hdist,\n                    optn=options)\n    else:\n        if reverse_out == 'NA':\n            raise ValueError('Reverse output reads must be specified.')\n        cmd = 'bbduk.sh in1={f_in} in2={r_in} out1={f_out} out2={r_out} qtrim=w trimq={trimq} k={k} ' \\\n              'minlength={minlength} forcetrimleft={forcetrimleft} ref=adapters overwrite hdist={hdist} tpe tbo{optn}' \\\n            .format(f_in=forward_in,\n                    r_in=reverse_in,\n                    f_out=forward_out,\n                    r_out=reverse_out,\n                    trimq=trimq,\n                    k=k,\n                    minlength=minlength,\n                    forcetrimleft=forcetrimleft,\n                    hdist=hdist,\n                    optn=options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Wrapper for using bbduk to quality trim a set of reads."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bbduk_filter(reference, forward_in, forward_out, returncmd=False, reverse_in='NA', reverse_out='NA', **kwargs):\n    options = kwargs_to_string(kwargs)\n    if os.path.isfile(forward_in.replace('_R1', '_R2')) and reverse_in == 'NA' and '_R1' in forward_in:\n        reverse_in = forward_in.replace('_R1', '_R2')\n        if reverse_out == 'NA':\n            if '_R1' in forward_out:\n                reverse_out = forward_out.replace('_R1', '_R2')\n            else:\n                raise ValueError('If you do not specify reverse_out, forward_out must contain _R1.\\n\\n')\n        cmd = 'bbduk.sh in={} in2={} out={} out2={} ref={}{}'.format(forward_in, reverse_in,\n                                                                     forward_out, reverse_out,\n                                                                     reference, options)\n    elif reverse_in == 'NA':\n        cmd = 'bbduk.sh in={} out={} ref={}{}'.format(forward_in, forward_out, reference, options)\n    else:\n        if reverse_out == 'NA':\n            raise ValueError('Reverse output reads must be specified.')\n        cmd = 'bbduk.sh in={} in2={} out={} out2={} ref={}{}'.format(forward_in, reverse_in,\n                                                                     forward_out, reverse_out,\n                                                                     reference, options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Uses bbduk to filter out reads that have kmers matching to a reference."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dedupe(input_file, output_file, returncmd=False, **kwargs):\n    options = kwargs_to_string(kwargs)\n    cmd = 'dedupe.sh in={} out={}{}'.format(input_file, output_file, options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Runs dedupe from bbtools package."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns seal from bbtools package.", "response": "def seal(reference, forward_in, output_file, reverse_in='NA', returncmd=False, **kwargs):\n    \"\"\"\n    Runs seal from the bbtools package.\n    :param reference: Reference file, in fasta format.\n    :param returncmd: If set to true, function will return the cmd string passed to subprocess as a third value.\n    :param forward_in: Forward reads, fastq format.\n    :param output_file: Output file to put rpkm statistics into.\n    :param reverse_in: Reverse reads. Not necessary to specify if in same folder and follow _R1/_R2 convention.\n    :param kwargs: Arguments to give to seal in parameter=argument format. See seal documentation for full list.\n    :return: out and err: stdout string and stderr string from running seal.\n    \"\"\"\n    options = kwargs_to_string(kwargs)\n    if os.path.isfile(forward_in.replace('_R1', '_R2')) and reverse_in == 'NA' and '_R1' in forward_in:\n        reverse_in = forward_in.replace('_R1', '_R2')\n        cmd = 'seal.sh ref={} in={} in2={} rpkm={} nodisk{}'.format(reference, forward_in, reverse_in, output_file, options)\n    elif reverse_in == 'NA':\n        cmd = 'seal.sh ref={} in={} rpkm={} nodisk{}'.format(reference, forward_in, output_file, options)\n    else:\n        cmd = 'seal.sh ref={} in={} in2={} rpkm={} nodisk{}'.format(reference, forward_in, reverse_in, output_file, options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef randomreads(reference, length, reads, out_fastq, paired=False, returncmd=False, **kwargs):\n    options = kwargs_to_string(kwargs)\n    # If the paired option is selected, set the name of the reverse reads to be the same as the forward reads\n    # but replace _R1 with _R2\n    if paired:\n        out_fastq2 = out_fastq.replace('_R1', '_R2')\n        # Create the call to randomreads - use paired=t\n        cmd = 'randomreads.sh ref={ref} out={out} out2={out2} length={length} reads={reads} paired=t{options}'\\\n            .format(ref=reference,\n                    out=out_fastq,\n                    out2=out_fastq2,\n                    length=length,\n                    reads=reads,\n                    options=options)\n    else:\n        cmd = 'randomreads.sh ref={ref} out={out} length={length} reads={reads}{options}'\\\n            .format(ref=reference,\n                    out=out_fastq,\n                    length=length,\n                    reads=reads,\n                    options=options)\n    if not os.path.isfile(out_fastq):\n        out, err = accessoryfunctions.run_subprocess(cmd)\n    else:\n        out = str()\n        err = str()\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Wrapper for bbmap. randomreads."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates precursor mz based on exact mass and precursor type", "response": "def get_precursor_mz(exact_mass, precursor_type):\n    \"\"\" Calculate precursor mz based on exact mass and precursor type\n\n    Args:\n        exact_mass (float): exact mass of compound of interest\n        precursor_type (str): Precursor type (currently only works with '[M-H]-', '[M+H]+' and '[M+H-H2O]+'\n\n    Return:\n          neutral mass of compound\n    \"\"\"\n\n    # these are just taken from what was present in the massbank .msp file for those missing the exact mass\n    d = {'[M-H]-': -1.007276,\n         '[M+H]+': 1.007276,\n         '[M+H-H2O]+': 1.007276 - ((1.007276 * 2) + 15.9949)\n         }\n\n    try:\n\n        return exact_mass + d[precursor_type]\n    except KeyError as e:\n        print(e)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef line_count(fn):\n\n    with open(fn) as f:\n        for i, l in enumerate(f):\n            pass\n    return i + 1", "response": "Get the line count of a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kmc(forward_in, database_name, min_occurrences=1, reverse_in='NA', k=31, cleanup=True,\n        returncmd=False, tmpdir='tmp', **kwargs):\n    \"\"\"\n    Runs kmc to count kmers.\n    :param forward_in: Forward input reads. Assumed to be fastq.\n    :param database_name: Name for output kmc database.\n    :param min_occurrences: Minimum number of times kmer must be seen to be included in database.\n    :param reverse_in: Reverse input reads. Automatically found.\n    :param k: Kmer size. Default 31.\n    :param cleanup: If true, deletes tmpdir that kmc needs.\n    :param tmpdir: Temporary directory to store intermediary kmc files. Default tmp.\n    :param returncmd: If true, will return the command used to call KMC as well as out and err.\n    :param kwargs: Other kmc arguments in parameter='argument' format.\n    :return: Stdout and stderr from kmc.\n    \"\"\"\n    # Create the tmpdir kmc needs if it isn't already present.\n    if not os.path.isdir(tmpdir):\n        os.makedirs(tmpdir)\n    options = kwargs_to_string(kwargs)\n    if os.path.isfile(forward_in.replace('_R1', '_R2')) and reverse_in == 'NA' and '_R1' in forward_in:\n        reverse_in = forward_in.replace('_R1', '_R2')\n        filelist = os.path.join(tmpdir, 'filelist.txt')\n        with open(filelist, 'w') as f:\n            f.write(forward_in + '\\n')\n            f.write(reverse_in + '\\n')\n        cmd = 'kmc -k{} -ci{} {} @{} {} {}'.format(k, min_occurrences, options, filelist, database_name, tmpdir)\n    elif reverse_in == 'NA':\n        cmd = 'kmc -k{} -ci{} {} {} {} {}'.format(k, min_occurrences, options, forward_in, database_name, tmpdir)\n    else:\n        filelist = os.path.join(tmpdir, 'filelist.txt')\n        with open(filelist, 'w') as f:\n            f.write(forward_in + '\\n')\n            f.write(reverse_in + '\\n')\n        cmd = 'kmc -k{} -ci{} {} @{} {} {}'.format(k, min_occurrences, options, filelist, database_name, tmpdir)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if cleanup:\n        shutil.rmtree(tmpdir)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Runs kmc to count kmers in a single input fastq file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds kmers that are present in 2 databases.", "response": "def intersect(database_1, database_2, results, returncmd=False):\n    \"\"\"\n    Finds kmers that are present in 2 databases.\n    :param database_1: First database generated by kmc.\n    :param database_2: Second database generated by kmc.\n    :param results: Result database, containing reads in both database 1 and 2.\n    :param returncmd: If true, will return the command used to call KMC as well as out and err.\n    :return: Stdout and stderr from kmc.\n    \"\"\"\n    cmd = 'kmc_tools intersect {} {} {}'.format(database_1, database_2, results)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubtracting database 2 from database 1.", "response": "def subtract(database_1, database_2, results, exclude_below=1, returncmd=False):\n    \"\"\"\n    Subtracts database 2 from database 1. Results can then be dumped to view what kmers are present only in database 1.\n    :param database_1: First database generated by kmc.\n    :param database_2: Second database generated by kmc.\n    :param results: Result database, containing reads in both database 1 but not in 2..\n    :param exclude_below: Don't subtract kmers from database 1 that have less than this many occurrences\n    in database 2.\n    :param returncmd: If true, will return the command used to call KMC as well as out and err.\n    :return: Stdout and stderr from kmc.\n    \"\"\"\n    cmd = 'kmc_tools kmers_subtract {} {} -ci{} {}'.format(database_1, database_2, str(exclude_below), results)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump(database, output, min_occurences=1, max_occurences=250, returncmd=False):\n    cmd = 'kmc_tools dump -ci{} -cx{} {} {}'.format(min_occurences, max_occurences, database, output)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Dumps the output of a kmc database into a tab - delimited format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwait for sec seconds.", "response": "async def wait(sec=5, **kwargs):\n    \"\"\"\n    Does nothing for *sec* seconds and then prints out a message.\n\n    .. note:: This function **is not** blocking.\n\n    .. seealso:: :func:`blocking_wait`\n\n    :param sec: Time to wait, in seconds.\n    :param kwargs: Additional parameters.\n    \"\"\"\n    await asyncio.sleep(sec)\n    print(\"{0}: Done waiting for {1} sec.\".format(kwargs['rulename'], sec))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main(args):\n\n    if args:\n        \n        if not args.outputRepository:\n            HOME_DIR = os.path.expanduser('~')\n            # Utility's base directory\n            BASE_DIR = os.path.abspath(os.path.dirname(__file__))\n            DOWNLOAD_DIR = HOME_DIR + '/landsat'\n            ZIP_DIR = DOWNLOAD_DIR + '/zip'\n        else:\n            ZIP_DIR = args.outputRepository\n        \n        if args.subs == 'search':\n\n            try:\n                if args.start:\n                    args.start = reformat_date(parse(args.start))\n\n                if args.end:\n                    args.end = reformat_date(parse(args.end))\n            except TypeError:\n                exit(\"You date format is incorrect. Please try again!\", 1)\n\n            s = Search()\n            \n            clipper = Clipper()\n            if args.search_subs == 'shapefile':\n                clipper.shapefile(args.path)\n            elif args.search_subs == 'query':\n                clipper.query(args.name)\n            \n            result = s.search(args.limit,args.start,args.end,clipper)\n            try:\n                if result['status'] == 'SUCCESS':\n                    if result['total'] > 200:\n                        exit('Too many results. Please narrow your search or limit your query with -l options')\n                    else:\n                        if args.outputRepository:\n                            with open(ZIP_DIR+'/result.geojson', 'w') as outfile:\n                                json.dump(result['results'], outfile)\n                            print (\"The geojsonFIle have been created  here: %s\" %\n                                 ZIP_DIR)\n                        else:\n                            print (\"the IDs which matched with request are : \")\n                            for i in result['ID']:\n                                print (i)\n                    \n                    if args.download:\n                        gs = GsHelper(ZIP_DIR)\n                        if (args.password) and (args.user):\n                            print('Starting the download:')\n                            for item in result['downloads']:\n                                login=args.user\n                                mdp=args.password\n                                gs.single_download(login,mdp,item['download'],item['id'],ZIP_DIR)\n                                print (\"%s have been downloaded ... continuing downloading\" % item['id'])\n                            print(\"%s images were downloaded\"\n                                  % result['total'])\n                            exit(\"The downloaded images are located here: %s\" %\n                                 ZIP_DIR)\n                        else:\n                             exit(\"Please give a loggin and a password for theia downloading\")\n                    else:\n                        exit(\"\")\n                elif result['status'] == 'error':\n                    exit(result['message'])\n            except KeyError:\n                exit('Too Many API queries. You can only query DevSeed\\'s '\n                     'API 5 times per minute', 1)\n\n        elif args.subs == 'download':\n            gs = GsHelper(ZIP_DIR)\n            print('Starting the download:')\n            if (args.password) and (args.user):\n                for scene in args.scenes:\n                    login=args.user\n                    mdp=args.password\n                    download='http://spirit.cnes.fr/resto/Landsat/'+scene+'/$download'\n                    testD=gs.checkifDownloadExist(login,mdp,download,scene)\n                    if testD:\n                        gs.single_download(login,mdp,download,scene,ZIP_DIR)\n                    else:\n                        exit(\"SceneID has not been founded or wrong User/Password given!\")\n                exit(\"The downloaded images are located here: %s\" % gs.zip_dir)\n            else:\n                exit(\"Please give a loggin and a password for theia downloading\")", "response": "This function is the main function of the program. It will launch the program\notope."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the voltage amplitude for this stimulus using the given calibration intensity and the given voltage", "response": "def amplitude(self, caldb, calv, atten=0):\n        \"\"\"Calculates the voltage amplitude for this stimulus, using\n        internal intensity value and the given reference intensity & voltage\n\n        :param caldb: calibration intensity in dbSPL\n        :type caldb: float\n        :param calv: calibration voltage that was used to record the intensity provided\n        :type calv: float\n        \"\"\"\n        amp = (10 ** (float(self._intensity+atten-caldb)/20)*calv)\n        return amp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks this component for invalidating conditions", "response": "def verify(self, **kwargs):\n        \"\"\"Checks this component for invalidating conditions\n\n        :returns: str -- message if error, 0 otherwise\n        \"\"\"\n        if 'duration' in kwargs:\n            if kwargs['duration'] < self._duration:\n                return \"Window size must equal or exceed stimulus length\"\n        if self._risefall > self._duration:\n            return \"Rise and fall times exceed component duration\"\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stateDict(self):\n        state = {\n                'duration' : self._duration,\n                'intensity' : self._intensity,\n                'risefall' : self._risefall,\n                'stim_type' : self.name\n                }\n        return state", "response": "Saves internal values to be loaded later\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loadState(self, state):\n        self._duration = state['duration']\n        self._intensity = state['intensity']\n        self._risefall = state['risefall']", "response": "Loads previously saved values to this component."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitiates an OAuth handshake with MediaWiki.", "response": "def initiate(self, callback=None):\n        \"\"\"\n        Initiate an OAuth handshake with MediaWiki.\n\n        :Parameters:\n            callback : `str`\n                Callback URL. Defaults to 'oob'.\n\n        :Returns:\n            A `tuple` of two values:\n\n            * a MediaWiki URL to direct the user to\n            * a :class:`~mwoauth.RequestToken` representing an access request\n\n\n        \"\"\"\n        return initiate(self.mw_uri, self.consumer_token,\n                        callback=callback or self.callback,\n                        user_agent=self.user_agent)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncomplete an OAuth handshake with MediaWiki by exchanging an anonymization token.", "response": "def complete(self, request_token, response_qs):\n        \"\"\"\n        Complete an OAuth handshake with MediaWiki by exchanging an\n\n        :Parameters:\n            request_token : `RequestToken`\n                A temporary token representing the user.  Returned by\n                `initiate()`.\n            response_qs : `bytes`\n                The query string of the URL that MediaWiki forwards the user\n                back after authorization.\n\n        :Returns:\n            An :class:`~mwoauth.AccessToken` containing an authorized\n            key/secret pair that can be stored and used by you.\n        \"\"\"\n        return complete(\n            self.mw_uri, self.consumer_token, request_token, response_qs,\n            user_agent=self.user_agent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef identify(self, access_token, leeway=10.0):\n        return identify(self.mw_uri, self.consumer_token, access_token,\n                        leeway=leeway, user_agent=self.user_agent)", "response": "Gather identifying information about a user via an authorized token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the meta data for a uniform or attribute", "response": "def _load_variable(func, program_id, index):\n    \"\"\"Loads the meta data for a uniform or attribute\"\"\"\n    n = 64  # max name length TODO: read from card\n    bufsize = GLsizei(n)\n    length = pointer(GLsizei(0))\n    size = pointer(GLint(0))\n    type = pointer(GLenum(0))\n    uname = create_string_buffer(n)\n    func(program_id, index, bufsize, length, size, type, uname)\n    return size[0], type[0], uname.value.decode('utf8')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addWidget(self, widget, name):\n        self.exploreStimTypeCmbbx.addItem(name)\n        self.componentStack.addWidget(widget)\n        widget.valueChanged.connect(self.valueChanged.emit)", "response": "Add a component editor widget"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef saveTemplate(self):\n        savedict = {}\n        for comp_editor in self.widgets():\n            stim = comp_editor.component()\n            comp_editor.saveToObject()\n            savedict[stim.name] = stim.stateDict()\n        savedict['delay'] = self.delaySpnbx.value()\n        return savedict", "response": "Get a json structure of the current inputs and \n        to be able to load later"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_optional(self, string):\n        option_map = self.option_map\n        if string in option_map:\n            return string, option_map[string], None\n        if '=' in string:\n            name, value = string.split('=', 1)\n            if name in option_map:\n                return name, option_map[name], value\n        return None", "response": "Parse an optional string into name option and value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand grouped short options like - abc to - a - b - c.", "response": "def expand_short_options(self, argv):\n        \"\"\"Convert grouped short options like `-abc` to `-a, -b, -c`.\n\n        This is necessary because we set ``allow_abbrev=False`` on the\n        ``ArgumentParser`` in :prop:`self.arg_parser`. The argparse docs\n        say ``allow_abbrev`` applies only to long options, but it also\n        affects whether short options grouped behind a single dash will\n        be parsed into multiple short options.\n\n        \"\"\"\n        new_argv = []\n        for arg in argv:\n            result = self.parse_multi_short_option(arg)\n            new_argv.extend(result)\n        return new_argv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_arg(self, name):\n        name = self.normalize_name(name)\n        return self.args.get(name)", "response": "Find an arg by normalized arg name or parameter name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_parameter(self, name):\n        name = self.normalize_name(name)\n        arg = self.args.get(name)\n        return None if arg is None else arg.parameter", "response": "Find parameter by name or normalized arg name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef args(self):\n        params = self.parameters\n        args = OrderedDict()\n\n        # This will be overridden if the command explicitly defines an\n        # arg named help.\n        args['help'] = HelpArg(command=self)\n\n        normalize_name = self.normalize_name\n        get_arg_config = self.get_arg_config\n        get_short_option = self.get_short_option_for_arg\n        get_long_option = self.get_long_option_for_arg\n        get_inverse_option = self.get_inverse_option_for_arg\n\n        names = {normalize_name(name) for name in params}\n\n        used_short_options = set()\n        for param in params.values():\n            annotation = get_arg_config(param)\n            short_option = annotation.short_option\n            if short_option:\n                used_short_options.add(short_option)\n\n        for name, param in params.items():\n            name = normalize_name(name)\n\n            skip = (\n                name.startswith('_') or\n                param.kind is param.VAR_KEYWORD or\n                param.kind is param.KEYWORD_ONLY)\n            if skip:\n                continue\n\n            annotation = get_arg_config(param)\n            container = annotation.container\n            type = annotation.type\n            choices = annotation.choices\n            help = annotation.help\n            inverse_help = annotation.inverse_help\n            short_option = annotation.short_option\n            long_option = annotation.long_option\n            inverse_option = annotation.inverse_option\n            action = annotation.action\n            nargs = annotation.nargs\n\n            default = param.default\n\n            if default is not param.empty:\n                if not short_option:\n                    short_option = get_short_option(name, names, used_short_options)\n                    used_short_options.add(short_option)\n                if not long_option:\n                    long_option = get_long_option(name)\n                if not inverse_option:\n                    # NOTE: The DISABLE marker evaluates as True\n                    inverse_option = get_inverse_option(long_option)\n\n            args[name] = Arg(\n                command=self,\n                parameter=param,\n                name=name,\n                container=container,\n                type=type,\n                default=default,\n                choices=choices,\n                help=help,\n                inverse_help=inverse_help,\n                short_option=short_option,\n                long_option=long_option,\n                inverse_option=inverse_option,\n                action=action,\n                nargs=nargs,\n            )\n\n        option_map = OrderedDict()\n        for arg in args.values():\n            for option in arg.options:\n                option_map.setdefault(option, [])\n                option_map[option].append(arg)\n\n        for option, option_args in option_map.items():\n            if len(option_args) > 1:\n                names = ', '.join(a.parameter.name for a in option_args)\n                message = (\n                    'Option {option} of command {self.name} maps to multiple parameters: {names}')\n                message = message.format_map(locals())\n                raise CommandError(message)\n\n        return args", "response": "Create args from function parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps command - line options to args.", "response": "def option_map(self):\n        \"\"\"Map command-line options to args.\"\"\"\n        option_map = OrderedDict()\n        for arg in self.args.values():\n            for option in arg.options:\n                option_map[option] = arg\n        return option_map"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef objectprep(self):\n        # Create .fastq files if necessary. Otherwise create the metadata object\n        if self.bcltofastq:\n            if self.customsamplesheet:\n                assert os.path.isfile(self.customsamplesheet), 'Cannot find custom sample sheet as specified {}' \\\n                    .format(self.customsamplesheet)\n            # Create the FASTQ files\n            self.samples = fastqCreator.CreateFastq(self)\n            # Create a dictionary of the object\n            samples_dict = vars(self.samples)\n            # Extract the required information from the dictionary\n            self.index = samples_dict['index']\n            self.index_length = samples_dict['indexlength']\n            self.forward = samples_dict['forwardlength']\n            self.reverse = samples_dict['reverselength']\n            self.forwardlength = samples_dict['forward']\n            self.reverselength = samples_dict['reverse']\n            self.header = samples_dict['header']\n        else:\n            self.samples = createObject.ObjectCreation(self)", "response": "Create the FASTQ files for the object and set the attributes of the object appropriately."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecompress and concatenate. fastq files", "response": "def fileprep(self):\n        \"\"\"Decompress and concatenate .fastq files\"\"\"\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.prep, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.metadata:\n            # Set the name of the decompressed, combined .fastq file\n            sample.general.combined = os.path.join(sample.general.outputdirectory, '{sample_name}_combined.fastq'\n                                                   .format(sample_name=sample.name))\n            self.queue.put(sample)\n        self.queue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the hexadecimal string in to C - style string.", "response": "def bytes_to_c_string(data):\n    \"\"\"\n    Convert the hexadecimal string in to C-style string.\n    \"\"\"\n    rows = chunked_join(data, 20, 2, '\"\\n    \"', '', r'\\x' + X)\n    logging.debug(_('Returning rows: %s'), rows)\n    return '\"{}\";'.format(rows)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking a C array using the given bytes.", "response": "def bytes_to_c_array(data):\n    \"\"\"\n    Make a C array using the given string.\n    \"\"\"\n    chars = [\n        \"'{}'\".format(encode_escape(i))\n        for i in decode_escape(data)\n    ]\n    return ', '.join(chars) + ', 0'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a Converter object from the specified section of the specified binary stream.", "response": "def from_section(cls, stream, section_name='.pic'):\n        \"\"\"Construct a Converter object from the specified section\n        of the specified binary stream.\"\"\"\n        binary = Executable(stream)\n        section_data = binary.get_section_data(section_name)\n        return cls(section_data, binary.system)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_esc(self):\n        chunks = chunked(self.stream, 2)\n        return ''.join(r'\\x' + ''.join(pair) for pair in chunks)", "response": "Convert to escape string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the image card.", "response": "def build(self, title, text, img_url):\n        \"\"\"\n        :param title: Title of the card\n        :param text: Description of the card\n        :param img_url: Image of the card\n        \"\"\"\n        super(ImageCard, self).build()\n        self.title = Title(id=self.id + \"-title\", text=title, classname=\"card-title\", size=3, parent=self)\n\n        self.block = Panel(id=self.id + \"-block\", classname=\"card-block\", parent=self)\n\n        self.image = Image(id=self.id + \"-image\", img_url=img_url, classname=\"card-image-top img-fluid\", parent=self.block)\n        self.text = Paragraph(id=self.id + \"-text\", text=text, classname=\"card-text\", parent=self.block)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding percentage between two numbers", "response": "def get_percentage_relative_to(val, other):\n    \"\"\"Finds percentage between 2 numbers\n\n    :param val: number\n    :param other: number to compare to\n    :return: percentage of delta between first and second\n    \"\"\"\n    val = float(val)\n    other = float(other)\n    ratio = val / other - 1\n\n    return ratio * 100.0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_ui(self, ):\n        self.main_vbox = QtGui.QVBoxLayout(self)\n        self.import_all_references_cb = QtGui.QCheckBox(\"Import references\")\n        self.main_vbox.addWidget(self.import_all_references_cb)", "response": "Create all ui elements and layouts"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the cleanup actions for a releaes depending on the selected options.", "response": "def get_cleanups(self, ):\n        \"\"\"Get the cleanup actions for a releaes depending on the selected options\n\n        :returns: the cleanup actions\n        :rtype: :class:`jukeboxcore.action.ActionCollection`\n        :raises: None\n        \"\"\"\n        cleanups = []\n        open_unit = ActionUnit(name=\"Open\",\n                               description=\"Open the maya scene.\",\n                               actionfunc=open_scene)\n        cleanups.append(open_unit)\n        if self._option_widget.import_references():\n            import_unit = ActionUnit(name=\"Import references\",\n                                     description=\"Import all references in the scene.\",\n                                     actionfunc=import_all_references,\n                                     depsuccess=[open_unit])\n            cleanups.append(import_unit)\n        update_scenenode_unit = ActionUnit(name=\"Update Scene Node\",\n                                           description=\"Change the id from the jbscene node from work to releasefile.\",\n                                           actionfunc=update_scenenode,\n                                           depsuccess=[open_unit])\n        cleanups.append(update_scenenode_unit)\n        save_unit = ActionUnit(name=\"Save\",\n                               description=\"Save the scene.\",\n                               actionfunc=save_scene,\n                               depsuccess=[update_scenenode_unit])\n        cleanups.append(save_unit)\n        return ActionCollection(cleanups)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, ):\n        ra = SceneReleaseActions()\n        mayawin = maya_main_window()\n        self.rw = ReleaseWin(FILETYPES[\"mayamainscene\"], parent=mayawin)\n        self.rw.set_release_actions(ra)\n        pm = MayaPluginManager.get()\n        genesis =  pm.get_plugin(\"MayaGenesis\")\n        c = genesis.get_config()\n        try:\n            f = models.TaskFile.objects.get(pk=c['lastfile'])\n        except models.TaskFile.DoesNotExist:\n            pass\n        else:\n            if f.releasetype == 'work':\n                self.rw.browser.set_selection(f)\n        self.rw.show()", "response": "Start the configeditor\n\n        :returns: None\n        :rtype: None\n        :raises: None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopulating the forward and reverse dictionaries of the ePCR primers based on the primer file.", "response": "def epcr_primers(self, primerfile):\n        \"\"\"\n        Read in the primer file, and create a properly formatted output file that takes any degenerate bases\n        into account\n        \"\"\"\n        logging.info('Populating primer dictionaries')\n        for record in SeqIO.parse(primerfile, 'fasta'):\n            # from https://stackoverflow.com/a/27552377 - find any degenerate bases in the primer sequence, and\n            # create all possibilities as a list\n            degenerates = Seq.IUPAC.IUPACData.ambiguous_dna_values\n            primerlist = list(map(''.join, product(*map(degenerates.get, str(record.seq).upper()))))\n            # As the record.id is being updated in the loop below, set the name of the primer here so that will\n            # be able to be recalled when setting the new record.ids\n            primername = record.id\n            # Iterate through all the possible primers created from any degenerate bases\n            for primer in primerlist:\n                # Split the base name of the target from the direction\n                # e.g. vtx1a-F1 is split in vtx1a and F1\n                basename, direction = primername.split('-')\n                # Populate the dictionaries of forward and reverse primers based on the direction determined above\n                if direction.startswith('F'):\n                    # Attempt to add the current primer sequence to the dictionary\n                    try:\n                        self.forward_dict[basename].append(primer)\n                    # On a key error, initialise the list of primers\n                    except KeyError:\n                        self.forward_dict[basename] = list()\n                        self.forward_dict[basename].append(primer)\n                else:\n                    try:\n                        self.reverse_dict[basename].append(primer)\n                    except KeyError:\n                        self.reverse_dict[basename] = list()\n                        self.reverse_dict[basename].append(primer)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef epcr_primer_file(self, formattedprimers):\n        logging.info('Creating re-PCR-compatible primer file')\n        with open(formattedprimers, 'w') as formatted:\n            # Iterate through all the targets\n            for basename in sorted(self.forward_dict):\n                # Use enumerate to number the iterations for each forward and reverse primer in the lists\n                for forward_index, forward_primer in enumerate(self.forward_dict[basename]):\n                    for reverse_index, reverse_primer in enumerate(self.reverse_dict[basename]):\n                        # Set the name of the primer using the target name, and the indices of the primers\n                        # e.g. vtx1a_0_0\n                        primer_name = '{bn}_{fi}_{ri}'.format(bn=basename,\n                                                              fi=forward_index,\n                                                              ri=reverse_index)\n                        # Create the string to write to the ePCR-compatible primer file\n                        # e.g. vtx1a_0_0\tCCTTTCCAGGTACAACAGCGGTT\tGGAAACTCATCAGATGCCATTCTGG\n                        output_string = '{pn}\\t{fp}\\t{rp}\\n'.format(pn=primer_name,\n                                                                    fp=forward_primer,\n                                                                    rp=reverse_primer)\n                        # Write the string to file\n                        formatted.write(output_string)", "response": "Create the ePCR - compatible primer file from the dictionaries of primer combinations and the formatted primer file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef epcr_threads(self, formattedprimers, ampliconsize=10000):\n        # Create the threads for the ePCR analysis\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                threads = Thread(target=self.epcr, args=())\n                threads.setDaemon(True)\n                threads.start()\n        logging.info('Running ePCR analyses')\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                setattr(sample, self.analysistype, GenObject())\n                # Get the primers ready\n                sample[self.analysistype].primers = formattedprimers\n                # Make the output path\n                sample[self.analysistype].reportdir = os.path.join(sample.general.outputdirectory,\n                                                                   self.analysistype)\n                make_path(sample[self.analysistype].reportdir)\n                outfile = os.path.join(sample[self.analysistype].reportdir, sample.name)\n                # Set the hashing and mapping commands\n                sample.commands.famap = '{famap} -b {outfile}.famap {fasta}'\\\n                    .format(famap=os.path.join(self.homepath, 'ePCR', 'famap'),\n                            outfile=outfile,\n                            fasta=sample.general.bestassemblyfile)\n                sample.commands.fahash = '{fahash} -b {outfile}.hash {outfile}.famap'\\\n                    .format(fahash=os.path.join(self.homepath, 'ePCR', 'fahash'),\n                            outfile=outfile)\n                # re-PCR uses the subtyping primers list to search the contigs file using the following parameters\n                # -S {hash file} (Perform STS lookup using hash-file), -r + (Enable/disable reverse STS lookup)\n                # -m 10000 (Set variability for STS size for lookup), this very large, as I don't necessarily know\n                # the size of the amplicon\n                # -n 1 (Set max allowed mismatches per primer pair for lookup)\n                # -g 0 (Set max allowed indels per primer pair for lookup),\n                # -G (Print alignments in comments)\n                # -o {output file}\n                sample.commands.epcr = \\\n                    '{rePCR} -S {outfile}.hash -r + -d 1-{ampsize} -n {mismatches} -g 0 -G -q ' \\\n                    '-o {outfile}.txt {primers}'\\\n                    .format(rePCR=os.path.join(self.homepath, 'ePCR', 're-PCR'),\n                            outfile=outfile,\n                            ampsize=ampliconsize,\n                            mismatches=self.mismatches,\n                            primers=sample[self.analysistype].primers)\n                sample[self.analysistype].resultsfile = '{of}.txt'.format(of=outfile)\n                # Add the sample object and the output file to the queue\n                self.epcrqueue.put((sample, outfile))\n        # Join the threads\n        self.epcrqueue.join()", "response": "Run the ePCR analysis in a multi - threaded fashion"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef epcr_parse(self):\n        logging.info('Parsing ePCR outputs')\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                # Create a set to store all the unique results\n                toxin_set = set()\n                if os.path.isfile(sample[self.analysistype].resultsfile):\n                    with open(sample[self.analysistype].resultsfile) as epcrresults:\n                        for result in epcrresults:\n                            # Only the lines without a # contain results\n                            if \"#\" not in result:\n                                # Split on \\t\n                                data = result.split('\\t')\n                                # The subtyping primer pair is the first entry on lines with results\n                                vttype = data[0].split('_')[0]\n                                # Add the verotoxin subtype to the set of detected subtypes\n                                toxin_set.add(vttype)\n                # Create a string of the entries in the sorted list of toxins joined with \";\"\n                sample[self.analysistype].toxinprofile = \";\".join(sorted(list(toxin_set))) if toxin_set else 'ND'\n            else:\n                setattr(sample, self.analysistype, GenObject())\n                sample[self.analysistype].toxinprofile = 'NA'", "response": "Parse the ePCR outputs and store the results in the object attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a report of the ePCR - calculated toxin profiles", "response": "def epcr_report(self):\n        \"\"\"\n        Create a report of the ePCR-calculated toxin profiles\n        \"\"\"\n        logging.info('Creating {at} report'.format(at=self.analysistype))\n        with open(os.path.join(self.reportpath, '{at}.csv'.format(at=self.analysistype)), 'w') as report:\n            data = 'Strain,ToxinProfile\\n'\n            for sample in self.metadata:\n                data += '{sn},{tp}\\n'.format(sn=sample.name,\n                                             tp=sample[self.analysistype].toxinprofile)\n            # Write the data to the report\n            report.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_epcr(self):\n        # Use the metadata object from the vtyper_object\n        for sample in self.vtyper_object.metadata:\n            # Initialise the dictionary\n            sample[self.analysistype].result_dict = dict()\n            # Read in the output file\n            with open(sample[self.analysistype].resultsfile) as epcrresults:\n                for result in epcrresults:\n                    # Only the lines without a # contain results\n                    if \"#\" not in result:\n                        # Split on \\t\n                        # vtx2a_0_0 2014-SEQ-0121_127_length_1407_cov_50.7797_ID_10924  -   228   576 2   0 349/100-350\n                        # primer_set: vtx2a_0_0, contig: 2014-SEQ-0121_127_length_1407_cov_50.7797_ID_10924, strand: -,\n                        # start: 228, stop: 576, number of forward mismatches: 2, number of reverse mismatches: 2\n                        # amplicon_combo: 349/100-350\n                        primer_set, contig, strand, start, stop, total_mismatches, indels, amplicon_combo = \\\n                            result.rstrip().split('\\t')\n                        # Set the mismatches to be an int\n                        total_mismatches = int(total_mismatches)\n                        # Set the position of the amplicon on the contig. Ensure that the lower value is first\n                        genome_pos = '{min}-{max}'.format(min=min([int(start), int(stop)]),\n                                                          max=max([int(start), int(stop)]))\n                        # Extract the gene name from the modified name used when creating the primer file: LMhlyA_0_0\n                        # becomes LMhlyA\n                        gene_re = re.search(r'([\\w-]+)_(\\d{1,3})_(\\d{1,3})', primer_set)\n                        gene = gene_re.groups()[0]\n                        # Split the amplicon length from amplicon_combo: 349/100-350 -> 349\n                        amplicon_length = amplicon_combo.split('/')[0]\n                        # Populate the dictionary if the 'total_mismatches' key doesn't exist, or if the current number\n                        # of mismatches is better than the previous 'best' number of mismatches\n                        try:\n                            if total_mismatches < sample[self.analysistype].result_dict[gene]['total_mismatches']:\n                                self.populate_results_dict(sample=sample,\n                                                           gene=gene,\n                                                           total_mismatches=total_mismatches,\n                                                           genome_pos=genome_pos,\n                                                           amplicon_length=amplicon_length,\n                                                           contig=contig,\n                                                           primer_set=primer_set)\n                        except KeyError:\n                            self.populate_results_dict(sample=sample,\n                                                       gene=gene,\n                                                       total_mismatches=total_mismatches,\n                                                       genome_pos=genome_pos,\n                                                       amplicon_length=amplicon_length,\n                                                       contig=contig,\n                                                       primer_set=primer_set)", "response": "Parse the ePCR output file and populate the dictionary of resutls based on the number of mismatches."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef populate_results_dict(self, sample, gene, total_mismatches, genome_pos, amplicon_length, contig, primer_set):\n        sample[self.analysistype].result_dict[gene] = {\n            'total_mismatches': total_mismatches,\n            'genome_pos': genome_pos,\n            'amplicon_length': amplicon_length,\n            'contig': contig,\n            'primer_set': primer_set\n        }", "response": "Populate the results dictionary with the required key value pairs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_epr_report(self):\n        # Open the report as a .csv file\n        with open(os.path.join(self.reportpath, 'ePCR_report.csv'), 'w') as report:\n            # Initialise a string to store the header\n            results = 'Sample,Gene,GenomeLocation,AmpliconSize,Contig,TotalMismatches,PrimerSet\\n'\n            for sample in self.vtyper_object.metadata:\n                # Check to see if there are strain-specific results\n                if sample[self.analysistype].result_dict:\n                    for gene, result_dict in sample[self.analysistype].result_dict.items():\n                        # Populate the string with the appropriate values extracted from the dictionary\n                        results += '{sn},{gene},{genomelocation},{ampliconsize},{contig},{nm},{ps}\\n'\\\n                            .format(sn=sample.name,\n                                    gene=gene,\n                                    genomelocation=result_dict['genome_pos'],\n                                    ampliconsize=result_dict['amplicon_length'],\n                                    contig=result_dict['contig'],\n                                    nm=result_dict['total_mismatches'],\n                                    ps=result_dict['primer_set'])\n                        if self.export_amplicons:\n                            self.ampliconfile(sample=sample,\n                                              contig=result_dict['contig'],\n                                              amplicon_range=result_dict['genome_pos'].split('-'),\n                                              primer_set=result_dict['primer_set'])\n                else:\n                    results += '{sn}\\n'.format(sn=sample.name)\n            # Write the complete string to the report\n            report.write(results)", "response": "Create a final ePCR report from the results dictionaries and write it to the file containing the final report."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ampliconfile(self, sample, contig, amplicon_range, primer_set):\n        # Open the file\n        sample[self.analysistype].ampliconfile = os.path.join(self.reportpath, '{sn}_amplicons.fa'\n                                                              .format(sn=sample.name))\n        with open(sample[self.analysistype].ampliconfile, 'a') as ampliconfile:\n            # Load the records from the assembly into the dictionary\n            for record in SeqIO.parse(sample.general.bestassemblyfile, 'fasta'):\n                if record.id == contig:\n                    try:\n                        # Extract the start and end positions of the supplied range\n                        start, end = amplicon_range\n                        # Slice the gene sequence from the sequence record - remember to subtract one to\n                        # allow for zero-based indexing\n                        genesequence = str(record.seq)[int(start) - 1:int(end)]\n                        # Set the record.id to be the sample name, the contig name,\n                        # the range, and the primers\n                        record.id = '{sn}_{contig}_{genomepos}_{primer_set}' \\\n                            .format(sn=sample.name,\n                                    contig=contig,\n                                    genomepos='_'.join(amplicon_range),\n                                    primer_set=primer_set)\n                        # Clear the record.description\n                        record.description = ''\n                        # Create a seq record from the sliced genome sequence\n                        record.seq = Seq.Seq(genesequence)\n                        # Write the amplicon to file\n                        SeqIO.write(record, ampliconfile, 'fasta')\n                    except AttributeError:\n                        pass", "response": "Extracts the amplicon sequence from the contig file and writes it to the sample metadata object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating metadata objects with necessary attributes for each FASTA file found in the sequence path.", "response": "def filer(args):\n        \"\"\"\n        Create metadata objects with necessary attributes for each FASTA file found in the sequence path\n        :param args: Argument parser object with necessary variables\n        :return: samples: List of metadata objects\n        \"\"\"\n        # List to store all the metadata objects\n        samples = list()\n        # Find all the sequence files in the path\n        fastas = sorted(glob(os.path.join(args.sequencepath, '*.fa*')))\n        for fasta in fastas:\n            # Create a metadata object for each sample\n            metadata = MetadataObject()\n            # Populate the metadata object with the required attributes\n            metadata.name = os.path.splitext(os.path.basename(fasta))[0]\n            metadata.general = GenObject()\n            metadata.commands = GenObject()\n            metadata.general.bestassemblyfile = fasta\n            metadata.general.outputdirectory = os.path.join(args.sequencepath, metadata.name)\n            samples.append(metadata)\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef samplesheet(self):\n        if self.demultiplex:\n            make_path(self.samplesheetpath)\n            self.customsamplesheet = os.path.join(self.samplesheetpath, 'SampleSheet.csv')\n            header = ['Sample_ID', 'Sample_Name', 'Sample_Plate', 'Sample_Well', 'I7_Index_ID', 'index', 'I5_Index_ID',\n                      'index2', 'Sample_Project', 'Description']\n            with open(self.customsamplesheet, 'w') as samplesheet:\n                lines = str()\n                lines += '[Header]\\n'\n                lines += 'IEMFileVersion,{}\\n'.format(self.header.IEMFileVersion)\n                lines += 'Investigator Name,{}\\n'.format(self.header.InvestigatorName)\n                lines += 'Experiment Name,{}\\n'.format(self.header.ExperimentName)\n                lines += 'Date,{}\\n'.format(self.header.Date)\n                lines += 'Workflow,{}\\n'.format(self.header.Workflow)\n                lines += 'Application,{}\\n'.format(self.header.Application)\n                lines += 'Assay,{}\\n'.format(self.header.Assay)\n                lines += 'Description,{}\\n'.format(self.header.Description)\n                lines += 'Chemistry,{}\\n'.format(self.header.Chemistry)\n                lines += '\\n'\n                lines += '[Reads]\\n'\n                lines += str(self.forward) + '\\n'\n                lines += str(self.reverse) + '\\n'\n                lines += '\\n'\n                lines += '[Settings]\\n'\n                lines += 'ReverseComplement,{}\\n'.format(self.header.ReverseComplement)\n                lines += 'Adapter,{}\\n'.format(self.header.Adapter)\n                lines += '\\n'\n                lines += '[Data]\\n'\n                lines += ','.join(header)\n                lines += '\\n'\n                # Correlate all the samples added to the list of incomplete samples with their metadata\n                for incomplete in self.incomplete:\n                    for sample in self.rundata:\n                        if incomplete == sample['SampleID']:\n                            # Use each entry in the header list as a key for the rundata dictionary\n                            for data in header:\n                                # Modify the key to be consistent with how the dictionary was populated\n                                result = sample[data.replace('_', '')]\n                                # Description is the final entry in the list, and shouldn't have a , following the value\n                                if data != 'Description':\n                                    lines += '{},'.format(result.replace('NA', ''))\n                                # This entry should have a newline instead of a ,\n                                else:\n                                    lines += '{}\\n'.format(result.replace('NA', ''))\n                # Write the string to the sample sheet\n                samplesheet.write(lines)", "response": "Create a custom sample sheet based on the original sample sheet for the run and only including the samples that did not pass the quality threshold on the previous iteration."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cast(self, value):\n        if value.isdigit():\n            value = int(value)\n        elif re.compile(\"^\\d+\\.\\d+\").match(value):\n            value = float(value)\n        return value", "response": "Try to cast value to int or float"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(connection=None, silent=False, hgnc_file_path=None, hcop_file_path=None, low_memory=False):\n    database = DbManager(connection)\n    database.db_import(silent=silent, hgnc_file_path=hgnc_file_path, hcop_file_path=hcop_file_path, low_memory=low_memory)\n    database.session.close()", "response": "Update the database with current version of HGNC\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_connection(connection=defaults.sqlalchemy_connection_string_default):\n    config_path = defaults.config_file_path\n    config = RawConfigParser()\n\n    if not os.path.exists(config_path):\n        with open(config_path, 'w') as config_file:\n            config['database'] = {'sqlalchemy_connection_string': connection}\n            config.write(config_file)\n            log.info('create configuration file {}'.format(config_path))\n    else:\n        config.read(config_path)\n        config.set('database', 'sqlalchemy_connection_string', connection)\n        with open(config_path, 'w') as configfile:\n            config.write(configfile)", "response": "Set the connection string for sqlalchemy and write it to the config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef relocate(self, destination):\n        for activate in self.bin.activates:\n\n            activate.vpath = destination\n\n        for binfile in self.bin.files:\n\n            if binfile.shebang and (\n                    'python' in binfile.shebang or 'pypy' in binfile.shebang\n            ):\n\n                binfile.shebang = '#!{0}'.format(\n                    os.path.join(destination, 'bin', 'python')\n                )", "response": "Configure the virtual environment for another path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move(self, destination):\n        self.relocate(destination)\n        shutil.move(self.path, destination)\n        self._path = destination", "response": "Reconfigure and move the virtual environment to another path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reports(self):\n        printtime('Finding reports', self.start)\n        # Initialise a list of runs and a list of all the metadata objects\n        runs = list()\n        samples = list()\n        # Glob all the assembly folders into a list\n        folders = glob(os.path.join(self.path, '*'.format(self.path)))\n        # Iterate through all the folders\n        for folder in folders:\n            if os.path.isdir(folder):\n                # Ignore any previously processed aggregate reports folder\n                if 'reports' not in folder:\n                    # Treat the external labs differently - instead of having all the assemblies in the path, each lab\n                    # has its own subfolder with assemblies\n                    if self.external:\n                        subfolder = glob(os.path.join(folder, '*'))\n                        for subsubfolder in subfolder:\n                            if os.path.isdir(subfolder):\n                                runs.append(subsubfolder)\n                    else:\n                        runs.append(folder)\n        # Create metadata objects for each assembly run\n        for assembly in sorted(runs):\n            # Initialise the Metadata object\n            metadata = MetadataObject()\n            metadata.name = assembly.split('/')[-2] if assembly.split('/')[-3] == 'WGSspades' else '{}-{}'\\\n                .format(assembly.split('/')[-3], assembly.split('/')[-2])\n            # Strip of any underscores and anything after an underscore\n            metadata.name = metadata.name.split('_')[0]\n            # Initialise Genobjects to store nested dictionaries of metadata in each metadata object\n            metadata.general = GenObject()\n            # Populate the GenObject\n            metadata.general.path = assembly\n            metadata.general.reportpath = os.path.join(str(assembly), 'reports')\n            metadata.general.reports = glob(os.path.join(metadata.general.reportpath, '*.csv'))\n            metadata.general.reportnames = [os.path.basename(x) for x in metadata.general.reports]\n            # Add all the names of the reports to the set of reports\n            for reportname in metadata.general.reportnames:\n                self.reportset.add(reportname)\n            # Add the metadata to list of metadata objects\n            samples.append(metadata)\n        # Return the list of metadata objects\n        return samples", "response": "Find all the runs in the path and create metadata objects for each run."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\naggregate all reports of the same type into a master report", "response": "def aggregate(self):\n        \"\"\"\n        Aggregate all reports of the same type into a master report\n        \"\"\"\n        for report in self.reportset:\n            printtime('Processing {}'.format(report.split('.')[0]), self.start)\n            # Initialise the header for each report - MLST is different, as the header is different for each\n            # MLST scheme. This provides a generic header instead\n            header = '' if report != 'mlst.csv' else 'Strain,Genus,SequenceType,Matches,1,2,3,4,5,6,7\\n'\n            # Initialise a string to hold the data for each report\n            data = ''\n            # Open the aggregated report\n            with open(os.path.join(self.reportpath, report), 'w') as aggregate:\n                for sample in self.runmetadata.samples:\n                    # Try to open the report for this run\n                    try:\n                        #\n                        with open(os.path.join(sample.general.reportpath, report), 'r') as runreport:\n                            # Only get the header from the first file\n                            if not header:\n                                header = runreport.readline()\n                            else:\n                                for row in runreport:\n                                    # The final entry in a report does not have a newline character. Add \\n as required\n                                    if not row.endswith('\\n'):\n                                        row += '\\n'\n                                    # For certain reports, the header row is printed above each strain - ignore multiple\n                                    # instances of the header\n                                    if row.split(',')[0] != header.split(',')[0]:\n                                        # Add the row to the string of data\n                                        data += row\n                    except IOError:\n                        pass\n                # Write the strings to the aggregate report file\n                aggregate.write(header)\n                aggregate.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clean_names(in_fasta, out_fasta='NA', method='split', delimiter='NA', truncate_value=10):\n    with open(in_fasta) as fasta_file:\n        lines = fasta_file.readlines()\n    if out_fasta == 'NA':\n        out_file = in_fasta\n    else:\n        out_file = out_fasta\n    with open(out_file, 'w') as f:\n        for line in lines:\n            if '>' in line:\n                if method == 'split':\n                    if delimiter == 'NA':\n                        f.write(line.split()[0] + '\\n')\n                    else:\n                        f.write(line.split(delimiter)[0] + '\\n')\n                elif method == 'truncate':\n                    if len(line) > truncate_value:\n                        f.write(line[0:truncate_value] + '\\n')\n                    else:\n                        f.write(line)\n                else:\n                    raise ValueError('Valid values of method are split or truncate. Please specify one of those options.')\n            else:\n                f.write(line)", "response": "This function cleans up the names of the user s tables in a single file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_args(request_args, allowed_int_args=(), allowed_str_args=(), allowed_bool_args=()):\n    args = {}\n\n    for allowed_int_arg in allowed_int_args:\n        int_value = request_args.get(allowed_int_arg, default=None, type=None)\n\n        if int_value:\n            args[allowed_int_arg] = int(int_value)\n\n    for allowed_str_arg in allowed_str_args:\n        str_value = request_args.get(allowed_str_arg, default=None, type=None)\n\n        if str_value:\n            args[allowed_str_arg] = str_value\n\n    for allowed_bool_arg in allowed_bool_args:\n        str_value = request_args.get(allowed_bool_arg, default=None, type=None)\n\n        if str_value == 'true':\n            args[allowed_bool_arg] = True\n\n        elif str_value == 'false':\n            args[allowed_bool_arg] = False\n\n    return args", "response": "Check allowed argument names and return is as dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the entry of a given set of parameters in a HGNC entry", "response": "def query_entry():\n    \"\"\"\n    Returns list of HGNC entries by query paramaters\n    ---\n\n    tags:\n\n      - Query functions\n\n    parameters:\n\n      - name: name\n        in: query\n        type: string\n        required: false\n        description: 'HGNC approved name for the gene'\n        default: 'lysine demethylase 1A'\n\n      - name: symbol\n        in: query\n        type: string\n        required: false\n        description: 'HGNC symbol'\n        default: KDM1A\n\n      - name: identifier\n        in: query\n        type: integer\n        required: false\n        description: 'HGNC ID. A unique ID created by the HGNC for every approved symbol'\n        default: 29079\n\n      - name: status\n        in: query\n        type: string\n        required: false\n        description: 'Status of the symbol report, which can be either \"Approved\" or \"Entry Withdrawn\"'\n        default: Approved\n\n      - name: uuid\n        in: query\n        type: string\n        required: false\n        description: 'universally unique identifier'\n        default: '032998f3-5339-4c36-a521-1960c2f86cee'\n\n      - name: orphanet\n        in: query\n        type: integer\n        required: false\n        description: 'Orphanet database identifier'\n        default: 478263\n\n      - name: locus_group\n        in: query\n        type: string\n        required: false\n        description: 'group name for a set of related locus types as defined by the HGNC'\n        default: 'protein-coding gene'\n\n      - name: locus_type\n        in: query\n        type: string\n        required: false\n        description: 'locus type as defined by the HGNC'\n        default: 'gene with protein product'\n\n      - name: date_name_changed\n        in: query\n        type: string\n        required: false\n        description: 'date the gene name was last changed'\n        default: '2016-02-12'\n\n      - name: date_modified\n        in: query\n        type: string\n        required: false\n        description: 'date the entry was last modified'\n        default: '2017-07-07'\n\n      - name: date_symbol_changed\n        in: query\n        type: string\n        required: false\n        description: 'date the gene symbol was last changed'\n        default: '2009-09-29'\n\n      - name: date_approved_reserved\n        in: query\n        type: string\n        required: false\n        description: 'date the entry was first approved'\n        default: '2004-02-26'\n\n      - name: ensembl_gene\n        in: query\n        type: string\n        required: false\n        description: 'Ensembl gene ID. Found within the \"GENE RESOURCES\" section of the gene symbol report'\n        default: 'ENSG00000004487'\n\n      - name: vega\n        in: query\n        type: string\n        required: false\n        description: 'Vega gene ID. Found within the \"GENE RESOURCES\" section of the gene symbol report'\n        default: 'OTTHUMG00000003220'\n\n      - name: lncrnadb\n        in: query\n        type: string\n        required: false\n        description: 'Noncoding RNA Database identifier'\n        default:\n\n      - name: horde\n        in: query\n        type: string\n        required: false\n        description: 'symbol used within HORDE for the gene (not available in JSON)'\n        default:\n\n      - name: entrez\n        in: query\n        type: string\n        required: false\n        description: 'Entrez gene ID. Found within the \"GENE RESOURCES\" section of the gene symbol report'\n        default: 23028\n\n      - name: mirbase\n        in: query\n        type: string\n        required: false\n        description: 'miRBase ID'\n        default:\n\n      - name: iuphar\n        in: query\n        type: string\n        required: false\n        description: 'The objectId used to link to the IUPHAR/BPS Guide to PHARMACOLOGY database'\n        default: 'objectId:2669'\n\n      - name: ucsc\n        in: query\n        type: string\n        required: false\n        description: 'UCSC gene ID. Found within the \"GENE RESOURCES\" section of the gene symbol report'\n        default: 'uc001bgi.3'\n\n      - name: snornabase\n        in: query\n        type: string\n        required: false\n        description: 'snoRNABase ID'\n        default:\n\n      - name: pseudogeneorg\n        in: query\n        type: string\n        required: false\n        description: 'Pseudogene.org ID'\n        default:\n\n      - name: bioparadigmsslc\n        in: query\n        type: string\n        required: false\n        description: 'Symbol used to link to the SLC tables database at bioparadigms.org for the gene'\n        default:\n\n      - name: locationsortable\n        in: query\n        type: string\n        required: false\n        description: 'locations sortable'\n        default: 01p36.12\n\n      - name: merops\n        in: query\n        type: string\n        required: false\n        description: 'ID used to link to the MEROPS peptidase database'\n        default:\n\n      - name: location\n        in: query\n        type: string\n        required: false\n        description: 'Cytogenetic location of the gene (e.g. 2q34)'\n        default: 1p36.12\n\n      - name: cosmic\n        in: query\n        type: string\n        required: false\n        description: 'Symbol used within the Catalogue of somatic mutations in cancer for the gene'\n        default: 'KDM1A'\n\n      - name: imgt\n        in: query\n        type: string\n        required: false\n        description: 'Symbol used within international ImMunoGeneTics information system'\n        default:\n\n      - name: limit\n        in: query\n        type: integer\n        required: false\n        default: 1\n    \"\"\"\n    allowed_str_args = ['name', 'symbol', 'status', 'uuid', 'locus_group', 'locus_type',\n                        'date_name_changed', 'date_modified', 'date_symbol_changed', 'date_approved_reserved',\n                        'ensembl_gene', 'vega', 'lncrnadb', 'horde', 'mirbase', 'iuphar', 'ucsc', 'snornabase',\n                        'pseudogeneorg', 'bioparadigmsslc', 'locationsortable', 'merops', 'location', 'cosmic', 'imgt'\n                        ]\n\n    allowed_int_args = ['limit', 'identifier', 'orphanet', 'entrez', ]\n\n    args = get_args(\n        request_args=request.args,\n        allowed_int_args=allowed_int_args,\n        allowed_str_args=allowed_str_args\n    )\n\n    return jsonify(query.hgnc(**args))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of alias name by query paramaters", "response": "def alias_name():\n    \"\"\"\n    Returns list of alias name by query paramaters\n    ---\n\n    tags:\n\n      - Query functions\n\n    parameters:\n\n      - name: alias_name\n        in: query\n        type: string\n        required: false\n        description: 'Other names used to refer to a gene'\n        default: 'peptidase nexin-II'\n\n      - name: is_previous_name\n        in: query\n        type: boolean\n        required: false\n        description: 'Other names used to refer to a gene'\n        default: false\n\n      - name: hgnc_symbol\n        in: query\n        type: string\n        required: false\n        description: 'HGNC symbol'\n        default: APP\n\n      - name: hgnc_identifier\n        in: query\n        type: integer\n        required: false\n        description: 'HGNC identifier'\n        default: 620\n\n      - name: limit\n        in: query\n        type: integer\n        required: false\n        default: 1\n    \"\"\"\n    allowed_str_args = ['alias_name', 'hgnc_symbol', 'hgnc_identifier']\n\n    allowed_int_args = ['limit', ]\n\n    allowed_bool_args = ['is_previous_name', ]\n\n    args = get_args(\n        request_args=request.args,\n        allowed_int_args=allowed_int_args,\n        allowed_str_args=allowed_str_args,\n        allowed_bool_args=allowed_bool_args,\n    )\n\n    return jsonify(query.alias_name(**args))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning list of Enzyme Commission numbers by query paramaters", "response": "def enzyme():\n    \"\"\"\n    Returns list of Enzyme Commission numbers by query paramaters\n    ---\n\n    tags:\n\n      - Query functions\n\n    parameters:\n\n      - name: ec_number\n        in: query\n        type: string\n        required: false\n        description: 'Enzyme Commission number'\n        default: '1.1.1.1'\n\n      - name: hgnc_symbol\n        in: query\n        type: string\n        required: false\n        description: 'HGNC symbol'\n        default: 'ADH7'\n\n      - name: hgnc_identifier\n        in: query\n        type: integer\n        required: false\n        description: 'HGNC identifier'\n        default: 256\n\n      - name: limit\n        in: query\n        type: integer\n        required: false\n        default: 1\n    \"\"\"\n    allowed_str_args = ['hgnc_symbol', 'ec_number']\n\n    allowed_int_args = ['limit', 'hgnc_identifier']\n\n    args = get_args(\n        request_args=request.args,\n        allowed_int_args=allowed_int_args,\n        allowed_str_args=allowed_str_args\n    )\n\n    return jsonify(query.enzyme(**args))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an alignment block from the given file handle.", "response": "def _parse_blocks(instream):\n    \"\"\"Parse an alignment block from the given file handle.\n\n    Block looks like:\n    [0_(1)=fa2cma(8){go=10000,gx=2000,pn=1000.0,lf=0,rf=0}:\n    (209)***********************************************...\n\n    ... sequences, numbered 1-8 ...\n\n    _0].\n\n    \"\"\"\n    ilines = sugar.unblank(instream)\n    for line in ilines:\n        if line.startswith('['):\n            # Start of block\n            level, one, name, seqcount, params = _parse_block_header(line)\n            qlen, qchars = _parse_block_postheader(next(ilines))\n            # Pass control to the sequence parser\n            sequences = list(_parse_sequences(ilines, qlen))\n            # Validation\n            if not len(sequences) == seqcount:\n                logging.warn(\"Expected %d sequences in block %s, found %d\",\n                             seqcount, name, len(sequences))\n            yield {'level': level,\n                   'one': one,\n                   'name': name,\n                   # 'seqcount': seqcount,\n                   'params': params,\n                   'query_length': qlen,\n                   'query_chars': qchars,\n                   'sequences': sequences,\n                   }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_sequences(ilines, expect_qlen):\n    while True:\n        first = next(ilines)\n        if first.startswith('_') and first.endswith('].'):\n            # End of sequences & end of block\n            break\n\n        # ENH: handle wrapped lines?\n        try:\n            index, this_len, query_len = _parse_seq_preheader(first)\n        except ValueError:\n            logging.warn('Unparseable line (SKIPPING):\\n%s', first)\n            continue\n        (rec_id, dbxrefs, headlen, taillen, phylum, taxchar, description\n                ) = _parse_seq_header(next(ilines))\n        try:\n            headseq, molseq, tailseq = _parse_seq_body(next(ilines))\n        except ValueError:\n            logging.warn('Unparseable sequence: %s -- SKIPPING', rec_id)\n            continue\n\n        # Validation\n        if expect_qlen != query_len:\n            logging.warn(\"Query length in %s given as %d; expected %d\",\n                         rec_id, query_len, expect_qlen)\n        if not headseq and not headlen:\n            headlen = 0\n        if not tailseq and not taillen:\n            taillen = 0\n        if headseq:\n            if headlen is None:\n                headlen = len(headseq)\n            elif headlen != len(headseq):\n                logging.warn(\"Conflicting head flank lengths in %s: %d, %d\",\n                             rec_id, headlen, len(headseq))\n        if tailseq:\n            if taillen is None:\n                taillen = len(tailseq)\n            elif taillen != len(tailseq):\n                logging.warn(\"Conflicting tail flank lengths in %s: %d, %d\",\n                             rec_id, taillen, len(tailseq))\n\n        yield {'index': index,\n               'id': rec_id,\n               'description': description,\n               'dbxrefs': dbxrefs,\n               'phylum': phylum,\n               'taxchar': taxchar,\n               'head_len': headlen,\n               'tail_len': taillen,\n               'head_seq': headseq,\n               'tail_seq': tailseq,\n               'length': this_len,\n               'seq': molseq,\n               }", "response": "Parse the sequences in the current block."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_block_header(line):\n    level = line[1]\n    one, _rest = line[4:].split(')=', 1)\n    name, _rest = _rest.split('(', 1)\n    seqcount, _rest = _rest.split(')', 1)\n    params = _rest.strip('{}:')\n    # try:\n    #     params = dict((key, float(val))\n    #             for key, val in (pair.split('=')\n    #                 for pair in _rest[1:-2].split(',')))\n    # except ValueError:\n    #     # Couldn't convert params to key-val pairs, for whatever reason\n    #     logging.warn(\"Failed to parse CMA params: %s\", _rest[1:-2])\n    #     params = {}\n    return int(level), int(one), name, int(seqcount), params", "response": "Parse a CMA block header line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_block_postheader(line):\n    parts = line[1:].split(')', 1)\n    qlen = int(parts[0])\n    if not len(parts[1]) == qlen:\n        logging.warn(\"postheader expected %d-long query, found %d\",\n                     qlen, len(parts[1]))\n    return qlen, parts[1]", "response": "Parse the postheader of a block of data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the seq - pre - header line.", "response": "def _parse_seq_preheader(line):\n    \"\"\"\n    $3=227(209):\n    \"\"\"\n    match = re.match(r\"\\$ (\\d+) = (\\d+) \\( (\\d+) \\):\", line, re.VERBOSE)\n    if not match:\n        raise ValueError(\"Unparseable header: \" + line)\n    index, this_len, query_len = match.groups()\n    return map(int, (index, this_len, query_len))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a FASTA sequence header line and returns a dict containing the unique ID head - tail - lengths and taxonomy - codes.", "response": "def _parse_seq_header(line):\n    \"\"\"Unique ID, head/tail lengths and taxonomy info from a sequence header.\n\n    The description is the part of the FASTA/CMA sequence header starting after\n    the first space (i.e. excluding ID), to the end of the line.\n\n    This function looks inside the first '{...}' pair to extract info.\n\n    Ex:\n\n    >consensus seq\n    >gi|15606894|ref|NP_214275.1|  {|2(244)|<Aquificae(B)>}DNA polymerase III gamma subunit [Aquifex aeolicus VF5] >gi|2984127|gb|AAC07663.1| DNA polymerase III gamma subunit [Aquifex aeolicus VF5] >gi|75\n    >gi|3212262|pdb|1A2K|C  {<Chordata(M)>}Chain C, Gdpran-Ntf2 Complex >gi|3212263|pdb|1A2K|D Chain D, Gdpran-Ntf2 Complex >gi|3212264|pdb|1A2K|E Chain E, Gdpran-Ntf2 Complex >gi|5542273|pdb|1IBR|A C\n\n    \"\"\"\n    # ENH: use the two functions in esbglib.parseutils\n    # or, move one or both of those functions into here\n    _parts = line[1:].split(None, 1)\n    rec_id = _parts[0]\n    descr = _parts[1] if _parts[1:] else ''\n    # Database cross references\n    dbxrefs = {}\n    if '|' in rec_id:\n        id_gen = iter(rec_id.rstrip('|').split('|'))\n        for key in id_gen:\n            try:\n                dbxrefs[key] = next(id_gen)\n            except StopIteration:\n                break\n\n    # Head/tail lengths and taxonomy codes\n    headlen = taillen = None\n    phylum = taxchar = ''\n    if descr.startswith('{'):\n        _deets, description = descr[1:].split('}', 1)\n        match = re.search(r\"\"\"\n        (?:\n        \\| (?P<headlen> \\d+)\n            \\( (?P<taillen> \\d+)\n            \\)\n        \\|\n        )?\n        (?:\n        <   (?P<phylum> .+?)\n            \\( (?P<taxchar> \\w)\n            \\)\n        >\n        )?\n        \"\"\", _deets, re.VERBOSE)\n        if match:\n            headlen, taillen, phylum, taxchar = match.groups()\n            if headlen is not None:\n                headlen = int(headlen)\n            if taillen is not None:\n                taillen = int(taillen)\n            if phylum is None:\n                phylum = ''\n            if taxchar is None:\n                taxchar = ''\n        else:\n            logging.warn(\"Couldn't match head/tail: %s\", _deets)\n    else:\n        description = descr\n\n    # TODO - return a dictionary here, update it in _parse_sequences\n    return rec_id, dbxrefs, headlen, taillen, phylum, taxchar, description"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_seq_body(line):\n    line = line.rstrip('*')\n    if '{()' in line:\n        head, _rest = line.split('{()', 1)\n    else:\n        # Match parens\n        _rest = line.split('{(', 1)[1]\n        head, _rest = _rest.split(')', 1)\n    if '()}' in _rest:\n        molseq, tail = _rest.split('()}', 1)\n    else:\n        # Match parens\n        molseq, _rest = _rest.split('(', 1)\n        tail = _rest.split(')}', 1)[0]\n    return (head, molseq, tail)", "response": "Parse the sequence body of a sequence file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding gaps to a block so all residues in a column are equivalent. Given a block, containing a list of \"sequences\" (dicts) each containing a \"seq\" (actual string sequence, where upper=match, lower=insert, dash=gap), insert gaps (- or .) into the sequences s.t. 1. columns line up properly, and 2. all resulting sequences have the same length The reason this needs to be done is that the query/consensus sequence is not assigned gaps to account for inserts in the other sequences. We need to add the gaps back to obtain a normal alignment. `return`: a list of realigned sequence strings.", "response": "def realign_seqs(block, gap_char='.', align_indels=False):\n    \"\"\"Add gaps to a block so all residues in a column are equivalent.\n\n    Given a block, containing a list of \"sequences\" (dicts) each containing a\n    \"seq\" (actual string sequence, where upper=match, lower=insert, dash=gap),\n    insert gaps (- or .) into the sequences s.t.\n\n    1. columns line up properly, and\n    2. all resulting sequences have the same length\n\n\n    The reason this needs to be done is that the query/consensus sequence is not\n    assigned gaps to account for inserts in the other sequences. We need to add\n    the gaps back to obtain a normal alignment.\n\n    `return`: a list of realigned sequence strings.\n    \"\"\"\n    # ENH: align inserts using an external tool (if align_indels)\n    all_chars = [list(sq['seq']) for sq in block['sequences']]\n    # NB: If speed is an issue here, consider Numpy or Cython\n    #   main problem: list.insert is O(n) -- would OrderedDict help?\n    nrows = len(all_chars)\n    i = 0\n    while i < len(all_chars[0]):\n        rows_need_gaps = [r for r in all_chars if not r[i].islower()]\n        if len(rows_need_gaps) != nrows:\n            for row in rows_need_gaps:\n                row.insert(i, gap_char)\n        i += 1\n    return [''.join(row) for row in all_chars]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a Biopython SeqRecord to a esbglib. cma block.", "response": "def consensus2block(record, level=0, name=None):\n    \"\"\"Convert a Biopython SeqRecord to a esbglib.cma block.\n\n    Ungapping is handled here.\n    \"\"\"\n    cons_ungap = str(record.seq).replace('-', '').replace('.', '').upper()\n    record.seq = cons_ungap\n    return dict(\n            level=level, #record.annotations.get('level', 0),\n            one=1,\n            name=name or record.id,\n            params='go=10000,gx=2000,pn=1000.0,lf=0,rf=0',\n            query_length=len(cons_ungap),\n            query_chars='*'*len(cons_ungap),\n            sequences=[seqrecord2sequence(record, len(cons_ungap), 1)]\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a Biopython SeqRecord to a esbglib. cma block.", "response": "def seqrecord2sequence(record, qlen, index):\n    \"\"\"Convert a Biopython SeqRecord to a esbglib.cma block.\n\n    Indels (gaps, casing) must have already been handled in the record.\n    \"\"\"\n    # aligned_len = sum(map(str.isupper, str(record.seq)))\n    # assert qlen == aligned_len, (\n    #         \"Aligned sequence length %s, query %s\\n%s\"\n    #         % (aligned_len, qlen, str(record)))\n\n    description = (record.description.split(' ', 1)[1]\n            if ' ' in record.description\n            else ' ')\n    return dict(index=index,\n                id=record.id,\n                description=description,\n                dbxrefs={},\n                phylum='',\n                taxchar='',\n                head_len=None,\n                tail_len=None,\n                head_seq='',\n                tail_seq='',\n                length=len(record.seq) - record.seq.count('-'),\n                seq=str(record.seq),\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollapsing sequences into a single block of sequences.", "response": "def collapse_to_consensus(seqrecords, strict=False, do_iron=True):\n    \"\"\"Opposite of realign_seqs.\n\n    Input sequences should all be the same length.\n\n    The first record must be the consensus.\n    \"\"\"\n    level = 0\n    name = seqrecords[0].id\n    # If this is a CMA alignment, extract additional info:\n    if hasattr(seqrecords, '_records'):\n        if hasattr(seqrecords, 'level'):\n            level = seqrecords.level\n        if hasattr(seqrecords, 'name'):\n            name = seqrecords.name\n        seqrecords = seqrecords._records\n\n    consensus = seqrecords.pop(0)\n    cons_length = len(consensus)\n    for i, s in enumerate(seqrecords):\n        if len(s) != cons_length:\n            raise ValueError(\n            \"Sequence #%d has length %d, consensus is %d\"\n            % (i+2, len(s), cons_length))\n\n    if '.' in str(consensus.seq):\n        # Strict -- error if there's a '-'\n        if '-' in str(consensus.seq):\n            if strict:\n                raise ValueError(\"Consensus contains '-' gap characters\")\n            logging.warn(\"Consensus sequence contains both '.' and '-' gap \"\n                         \"characters -- is it really the consensus?\")\n            aligned_cols = [(c not in '.-') for c in str(consensus.seq)]\n        else:\n            aligned_cols = [c != '.' for c in str(consensus.seq)]\n    else:\n        # A little more ambiguous...\n        aligned_cols = [c != '-' for c in str(consensus.seq)]\n\n    consensus.seq = replace_asterisks(consensus.seq, 'consensus')\n    # Start a block with the consensus sequence\n    block = consensus2block(consensus, level=level, name=name)\n    qlen = block['query_length']\n\n    # Collapse & add remaining sequences to the block\n    for index, rec in zip(xrange(2, len(seqrecords)+2), seqrecords):\n        # Collapse rec.seq down to aligned size\n        new_mol_seq = []\n        is_beginning = True\n        for aligned_col, char in zip(aligned_cols,\n                                     replace_asterisks(rec.seq, index)):\n            if aligned_col:\n                is_beginning = False\n                if char in '-.':\n                    # deletion\n                    new_mol_seq.append('-')\n                else:\n                    # aligned character\n                    new_mol_seq.append(char.upper())\n            else:\n                # it's an insert or nothing\n                # (also, skip any left-side inserts)\n                if char not in '-.' and not is_beginning:\n                    new_mol_seq.append(char.lower())\n        rec.seq = ''.join(new_mol_seq)\n        if do_iron:\n            rec.seq = iron(rec.seq)\n        block['sequences'].append(seqrecord2sequence(rec, qlen, index))\n\n    return block"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iron(sequence):\n    r_indel = re.compile(r'(-[a-y]|[a-y]-)')\n    orig_sequence = sequence\n    while r_indel.search(sequence):\n        in_insert = False\n        in_gap = False\n        seen_gaps = 0\n        inserts = []\n        outchars = []\n\n        for char in sequence:\n            if in_insert:\n                if char.islower():\n                    # Extend the insert\n                    inserts.append(char)\n                elif char.isupper():\n                    # Indel is over; 'iron' out & emit inserts, then gaps\n                    in_insert = False\n                    outchars.extend(inserts)\n                    inserts = []\n                    outchars.append('-' * seen_gaps)\n                    seen_gaps = 0\n                    outchars.append(char)\n                else:\n                    # Convert a preceding indel char to a 'match' (uppercase)\n                    # If the indel and gap are both multiple chars, this will\n                    # capitalize the insert left-to-right, then leave any gap\n                    # remainer as-is.\n                    assert char == '-'\n                    if not inserts:\n                        in_insert = False\n                        in_gap = True\n                        seen_gaps += 1\n                    else:\n                        outchars.append(inserts.pop(0).upper())\n                        # NB: Only leave the insert region if we've finished\n                        # converting all the insert chars\n                        if not inserts:\n                            in_insert = False\n                            in_gap = True\n\n            elif in_gap:\n                if char.islower():\n                    in_insert = True\n                    in_gap = False\n                    # If some inserts previously seen, emit them now\n                    # If no inserts have been seen yet, we'll iron this indel\n                    if inserts:\n                        outchars.extend(inserts)\n                        outchars.append('-' * seen_gaps)\n                        seen_gaps = 0\n                    inserts = [char]\n                elif char.isupper():\n                    in_gap = False\n                    # End of the gap -- emit\n                    if inserts:\n                        outchars.extend(inserts)\n                        inserts = []\n                    outchars.append('-' * seen_gaps)\n                    seen_gaps = 0\n                    outchars.append(char)\n                else:\n                    # Extend the gap\n                    assert char == '-'\n                    seen_gaps += 1\n\n            else:\n                assert not inserts and not seen_gaps, (\n                    \"Inserts: %s, gaps: %s, seq: %s, in_ins=%s, in_gap=%s\"\n                    % (inserts, seen_gaps, sequence, in_insert, in_gap))\n                # Coming from Match state\n                if char.isupper():\n                    # Extend the match\n                    outchars.append(char)\n                elif char.islower():\n                    inserts.append(char)\n                    in_insert = True\n                else:\n                    assert char == '-'\n                    seen_gaps += 1\n                    in_gap = True\n\n        # Emit any trailing indel\n        if inserts:\n            outchars.extend(inserts)\n        if seen_gaps:\n            outchars.append('-' * seen_gaps)\n        sequence = ''.join(outchars)\n        # logging.info(sequence)\n        assert (sequence.replace('-', '').upper()\n                ==\n                orig_sequence.replace('-', '').upper()), \\\n                '\\nOrig: ' + orig_sequence + \\\n                '\\nIron: ' + sequence\n    return sequence", "response": "Iron out indel regions in a CMA string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving text files from a github repo", "response": "def get_github_content(repo,path,auth=None):\n\t\"\"\"\n\tRetrieve text files from a github repo\n\t\"\"\"\n\trequest = requests.get(file_url.format(repo=repo, path=path), auth=auth)\n\tif not request.ok:\n\t\tprint(\"There is a problem with the request\")\n\t\tprint(file_url.format(repo=repo, path=path))\n\t\tprint(request.json())\n\t\texit(1)\n\tif not request.json()['encoding'] == 'base64':\n\t\traise RuntimeError(\"Unknown Encoding encountered when fetching {} from repo {}: {}\".format(path,repo,request.json()['encoding']))\n\treturn request.json()['content'].decode('base64').decode('utf8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncollects the repos to consider by default from the contents of the working directory.", "response": "def collect_reponames():\n\t\"\"\"\n\tTry to figure out a list of repos to consider by default from the contents of the working directory.\n\t\"\"\"\n\treponames = []\n\n\t#try to figure out the repo from git repo in current directory\n\ttry:\n\t\twith open(os.devnull) as devnull:\n\t\t\tremote_data = subprocess.check_output([\"git\",\"remote\",\"-v\",\"show\"],stderr=devnull)\n\t\tbranches = {}\n\t\tfor line in remote_data.decode('utf-8').split(\"\\n\"):\n\t\t\tif line.strip() == \"\":\n\t\t\t\tcontinue\n\t\t\tremote_match = re_mote.match(line)\n\t\t\tif not remote_match is None:\n\t\t\t\tbranches[remote_match.group(1)] = remote_match.group(5)\n\t\tif len(branches) > 0:\n\t\t\tif \"origin\" in branches:\n\t\t\t\treponames.append(branches[\"origin\"])\n\t\t\telse:\n\t\t\t\treponames.append(branches.values()[0])\n\texcept OSError:\n\t\tpass\n\texcept subprocess.CalledProcessError:\n\t\tpass\n\n\t#scan html files for further repos to consider\n\tfor fname in glob.iglob(\"*.html\"):\n\t\tfid = open(fname,\"r\",\"utf8\")\n\t\t#check the second line for the repo marker\n\t\tfid.readline()\n\t\tline = fid.readline()\n\t\tmatch = re.match(repo_marker_re,line)\n\t\tif not match is None:\n\t\t\treponames.append(match.group(1))\n\n\t\treponames = list(set(reponames))\n\treturn reponames"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload Github configuration such as usernames from the local or global git config", "response": "def collect_github_config():\n\t\"\"\"\n\tTry load Github configuration such as usernames from the local or global git config\n\t\"\"\"\n\tgithub_config = {}\n\tfor field in [\"user\", \"token\"]:\n\t\ttry:\n\t\t\tgithub_config[field] = subprocess.check_output([\"git\", \"config\", \"github.{}\".format(field)]).decode('utf-8').strip()\n\t\texcept (OSError, subprocess.CalledProcessError):\n\t\t\tpass\n\treturn github_config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the stimulus model for the calibration curve test", "response": "def setCurveModel(self, model):\n        \"\"\"Sets the stimulus model for the calibration curve test\n\n        :param model: Stimulus model that has a tone curve configured\n        :type model: :class:`StimulusModel <sparkle.stim.stimulus_model.StimulusModel>`\n        \"\"\"\n        self.stimModel = model\n        self.ui.curveWidget.setModel(model)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a stimulus to the list of stims to use for testing calibration", "response": "def addOption(self, stim):\n        \"\"\"Adds a stimulus to the list of stims to use for testing calibration\n\n        :param stim: stimulus to add to drop-down list\n        :type stim: :class:`AbstractStimulusComponent<sparkle.stim.abstract_component.AbstractStimulusComponent>`\n        \"\"\"\n        # set the editor widgets for noise and sweep\n        self.ui.calTypeCmbbx.insertItem(0,stim.name)\n        editor = stim.showEditor()\n        # should probably make this less coupled\n        durInput = editor.durationInputWidget()\n        self.durationWidgets.append(durInput)\n        durInput.setEnabled(False)\n        self.ui.caleditorStack.insertWidget(0, editor)\n        self.ui.calTypeCmbbx.setCurrentIndex(0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef saveToObject(self):\n        for i in range(self.ui.caleditorStack.count()):\n            try:\n                self.ui.caleditorStack.widget(i).saveToObject()\n            except AttributeError:\n                logger = logging.getLogger('main')\n                logger.debug('index {} does not have method saveToObject'.format(i))", "response": "Saves the current UI setting to the model"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isToneCal(self):\n        return self.ui.calTypeCmbbx.currentIndex() == self.ui.calTypeCmbbx.count() -1", "response": "Returns true if the currently selected calibration stimulus type is the calibration curve"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_stim(self, signal, fs, attenuation=0):\n\n        self.tone_lock.acquire()\n        self.stim = signal\n        self.fs = fs\n        self.atten = attenuation\n        self.stim_changed = True\n\n        self.tone_lock.release()", "response": "Sets any vector as the next stimulus to be output. Does not call write to hardware"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect_attenuator(self, connect=True):\n        if connect:\n            try:\n                pa5 = win32com.client.Dispatch(\"PA5.x\")\n                success = pa5.ConnectPA5('GB', 1)\n                if success == 1:\n                    print 'Connection to PA5 attenuator established'\n                    pass\n                else:\n                    print 'Connection to PA5 attenuator failed'\n                    errmsg = pa5.GetError()\n                    print u\"Error: \", errmsg\n                    raise Exception(u\"Attenuator connection failed\")\n            except:\n                print \"Error connecting to attenuator\"\n                pa5 = None\n\n            self.attenuator = pa5\n        else:\n            # if there is an attenuator, make sure it is set to 0 before disconnecting\n            if self.attenuator:\n                self.attenuator.setAtten(0)\n            self.attenuator = None\n        return self.attenuator", "response": "Establish a connection to the TDT PA5 attenuator and return the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_timer(self, reprate):\n        print 'starting digital output at rate {} Hz'.format(reprate)\n        self.trigger_task = DigitalOutTask(self.trigger_src, reprate)\n        self.trigger_task.start()", "response": "Start the digital output task that serves as the acquistion trigger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite output buffer and settings to device", "response": "def start(self):\n        \"\"\"Writes output buffer and settings to device\n\n        :returns: numpy.ndarray -- if the first presentation of a novel stimulus, or None if a repeat stimulus\n        \"\"\"\n\n        # this shouldn't actually be possible still...\n        if self.aitask is not None:\n            self.stop()\n            raise Exception(\"FIX ME : NESTED START OPERATIONS ALLOWED\")\n\n        self.daq_lock.acquire()\n\n        self.ngenerated = 0\n        self.nacquired = 0\n\n        return self.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbegin simultaneous generation of the specified calendar and reads the data from the AOTTask.", "response": "def run(self):\n        \"\"\"Begins simultaneous generation/acquisition\n\n        :returns: numpy.ndarray -- read samples\n        \"\"\"\n        try:\n            if self.aotask is None:\n                print u\"You must arm the calibration first\"\n                return\n            # acquire data and stop task, lock must have been release by\n            # previous reset\n            self.daq_lock.acquire()\n            self.aotask.StartTask()\n            self.aitask.StartTask()\n\n            # blocking read\n            data = self.aitask.read()\n\n            # write task should always be shorter than read\n            # self.aotask.WaitUntilTaskDone(10)\n\n            self.nacquired += 1\n            \n            self.aitask.stop()\n            self.aotask.stop()\n            \n        except:\n            print u'ERROR! TERMINATE!'\n            self.daq_lock.release()\n            self.stop()\n            raise\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrearm the gen and acq task to the same channels as before", "response": "def reset(self):\n        \"\"\"Rearms the gen/acq task, to the same channels as before\"\"\"\n\n        response_npts = int(self.aitime*self.aifs)\n        try:\n            self.aitask = AITaskFinite(self.aichan, self.aifs, response_npts, trigsrc=self.trigger_dest)\n            new_gen = self.reset_generation(u\"ai/StartTrigger\")\n        except:\n            print u'ERROR! TERMINATE!'\n            self.daq_lock.release()\n            self.stop()\n            raise\n\n        self.daq_lock.release()\n        return new_gen"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(self):\n        try:\n            self.aitask.stop()\n            self.aotask.stop()\n            pass\n        except:     \n            print u\"No task running\"\n        self.aitask = None\n        self.aotask = None", "response": "Halts the acquisition, this must be called before resetting acquisition"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_continuous(self, aichans, update_hz=10):\n        self.daq_lock.acquire()\n\n        self.ngenerated = 0 # number of stimuli presented during chart run\n        npts = int(self.aifs/update_hz) #update display at 10Hz rate\n        nchans = len(aichans)\n        self.aitask = AITask(aichans, self.aifs, npts*5*nchans)\n        self.aitask.register_callback(self._read_continuous, npts)\n        self.aitask.start()", "response": "Starts a continuous analog generation of the specified aichans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        self.aotask.StartTask()\n        self.aotask.wait() # don't return until generation finished\n        self.aotask.stop()\n        self.aotask = None", "response": "Executes the stimulus generation and returns when completed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_all(self):\n        if self.aotask is not None:\n            self.aotask.stop()\n        self.aitask.stop()\n        self.daq_lock.release()\n        self.aitask = None\n        self.aotask = None", "response": "Halts both the analog output and input tasks"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, db):\n        dbnode = ET.Element('database')\n    \n        if self.include_comment:\n            now = datetime.now()\n            filepath = db.filepath\n            if filepath:\n                comment = ET.Comment('Generated by keepassdb from {0} on {1}'.format(filepath, now.strftime(\"%c\")))\n            else:\n                comment = ET.Comment('Generated by keepassdb on {0}'.format(now.strftime(\"%c\")))\n            dbnode.append(comment)\n        \n        def _date(dt):\n            if dt == const.NEVER:\n                return 'Never'\n            else:\n                # 2012-12-20T20:56:56\n                return dt.strftime('%Y-%m-%dT%H:%M:%S')\n            \n        def group_to_xml(group, node):\n            gnode = ET.SubElement(node, 'group')\n            title = ET.SubElement(gnode, 'title')\n            title.text = group.title\n            icon = ET.SubElement(gnode, 'icon')\n            icon.text = str(group.icon)\n            \n            for subgroup in group.children:\n                group_to_xml(subgroup, gnode)\n            \n            for entry in group.entries:\n                if entry.title == 'Meta-Info' and entry.username == 'SYSTEM':\n                    continue\n                enode = ET.SubElement(gnode, 'entry')\n                ET.SubElement(enode, 'title').text = entry.title\n                ET.SubElement(enode, 'username').text = entry.username\n                ET.SubElement(enode, 'password').text = entry.password\n                ET.SubElement(enode, 'url').text = entry.url\n                ET.SubElement(enode, 'comment').text = entry.notes\n                ET.SubElement(enode, 'icon').text = str(entry.icon)\n                ET.SubElement(enode, 'creation').text = _date(entry.created)\n                ET.SubElement(enode, 'lastaccess').text = _date(entry.accessed)\n                ET.SubElement(enode, 'lastmod').text = _date(entry.modified)\n                ET.SubElement(enode, 'expire').text = _date(entry.expires)\n            return gnode\n        \n        for group in db.root.children:\n            dbnode.append(group_to_xml(group, dbnode))\n            \n        xmlstr = ET.tostring(dbnode)\n        if self.prettyprint:\n            reparsed = minidom.parseString(xmlstr)\n            xmlstr = reparsed.toprettyxml(indent=\" \")\n        \n        return xmlstr", "response": "Export the database node to KeePassX XML format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, url, params=None, raw=False, stream=False, **request_kwargs):\n\n        full_url = self.build_url(url)\n        params = params or {}\n\n        # Add token (if it's not already there)\n        if self._token:\n            params.setdefault('token', self._token)\n\n        response = requests.get(full_url, params=params, stream=stream,\n                                **request_kwargs)\n        self.check_for_errors(response)  # Raise exception if something failed\n\n        if stream:\n            return response\n        if raw or not response.content:\n            return response.content\n        return json.loads(response.text)", "response": "A method to make a GET request to AmigoCloud endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nposts request to AmigoCloud endpoint.", "response": "def post(self, url, data=None, files=None, headers=None, raw=False,\n             send_as_json=True, content_type=None, **request_kwargs):\n        \"\"\"\n        POST request to AmigoCloud endpoint.\n        \"\"\"\n\n        return self._secure_request(\n            url, 'post', data=data, files=files, headers=headers, raw=raw,\n            send_as_json=send_as_json, content_type=content_type,\n            **request_kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_file(self, simple_upload_url, chunked_upload_url, file_obj,\n                    chunk_size=CHUNK_SIZE, force_chunked=False,\n                    extra_data=None):\n        \"\"\"\n        Generic method to upload files to AmigoCloud. Can be used for different\n        API endpoints.\n        `file_obj` could be a file-like object or a filepath.\n        If the size of the file is greater than MAX_SIZE_SIMPLE_UPLOAD (8MB)\n        `chunked_upload_url` will be used, otherwise `simple_upload_url` will\n        be.\n        If `simple_upload_url` evaluates to False, or `force_chunked` is True,\n        the `chunked_upload_url` will always be used.\n        \"\"\"\n\n        if isinstance(file_obj, string_types):\n            # file_obj is a filepath: open file and close it at the end\n            file_obj = open(file_obj, 'rb')\n            close_file = True\n        else:\n            # assume file_obj is a file-like object\n            close_file = False\n\n        # Get file size\n        file_obj.seek(0, os.SEEK_END)\n        file_size = file_obj.tell()\n        file_obj.seek(0)\n\n        try:\n            # Simple upload?\n            if (simple_upload_url and not force_chunked\n                    and file_size < MAX_SIZE_SIMPLE_UPLOAD):\n                return self.post(simple_upload_url, data=extra_data,\n                                 files={'datafile': file_obj})\n            # Chunked upload\n            data = {}\n            md5_hash = hashlib.md5()\n            start_byte = 0\n            while True:\n                chunk = file_obj.read(chunk_size)\n                md5_hash.update(chunk)\n                end_byte = start_byte + len(chunk) - 1\n                content_range = 'bytes %d-%d/%d' % (start_byte, end_byte,\n                                                    file_size)\n                ret = self.post(chunked_upload_url, data=data,\n                                files={'datafile': chunk},\n                                headers={'Content-Range': content_range})\n                data.setdefault('upload_id', ret['upload_id'])\n                start_byte = end_byte + 1\n                if start_byte == file_size:\n                    break\n            # Complete request\n            if chunked_upload_url.endswith('/'):\n                chunked_upload_complete_url = chunked_upload_url + 'complete'\n            else:\n                chunked_upload_complete_url = chunked_upload_url + '/complete'\n            data['md5'] = md5_hash.hexdigest()\n            if extra_data:\n                data.update(extra_data)\n            return self.post(chunked_upload_complete_url, data=data)\n        finally:\n            if close_file:\n                file_obj.close()", "response": "Uploads a file to AmigoCloud."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload_datafile(self, project_owner, project_id, file_obj,\n                        chunk_size=CHUNK_SIZE, force_chunked=False):\n        \"\"\"\n        Upload datafile to a project. The file must be a supported format or a\n        zip file containing supported formats.\n        To see the formats we support, go to this URL:\n        http://help.amigocloud.com/hc/en-us/articles/202413410-Supported-Format\n        \"\"\"\n\n        simple_upload_url = 'users/%s/projects/%s/datasets/upload' % (\n            project_owner, project_id\n        )\n        chunked_upload_url = 'users/%s/projects/%s/datasets/chunked_upload' % (\n            project_owner, project_id\n        )\n\n        return self.upload_file(simple_upload_url, chunked_upload_url,\n                                file_obj, chunk_size=chunk_size,\n                                force_chunked=force_chunked)", "response": "Uploads a file to a project."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upload_gallery_photo(self, gallery_id, source_amigo_id, file_obj,\n                             chunk_size=CHUNK_SIZE, force_chunked=False,\n                             metadata=None):\n        \"\"\"\n        Upload a photo to a dataset's gallery.\n        \"\"\"\n\n        simple_upload_url = 'related_tables/%s/upload' % gallery_id\n        chunked_upload_url = 'related_tables/%s/chunked_upload' % gallery_id\n\n        data = {'source_amigo_id': source_amigo_id}\n        if isinstance(file_obj, basestring):\n            data['filename'] = os.path.basename(file_obj)\n        else:\n            data['filename'] = os.path.basename(file_obj.name)\n        if metadata:\n            data.update(metadata)\n\n        return self.upload_file(simple_upload_url, chunked_upload_url,\n                                file_obj, chunk_size=chunk_size,\n                                force_chunked=force_chunked, extra_data=data)", "response": "Uploads a photo to a dataset s gallery."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nauthenticating to start listening to user events.", "response": "def listen_user_events(self):\n        \"\"\"\n        Authenticate to start listening to user events.\n        \"\"\"\n\n        if not self._user_id:\n            raise AmigoCloudError(self.error_msg['logged_in_websockets'])\n\n        response = self.get('/me/start_websocket_session')\n        websocket_session = response['websocket_session']\n        auth_data = {'userid': self._user_id,\n                     'websocket_session': websocket_session}\n        self.amigosocket.emit('authenticate', auth_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nauthenticate to start using dataset events.", "response": "def listen_dataset_events(self, owner_id, project_id, dataset_id):\n        \"\"\"\n        Authenticate to start using dataset events.\n        \"\"\"\n\n        if not self._user_id:\n            raise AmigoCloudError(self.error_msg['logged_in_websockets'])\n\n        url = '/users/%s/projects/%s/datasets/%s/start_websocket_session'\n        response = self.get(url % (owner_id, project_id, dataset_id))\n        websocket_session = response['websocket_session']\n        auth_data = {'userid': self._user_id,\n                     'datasetid': dataset_id,\n                     'websocket_session': websocket_session}\n        self.amigosocket.emit('authenticate', auth_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_markdown_table(headers, rows, row_keys=None):\n    row_maxes = _find_row_maxes(headers, rows)\n    row_keys = row_keys or [key for key, value in headers.items()]\n    table = [\n        _build_row(headers, row_maxes, row_keys),\n        _build_separator(row_maxes, row_keys)\n    ]\n\n    for row in rows:\n        table.append(_build_row(row, row_maxes, row_keys))\n    return '\\n'.join(table) + '\\n'", "response": "Builds a markdown table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_markdown_doc(app_name, spec):\n\n    # Apply standard headers.\n    sections = [\n        HEADER.format(app_name=app_name),\n        SOURCES_HEADER.format(app_name=app_name)\n    ]\n\n    # Generate the sources section of the documentation\n    sorted_labels = sorted(list(spec.sources))\n    for label in sorted_labels:\n        sections.append(\n            _generate_source_section(label, spec.sources[label], app_name)\n        )\n\n    # Generate the config section.\n    sections.append(CONFIG_HEADER.format(app_name=app_name))\n    table_rows, item_sections = _generate_item_sections(\n        _sorted_dict_values(spec.items),\n        app_name\n    )\n\n    headers = {\n        'name': 'Name',\n        'type': 'Type',\n        'default': 'Default',\n        'description': 'Description'\n    }\n\n    sections.append(\n        build_markdown_table(\n            headers,\n            table_rows,\n            ['name', 'type', 'default', 'description'],\n        )\n    )\n    for item_section in item_sections:\n        sections.append(item_section)\n\n    return '\\n'.join([section for section in sections])", "response": "Generate a markdown string representation of the given spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noutputs the data the dataframe s image column to a directory structured by project -> sample and named by frame_name and write it to the path.", "response": "def write_to_path(self,path,suffix='',format='png',overwrite=False):\n        \"\"\"\n        Output the data the dataframe's 'image' column to a directory structured by project->sample and named by frame\n\n        Args:\n            path (str): Where to write the directory of images\n            suffix (str): for labeling the imaages you write\n            format (str): default 'png' format to write the file\n            overwrite (bool): default False. if true can overwrite files in the path\n\n        Modifies:\n            Creates path folder if necessary and writes images to path\n        \"\"\"\n        if os.path.exists(path) and overwrite is False: raise ValueError(\"Error: use ovewrite=True to overwrite images\")\n        if not os.path.exists(path): os.makedirs(path)\n        for i,r in self.iterrows():\n            spath = os.path.join(path,r['project_name'],r['sample_name'])\n            if not os.path.exists(spath): os.makedirs(spath)\n            if suffix == '':\n                fname = os.path.join(spath,r['frame_name']+'.'+format)\n            else: fname = os.path.join(spath,r['frame_name']+'_'+suffix+'.'+format)\n            imageio.imwrite(fname, r['image'],format=format)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_segmentation_image(self,schema,background=(0,0,0,0)):\n        cummulative = self.copy()\n        def _set_blank(img,blank):\n            img[:][:] = blank\n            return img\n        cummulative['merged'] = cummulative.apply(lambda x: \n            _set_blank(np.zeros(list(x['shape'])+[4]),background)\n            ,1)\n        for layer in schema:\n            if self.verbose: sys.stderr.write(\"Calculating layer \"+str(layer)+\"\\n\")\n            images  = self.get_outline_images(subset_logic=layer['subset_logic'],\n                                              edge_color=layer['edge_color'],\n                                              watershed_steps=layer['watershed_steps'],\n                                              fill_color=layer['fill_color'])\n            cummulative = cummulative.rename(columns={'merged':'old'})\n            cummulative = cummulative.merge(images,on=list(self.columns))\n            cummulative['new'] = cummulative.apply(lambda x: _merge_images(x['merged'],x['old']),1)\n            cummulative = cummulative.drop(columns=['old','merged']).rename(columns={'new':'merged'})\n        cummulative = cummulative.rename(columns={'merged':'image'})\n        return SegmentationImageOutput(cummulative)", "response": "Builds a segmentation image for the given schema and background."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef valid(number):\n    checksum = 0\n\n    number_len = len(number)\n    offset = ord('0')\n\n    i = number_len - 1\n    while i >= 0:\n      n = ord(number[i]) - offset\n      checksum += n\n      i -= 2\n\n    i = number_len - 2\n    while i >= 0:\n      n = ord(number[i]) - offset\n      n *= 2\n      if n > 9:\n          n -= 9\n      checksum += n\n      i -= 2\n\n    return checksum%10 == 0", "response": "Returns true if the number string is luhn valid and false otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an assistant for a dataset that allows to make requests for the dataset and all of its files.", "response": "def create_publication_assistant(self, **args):\n        '''\n        Create an assistant for a dataset that allows to make PID\n        requests for the dataset and all of its files.\n\n        :param drs_id: Mandatory. The dataset id of the dataset\n            to be published.\n\n        :param version_number: Mandatory. The version number of the\n            dataset to be published.\n\n        :param is_replica: Mandatory. Flag to indicate whether the\n            dataset is a replica.\n\n        .. note:: If the replica flag is set to False, the publication\n            may still be considered a replica by the consuming servlet,\n            namely if the dataset was already published at a different\n            host. For this, please refer to the consumer documentation.\n\n        :return: A publication assistant which provides all necessary\n            methods to publish a dataset and its files.\n        '''\n\n        # Check args\n        logdebug(LOGGER, 'Creating publication assistant..')\n        mandatory_args = ['drs_id', 'version_number', 'is_replica']\n        esgfpid.utils.check_presence_of_mandatory_args(args, mandatory_args)\n        # Check if service path is given\n        if self.__thredds_service_path is None:\n            msg = 'No thredds_service_path given (but it is mandatory for publication)'\n            logwarn(LOGGER, msg)\n            raise esgfpid.exceptions.ArgumentError(msg)\n        # Check if data node is given\n        if self.__data_node is None:\n            msg = 'No data_node given (but it is mandatory for publication)'\n            logwarn(LOGGER, msg)\n            raise esgfpid.exceptions.ArgumentError(msg)\n\n        # Check if solr has access:\n        if self.__coupler.is_solr_switched_off():\n            pass # solr access not mandatory anymore\n\n        # Create publication assistant\n        assistant = esgfpid.assistant.publish.DatasetPublicationAssistant(\n            drs_id=args['drs_id'],\n            version_number=args['version_number'],\n            thredds_service_path=self.__thredds_service_path,\n            data_node=self.__data_node,\n            prefix=self.prefix,\n            coupler=self.__coupler,\n            is_replica=args['is_replica'],\n            consumer_solr_url=self.__consumer_solr_url # may be None\n        )\n        logdebug(LOGGER, 'Creating publication assistant.. done')\n        return assistant"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unpublish_one_version(self, **args):\n        '''\n        Sends a PID update request for the unpublication of one version\n        of a dataset currently published at the given data node.\n\n        Either the handle or the pair of drs_id and version_number\n        have to be provided, otherwise an exception will occur.\n\n        The consumer will of course check the PID request message's\n        timestamp with the timestamp of the last publication, so that\n        republications in the mean time are not unpublished.\n\n        The unpublication of the files is included in this method.\n\n        :param handle: Optional. The handle of the dataset\n            to be unpublished.\n\n        :param drs_id: Optional. The dataset id of the dataset\n            to be unpublished.\n\n        :param version_number: Optional. The version number of\n            the dataset to be unpublished.\n\n        :raises: ArgumentError: If not enough arguments are passed\n            to identify the dataset, or if no data node was specified\n            during library init.\n\n        '''\n\n        # Check args\n        optional_args = ['handle', 'drs_id', 'version_number']\n        esgfpid.utils.add_missing_optional_args_with_value_none(args, optional_args)\n\n        # Check if data node is given\n        if self.__data_node is None:\n            msg = 'No data_node given (but it is mandatory for unpublication)'\n            logwarn(LOGGER, msg)\n            raise esgfpid.exceptions.ArgumentError(msg)\n\n        # Unpublish\n        assistant = esgfpid.assistant.unpublish.AssistantOneVersion(\n            drs_id = args['drs_id'],\n            data_node = self.__data_node,\n            prefix=self.prefix,\n            coupler=self.__coupler,\n            message_timestamp=esgfpid.utils.get_now_utc_as_formatted_string()\n        )\n        assistant.unpublish_one_dataset_version(\n            handle = args['handle'],\n            version_number = args['version_number']\n        )", "response": "Unpublishes one version of a dataset currently published at the given data node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a PID update request for the unpublication of all versions of a dataset currently published at the given data node. If the library has solr access, it will try to find all the dataset versions and their handles from solr, and send individual messages for each version. Otherwise, one message is sent, and the queue consuming servlet has to identify the relevant versions, also making sure not to unpublish any versions that may have been republished in the meantime. :param drs_id: Dataset id of the dataset to be unpublished. :raises: ArgumentError: If the data node was not provided at library initialization.", "response": "def unpublish_all_versions(self, **args):\n        '''\n        Sends a PID update request for the unpublication of all versions\n        of a dataset currently published at the given data node.\n\n        If the library has solr access, it will try to find all the\n        dataset versions and their handles from solr, and send individual\n        messages for each version. Otherwise, one message is sent, and the\n        queue consuming servlet has to identify the relevant versions,\n        also making sure not to unpublish any versions that may have been\n        republished in the meantime.\n\n        :param drs_id: Dataset id of the dataset to be unpublished.\n        :raises: ArgumentError: If the data node\n                 was not provided at library initialization.\n\n        '''\n\n        # Check args\n        mandatory_args = ['drs_id']\n        esgfpid.utils.check_presence_of_mandatory_args(args, mandatory_args)\n\n        # Check if data node is given\n        if self.__data_node is None:\n            msg = 'No data_node given (but it is mandatory for publication)'\n            logwarn(LOGGER, msg)\n            raise esgfpid.exceptions.ArgumentError(msg)\n\n        # Check if solr has access:\n        if self.__coupler.is_solr_switched_off():\n            msg = 'Unpublication of all versions. Without solr access, we cannot identify the versions, so the consumer will have to take care of this.'\n            logdebug(LOGGER, msg)\n            #raise esgfpid.exceptions.ArgumentError('No solr access. Solr access is needed for publication. Please provide access to a solr index when initializing the library')\n\n        # Unpublish\n        assistant = esgfpid.assistant.unpublish.AssistantAllVersions(\n            drs_id = args['drs_id'],\n            data_node = self.__data_node,\n            prefix=self.prefix,\n            coupler=self.__coupler,\n            message_timestamp=esgfpid.utils.get_now_utc_as_formatted_string(),\n            consumer_solr_url = self.__consumer_solr_url # may be None\n        )\n        assistant.unpublish_all_dataset_versions()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds errata ids to a dataset handle record.", "response": "def add_errata_ids(self, **args):\n        '''\n        Add errata ids to a dataset handle record.\n\n        To call this method, you do not need to provide the\n        PID of the dataset. Instead, the PID string is derived\n        from the dataset id and the version number.\n\n        :param errata_ids: Mandatory. A list of errata ids (strings)\n            to be added to the handle record.\n\n        :param drs_id: Mandatory. The dataset id of the dataset\n            to whose handle record the errata ids are to be\n            added. (This is needed because the handle is found\n            by making a hash over dataset id and version number).\n\n        :param version_number: Mandatory. The version number of the\n            dataset to whose handle record the errata ids are to be\n            added. (This is needed because the handle is found by\n            making a hash over dataset id and version number).\n        '''\n\n        # Check args:\n        mandatory_args = ['drs_id', 'version_number', 'errata_ids']\n        esgfpid.utils.check_presence_of_mandatory_args(args, mandatory_args)\n        esgfpid.utils.check_noneness_of_mandatory_args(args, mandatory_args)\n\n        # Perform metadata update\n        assistant = esgfpid.assistant.errata.ErrataAssistant(\n            coupler=self.__coupler,\n            prefix=self.prefix\n        )\n        assistant.add_errata_ids(\n            drs_id=args['drs_id'],\n            version_number=args['version_number'],\n            errata_ids=args['errata_ids']\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_data_cart_pid(self, dict_of_drs_ids_and_pids):\n        '''\n        Create a handle record for a data cart (a custom set of datasets).\n\n        The handle string is made of the prefix passed to tbe library,\n        and a hash over all the dataset ids in the cart. This way, if exactly\n        the same set of datasets is passed several times, the same handle\n        record is created, instead of making a new one.\n\n        :param dict_of_drs_ids_and_pids: A dictionary of all dataset ids\n            and their pid strings. If a dataset has no (known) PID, use\n            \"None\".\n\n        :return: The handle string for this data cart.\n        '''\n        assistant = esgfpid.assistant.datacart.DataCartAssistant(\n            prefix=self.prefix,\n            coupler=self.__coupler\n        )\n        return assistant.make_data_cart_pid(dict_of_drs_ids_and_pids)", "response": "Create a handle string for a data cart."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_handle_from_drsid_and_versionnumber(self, **args):\n        '''\n        Create a handle string for a specific dataset, based\n        on its dataset id and version number, and the prefix\n        passed to the library at initializing.\n\n        :param drs_id: The dataset id of the dataset.\n        :param version_number: The version number of the dataset\n            (as a string or integer, this does not matter)\n        :return: A handle string (e.g. \"hdl:21.14100/abcxyzfoo\")\n        '''\n        args['prefix'] = self.prefix\n        return esgfpid.utils.make_handle_from_drsid_and_versionnumber(**args)", "response": "Create a handle string for a specific dataset based on its dataset id and version number and the prefix passed to the library at initializing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mousePressEvent(self, event):\n        super(AbstractDragView, self).mousePressEvent(event)\n        self.dragStartPosition = event.pos()", "response": "saves the drag position so we know when a drag should be initiated"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a drag is taking place and initiates it", "response": "def mouseMoveEvent(self, event):\n        \"\"\"Determines if a drag is taking place, and initiates it\"\"\"\n        super(AbstractDragView, self).mouseMoveEvent(event)\n        if self.dragStartPosition is None or \\\n            (event.pos() - self.dragStartPosition).manhattanLength() < QtGui.QApplication.startDragDistance():\n            # change cursor to reflect actions for what its hovering on\n            index = self.indexAt(event.pos())\n            cursor = self.model().data(index, CursorRole)\n            self.setCursor(cursor)\n            return\n        # mouse has been dragged past a threshold distance\n\n        index = self.indexAt(self.dragStartPosition)\n        if not index.isValid():\n            return\n        # grab the pixmap first, as it may be cleared from component,\n        # and slows GUI due to redraw.\n        pixmap = self.grabImage(index)\n\n        # get the item at the drug index\n        selected = self.model().data(index, self.DragRole)\n        if selected is None:\n            return\n            \n        ## convert to  a bytestream\n        bstream = cPickle.dumps(selected)\n        mimeData = QtCore.QMimeData()\n        mimeData.setData(\"application/x-protocol\", bstream)\n\n        # save this component in case the drag ends not in a droppable region, \n        # and we want to return it to it's original place\n        self.limbo_component = selected\n        self.originalPos = index\n        \n        drag = QtGui.QDrag(self)\n        drag.setMimeData(mimeData)\n\n        # this makes the pixmap half transparent\n        painter = QtGui.QPainter(pixmap)\n        painter.setCompositionMode(painter.CompositionMode_DestinationIn)\n        painter.fillRect(pixmap.rect(), QtGui.QColor(0, 0, 0, 127))\n        painter.end()\n        \n        drag.setPixmap(pixmap)\n\n        x, y = self.indexXY(index)\n        drag.setHotSpot(QtCore.QPoint(event.x()-x, event.y()-y))\n        # drag.setHotSpot(QtCore.QPoint(pixmap.width()/2, pixmap.height()/2))\n        drag.setPixmap(pixmap)\n\n        self.model().removeItem(index)\n        result = drag.exec_(QtCore.Qt.MoveAction)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dragEnterEvent(self, event):\n        super(AbstractDragView, self).dragEnterEvent(event)\n        if event.mimeData().hasFormat(\"application/x-protocol\"):\n            event.setDropAction(QtCore.Qt.MoveAction)\n            event.accept()\n        else:\n            event.ignore()", "response": "Determines if the mouse can recieve the drop"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if the mouse can recieve the drop", "response": "def dragMoveEvent(self, event):\n        \"\"\"Determines if the widget under the mouse can recieve the drop\"\"\"\n        super(AbstractDragView, self).dragMoveEvent(event)\n        if event.mimeData().hasFormat(\"application/x-protocol\"):\n            # find the nearest break to cursor\n            self.dragline = self.cursor(event.pos())\n            self.viewport().update()\n            event.setDropAction(QtCore.Qt.MoveAction)\n            event.accept()\n        else:\n            event.ignore()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears drop cursor line", "response": "def dragLeaveEvent(self, event):\n        \"\"\"Clears drop cursor line\"\"\"\n        super(AbstractDragView, self).dragLeaveEvent(event)\n        self.dragline = None\n        self.viewport().update()\n        event.accept()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dropEvent(self, event):\n        super(AbstractDragView, self).dropEvent(event)\n        self.dragStartPosition = None\n        self.dragline = None\n        self.originalPos = None\n        data = event.mimeData()\n        stream = data.retrieveData(\"application/x-protocol\",\n            QtCore.QVariant.ByteArray)\n        item = cPickle.loads(str(stream.toByteArray()))\n\n        self.dropped(item, event)\n\n        event.accept()", "response": "Handles an item being dropped onto view calls the drop method in order to handle it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef childEvent(self, event):    \n        super(AbstractDragView, self).childEvent(event)\n        if event.type() == QtCore.QEvent.ChildRemoved:\n            # hack to catch drop offs   \n            if self.originalPos is not None:\n                selected = self.limbo_component\n                self.model().insertItem(self.originalPos, selected)\n                self.originalPos = None\n                self.dragStartPosition = None\n                self.viewport().update()", "response": "Catches items dropped off edge of view and reinserts at original position"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mouseReleaseEvent(self, event):\n        super(AbstractDragView, self).mouseReleaseEvent(event)\n        self.dragStartPosition = None", "response": "Resets the drag start position"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def setup(self):\n        try:\n            db = await self.db\n            collections = await db.list_collection_names()\n            created = False\n            if self.table_name not in collections:\n                # create table\n                logger.info(\"Creating MongoDB collection [{}]\".format(self.table_name))\n                await db.create_collection(self.table_name)\n                await db[self.table_name].create_index([(\"target_id\", DESCENDING), (\"post_id\", DESCENDING)])\n                created = True\n            # create control collection if not already created.\n            if self.control_table_name and self.control_table_name not in collections:\n                # create table\n                logger.info(\"Creating MongoDB control data collection [{}]\".format(self.control_table_name))\n                await db.create_collection(self.control_table_name)\n                created = True\n            return created\n        except Exception as exc:\n            logger.error(\"[DB] Error when setting up MongoDB collections: {}\".format(exc))\n        return False", "response": "Setup MongoDB collections if they not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the documentation to display", "response": "def setDoc(self, docs):\n        \"\"\"Sets the documentation to display\n\n        :param docs: a list of the stimuli doc, which are dicts\n        :type docs: list<dict>\n        \"\"\"\n        # sort stim by start time\n        docs = sorted(docs, key=lambda k: k['start_s'])\n\n        for doc in docs:\n            stim_type = doc['stim_type']\n            if not stim_type in self.displayTable:\n                continue\n            if not stim_type in self.displayTable[stim_type]:\n                continue\n            display_attributes = self.displayTable.get(stim_type, self.defaultAttributes)\n            \n            self.lyt.addWidget(ComponentDetailFrame(doc, display_attributes))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclearing and sets the components contained in this widget", "response": "def setComponents(self, components):\n        \"\"\"Clears and sets the components contained in this widget\n\n        :param components: list of documentation for subclasses of AbStractStimulusComponents\n        :type Components: list<dict>\n        \"\"\"\n        layout = self.layout()\n        for comp in components:\n            attrWidget = ComponentAttributerChecker(comp)\n            layout.addWidget(attrWidget)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setCheckedDetails(self, checked):\n        layout = self.layout()\n        for i in range(layout.count()):\n            w = layout.itemAt(i).widget()\n            if w.stimType in checked:\n                w.setChecked(checked[w.stimType])", "response": "Sets the details of the components and their attributes to be checked"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getCheckedDetails(self):\n        attrs = {}\n        layout = self.layout()\n        for i in range(layout.count()):\n            w = layout.itemAt(i).widget()\n            attrs[w.stimType] = w.getChecked()\n        return attrs", "response": "Gets the currently checked components and checked attributes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the attributes tocheck as checked", "response": "def setChecked(self, tocheck):\n        \"\"\"Sets the attributes *tocheck* as checked\n\n        :param tocheck: attributes names to check\n        :type tocheck: list<str>\n        \"\"\"\n        layout = self.layout()\n        for i in range(layout.count()):\n            w = layout.itemAt(i).widget()\n            if w.text() in tocheck:\n                w.setChecked(True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getChecked(self):\n        attrs = []\n        layout = self.layout()\n        for i in range(layout.count()):\n            w = layout.itemAt(i).widget()\n            if w.isChecked():\n                attrs.append(str(w.text()))\n        return attrs", "response": "Gets the checked attributes of the item in the item layout"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _convert_credentials(token_url, username=None, password=None, refresh_token=None):\n    if username and password:\n        data = {'grant_type': 'password', 'username': username, 'password': password}\n    elif refresh_token:\n        data = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    else:\n        raise NotImplementedError()\n\n    response = requests.post(token_url, data=data)\n    if response.status_code == 401:\n        raise Unauthorized('401 Client Error: Unauthorized')\n    elif response.status_code == 200:\n        d = response.json()\n        expiry = datetime.utcnow() + timedelta(seconds=d['expires_in'])\n        return Credentials(access_token=d['access_token'],\n                           refresh_token=d['refresh_token'],\n                           expiry=expiry)\n    else:\n        raise YamcsError('{} Server Error'.format(response.status_code))", "response": "Converts username and password credentials to token credentials using Yamcs as the authenticaton server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef count(forward_in, reverse_in='NA', kmer_size=31, count_file='mer_counts.jf', hash_size='100M', options='',\n          returncmd=False):\n    \"\"\"\n    Runs jellyfish count to kmerize reads to a desired kmer size.\n    :param forward_in: Forward input reads or fasta file. Can be uncompressed or gzip compressed.\n    :param reverse_in: Reverse input reads. Found automatically if in same folder as forward and _R1/_R2 naming convention\n    used.\n    :param kmer_size: Kmer size to get jellyfish to use. Default 31.\n    :param count_file: File to have jellyfish output mer counts to. Default mer_counts.jf\n    :param hash_size: Hash size. Should be suitable for most, if not all, bacterial genomes, and as of jellyfish2 should\n    adjust to be larger automatically if needed.\n    :param options: Other options to pass to jellyfish. Input should be a string, with options typed as they would be\n    on the command line.\n    :param returncmd: If set to true, function will return the cmd string passed to subprocess as a third value.\n    :return: Stdout and stderr from calling jellyfish.\n    \"\"\"\n    create_uncompressed = False\n    to_remove = list()\n    if os.path.isfile(forward_in.replace('_R1', '_R2')) and reverse_in == 'NA' and forward_in.replace('_R1', '_R2') != forward_in:\n        reverse_in = forward_in.replace('_R1', '_R2')\n        if forward_in.endswith('.gz'):\n            forward_in = accessoryfunctions.uncompress_gzip(forward_in)\n            create_uncompressed = True\n            to_remove.append(forward_in)\n        if reverse_in.endswith('.gz'):\n            reverse_in = accessoryfunctions.uncompress_gzip(reverse_in)\n            create_uncompressed = True\n            to_remove.append(reverse_in)\n        cmd = 'jellyfish count -m {} -C -s {} -o {} {} -F 2 {} {}'.format(str(kmer_size), hash_size, count_file,\n                                                                          options, forward_in, reverse_in)\n    elif reverse_in == 'NA':\n        cmd = 'jellyfish count -m {} -C -s {} -o {} {} {}'.format(str(kmer_size), hash_size, count_file,\n                                                                  options, forward_in)\n    else:\n        if forward_in.endswith('.gz'):\n            forward_in = accessoryfunctions.uncompress_gzip(forward_in)\n            create_uncompressed = True\n            to_remove.append(forward_in)\n        if reverse_in.endswith('.gz'):\n            reverse_in = accessoryfunctions.uncompress_gzip(reverse_in)\n            create_uncompressed = True\n            to_remove.append(reverse_in)\n        cmd = 'jellyfish count -m {} -C -s {} -o {} {} -F 2 {} {}'.format(str(kmer_size), hash_size, count_file,\n                                                                          options, forward_in, reverse_in)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if create_uncompressed:\n        for item in to_remove:\n            os.remove(item)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Run jellyfish count to kmerize reads."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump(mer_file, output_file='counts.fasta', options='', returncmd=False):\n    cmd = 'jellyfish dump {} -o {} {}'.format(mer_file, output_file, options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err", "response": "Dumps the counts from a jellyfish count file into a human - readable format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef headerData(self, section, orientation, role):\n        if role == QtCore.Qt.DisplayRole:\n            if orientation == QtCore.Qt.Horizontal:\n                return self.headers[section]", "response": "Get the Header for the columns in the table"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef data(self, index, role):\n        if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole:\n            test = self._testmodel.test(index.row())\n            col = index.column()\n            if col == 0:\n                item = test.userTag()\n            elif col == 1:\n                item = test.stimType()\n            elif col == 2:\n                item = test.repCount()\n            elif col == 3:\n                item = test.traceCount()\n            elif col == 4:\n                item = test.traceCount()*test.loopCount()*test.repCount()\n            elif col == 5:\n                item = test.samplerate()\n\n            return item\n        elif role == QtCore.Qt.UserRole:  #return the whole python object\n            test = self._testmodel.test(index.row())\n            return QStimulusModel(test)\n        elif role == QtCore.Qt.UserRole + 1:  #return the whole python object\n            test = self._testmodel.test(index.row())\n            return test\n        elif role == CursorRole:\n            col = index.column()\n            if not index.isValid():\n                return QtGui.QCursor(QtCore.Qt.ArrowCursor)\n            elif col == 0:\n                return QtGui.QCursor(QtCore.Qt.IBeamCursor)\n            else:\n                return cursors.openHand()", "response": "Return the data for the item at the given index and role"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines interaction allowed with table cells.", "response": "def flags(self, index):\n        \"\"\"\"Determines interaction allowed with table cells.\n\n        See :qtdoc:`QAbstractItemModel<QAbstractItemModel.flags>`, \n        and :qtdoc:`subclassing<qabstractitemmodel.subclassing>`\n        \"\"\"\n        if index.column() == 0 or index.column == self.headers.index('Reps'):\n            return QtCore.Qt.ItemIsEditable | QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable\n        else:\n            return QtCore.Qt.ItemIsEnabled"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setData(self, index, value, role):\n        if role == QtCore.Qt.EditRole:\n            if isinstance(value, QtCore.QVariant):\n                value = value.toPyObject()\n            if index.column() == 0:\n                test = self._testmodel.test(index.row())\n                test.setUserTag(str(value))\n                return True\n            if index.column() == 2:\n                test = self._testmodel.test(index.row())\n                test.setRepCount(value)\n                return True\n        return False", "response": "Sets the data at the given index to value in underlying data structure returning True if the data was set to be updated False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the dropped test item into the protocol list.", "response": "def dropped(self, item, event):\n        \"\"\"Adds the dropped test *item* into the protocol list.\n\n        Re-implemented from :meth:`AbstractDragView<sparkle.gui.abstract_drag_view.AbstractDragView.dropped>`\n        \"\"\"\n        location = self.rowAt(event.pos().y())\n\n        if isinstance(item, StimFactory):\n            factory = item\n            # create new stimulus then!\n            stim = factory.create()\n            if stim is not None:\n                self.model().insertTest(stim, location)\n        elif event.source() == self:\n            self.model().insertTest(item, location)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cursor(self, pos):\n        row = self.indexAt(pos).row()\n        if row == -1:\n            row = self.model().rowCount()\n        row_height = self.rowHeight(0)\n        y = row_height*row\n        x = self.width()\n        return QtCore.QLine(0,y,x,y)", "response": "Returns a line at the nearest row split between tests."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mousePressEvent(self, event):\n        index = self.indexAt(event.pos())\n        if index.isValid():\n            if index.column() == 0:\n                self.edit(index, QtGui.QAbstractItemView.DoubleClicked, event)\n            else:\n                super(ProtocolView, self).mousePressEvent(event)", "response": "Launches edit of cell if first column clicked otherwise passes to super class"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mouseDoubleClickEvent(self, event):\n        if event.button() == QtCore.Qt.LeftButton:\n            index = self.indexAt(event.pos())\n            if index.isValid():\n                selectedStimModel = self.model().data(index, QtCore.Qt.UserRole)\n                self.stimEditor = selectedStimModel.showEditor()\n                self.stimEditor.show()", "response": "Creates and shows editor for stimulus selected"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncoordinate for the test row at index", "response": "def indexXY(self, index):\n        \"\"\"Coordinates for the test row at *index*\n\n        Re-implemented from :meth:`AbstractDragView<sparkle.gui.abstract_drag_view.AbstractDragView.indexXY>`\n        \"\"\"\n        # just want the top left of row selected\n        row = index.row()\n        if row == -1:\n            row = self.model().rowCount()\n        y = self.rowHeight(0)*row\n        return 0, y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_args():\n    prog = 'github-list-repos'\n\n    parser = argparse.ArgumentParser(\n        prog=prog,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=textwrap.dedent(\"\"\"\n        List repositories on Github using various criteria.\n\n        Examples:\n\n            {prog} --org lsst\n\n            {prog} \\\\\n                    --hide 'Data Management' \\\\\n                    --hide 'Owners' \\\\\n                    --org lsst\n\n        Note: --mint and --maxt limits are applied after --hide.\n\n        So for example,\n\n            {prog} --maxt 0 --hide Owners --org lsst\n\n        returns the list of repos that are owned by no team besides Owners.\n        \"\"\").format(prog=prog),\n        epilog='Part of codekit: https://github.com/lsst-sqre/sqre-codekit')\n    parser.add_argument(\n        '-o', '--org',\n        dest='organization',\n        help='GitHub Organization name',\n        required=True)\n    parser.add_argument(\n        '--hide', action='append',\n        help='Hide a specific team from the output')\n    parser.add_argument(\n        '--mint', type=int, default='0',\n        help='Only list repos that have more than MINT teams')\n    parser.add_argument(\n        '--maxt', type=int,\n        help='Only list repos that have fewer than MAXT teams')\n    parser.add_argument(\n        '--delimiter', default=', ',\n        help='Character(s) separating teams in print out')\n    parser.add_argument(\n        '--token-path',\n        default='~/.sq_github_token',\n        help='Use a token (made with github-auth) in a non-standard loction')\n    parser.add_argument(\n        '--token',\n        default=None,\n        help='Literal github personal access token string')\n    parser.add_argument(\n        '-d', '--debug',\n        action='count',\n        default=codetools.debug_lvl_from_env(),\n        help='Debug mode (can specify several times)')\n    parser.add_argument('-v', '--version', action=codetools.ScmVersionAction)\n    return parser.parse_args()", "response": "Parse command - line arguments and return a list of repos that are owned by the current user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist repos and teams", "response": "def run():\n    \"\"\"List repos and teams\"\"\"\n    args = parse_args()\n\n    codetools.setup_logging(args.debug)\n\n    global g\n    g = pygithub.login_github(token_path=args.token_path, token=args.token)\n\n    if not args.hide:\n        args.hide = []\n\n    org = g.get_organization(args.organization)\n\n    try:\n        repos = list(org.get_repos())\n    except github.RateLimitExceededException:\n        raise\n    except github.GithubException as e:\n        msg = 'error getting repos'\n        raise pygithub.CaughtOrganizationError(org, e, msg) from None\n\n    for r in repos:\n        try:\n            teamnames = [t.name for t in r.get_teams()\n                         if t.name not in args.hide]\n        except github.RateLimitExceededException:\n            raise\n        except github.GithubException as e:\n            msg = 'error getting teams'\n            raise pygithub.CaughtRepositoryError(r, e, msg) from None\n\n        maxt = args.maxt if (args.maxt is not None and\n                             args.maxt >= 0) else len(teamnames)\n        if args.debug:\n            print(\"MAXT=\", maxt)\n\n        if args.mint <= len(teamnames) <= maxt:\n            print(r.name.ljust(40) + args.delimiter.join(teamnames))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_bar_chart(self, x_labels, y_values, y_label):\n        self.setup(0.25)\n        ax1 = self.get_ax()\n        ax1.set_xticks(list(range(len(x_labels))))\n        ax1.set_xticklabels([x_labels[i] for i in range(len(x_labels))],\n                            rotation=90)\n        plt.ylabel(y_label)\n\n        x_pos = range(len(x_labels))\n        plt.bar(x_pos, y_values, align=\"center\")\n\n        return ax1", "response": "Creates a bar chart for the current variable count."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a bar chart with multiple lines.", "response": "def create_multiple_bar_chart(self, x_labels, mul_y_values, mul_y_labels,\n                                  normalize=False):\n        \"\"\"Creates bar chart with multiple lines\n\n        :param x_labels: Names for each variable\n        :param mul_y_values: list of values of x labels\n        :param mul_y_labels: list of labels for each y value\n        :param normalize: True iff you want to normalize each y series\n        :return: Bar chart\n        \"\"\"\n        self.setup(0.25)\n        ax1 = self.get_ax()\n        ax1.set_xticks(list(range(len(x_labels))))\n        ax1.set_xticklabels([x_labels[i] for i in range(len(x_labels))],\n                            rotation=90)\n\n        y_counts = len(mul_y_values)\n        colors = cm.rainbow(np.linspace(0, 1, y_counts))  # different colors\n        max_bar_width = 0.6\n        bar_width = max_bar_width / y_counts  # width of each bar\n        x_shifts = np.linspace(0, max_bar_width,\n                               y_counts) - max_bar_width * 0.5  # center in 0\n        ax_series = []\n        for i in range(y_counts):\n            x_pos = range(len(x_labels))  # x points\n            x_pos = np.array(x_pos) + x_shifts[i]  # shift for each y series\n            if normalize:  # normalize array\n                y_values = normalize_array(mul_y_values[i])\n            else:\n                y_values = mul_y_values[i]\n\n            ax_series.append(\n                ax1.bar(\n                    x_pos,\n                    y_values,\n                    width=bar_width,\n                    align=\"center\",\n                    color=colors[i]\n                )\n            )\n\n        ax1.legend(ax_series, mul_y_labels)\n\n        return ax1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_sym_log_bar_chart(self, x_labels, y_values, y_label):\n        ax1 = self.create_bar_chart(x_labels, y_values, y_label)\n        ax1.set_yscale(\"sym-log\", linthreshy=1e-12)  # logarithmic plot\n        return ax1", "response": "Creates a Sym - Log bar chart"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef file_parts(self):\n        file_parts = []\n        for part in self.parts:\n            try:\n                for sub_part in part:\n                    if isinstance(sub_part, FileToken):\n                        file_parts.append(sub_part)\n            except TypeError:\n                if isinstance(part, FileToken):\n                    file_parts.append(part)\n        return file_parts", "response": "Returns a list of the file tokens in the list of parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_dependent_files(self, prev_commands=[]):\n        for command in prev_commands:\n            for my_input in self.input_parts:\n                for their_output in command.output_parts:\n                    if their_output == my_input:\n                        my_input.filename = their_output.eval()", "response": "Update the command s dependent files based on the evaluated input and the output of previous commands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eval(self):\n        eval = []\n        for part in self.parts:\n            try:\n                result = part.eval()\n            except AttributeError:\n                result = part\n            if result[-1] != '\\n':\n                result += ' '\n            eval.append(result)\n        return ''.join(eval).strip()", "response": "Evaluate the given job and return a complete shell script to be run\n        by the job manager."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_type(cls, typ):\n        if not isinstance(typ, basestring):\n            raise TypeError(\"The type should be a string. But is %s\" % type(typ))\n        cls.types.append(typ)", "response": "Register a type for jb_reftrack nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_from_args(args):\n    # Empty args\n    if not args:\n        return []\n\n    # Get argument type\n    arg_type = type(args[0])\n    is_list = arg_type in LIST_TYPES\n\n    # Check that the arguments are uniforn (of same type)\n    same_type = all([\n        isinstance(arg, arg_type)\n        for arg in args\n    ])\n\n    if not same_type:\n        raise Exception('Expected uniform arguments of same type !')\n\n    # Flatten iterables\n    # ['x', 'y'], ...\n    if is_list:\n        args_lists = map(list, args)\n        flattened_args = sum(args_lists, [])\n        return flattened_args\n    # Flatten set\n    # 'x', 'y'\n    return list(args)", "response": "Flatten list of args so as to accept either an array\n    Or as many arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying a transformation to a functions return value", "response": "def transform(transform_func):\n    \"\"\"Apply a transformation to a functions return value\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def f(*args, **kwargs):\n            return transform_func(\n                func(*args, **kwargs)\n            )\n        return f\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subkey(dct, keys):\n    key = keys[0]\n    if len(keys) == 1:\n        return dct[key]\n    return subkey(dct[key], keys[1:])", "response": "Get an entry from a dict of dicts by the list of keys to follow"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_new_user_credentials(self):\n        # OAuth2.0 authorization flow\n        flow = client.flow_from_clientsecrets(self.app_secrets, self.scope)\n        flow.user_agent = self.app_name\n        return tools.run_flow(flow, self.store)", "response": "Gets new user credentials file upon user prompt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_user_credentials(self):\n        # create path to user credentials if needed\n        if not os.path.exists(os.path.dirname(self.user_credentials)):\n            os.makedirs(os.path.dirname(self.user_credentials))\n\n        credentials = self.store.get()  # retrieve credentials\n        needs_to_be_updated = not credentials or credentials.invalid\n        if needs_to_be_updated:\n            self.get_new_user_credentials()  # get new user credentials\n            credentials = self.store.get()  # retrieve new credentials\n\n        return credentials", "response": "Gets new user credentials"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_driver(self, name, version):\n        user_credentials = self.get_user_credentials()  # get credentials\n        return discovery.build(\n            name, version,\n            http=self.authenticate(user_credentials)\n        )", "response": "Authenticates and creates a new API driver to perform scope stuff\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart on - going chart style acqusition", "response": "def start_chart(self):\n        \"\"\"Begin on-going chart style acqusition\"\"\"\n        self.current_dataset_name = self.chart_name\n        self.datafile.init_data(self.current_dataset_name, mode='continuous')\n        self.chart_name = increment_title(self.chart_name)\n        \n        # stimulus tracker channel hard-coded at least chan for now\n        self.player.start_continuous([self.aichan, u\"PCI-6259/ai31\"])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef qtdoc_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n    base = 'http://qt-project.org/doc/qt-4.8/'\n\n    match = re.search('([^<]+)(<[^<>]+>)?', text)\n    if match is None:\n        raise ValueError\n\n    label = match.group(1)\n    if match.lastindex == 2:\n        # remove the carots from second group\n        clsmeth = match.group(2)[1:-1]\n        # assumes single . separating a class and a method or property name\n        cls, meth = clsmeth.split('.')\n        ref = base + cls + '.html#' + meth\n    else:\n        ref = base + label.lower() + '.html'\n\n    node = nodes.reference(rawtext, label, refuri=ref, **options)\n    return [node], []", "response": "Links to a Qt class s doc\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_func_body(original, updater=None):\n    updated = ''\n    regex = r'([_\\w][_\\w\\d]*)\\s*\\(.*\\)\\s*\\{'\n    match = re.search(regex, original)\n    while match:\n        name = match.group(1)\n        logging.debug(_('Found candidate: %s'), name)\n        start = match.end()\n        end = start + find_balance_index(original[start:])\n        body = original[start:end]\n        if updater:\n            body = updater(body, name)\n        updated += original[:start] + '\\n' + body + original[end]\n        original = original[end + 1:]\n        match = re.search(regex, original)\n    return updated", "response": "Update all function body using the updating function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the first balance index in a source string.", "response": "def find_balance_index(source, start='{', end='}'):\n    \"\"\"Get the first balance index.\"\"\"\n    state = 1\n    for index, char in enumerate(source):\n        if char == start:\n            state += 1\n        elif char == end:\n            state -= 1\n        if state == 0:\n            return index\n    raise RuntimeError('This should not happen: Balance Not Found')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform_sources(self, sources, with_string=False):\n        modules = {}\n        updater = partial(\n            self.replace_source, modules=modules, prefix='string_')\n        for filename in sources:\n            updated = update_func_body(sources[filename], updater)\n            sources[filename] = EXTERN_AND_SEG + updated\n        logging.debug('modules: %s', modules)\n        return sources, self.build_funcs(modules)", "response": "Transform sources to replace strings and functions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef replace_source(self, source, name, modules, prefix):\n        needs_windll = False\n\n        def _func_replacer(match, modules, windll):\n            matched = match.group(0)\n            if matched in self.BLACKLIST:\n                return matched\n            module = self.database.query_func_module(matched)\n            if module:\n                try:\n                    modules[module[0]] += [module[1]]\n                except KeyError:\n                    modules[module[0]] = [module[1]]\n                if windll:\n                    return '{}->{}.{}'.format(windll, *module)\n                return '{}->{}'.format(*module)\n            return matched\n\n        replacer = partial(\n            _func_replacer, modules=modules, windll='windll')\n        replaced = re.sub(r'[_\\w][_\\w\\d]*(?=\\s*\\()',\n                          replacer, source)\n\n        if source != replaced:\n            needs_windll = True\n\n        str_table = {}\n\n        def _string_replacer(match):\n            matched = match.group()[1:-1]\n            try:\n                number = str_table[matched]\n            except KeyError:\n                number = len(str_table) + 1\n                str_table.update({matched: number})\n            return '{}{}'.format(prefix, number)\n\n        replaced = re.sub(r'\".+?\"', _string_replacer, replaced)\n        strings, relocs = self.build_strings(str_table, prefix)\n        strings = ''.join(strings).strip()\n        windll32 = reloc_var('windll', 'reloc_delta', True,\n                             'windll_t')\n        if needs_windll:\n            relocs += [windll32]\n        if strings:\n            strings = '\\n' + strings\n            if not needs_windll:\n                relocs += [windll32]\n                needs_windll = True\n        windll64 = ''\n        if needs_windll:\n            windll64 = '{0} *{1} = &_{1};\\n'.format('windll_t',\n                                                    'windll')\n        relocs = reloc_both(''.join(relocs), windll64)\n        if name in ['main']:\n            replaced = '\\ninit();' + replaced\n        return strings + relocs + replaced", "response": "Replace the source code with the name and modules."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_funcs(modules):\n        kernel32 = ['kernel32_']\n        try:\n            kernel32 += remove_dups(modules['kernel32'])\n        except KeyError:\n            if len(modules) and 'LoadLibraryA' not in kernel32:\n                kernel32.insert(1, 'LoadLibraryA')\n        if len(modules) > 1 and 'LoadLibraryA' not in kernel32:\n            kernel32.insert(1, 'LoadLibraryA')\n        if 'GetProcAddress' not in kernel32:\n            kernel32.insert(1, 'GetProcAddress')\n        logging.debug('kernel32: %s', kernel32)\n        for module, funcs in modules.items():\n            logging.debug('%s: %s', module, funcs)\n            if module != 'kernel32':\n                kernel32.extend([module + '_'] + remove_dups(funcs))\n        return kernel32", "response": "Build a used functions and modules list\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing string definitions according to COOKIE.", "response": "def build_strings(strings, prefix):\n        \"\"\"Construct string definitions according to\n        the previously maintained table.\n        \"\"\"\n        strings = [\n            (\n                make_c_str(prefix + str(number), value),\n                reloc_ptr(\n                    prefix + str(number), 'reloc_delta', 'char *'\n                )\n            ) for value, number in sort_values(strings)\n        ]\n        return [i[0] for i in strings], [i[1] for i in strings]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a file to the set of files published in this dataset.", "response": "def add_file(self, **args):\n        '''\n        Adds a file's information to the set of files to be\n        published in this dataset.\n\n        :param file_name: Mandatory. The file name (string).\n            This information will simply be included in the\n            PID record, but not used for anything.\n\n        :param file_handle: Mandatory. The handle (PID) of\n            this file (string). It is included in the file's netcdf\n            header. It must bear the prefix that this library \n            (or rather, the consuming servlet that will consume\n            this library's requests), has write access to.\n\n        :param file_size: Mandatory. The file size (as string or\n            integer. Will be transformed to integer). This\n            information will be included in the handle record\n            and used for consistency checks during republications\n            of files with the same handle.\n\n        :param checksum: Mandatory. The file's checksum. This\n            information will be included in the handle record\n            and used for consistency checks during republications\n            of files with the same handle.\n\n        :param checksum_type: Mandatory. The checksum type/method\n            (string), e.g. \"MD5\" or \"SHA256\". This information will\n            be included in the handle record and used for consistency\n            checks during republications of files with the same handle.\n\n        :param publish_path: Mandatory. The THREDDS publish path as\n            a string. This is part of the URL for accessing the file,\n            which will be part of the handle record. It will not be\n            accessed, neither by the library nor by the consumer.\n            The URL consists of the dataset's \"data_node\", the dataset's \n            \"thredds_service_path\", and this \"publish_path\". Redundant\n            slashes are removed. If the URL does not start with \"http\",\n            \"http://\" is added.\n\n        :param file_version: Mandatory. Any string. File versions\n            are not managed in the PID. This information will simply be\n            included in the PID record, but not used for any reasoning.\n        '''\n\n        # Check if allowed:\n        self.__check_if_adding_files_allowed_right_now()\n\n        # Check if args ok:\n        mandatory_args = ['file_name', 'file_handle', 'file_size',\n                          'checksum', 'publish_path', 'checksum_type',\n                          'file_version']\n        utils.check_presence_of_mandatory_args(args, mandatory_args)\n        self.__enforce_integer_file_size(args)\n        self.__enforce_string_file_version(args)\n\n        # Add file:\n        self.__check_and_correct_handle_syntax(args)\n        self.__add_file(**args)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_error(*error, cause=None):\n    thread_id = threading.current_thread().ident\n    text = \" \".join(error)\n    if cause:\n        text += \" due to \" + str(cause)\n\n    logger = get_logger()\n    logger.error(LOG_THREAD_FORMAT.format(thread_id, text))", "response": "Logs an error to the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_printable(iterable):\n    if iterable:\n        return ''.join(i for i in iterable if i in string.printable)\n    return ''", "response": "Get printable characters from the specified string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping subprocess. check_output to make life easier.", "response": "def run_program(program, *args):\n    \"\"\"Wrap subprocess.check_output to make life easier.\"\"\"\n    real_args = [program]\n    real_args.extend(args)\n    logging.debug(_('check_output arguments: %s'), real_args)\n    check_output(real_args, universal_newlines=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the parent directory of a filename.", "response": "def get_parent_dir(name):\n    \"\"\"Get the parent directory of a filename.\"\"\"\n    parent_dir = os.path.dirname(os.path.dirname(name))\n    if parent_dir:\n        return parent_dir\n    return os.path.abspath('.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_ext(path, basename=True):\n    if basename:\n        path = os.path.basename(path)\n    return os.path.splitext(path)", "response": "Wrap them to make life easier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ad_hoc_magic_from_file(filename, **kwargs):\n    with open(filename, 'rb') as stream:\n        head = stream.read(16)\n        if head[:4] == b'\\x7fELF':\n            return b'application/x-executable'\n        elif head[:2] == b'MZ':\n            return b'application/x-dosexec'\n        else:\n            raise NotImplementedError()", "response": "Ad - hoc emulation of magic. from_file from python - magic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expand_path(*paths):\n    return os.path.join(\n        os.path.dirname(os.path.realpath(sys.argv[0])), *paths)", "response": "Expand the path with the directory of the executed file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert filenames from Linux to Windows.", "response": "def translate_filenames(filenames):\n    \"\"\"Convert filenames from Linux to Windows.\"\"\"\n    if is_windows():\n        return filenames\n    for index, filename in enumerate(filenames):\n        filenames[index] = vboxsf_to_windows(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vboxsf_to_windows(filename, letter='f:'):\n    home = os.path.expanduser('~')\n    filename = os.path.abspath(filename).replace(home, letter)\n    return filename.replace('/', '\\\\')", "response": "Convert the Linux path name to a Windows one."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_file(filename, text):\n    logging.debug(_('Writing file: %s'), filename)\n    try:\n        with open(filename, 'w') as writable:\n            writable.write(text)\n    except (PermissionError, NotADirectoryError):\n        logging.error(_('Error writing file: %s'), filename)\n        return False\n    return True", "response": "Write text to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stylify_code(code):\n    try:\n        output = check_output(\n            ['astyle', '--max-code-length=69', '--indent=spaces=2'],\n            universal_newlines=True, input=code\n        )\n    except (OSError, CalledProcessError, TypeError):\n        logging.debug(_('failed to stylify code'))\n        return code\n    return output", "response": "Stylify the C source code using astyle."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort_item(iterable, number, reverse=False):\n    return sorted(iterable, key=itemgetter(number), reverse=reverse)", "response": "Sort the itertable according to the given number item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hash_func(name):\n    ret = 0\n    for char in name:\n        ret = ((ret << 5) + ret + ord(char)) & 0xffffffff\n    return hex(ret)", "response": "Hash the string using a hash algorithm found in\n    tombkeeper / Shellcode_Template_in_C.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_by(keys, original):\n    for i in [\n            original[index]\n            for index, needed in enumerate(keys) if not needed\n    ]:\n        original.remove(i)", "response": "Remove items in a list according to another list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef group_by(iterable, key_func):\n    groups = (\n        list(sub) for key, sub in groupby(iterable, key_func)\n    )\n    return zip(groups, groups)", "response": "Wrap itertools. groupby to make life easier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting multiple attributes from multiple objects.", "response": "def get_attrs(self, *names):\n        \"\"\"Get multiple attributes from multiple objects.\"\"\"\n        attrs = [getattr(self, name) for name in names]\n        return attrs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the resource doc as at the target when the posting was already created \\ at the target.", "response": "def target_doc(self):\n        \"\"\"Returns resource doc as at the target, when the posting was already created \\\n           at the target. This property normally contains the **target_doc** data from \\\n           the livebrigde storage item, saved in a syndication earlier.\n\n        :returns: dict\"\"\"\n        if not hasattr(self, \"_target_doc\") or not self._target_doc:\n            if self._existing:\n                self._target_doc = self._existing.get(\"target_doc\", {})\n        return self._target_doc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef target_id(self):\n        # already set?\n        if self._target_id:\n            return self._target_id\n        # post already exists?\n        if self._existing:\n            self._target_id = self._existing.get(\"target_id\")\n        return self._target_id", "response": "Returns the id of the target to which this post has to be syndicated."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies a formatter function the return value of the decorated function.", "response": "def with_formatter(formatter):\n    \"\"\"Apply a formatter function the return value\n    of the decorated function.\n    \"\"\"\n    def _decorator_after_args(unwrapped):\n        def _wrapped(self, *args, **kwargs):\n            logging.debug('unwrapped: %s', unwrapped)\n            logging.debug('self: %s', self)\n            logging.debug('args: %s', args)\n            logging.debug('kwargs: %s', kwargs)\n            return_value = unwrapped(self, *args, **kwargs)\n            if 'raw' in kwargs and kwargs['raw']:\n                return return_value\n            else:\n                return formatter(return_value)\n        return _wrapped\n    return _decorator_after_args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat a string representing the information concerning the name.", "response": "def format_info(raw):\n    \"\"\"Format a string representing the information\n    concerning the name.\n    \"\"\"\n    logging.debug(_('raw[0]: %s'), raw[0])\n    results, sense = raw\n    # A scenario where ORM really stands out.\n    new = '\\n'.join(\n        '{} {} {} {}'.format(\n            i[0], sense.kind_id_to_name(i[1]),\n            sense.file_id_to_name(i[2]).lower(),\n            i[3] + ' ' if i[3] else '').strip()\n        for i in results)\n    return new"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a string representing the names contained in the files.", "response": "def format_names(raw):\n    \"\"\"Format a string representing the names contained in the files.\n    \"\"\"\n    if raw:\n        raw = [\n            '{}:\\n{}'.format(\n                header.lower(), ' '.join(func[0] for func in funcs)\n            )\n            for header, funcs in raw\n        ]\n        return '\\n'.join(raw)\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_kinds(raw):\n    output = ' '.join('{} {}'.format(*kind) for kind in raw if kind)\n    return output", "response": "Format a string representing the kinds."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_seq_rec(block, name, case_sensitive=True):\n    if case_sensitive:\n        def test(name, rec):\n            return name in rec['id']\n    else:\n        def test(name, rec):\n            return name.upper() in rec['id'].upper()\n\n    for rec in block['sequences']:\n        if test(name, rec):\n            return rec\n    raise ValueError(\"No sequence ID matches %s\" % repr(name))", "response": "Given a sequence ID find the first matching record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a sequence ID find the first actual ID that contains it.", "response": "def find_seq_id(block, name, case_sensitive=True):\n    \"\"\"Given part of a sequence ID, find the first actual ID that contains it.\n\n    Example::\n\n        >>> find_seq_id(block, '2QG5')\n        'gi|158430190|pdb|2QG5|A'\n\n    Raise a ValueError if no matching key is found.\n    \"\"\"\n    # logging.warn(\"DEPRECATED: Try to use cma.find_seq_rec instead\")\n    rec = find_seq_rec(block, name, case_sensitive)\n    return rec['id']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_consensus(block):\n    from collections import Counter\n\n    # Take aligned (non-insert) chars from all rows; transpose\n    columns = zip(*[[c for c in row['seq'] if not c.islower()]\n                    for row in block['sequences']])\n    cons_chars = [Counter(col).most_common()[0][0] for col in columns]\n    cons_chars = [c if c != '-' else 'X' for c in cons_chars]\n    assert len(cons_chars) == block['query_length']\n    cons_sequence = {\n        'index': 1,\n        'id': 'consensus',\n        'description': '',\n        'dbxrefs': {},\n        'phylum': '',\n        'taxchar': '',\n        'head_len': None,\n        'tail_len': None,\n        'head_seq': '',\n        'tail_seq': '',\n        'length': block['query_length'],\n        'seq': ''.join(cons_chars),\n    }\n    return cons_sequence", "response": "Calculate a simple consensus sequence for the block."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_conservation(block):\n    consensus = block['sequences'][0]['seq']\n    assert all(c.isupper() for c in consensus), \\\n            \"So-called consensus contains indels!\"\n    # remove all non-consensus positions -- now alignment is easy\n    cleaned = [[c for c in s['seq'] if not c.islower()]\n               for s in block['sequences'][1:]]\n    height = float(len(cleaned))\n    # validation\n    for row in cleaned:\n        if len(row) != len(consensus):\n            raise ValueError(\"Aligned sequence length (%s) doesn't match \"\n                             \"consensus (%s)\"\n                             % (len(row), len(consensus)))\n    # transpose & go\n    columns = zip(*cleaned)\n    return dict((idx + 1, columns[idx].count(cons_char) / height)\n                for idx, cons_char in enumerate(consensus))", "response": "Calculate conservation levels at each consensus position."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_equivalent_positions(block):\n    consensus = block['sequences'][0]['seq']\n    rest = block['sequences'][1:]\n\n    # Validation\n    if '-' in consensus or '.' in consensus:\n        raise ValueError(\"First sequence (consensus?) contains gaps\")\n    # Check for duplicate sequence IDs\n    seen = set()\n    dupes = set()\n    for rec in rest:\n        if rec['id'] in seen:\n            dupes.add(rec['id'])\n        else:\n            seen.add(rec['id'])\n    if dupes:\n        raise ValueError(\"Duplicate sequences:\\n\" + '\\n'.join(dupes))\n\n    curr_shift = {}\n    curr_resn = {}\n    # NB: consensus doesn't have head/tail, but other sequences may\n    for rec in rest:\n        # Count inserts seen so far -- shift string indexes by this far ahead to\n        # get the \"equivalent\" location in the sequence string\n        #   - as in, how far ahead in the current seq do we need to jump to get\n        #     to a position equivalent to what's in the consensus?\n        #   - can this ever be less than 0 (==consensus)?  No, because that's\n        #     where gaps come from. Good.\n        curr_shift[rec['id']] = 0\n        # Residue number in the actual sequence at the current (shifted)\n        # location\n        #   curr_posn[id] = current equivalent res.num in `id` to cons[i]\n        curr_resn[rec['id']] = rec['head_len']\n\n    equivalencies = dict((i+1, {}) for i in xrange(len(consensus)))\n    # Map each character position i in the consensus sequence\n    # to equivalent residues in each of the other sequences\n    #   i = index in the consensus string (== consensus res.num - 1)\n    for i, char in enumerate(consensus):\n        assert char.isupper()\n        for rec in rest:\n            rid = rec['id']\n            strposn = i + curr_shift[rid]\n            if rec['seq'][strposn].isupper():\n                # Match\n                curr_resn[rid] += 1\n            elif rec['seq'][strposn].islower():\n                # Insert\n                while rec['seq'][strposn].islower():\n                    # Count the whole insert size\n                    curr_shift[rid] += 1\n                    curr_resn[rid] += 1\n                    strposn += 1\n                curr_resn[rid] += 1 # Count the next match, too\n            else:\n                # Deletion / gap\n                assert rec['seq'][strposn] in '.-'\n                continue\n            equivalencies[i+1][rid] = curr_resn[rid]\n\n    return equivalencies", "response": "Create a mapping of equivalent residue positions to consensus."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nidentifies the inserts in sequence in a CMA block.", "response": "def get_inserts(block):\n    \"\"\"Identify the inserts in sequence in a block.\n\n    Inserts are relative to the consensus (theoretically), and identified by\n    lowercase letters in the sequence. The returned integer pairs represent the\n    insert start and end positions in the full-length sequence, using one-based\n    numbering.\n\n    The first sequence of the CMA block is included, though it may just be the\n    consensus sequence, which shouldn't have any inserts.\n\n    Output:\n        {id1: [(start, end), (start, end), ...], id2: ..., ...}\n\n    \"\"\"\n    def find_inserts(seq, head_len):\n        \"\"\"Locate the lowercase regions in a character sequence.\n\n        Yield the insert ranges as tuples using 1-based numbering, shifted by\n        head_len.\n        \"\"\"\n        in_insert = False\n        curr_start = None\n        deletions = 0\n        for idx, is_lower in enumerate(map(str.islower, seq)):\n            if is_lower:\n                if not in_insert:\n                    # Start of a new insert region\n                    curr_start = head_len + idx + 1 - deletions\n                    in_insert = True\n            else:\n                if in_insert:\n                    # End of the current insert region\n                    yield (curr_start, head_len + idx - deletions)\n                    in_insert = False\n                if seq[idx] == '-':\n                    deletions += 1\n\n\n    return dict((record['id'],\n                    list(find_inserts(record['seq'], record['head_len'])))\n                for record in block['sequences'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef number_letters(block_or_record, key=None):\n    if key:\n        logging.warn(\"DEPRECATED: Pass a record instead\")\n        assert 'sequences' in block_or_record, \"Expected a block and a key\"\n        record = find_seq_rec(block_or_record, key)\n    else:\n        assert 'id' in block_or_record, \"Expected argument to be a record\"\n        record = block_or_record\n    # Ungapped sequence -- accounts for dels vs. consensus\n    seq = record['seq'].replace('-', '').replace('.', '')\n    # Difference between string positions and residue numbers\n    shift = record['head_len'] + 1\n    return dict((idx + shift, letter)\n                for idx, letter in enumerate(seq))", "response": "Return a dict of posn and restype for each letter in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a dictionary of regex for extracting the meta data for the spectra", "response": "def get_meta_regex(schema='mona'):\n    \"\"\" Create a dictionary of regex for extracting the meta data for the spectra\n    \"\"\"\n    # NOTE: will just ignore cases, to avoid repetition here\n    meta_parse = collections.OrderedDict()\n\n    if schema == 'mona':\n        meta_parse['collision_energy'] = ['^collision energy(?:=|:)(.*)$']\n        meta_parse['ms_level'] = ['^ms.*level(?:=|:)\\D*(\\d*)$', '^ms type(?:=|:)\\D*(\\d*)$',\n                              '^Spectrum_type(?:=|:)\\D*(\\d*)$']\n        meta_parse['accession'] = ['^accession(?:=|:)(.*)$', '^DB#(?:=|:)(.*)$']\n        meta_parse['resolution'] = ['^resolution(?:=|:)(.*)$']\n        meta_parse['polarity'] = ['^ion.*mode(?:=|:)(.*)$', '^ionization.*mode(?:=|:)(.*)$', '^polarity(?:=|:)(.*)$']\n        meta_parse['fragmentation_type'] = ['^fragmentation.*mode(?:=|:)(.*)$', '^fragmentation.*type(?:=|:)(.*)$']\n        meta_parse['precursor_mz'] = ['^precursor m/z(?:=|:)\\s*(\\d*[.,]?\\d*)$', '^precursor.*mz(?:=|:)\\s*(\\d*[.,]?\\d*)$']\n        meta_parse['precursor_type'] = ['^precursor.*type(?:=|:)(.*)$', '^adduct(?:=|:)(.*)$']\n        meta_parse['instrument_type'] = ['^instrument.*type(?:=|:)(.*)$']\n        meta_parse['instrument'] = ['^instrument(?:=|:)(.*)$']\n        meta_parse['copyright'] = ['^copyright(?:=|:)(.*)$']\n        # meta_parse['column'] = ['^column(?:=|:)(.*)$']\n        meta_parse['mass_accuracy'] = ['^mass.*accuracy(?:=|:)\\s*(\\d*[.,]?\\d*)$']\n        meta_parse['mass_error'] = ['^mass.*error(?:=|:)\\s*(\\d*[.,]?\\d*)$']\n        meta_parse['origin'] = ['^origin(?:=|:)(.*)$']\n        meta_parse['name'] = ['^Name(?:=|:)(.*)$']\n        meta_parse['splash'] = ['^splash:(.*)$']\n        meta_parse['retention_time'] = ['^retention.*time(?:=|:)\\s*(\\d*[.,]?\\d*)$']\n        meta_parse['retention_index'] = ['^retention.*index(?:=|:)\\s*(\\d*[.,]?\\d*)$']\n\n    elif schema == 'massbank':\n        meta_parse['collision_energy'] = ['^AC\\$MASS_SPECTROMETRY:\\s+COLLISION_ENERGY\\s+(.*)$']\n        meta_parse['ms_level'] = ['^AC\\$MASS_SPECTROMETRY:\\s+MS_TYPE\\s+\\D*(\\d*)$']\n        meta_parse['accession'] = ['^ACCESSION:(.*)$']\n        meta_parse['resolution'] = ['^AC\\$MASS_SPECTROMETRY:\\s+RESOLUTION\\s+(.*)$']\n        meta_parse['polarity'] = ['^AC\\$MASS_SPECTROMETRY:\\s+ION_MODE\\s+(.*)$']\n        meta_parse['fragmentation_type'] = ['^AC\\$MASS_SPECTROMETRY:\\s+FRAGMENTATION_MODE\\s+(.*)$']\n        meta_parse['precursor_mz'] = ['^MS\\$FOCUSED_ION:\\s+PRECURSOR_M/Z\\s+(\\d*[.,]?\\d*)$']\n        meta_parse['precursor_type'] = ['^MS\\$FOCUSED_ION:\\s+PRECURSOR_TYPE\\s+(.*)$']\n        meta_parse['instrument_type'] = ['^AC\\$INSTRUMENT_TYPE:\\s+(.*)$']\n        meta_parse['instrument'] = ['^AC\\$INSTRUMENT:\\s+(.*)$']\n        meta_parse['copyright'] = ['^COPYRIGHT:\\s+(.*)']\n        # meta_parse['column'] = ['^column(?:=|:)(.*)$']\n        meta_parse['mass_accuracy'] = ['^AC\\$MASS_SPECTROMETRY:\\s+ACCURACY\\s+(.*)$']  # need to check\n        meta_parse['mass_error'] = ['^AC\\$MASS_SPECTROMETRY:\\s+ERROR\\s+(.*)$']  # need to check\n        meta_parse['splash'] = ['^PK\\$SPLASH:\\s+(.*)$']\n        meta_parse['origin'] = ['^origin(?:=|:)(.*)$']\n        meta_parse['name'] = ['^RECORD_TITLE:\\s+(.*)$']\n        meta_parse['retention_time'] = ['^AC\\$CHROMATOGRAPHY:\\s+RETENTION.*TIME\\s+(\\d*[.,]?\\d*)$']\n        meta_parse['retention_index'] = ['^AC\\$CHROMATOGRAPHY:\\s+RETENTION.*INDEX\\s+(\\d*[.,]?\\d*)$']\n\n\n\n\n    return meta_parse"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_compound_regex(schema='mona'):\n\n    # NOTE: will just ignore cases in the regex, to avoid repetition here\n    meta_parse = collections.OrderedDict()\n\n    if schema == 'mona':\n        meta_parse['name'] = ['^Name(?:=|:)(.*)$']\n        meta_parse['inchikey_id'] = ['^inchikey(?:=|:)(.*)$']\n        meta_parse['molecular_formula'] = ['^molecular formula(?:=|:)(.*)$', '^formula:(.*)$']\n        meta_parse['molecular_weight'] = ['^MW(?:=|:)(\\d*[.,]?\\d*)$']\n        meta_parse['pubchem_id'] = ['^pubchem.*cid(?:=|:)(\\d*)\".*$']\n        meta_parse['chemspider_id'] = ['^chemspider(?:=|:)(\\d*)\".*$']\n        meta_parse['compound_class'] = ['^compound.*class(?:=|:)(.*)$']\n        meta_parse['exact_mass'] = ['^exact.*mass(?:=|:)(\\d*[.,]?\\d*)$']\n        meta_parse['smiles'] = ['^SMILES(?:=|:)(.*)$']\n        meta_parse['other_names'] = ['^Synonym(?:=|:)(.*)$']\n    elif schema == 'massbank':\n        meta_parse['name'] = ['^CH\\$NAME:\\s+(.*)$']\n        meta_parse['other_names'] = ['^CH\\$NAME:\\s+(.*)$']\n\n        meta_parse['inchikey_id'] = ['^CH\\$LINK:\\s+INCHIKEY\\s+(.*)$']\n        meta_parse['molecular_formula'] = ['^CH\\$FORMULA:\\s+(.*)$']\n        meta_parse['molecular_weight'] = ['^CH\\$MOLECULAR_WEIGHT:\\s+(.*)$']\n        meta_parse['pubchem_id'] = ['^CH\\$LINK:\\s+PUBCHEM\\s+CID:(.*)$']\n        meta_parse['chemspider_id'] = ['^CH\\$LINK:\\s+CHEMSPIDER\\s+(.*)$']\n        meta_parse['compound_class'] = ['^CH\\$COMPOUND_CLASS:\\s+(.*)$']\n        meta_parse['exact_mass'] = ['^CH\\$EXACT_MASS:\\s+(.*)$']\n        meta_parse['smiles'] = ['^CH\\$SMILES:\\s+(.*)$']\n\n    return meta_parse", "response": "Create a dictionary of regex for extracting the compound information for the spectra\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handler(self):\n        printtime('Creating and populating objects', self.start)\n        self.populate()\n        printtime('Populating {} sequence profiles'.format(self.analysistype), self.start)\n        self.profiler()\n        # Annotate sequences with prokka\n        self.annotatethreads()\n        # Run the analyses\n        self.cdsthreads()\n        # Find core coding features\n        self.cdssequencethreads()\n        # Extract the sequence for each coding feature\n        self.allelematchthreads()\n        # Determine sequence types from the analyses\n        printtime('Determining {} sequence types'.format(self.analysistype), self.start)\n        self.sequencetyper()\n        # Create reports\n        printtime('Creating {} reports'.format(self.analysistype), self.start)\n        self.reporter()", "response": "Run the required analyses and create the required objects"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef profiler(self):\n        # Initialise variables\n        profiledata = defaultdict(make_dict)\n        profileset = set()\n        genedict = {}\n        # Find all the unique profiles to use with a set\n        for sample in self.metadata.samples:\n            if sample[self.analysistype].profile != 'NA':\n                profileset.add(sample[self.analysistype].profile[0])\n        # Extract the profiles for each set\n        for sequenceprofile in profileset:\n            # Clear the list of genes\n            genelist = []\n            for sample in self.metadata.samples:\n                if sequenceprofile == sample[self.analysistype].profile[0]:\n                    genelist = [os.path.split(x)[1].split('.')[0] for x in sample[self.analysistype].alleles]\n            try:\n                # Open the sequence profile file as a dictionary\n                profile = DictReader(open(sequenceprofile))\n            # Revert to standard comma separated values\n            except KeyError:\n                # Open the sequence profile file as a dictionary\n                profile = DictReader(open(sequenceprofile))\n            # Iterate through the rows\n            for row in profile:\n                # Iterate through the genes\n                for gene in genelist:\n                    # Add the sequence profile, and type, the gene name and the allele number to the dictionary\n                    try:\n                        profiledata[sequenceprofile][row['ST']][gene] = row[gene]\n                    except KeyError:\n                        pass\n            # Add the gene list to a dictionary\n            genedict[sequenceprofile] = sorted(genelist)\n            # Add the profile data, and gene list to each sample\n            for sample in self.metadata.samples:\n                if sample.general.bestassemblyfile != 'NA':\n                    if sequenceprofile == sample[self.analysistype].profile[0]:\n                        # Populate the metadata with the profile data\n                        sample[self.analysistype].profiledata = profiledata[sample[self.analysistype].profile[0]]\n                        # Add the allele directory to a list of directories used in this analysis\n                        self.allelefolders.add(sample[self.analysistype].alleledir)\n                        dotter()", "response": "Creates a dictionary of unique sequence class names and genes for each sequence class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef annotatethreads(self):\n        # Move the files to subfolders and create objects\n        self.runmetadata = createobject.ObjectCreation(self)\n        # Fix headers\n        self.headers()\n        printtime('Performing prokka analyses', self.start)\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.annotate, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.metadata.samples:\n            # Create the prokka attribute in the metadata object\n            setattr(sample, 'prokka', GenObject())\n            sample.prokka.outputdir = os.path.join(sample.general.outputdirectory, 'prokka')\n            if not os.path.isdir(sample.prokka.outputdir):\n                os.makedirs(sample.prokka.outputdir)\n            # TODO Incorporate MASH/rMLST/user inputted genus, species results in the system call\n            # Create the system call\n            # prokka 2014-SEQ-0275.fasta --force --genus Escherichia --species coli --usegenus --addgenes\n            # --prefix 2014-SEQ-0275 --locustag EC0275 --outputdir /path/to/sequences/2014-SEQ-0275/prokka\n            sample.prokka.command = 'prokka {} ' \\\n                                    '--force ' \\\n                                    '--genus {} ' \\\n                                    '--species {} ' \\\n                                    '--usegenus ' \\\n                                    '--addgenes ' \\\n                                    '--prefix {} ' \\\n                                    '--locustag {} ' \\\n                                    '--outdir {}' \\\n                .format(sample.general.fixedheaders,\n                        self.genus, self.species, sample.name, sample.name, sample.prokka.outputdir)\n            self.queue.put(sample)\n        self.queue.join()", "response": "Create the object containing the object containing the object that is used to annotate each strain in the metadata file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef headers(self):\n        for sample in self.metadata.samples:\n            # Create an attribute to store the path/file name of the fasta file with fixed headers\n            sample.general.fixedheaders = sample.general.bestassemblyfile.replace('.fasta', '.ffn')\n            sample.general.fixedheaders = os.path.abspath(sample.general.fixedheaders)\n            # A list of contigs with modified record.id values\n            fixedheaders = list()\n            # Only do this if the file with fixed headers hasn't previously been created\n            if not os.path.isfile(sample.general.fixedheaders):\n                # Refseq genomes don't necessarily have underscores (or contig numbers) in the headers\n                count = 0\n                formatcount = '{:04d}'.format(count)\n                for record in SeqIO.parse(open(sample.general.bestassemblyfile, \"rU\"), \"fasta\"):\n                    # Split off anything following the contig number\n                    # >2013-SEQ-0129_1_length_303005_cov_13.1015_ID_17624 becomes\n                    # >2013-SEQ-0129_1\n                    record.id = record.id.split('_length')[0]\n                    # Prokka has a requirement that the header is unique and less than or equal to 20 characters\n                    if len(record.id) > 20:\n                        # Extract the contig number from the string - assumption is that this number is the final\n                        # entry in the string, and that there are underscore separating the different components\n                        contignumber = record.id.split('_')[-1] if '_' in record.id else formatcount\n                        # Subtract the length of the contig number (and an additional one for the underscore) from\n                        # 20 for the string slice, and add the contig number at the end\n                        record.id = record.id[:(20 - len(contignumber) - 1)] + '_{}'.format(formatcount)\n                    # Clear the name and description attributes of the record\n                    record.name = ''\n                    record.description = ''\n                    # Add this record to our list\n                    fixedheaders.append(record)\n                # Open the filtered assembly file\n                with open(sample.general.fixedheaders, 'w') as formatted:\n                    # Write the records in the list to the file\n                    SeqIO.write(fixedheaders, formatted, 'fasta')", "response": "Create the headers for the refseq genomes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining which core genes from a pre-calculated database are present in each strain", "response": "def cdsthreads(self):\n        \"\"\"\n        Determines which core genes from a pre-calculated database are present in each strain\n        \"\"\"\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.cds, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.metadata.samples:\n            #\n            sample[self.analysistype].corepresence = dict()\n            self.cdsqueue.put(sample)\n        self.cdsqueue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the sequence of each core gene for each strain", "response": "def cdssequencethreads(self):\n        \"\"\"\n        Extracts the sequence of each gene for each strain\n        \"\"\"\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.cdssequence, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.metadata.samples:\n            # Initialise a dictionary to store the sequence of each core gene\n            sample[self.analysistype].coresequence = dict()\n            self.sequencequeue.put(sample)\n        self.sequencequeue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate threads for all the allele elements of each gene", "response": "def allelematchthreads(self):\n        \"\"\"\n        Determine allele of each gene\n        \"\"\"\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.allelematch, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.metadata.samples:\n            sample[self.analysistype].allelematches = dict()\n            self.allelequeue.put(sample)\n        self.allelequeue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reporter(self):\n        # Initialise variables\n        header = ''\n        row = ''\n        databasedict = dict()\n        # Load the database sequence type into a dictionary\n        strainprofile = os.path.join(self.profilelocation, 'strainprofiles.txt')\n        databaseprofile = DictReader(open(strainprofile))\n        # Put the strain profile dictionary into a more easily searchable format\n        for data in databaseprofile:\n            databasedict[data['Strain']] = data['SequenceType']\n        for sample in self.metadata.samples:\n            closestmatches = list()\n            if sample[self.analysistype].reportdir != 'NA':\n                if type(sample[self.analysistype].allelenames) == list:\n                    # Populate the header with the appropriate data, including all the genes in the list of targets\n                    header = 'Strain,SequenceType,Matches,Mismatches,NA,TotalGenes,ClosestDatabaseMatch,{},\\n' \\\n                        .format(','.join(sorted(sample[self.analysistype].allelenames)))\n                sortedmatches = sorted(sample[self.analysistype].profilematches.items(),\n                                       key=operator.itemgetter(1), reverse=True)[0]\n                closestseqtype = sortedmatches[0]\n                # Pull out the closest database match\n                for strain, seqtype in databasedict.items():\n                    if seqtype == closestseqtype:\n                        closestmatches.append(strain)\n                sample[self.analysistype].closestseqtype = closestseqtype\n                nummatches = int(sortedmatches[1])\n                numna = 0\n                queryallele = list()\n                # Get all the alleles into a list\n                for gene, allele in sorted(sample[self.analysistype].profiledata[closestseqtype].items()):\n                    try:\n                        # Extract the allele (anything after the -) from the allele matches\n                        query = sample[self.analysistype].allelematches[gene].split('-')[1]\n                        if allele == query:\n                            queryallele.append(query)\n                        else:\n                            queryallele.append('{} ({})'.format(query, allele))\n                    except KeyError:\n                        queryallele.append('NA')\n                        numna += 1\n                mismatches = len(sample[self.analysistype].alleles) - nummatches - numna\n                row += '{},{},{},{},{},{},{},{}'\\\n                    .format(sample.name, closestseqtype, nummatches, mismatches, numna,\n                            len(sample[self.analysistype].alleles), ','.join(closestmatches), ','.join(queryallele))\n                row += '\\n'\n        # Create the report folder\n        make_path(self.reportpath)\n        # Create the report containing all the data from all samples\n        with open(os.path.join(self.reportpath, '{}.csv'.format(self.analysistype)), 'w') as combinedreport:\n            # Write the results to this report\n            combinedreport.write(header)\n            combinedreport.write(row)", "response": "Parse the results into a report containing the information for the current sequence type and the number of genes that are closest to the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the removes from the tail of segments.", "response": "def remove_path_segments(segments, removes):\n    \"\"\"Removes the removes from the tail of segments.\n\n    Examples::\n        >>> # '/a/b/c' - 'b/c' == '/a/'\n        >>> assert remove_path_segments(['', 'a', 'b', 'c'], ['b', 'c']) == ['', 'a', '']\n        >>> # '/a/b/c' - '/b/c' == '/a\n        >>> assert remove_path_segments(['', 'a', 'b', 'c'], ['', 'b', 'c']) == ['', 'a']\n\n    :param segments:  :class:`list`, a list of the path segment\n    :param removes:  :class:`list`, a list of the path segment\n    :return:  :class:`list`, The list of all remaining path segments after all segments\n        in ``removes`` have been removed from the end of ``segments``. If no segment\n        from ``removes`` were removed from the ``segments``, the ``segments`` is\n        return unmodified.\n\n    \"\"\"\n\n    if segments == ['']:\n        segments.append('')\n    if removes == ['']:\n        removes.append('')\n\n    if segments == removes:\n        ret = []\n    elif len(removes) > len(segments):\n        ret = segments\n    else:\n        # TODO(benjamin): incomplete\n        removes2 = list(removes)\n        if len(removes) > 1 and removes[0] == '':\n            removes2.pop(0)\n\n        if removes2 and removes2 == segments[-1 * len(removes2):]:\n            ret = segments[:len(segments) - len(removes2)]\n            if removes[0] != '' and ret:\n                ret.append('')\n        else:\n            ret = segments\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join_path_segments(*args):\n    finals = []\n    for segments in args:\n        if not segments or segments[0] == ['']:\n            continue\n        elif not finals:\n            finals.extend(segments)\n        else:\n            # Example #1: ['a',''] + ['b'] == ['a','b']\n            # Example #2: ['a',''] + ['','b'] == ['a','','b']\n            if finals[-1] == '' and (segments[0] != '' or len(segments) > 1):\n                finals.pop(-1)\n            # Example: ['a'] + ['','b'] == ['a','b']\n            elif finals[-1] != '' and segments[0] == '' and len(segments) > 1:\n                segments.pop(0)\n            finals.extend(segments)\n    return finals", "response": "This function joins multiple list of path segments together."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing the path. Turn file title. author to file author.", "response": "def normalize(self):\n        \"\"\"\n        Normalize the path. Turn /file/title/../author to /file/author\n\n        :return: <self>\n        \"\"\"\n        if str(self):\n            normalized = normpath(str(self)) + ('/' * self.is_dir)\n            if normalized.startswith('//'):  # http://bugs.python.org/636648\n                normalized = '/' + normalized.lstrip('/')\n            self.load(normalized)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nincrements a given index according to the shape of the data added by the data structure.", "response": "def increment(index, dims, data_shape):\n    \"\"\"Increments a given index according to the shape of the data added\n\n    :param index: Current index to be incremented\n    :type index: list\n    :param dims: Shape of the data that the index is being incremented by\n    :type dims: tuple\n    :param data_shape: Shape of the data structure being incremented, this is check that incrementing is correct\n    :returns: list - the incremented index\n    \"\"\"\n\n    # check dimensions of data match structure\n    inc_to_match = data_shape[1:]\n    for dim_a, dim_b in zip(inc_to_match, dims[-1*(len(inc_to_match)):]):\n        if dim_a != dim_b:\n            raise DataIndexError()\n\n    # now we can safely discard all but the highest dimension\n    inc_index = len(index) - len(data_shape)\n    inc_amount = data_shape[0]\n    # make the index and increment amount dimensions match\n    index[inc_index] += inc_amount\n\n    # finally check that we did not run over allowed dimension\n    if index[inc_index] > dims[inc_index]:\n        raise DataIndexError()\n\n    while inc_index > 0 and index[inc_index] == dims[inc_index]:\n        index[inc_index-1] +=1\n        index[inc_index:] = [0]*len(index[inc_index:])\n        inc_index -=1\n    return index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the md5 hash of a file. Memory - friendly solution is it reads the file piece by piece and returns the md5 hash of the file piece by piece.", "response": "def file_to_md5(filename, block_size=8192):\n    \"\"\"Calculate the md5 hash of a file. Memory-friendly solution,\n    it reads the file piece by piece. See stackoverflow.com/questions/1131220/\n\n    :param filename: filename to convert\n    :param block_size: size of block\n    :return: MD5 hash of file content\n    \"\"\"\n    md5 = hashlib.md5()\n    with open(filename, 'rb') as f:\n        while True:\n            data = f.read(block_size)\n            if not data:\n                break\n            md5.update(data)\n    return md5.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(cls, settings=None):\n        global _Thread\n        if not settings:\n            return\n        settings = wrap(settings)\n\n        Log.stop()\n\n        cls.settings = settings\n        cls.trace = coalesce(settings.trace, False)\n        if cls.trace:\n            from mo_threads import Thread as _Thread\n            _ = _Thread\n\n        # ENABLE CPROFILE\n        if settings.cprofile is False:\n            settings.cprofile = {\"enabled\": False}\n        elif settings.cprofile is True:\n            if isinstance(settings.cprofile, bool):\n                settings.cprofile = {\"enabled\": True, \"filename\": \"cprofile.tab\"}\n        if settings.cprofile.enabled:\n            from mo_threads import profiles\n            profiles.enable_profilers(settings.cprofile.filename)\n\n        if settings.profile is True or (is_data(settings.profile) and settings.profile.enabled):\n            Log.error(\"REMOVED 2018-09-02, Activedata revision 3f30ff46f5971776f8ba18\")\n            # from mo_logs import profiles\n            #\n            # if isinstance(settings.profile, bool):\n            #     profiles.ON = True\n            #     settings.profile = {\"enabled\": True, \"filename\": \"profile.tab\"}\n            #\n            # if settings.profile.enabled:\n            #     profiles.ON = True\n\n        if settings.constants:\n            constants.set(settings.constants)\n\n        if settings.log:\n            cls.logging_multi = StructuredLogger_usingMulti()\n            for log in listwrap(settings.log):\n                Log.add_log(Log.new_instance(log))\n\n            from mo_logs.log_usingThread import StructuredLogger_usingThread\n            cls.main_log = StructuredLogger_usingThread(cls.logging_multi)", "response": "Start the log instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop(cls):\n        main_log, cls.main_log = cls.main_log, StructuredLogger_usingStream(STDOUT)\n        main_log.stop()", "response": "STOP ALL LOGGINGS AND RETURNS TO DIRECT - TO - stdout LOGGING\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef note(\n        cls,\n        template,\n        default_params={},\n        stack_depth=0,\n        log_context=None,\n        **more_params\n    ):\n        \"\"\"\n        :param template: *string* human readable string with placeholders for parameters\n        :param default_params: *dict* parameters to fill in template\n        :param stack_depth:  *int* how many calls you want popped off the stack to report the *true* caller\n        :param log_context: *dict* extra key:value pairs for your convenience\n        :param more_params: *any more parameters (which will overwrite default_params)\n        :return:\n        \"\"\"\n        timestamp = datetime.utcnow()\n        if not is_text(template):\n            Log.error(\"Log.note was expecting a unicode template\")\n\n        Log._annotate(\n            LogItem(\n                context=exceptions.NOTE,\n                format=template,\n                template=template,\n                params=dict(default_params, **more_params)\n            ),\n            timestamp,\n            stack_depth+1\n        )", "response": "This method will annotate a log item with the given template."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef warning(\n        cls,\n        template,\n        default_params={},\n        cause=None,\n        stack_depth=0,\n        log_context=None,\n        **more_params\n    ):\n        \"\"\"\n        :param template: *string* human readable string with placeholders for parameters\n        :param default_params: *dict* parameters to fill in template\n        :param cause: *Exception* for chaining\n        :param stack_depth:  *int* how many calls you want popped off the stack to report the *true* caller\n        :param log_context: *dict* extra key:value pairs for your convenience\n        :param more_params: *any more parameters (which will overwrite default_params)\n        :return:\n        \"\"\"\n        timestamp = datetime.utcnow()\n        if not is_text(template):\n            Log.error(\"Log.warning was expecting a unicode template\")\n\n        if isinstance(default_params, BaseException):\n            cause = default_params\n            default_params = {}\n\n        if \"values\" in more_params.keys():\n            Log.error(\"Can not handle a logging parameter by name `values`\")\n\n        params = Data(dict(default_params, **more_params))\n        cause = unwraplist([Except.wrap(c) for c in listwrap(cause)])\n        trace = exceptions.extract_stack(stack_depth + 1)\n\n        e = Except(exceptions.WARNING, template=template, params=params, cause=cause, trace=trace)\n        Log._annotate(\n            e,\n            timestamp,\n            stack_depth+1\n        )", "response": "A method to create a new log entry for a warning message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nraise an exception with a trace for the cause too :param template: *string* human readable string with placeholders for parameters :param default_params: *dict* parameters to fill in template :param cause: *Exception* for chaining :param stack_depth: *int* how many calls you want popped off the stack to report the *true* caller :param log_context: *dict* extra key:value pairs for your convenience :param more_params: *any more parameters (which will overwrite default_params) :return:", "response": "def error(\n        cls,\n        template,  # human readable template\n        default_params={},  # parameters for template\n        cause=None,  # pausible cause\n        stack_depth=0,\n        **more_params\n    ):\n        \"\"\"\n        raise an exception with a trace for the cause too\n\n        :param template: *string* human readable string with placeholders for parameters\n        :param default_params: *dict* parameters to fill in template\n        :param cause: *Exception* for chaining\n        :param stack_depth:  *int* how many calls you want popped off the stack to report the *true* caller\n        :param log_context: *dict* extra key:value pairs for your convenience\n        :param more_params: *any more parameters (which will overwrite default_params)\n        :return:\n        \"\"\"\n        if not is_text(template):\n            sys.stderr.write(str(\"Log.error was expecting a unicode template\"))\n            Log.error(\"Log.error was expecting a unicode template\")\n\n        if default_params and isinstance(listwrap(default_params)[0], BaseException):\n            cause = default_params\n            default_params = {}\n\n        params = Data(dict(default_params, **more_params))\n\n        add_to_trace = False\n        if cause == None:\n            causes = None\n        elif is_list(cause):\n            causes = []\n            for c in listwrap(cause):  # CAN NOT USE LIST-COMPREHENSION IN PYTHON3 (EXTRA STACK DEPTH FROM THE IN-LINED GENERATOR)\n                causes.append(Except.wrap(c, stack_depth=1))\n            causes = FlatList(causes)\n        elif isinstance(cause, BaseException):\n            causes = Except.wrap(cause, stack_depth=1)\n        else:\n            causes = None\n            Log.error(\"can only accept Exception, or list of exceptions\")\n\n        trace = exceptions.extract_stack(stack_depth + 1)\n\n        if add_to_trace:\n            cause[0].trace.extend(trace[1:])\n\n        e = Except(context=exceptions.ERROR, template=template, params=params, cause=causes, trace=trace)\n        raise_from_none(e)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nannotating a log item with the information from the log file.", "response": "def _annotate(\n        cls,\n        item,\n        timestamp,\n        stack_depth\n    ):\n        \"\"\"\n        :param itemt:  A LogItemTHE TYPE OF MESSAGE\n        :param stack_depth: FOR TRACKING WHAT LINE THIS CAME FROM\n        :return:\n        \"\"\"\n        item.timestamp = timestamp\n        item.machine = machine_metadata\n        item.template = strings.limit(item.template, 10000)\n\n        item.format = strings.limit(item.format, 10000)\n        if item.format == None:\n            format = text_type(item)\n        else:\n            format = item.format.replace(\"{{\", \"{{params.\")\n        if not format.startswith(CR) and format.find(CR) > -1:\n            format = CR + format\n\n        if cls.trace:\n            log_format = item.format = \"{{machine.name}} (pid {{machine.pid}}) - {{timestamp|datetime}} - {{thread.name}} - \\\"{{location.file}}:{{location.line}}\\\" - ({{location.method}}) - \" + format\n            f = sys._getframe(stack_depth + 1)\n            item.location = {\n                \"line\": f.f_lineno,\n                \"file\": text_type(f.f_code.co_filename),\n                \"method\": text_type(f.f_code.co_name)\n            }\n            thread = _Thread.current()\n            item.thread = {\"name\": thread.name, \"id\": thread.id}\n        else:\n            log_format = item.format = \"{{timestamp|datetime}} - \" + format\n\n        cls.main_log.write(log_format, item.__data__())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating over to_go, building the list of parts. To provide items for the beginning, use so_far.", "response": "def _get_parts_list(to_go, so_far=[[]], ticker=None):\n    \"\"\" Iterates over to_go, building the list of parts. To provide\n    items for the beginning, use so_far.\n    \"\"\"\n    try:\n        part = to_go.pop(0)\n    except IndexError:\n        return so_far, ticker\n\n    # Lists of input groups\n    if isinstance(part, list) and any(isinstance(e, list) for e in part):\n        while len(part) > 0:\n            so_far, ticker = _get_parts_list(part, so_far, ticker)\n            ticker.tick()\n    # Input Group\n    elif isinstance(part, list) and any(isinstance(e, Input) for e in part):\n        while len(part) > 0:\n            so_far, ticker = _get_parts_list(part, so_far, ticker)\n    # Magic Inputs\n    elif isinstance(part, Input) and part.is_magic:\n        inputs = part.eval()\n        while len(inputs) > 0:\n            so_far, ticker = _get_parts_list(inputs, so_far, ticker)\n            ticker.tick()\n    # Normal inputs\n    elif isinstance(part, Input) and not part.is_magic:\n        so_far[ticker.value].append(part)\n    # Everything else\n    else:\n        so_far = _append(so_far, part)\n\n    return so_far, ticker"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a list of parts find the maximum number of commands that are contained in it.", "response": "def _get_max_size(parts, size=1):\n    \"\"\" Given a list of parts, find the maximum number of commands\n    contained in it.\n    \"\"\"\n    max_group_size = 0\n    for part in parts:\n        if isinstance(part, list):\n            group_size = 0\n            for input_group in part:\n                group_size += 1\n\n            if group_size > max_group_size:\n                max_group_size = group_size\n\n    magic_size = _get_magic_size(parts)\n    return max_group_size * magic_size"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _grow(list_of_lists, num_new):\n    first = list_of_lists[0]\n    for i in range(num_new):\n        list_of_lists.append(copy.deepcopy(first))\n    return list_of_lists", "response": "Given a list of lists and a number of new lists to add copy the first list into the new ones and add them to the list of lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a list of parts return all of the nested file parts.", "response": "def _search_for_files(parts):\n    \"\"\" Given a list of parts, return all of the nested file parts. \"\"\"\n    file_parts = []\n    for part in parts:\n        if isinstance(part, list):\n            file_parts.extend(_search_for_files(part))\n        elif isinstance(part, FileToken):\n            file_parts.append(part)\n    return file_parts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eval(self):\n        max_size = _get_max_size(self.parts)\n        parts_list = _grow([[]], max_size-1)\n\n        counter = Ticker(max_size)\n        parts = self.parts[:]\n        while len(parts) > 0:\n            parts_list, counter = _get_parts_list(parts,\n                parts_list, counter)\n\n        commands = []\n        for i, parts in enumerate(parts_list):\n            alias = self._get_alias(i+1)\n            new_parts = copy.deepcopy(parts)\n            commands.append(Command(alias=alias, parts=new_parts))\n        return commands", "response": "Returns a list of Command objects that can be evaluated as their\n        string values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, *args, **kwargs):\n        self.uid = '{}_date:{}'.format(\n            self.cycle.uid,\n            self.date\n        )\n        self.slug = '{}'.format(self.date)\n        super(ElectionDay, self).save(*args, **kwargs)", "response": "Save the object to the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting all components on component map.", "response": "def start_system(components, bind_to, hooks={}):\n    \"\"\"Start all components on component map.\"\"\"\n    deps = build_deps_graph(components)\n    started_components = start_components(components, deps, None)\n\n    run_hooks(hooks, started_components)\n\n    if type(bind_to) is str:\n        master = started_components[bind_to]\n    else:\n        master = bind_to\n\n    setattr(master, '__components', started_components)\n    return master"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntoggle the component in or out of the selection for parameter row *row*", "response": "def toggleSelection(self, row, component):\n        \"\"\"Toggles the *component* in or out of the selection \n        for parameter *row*\n\n        :param row: the ith parameter number\n        :type row: int\n        :param component: the component to toggle its selection membership\n        :type component: :class:`AbstractStimulusComponent<sparkle.stim.abstract_component.AbstractStimulusComponent>`\n        \"\"\"\n        selection = self._parameters[row]['selection']\n        if component in selection:\n            selection.remove(component)\n        else:\n            selection.append(component)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setVerifiedValue(self, row, field, value):\n        if self._parameters[row]['parameter'] == 'filename':\n            return # cannot be set this way?\n        if field == 'parameter':\n            self.setParamValue(row, parameter=value)\n        elif field in ['start', 'stop', 'step']:\n            if self.checkLimits(row, value):\n                kwd = {field : value}\n                self.setParamValue(row, **kwd)", "response": "Sets the value for the given field in the parameter\n        indexed by row only if the value is within the set limits\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the values of the parameters as field = val for the ith parameter", "response": "def setParamValue(self, row, **kwargs):\n        \"\"\"Sets the arguments as field=val for parameter\n        indexed by *row*\n\n        :param row: the ith parameter number\n        :type row: int\n        \"\"\"\n        param = self._parameters[row]\n        for key, val in kwargs.items():\n            param[key] = val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the value for the field in the ith parameter indexed by row *", "response": "def paramValue(self, row, field):\n        \"\"\"Gets the value for *field* for parameter indexed by\n        *row*\n        \n        :param row: the ith parameter number\n        :type row: int\n        :param field: detail of the parameter to set\n        :type field: str\n        :returns: value -- type appropriate to parameter\n        \"\"\"\n        if field == 'nsteps':\n            return self.numSteps(row)\n        if field in ['start', 'stop', 'step'] and self._parameters[row]['parameter'] == 'filename':\n                return '-'\n        else:\n            param = self._parameters[row]\n            return param[field]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overwriteParam(self, row, param):\n        if row == -1:\n            row = self.nrows() - 1\n        self._parameters[row] = param", "response": "Assigns the parameter to index row with the value param."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the number of steps for the parameter at index row", "response": "def numSteps(self, row):\n        \"\"\"Gets the number of steps for the parameter at \n        index *row* will yeild\n        \"\"\"\n        param = self._parameters[row]\n        return self.nStepsForParam(param)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the number of steps for a parameter", "response": "def nStepsForParam(self, param):\n        \"\"\"Gets the number of steps *parameter* will yeild\n\n        :param param: parameter to get the expansion count for\n        :type param: dict\n        \"\"\"\n        if param['parameter'] == 'filename':\n            return len(param['names'])\n        else:\n            if param['step'] > 0:\n                if abs(param['start'] - param['stop']) < param['step']:\n                    return 0\n                # print 'range', param['start'] - param['stop']\n                nsteps = np.around(abs(param['start'] - param['stop']), 4) / float(param['step'])\n                nsteps = int(np.ceil(nsteps)+1)\n            elif param['start'] == param['stop']:\n                nsteps = 1\n            else:\n                nsteps = 0\n            return nsteps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value of the detail_field of the parameter at index row from its selected components auto_details.", "response": "def getDetail(self, row, detail_field):\n        \"\"\"Gets the value of the detail *detail_field* of paramter\n        at index *row* from its selected components `auto_details`.\n        All of the selected components value for *detail_field* must\n        match\n\n        :param row: the ith parameter number\n        :type row: int\n        :param detail_field: auto_details member key\n        :type detail_field: str\n        :returns: value type appropriate for parameter\n        \"\"\"\n        param = self._parameters[row]\n        param_type = param['parameter']\n        components = param['selection']\n        if len(components) == 0 or param_type == '':\n            return None\n        # all components must match\n        matching_details = []\n        # for comp in components:\n        for comp in components:\n            alldetails = comp.auto_details()\n            if not param_type in alldetails:\n                # self.hintRequested.emit('INCOMPATABLE COMPONENTS FOR PARAMETER TYPE {}'.format(param_type))\n                return None\n            details = alldetails[param_type]\n            matching_details.append(details[detail_field])\n        matching_details = set(matching_details)\n        if len(matching_details) > 1:\n            print 'Components with mis-matched units!'\n            return None\n        return matching_details.pop()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies the value in field at index", "response": "def isFieldValid(self, row, field):\n        \"\"\"Verifies the value in *field* for parameter at index \n        *row*\n\n        :param row: the ith parameter number\n        :type row: int\n        :param field: detail of the parameter to check\n        :type field: str\n        :returns: bool -- True if valid\n        \"\"\"\n        param = self._parameters[row]\n        if param['parameter'] == '':\n            return False\n        if field == 'nsteps':\n            return self.numSteps(row) != 0\n        if param['parameter'] == 'filename':\n            # do something here... check filenames?\n            return True\n        if field == 'parameter':\n            return True\n        # else check that value is between min and max allowed\n        return self.checkLimits(row, param[field])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findFileParam(self, comp):\n        for p in self._parameters:\n            if p['parameter'] == 'filename' and comp in p['selection']:\n                return p['names']", "response": "Finds the filename auto - parameter that component comp is\n        in and returns all the filenames for that parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkLimits(self, row, value):\n        # extract the selected component names\n        param = self._parameters[row]\n        components = param['selection']\n        if len(components) == 0:\n            return False\n        ptype = param['parameter']\n        mins = []\n        maxs = []\n        for comp in components:\n            # get the limit details for the currently selected parameter type\n            try:\n                details = comp.auto_details()[ptype]\n                mins.append(details['min'])\n                maxs.append(details['max'])\n            except KeyError:\n                raise\n                return False\n        lower = max(mins)\n        upper = min(maxs)\n        if lower <= value <= upper:\n            return True\n        else:\n            # print 'value out of bounds:'\n            # print 'lower', lower, 'upper', upper, 'value', value\n            return False", "response": "Checks that the value in the minimum and maximum allowable \n        range for the parameter at index row."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting an empty parameter at the specified position in the table.", "response": "def insertRow(self, position):\n        \"\"\"Inserts an empty parameter at index *position*\n\n        :param position: order to insert new parameter to\n        :type position: int\n        \"\"\"\n        if position == -1:\n            position = self.nrows()\n        defaultparam = { 'start': 0,\n                         'step': 0,\n                         'stop': 0,\n                         'parameter': '',\n                         'selection' : [],\n                        }\n        self._parameters.insert(position, defaultparam)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef selectedParameterTypes(self, row):\n        param = self._parameters[row]\n        return self._selectionParameters(param)", "response": "Gets a list of the intersection of the editable properties in the parameteter *param*s\n        component selection. E. g. frequency intensity"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ranges(self):\n        steps = []\n        for p in self._parameters:\n            # inclusive range\n            if p['parameter'] == 'filename':\n                steps.append(p['names'])\n            else:\n                if p['step'] > 0:\n                    start = p['start']\n                    stop = p['stop']\n                    if start > stop:\n                        step = p['step']*-1\n                    else:\n                        step = p['step']\n                    # nsteps = np.ceil(np.around(abs(start - stop), 4) / p['step'])\n                    nsteps = self.nStepsForParam(p)\n                    # print 'nsteps', np.around(abs(start - stop), 4), p['step']\n                    # print 'start, stop, steps', start, stop, nsteps\n                    step_tmp = np.linspace(start, start+step*(nsteps-2), nsteps-1)\n                    # print 'step_tmp', step_tmp\n\n                    # if step_tmp[-1] != stop:\n                    step_tmp = np.append(step_tmp,stop)\n                    # print 'step range', step_tmp\n\n                    steps.append(np.around(step_tmp,4))\n                else:\n                    assert p['start'] == p['stop']\n                    steps.append([p['start']])\n        return steps", "response": "The expanded lists of values generated from the parameter fields\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all parameters that are editable in the parameter", "response": "def _selectionParameters(self, param):\n        \"\"\"see docstring for selectedParameterTypes\"\"\"\n        components = param['selection']\n        if len(components) == 0:\n            return []\n        # extract the selected component names\n        editable_sets = []\n        for comp in components:\n            # all the keys (component names) for the auto details for components in selection\n            details = comp.auto_details()\n            editable_sets.append(set(details.keys()))\n        editable_paramters = set.intersection(*editable_sets)\n        # do not allow selecting of filename from here\n        return list(editable_paramters)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngoing through selected components and set the start value", "response": "def updateComponentStartVals(self):\n        \"\"\"Go through selected components for each auto parameter and set the start value\"\"\"\n        for param in self._parameters:\n            for component in param['selection']:\n                if param['parameter'] == 'filename':\n                    component.set(param['parameter'], param['names'][0])\n                else:\n                    component.set(param['parameter'], param['start'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the row which component comp can be found in the selections of and is also a filename parameter", "response": "def fileParameter(self, comp):\n        \"\"\"Returns the row which component *comp* can be found in the \n        selections of, and is also a filename parameter\n\n        :returns: int -- the index of the (filename) parameter *comp* is a member of \n        \"\"\"\n        for row in range(self.nrows()):\n            p = self._parameters[row]\n            if p['parameter'] == 'filename':\n                # ASSUMES COMPONENT IN ONE SELECTION\n                if comp in p['selection']:\n                    return row"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify(self):\n        for row in range(self.nrows()):\n            result = self.verify_row(row)\n            if result != 0:\n                return result\n        return 0", "response": "Checks all parameters for invalidating conditions returning 0 if error"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the parameter at the given index for invalidating conditions. Returns 0 if error", "response": "def verify_row(self, row):\n        \"\"\"Checks parameter at index *row* for invalidating conditions\n\n        :returns: str -- message if error, 0 otherwise\n        \"\"\"\n        param = self._parameters[row]\n        if param['parameter'] == '':\n            return \"Auto-parameter type undefined\"\n        if len(param['selection']) == 0:\n            return \"At least one component must be selected for each auto-parameter\"\n        if param['parameter'] not in self._selectionParameters(param):\n            return 'Parameter {} not present in all selected components'.format(param['parameter'])\n        if param['parameter'] == 'filename':\n            if len(param['names']) < 1:\n                return \"No filenames in file auto-parameter list\"\n        else:\n            if param['step'] == 0 and param['start'] != param['stop']:\n                return \"Auto-parameter step size of 0 not allowed\"\n            if abs(param['stop'] - param['start']) < param['step']:\n                return \"Auto-parameter step size larger than range\"\n            if not self.checkLimits(row, param['start']):\n                return \"Auto-parameter start value invalid\"\n            if not self.checkLimits(row, param['stop']):\n                return \"Auto-parameter stop value invalid\"\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef recon(self):\n        logging.info('Running MOB-recon on Assemblies')\n        with progressbar(self.metadata) as bar:\n            commands = list()\n            for sample in bar:\n                # Create and populate the mob_recon genobject\n                setattr(sample, self.analysistype, GenObject())\n                sample[self.analysistype].outputdir = os.path.join(sample.general.outputdirectory, self.analysistype)\n                sample[self.analysistype].contig_report = os.path.join(sample[self.analysistype].outputdir,\n                                                                       'contig_report.txt')\n                sample[self.analysistype].report_dict = dict()\n                sample[self.analysistype].logout = os.path.join(sample[self.analysistype].outputdir, 'out')\n                sample[self.analysistype].logerr = os.path.join(sample[self.analysistype].outputdir, 'err')\n                make_path(sample[self.analysistype].outputdir)\n                if sample.general.bestassemblyfile != 'NA':\n                    sample.commands.mobrecon = 'mob_recon -i {fasta} -o {outdir} --run_typer -n {threads} -d {db}'\\\n                        .format(fasta=sample.general.bestassemblyfile,\n                                outdir=sample[self.analysistype].outputdir,\n                                threads=self.threads,\n                                db=os.path.join(self.databasepath, 'mob_suite'))\n                    # Ensure that the report doesn't already exist\n                    if not os.path.isfile(sample[self.analysistype].contig_report):\n                        # Run the analyses\n                        commands.append(sample.commands.mobrecon)\n            p = multiprocessing.Pool(processes=self.threads)\n            out_err = p.map(MobRecon.run_cmd, commands)\n            p.close()\n            p.join()\n            # At this point, out_err has a list of tuples with out as index 0 in each tuple and\n            # err at index 1 in each tuple. These will be in the same order as the samples, so retrieve them by index.\n            index = 0\n            for sample in bar:\n                # Write the outputs to the log file\n                out = out_err[index][0]\n                err = out_err[index][1]\n                write_to_logfile(out=sample.commands.mobrecon,\n                                 err=sample.commands.mobrecon,\n                                 logfile=self.logfile,\n                                 samplelog=sample.general.logout,\n                                 sampleerr=sample.general.logerr,\n                                 analysislog=sample[self.analysistype].logout,\n                                 analysiserr=sample[self.analysistype].logerr)\n                write_to_logfile(out=out,\n                                 err=err,\n                                 logfile=self.logfile,\n                                 samplelog=sample.general.logout,\n                                 sampleerr=sample.general.logerr,\n                                 analysislog=sample[self.analysistype].logout,\n                                 analysiserr=sample[self.analysistype].logerr)\n                index += 1", "response": "Run MOB - recon on Assemblies"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_tsv(self):\n        logging.info('Parsing MOB-recon outputs')\n        for sample in self.metadata:\n            if os.path.isfile(sample[self.analysistype].contig_report):\n                # Read in the .tsv file with pandas. Skip the comment lines\n                df = pd.read_csv(sample[self.analysistype].contig_report, delimiter='\\t')\n                for header in df:\n                    # Remove any unwanted whitespace\n                    clean_header = header.lstrip().rstrip()\n                    # primary_key is the primary key, and value is the value of the cell for that key + header combo\n                    for primary_key, value in df[header].items():\n                        # Update the dictionary with the new data\n                        try:\n                            sample[self.analysistype].report_dict[primary_key].update({clean_header: value})\n                        # Create the nested dictionary if it hasn't been created yet\n                        except KeyError:\n                            sample[self.analysistype].report_dict[primary_key] = dict()\n                            sample[self.analysistype].report_dict[primary_key].update({clean_header: value})", "response": "Read in the. tsv contig report file with pandas and create a nested dictionary of all the headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef summary_reporter(self):\n        logging.info('Creating MOB-recon summary report')\n        with open(os.path.join(self.reportpath, 'mob_recon_summary.csv'), 'w') as summary:\n            data = 'Strain,Location,Contig,Incompatibility,IncompatibilityAccession,RelaxaseType,' \\\n                   'MashNearestNeighbor,MashNeighborDistance\\n'\n            for sample in self.metadata:\n                # Initialise a dictionary to store results for the COWBAT final report\n                sample[self.analysistype].pipelineresults = dict()\n                for primarykey, results in sample[self.analysistype].report_dict.items():\n                    # Only process results if they are not calculated to be chromosomal\n                    if results['cluster_id'] != 'chromosome':\n                        data += ','.join(str(result).replace(',', ';') if str(result) != 'nan' else 'ND'\n                                         for result in [\n                                             sample.name,\n                                             results['cluster_id'],\n                                             results['contig_id'].split('|')[1],\n                                             results['rep_type'],\n                                             results['rep_type_accession'],\n                                             results['relaxase_type'],\n                                             results['mash_nearest_neighbor'],\n                                             results['mash_neighbor_distance']]\n                                         )\n                        data += '\\n'\n                        # Add the calculated incompatibility to the pipeline results for use in the final COWBAT report\n                        sample[self.analysistype].pipelineresults[results['cluster_id']] =  \\\n                            ';'.join(str(result).replace(',', ';') if str(result) != 'nan' else 'ND'\n                                     for result in [\n                                         results['rep_type']]\n                                     )\n            summary.write(data)", "response": "Parse the MOB Recon report into a summary report"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a report combining results from resfinder_assembled and mob_recon_summary reports and write it to the file amr_summary. csv", "response": "def amrsummary(self):\n        \"\"\"\n        Create a report combining results from resfinder_assembled and mob_recon_summary reports\n        \"\"\"\n        logging.info('Creating AMR summary table from ResFinder and MOB-recon outputs')\n        with open(os.path.join(self.reportpath, 'amr_summary.csv'), 'w') as amr:\n            data = 'Strain,Gene,Allele,Resistance,PercentIdentity,Contig,Location,PlasmidIncompatibilitySets\\n'\n            for sample in self.metadata:\n                # Initialise a dictionary to store a set of all the incompatibility types listed for a contig.\n                # As the inc type will only be located on one of possibly several contigs associated with a predicted\n                # plasmid, it is nice to know details about the plasmid\n                inc_dict = dict()\n                for primarykey, results in sample[self.analysistype].report_dict.items():\n                    try:\n                        inc = results['cluster_id']\n                        # Convert the rep_type field (predicted incompatibilities) into a more a consistent\n                        # format - pandas will call empty fields 'nan', which is a float\n                        rep = str(results['rep_type']).replace(',', ';') if str(results['rep_type']) != 'nan' else 'ND'\n                        # Add the incompatibility to the set\n                        try:\n                            inc_dict[inc].add(rep)\n                        except KeyError:\n                            inc_dict[inc] = set()\n                            inc_dict[inc].add(rep)\n                    except KeyError:\n                        pass\n                #\n                for primarykey, results in sample[self.analysistype].report_dict.items():\n                    try:\n                        contig = results['contig_id'].split('|')[1]\n                        # Unicycler gives contigs names such as: 3_length=187116_depth=1.60x_circular=true - test\n                        # to see if the contig name looks unicycler-like, and set the name appropriately (in this\n                        # case, it would be 3)\n                        if contig.split('_')[1].startswith('length'):\n                            contig = contig.split('_')[0]\n                        # Use the list of results from the resfinder analyses\n                        for amr_result in sample.resfinder_assembled.sampledata:\n                            # Ensure that the current contig is the same as the one in the resfinder results. Ensure\n                            # that the slice of the amr result is treated as a string. Unicycler contigs seem to be\n                            # treated as integers\n                            if contig == str(amr_result[-1]):\n                                # Set up the output string\n                                data += '{sn},'.format(sn=sample.name)\n                                # Add the resistance and MOB recon outputs for the strain\n                                data += '{amr},{mob}\\n'\\\n                                    .format(amr=','.join(str(res) if str(res) != 'nan' else 'ND' for res in\n                                                         amr_result[0:4]),\n                                            mob=','.join(str(res) if str(res) != 'nan' else 'ND' for res in\n                                                         [contig, results['cluster_id'],\n                                                          ';'.join(sorted(inc_dict[str(results['cluster_id'])]))\n                                                          ]\n                                                         )\n                                            )\n                    except KeyError:\n                        pass\n            amr.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef geneseekrsummary(self):\n        logging.info('Creating predicted plasmid-borne gene summary table')\n        with open(os.path.join(self.reportpath, 'plasmid_borne_summary.csv'), 'w') as pbs:\n            data = 'Strain,Gene,PercentIdentity,Contig,Location,PlasmidIncompatibilitySets\\n'\n            for sample in self.metadata:\n                # Create a flag to determine whether the strain name needs to be added to the data string if there\n                # were no results\n                result_bool = False\n                # Initialise a dictionary to store a set of all the incompatibility types listed for a contig.\n                # As the inc type will only be located on one of possibly several contigs associated with a predicted\n                # plasmid, it is nice to know details about the plasmid\n                inc_dict = dict()\n                # Iterate through all the MOB recon outputs to populate the incompatibility set\n                for primarykey, results in sample[self.analysistype].report_dict.items():\n                    try:\n                        inc = results['cluster_id']\n                        # Convert the rep_type field (predicted incompatibilities) into a more a consistent\n                        # format - pandas will call empty fields 'nan', which is a float\n                        rep = str(results['rep_type']).replace(',', ';') if str(results['rep_type']) != 'nan' else 'ND'\n                        # Add the incompatibility to the set\n                        try:\n                            inc_dict[inc].add(rep)\n                        except KeyError:\n                            inc_dict[inc] = set()\n                            inc_dict[inc].add(rep)\n                    except KeyError:\n                        pass\n                for primarykey, results in sample[self.analysistype].report_dict.items():\n                    try:\n                        contig = results['contig_id'].split('|')[1]\n                        # Unicycler gives contigs names such as: 3_length=187116_depth=1.60x_circular=true - test\n                        # to see if the contig name looks unicycler-like, and set the name appropriately (in this\n                        # case, it would be 3)\n                        if contig.split('_')[1].startswith('length'):\n                            contig = contig.split('_')[0]\n                        for gene, result_dict in sample.geneseekr_results.sampledata.items():\n                            if contig == result_dict['query_id']:\n                                percent_identity = result_dict['PercentIdentity']\n                                # Set up the output string if the percent identity of the match is greater than the\n                                # cutoff\n                                if float(result_dict['PercentIdentity']) >= self.cutoff:\n                                    # As there was at least a single gene passing the threshold, set the boolean to True\n                                    result_bool = True\n                                    data += '{sn},'.format(sn=sample.name)\n                                    data += '{gene},{pi},{contig},{cid},{inc}\\n'\\\n                                        .format(gene=gene,\n                                                pi=percent_identity,\n                                                contig=contig,\n                                                cid=results['cluster_id'],\n                                                inc=';'.join(sorted(inc_dict[str(results['cluster_id'])])))\n                    except KeyError:\n                        pass\n                # If there were no results associated with the strain, make the row the strain name only\n                if not result_bool:\n                    data += '{sn}\\n'.format(sn=sample.name)\n            # Write the string to the report\n            pbs.write(data)", "response": "Create a report combining GeneSeekr and MOB Recon outputs and a GeneSeekr summary table"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns a command in a subshell.", "response": "def _execute(cmd):\n        \"\"\"Run a command in a subshell.\"\"\"\n        cmd_parts = shlex.split(cmd)\n        if sys.version_info[0] < 3:\n\n            cmd_parts = shlex.split(cmd.encode('ascii'))\n\n        proc = subprocess.Popen(\n            cmd_parts,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        out, err = proc.communicate()\n        if proc.returncode != 0:\n\n            raise subprocess.CalledProcessError(\n                returncode=proc.returncode,\n                cmd=cmd,\n                output=err,\n            )\n\n        return CommandResult(\n            code=proc.returncode,\n            out=out.decode('utf8'),\n            err=err.decode('utf8'),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the full path to a command in the virtual if it exists.", "response": "def cmd_path(self, cmd):\n        \"\"\"Get the path of a command in the virtual if it exists.\n\n        Args:\n            cmd (str): The command to look for.\n\n        Returns:\n            str: The full path to the command.\n\n        Raises:\n            ValueError: If the command is not present.\n        \"\"\"\n        for binscript in self.bin.files:\n\n            if binscript.path.endswith('/{0}'.format(cmd)):\n\n                return binscript.path\n\n        raise ValueError('The command {0} was not found.'.format(cmd))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a python script using the virtual environment python.", "response": "def python(self, cmd):\n        \"\"\"Execute a python script using the virtual environment python.\"\"\"\n        python_bin = self.cmd_path('python')\n        cmd = '{0} {1}'.format(python_bin, cmd)\n        return self._execute(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute some pip function using the virtual environment pip.", "response": "def pip(self, cmd):\n        \"\"\"Execute some pip function using the virtual environment pip.\"\"\"\n        pip_bin = self.cmd_path('pip')\n        cmd = '{0} {1}'.format(pip_bin, cmd)\n        return self._execute(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter multiple BLAST hits in a common region of the genome. Leaves only the best hit.", "response": "def filterunique(self):\n        \"\"\"\n        Filters multiple BLAST hits in a common region of the genome. Leaves only the best hit\n        \"\"\"\n        for sample in self.metadata:\n            # Initialise variables\n            sample[self.analysistype].blastresults = list()\n            resultdict = dict()\n            rowdict = dict()\n            try:\n                # Iterate through all the contigs, which had BLAST hits\n                for contig in sample[self.analysistype].queryranges:\n                    # Find all the locations in each contig that correspond to the BLAST hits\n                    for location in sample[self.analysistype].queryranges[contig]:\n                        # Extract the BLAST result dictionary for the contig\n                        for row in sample[self.analysistype].results[contig]:\n                            # Initialise variable to reduce the number of times row['value'] needs to be typed\n                            contig = row['query_id']\n                            high = row['high']\n                            low = row['low']\n                            percentidentity = row['percentidentity']\n                            # Join the two ranges in the location list with a comma\n                            locstr = ','.join([str(x) for x in location])\n                            # Create a set of the location of all the base pairs between the low and high (-1) e.g.\n                            # [6, 10] would give 6, 7, 8, 9, but NOT 10. This turns out to be useful, as there are\n                            # genes located back-to-back in the genome e.g. strB and strA, with locations of 2557,3393\n                            # and 3393,4196, respectively. By not including 3393 in the strB calculations, I don't\n                            # have to worry about this single bp overlap\n                            loc = set(range(low, high))\n                            # Use a set intersection to determine whether the current result overlaps with location\n                            # This will allow all the hits to be grouped together based on their location\n                            if loc.intersection(set(range(location[0], location[1]))):\n                                # Populate the grouped hits for each location\n                                try:\n                                    resultdict[contig][locstr].append(percentidentity)\n                                    rowdict[contig][locstr].append(row)\n                                # Initialise and populate the lists of the nested dictionary\n                                except KeyError:\n                                    try:\n                                        resultdict[contig][locstr] = list()\n                                        resultdict[contig][locstr].append(percentidentity)\n                                        rowdict[contig][locstr] = list()\n                                        rowdict[contig][locstr].append(row)\n                                    # As this is a nested dictionary, it needs to be initialised here\n                                    except KeyError:\n                                        resultdict[contig] = dict()\n                                        resultdict[contig][locstr] = list()\n                                        resultdict[contig][locstr].append(percentidentity)\n                                        rowdict[contig] = dict()\n                                        rowdict[contig][locstr] = list()\n                                        rowdict[contig][locstr].append(row)\n            except KeyError:\n                pass\n            # Find the best hit for each location based on percent identity\n            for contig in resultdict:\n                # Do not allow the same gene to be added to the dictionary more than once\n                genes = list()\n                for location in resultdict[contig]:\n                    # Initialise a variable to determine whether there is already a best hit found for the location\n                    multiple = False\n                    # Iterate through the BLAST results to find the best hit\n                    for row in rowdict[contig][location]:\n                        # Add the best hit to the .blastresults attribute of the object\n                        if row['percentidentity'] == max(resultdict[contig][location]) and not multiple \\\n                                and row['subject_id'] not in genes:\n                            sample[self.analysistype].blastresults.append(row)\n                            genes.append(row['subject_id'])\n                            multiple = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef makedbthreads(self):\n        # Find all the target folders in the analysis and add them to the targetfolders set\n        for sample in self.metadata:\n            if sample[self.analysistype].combinedtargets != 'NA':\n                self.targetfolders.add(sample[self.analysistype].targetpath)\n        # Create and start threads for each fasta file in the list\n        for i in range(len(self.targetfolders)):\n            # Send the threads to makeblastdb\n            threads = Thread(target=self.makeblastdb, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        # Make blast databases for MLST files (if necessary)\n        for targetdir in self.targetfolders:\n            # List comprehension to remove any previously created database files from list\n            self.targetfiles = glob(os.path.join(targetdir, '*.fasta'))\n            try:\n                _ = self.targetfiles[0]\n            except IndexError:\n                self.targetfiles = glob(os.path.join(targetdir, '*.fasta'))\n            for targetfile in self.targetfiles:\n                # Read the sequences from the target file to a dictionary\n                self.records[targetfile] = SeqIO.to_dict(SeqIO.parse(targetfile, 'fasta'))\n                # Add the fasta file to the queue\n                self.dqueue.put(targetfile)\n        self.dqueue.join()", "response": "Setup and create threads for each target file in the list of target folders and makeblastdb for each MLST file in the list of target files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makeblastdb(self):\n        while True:  # while daemon\n            fastapath = self.dqueue.get()  # grabs fastapath from dqueue\n            # remove the path and the file extension for easier future globbing\n            db = os.path.splitext(fastapath)[0]\n            nhr = '{}.nhr'.format(db)  # add nhr for searching\n            # fnull = open(os.devnull, 'w')  # define /dev/null\n            if not os.path.isfile(str(nhr)):  # if check for already existing dbs\n                # Create the databases\n                # TODO use MakeBLASTdb class\n                threadlock = threading.Lock()\n                command = 'makeblastdb -in {} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {}'.format(fastapath, db)\n                # subprocess.call(shlex.split('makeblastdb -in {} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {}'\n                #                            .format(fastapath, db)), stdout=fnull, stderr=fnull)\n                out, err = run_subprocess(command)\n                threadlock.acquire()\n                write_to_logfile(command, command, self.logfile, None, None, None, None)\n                write_to_logfile(out, err, self.logfile, None, None, None, None)\n                threadlock.release()\n            self.dqueue.task_done()", "response": "Make blast database files from targets as necessary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef blastnthreads(self):\n        # Create the threads for the BLAST analysis\n        for i in range(self.cpus):\n            threads = Thread(target=self.runblast, args=())\n            threads.setDaemon(True)\n            threads.start()\n        # Populate threads for each gene, genome combination\n        for sample in self.metadata:\n            make_path(sample[self.analysistype].reportdir)\n            sample[self.analysistype].report = os.path.join(\n                sample[self.analysistype].reportdir, '{}.csv'.format(sample.name))\n            if sample[self.analysistype].combinedtargets != 'NA':\n                # Add each fasta file combination to the threads\n                self.blastqueue.put((sample.general.bestassemblyfile, sample[self.analysistype].combinedtargets,\n                                     sample))\n        # Join the threads\n        self.blastqueue.join()", "response": "Setup and create threads for blastn and xml path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _qstr(self, question):\n        \"we need to cope with a list, or a list of lists\"\n        parts = []\n        for entry in question:\n            if type(entry) is list:\n                parts.append(self._qstr(entry))\n            else:\n                parts.append('\"%s\"<%d>' % (self._count_data.get_candidate_title(entry), entry))\n        return ', '.join(parts)", "response": "we need to cope with a list or a list of lists"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_callback(self):\n        def __callback(question_posed):\n            logger.debug(\"%s: asked to choose between: %s\" % (self._name, self._qstr(question_posed)))\n            if self._upto == len(self._data):\n                logger.error(\"%s: out of automation data, requested to pick between %s\" % (self._name, self._qstr(question_posed)))\n                raise AutomationException(\"out of automation data\")\n            question_archived, answer = self._data[self._upto]\n            if question_archived != question_posed:\n                logger.error(\"%s: automation data mismatch, expected question `%s', got question `%s'\" % (self._name, self._qstr(question_archived), self._qstr(question_posed)))\n            resp = question_posed.index(answer)\n            self._upto += 1\n            return resp\n        return __callback", "response": "create a callback that returns the index of the answer that was selected between the question_posed and the next answer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery the HEK API for a given time", "response": "def query_hek(time, time_window=1):\n    \"\"\"\n    requests hek responses for a given time\n    :param time: datetime object\n    :param time_window: how far in hours on either side of the input time to look for results\n    :return: hek response list\n    \"\"\"\n    hek_client = hek.HEKClient()\n    start_time = time - timedelta(hours=time_window)\n    end_time = time + timedelta(hours=time_window)\n    responses = hek_client.query(hek.attrs.Time(start_time, end_time))\n    return responses"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_thmap(suvi_data, responses, config, include_human=False,\n               origins=('SPoCA', 'Flare Detective - Trigger Module'),\n               themes=('AR', 'CH', 'FI', 'FL')):\n    \"\"\"\n    constructs thematic map from input information\n    :param suvi_data: (header, data) for a suvi image, prefer 195\n    :param responses: a list of hek responses\n    :param config: a configuration object from suvitrainer\n    :param include_human: if true, will automatically include all human labels\n    :param origins: which systems to use in thematic map from hek\n    :param themes: which hek labels to use\n    :return: a (m,n) array of hek labels as thematic map image gridded to suvi_data\n    \"\"\"\n    theme_name_map = {'AR': \"bright_region\", \"CH\": \"coronal_hole\", \"FI\": \"filament\", \"FL\": \"flare\"}\n    suvimap = sunpy.map.Map(suvi_data[1], suvi_data[0])  # scaled version of the above for nice display\n    thmap = np.zeros_like(suvi_data[1])\n    responses = sorted(responses, key=lambda e: e['event_type'])\n    for response in responses:\n        if response['event_type'] == 'FI' or response['event_type'] == 'FL':\n            print(response['event_type'], response['frm_name'])\n        if response['event_type'] in themes and \\\n                ((response['frm_name'] in origins) or (include_human and response['frm_humanflag']=='true')):\n            if response['frm_name'] == \"SPoCA\":\n                p1 = response['hpc_boundcc'][9:-2]\n            else:\n                p1 = response[\"hpc_bbox\"][9:-2]\n            p2 = p1.split(',')\n            p3 = [v.split(\" \") for v in p2]\n            date = parse_time(response['event_starttime'])\n\n            boundary = SkyCoord(\n                [(float(v[0]), float(v[1])) * u.arcsec for v in p3],\n                obstime=date,\n                frame=frames.Helioprojective)\n            xs, ys = boundary.to_pixel(suvimap.wcs)\n            xx, yy = polygon(xs, ys)\n            thmap[yy, xx] = config.solar_class_index[theme_name_map[response['event_type']]]\n    return thmap", "response": "Creates a Thematic Map from a list of suvi image responses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    args = get_args()\n    config = Config(args.config)\n\n    # Load dates\n    if os.path.isfile(args.dates):\n        with open(args.dates) as f:\n            dates = [dateparser.parse(line.split(\" \")[0]) for line in f.readlines()]\n    else:  # assume it's a date\n        dates = [dateparser.parse(args.dates)]\n\n    if args.verbose:\n        print(\"Dates are:\")\n        for date in dates:\n            print(date)\n\n    for date in dates:\n        if args.verbose:\n            print('Processing {}'.format(date))\n        suvi_data = Fetcher(date, ['suvi-l2-ci195'],\n                            suvi_composite_path=config.suvi_composite_path).fetch(multithread=False)['suvi-l2-ci195']\n        if suvi_data[0] is not None:\n            config.expert = 'HEK'\n            responses = query_hek(date)\n            thmap = make_thmap(suvi_data, responses, config)\n            Outgest(os.path.join(args.output, \"thmap_hek_{}.fits\".format(date.strftime(\"%Y%m%d%H%M%S\"))),\n                    thmap, {\"c195\": suvi_data[0], \"suvi-l2-ci195\": suvi_data[0]}, args.config).save()", "response": "This function fetches hek data and makes thematic maps as requested\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_jobs_from_template(self, template, job_class):\n        jobs = []\n        for command in template.eval():\n            alias = command.alias\n            depends_on = [job.alias\n                for job in self.queue.all_jobs\n                    for deps in command.depends_on\n                        if deps == job.alias]\n            command.update_dependent_files([job.command\n                for job in self.queue.all_jobs\n                    if not isinstance(job, JobTemplate)])\n\n            job = job_class(alias, command, depends_on)\n            jobs.append(job)\n        return jobs", "response": "Given a template a job class construct jobs from the given template."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the data of the section.", "response": "def get_section_data(self, name):\n        \"\"\"Get the data of the section.\"\"\"\n        logging.debug(_('Obtaining PE section: %s'), name)\n        for section in self.binary.sections:\n            if section.Name.rstrip(b'\\x00') == name:\n                return section.get_data()\n        return b''"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_export_table(self):\n        symbols = self.binary.DIRECTORY_ENTRY_EXPORT.symbols\n        names = AttrsGetter(symbols, join=False).name\n        return names", "response": "Get the export table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resolve_pkix_certificate(url):\n  http = urllib3.PoolManager()\n  rsp = http.request('GET', url, headers={'Content-Type': 'application/pkix-cert'})\n  if rsp.status == 200:\n    # if strict_compliance and 'application/x-x509-ca-cert' not in rsp.headers:\n    #   # This web server's response isn't following the RFC, but might contain\n    #   # data representing a DER encoded certificate.\n    #   return\n    try:\n      return load_certificate(crypto.FILETYPE_ASN1, rsp.data)\n    except crypto.Error:\n      log.error('Failed to load DER encoded certificate from %s', url)\n    try:\n      return load_certificate(crypto.FILETYPE_PEM, rsp.data)\n    except crypto.Error:\n      log.error('Failed to load PEM encoded certificate from %s', url)\n    raise RuntimeError('Failed to load any certificate from %s', url)\n  else:\n    raise RuntimeError('Failed to fetch intermediate certificate at {0}!'.format(url))", "response": "Resolve a certificate from a remote host."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_pkey(key_type=crypto.TYPE_RSA, key_bits=4096):\n  key = crypto.PKey()\n  key.generate_key(key_type, key_bits)\n  return key", "response": "Make a public key pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a certificate signing request.", "response": "def make_certificate_signing_request(pkey, digest='sha512', **name):\n  \"\"\"Make a certificate signing request.\n\n  :param OpenSSL.crypto.PKey pkey: A private key.\n  :param str digest: A valid digest to use. For example, `sha512`.\n  :param name: Key word arguments containing subject name parts: C, ST, L, O,\n    OU, CN.\n  :return: A certificate signing request.\n  :rtype: :class:`OpenSSL.crypto.X509Request`\n\n  \"\"\"\n  csr = crypto.X509Req()\n  subj = csr.get_subject()\n  subj.C = name.get('C', 'US')\n  subj.ST = name.get('ST', 'CA')\n  subj.L = name.get('L', 'Home')\n  subj.O = name.get('O', 'Home')\n  subj.OU = name.get('OU', 'Unit')\n  subj.CN = name.get('CN', 'Common')\n  csr.set_pubkey(pkey)\n  csr.set_version(3)\n  csr.sign(pkey, digest)\n  return csr"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a certificate. The following extensions are added to all certificates in the following order *before* additional extensions specified by `exts` kwarg: - subjectKeyIdentifier - authorityKeyIdentifier :param OpenSSL.crypto.X509Request csr: A certificate signing request. :param OpenSSL.crypto.PKey ca_key: The signing authority's key. :param OpenSSL.crypto.X509 ca_cert: The signing authority's certificate. :param int serial: A serial number. :param int not_before: A number of seconds from now to wait before the certificate is valid. :param int not_after: A number of seconds from now to wait before expiring the certificate. :param str digest: A valid digest. :param int version: The version of SSL to use with this certificate. :param list[OpenSSL.crypto.X509Extension] exts: A list of extensions to add to this certificate. :return: A X.509 certificate. :rtype: :class:`OpenSSL.crypto.X509`", "response": "def make_certificate(csr, ca_key, ca_cert, serial, not_before, not_after, digest='sha512', version=2, exts=()):\n  \"\"\"Make a certificate.\n\n  The following extensions are added to all certificates in the following order\n  *before* additional extensions specified by `exts` kwarg:\n\n    - subjectKeyIdentifier\n    - authorityKeyIdentifier\n\n  :param OpenSSL.crypto.X509Request csr: A certificate signing request.\n  :param OpenSSL.crypto.PKey ca_key: The signing authority's key.\n  :param OpenSSL.crypto.X509 ca_cert: The signing authority's certificate.\n  :param int serial: A serial number.\n  :param int not_before: A number of seconds from now to wait before the certificate is valid.\n  :param int not_after: A number of seconds from now to wait before expiring the certificate.\n  :param str digest: A valid digest.\n  :param int version: The version of SSL to use with this certificate.\n  :param list[OpenSSL.crypto.X509Extension] exts: A list of extensions to add to this certificate.\n  :return: A X.509 certificate.\n  :rtype: :class:`OpenSSL.crypto.X509`\n\n  \"\"\"\n  crt = crypto.X509()\n  crt.set_serial_number(serial)\n  crt.gmtime_adj_notBefore(not_before)\n  crt.gmtime_adj_notAfter(not_after)\n  crt.set_issuer(ca_cert.get_subject())\n  crt.set_subject(csr.get_subject())\n  crt.set_pubkey(csr.get_pubkey())\n  crt.set_version(version)\n\n  crt.add_extensions([\n    crypto.X509Extension(b'subjectKeyIdentifier', False, b'hash', subject=crt)])\n  if ca_cert.get_subject() == crt.get_subject():\n    crt.add_extensions([\n      crypto.X509Extension(b'authorityKeyIdentifier', False, b'keyid:always', issuer=crt)])\n  else:\n    crt.add_extensions([\n      crypto.X509Extension(b'authorityKeyIdentifier', False, b'keyid:always', issuer=ca_cert)])\n\n  crt.add_extensions(exts)\n\n  crt.sign(ca_key, digest)\n  return crt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a certificate authority.", "response": "def make_certificate_authority(**name):\n  \"\"\"Make a certificate authority.\n\n  A certificate authority can sign certificates. For clients to be able to\n  validate certificates signed by your certificate authorithy, they must trust\n  the certificate returned by this function.\n\n  :param name: Key word arguments containing subject name parts: C, ST, L, O,\n    OU, CN.\n  :return: A root self-signed certificate to act as an authority.\n  :rtype: :class:`OpenSSL.crypto.X509`\n\n  \"\"\"\n  key = make_pkey()\n  csr = make_certificate_signing_request(key, **name)\n  crt = make_certificate(csr, key, csr, make_serial(), 0, 10 * 365 * 24 * 60 * 60, exts=[crypto.X509Extension(b'basicConstraints', True, b'CA:TRUE')])\n  return key, crt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a certificate using the same API as a .", "response": "def load_certificate(filetype, buf):\n  \"\"\"Load a certificate and patch in incubating functionality.\n\n  Load a certificate using the same API as\n  :func:`OpenSSL.crypto.load_certificate` so clients can use this function as a\n  drop in replacement. Doing so patches in *incubating* functionality:\n  functionality that is not yet (or possibly will never be) present in\n  pyOpenSSL.\n\n  :param int filetype: The type of data in ``buf`` -- either\n    :py:data:`OpenSSL.crypto.FILETYPE_PEM` or\n    :py:data:`OpenSSL.crypto.FILETYPE_ASN1`.\n  :param str buf: The buffer containing the certificate.\n  \"\"\"\n  x509cert = crypto.load_certificate(filetype, buf)\n  patch_certificate(x509cert)\n  return x509cert"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload one or multiple X.509 certificates from a buffer.", "response": "def load_x509_certificates(buf):\n  \"\"\"Load one or multiple X.509 certificates from a buffer.\n\n  :param str buf: A buffer is an instance of `basestring` and can contain multiple\n    certificates.\n  :return: An iterator that iterates over certificates in a buffer.\n  :rtype: list[:class:`OpenSSL.crypto.X509`]\n\n  \"\"\"\n  if not isinstance(buf, basestring):\n    raise ValueError('`buf` should be an instance of `basestring` not `%s`' % type(buf))\n\n  for pem in re.findall('(-----BEGIN CERTIFICATE-----\\s(\\S+\\n*)+\\s-----END CERTIFICATE-----\\s)', buf):\n    yield load_certificate(crypto.FILETYPE_PEM, pem[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_external_ip():\n    response = requests.get('http://icanhazip.com/')\n    if not response.ok:\n        raise RuntimeError('Failed to get external ip: %s' % response.content)\n    return response.content.strip()", "response": "Returns the current external IP based on http://icanhazip. com."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns all the methods in the COWBAT database", "response": "def cowbat(self):\n        \"\"\"\n        Run all the methods\n        \"\"\"\n        logging.info('Beginning COWBAT database downloads')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'genesippr')):\n            self.sipprverse_targets(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'coregenome')):\n            self.cowbat_targets(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'ConFindr')):\n            self.confindr_targets()\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mash')):\n            self.mash(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'MLST')):\n            self.mlst(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'rMLST')):\n            self.rmlst(databasepath=self.databasepath,\n                       credentials=self.credentials)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'univec')):\n            self.univec(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'resfinder')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='resfinder',\n                                   dbname='resfinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'virulence')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='virulence',\n                                   dbname='virulencefinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'serosippr')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='serosippr',\n                                   dbname='serotypefinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'pointfinder')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='pointfinder',\n                                   dbname='pointfinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'clark')):\n            self.clark(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mob_suite')):\n            self.mob_suite_targets()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a subset of the sipprverse methods and download the results", "response": "def sipprverse_full(self):\n        \"\"\"\n        Run a subset of the methods - only the targets used in the sipprverse are required here\n        \"\"\"\n        logging.info('Beginning sipprverse full database downloads')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'genesippr')):\n            self.sipprverse_targets(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'ConFindr')):\n            self.confindr_targets()\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mash')):\n            self.mash(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'MLST')):\n            self.mlst(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'rMLST')):\n            self.rmlst(databasepath=self.databasepath,\n                       credentials=self.credentials)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'resfinder')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='resfinder',\n                                   dbname='resfinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'virulence')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='virulence',\n                                   dbname='virulencefinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'serosippr')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='serosippr',\n                                   dbname='serotypefinder_db')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sipprverse_method(self):\n        logging.info('Beginning sipprverse method database downloads')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'genesippr')):\n            self.sipprverse_targets(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'ConFindr')):\n            self.confindr_targets()\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mash')):\n            self.mash(databasepath=self.databasepath)", "response": "Reduced subset again. Only sipprverse MASH and confindr targets are required."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sipprverse_targets(self, databasepath, database_name='sipprverse', download_id='13699538'):\n        self.custom_databases(databasepath=databasepath,\n                              database_name=database_name,\n                              download_id=download_id)", "response": "Download sipprverse targets for OLC - specific database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload OLC - specific COWBAT targets", "response": "def cowbat_targets(self, databasepath, database_name='COWBAT', download_id='13197437'):\n        \"\"\"\n        Download OLC-specific COWBAT targets\n        :param databasepath: path to use to save the database\n        :param database_name: name of current database\n        :param download_id: figshare identifier of .tar.gz file\n        \"\"\"\n        self.custom_databases(databasepath=databasepath,\n                              database_name=database_name,\n                              download_id=download_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confindr_targets(self, database_name='ConFindr'):\n        logging.info('Downloading ConFindr databases.')\n        # NOTE: Need ConFindr >= 0.5.0 for this to work.\n        secret_file = os.path.join(self.credentials, 'secret.txt')\n        confindr_db_setup.setup_confindr_database(output_folder=os.path.join(self.databasepath, database_name),\n                                                  consumer_secret=secret_file)", "response": "Download OLC - specific ConFindr targets."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mob_suite_targets(self, database_name='mob_suite'):\n        logging.info('Download MOB-suite databases')\n        # NOTE: This requires mob_suite >=1.4.9.1. Versions before that don't have the -d option.\n        cmd = 'mob_init -d {}'.format(os.path.join(self.databasepath, database_name))\n        out, err = run_subprocess(cmd)", "response": "Download MOB - suite databases"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload the MLST databases and create the complete. txt file for each alleles and profile.", "response": "def mlst(databasepath, genera=('Escherichia', 'Vibrio', 'Campylobacter', 'Listeria',\n                                   'Bacillus', 'Staphylococcus', 'Salmonella')):\n        \"\"\"\n        Download the necessary up-to-date MLST profiles and alleles\n        :param databasepath: path to use to save the database\n        :param genera: default genera for which alleles and profiles should be downloaded\n        \"\"\"\n        logging.info('Downloading MLST databases')\n        for genus in genera:\n            # Create an object to pass to the get_mlst script\n            args = MetadataObject()\n            # Populate the object with the necessary attributes\n            args.genus = genus\n            args.repository_url = 'http://pubmlst.org/data/dbases.xml'\n            args.force_scheme_name = False\n            args.path = os.path.join(databasepath, 'MLST', genus)\n            # Create the name of the file to be used to determine if the database download and setup was successful\n            completefile = os.path.join(args.path, 'complete')\n            # Only download the files if the download was not previously successful\n            if not os.path.isfile(completefile):\n                # Run the download\n                get_mlst.main(args)\n                # Create and populate the complete.txt file\n                with open(completefile, 'w') as complete:\n                    complete.write('\\n'.join(glob(os.path.join(args.path, '*'))))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rmlst(databasepath, credentials):\n        logging.info('Downloading rMLST database')\n        # Set the name of the file to be used to determine if the database download and set-up was successful\n        completefile = os.path.join(databasepath, 'rMLST', 'complete')\n        if not os.path.isfile(completefile):\n            # Create an object to send to the rMLST download script\n            args = MetadataObject()\n            # Add the path and start time attributes\n            args.path = databasepath\n            args.logging = logging\n            args.credentials = credentials\n            # Run the rMLST download\n            get_rmlst.Get(args)\n            # Create and populate the complete.txt file\n            with open(completefile, 'w') as complete:\n                complete.write('\\n'.join(glob(os.path.join(databasepath, 'rMLST', '*'))))", "response": "Download and set up the most up - to - date profiles and alleles from pubmlst."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clark(self, databasepath):\n        if self.clarkpath:\n            logging.info('Downloading CLARK database')\n            # Create the folder in which the database is to be stored\n            databasepath = self.create_database_folder(databasepath, 'clark')\n            # Set the call to create the database - use the --light option, as we don't require the full database\n            targetcall = 'cd {clarkpath} && ../opt/clark/set_targets.sh {dbpath} bacteria --species --light'\\\n                .format(clarkpath=self.clarkpath,\n                        dbpath=databasepath)\n            # Download the database\n            self.database_clone(targetcall, databasepath)\n        else:\n            logging.warning('No CLARK scripts detected in $PATH. Cannot download database.')", "response": "Download and set - up the CLARK database using the set_targets. sh script."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads the pre - computed RefSeq database and compress it with gzip", "response": "def mash(self, databasepath):\n        \"\"\"\n        Download the pre-computed sketch of the RefSeq database, and compress it with gzip\n        :param databasepath: path to use to save the database\n        \"\"\"\n        logging.info('Downloading pre-computed RefSeq MASH sketches')\n        # Create the folder in which the database is to be stored\n        databasepath = self.create_database_folder(databasepath=databasepath,\n                                                   database='mash')\n        output_file = os.path.join(databasepath, 'assembly_summary_refseq.txt')\n        # Download the assembly summary RefSeq document\n        if not os.path.isfile(output_file):\n            self.database_download(output_file=output_file,\n                                   database_path=databasepath,\n                                   target_url='ftp://ftp.ncbi.nih.gov/genomes/ASSEMBLY_REPORTS/'\n                                              'assembly_summary_refseq.txt'\n                                   )\n        # Set the call to create the database\n        output_file = os.path.join(databasepath, 'RefSeqSketchesDefaults.msh')\n        # Download the database\n        if not os.path.isfile(output_file):\n            self.database_download(output_file=output_file,\n                                   database_path=databasepath,\n                                   target_url='https://gembox.cbcb.umd.edu/mash/refseq.genomes.k21s1000.msh',\n                                   complete=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads the UniVec core database and save it to the databasepath", "response": "def univec(self, databasepath):\n        \"\"\"\n        Download the UniVec core database\n        :param databasepath: path to use to save the database\n        \"\"\"\n        logging.info('Downloading univec database')\n        databasepath = self.create_database_folder(databasepath, 'univec')\n        # Set the name of the output file\n        outputfile = os.path.join(databasepath, 'UniVec_core.tfa')\n        target_url = 'ftp://ftp.ncbi.nlm.nih.gov/pub/UniVec/UniVec_Core'\n        self.database_download(output_file=outputfile,\n                               target_url=target_url,\n                               database_path=databasepath)\n        # Create a copy of the file with a .fasta extension\n        if os.path.isfile(outputfile):\n            renamed = os.path.splitext(outputfile)[0] + '.fasta'\n            shutil.copy(outputfile, renamed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef url_request(target_url, output_file):\n        # Create the request\n        request = urllib.request.urlopen(target_url)\n        # Open the destination file to write\n        with open(output_file, 'wb') as targets:\n            # Calculate the total file size - will be used by the progress bar\n            total_length = int(request.headers.get('content-length'))\n            # Create a click progress bar using the total length calculated above\n            with click.progressbar(length=total_length,\n                                   label='Downloading files') as bar:\n                while True:\n                    # Break up the download into chunks of 4096 bytes\n                    data = request.read(4096)\n                    # Break the loop when the download finishes/errors\n                    if not data:\n                        break\n                    # Write the chunk to file\n                    targets.write(data)\n                    # Update the progress bar\n                    bar.update(len(data))", "response": "Download the requested file from the target URL and write it to the output file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading and extract a. tar. gz file from a specific database.", "response": "def custom_databases(self, databasepath, database_name, download_id, f_type='files', post_id=None,\n                         compression='tar', nested=False, complete=False):\n        \"\"\"\n        Download and extract a .tar.gz file from figshare\n        :param databasepath: Name and path of where the database files are to be downloaded\n        :param database_name: Name of the database e.g. sipprverse\n        :param download_id: Figshare ID of the targets file\n        :param f_type: STR MOB-suite databases have the 'articles' keyword in the figshare URL, while OLC databases\n        all have the 'files' keyword\n        :param post_id: STR MOB-suite databases have 'versions/1' appended at the end of the figshare URL.\n        :param compression: STR MOB-suite databases are .zip files, while OLC databases are .tar.gz\n        :param nested: Boolean of whether the targets file has nested folders that must be accounted for\n        :param complete: Boolean of whether the completefile should be completed\n        \"\"\"\n        logging.info('Downloading {} databases'.format(database_name))\n        # ConFindr has a nested 'databases' folder\n        if nested:\n            databasepath = os.path.join(databasepath, database_name)\n        # Set the name and path of the file that is created when the download is successful\n        completefile = os.path.join(databasepath, 'complete')\n        # Create the database folder if necessary\n        make_path(databasepath)\n        # Set the name of the targets file\n        tar_file = os.path.join(databasepath, download_id)\n        # Create the target download call\n        target_url = 'https://ndownloader.figshare.com/{type}/{id}'.format(type=f_type,\n                                                                           id=download_id)\n        if post_id:\n            target_url += '/{post}'.format(post=post_id)\n        logging.debug(target_url)\n        if not os.path.isfile(completefile):\n            self.url_request(target_url=target_url,\n                             output_file=tar_file)\n        # Decompress the file\n        self.decompress(databasepath=databasepath,\n                        database_name=database_name,\n                        compression=compression,\n                        compressed_file=tar_file)\n        # Create the completefile\n        if complete:\n            with open(completefile, 'w') as complete:\n                complete.write('')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decompress(databasepath, database_name, compression, compressed_file):\n        # Extract the databases from the archives\n        if os.path.isfile(compressed_file):\n            if compression == 'tar':\n                logging.info('Extracting {dbname} from archives'.format(dbname=database_name))\n                with tarfile.open(compressed_file, 'r') as tar:\n                    # Decompress the archive\n                    tar.extractall(path=databasepath)\n            elif compression == 'gz':\n                with gzip.open(compressed_file, 'rb') as gz:\n                    file_name = os.path.basename(os.path.splitext(compressed_file)[0])\n                    output_file = os.path.join(databasepath,\n                                               database_name,\n                                               file_name)\n                    logging.info('Extracting {file_name} from archives'.format(file_name=file_name))\n                    with open(output_file, 'wb') as output:\n                        shutil.copyfileobj(gz, output)\n            else:\n                logging.info('Extracting {dbname} from archives'.format(dbname=database_name))\n                with zipfile.ZipFile(compressed_file, 'r') as zip_file:\n                    zip_file.extractall(path=databasepath)\n            # Delete the archive file\n            os.remove(compressed_file)", "response": "Decompress the provided file using the appropriate library."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an appropriately named folder in which the database is to be stored.", "response": "def create_database_folder(databasepath, database):\n        \"\"\"\n        Create an appropriately named folder in which the database is to be stored\n        :param databasepath: path to use to save the database\n        :param database: the name of the database folder to create\n        :return: the absolute path of the folder\n        \"\"\"\n        logging.info('Setting up {} database'.format(database))\n        # Define the path to store the database files\n        databasepath = os.path.join(databasepath, database)\n        # Create the path as required\n        make_path(databasepath)\n        return databasepath"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the database if necessary. Create the completefile if necessary.", "response": "def database_download(self, output_file, target_url, database_path, complete=False):\n        \"\"\"\n        Check to see if the download has previously been completed. Run the download if necessary. Create the\n        completefile if required\n        :param output_file: Name and path of local copy of downloaded target\n        :param target_url: URL of the target to download\n        :param database_path: Path on the local filesystem in which the file is to be downloaded\n        :param complete: Boolean to determine whether a completefile should be created\n        \"\"\"\n        # Create a file to store the logs; it will be used to determine if the database was downloaded and set-up\n        completefile = os.path.join(database_path, 'complete')\n        if not os.path.isfile(completefile):\n            self.url_request(target_url=target_url,\n                             output_file=output_file)\n            if complete:\n                # Create the completefile\n                with open(completefile, 'w') as complete:\n                    complete.write('')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the system call to retrieve the database and sets up the database completeness assessment file.", "response": "def database_clone(targetcall, databasepath, complete=False):\n        \"\"\"\n        Checks to see if the database has already been downloaded. If not, runs the system call to\n        download the database, and writes stdout and stderr to the logfile\n        :param targetcall: system call to download, and possibly set-up the database\n        :param databasepath: absolute path of the database\n        :param complete: boolean variable to determine whether the complete file should be created\n        \"\"\"\n        # Create a file to store the logs; it will be used to determine if the database was downloaded and set-up\n        completefile = os.path.join(databasepath, 'complete')\n        # Run the system call if the database is not already downloaded\n        if not os.path.isfile(completefile):\n            out, err = run_subprocess(targetcall)\n            if complete:\n                # Create the database completeness assessment file and populate it with the out and err streams\n                with open(completefile, 'w') as complete:\n                    complete.write(out)\n                    complete.write(err)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the severity of the event.", "response": "def severity(self):\n        \"\"\"\n        Severity level of the event. One of ``INFO``, ``WATCH``,\n        ``WARNING``, ``DISTRESS``, ``CRITICAL`` or ``SEVERE``.\n        \"\"\"\n        if self._proto.HasField('severity'):\n            return yamcs_pb2.Event.EventSeverity.Name(self._proto.severity)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the state of this instance.", "response": "def state(self):\n        \"\"\"\n        State of this instance. One of ``OFFLINE``, ``INITIALIZING``,\n        ``INITIALIZED``, ``STARTING``, ``RUNNING``, ``STOPPING`` or\n        ``FAILED``.\n        \"\"\"\n        if self._proto.HasField('state'):\n            return yamcsManagement_pb2.YamcsInstance.InstanceState.Name(self._proto.state)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the state of this service.", "response": "def state(self):\n        \"\"\"State of this service.\"\"\"\n        if self._proto.HasField('state'):\n            return yamcsManagement_pb2.ServiceState.Name(self._proto.state)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a child element to this widget.", "response": "def add_child(self, child):\n        \"\"\"\n        Add a new child element to this widget.\n\n        :param child: Object inheriting :class:`BaseElement`.\n        \"\"\"\n        self.children.append(child)\n        child.parent = self\n\n        if self.view and self.view.is_loaded:\n            self.view.dispatch({\n                'name': 'append',\n                'html': child.compile(),\n                'selector': '#' + str(self.id)\n            })"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a child from this widget.", "response": "def remove_child(self, child):\n        \"\"\"\n        Remove a child widget from this widget.\n\n        :param child: Object inheriting :class:`BaseElement`\n        \"\"\"\n        self.children.remove(child)\n        child.parent = None\n\n        if self.view and self.view.is_loaded:\n            self.view.dispatch({\n                'name': 'remove',\n                'selector': '#' + child.id\n            })"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compile(self):\n        self.content = \"\".join(map(lambda x: x.compile(), self.children))\n        return self._generate_html()", "response": "Recursively compile this widget and all of its children to HTML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling the new configuration.", "response": "def handle_config_change(self, new_config):\n        \"\"\"Handle the new configuration.\n\n        Args:\n            new_config (dict): The new configuration\n\n        \"\"\"\n        if self.user_handler:\n            self.user_handler(self.current_config, new_config)\n        self._call_spec_handlers(new_config)\n        self.current_config = copy.deepcopy(new_config)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the reference point to determine what outgoing voltage will produce what intensity of components", "response": "def setReferenceVoltage(self, caldb, calv):\n        \"\"\"Sets the reference point to determine what outgoing voltage will produce what intensity, \n        used to calculate the proper output amplitude of components\n\n        :param caldb: calibration intensity in dbSPL\n        :type caldb: float\n        :param calv: calibration voltage that was used to record the intensity provided\n        :type calv: float\n        \"\"\"\n        self.caldb = caldb\n        self.calv = calv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setCalibration(self, dbBoostArray, frequencies, frange):\n        if dbBoostArray is not None and frequencies is not None:\n            logger = logging.getLogger('main')\n            if dbBoostArray.shape != frequencies.shape:\n                logger.error(\"ERROR: calibration array and frequency array must have same dimensions\")\n                return\n            if frange is None:\n                # maximum possible range\n                frange = (frequencies[0], frequencies[-1])\n\n            logger.debug('setting calibration with samplerate {}'.format(self.samplerate()))\n            fs = self.samplerate()\n            if fs in StimulusModel.kernelCache:\n                logger.debug('---->using cached filter')\n                # makes the assumption that the cache will be cleared if the frequency reponse\n                # changes\n                self.impulseResponse = StimulusModel.kernelCache[fs]\n            else:\n                logger.debug('---->calculating new filter for fs {}'.format(fs))\n                self.impulseResponse = impulse_response(fs, dbBoostArray, frequencies, frange)\n                # mutable type so will affect data structure persistently\n                StimulusModel.kernelCache[fs] = self.impulseResponse\n\n            # store this so we can quickly check if a calibration needs to be re-done    \n            self._calibration_fs = fs\n            \n            # calculate for the default samplerate, if not already, since\n            # we are very likely to need it, and it's better to have this done\n            # up front, than cause lag in the UI later\n            if DEFAULT_SAMPLERATE not in StimulusModel.kernelCache:\n                StimulusModel.kernelCache[DEFAULT_SAMPLERATE] = impulse_response(DEFAULT_SAMPLERATE, dbBoostArray, frequencies, frange)\n\n            # hang on to these for re-calculating impulse response on samplerate change\n            self._attenuationVector = dbBoostArray\n            self._calFrequencies = frequencies\n            self._calFrange = frange\n\n        else:\n            self.impulseResponse = None", "response": "Sets the calibration to use with this stimulus."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the current calibration according to intenal values.", "response": "def updateCalibration(self):\n        \"\"\"Updates the current calibration according to intenal values. For example, if the stimulus samplerate changes\n        the calibration needs to be recalculated.\"\"\"\n        if self.samplerate() != self._calibration_fs:\n            self.setCalibration(self._attenuationVector, self._calFrequencies, self._calFrange)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef samplerate(self):\n        rates = []\n        for track in self._segments:\n            for component in track:\n                # special case, where component is a wav file:\n                # it will set the master samplerate to match its own\n                if component.__class__.__name__ == 'Vocalization':\n                    if component.samplerate() is not None:\n                        rates.append(component.samplerate())\n\n        if len(set(rates)) > 1:\n            # error check\n            # raise Exception(\"Wav files with different sample rates in same stimulus\")\n            logger = logging.getLogger('main')\n            logger.error(\"Wav files with different sample rates in same stimulus\")\n            return None\n        elif len(set(rates)) == 1:\n            return rates[0]\n        else:\n            return DEFAULT_SAMPLERATE", "response": "Returns the samplerate for this stimulus"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the number of components in a track or the max number of components in any row if none given", "response": "def columnCount(self, row=None):\n        \"\"\"Returns the number of components in a track, \n        or the max number of components in any row, if none given\n\n        :param row: track to get count for\n        :type row: int\n        :returns: int -- number of components for *row*\n        \"\"\"\n        if row is not None:\n            wholerow = self._segments[row]\n            return len(wholerow)\n        else:\n            column_lengths = [len(x) for x in self._segments]\n            return max(column_lengths)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the total number of components in stimulus", "response": "def componentCount(self):\n        \"\"\"Returns the total number of components in stimulus\n\n        :returns: number of components (not including expanded auto-params)\n        \"\"\"\n        return sum([self.columnCountForRow(x) for x in range(self.rowCount())])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef component(self, row, col):\n        try:\n            comp = self._segments[row][col]\n        except:\n            # invalid index\n            print 'Invalid index'\n            return None\n        return comp", "response": "Gets the components for the location\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninsert a component into the stimulus s internal list.", "response": "def insertComponent(self, comp, row=0, col=0):\n        \"\"\"Inserts component into model\n\n        :param comp: Component to insert into the stimulus\n        :type comp: :class:`AbstractStimulusComponent<sparkle.stim.abstract_component.AbstractStimulusComponent>`\n        :param row: Track number to place comp in\n        :type row: int\n        :param col: location in track to insert component to\n        :type col: int\n        \"\"\"\n        if row > len(self._segments) -1:\n            self.insertEmptyRow()\n        self._segments[row].insert(col, comp)\n\n        # in case of samplerate change, just always update\n        self.updateCalibration()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overwriteComponent(self, comp, row, col):\n        self._segments[row][col] = comp\n\n        # in case of samplerate change, just always update\n        self.updateCalibration()", "response": "Overwrites the component at the specified location with a provided one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef removeLastRow(self):\n        lastrow = self._segments.pop(len(self._segments)-1)\n        if len(lastrow) > 0:\n            raise Exception(\"Attempt to remove non-empty stimulus track\")", "response": "Removes the last track from the stimulus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef removeComponent(self, row,col):\n        self._segments[row].pop(col)\n\n        # If this row is now empty we should remove it?\n        if self.columnCountForRow(-1) == 0:\n            self.removeRow(len(self._segments)-1)\n\n        # in case of samplerate change, just always update\n        self.updateCalibration()", "response": "Removes the component at the given location"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef indexByComponent(self, component):\n        for row, rowcontents in enumerate(self._segments):\n            if component in rowcontents:\n                column = rowcontents.index(component)\n                return (row, column)", "response": "Returns a location for the given component in the model."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef traceCount(self):\n        nsegs = sum([len(track) for track in self._segments])\n        if nsegs == 0:\n            return 0\n        ntraces = 1\n        for irow in range(self._autoParams.nrows()):\n            ntraces = ntraces*self._autoParams.numSteps(irow)\n        return ntraces", "response": "Returns the number of unique stimului for this stimulus object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns whether the specified stimlus type is a component in this stimulus.", "response": "def contains(self, stimtype):\n        \"\"\"Returns whether the specified stimlus type is a component in this stimulus\n\n        :param stimtype: :class:`AbstractStimulusComponent<sparkle.stim.abstract_component.AbstractStimulusComponent>` subclass class name to test for membership in the components of this stimulus\n        :type stimtype: str\n        :returns: bool -- if the stimtype is in the model\n        \"\"\"\n        for track in self._segments:\n            for component in track:\n                if component.__class__.__name__ == stimtype:\n                    return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclearing out orphaned auto parameters", "response": "def purgeAutoSelected(self):\n        \"\"\"Clears out orphaned auto parameters\"\"\"\n        params = self._autoParams.allData()\n        for p in params:\n            comps_to_remove = []\n            for comp in p['selection']:\n                if self.indexByComponent(comp) is None:\n                    comps_to_remove.append(comp)\n            for orphaned in comps_to_remove:\n                p['selection'].remove(orphaned)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef expandFunction(self, func, args=[]):\n        # initilize array to hold all varied parameters\n        params = self._autoParams.allData()\n\n        steps = self.autoParamRanges()\n        ntraces = 1\n        for p in steps:\n            ntraces = ntraces*len(p)\n        varylist = [[None for x in range(len(params))] for y in range(ntraces)]\n        x = 1\n        for iset, step_set in enumerate(steps):\n            for itrace in range(ntraces):\n                idx = (itrace / x) % len(step_set)\n                varylist[itrace][iset] = step_set[idx]\n            x = x*len(step_set)\n        # now create the stimuli according to steps\n        # go through list of modifing parameters, update this stimulus,\n        # and then save current state to list\n        stim_list = []\n        for itrace in range(ntraces):\n            for ip, param in enumerate(params):\n                for component in param['selection']:\n                    # print 'setting component {} parameter {} to {}'.format(component.name, param['parameter'], varylist[itrace][ip])\n                    \n                    # so I encountered a bug when the parameters were dragged the\n                    # pickling/unpickling seems to either make a copy or somehow\n                    # otherwise loose connection to the original components\n                    # make sure to be setting the components that are in this model.\n                    index = self.indexByComponent(component)\n                    component = self.component(*index)\n\n                    component.set(param['parameter'], varylist[itrace][ip])\n            # copy of current stim state, or go ahead and turn it into a signal?\n            # so then would I want to formulate some doc here as well?\n            stim_list.append(func(*args))\n\n        # now reset the components to start value\n        for ip, param in enumerate(params):\n            for component in param['selection']:\n                component.set(param['parameter'], varylist[0][ip])\n\n        return stim_list", "response": "Applies the given function to each of the stimuli in the model and returns a list of results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setReorderFunc(self, func, name=None):\n        self.reorder = func\n        self.reorderName = name", "response": "Sets the function that will be called when the stimulus is reordered."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of numpy. ndarrays the signals docs and undesired attenuations and a list of undesired attenuations and a complimentary list of undesired attenuations.", "response": "def expandedStim(self):\n        \"\"\"\n        Apply the autoparameters to this stimulus and return a list of\n        the resulting stimuli, a complimentary list of doc dictionaries, and\n        a complimentary list of undesired attenuations.\n\n        :returns: list<numpy.ndarray>, list<dict>, list<float> -- the signals, their doc, undesired attenuations (dB)\n        \"\"\"\n        logger = logging.getLogger('main')\n        logger.debug(\"Generating Expanded Stimulus\")\n\n        # 3 loops now -- could be done in one...\n        signals = self.expandFunction(self.signal)\n        docs = self.expandFunction(self.componentDoc)\n        overloads = []\n        for s, d in zip(signals, docs):\n            d['overloaded_attenuation'] = s[2]\n            overloads.append(s[2])\n\n        # remove the undesired attenuation argument\n        signals = [sig[0:2] for sig in signals]\n\n        if self.reorder:\n            order = self.reorder(docs)\n            signals = [signals[i] for i in order]\n            docs = [docs[i] for i in order]\n\n        return signals, docs, overloads"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the stimulus model from a template dict", "response": "def loadFromTemplate(template, stim=None):\n        \"\"\"Loads the stimlus to the state provided by a template\n\n        :param template: dict that includes all info nesessary to recreate stim\n        :type template: dict\n        :param stim: Stimulus to apply to, creates a new model if None\n        :type stim: StimulusModel\n        \"\"\"\n        if stim is None:\n            stim = StimulusModel()\n        stim.setRepCount(template['reps'])\n        stim.setUserTag(template.get('user_tag', ''))\n        # don't set calibration details - this should be the same application wide\n        component_classes = get_stimuli_models()\n        for comp_doc in template['components']:\n            comp = get_component(comp_doc['stim_type'], component_classes)\n            comp.loadState(comp_doc) # ignore extra dict entries\n            stim.insertComponent(comp, *comp_doc['index'])\n\n        # revert from location based selection to component list\n        autoparams = template['autoparameters']\n        for p in autoparams:\n            selection = p['selection']\n            component_selection = []\n            for index in selection:\n                component = stim.component(*index)\n                component_selection.append(component)\n            p['selection'] = component_selection\n\n        stim.autoParams().setParameterList(autoparams)\n        stim.setReorderFunc(order_function(template['reorder']), template['reorder'])\n        stim.setStimType(template['testtype'])\n        return stim"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the duration of this stimulus", "response": "def duration(self):\n        \"\"\"The duration of this stimulus\n\n        :returns: float -- duration in seconds\n        \"\"\"\n        durs = []\n        for track in self._segments:\n            durs.append(sum([comp.duration() for comp in track]))\n            \n        return max(durs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef componentDoc(self, starttime=True):\n        samplerate = self.samplerate()\n        doc_list = []\n        for row, track in enumerate(self._segments):\n            start_time = 0\n            for col, component in enumerate(track):\n                info = component.stateDict()\n                info['stim_type'] = component.name\n                if starttime:\n                    info['start_s'] = start_time\n                info['index'] = (row, col)\n                start_time += info['duration']\n                doc_list.append(info)\n                \n        return {'samplerate_da':samplerate, 'components' : doc_list}", "response": "The documentation for the components as a dict\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef containsPval(self, paramName, value):\n        # this may break if there are two parameters in the model with \n        # the same parameter type!!!\n        params = self._autoParams.allData()\n        steps = self.autoParamRanges()\n        pnames = [p['parameter'] for p in params]\n        if paramName in pnames:\n            pidx = pnames.index(paramName)\n            return value in steps[pidx]\n        else:\n            return False", "response": "Returns true if value for the given parameter type paramName is in the \n        auto - parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef warning(self):\n        signals, docs, overs = self.expandedStim()\n        if np.any(np.array(overs) > 0):\n            msg = 'Stimuli in this test are over the maximum allowable \\\n                voltage output. They will be rescaled with a maximum \\\n                undesired attenuation of {:.2f}dB.'.format(np.amax(overs))\n            return msg\n        return 0", "response": "Checks Stimulus for any warning conditions and returns a string if any otherwise 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verifyExpanded(self, samplerate):\n        results = self.expandFunction(self.verifyComponents, args=(samplerate,))\n        msg = [x for x in results if x]\n        if len(msg) > 0:\n            return msg[0]\n        else:\n            return 0", "response": "Checks the expanded parameters for invalidating conditions returning an error message if any"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the current components for invalidating conditions returning an error message if any", "response": "def verifyComponents(self, samplerate):\n        \"\"\"Checks the current components for invalidating conditions\n\n        :param samplerate: generation samplerate (Hz), passed on to component verification\n        :type samplerate: int\n        :returns: str -- error message, if any, 0 otherwise\n        \"\"\"\n        # flatten list of components\n        components = [comp for track in self._segments for comp in track]\n        for comp in components:\n            msg = comp.verify(samplerate=samplerate)\n            if msg:\n                return msg\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify(self, windowSize=None):\n        if self.samplerate() is None:\n            return \"Multiple recording files with conflicting samplerates\"\n        msg = self._autoParams.verify()\n        if msg:\n            return msg\n        if self.traceCount() == 0:\n            return \"Test is empty\"\n        if windowSize is not None:\n            durations = self.expandFunction(self.duration)\n            # print 'windowSize', windowSize, 'self', durations[0],  durations[-1]\n            # ranges are linear, so we only need to test first and last\n            if durations[0] > windowSize or durations[-1] > windowSize:\n                return \"Stimulus duration exceeds window duration\"\n        msg = self.verifyExpanded(self.samplerate())\n        if msg:\n            return msg\n        if self.caldb is None or self.calv is None:\n            return \"Test reference voltage not set\"\n        if None in self.voltage_limits:\n            return \"Device voltage limits not set\"\n        return 0", "response": "Checks the stimulus for invalidating conditions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef implementation(self,\n                       commands_module: arg(short_option='-m') = DEFAULT_COMMANDS_MODULE,\n                       config_file: arg(short_option='-f') = None,\n                       # Globals\n                       globals_: arg(\n                           container=dict,\n                           type=json_value,\n                           help='Global variables & default args for *all* commands; will be '\n                                'injected into itself, default args, and environment variables '\n                                '(higher precedence than keyword args)'\n                       ) = None,\n                       # Special globals (for command line convenience)\n                       env: arg(help='env will be added to globals if specified') = None,\n                       version: arg(help='version will be added to globals if specified') = None,\n                       echo: arg(\n                           type=bool,\n                           help='echo=True will be added to globals',\n                           inverse_help='echo=False will be added to globals'\n                       ) = None,\n                       # Environment variables\n                       environ: arg(\n                           container=dict,\n                           help='Additional environment variables; '\n                                'added just before commands are run'\n                       ) = None,\n                       # Meta\n                       info: arg(help='Show info and exit') = False,\n                       list_commands: arg(help='Show info & commands and exit') = False,\n                       debug: arg(\n                           type=bool,\n                           help='Print debugging info & re-raise exceptions; also added to globals'\n                       ) = None,\n                       *,\n                       all_argv=(),\n                       run_argv=(),\n                       command_argv=(),\n                       cli_args=()):\n        \"\"\"Run one or more commands in succession.\n\n        For example, assume the commands ``local`` and ``remote`` have been\n        defined; the following will run ``ls`` first on the local host and\n        then on the remote host::\n\n            runcommands local ls remote <host> ls\n\n        When a command name is encountered in ``argv``, it will be considered\n        the starting point of the next command *unless* the previous item in\n        ``argv`` was an option like ``--xyz`` that expects a value (i.e.,\n        it's not a flag).\n\n        To avoid ambiguity when an option value matches a command name, the\n        value can be prepended with a colon to force it to be considered\n        a value and not a command name.\n\n        \"\"\"\n        collection = Collection.load_from_module(commands_module)\n        config_file = self.find_config_file(config_file)\n        cli_globals = globals_ or {}\n\n        if env:\n            cli_globals['env'] = env\n        if version:\n            cli_globals['version'] = version\n        if echo is not None:\n            cli_globals['echo'] = echo\n        if debug is not None:\n            cli_globals['debug'] = debug\n\n        if config_file:\n            args_from_file = self.read_config_file(config_file, collection)\n            args = merge_dicts(args_from_file, {'environ': environ or {}})\n\n            config_file_globals = args['globals']\n\n            env = cli_globals.get('env') or config_file_globals.get('env')\n            if env:\n                envs = args['envs']\n                try:\n                    env_globals = envs[env]\n                except KeyError:\n                    raise RunnerError('Unknown env: {env}'.format_map(locals()))\n                globals_ = merge_dicts(config_file_globals, env_globals, cli_globals)\n                globals_['envs'] = envs\n            else:\n                globals_ = merge_dicts(config_file_globals, cli_globals)\n\n            default_args = {name: {} for name in collection}\n            default_args = merge_dicts(default_args, args.get('args') or {})\n\n            for command_name, command_default_args in default_args.items():\n                command = collection[command_name]\n\n                # Normalize arg names from default args section.\n                for name in tuple(command_default_args):\n                    param = command.find_parameter(name)\n                    if param is None:\n                        raise RunnerError(\n                            'Unknown arg for command {command_name} in default args section of '\n                            '{config_file}: {name}'\n                            .format_map(locals()))\n                    if param is not None and name != param.name:\n                        command_default_args[param.name] = command_default_args.pop(name)\n\n                # Add globals that correspond to this command (that\n                # aren't present in default args section).\n                for name, value in globals_.items():\n                    param = command.find_parameter(name)\n                    if param is not None:\n                        if param.name not in command_default_args:\n                            command_default_args[param.name] = value\n                    elif command.has_kwargs:\n                        name = name.replace('-', '_')\n                        command_default_args[name] = value\n\n                # Convert lists to tuples for the command's args that are\n                # specified as being tuples.\n                for name, value in command_default_args.items():\n                    command_arg = command.find_arg(name)\n                    if command_arg.container and isinstance(value, list):\n                        command_default_args[name] = command_arg.container(value)\n\n            default_args = {name: args for name, args in default_args.items() if args}\n\n            environ = args['environ']\n        else:\n            globals_ = cli_globals\n            default_args = {}\n            environ = environ or {}\n\n        debug = globals_.get('debug', False)\n        show_info = info or list_commands or not command_argv or debug\n        print_and_exit = info or list_commands\n\n        globals_, default_args, environ = self.interpolate(globals_, default_args, environ)\n\n        if show_info:\n            print('RunCommands', __version__)\n\n        if debug:\n            print()\n            printer.debug('Commands module:', commands_module)\n            printer.debug('Config file:', config_file)\n            printer.debug('All args:', all_argv)\n            printer.debug('Run args:', run_argv)\n            printer.debug('Command args:', command_argv)\n            items = (\n                ('Globals:', globals_),\n                ('Default args:', default_args),\n                ('Environment variables:', environ),\n            )\n            for label, data in items:\n                if data:\n                    printer.debug(label)\n                    for k in sorted(data):\n                        v = data[k]\n                        printer.debug('  - {k} = {v!r}'.format_map(locals()))\n\n        if environ:\n            os.environ.update(environ)\n\n        collection.set_attrs(debug=debug)\n        collection.set_default_args(default_args)\n        runner = CommandRunner(collection, debug)\n\n        if print_and_exit:\n            if list_commands:\n                runner.print_usage()\n        elif not command_argv:\n            printer.warning('\\nNo command(s) specified')\n            runner.print_usage()\n        else:\n            runner.run(command_argv)", "response": "This function is used to create a new instance of the command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_plot(cls, data, columns, plottype, fmt='json', **kwargs):\n        cls._verify_export_dir()\n        plot = {\"use_visavis_type\": plottype, \"payload\": {}}\n\n        if isinstance(data, pd.DataFrame):\n            iter_data = data.iterrows\n            pointers = columns\n        else:\n            iter_data = lambda: enumerate(data)\n            pointers = range(len(data[0]))\n\n        if fmt == 'csv':\n            fmt_export = os.path.join(cls.export_dir, cls._gen_basename() + \".csv\")\n            f_export = open(fmt_export, \"w\")\n            f_export.write(\"%s\\n\" % \",\".join(map(str, columns)))\n            for _, row in iter_data():\n                f_export.write(\"%s\\n\" % \",\".join([str(row[i]) for i in pointers]))\n            f_export.close()\n\n        else:\n            fmt_export = os.path.join(cls.export_dir, cls._gen_basename() + \".json\")\n            f_export = open(fmt_export, \"w\")\n\n            if plottype == 'bar':\n\n                plot[\"payload\"] = {\"x\": [], \"y\": [], \"xtitle\": cls._get_title(columns[0]), \"ytitle\": cls._get_title(columns[1])}\n\n                for _, row in iter_data():\n                    plot[\"payload\"][\"x\"].append(row[pointers[0]])\n                    plot[\"payload\"][\"y\"].append(row[pointers[1]])\n\n            elif plottype == 'plot3d':\n\n                plot[\"payload\"][\"points\"] = {\"x\": [], \"y\": [], \"z\": [], \"labels\": []}\n                plot[\"payload\"][\"meshes\"] = []\n                plot[\"payload\"][\"xtitle\"] = cls._get_title(columns[0])\n                plot[\"payload\"][\"ytitle\"] = cls._get_title(columns[1])\n                plot[\"payload\"][\"ztitle\"] = cls._get_title(columns[2])\n                recent_mesh = 0\n\n                for _, row in iter_data():\n                    plot[\"payload\"][\"points\"][\"x\"].append(row[pointers[0]])\n                    plot[\"payload\"][\"points\"][\"y\"].append(row[pointers[1]])\n                    plot[\"payload\"][\"points\"][\"z\"].append(row[pointers[2]])\n                    plot[\"payload\"][\"points\"][\"labels\"].append(row[pointers[3]])\n\n                    if row[4] != recent_mesh:\n                        plot[\"payload\"][\"meshes\"].append({\"x\": [], \"y\": [], \"z\": []})\n                    recent_mesh = row[4]\n\n                    if plot[\"payload\"][\"meshes\"]:\n                        plot[\"payload\"][\"meshes\"][-1][\"x\"].append(row[pointers[0]])\n                        plot[\"payload\"][\"meshes\"][-1][\"y\"].append(row[pointers[1]])\n                        plot[\"payload\"][\"meshes\"][-1][\"z\"].append(row[pointers[2]])\n\n            if kwargs:\n                plot[\"payload\"].update(kwargs)\n\n            else: raise RuntimeError(\"\\r\\nError: %s is an unknown plot type\" % plottype)\n\n            f_export.write(json.dumps(plot, escape_forward_slashes=False, indent=4))\n            f_export.close()\n\n        return fmt_export", "response": "Saves the data in the specified format for plotting."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef saveToObject(self):\n        details = self._component.auto_details()\n        for field, widget in self.inputWidgets.items():\n            self._component.set(field, widget.value())\n        self.attributesSaved.emit(self._component.__class__.__name__, self._component.stateDict())", "response": "Re - implemented from : meth : AbstractComponentWidget. saveToObject"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that the sequencing run is completed and then copy the directory to self. path", "response": "def fastqlinker(self):\n        \"\"\"Ensure that the sequencing run is completed, and then copy the directory to :self.path\"\"\"\n        # Module-specific imports\n        import time\n        import re\n        import shutil\n        import errno\n        # Glob for .gz files in the appropriate subfolder of :miseqfolder. Discard 'Undetermined' files\n        gzfiles = [gzfile for gzfile in glob('{}Data/Intensities/BaseCalls/*.gz'.format(self.miseqfolder))\n                   if \"Undetermined\" not in gzfile]\n        # While loop to wait until run is complete - two .gz files are created for each sample\n        while len(gzfiles) < 2 * self.samplecount:\n            printtime('Waiting for run to finish. Currently, {} out of a total of {} fastq.gz files '\n                      'have been created'.format(len(gzfiles), 2 * self.samplecount), self.start)\n            # Sleep for five minutes\n            time.sleep(300)\n            # Check the number of .gz files again\n            gzfiles = [gzfile for gzfile in glob('{}Data/Intensities/BaseCalls/*.gz'.format(self.miseqfolder))\n                       if \"Undetermined\" not in gzfile]\n        # Iterate through each .gz file\n        for gzfile in sorted(gzfiles):\n            # Extract the strain name from the .gz file\n            filename = re.split(\"_S\\d+_L001\", os.path.basename(gzfile))[0]\n            # Make the outputdir variable\n            outputdir = '{}{}'.format(self.path, filename)\n            make_path(outputdir)\n            # Don't link the files if they have already been linked\n            if len(glob('{}/*fastq*'.format(outputdir))) < self.numreads:\n                try:\n                    # Link the .gz files to :self.path/:filename\n                    os.symlink(gzfile, '{}/{}'.format(outputdir, os.path.basename(gzfile)))\n                # Except os errors\n                except OSError as exception:\n                    # If there is an exception other than the file exists, raise it\n                    if exception.errno != errno.EEXIST:\n                        raise\n        # Add the location/name of the fastq files to the metadata object\n        for sample in self.metadata.runmetadata.samples:\n            # Find any fastq files with the sample name\n            fastqfiles = sorted(glob('{}{}/{}*.fastq*'.format(self.path, sample.name, sample.name)))\n            fastqfiles = [fastq for fastq in fastqfiles if 'trimmed' not in fastq]\n            # Update the metadata with the path/name of the fastq files\n            sample.general.fastqfiles = fastqfiles\n        # Copy the GenerateFASTQRunStatistics.xml, RunInfo.xml, and SampleSheet.csv to self.path\n        map(lambda x: shutil.copyfile('{}/{}'.format(self.miseqfolder, x), '{}{}'.format(self.path, x))\n            # Don't copy if the file is already present\n            if not os.path.isfile('{}{}'.format(self.path, x)) else x,\n            # List of the files of interest\n            ['GenerateFASTQRunStatistics.xml', 'RunInfo.xml', 'SampleSheet.csv'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndiscover and return a list of the names of all analog output channels for the given device.", "response": "def get_ao_chans(dev):\n    \"\"\"Discover and return a list of the names of all analog output channels for the given device\n\n    :param dev: the device name\n    :type dev: str\n    \"\"\"\n    buf = create_string_buffer(256)\n    buflen = c_uint32(sizeof(buf))\n    DAQmxGetDevAOPhysicalChans(dev.encode(), buf, buflen)\n    pybuf = buf.value\n    chans = pybuf.decode(u'utf-8').split(u\",\")\n    return chans"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndiscover and return a list of the names of all analog input channels for the given device.", "response": "def get_ai_chans(dev):\n    \"\"\"Discover and return a list of the names of all analog input channels for the given device\n    \n    :param dev: the device name\n    :type dev: str\n    \"\"\"\n    buf = create_string_buffer(512)\n    buflen = c_uint32(sizeof(buf))\n    DAQmxGetDevAIPhysicalChans(dev.encode(), buf, buflen)\n    pybuf = buf.value\n    chans = pybuf.decode(u'utf-8').split(u\", \")\n    return chans"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_devices():\n    buf = create_string_buffer(512)\n    buflen = c_uint32(sizeof(buf))\n    DAQmxGetSysDevNames(buf, buflen)\n    pybuf = buf.value\n    devices = pybuf.decode(u'utf-8').split(u\",\")\n    return devices", "response": "Discover and return a list of the names of all NI devices on this system"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a function to be executed periodically on the specified number .", "response": "def register_callback(self, fun, npts):\n        \"\"\" Provide a function to be executed periodically on \n        data collection, every time after the specified number \n        of points are collected.\n        \n        :param fun: the function that gets called, it must have a single positional argument that will be the data buffer read\n        :type fun: function\n        :param npts: The number of data points collected before the function is called.\n        :type npts: int\n        \"\"\"\n        self.callback_fun = fun\n        self.n = npts\n        self.AutoRegisterEveryNSamplesEvent(DAQmx_Val_Acquired_Into_Buffer,\n                                            npts, 0, name=u\"_run_callback\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the data to the device buffer", "response": "def write(self, output):\n        \"\"\"Writes the data to be output to the device buffer, output will be looped when the data runs out\n\n        :param output: data to output\n        :type output: numpy.ndarray\n        \"\"\"\n        w = c_int32()\n        # print \"output max\", max(abs(output))\n        self.WriteAnalogF64(self.bufsize, 0, 10.0, DAQmx_Val_GroupByChannel,\n                            output, w, None);"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self):\n        r = c_int32()\n        bufsize = self.npts*self.nchans\n        inbuffer = np.zeros(bufsize)\n        self.ReadAnalogF64(self.npts, 10.0, DAQmx_Val_GroupByChannel, inbuffer,\n                           bufsize, byref(r), None)\n        self.WaitUntilTaskDone(10.0)\n\n\n        return inbuffer.reshape(self.nchans, self.npts)", "response": "Reads the data off of the device input buffer. Blocks for a timeout of 10 seconds to be done with a timeout of 10 seconds\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self,output):\n        w = c_int32()\n        self.WriteAnalogF64(self.npoints, 0, 10.0, DAQmx_Val_GroupByChannel,\n                            output, w, None);", "response": "Writes the data to the device buffer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the spectrogram given by kwarg *'plot' *.", "response": "def updateSpec(self, *args, **kwargs):\n        \"\"\"Updates the spectrogram given by kwarg *'plot'*, which is\n        either 'response' or (well actually anything). If no arguments \n        are given, clears both spectrograms.\n\n        For other arguments, see: :meth:`SpecWidget.updateData<sparkle.gui.plotting.pyqtgraph_widgets.SpecWidget.updateData>`\n        \"\"\"\n        if args[0] is None:\n            self.stimSpecPlot.clearImg()\n            self.responseSpecPlot.clearImg()\n        else:\n            p = kwargs.pop('plot')\n            if p == 'response':\n                self.responseSpecPlot.updateData(*args, **kwargs)\n            else:\n                self.stimSpecPlot.updateData(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the signal plots kwarg *'plot'* which is either response or stim.", "response": "def updateSignal(self, *args, **kwargs):\n        \"\"\"Updates the signal plots kwarg *'plot'*, which is\n        either 'response' or (well actually anything).\n\n        For other arguments, see: :meth:`FFTWidget.updateData<sparkle.gui.plotting.pyqtgraph_widgets.FFTWidget.updateData>`\n        \"\"\"\n        p = kwargs.pop('plot')\n        if p == 'response':\n            self.responseSignalPlot.updateData(*args, **kwargs)\n        else:\n            self.stimSignalPlot.updateData(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the FFT plots kwarg *'plot'* which is either response or stim.", "response": "def updateFft(self, *args, **kwargs):\n        \"\"\"Updates the FFT plots kwarg *'plot'*, which is\n        either 'response' or (well actually anything).\n\n        For other arguments, see: :meth:`FFTWidget.updateData<sparkle.gui.plotting.pyqtgraph_widgets.FFTWidget.updateData>`\n        \"\"\"\n        p = kwargs.pop('plot')\n        if p == 'response': \n            self.responseFftPlot.updateData(*args, **kwargs)\n        else:\n            self.stimFftPlot.updateData(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the x axis limits of the signal plots of the data available for this log entry.", "response": "def setXlimits(self, lims):\n        \"\"\"Sets the X axis limits of the signal plots\n\n        :param lims: (min, max) of x axis, in same units as data\n        :type lims: (float, float)\n        \"\"\"\n        self.responseSignalPlot.setXlim(lims)\n        self.stimSignalPlot.setXlim(lims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs a Pat object from the specified string and optimal position count.", "response": "def from_chars(cls, chars='', optimal=3):\n        \"\"\"Construct a Pat object from the specified string\n        and optimal position count.\"\"\"\n        if not chars:\n            chars = ''.join(ALNUM)\n        sets = most_even_chunk(chars, optimal)\n        return cls(sets)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, count):\n        space, self.space = tee(self.space)\n        limit = reduce(mul, map(len, self.sets)) * self.position\n        logging.debug('limit: %s', limit)\n        if limit >= count:\n            return ''.join(islice(space, count))\n        else:\n            raise IndexError('{count} Overflows {sets}!'.format(\n                count=count, sets=self.sets))", "response": "Create a pattern of the specified length."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preserve_namespace(newns=None):\n    ns = cmds.namespaceInfo(an=True)\n    try:\n        cmds.namespace(set=newns)\n        yield\n    finally:\n        cmds.namespace(set=ns)", "response": "Contextmanager that will restore the current namespace"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_top_namespace(node):\n    name = node.rsplit(\"|\", 1)[-1]  # get the node name, in case we get a dagpath\n    name = name.lstrip(\":\")  # strip the root namespace\n    if \":\" not in name:  # if there is no namespace return root\n        return \":\"\n    else:\n        # get the top namespace\n        return name.partition(\":\")[0]", "response": "Return the top level namespace of the given node."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisconnects all connections from a node", "response": "def disconnect_node(node, src=True, dst=True):\n    \"\"\"Disconnect all connections from node\n\n    :param node: the node to disconnect\n    :type node: str\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    if dst:\n        destconns = cmds.listConnections(node, connections=True, plugs=True, source=False) or []\n        for i in range(0, len(destconns), 2):\n            source, dest = destconns[i], destconns[i+1]\n            cmds.disconnectAttr(source, dest)\n    if src:\n        srcconns = cmds.listConnections(node, connections=True, plugs=True, destination=False) or []\n        for i in range(0, len(srcconns), 2):\n            source, dest = srcconns[i+1], srcconns[i]\n            cmds.disconnectAttr(source, dest)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fuzzy_match(self, other):\n        magic, fuzzy = False, False\n        try:\n            magic = self.alias == other.magic\n        except AttributeError:\n            pass\n\n        if '.' in self.alias:\n            major = self.alias.split('.')[0]\n            fuzzy = major == other.alias\n        return magic or fuzzy", "response": "Given another token see if either the major alias identifier\n        matches the other alias or if either the major alias identifier\n            matches the other alias."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eval(self):\n        if self.and_or == 'or':\n            return [Input(self.alias, file, self.cwd, 'and')\n                for file in self.files]\n        return ' '.join(self.files)", "response": "Evaluates the given input and returns a string containing the actual filenames represented by the input token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all the files that match the given input token.", "response": "def files(self):\n        \"\"\" Returns a list of all the files that match the given\n        input token.\n        \"\"\"\n        res = None\n        if not res:\n            res = glob.glob(self.path)\n        if not res and self.is_glob:\n            res = glob.glob(self.magic_path)\n        if not res:\n            res = glob.glob(self.alias)\n        if not res:\n            raise ValueError('No files match. %s' % self)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a given string and turn it into an input token.", "response": "def from_string(string, _or=''):\n        \"\"\" Parse a given string and turn it into an input token. \"\"\"\n        if _or:\n            and_or = 'or'\n        else:\n            and_or = ''\n        return Input(string, and_or=and_or)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a filename to be used for script output.", "response": "def eval(self):\n        \"\"\" Returns a filename to be used for script output. \"\"\"\n        if self.magic:\n            return self.magic\n        if not self.filename:\n            return file_pattern.format(self.alias, self.ext)\n        return self.path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a magic string remove the output tag designator.", "response": "def _clean(self, magic):\n        \"\"\" Given a magic string, remove the output tag designator. \"\"\"\n        if magic.lower() == 'o':\n            self.magic = ''\n        elif magic[:2].lower() == 'o:':\n            self.magic = magic[2:]\n        elif magic[:2].lower() == 'o.':\n            self.ext = magic[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the object to the database.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        **uid**: :code:`{person.uid}_candidate:{party.uid}-{cycle.ap_code}`\n        \"\"\"\n        self.uid = \"{}_candidate:{}-{}\".format(\n            self.person.uid, self.party.uid, self.race.cycle.uid\n        )\n        super(Candidate, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_candidate_election(self, election):\n        return CandidateElection.objects.get(candidate=self, election=election)", "response": "Get a CandidateElection object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all votes for this candidate in an election.", "response": "def get_election_votes(self, election):\n        \"\"\"Get all votes for this candidate in an election.\"\"\"\n        candidate_election = CandidateElection.objects.get(\n            candidate=self, election=election\n        )\n\n        return candidate_election.votes.all()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_election_electoral_votes(self, election):\n        candidate_election = CandidateElection.objects.get(\n            candidate=self, election=election\n        )\n\n        return candidate_election.electoral_votes.all()", "response": "Get all electoral votes for this candidate in an election."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_election_delegates(self, election):\n        candidate_election = CandidateElection.objects.get(\n            candidate=self, election=election\n        )\n\n        return candidate_election.delegates.all()", "response": "Get all pledged delegates for this candidate in an election."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_config(self, config_file=None):\n        if config_file is None:\n            config_file = [\n                '/etc/ellis.conf',\n                '/etc/ellis/ellis.conf',\n                os.path.join(os.path.dirname(__file__), 'ellis.conf'),\n            ]\n\n        self.config.read(config_file, encoding='utf-8')\n\n        return self", "response": "Loads the configuration file and returns the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the Rules from the config file.", "response": "def load_rules(self):\n        \"\"\"\n        Loads the Rules from the config file.\n\n        An invalid Rule (no Filter or no Action) will trigger a warning\n        message and will be ignored.\n        \"\"\"\n        for rule_name in self.config.sections():\n\n            limit = 1\n\n            try:\n                limit = self.config.getint(rule_name, 'limit')\n            except ValueError:\n                warnings.warn(\"Rule '{0}': invalid value for 'limit' option. \"\n                              \"Limit must be an integer > 0. \"\n                              \"Going on with the default value of 1.\"\n                              .format(rule_name))\n            except configparser.NoOptionError:\n                warnings.warn(\"Rule '{0}': no value specified for 'limit' \"\n                              \"option. Going on with the default value of 1.\"\n                              .format(rule_name))\n\n            try:\n                filter_str = self.config.get(rule_name, 'filter')\n                action_str = self.config.get(rule_name, 'action')\n            except configparser.NoOptionError as e:\n                warnings.warn(\"Ignoring '{0}' rule: {1}.\"\n                              .format(rule_name, e))\n            else:\n                try:\n                    rule = Rule(rule_name, filter_str, limit, action_str)\n                except ValueError as e:\n                    warnings.warn(\"Ignoring '{0}' rule: {1}.\"\n                                  .format(rule_name, e))\n                else:\n                    self.rules.append(rule)\n\n        if not self.rules:\n            raise NoRuleError()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the set of systemd units that Ellis will watch.", "response": "def load_units(self):\n        \"\"\"\n        Build a set of systemd units that Ellis will watch.\n\n        This set will be used to filter journald entries so that we only\n        process entries that were produced by these units.\n        This should result in better performance.\n        \"\"\"\n        # Of course, we only consider valid Rules.\n        for rule in self.rules:\n            try:\n                systemd_unit = self.config.get(rule.name, 'systemd_unit')\n\n            except configparser.NoOptionError:\n                warnings.warn(\"Rule '{0}' doesn't have a `systemd_unit` \"\n                              \"option set.\\nThe filters will be checked \"\n                              \"against all journald entries, which will \"\n                              \"probably result in poor performance.\"\n                              .format(rule.name))\n\n                # At this point,  we can clear `self.units` because in any\n                # case, we will need to process every journald entries\n                # for THIS Rule.\n                self.units.clear()\n\n                # And we can also stop looping through rules.\n                break\n\n            else:\n                # Append \".service\" if not present.\n                # Note that we don't check if the service actually exists.\n                # FIXME ?\n                if not systemd_unit.endswith(\".service\"):\n                    systemd_unit += \".service\"\n\n                self.units.add(systemd_unit)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing list :param lst: Array of floats :return: Normalized (in [0, 1]) input array", "response": "def normalize_array(lst):\n    \"\"\"Normalizes list\n\n    :param lst: Array of floats\n    :return: Normalized (in [0, 1]) input array\n    \"\"\"\n    np_arr = np.array(lst)\n    x_normalized = np_arr / np_arr.max(axis=0)\n    return list(x_normalized)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds common values between inner lists", "response": "def find_commons(lists):\n    \"\"\"Finds common values\n\n    :param lists: List of lists\n    :return: List of values that are in common between inner lists\n    \"\"\"\n    others = lists[1:]\n    return [\n        val\n        for val in lists[0]\n        if is_in_all(val, others)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving file handles of the same dataset.", "response": "def retrieve_file_handles_of_same_dataset(self, **args):\n        '''\n        :return: List of handles, or empty list. Should never return None.\n        :raise: SolrSwitchedOff\n        :raise SolrError: If both strategies to find file handles failed.\n        '''\n\n        mandatory_args = ['drs_id', 'version_number', 'data_node']\n        esgfpid.utils.check_presence_of_mandatory_args(args, mandatory_args)\n        LOGGER.debug('Looking for files of dataset \"%s\", version \"%s\".',\n                     args['drs_id'], str(args['version_number']))\n\n        if self.__switched_on:\n            return self.__retrieve_file_handles_of_same_dataset(**args)\n        else:\n            msg = 'Cannot retrieve handles of files of the same dataset.'\n            raise esgfpid.exceptions.SolrSwitchedOff(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_unpaired_reads(fastq_directory, forward_id='_R1', reverse_id='_R2'):\n    unpaired_list = list()\n    fastq_files = glob.glob(os.path.join(fastq_directory, '*.f*q*'))\n    for name in fastq_files:\n        if forward_id not in name and reverse_id not in name:\n            unpaired_list.append(name)\n        elif forward_id in name and not os.path.isfile(name.replace(forward_id, reverse_id)):\n            unpaired_list.append(name)\n        elif reverse_id in name and not os.path.isfile(name.replace(reverse_id, forward_id)):\n            unpaired_list.append(name)\n    return unpaired_list", "response": "Finds unpaired reads in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading a file from the specified address to the specified output_name.", "response": "def download_file(address, output_name, hour_start=18, hour_end=6, day_start=5, day_end=6, timeout=600):\n    \"\"\"\n    Downloads a file, between specified hours. (Hour start has to be greater than hour end for this to work in current\n    iteration).\n    :param address: Address of file that you want to download.\n    :param output_name: Where you want to save the file to.\n    :param hour_start: Start of window where downloading is acceptable. Default 6PM (1800h)\n    :param hour_end: End of window where downloading is acceptable. Default 6AM (600h)\n    :param day_start: Start of window where it's always OK to download. Default Saturday (day 5).\n    :param day_end: End of window where it's always OK to download. Default Sunday (day 6).\n    :param timeout: How often to check if you're outside the acceptable download window (default 600 seconds).\n    :return:\n    \"\"\"\n    out = open(os.devnull, 'w')\n    returncode = 28  # While loop is based on returncode given by curl, so need to initialize it to something.\n    while returncode != 0:  # 0 means that the file has already been downloaded completely, so stop looping then.\n        # Figure out what hour it is. If not in acceptable download window, wait a while before checking again.\n        hour = datetime.datetime.now().time().hour\n        minute = datetime.datetime.now().time().minute\n        day = datetime.datetime.today().weekday()\n        acceptable_hour = not(hour_end < hour < hour_start)  # True if current hour is between start and end.\n        acceptable_day = day_start <= day <= day_end  # True if current day is a weekend day.\n        if not(acceptable_hour or acceptable_day):\n            print('Current time is {hour}:{minute}. I am not allowed to start downloading until'\n                  ' {start_hour}:00.'.format(hour=hour, minute=minute, start_hour=hour_start))\n            time.sleep(timeout)\n        # If the file doesn't already exist, start downloading it.\n        elif not os.path.exists(output_name):\n            cmd = 'curl -o {outname} --max-time {timeout} {address}'.format(timeout=timeout,\n                                                                            address=address,\n                                                                            outname=output_name)\n            returncode = subprocess.call(cmd, shell=True, stdout=out, stderr=out)\n        # If the file does already exist, download it starting from filesize offset.\n        else:\n            file_size = os.path.getsize(output_name)\n            cmd = 'curl -o {outname} --max-time {timeout} -C {file_size} {address}'.format(timeout=timeout,\n                                                                                           address=address,\n                                                                                           outname=output_name,\n                                                                                           file_size=file_size)\n            returncode = subprocess.call(cmd, shell=True, stdout=out, stderr=out)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting out and err to logfile.", "response": "def write_to_logfile(out, err, logfile, samplelog=None, sampleerr=None, analysislog=None, analysiserr=None):\n    \"\"\"\n    Writes out and err (both should be strings) to logfile.\n    \"\"\"\n    # Run log\n    with open(logfile + '_out.txt', 'a+') as outfile:\n        outfile.write(out + '\\n')\n    with open(logfile + '_err.txt', 'a+') as outfile:\n        outfile.write(err + '\\n')\n    # Sample log\n    if samplelog:\n        with open(samplelog, 'a+') as outfile:\n            outfile.write(out + '\\n')\n        with open(sampleerr, 'a+') as outfile:\n            outfile.write(err + '\\n')\n    # Analysis log\n    if analysislog:\n        with open(analysislog, 'a+') as outfile:\n            outfile.write(out + '\\n')\n        with open(analysiserr, 'a+') as outfile:\n            outfile.write(err + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear out the logfiles of the specified logfile", "response": "def clear_logfile(logfile):\n    \"\"\"\n    As logfiles are appended to each time the same data are processed, sometimes it is desirable to clear out\n    logsfiles from previous iterations\n    :param logfile: Base name of logfile\n    \"\"\"\n    try:\n        os.remove(logfile + '_out.txt')\n    except IOError:\n        pass\n    try:\n        os.remove(logfile + '_err.txt')\n    except IOError:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the version of the sequence of available modules.", "response": "def get_version(exe):\n    \"\"\"\n    :param exe: :type list required\n    \"\"\"\n    assert isinstance(exe, list)\n    return Popen(exe, stdout=PIPE, stderr=STDOUT).stdout.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_path(inpath):\n    if not os.path.isfile(inpath):\n        try:\n            os.makedirs(inpath)\n        except FileExistsError:\n            pass\n    else:\n        raise OSError", "response": "Make the path if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting a string with the elapsed time.", "response": "def printtime(string, start, option=None, output=None):\n    \"\"\"Prints a string with colour options with the elapsed time\n    # Reset\n    Color_Off='\\033[0m'       # Text Reset\n\n    # Regular Colors\n    Black='\\033[0;30m'        # Black\n    Red='\\033[0;31m'          # Red\n    Green='\\033[0;32m'        # Green\n    Yellow='\\033[0;33m'       # Yellow\n    Blue='\\033[0;34m'         # Blue\n    Purple='\\033[0;35m'       # Purple\n    Cyan='\\033[0;36m'         # Cyan\n    White='\\033[0;37m'        # White\n\n    # Bold\n    BBlack='\\033[1;30m'       # Black\n    BRed='\\033[1;31m'         # Red\n    BGreen='\\033[1;32m'       # Green\n    BYellow='\\033[1;33m'      # Yellow\n    BBlue='\\033[1;34m'        # Blue\n    BPurple='\\033[1;35m'      # Purple\n    BCyan='\\033[1;36m'        # Cyan\n    BWhite='\\033[1;37m'       # White\n\n    # Underline\n    UBlack='\\033[4;30m'       # Black\n    URed='\\033[4;31m'         # Red\n    UGreen='\\033[4;32m'       # Green\n    UYellow='\\033[4;33m'      # Yellow\n    UBlue='\\033[4;34m'        # Blue\n    UPurple='\\033[4;35m'      # Purple\n    UCyan='\\033[4;36m'        # Cyan\n    UWhite='\\033[4;37m'       # White\n\n    # Background\n    On_Black='\\033[40m'       # Black\n    On_Red='\\033[41m'         # Red\n    On_Green='\\033[42m'       # Green\n    On_Yellow='\\033[43m'      # Yellow\n    On_Blue='\\033[44m'        # Blue\n    On_Purple='\\033[45m'      # Purple\n    On_Cyan='\\033[46m'        # Cyan\n    On_White='\\033[47m'       # White\n\n    # High Intensity\n    IBlack='\\033[0;90m'       # Black\n    IRed='\\033[0;91m'         # Red\n    IGreen='\\033[0;92m'       # Green\n    IYellow='\\033[0;93m'      # Yellow\n    IBlue='\\033[0;94m'        # Blue\n    IPurple='\\033[0;95m'      # Purple\n    ICyan='\\033[0;96m'        # Cyan\n    IWhite='\\033[0;97m'       # White\n\n    # Bold High Intensity\n    BIBlack='\\033[1;90m'      # Black\n    BIRed='\\033[1;91m'        # Red\n    BIGreen='\\033[1;92m'      # Green\n    BIYellow='\\033[1;93m'     # Yellow\n    BIBlue='\\033[1;94m'       # Blue\n    BIPurple='\\033[1;95m'     # Purple\n    BICyan='\\033[1;96m'       # Cyan\n    BIWhite='\\033[1;97m'      # White\n\n    # High Intensity backgrounds\n    On_IBlack='\\033[0;100m'   # Black\n    On_IRed='\\033[0;101m'     # Red\n    On_IGreen='\\033[0;102m'   # Green\n    On_IYellow='\\033[0;103m'  # Yellow\n    On_IBlue='\\033[0;104m'    # Blue\n    On_IPurple='\\033[0;105m'  # Purple\n    On_ICyan='\\033[0;106m'    # Cyan\n    On_IWhite='\\033[0;107m'   # White\n    :param string: a string to be printed\n    :param start: integer of the starting time\n    :param option: Additional option for the text style\n    :param output: name and path of the logfile to store the message\n    \"\"\"\n    # If not option is provided, default to bold high-intensity white\n    if not option:\n        # option = '\\033[1;97m'\n        option = '\\033[1;94m'\n    # Add the string formatting option to the message. Reset the format back to normal at the end with \\033[0m\n    print('{} [Elapsed Time: {:.2f} seconds] {} \\033[0m'.format(option, time.time() - start, string))\n    if output:\n        try:\n            with open(output, 'a') as log:\n                log.write('[Elapsed Time: {:.2f} seconds] {}\\n'.format(time.time() - start, string))\n        except FileNotFoundError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dotter():\n    # Use a global variable\n    global globalcount\n    if globalcount <= 80:\n        sys.stdout.write('.')\n        globalcount += 1\n    else:\n        sys.stdout.write('\\n.')\n        globalcount = 1", "response": "Prints formatted time to stdout at the start of a line and a."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(command, outfile=\"\"):\n    # Initialise count\n    count = 0\n    # Initialise the starting time\n    start = int(time.time())\n    maxtime = 0\n    # Removing Shell=True to prevent excess memory use thus shlex split if needed\n    if type(command) is not list:\n        command = shlex.split(command)\n    # Run the commands - direct stdout to PIPE and stderr to stdout\n    # DO NOT USE subprocess.PIPE if not writing it!\n    if outfile:\n        process = Popen(command, stdout=PIPE, stderr=STDOUT)\n    else:\n        devnull = open(os.devnull, 'wb')\n        process = Popen(command, stdout=devnull, stderr=STDOUT)\n    # Write the initial time\n    sys.stdout.write('[{:}] '.format(time.strftime('%H:%M:%S')))\n    # Create the output file - if not provided, then nothing should happen\n    writeout = open(outfile, \"ab+\") if outfile else \"\"\n    # Poll process for new output until finished\n    while True:\n        # If an output file name is provided\n        if outfile:\n            # Get stdout into a variable\n            nextline = process.stdout.readline()\n            # Print stdout to the file\n            writeout.write(nextline)\n        # Break from the loop if the command is finished\n        if process.poll() is not None:\n            break\n        # Adding sleep commands slowed down this method when there was lots of output. Difference between the start time\n        # of the analysis and the current time. Action on each second passed\n        currenttime = int(time.time())\n        if currenttime - start > maxtime:\n            # Set the max time for each iteration\n            maxtime = currenttime - start\n            # Print up to 80 dots on a line, with a one second delay between each dot\n            if count <= 80:\n                sys.stdout.write('.')\n                count += 1\n            # Once there are 80 dots on a line, start a new line with the the time\n            else:\n                sys.stdout.write('\\n[{:}] .'.format(time.strftime('%H:%M:%S')))\n                count = 1\n    # Close the output file\n    writeout.close() if outfile else \"\"\n    sys.stdout.write('\\n')", "response": "Execute a command and return the count of the command output."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a relative symlink to the source file and dest_file.", "response": "def relativesymlink(src_file, dest_file):\n    \"\"\"\n    https://stackoverflow.com/questions/9793631/creating-a-relative-symlink-in-python-without-using-os-chdir\n    :param src_file: the file to be linked\n    :param dest_file: the path and filename to which the file is to be linked\n    \"\"\"\n    # Perform relative symlinking\n    try:\n        print(os.path.relpath(src_file), os.path.relpath(dest_file))\n        os.symlink(\n            # Find the relative path for the source file and the destination file\n            os.path.relpath(src_file),\n            os.path.relpath(dest_file)\n        )\n    # Except os errors\n    except OSError as exception:\n        # If the os error is anything but directory exists, then raise\n        if exception.errno != errno.EEXIST:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating relative symlinks for the given file.", "response": "def relative_symlink(src_file, output_dir, output_name=None, export_output=False):\n    \"\"\"\n    Create relative symlinks files - use the relative path from the desired output directory to the storage path\n    e.g. ../../2013-SEQ-0072/simulated/40/50_150/simulated_trimmed/2013-SEQ-0072_simulated_40_50_150_R1.fastq.gz\n    is the relative path to the output_dir. The link name is the base name of the source file joined to the desired\n    output directory e.g. output_dir/2013-SEQ-0072/2013-SEQ-0072_simulated_40_50_150_R1.fastq.gz\n    https://stackoverflow.com/questions/9793631/creating-a-relative-symlink-in-python-without-using-os-chdir\n    :param src_file: Source file to be symbolically linked\n    :param output_dir: Destination folder for the link\n    :param output_name: Optionally allow for the link to have a different name\n    :param export_output: type BOOL: Optionally return the absolute path of the new, linked file\n    :return output_file: type STR: Absolute path of the newly-created symlink\n    \"\"\"\n    if output_name:\n        file_name = output_name\n    else:\n        file_name = os.path.basename(src_file)\n    #\n    output_file = os.path.join(output_dir, file_name)\n    try:\n        os.symlink(\n            os.path.relpath(\n                src_file,\n                output_dir),\n            output_file\n        )\n    # Ignore FileExistsErrors\n    except FileExistsError:\n        pass\n    # Return the absolute path of the symlink if requested\n    if export_output:\n        return output_file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncombine all unique sequences in a list of fasta gene targets into a single set of unique sequences.", "response": "def combinetargets(targets, targetpath, mol_type='nt'):\n    \"\"\"\n    Creates a set of all unique sequences in a list of supplied FASTA files. Properly formats headers and sequences\n    to be compatible with local pipelines. Splits hybrid entries. Removes illegal characters.\n    :param targets: fasta gene targets to combine\n    :param targetpath: folder containing the targets\n    \"\"\"\n    make_path(targetpath)\n    with open(os.path.join(targetpath, 'combinedtargets.fasta'), 'w') as combined:\n        idset = set()\n        for target in targets:\n            # Remove non-unicode characters present in the FASTA files\n            cleanedstring = str()\n            # Read in the file as binary\n            with open(target, 'rb') as fasta:\n                # Import all the text\n                text = fasta.read()\n                # Convert the binary variable to a string, ignoring non-UTF-8 characters\n                cleanedstring += text.decode('utf-8', 'ignore')\n            # Overwrite the file with the clean string\n            with open(target, 'w') as fasta:\n                fasta.write(cleanedstring)\n            # Clean up each record\n            for record in SeqIO.parse(target, 'fasta'):\n                # In case FASTA records have been spliced together, allow for the splitting of\n                # these records\n                if '>' in record.seq:\n                    # Split the two records apart on '>' symbols\n                    record.seq, hybrid = record.seq.split('>')\n                    # Split the header from the sequence e.g. sspC:6:CP003808.1ATGGAAAGTACATTAGA...\n                    # will be split into sspC:6:CP003808.1 and ATGGAAAGTACATTAGA\n                    hybridid, seq = re.findall('(.+\\\\d+\\\\.\\\\d)(.+)', str(hybrid))[0]\n                    # Replace and dashes in the record.id with underscores\n                    hybridid = hybridid.replace('-', '_')\n                    # Convert the string to a seq object\n                    if mol_type == 'nt':\n                        hybridseq = Seq(seq, generic_dna)\n                    else:\n                        hybridseq = Seq(seq, generic_protein)\n                    # Create a SeqRecord of the sequence - use the sequence object and id\n                    hybridrecord = SeqRecord(hybridseq,\n                                             description='',\n                                             id=hybridid)\n\n                    # Remove and dashes or 'N's from the sequence data - makeblastdb can't handle sequences\n                    # with gaps\n                    # noinspection PyProtectedMember\n                    hybridrecord.seq._data = hybridrecord.seq._data.replace('-', '').replace('N', '')\n                    # Write the original record to the file\n                    # Extract the sequence record from each entry in the multifasta\n                    # Replace and dashes in the record.id with underscores\n                    record.id = record.id.replace('-', '_')\n                    # Remove and dashes or 'N's from the sequence data - makeblastdb can't handle sequences\n                    # with gaps\n                    # noinspection PyProtectedMember\n                    record.seq._data = record.seq._data.replace('-', '').replace('N', '')\n                    # Clear the name and description attributes of the record\n                    record.name = ''\n                    record.description = ''\n                    if record.id not in idset:\n                        SeqIO.write(record, combined, 'fasta')\n                    if hybridrecord.id not in idset:\n                        # Write the second record to file\n                        SeqIO.write(hybridrecord, combined, 'fasta')\n                        idset.add(hybridrecord.id)\n\n                else:\n                    # Extract the sequence record from each entry in the multifasta\n                    # Replace and dashes in the record.id with underscores\n                    record.id = record.id.replace('-', '_')\n                    # Remove and dashes or 'N's from the sequence data - makeblastdb can't handle sequences\n                    # with gaps\n                    # noinspection PyProtectedMember\n                    record.seq._data = record.seq._data.replace('-', '').replace('N', '')\n                    # Clear the name and description attributes of the record\n                    record.name = ''\n                    record.description = ''\n                    if record.id not in idset:\n                        SeqIO.write(record, combined, 'fasta')\n                        idset.add(record.id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlocating all the FASTA files in the supplied sequence path and create the basic metadata objects for each sample.", "response": "def strainer(sequencepath):\n    \"\"\"\n    Locate all the FASTA files in the supplied sequence path. Create basic metadata objects for\n    each sample\n    \"\"\"\n    metadata_list = list()\n    assert os.path.isdir(sequencepath), 'Cannot locate sequence path as specified: {}' \\\n        .format(sequencepath)\n    # Get the sequences in the sequences folder into a list. Note that they must have a file extension that\n    # begins with .fa\n    strains = sorted(glob.glob(os.path.join(sequencepath, '*.fa*')))\n    # Populate the metadata object. This object will be populated to mirror the objects created in the\n    # genome assembly pipeline. This way this script will be able to be used as a stand-alone, or as part\n    # of a pipeline\n    assert strains, 'Could not find any files with an extension starting with \"fa\" in {}. Please check ' \\\n                    'to ensure that your sequence path is correct'.format(sequencepath)\n    for sample in strains:\n        # Create the object\n        metadata = MetadataObject()\n        # Set the base file name of the sequence. Just remove the file extension\n        filename = os.path.splitext(os.path.split(sample)[1])[0]\n        # Set the .name attribute to be the file name\n        metadata.name = filename\n        # Create the .general attribute\n        metadata.general = GenObject()\n        metadata.commands = GenObject()\n        metadata.general.outputdirectory = os.path.join(sequencepath, filename)\n        # Set the .general.bestassembly file to be the name and path of the sequence file\n        metadata.general.bestassemblyfile = os.path.join(metadata.general.outputdirectory, '{sn}.fasta'\n                                                         .format(sn=filename))\n        make_path(metadata.general.outputdirectory)\n        # Create a symlink to the directory\n        relative_symlink(sample,\n                         metadata.general.outputdirectory)\n        metadata.general.logout = os.path.join(metadata.general.outputdirectory, 'out')\n        metadata.general.logerr = os.path.join(metadata.general.outputdirectory, 'err')\n        # Append the metadata for each sample to the list of samples\n        metadata_list.append(metadata)\n    return strains, metadata_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef modify_usage_error(subcommand, program_list):\n    import click\n    from click._compat import get_text_stderr\n    from click.utils import echo\n\n    def show(self, file=None):\n        import sys\n        if file is None:\n            file = get_text_stderr()\n        color = None\n        if self.ctx is not None:\n            color = self.ctx.color\n        echo('Error: %s\\n' % self.format_message(), file=file, color=color)\n        # Set the sys.argv to be the first two arguments passed to the script if the subcommand was specified\n        arg2 = sys.argv[1] if sys.argv[1] in program_list else str()\n        sys.argv = [' '.join([sys.argv[0], arg2])] if arg2 else [sys.argv[0]]\n        # Call the help\n        subcommand(['--help'])\n\n    click.exceptions.UsageError.show = show", "response": "Modify the usage error for a given subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dotter(self):\n        if self.globalcount <= 80:\n            sys.stdout.write('.')\n            self.globalcount += 1\n        else:\n            sys.stdout.write('\\n.')\n            self.globalcount = 1", "response": "Prints formatted time to stdout at the start of a line and a."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string of either the datastore [ key ] or ND", "response": "def returnattr(self, key, number=False):\n        \"\"\"\n        Returns a string of either datastore[key], or 'ND' if datastore[key] doesn't exist formatted for a CSV report\n        Replace any commas with semicolons.\n        :param key: Dictionary key to be used to return the value from datastore[key]\n        :param number: Boolean whether the type of the attribute is a number (int, float, etc). Will return 0\n        instead of ND\n        \"\"\"\n        # String to return if the key is not in the datastore\n        negative_return = 'ND,' if not number else '0,'\n        try:\n            if key in self.datastore:\n                # Return the string of the value with any commas replaced by semicolons. Append a comma to the\n                # end of the string for the CSV format\n                return_key = '{},'.format(str(self.datastore[key]).replace(',', ';'))\n                if not number:\n                    return return_key\n                else:\n                    if return_key == 'ND,':\n                        return negative_return\n                    else:\n                        return return_key\n            else:\n                return negative_return\n        except AttributeError:\n            return negative_return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isattr(self, key):\n        try:\n            if key in self.datastore:\n                return True\n            else:\n                return False\n        except AttributeError:\n            return False", "response": "Checks to see if an attribute exists in the datastore."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump(self):\n        metadata = dict()\n        for attr in sorted(self.datastore):\n            # Initialise the attribute (e.g. sample.general) key in the metadata dictionary\n            metadata[attr] = dict()\n            # Ignore attributes that begin with '__'\n            if not attr.startswith('__'):\n                # If self.datastore[attribute] is a primitive datatype, populate the metadata dictionary with\n                # the attr: self.datastore[attr] pair\n                # e.g. attr: name,  self.datastore[attr]: 2013-SEQ-0072\n                if isinstance(self.datastore[attr], str) or \\\n                        isinstance(self.datastore[attr], list) or \\\n                        isinstance(self.datastore[attr], dict) or \\\n                        isinstance(self.datastore[attr], int):\n                    metadata[attr] = self.datastore[attr]\n                else:\n                    # Otherwise, recursively convert GenObjects to nested dictionaries\n                    metadata.update(self.nested_genobject(metadata, attr, self.datastore))\n        return metadata", "response": "Prints only the nested dictionary values ; removes methods members and attributes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nallow for the printing of nested GenObjects :param metadata: Nested dictionary containing the metadata. Will be further populated by this method :param attr: Current attribute being evaluated. Must be a GenObject e.g. sample.general :param datastore: The dictionary of the current attribute. Will be converted to nested dictionaries :return: Updated nested metadata dictionary with all GenObjects safely converted to dictionaries", "response": "def nested_genobject(self, metadata, attr, datastore):\n        \"\"\"\n        Allow for the printing of nested GenObjects\n        :param metadata: Nested dictionary containing the metadata. Will be further populated by this method\n        :param attr: Current attribute being evaluated. Must be a GenObject e.g. sample.general\n        :param datastore: The dictionary of the current attribute. Will be converted to nested dictionaries\n        :return: Updated nested metadata dictionary with all GenObjects safely converted to dictionaries\n        \"\"\"\n        # Iterate through all the key: value pairs of the current datastore[attr] datastore\n        # e.g. reverse_reads <accessoryFunctions.accessoryFunctions.GenObject object at 0x7fe153b725f8>\n        for key, value in sorted(datastore[attr].datastore.items()):\n            # If the type(value) is a GenObject, then JSON serialization will not work\n            if 'GenObject' in str(type(value)):\n                # Initialise the nested attribute: key nested dictionary within the metadata dictionary\n                # e.g. attr: 100_100, key: reverse_reads\n                metadata[attr][key] = dict()\n                # Iterate through the nested keys and nested values within the value datastore\n                # e.g. nested_key: length, nested_value: 100\n                for nested_key, nested_datastore in sorted(value.datastore.items()):\n                    # Create an additional dictionary layer within the metadata dictionary\n                    metadata[attr][key][nested_key] = dict()\n                    # If the type(nested_datastore) is a GenObject, recursively run this method to update the\n                    # metadata dictionary, supply the newly created nested dictionary: metadata[attr][key] as\n                    # the input metadata dictionary, the nested key as the input attribute, and the datastore of\n                    # value as the input datastore\n                    # e.g. key: 100_100,\n                    # datastore: <accessoryFunctions.accessoryFunctions.GenObject object at 0x7fc526001e80>\n                    if 'GenObject' in str(type(nested_datastore)):\n                        metadata[attr][key].update(\n                            self.nested_genobject(metadata[attr][key], nested_key, value.datastore))\n                    # If the nested datastore is not a GenObject, populate the nested metadata dictionary with\n                    # the attribute, key, nested key, and nested datastore\n                    # e.g. attr: 100_100, key: reverse_reads, nested_key: length, nested_datastore: 100\n                    else:\n                        metadata[attr][key][nested_key] = nested_datastore\n            # Non-GenObjects can (usually) be added to the metadata dictionary without issues\n            else:\n                try:\n                    if key not in self.unwanted_keys:\n                        metadata[attr][key] = value\n                except AttributeError:\n                    print('dumperror', attr)\n        # Return the metadata\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering all turtles of a given shape.", "response": "def render(self, model, color, num_turtles):\n        \"\"\"Renders all turtles of a given shape\"\"\"\n        self.program.bind()\n        glBindVertexArray(self.vao)\n\n        self.model_buffer.load(model.data, model.byte_size)\n        self.color_buffer.load(color.data, color.byte_size)\n\n        glDrawArraysInstanced(\n            GL_TRIANGLES,\n            0,\n            len(self.geometry.edges) // 7,  # 7 = 4 for vertex, 3 for edge\n            num_turtles\n        )\n\n        glBindVertexArray(0)\n        self.program.unbind()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef renew_connection(password):\n    with Controller.from_port(port=9051) as controller:\n        controller.authenticate(password=password)\n        controller.signal(Signal.NEWNYM)", "response": "Renews TOR session\n\n    :param password: new password"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_url(url):\n        parsed = url\n\n        if not url.startswith(\"http://\") and not url.startswith(\n                \"https://\"):  # if url is like www.yahoo.com\n            parsed = \"http://\" + parsed\n        elif url.startswith(\"https://\"):\n            parsed = parsed[8:]\n            parsed = \"http://\" + parsed\n\n        index_hash = parsed.rfind(\"#\")  # remove trailing #\n        index_slash = parsed.rfind(\"/\")\n        if index_hash > index_slash:\n            parsed = parsed[0: index_hash]\n\n        return parsed", "response": "Parses correctly url\n\n        :param url: url to parse"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_html_source(self):\n        req = urllib.request.Request(self.url)\n        req.add_header(\"user-agent\", random.choice(USER_AGENTS))\n        req_text = urllib.request.urlopen(req).read()\n        self.source = str(req_text)\n        self.soup = BeautifulSoup(self.source, \"html.parser\")\n        return self.source", "response": "Gets source page of url\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_links(self, recall, timeout):\n        for _ in range(recall):\n            try:  # setting timeout\n                soup = BeautifulSoup(self.source)  # parse source\n                out_links = []\n\n                for tag in soup.findAll([\"a\", \"link\"], href=True):\n                    tag[\"href\"] = urljoin(self.url, tag[\"href\"])\n                    out_links.append(tag[\"href\"])\n\n                return sorted(out_links)  # sort array\n            except:\n                time.sleep(timeout)", "response": "Gets links in page\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_in_browser(self, n_times):\n\n        for _ in range(n_times):\n            webbrowser.open(self.url)", "response": "Opens the page in browser n_times times"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_url(self, local_file):\n\n        downloader = urllib.request.URLopener()\n        downloader.retrieve(self.url, local_file)", "response": "Downloads url to local file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_to_file(self, local_file, headers=None, cookies=None,\n                         chunk_size=1024):\n        \"\"\"Downloads link to local file\n\n        :param local_file: Save url as this path\n        :param headers: Headers to fetch url\n        :param cookies: Cookies to fetch url\n        :param chunk_size: int\n        \"\"\"\n\n        if not headers:\n            headers = HEADERS\n\n        if not cookies:\n            cookies = {}\n\n        req = requests.get(self.url, headers=headers, cookies=cookies,\n                           stream=True)\n        with open(local_file, \"wb\") as local_download:\n            for chunk in req.iter_content(chunk_size):\n                if chunk:\n                    local_download.write(chunk)", "response": "Downloads link to local file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getLogicalLines(fp, allowQP=True, findBegin=False):\n    if not allowQP:\n        val = fp.read(-1)\n\n        #Shouldn't need this anymore...\n        \"\"\"\n        if len(val) > 0:\n            if not findBegin:\n                val = val.decode('utf-8')\n            else:\n                for encoding in 'utf-8', 'utf-16-LE', 'utf-16-BE', 'iso-8859-1':\n                    try:\n                        val = val.decode(encoding)\n                        if begin_re.search(val) is not None:\n                            break\n                    except UnicodeDecodeError:\n                        pass\n                else:\n                    raise ParseError('Could not find BEGIN when trying to determine encoding')\n        \"\"\"\n        # strip off any UTF8 BOMs which Python's UTF8 decoder leaves\n        #val = val.lstrip( unicode( codecs.BOM_UTF8, \"utf8\" ) )\n\n        lineNumber = 1\n        for match in logical_lines_re.finditer(val):\n            line, n = wrap_re.subn('', match.group())\n            if line != '':\n                yield line, lineNumber\n            lineNumber += n\n\n    else:\n        quotedPrintable = False\n        newbuffer = six.StringIO\n        logicalLine = newbuffer()\n        lineNumber = 0\n        lineStartNumber = 0\n        while True:\n            line = fp.readline()\n            if line == '':\n                break\n            else:\n                line = line.rstrip(CRLF)\n                lineNumber += 1\n            if line.rstrip() == '':\n                if logicalLine.tell() > 0:\n                    yield logicalLine.getvalue(), lineStartNumber\n                lineStartNumber = lineNumber\n                logicalLine = newbuffer()\n                quotedPrintable = False\n                continue\n\n            if quotedPrintable and allowQP:\n                logicalLine.write('\\n')\n                logicalLine.write(line)\n                quotedPrintable = False\n            elif line[0] in SPACEORTAB:\n                logicalLine.write(line[1:])\n            elif logicalLine.tell() > 0:\n                yield logicalLine.getvalue(), lineStartNumber\n                lineStartNumber = lineNumber\n                logicalLine = newbuffer()\n                logicalLine.write(line)\n            else:\n                logicalLine = newbuffer()\n                logicalLine.write(line)\n\n            # vCard 2.1 allows parameters to be encoded without a parameter name.\n            # False positives are unlikely, but possible.\n            val = logicalLine.getvalue()\n            if val[-1]=='=' and val.lower().find('quoted-printable') >= 0:\n                quotedPrintable=True\n\n        if logicalLine.tell() > 0:\n            yield logicalLine.getvalue(), lineStartNumber", "response": "Iterate through a file and yield a list of logical lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef foldOneLine(outbuf, input, lineLength = 75):\n\n    if len(input) < lineLength:\n        # Optimize for unfolded line case\n        try:\n            outbuf.write(bytes(input, 'UTF-8'))\n        except Exception:\n            # fall back on py2 syntax\n            outbuf.write(input)\n\n    else:\n        # Look for valid utf8 range and write that out\n        start = 0\n        written = 0\n        while written < len(input):\n            # Start max length -1 chars on from where we are\n            offset = start + lineLength - 1\n            if offset >= len(input):\n                line = input[start:]\n                try:\n                    outbuf.write(bytes(line, 'UTF-8'))\n                except Exception:\n                    # fall back on py2 syntax\n                    outbuf.write(line)\n                written = len(input)\n            else:\n                # Check whether next char is valid utf8 lead byte\n                # while (input[offset] > 0x7F) and ((ord(input[offset]) & 0xC0) == 0x80):\n                #    # Step back until we have a valid char\n                #    offset -= 1\n\n                line = input[start:offset]\n                try:\n                    outbuf.write(bytes(line, 'UTF-8'))\n                    outbuf.write(bytes(\"\\r\\n \", 'UTF-8'))\n                except Exception:\n                    # fall back on py2 syntax\n                    outbuf.write(line)\n                    outbuf.write(\"\\r\\n \")\n                written += offset - start\n                start = offset\n    try:\n        outbuf.write(bytes(\"\\r\\n\", 'UTF-8'))\n    except Exception:\n        # fall back on py2 syntax\n        outbuf.write(\"\\r\\n\")", "response": "Folds one line of a single - byte utf - 8 sequence into a single - byte sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a name return a behaviored ContentLine or Component.", "response": "def newFromBehavior(name, id=None):\n    \"\"\"\n    Given a name, return a behaviored ContentLine or Component.\n    \"\"\"\n    name = name.upper()\n    behavior = getBehavior(name, id)\n    if behavior is None:\n        raise VObjectError(\"No behavior found named %s\" % name)\n    if behavior.isComponent:\n        obj = Component(name)\n    else:\n        obj = ContentLine(name, [], '')\n    obj.behavior = behavior\n    obj.isNative = False\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transformToNative(self):\n        if self.isNative or not self.behavior or not self.behavior.hasNative:\n            return self\n        else:\n            try:\n                return self.behavior.transformToNative(self)\n            except Exception as e:\n                # wrap errors in transformation in a ParseError\n                lineNumber = getattr(self, 'lineNumber', None)\n\n                if isinstance(e, ParseError):\n                    if lineNumber is not None:\n                        e.lineNumber = lineNumber\n                    raise\n                else:\n                    msg = \"In transformToNative, unhandled exception on line %s: %s: %s\"\n                    msg = msg % (lineNumber, sys.exc_info()[0], sys.exc_info()[1])\n                    raise ParseError(msg, lineNumber)", "response": "Transform this object into a custom VBase subclass."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transformFromNative(self):\n        if self.isNative and self.behavior and self.behavior.hasNative:\n            try:\n                return self.behavior.transformFromNative(self)\n            except Exception as e:\n                # wrap errors in transformation in a NativeError\n                lineNumber = getattr(self, 'lineNumber', None)\n                if isinstance(e, NativeError):\n                    if lineNumber is not None:\n                        e.lineNumber = lineNumber\n                    raise\n                else:\n                    msg = \"In transformFromNative, unhandled exception on line %s %s: %s\"\n                    msg = msg % (lineNumber, sys.exc_info()[0], sys.exc_info()[1])\n                    raise NativeError(msg, lineNumber)\n        else:\n            return self", "response": "Returns a copy of this object with the original content if needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _wrap_callback_parse_time_info(subscription, on_data, message):\n    if message.type == message.REPLY:\n        time_response = web_pb2.TimeSubscriptionResponse()\n        time_response.ParseFromString(message.reply.data)\n        time = parse_isostring(time_response.timeInfo.currentTimeUTC)\n        #pylint: disable=protected-access\n        subscription._process(time)\n        if on_data:\n            on_data(time)\n    elif message.type == message.DATA:\n        if message.data.type == yamcs_pb2.TIME_INFO:\n            time_message = getattr(message.data, 'timeInfo')\n            time = parse_isostring(time_message.currentTimeUTC)\n            #pylint: disable=protected-access\n            subscription._process(time)\n            if on_data:\n                on_data(time)", "response": "Wraps a user callback to parse TimeInfo\n SARL message from a WebSocket data message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _wrap_callback_parse_event(on_data, message):\n    if message.type == message.DATA:\n        if message.data.type == yamcs_pb2.EVENT:\n            event = Event(getattr(message.data, 'event'))\n            #pylint: disable=protected-access\n            on_data(event)", "response": "Wraps a user callback to parse Events from a WebSocket data message"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _wrap_callback_parse_link_event(subscription, on_data, message):\n    if message.type == message.DATA:\n        if message.data.type == yamcs_pb2.LINK_EVENT:\n            link_message = getattr(message.data, 'linkEvent')\n            link_event = LinkEvent(link_message)\n            #pylint: disable=protected-access\n            subscription._process(link_event)\n            if on_data:\n                on_data(link_event)", "response": "Wraps a user callback to parse LinkEvents\n    from a WebSocket data message."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current mission time for the specified instance.", "response": "def get_time(self, instance):\n        \"\"\"\n        Return the current mission time for the specified instance.\n\n        :rtype: ~datetime.datetime\n        \"\"\"\n        url = '/instances/{}'.format(instance)\n        response = self.get_proto(url)\n        message = yamcsManagement_pb2.YamcsInstance()\n        message.ParseFromString(response.content)\n        if message.HasField('missionTime'):\n            return parse_isostring(message.missionTime)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning general server info.", "response": "def get_server_info(self):\n        \"\"\"\n        Return general server info.\n\n        :rtype: .ServerInfo\n        \"\"\"\n        response = self.get_proto(path='')\n        message = rest_pb2.GetApiOverviewResponse()\n        message.ParseFromString(response.content)\n        return ServerInfo(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_auth_info(self):\n        try:\n            response = self.session.get(self.auth_root, headers={\n                'Accept': 'application/protobuf'\n            })\n            message = web_pb2.AuthInfo()\n            message.ParseFromString(response.content)\n            return AuthInfo(message)\n        except requests.exceptions.ConnectionError:\n            raise ConnectionFailure('Connection to {} refused'.format(self.address))", "response": "Returns general authentication information."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_user_info(self):\n        response = self.get_proto(path='/user')\n        message = yamcsManagement_pb2.UserInfo()\n        message.ParseFromString(response.content)\n        return UserInfo(message)", "response": "Get information on the authenticated user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_instance(self, name, template, args=None, labels=None):\n        req = rest_pb2.CreateInstanceRequest()\n        req.name = name\n        req.template = template\n        if args:\n            for k in args:\n                req.templateArgs[k] = args[k]\n        if labels:\n            for k in labels:\n                req.labels[k] = labels[k]\n        url = '/instances'\n        self.post_proto(url, data=req.SerializeToString())", "response": "Create a new Yamcs instance based on an existing template. This method blocks until the instance is fully started."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_instance_templates(self):\n        response = self.get_proto(path='/instance-templates')\n        message = rest_pb2.ListInstanceTemplatesResponse()\n        message.ParseFromString(response.content)\n        templates = getattr(message, 'template')\n        return iter([InstanceTemplate(template) for template in templates])", "response": "Returns an iterator over the available instance templates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_services(self, instance):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        url = '/services/{}'.format(instance)\n        response = self.get_proto(path=url)\n        message = rest_pb2.ListServiceInfoResponse()\n        message.ParseFromString(response.content)\n        services = getattr(message, 'service')\n        return iter([Service(service) for service in services])", "response": "Returns an iterator over the services for an instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop_service(self, instance, service):\n        req = rest_pb2.EditServiceRequest()\n        req.state = 'stopped'\n        url = '/services/{}/{}'.format(instance, service)\n        self.patch_proto(url, data=req.SerializeToString())", "response": "Stops a single service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_processors(self, instance=None):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        url = '/processors'\n        if instance:\n            url += '/' + instance\n        response = self.get_proto(path=url)\n        message = rest_pb2.ListProcessorsResponse()\n        message.ParseFromString(response.content)\n        processors = getattr(message, 'processor')\n        return iter([Processor(processor) for processor in processors])", "response": "Lists the processors.\n\n        Processors are returned in lexicographical order.\n\n        :param Optional[str] instance: A Yamcs instance name.\n        :rtype: ~collections.Iterable[.Processor]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_clients(self, instance=None):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        url = '/clients'\n        if instance:\n            url = '/instances/{}/clients'.format(instance)\n        response = self.get_proto(path=url)\n        message = rest_pb2.ListClientsResponse()\n        message.ParseFromString(response.content)\n        clients = getattr(message, 'client')\n        return iter([Client(client) for client in clients])", "response": "Lists the clients.\n\n        :param Optional[str] instance: A Yamcs instance name.\n        :rtype: ~collections.Iterable[yamcs.model.Client]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting the instances. Instances are returned in lexicographical order. :rtype: ~collections.Iterable[.Instance]", "response": "def list_instances(self):\n        \"\"\"\n        Lists the instances.\n\n        Instances are returned in lexicographical order.\n\n        :rtype: ~collections.Iterable[.Instance]\n        \"\"\"\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        response = self.get_proto(path='/instances')\n        message = rest_pb2.ListInstancesResponse()\n        message.ParseFromString(response.content)\n        instances = getattr(message, 'instance')\n        return iter([Instance(instance) for instance in instances])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_instance(self, instance):\n        params = {'state': 'running'}\n        url = '/instances/{}'.format(instance)\n        self.patch_proto(url, params=params)", "response": "Starts a single instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_instance(self, instance):\n        params = {'state': 'stopped'}\n        url = '/instances/{}'.format(instance)\n        self.patch_proto(url, params=params)", "response": "Stops a single instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restart_instance(self, instance):\n        params = {'state': 'restarted'}\n        url = '/instances/{}'.format(instance)\n        self.patch_proto(url, params=params)", "response": "Restarts a single instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_data_links(self, instance):\n        # Server does not do pagination on listings of this resource.\n        # Return an iterator anyway for similarity with other API methods\n        response = self.get_proto(path='/links/' + instance)\n        message = rest_pb2.ListLinkInfoResponse()\n        message.ParseFromString(response.content)\n        links = getattr(message, 'link')\n        return iter([Link(link) for link in links])", "response": "Lists the data links visible to this client."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending an event to Yamcs.", "response": "def send_event(self, instance, message, event_type=None, time=None,\n                   severity='info', source=None, sequence_number=None):\n        \"\"\"\n        Post a new event.\n\n        :param str instance: A Yamcs instance name.\n        :param str message: Event message.\n        :param Optional[str] event_type: Type of event.\n\n        :param severity: The severity level of the event. One of ``info``,\n                         ``watch``, ``warning``, ``critical`` or ``severe``.\n                         Defaults to ``info``.\n        :type severity: Optional[str]\n\n        :param time: Time of the event. If unspecified, defaults to mission time.\n        :type time: Optional[~datetime.datetime]\n\n        :param source: Source of the event. Useful for grouping events in the\n                       archive. When unset this defaults to ``User``.\n        :type source: Optional[str]\n\n        :param sequence_number: Sequence number of this event. This is primarily\n                                used to determine unicity of events coming from\n                                the same source. If not set Yamcs will\n                                automatically assign a sequential number as if\n                                every submitted event is unique.\n        :type sequence_number: Optional[int]\n        \"\"\"\n        req = rest_pb2.CreateEventRequest()\n        req.message = message\n        req.severity = severity\n        if event_type:\n            req.type = event_type\n        if time:\n            req.time = to_isostring(time)\n        if source:\n            req.source = source\n        if sequence_number is not None:\n            req.sequence_number = sequence_number\n\n        url = '/archive/{}/events'.format(instance)\n        self.post_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data_link(self, instance, link):\n        response = self.get_proto('/links/{}/{}'.format(instance, link))\n        message = yamcsManagement_pb2.LinkInfo()\n        message.ParseFromString(response.content)\n        return Link(message)", "response": "Gets a single data link."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenable a data link.", "response": "def enable_data_link(self, instance, link):\n        \"\"\"\n        Enables a data link.\n\n        :param str instance: A Yamcs instance name.\n        :param str link: The name of the data link.\n        \"\"\"\n        req = rest_pb2.EditLinkRequest()\n        req.state = 'enabled'\n        url = '/links/{}/{}'.format(instance, link)\n        self.patch_proto(url, data=req.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_data_link_subscription(self, instance, on_data=None, timeout=60):\n        manager = WebSocketSubscriptionManager(self, resource='links')\n\n        # Represent subscription as a future\n        subscription = DataLinkSubscription(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_link_event, subscription, on_data)\n\n        manager.open(wrapped_callback, instance)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription", "response": "Create a new subscription for receiving data link updates of an instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new subscription for receiving time updates of an instance.", "response": "def create_time_subscription(self, instance, on_data=None, timeout=60):\n        \"\"\"\n        Create a new subscription for receiving time updates of an instance.\n        Time updates are emitted at 1Hz.\n\n        This method returns a future, then returns immediately. Stop the\n        subscription by canceling the future.\n\n        :param str instance: A Yamcs instance name\n\n        :param on_data: Function that gets called with\n                        :class:`~datetime.datetime` updates.\n        :type on_data: Optional[Callable[~datetime.datetime])\n\n        :param timeout: The amount of seconds to wait for the request to\n                        complete.\n        :type timeout: Optional[float]\n\n        :return: Future that can be used to manage the background websocket\n                 subscription.\n        :rtype: .TimeSubscription\n        \"\"\"\n        manager = WebSocketSubscriptionManager(self, resource='time')\n\n        # Represent subscription as a future\n        subscription = TimeSubscription(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_time_info, subscription, on_data)\n\n        manager.open(wrapped_callback, instance)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new subscription for receiving events of an instance.", "response": "def create_event_subscription(self, instance, on_data, timeout=60):\n        \"\"\"\n        Create a new subscription for receiving events of an instance.\n\n        This method returns a future, then returns immediately. Stop the\n        subscription by canceling the future.\n\n        :param str instance: A Yamcs instance name\n\n        :param on_data: Function that gets called on each :class:`.Event`.\n        :type on_data: Optional[Callable[.Event])\n\n        :param timeout: The amount of seconds to wait for the request to\n                        complete.\n        :type timeout: Optional[float]\n\n        :return: Future that can be used to manage the background websocket\n                 subscription.\n        :rtype: .WebSocketSubscriptionFuture\n        \"\"\"\n        manager = WebSocketSubscriptionManager(self, resource='events')\n\n        # Represent subscription as a future\n        subscription = WebSocketSubscriptionFuture(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_event, on_data)\n\n        manager.open(wrapped_callback, instance)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __complete_info_dict(self, node_info_dict, is_open):\n\n        # Make pika credentials\n        creds = pika.PlainCredentials(\n            node_info_dict['username'],\n            node_info_dict['password']\n        )\n        node_info_dict['credentials'] = creds\n        if 'priority' in node_info_dict and node_info_dict['priority'] is not None:\n            node_info_dict['priority'] = str(node_info_dict['priority'])\n        else:\n            node_info_dict['priority'] = DEFAULT_PRIO\n\n        # Mandatories:\n        host = node_info_dict['host']\n        credentials = node_info_dict['credentials']\n\n        # Optional ones\n        # If not specified, fill in defaults.\n        vhost = \"\"\n        if 'vhost' in node_info_dict and node_info_dict['vhost'] is not None:\n            vhost = node_info_dict['vhost']\n        port = 15672\n        if 'port' in node_info_dict and node_info_dict['port'] is not None:\n            port = node_info_dict['port']\n        ssl_enabled = False\n        if 'ssl_enabled' in node_info_dict and node_info_dict['ssl_enabled'] is not None:\n            ssl_enabled = node_info_dict['ssl_enabled']\n\n\n        # Get some defaults:\n        socket_timeout = esgfpid.defaults.RABBIT_PIKA_SOCKET_TIMEOUT\n        connection_attempts = esgfpid.defaults.RABBIT_PIKA_CONNECTION_ATTEMPTS\n        retry_delay = esgfpid.defaults.RABBIT_PIKA_CONNECTION_RETRY_DELAY_SECONDS\n        \n        # Make pika connection params\n        # https://pika.readthedocs.org/en/0.9.6/connecting.html\n        params = pika.ConnectionParameters(\n            host=host,\n            ssl=ssl_enabled,\n            port=port,\n            virtual_host=vhost,\n            credentials=credentials,\n            socket_timeout=socket_timeout,\n            connection_attempts=connection_attempts,\n            retry_delay=retry_delay\n        )\n\n        node_info_dict['params'] = params\n\n        # Add some stuff\n        node_info_dict['is_open'] = is_open\n        '''\n        https://pika.readthedocs.org/en/0.9.6/connecting.html\n        class pika.connection.ConnectionParameters(\n            host=None, port=None, virtual_host=None, credentials=None, channel_max=None,\n            frame_max=None, heartbeat_interval=None, ssl=None, ssl_options=None,\n            connection_attempts=None, retry_delay=None, socket_timeout=None, locale=None,\n            backpressure_detection=None)\n        '''\n        return node_info_dict", "response": "Complete the node info dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clear(self):\n        self.root = RootGroup()\n        self.groups = []\n        self.entries = []\n        self.readonly = False\n        self.header = None\n        self.password = None\n        self.keyfile = None\n        self.filepath = None", "response": "Resets internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the database from file.", "response": "def load(self, dbfile, password=None, keyfile=None, readonly=False):\n        \"\"\"\n        Load the database from file/stream.\n        \n        :param dbfile: The database file path/stream.\n        :type dbfile: str or file-like object\n        :param password: The password for the database.\n        :type password: str\n        :param keyfile: Path to a keyfile (or a stream) that can be used instead of or in conjunction with password for database.\n        :type keyfile: str or file-like object\n        :param readonly: Whether to open the database read-only.\n        :type readonly: bool\n        \"\"\"\n        \n        self._clear()\n        buf = None\n        is_stream = hasattr(dbfile, 'read') \n        if is_stream:\n            buf = dbfile.read()\n        else:\n            if not os.path.exists(dbfile):\n                raise IOError(\"File does not exist: {0}\".format(dbfile))\n            \n            with open(dbfile, 'rb') as fp:\n                buf = fp.read()\n                \n        self.load_from_buffer(buf, password=password, keyfile=keyfile, readonly=readonly)\n        \n        # One we have successfully loaded the file, go ahead and set the internal attribute\n        # (in the LockingDatabase subclass, this will effectivley take out the lock on the file)\n        if not is_stream:\n            self.filepath = dbfile"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a database from a byte - string buffer.", "response": "def load_from_buffer(self, buf, password=None, keyfile=None, readonly=False):\n        \"\"\"\n        Load a database from passed-in buffer (bytes).\n\n        :param buf: A string (bytes) of the database contents.\n        :type buf: str\n        :param password: The password for the database.\n        :type password: str\n        :param keyfile: Path to a keyfile (or a stream) that can be used instead of or in conjunction with password for database.\n        :type keyfile: str or file-like object\n        :param readonly: Whether to open the database read-only.\n        :type readonly: bool\n        \"\"\"\n        if password is None and keyfile is None:\n            raise ValueError(\"Password and/or keyfile is required.\")\n        \n        # Save these to use as defaults when saving the database\n        self.password = password\n        self.keyfile = keyfile\n        \n        # The header is 124 bytes long, the rest is content\n        hdr_len = HeaderStruct.length\n        header_bytes = buf[:hdr_len]\n        crypted_content = buf[hdr_len:]\n        \n        self.header = HeaderStruct(header_bytes)\n        \n        self.log.debug(\"Extracted header: {0}\".format(self.header))\n        # Check if the database is supported\n        if self.header.version & const.DB_SUPPORTED_VERSION_MASK != const.DB_SUPPORTED_VERSION & const.DB_SUPPORTED_VERSION_MASK:\n            raise exc.UnsupportedDatabaseVersion('Unsupported file version: {0}'.format(hex(self.header.version)))\n            \n        #Actually, only AES is supported.\n        if not self.header.flags & HeaderStruct.AES:\n            raise exc.UnsupportedDatabaseEncryption('Only AES encryption is supported.')\n        \n        final_key = util.derive_key(seed_key=self.header.seed_key,\n                                    seed_rand=self.header.seed_rand,\n                                    rounds=self.header.key_enc_rounds,\n                                    password=password, keyfile=keyfile)\n        \n        # FIXME: Remove this once we've tracked down issues.\n        self.log.debug(\"(load) Final key: {0!r}, pass={1}\".format(final_key, password))\n        \n        decrypted_content = util.decrypt_aes_cbc(crypted_content, key=final_key, iv=self.header.encryption_iv)\n        \n        # Check if decryption failed\n        if ((len(decrypted_content) > const.DB_MAX_CONTENT_LEN) or\n            (len(decrypted_content) == 0 and self.header.ngroups > 0)):\n            raise exc.IncorrectKey(\"Decryption failed! The key is wrong or the file is damaged.\")\n        \n        if not self.header.contents_hash == hashlib.sha256(decrypted_content).digest():\n            self.log.debug(\"Decrypted content: {0!r}\".format(decrypted_content))\n            self.log.error(\"Hash mismatch. Header hash = {0!r}, hash of contents = {1!r}\".format(self.header.contents_hash,                                                                                                 hashlib.sha256(decrypted_content).digest()))\n            raise exc.AuthenticationError(\"Hash test failed. The key is wrong or the file is damaged.\")\n            \n        # First thing (after header) are the group definitions.\n        for _i in range(self.header.ngroups):\n            gstruct = GroupStruct(decrypted_content)\n            self.groups.append(Group.from_struct(gstruct))\n            length = len(gstruct)\n            decrypted_content = decrypted_content[length:]\n        \n        # Next come the entry definitions.\n        for _i in range(self.header.nentries):\n            estruct = EntryStruct(decrypted_content)\n            self.entries.append(Entry.from_struct(estruct))\n            length = len(estruct)\n            decrypted_content = decrypted_content[length:]\n            \n        # Sets up the hierarchy, relates the group/entry model objects.\n        self._bind_model()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, dbfile=None, password=None, keyfile=None):\n        if self.readonly:\n            # We might wish to make this more sophisticated.  E.g. if a new path is specified\n            # as a parameter, then it's probably ok to ignore a readonly flag?  In general\n            # this flag doens't make a ton of sense for a library ...\n            raise exc.ReadOnlyDatabase()\n        \n        if dbfile is not None and not hasattr(dbfile, 'write'):\n            self.filepath = dbfile\n            \n        if password is not None or self.keyfile is not None:\n            # Do these together so we don't end up with some hybrid of old & new key material\n            self.password = password\n            self.keyfile = keyfile  \n        else: \n            raise ValueError(\"Password and/or keyfile is required.\")\n        \n        if self.filepath is None and dbfile is None:\n            raise ValueError(\"Unable to save without target file.\")\n        \n        buf = bytearray()\n        \n        # First, serialize the groups\n        for group in self.groups:\n            # Get the packed bytes\n            group_struct = group.to_struct()\n            self.log.debug(\"Group struct: {0!r}\".format(group_struct))\n            buf += group_struct.encode()\n            \n        # Then the entries.\n        for entry in self.entries:\n            entry_struct = entry.to_struct()\n            buf += entry_struct.encode()\n\n        # Hmmmm ... these defaults should probably be set elsewhere....?\n        header = HeaderStruct()\n        header.signature1 = const.DB_SIGNATURE1\n        header.signature2 = const.DB_SIGNATURE2\n        header.flags = header.AES\n        header.version = 0x00030002\n        header.key_enc_rounds = 50000\n        header.seed_key = get_random_bytes(32)\n        \n        # Convert buffer to bytes for API simplicity\n        buf = bytes(buf)\n        \n        # Generate new seed & vector; update content hash        \n        header.encryption_iv = get_random_bytes(16)\n        header.seed_rand = get_random_bytes(16)\n        header.contents_hash = hashlib.sha256(buf).digest()\n        \n        self.log.debug(\"(Unencrypted) content: {0!r}\".format(buf))\n        self.log.debug(\"Generating hash for {0}-byte content: {1}\".format(len(buf), hashlib.sha256(buf).digest()))\n        # Update num groups/entries to match curr state\n        header.nentries = len(self.entries)\n        header.ngroups = len(self.groups)\n        \n        final_key = util.derive_key(seed_key=header.seed_key,\n                                    seed_rand=header.seed_rand,\n                                    rounds=header.key_enc_rounds,\n                                    password=password, keyfile=keyfile)\n        \n        # FIXME: Remove this once we've tracked down issues.\n        self.log.debug(\"(save) Final key: {0!r}, pass={1}\".format(final_key, password))\n        \n        encrypted_content = util.encrypt_aes_cbc(buf, key=final_key, iv=header.encryption_iv)\n        \n        if hasattr(dbfile, 'write'):\n            dbfile.write(header.encode() + encrypted_content)\n        else:\n            with open(self.filepath, \"wb\") as fp:\n                fp.write(header.encode() + encrypted_content)", "response": "Save the current state of the object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_group(self, title, parent=None, icon=1, expires=None):\n        if parent and not isinstance(parent, Group):\n            raise TypeError(\"Parent must be of type Group\")\n        \n        if expires is None:\n            expires = const.NEVER\n        \n        if self.groups:\n            group_id = max([g.id for g in self.groups]) + 1\n        else:\n            group_id = 1\n        \n        group = Group(id=group_id, title=title, icon=icon, db=self, \n                      created=util.now(), modified=util.now(), accessed=util.now(),\n                      expires=expires)\n        \n        # If no parent is given, just append the new group at the end\n        if parent is None:\n            group.parent = self.root\n            self.root.children.append(group)\n            group.level = 0\n            self.groups.append(group)\n            \n        # Else insert the group behind the parent\n        else:\n            if parent not in self.groups:\n                raise ValueError(\"Group doesn't exist / is not bound to this database.\")\n            parent.children.append(group)\n            group.parent = parent\n            group.level = parent.level + 1\n            self.groups.insert(self.groups.index(parent) + 1, group)\n                \n        return group", "response": "This method creates a new group."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the specified group from the database.", "response": "def remove_group(self, group):\n        \"\"\"\n        Remove the specified group.\n        \"\"\"\n        if not isinstance(group, Group):\n            raise TypeError(\"group must be Group\")\n        if group not in self.groups:\n            raise ValueError(\"Group doesn't exist / is not bound to this database.\")\n        \n        #save num entries and children before removal to avoid for loop problems\n        num_entries = len(group.entries)\n        for i in xrange(num_entries):\n            self.remove_entry(group.entries[0])\n\n        # Recurse down to remove sub-groups\n        num_children = len(group.children)\n        for i in xrange(num_children): # We may need to copy this to avoid CME (see below)\n            self.remove_group(group.children[0])\n        \n        # Finally remove group from the parent's list.\n        group.parent.children.remove(group) # Concurrent modification exception? Parent in recursive stack is iterating ...\n        self.groups.remove(group)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving a group to a child of a parent.", "response": "def move_group(self, group, parent, index=None):\n        \"\"\"\n        Move group to be a child of new parent.\n        \n        :param group: The group to move.\n        :type group: :class:`keepassdb.model.Group`\n        :param parent: The new parent for the group.\n        :type parent: :class:`keepassdb.model.Group`\n        :param index: The 0-based index within the parent (defaults to appending\n                      group to end of parent's children).\n        :type index: int\n        \"\"\"\n        if not isinstance(group, Group):\n            raise TypeError(\"group param must be of type Group\")\n        \n        if parent is not None and not isinstance(parent, Group):\n            raise TypeError(\"parent param must be of type Group\")\n            \n        if group is parent:\n            raise ValueError(\"group and parent are the same\")\n            \n        if parent is None:\n            parent = self.root\n        elif parent not in self.groups:\n            raise exc.UnboundModelError(\"Parent group doesn't exist / is not bound to this database.\")\n            \n        if group not in self.groups:\n            raise exc.UnboundModelError(\"Group doesn't exist / is not bound to this database.\")\n        \n        curr_parent = group.parent\n        curr_parent.children.remove(group)\n        \n        if index is None:\n            parent.children.append(group)\n            self.log.debug(\"Moving {0!r} to child of {1!r}, (appending)\".format(group, parent))\n        else:\n            parent.children.insert(index, group)\n            self.log.debug(\"Moving {0!r} to child of {1!r}, (at position {2!r})\".format(group, parent, index))\n        \n        #Recurse down and reset level of all moved nodes\n        def set_level(g):\n            g.level = g.parent.level + 1\n            for child in g.children:\n                set_level(child)\n\n        group.parent = parent\n        set_level(group)\n        group.modified = util.now()\n        \n        self._rebuild_groups()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrebuilding the master list based on the groups hierarchy.", "response": "def _rebuild_groups(self):\n        \"\"\"\n        Recreates the groups master list based on the groups hierarchy (order matters here,\n        since the parser uses order to determine lineage).\n        \"\"\"\n        self.groups = []\n        \n        def collapse_group(group):\n            for subgroup in group.children:\n                self.groups.append(subgroup)\n                collapse_group(subgroup)\n                \n        collapse_group(self.root)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new entry object.", "response": "def create_entry(self, group, **kwargs):\n        \"\"\"\n        Create a new Entry object.\n        \n        The group which should hold the entry is needed.\n\n        image must be an unsigned int >0, group a Group.\n        \n        :param group: The associated group.\n        :keyword title: \n        :keyword icon:\n        :keyword url:\n        :keyword username:\n        :keyword password:\n        :keyword notes:\n        :keyword expires: Expiration date (if None, entry will never expire). \n        :type expires: datetime\n         \n        :return: The new entry.\n        :rtype: :class:`keepassdb.model.Entry`\n        \"\"\"\n        if group not in self.groups:\n            raise ValueError(\"Group doesn't exist / is not bound to this database.\")\n                 \n        uuid = binascii.hexlify(get_random_bytes(16))\n        \n        entry = Entry(uuid=uuid,\n                      group_id=group.id,\n                      created=util.now(),\n                      modified=util.now(),\n                      accessed=util.now(),\n                      **kwargs)\n        \n        self.entries.append(entry)\n        group.entries.append(entry)\n        \n        return entry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_entry(self, entry):\n        if not isinstance(entry, Entry):\n            raise TypeError(\"entry param must be of type Entry.\")\n        if not entry in self.entries:\n            raise ValueError(\"Entry doesn't exist / not bound to this datbase.\")\n        \n        entry.group.entries.remove(entry)\n        self.entries.remove(entry)", "response": "Removes specified entry from the datbase."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves an entry to another group.", "response": "def move_entry(self, entry, group, index=None):\n        \"\"\"\n        Move an entry to another group.\n        \n        :param entry: The Entry object to move.\n        :type entry: :class:`keepassdb.model.Entry`\n        \n        :param group: The new parent Group object for the entry.\n        :type group: :class:`keepassdb.model.Group`\n        \n        :param index: The 0-based index within the parent (defaults to appending\n                      group to end of parent's children).\n        :type index: int\n        \"\"\"\n        if not isinstance(entry, Entry):\n            raise TypeError(\"entry param must be of type Entry\")\n        if not isinstance(group, Group):\n            raise TypeError(\"group param must be of type Group\")\n        \n        if entry not in self.entries:\n            raise exc.UnboundModelError(\"Invalid entry (or not bound to this database): {0!r}\".format(entry))\n        if group not in self.groups:\n            raise exc.UnboundModelError(\"Invalid group (or not bound to this database): {0!r}\".format(group))\n        \n        curr_group = entry.group\n        \n        curr_group.entries.remove(entry)\n        if index is None:\n            group.entries.append(entry)\n            self.log.debug(\"Moving {0!r} to child of {1!r}, (appending)\".format(entry, group))\n        else:\n            group.entries.insert(index, entry)\n            self.log.debug(\"Moving {0!r} to child of {1!r}, (at position {2})\".format(entry, group, index))\n            \n        entry.group = group\n        \n        entry.modified = util.now()\n        \n        self._rebuild_entries()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _rebuild_entries(self):\n        self.entries = []\n        def collapse_entries(group):\n            for entry in group.entries:\n                self.entries.append(entry)\n            for subgroup in group.children:\n                collapse_entries(subgroup)\n                \n        collapse_entries(self.root)", "response": "Rebuilds the master list based on the groups hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbind the various model objects together in the correct hierarchy and adds referneces to this database object in the groups.", "response": "def _bind_model(self):\n        \"\"\"\n        This method binds the various model objects together in the correct hierarchy\n        and adds referneces to this database object in the groups.\n        \"\"\"\n\n        if self.groups[0].level != 0:\n            self.log.info(\"Got invalid first group: {0}\".format(self.groups[0]))\n            raise ValueError(\"Invalid group tree: first group must have level of 0 (got {0})\".format(self.groups[0].level))\n        \n        # The KeePassX source code maintains that first group to have incremented \n        # level is a child of the previous group with a lower level.\n        #\n        # [R]\n        #  | A (1)\n        #  +-| B (2)\n        #  | | C (2)\n        #  | D (1)\n        #  +-| E (2)\n        #    | F (2)\n        #    +-| G (3)\n        #      | H (3)\n        #      | I (3)\n        #       \n        \n        class Stack(list):\n            \"\"\" A class to make parsing code slightly more semantic. \"\"\"\n            def push(self, el):\n                self.append(el)\n\n        # This is a different parsing approach than taken by KeePassX (or other python               \n        # libs), but seems a little more intuitive.  It could be further simplified\n        # by noting that current_parent is always parent_stack[-1], but this is a bit\n        # more readable.\n        parent_stack = Stack([self.root])        \n        current_parent = self.root\n        prev_group = None\n        for g in self.groups:\n            g.db = self # Bind database to group objects\n            if prev_group is not None: # first iteration is exceptional\n                if g.level > prev_group.level: # Always true for iteration 1 since root has level of -1\n                    # Dropping down a level; the previous group is the parent\n                    current_parent = prev_group\n                    parent_stack.push(current_parent)\n                elif g.level < prev_group.level:\n                    # Pop off parents until we have a parent with a level that is less than current\n                    while g.level <= current_parent.level:\n                        current_parent = parent_stack.pop()\n                    parent_stack.push(current_parent) # We want to make sure that the top of the stack always matches current parent\n                \n            # bi-directional child-parent binding\n            g.parent = current_parent\n            current_parent.children.append(g)\n            \n            prev_group = g\n            \n        # Bind group objects to entries\n        for entry in self.entries:\n            for group in self.groups:\n                if entry.group_id == group.id:\n                    group.entries.append(entry)\n                    entry.group = group\n                    break\n            else:\n                # KeePassX adds these to the first group (i.e. root.children[0])\n                raise NotImplementedError(\"Orphaned entries not (yet) supported.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filepath(self, value):\n        if not self.readonly and self._filepath != value:\n            if self._locked:\n                self.log.debug(\"Releasing previously-held lock file: {0}\".format(self.lockfile))\n                # Release the lock on previous filepath.\n                self.release_lock()\n            self._filepath = value\n            if self._filepath is not None:\n                self.acquire_lock()\n        else:\n            self._filepath = value", "response": "Property for setting current filepath."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef acquire_lock(self, force=False):\n        if self.readonly:\n            raise exc.ReadOnlyDatabase()\n        if not self._locked:\n            self.log.debug(\"Acquiring lock file: {0}\".format(self.lockfile))\n            if os.path.exists(self.lockfile) and not force:\n                raise exc.DatabaseAlreadyLocked('Lock file already exists: {0}'.format(self.lockfile)) \n            open(self.lockfile, 'w').close()\n            self._locked = True", "response": "Acquires a lock file for the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrelease the lock file if it was acquired by this class.", "response": "def release_lock(self, force=False):\n        \"\"\"\n        Releases the lock  (deletes the <dbname>.lock file) if it was acquired by this class or force is set to True.\n        \n        :param force: Whether to force releasing the lock (e.g. if it was not acquired during this session).\n        :type force: bool\n        \"\"\"\n        if self.readonly:\n            raise exc.ReadOnlyDatabase()\n        if self._locked or force:\n            self.log.debug(\"Removing lock file: {0}\".format(self.lockfile))\n            if os.path.exists(self.lockfile):\n                os.remove(self.lockfile)\n                self._locked = False\n        else:\n            self.log.debug(\"Database not locked (not removing)\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        super(LockingDatabase, self).close()\n        if not self.readonly:\n            self.release_lock()", "response": "Closes the database releasing the lock."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a date string from the format 2018 - 08 - 15T23. 55. 17 into a datetime object", "response": "def convert_time_string(date_str):\n    \"\"\" Change a date string from the format 2018-08-15T23:55:17 into a datetime object \"\"\"\n    dt, _, _ = date_str.partition(\".\")\n    dt = datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\")\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dates_file(path):\n    with open(path) as f:\n        dates = f.readlines()\n    return [(convert_time_string(date_string.split(\" \")[0]), float(date_string.split(\" \")[1]))\n            for date_string in dates]", "response": "parse dates file of dates and probability of choosing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dates_link(url):\n    urllib.request.urlretrieve(url, \"temp.txt\")\n    dates = get_dates_file(\"temp.txt\")\n    os.remove(\"temp.txt\")\n    return dates", "response": "Download the dates file from the internet and parse it as a dates file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch(self, multithread=True, median_kernel=5, solar_diam=740):\n\n        # helper function to pull data\n        def func_map(product):\n            \"\"\"\n            determines which function to call for a specific product and gets\n            :param product: which product to fetch\n            :type product: str\n            :return: product tuple\n            :rtype: (header, data)\n            \"\"\"\n            if \"halpha\" in product:\n                result = self.fetch_halpha(median_kernel=median_kernel)\n            elif \"aia\" in product:\n                result = self.fetch_aia(product, median_kernel=median_kernel)\n            elif \"l1b\" in product:\n                result = self.fetch_suvi_l1b(product, median_kernel=median_kernel)\n            elif \"l2-ci\" in product:\n                result = self.fetch_suvi_composite(product, median_kernel=median_kernel)\n            elif \"limb\" in product:\n                result = self.fetch_limb(solar_diam)\n            else:\n                raise ValueError(\"{} is not a valid product.\".format(product))\n            return result\n\n        if multithread:\n            pool = ThreadPool()\n            results = pool.map(func_map, self.products)\n        else:\n            results = [func_map(product) for product in self.products]\n\n        results = {product: (head, data) for product, head, data in results}\n        return results", "response": "This method will fetch all the available images from the system and return a dictionary of all the products in the system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_limb(self, diam_sun, base_shape=(1280, 1280)):\n        from scipy.ndimage.interpolation import zoom\n\n        fn = pkg_resources.resource_filename(\"suvitrainer\", \"path_length_280_noisy.fits\")\n        calculated_diam = int(os.path.basename(fn).split(\"_\")[2]) * 2\n        with fits.open(fn) as hdus:\n            limb_unscaled = hdus[0].data\n        scale_factor = diam_sun / calculated_diam\n        limb_scaled = zoom(limb_unscaled, scale_factor)\n        shape = limb_scaled.shape[0]\n        excess = int((shape - base_shape[0]) / 2.0)\n        limb_scaled = limb_scaled[excess:shape - excess, excess:shape - excess]\n        limb_scaled = limb_scaled[0:base_shape[0], 0:base_shape[1]]\n        return \"limb\", None, limb_scaled", "response": "reads the solar limb template file and adjusts to the requested solar diameter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch the halpha image from Virtual Solar Observatory GONG archive", "response": "def fetch_halpha(self, correct=True, median_kernel=5):\n        \"\"\"\n        pull halpha from that time from Virtual Solar Observatory GONG archive\n        :param correct: remove nans and negatives\n        :return: tuple of \"halpha\", a fits header, and data object for the GONG image at that time\n            header and data will be None if request failed\n        \"\"\"\n\n        if self.verbose:\n            print(\"Requesting halpha\")\n\n        def time_interval(time):\n            \"\"\" get a window of three minutes around the requested time to ensure an image at GONG cadence\"\"\"\n            return time - timedelta(minutes=3), time + timedelta(minutes=3)\n\n        # setup the query for an halpha image and fetch, saving the image in the current directory\n        client = vso.VSOClient()\n        halpha, source = Quantity(656.28, \"nm\"), \"gong\"\n        query = client.search(vso.attrs.Time(*time_interval(self.date)),\n                              vso.attrs.Source(source),\n                              vso.attrs.Wavelength(halpha))\n\n        if not query: # no images found in query\n            return \"halpha\", None, None\n        else:\n            query = query[0]\n\n        result = client.fetch([query], path=\"./\").wait(progress=False)\n        time.sleep(1) # sleep because on macs there's sometimes a bug where the file isn't recognized if you dont...\n\n        # open the image and remove the file\n        with fits.open(result[0]) as hdu:\n            head = hdu[1].header\n            data = hdu[1].data\n        os.remove(result[0])\n\n        # scale halpha to suvi\n        scale = 2.35\n        tform = AffineTransform(scale=(scale, scale))  # , translation=(-1280/2,-1280/2))\n        data = warp(data, tform, output_shape=(1280, 1280))\n        tform = AffineTransform(\n            translation=(-(640 - 1024 / scale), -(640 - 1024 / scale)))\n        data = warp(data, tform)\n\n        if correct:\n            data[np.isnan(data)] = 0\n            data[data < 0] = 0\n\n        if median_kernel:\n            data = medfilt(data, median_kernel)\n\n        return \"halpha\", head, data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch a specific aia from Virtual Solar Observatory GONG archive", "response": "def fetch_aia(self, product, correct=True, median_kernel=5):\n        \"\"\"\n        pull halpha from that time from Virtual Solar Observatory GONG archive\n        :param product: aia-[REQUESTED CHANNEL IN ANGSTROMS], e.g. aia-131 gets the 131 angstrom image\n        :param correct: remove nans and negatives\n        :return: tuple of product name, fits header, and data object\n            the header and data object will be None if the request failed\n        \"\"\"\n\n        if self.verbose:\n            print(\"Requesting {}\".format(product))\n\n        wavelength = product.split(\"-\")[1]\n\n        def time_interval(time):\n            \"\"\" get a window of three minutes around the requested time to ensure an image at GONG cadence\"\"\"\n            return time - timedelta(minutes=15), time + timedelta(minutes=15)\n\n        # setup the query for an halpha image and fetch, saving the image in the current directory\n        client = vso.VSOClient()\n        wave, source = Quantity(wavelength, \"angstrom\"), \"aia\"\n        query = client.search(vso.attrs.Time(*time_interval(self.date)),\n                              vso.attrs.Instrument(source),\n                              vso.attrs.Wavelength(wave))\n        if self.verbose:\n            print(\"Query length for {} is {}\".format(product, len(query)))\n\n        if not query: # failed to get a result\n            return product, None, None\n        else:\n            query = query[0]\n\n        result = client.fetch([query], path=\"./\").wait(progress=False)\n        time.sleep(1)  # sleep because on macs there's sometimes a bug where the file isn't recognized if you dont...\n\n        # open the image and remove the file\n        with fits.open(result[0]) as hdu:\n            hdu.verify('fix')\n            head = hdu[1].header\n            data = hdu[1].data\n        os.remove(result[0])\n\n        data, head = self.align_solar_fov(head, data, 2.5, 1280, rotate=False)\n        data = resize(data, (1280, 1280))\n        head['NAXIS1'] = 1280\n        head['NAXIS2'] = 1280\n        head['CRPIX1'] = 640\n        head['CRPIX2'] = 640\n        head['CDELT1'] = 2.5\n        head['CDELT2'] = 2.5\n\n        if correct:\n            data[np.isnan(data)] = 0\n            data[data < 0] = 0\n\n        if median_kernel:\n            data = medfilt(data, median_kernel)\n\n        return product, head, data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload the SUVI l1b image into the current directory.", "response": "def fetch_suvi_l1b(self, product, correct=True, median_kernel=5):\n        \"\"\"\n        Given a product keyword, downloads the SUVI l1b image into the current directory.\n        NOTE: the suvi_l1b_url must be properly set for the Fetcher object\n        :param product: the keyword for the product, e.g. suvi-l1b-fe094\n        :param correct: remove nans and negatives\n        :return: tuple of product name, fits header, and data object\n            the header and data object will be None if the request failed\n        \"\"\"\n        if self.date < datetime(2018, 5, 23) and not (self.date >= datetime(2017, 9, 6) \\\n                and self.date <= datetime(2017, 9, 10, 23, 59)):\n            print(\"SUVI data is only available after 2018-5-23\")\n            return product, None, None\n\n        url = self.suvi_base_url + product + \"/{}/{:02d}/{:02d}\".format(self.date.year, self.date.month, self.date.day)\n        if self.verbose:\n            print(\"Requesting from {}\".format(url))\n\n        try:\n            req = urllib.request.Request(url)\n            with urllib.request.urlopen(req) as response:\n                page = response.read()\n        except (URLError, HTTPError):\n            msg = \"The SUVI URL you requested, {}, appears to be unavailable. Check it through a web browser.\"\n            raise RuntimeError(msg.format(url))\n\n        soup = BeautifulSoup(page, 'html.parser')\n        links = [link['href'] for link in soup.find_all('a', href=True)]\n        links = [link for link in links if \"SUVI\" in link]\n        meta = [self.parse_filename_meta(fn) for fn in links if \".fits\" in fn]\n        links = sorted(meta, key=lambda m: np.abs((m[2] - self.date).total_seconds()))[:10]\n        links = [fn for fn, _, _, _, _ in links]\n\n        i = 0\n\n        def download_and_check(i):\n            try:\n                urllib.request.urlretrieve(url + \"/\" + links[i], \"{}.fits\".format(product))\n            except (URLError, HTTPError):\n                msg = \"THE SUVI file you requested, {}, appears to be unvailable. Check if the website is correct.\"\n                raise RuntimeError(msg.format(url + \"/\" + links[i]))\n\n            with fits.open(\"{}.fits\".format(product)) as hdu:\n                head = hdu[0].header\n            return head['exptime'] > 0.5\n\n        while not download_and_check(i):\n            i += 1\n\n        with fits.open(\"{}.fits\".format(product)) as hdu:\n            head = hdu[0].header\n            data = hdu[0].data\n        os.remove(\"{}.fits\".format(product))\n\n        if correct:\n            data[np.isnan(data)] = 0\n            data[data < 0] = 0\n\n        if median_kernel:\n            data = medfilt(data, median_kernel)\n\n        data, head = self.align_solar_fov(head, data, 2.5, 2.0, rotate=True, scale=False)\n        if self.verbose:\n            print(product, \" is using \", head['date-obs'])\n        return product, head, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch_suvi_composite(self, product, correct=True, median_kernel=5):\n        path = os.path.join(self.suvi_composite_path, product,\n                            \"{:4d}/{:02d}/{:02d}/\".format(self.date.year, self.date.month, self.date.day))\n\n        if not os.path.isdir(path):\n            return product, None, None\n        else:  # exists!\n            # find the composite with the closest time code\n            candidate_fns = [fn for fn in os.listdir(path) if \".fits\" in fn\n                             and os.path.getsize(os.path.join(path, fn)) > 0]\n            candidate_fns = sorted([self.parse_filename_meta(fn) for fn in candidate_fns],\n                                   key=lambda entry: abs((self.date - entry[2]).total_seconds()))\n            candidate_fns = [entry[0] for entry in candidate_fns]\n            with fits.open(os.path.join(path, candidate_fns[0])) as hdus:\n                # if the file has the expected number of headers, use it!\n                if len(hdus) == 2 and 'empty' in hdus[1].header and not hdus[1].header['empty']:\n                    head, data = hdus[1].header, hdus[1].data\n                    if correct:\n                        data[np.isnan(data)] = 0\n                        data[data < 0] = 0\n\n                    if median_kernel:\n                        data = medfilt(data, median_kernel)\n                    return product, head, data\n                else:\n                    return product, None, None", "response": "Fetch a suvi composite from a local directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the metadata from a suvi - specific product filename.", "response": "def parse_filename_meta(filename):\n        \"\"\"\n        taken from suvi code by vhsu\n        Parse the metadata from a product filename, either L1b or l2.\n           - file start\n           - file end\n           - platform\n           - product\n\n        :param filename: string filename of product\n        :return: (start datetime, end datetime, platform)\n        \"\"\"\n        common_pattern = \"_%s_%s\" % (\n            \"(?P<product>[a-zA-Z]{3}[a-zA-Z]?-[a-zA-Z0-9]{2}[a-zA-Z0-9]?-[a-zA-Z0-9]{4}[a-zA-Z0-9]?)\",\n            # product l1b, or l2\n            \"(?P<platform>[gG][1-9]{2})\"  # patform, like g16\n        )\n        patterns = {  # all patterns must have the common componennt\n            \"l2_pattern\": re.compile(\"%s_s(?P<start>[0-9]{8}T[0-9]{6})Z_e(?P<end>[0-9]{8}T[0-9]{6})Z\" % common_pattern),\n            \"l1b_pattern\": re.compile('%s_s(?P<start>[0-9]{14})_e(?P<end>[0-9]{14})' % common_pattern),\n            \"dayfile_pattern\": re.compile(\"%s_d(?P<start>[0-9]{8})\" % common_pattern),\n            \"monthfile_pattern\": re.compile(\"%s_m(?P<start>[0-9]{6})\" % common_pattern),\n            \"yearfile_pattern\": re.compile(\"%s_y(?P<start>[0-9]{4})\" % common_pattern),\n        }\n        match, dt_start, dt_end = None, None, None\n        for pat_type, pat in patterns.items():\n            match = pat.search(filename)\n            if match is not None:\n                if pat_type == \"l2_pattern\":\n                    # parse l2\n                    dt_start = datetime.strptime(match.group(\"start\"), '%Y%m%dT%H%M%S')\n                    dt_end = datetime.strptime(match.group(\"end\"), '%Y%m%dT%H%M%S')\n                elif pat_type == \"l1b_pattern\":\n                    # parse l1b\n                    dt_start = datetime.strptime(match.group(\"start\"), '%Y%j%H%M%S%f')\n                    dt_end = datetime.strptime(match.group(\"end\"), '%Y%j%H%M%S%f')\n                elif pat_type == \"dayfile_pattern\":\n                    dt_start = datetime.strptime(match.group(\"start\"), \"%Y%m%d\")\n                    dt_end = dt_start + timedelta(hours=24)\n                elif pat_type == \"monthfile_pattern\":\n                    dt_start = datetime.strptime(match.group(\"start\"), \"%Y%m\")\n                    dt_end = datetime(dt_start.year, dt_start.month + 1,\n                                      1)  # will raise exception in December, fix when needed\n                elif pat_type == \"yearfile_pattern\":\n                    dt_start = datetime.strptime(match.group(\"start\"), \"%Y\")\n                    dt_end = datetime(dt_start.year + 1, 1, 1)\n                break\n\n        if match is None:\n            if \"NCEI\" in filename and \".fits\" in filename:\n                dt_start = datetime.strptime(\"T\".join(filename.split(\"_\")[4:6]), \"%Y%m%dT%H%M%S\")\n                dt_end = dt_start\n                angstroms = int(filename.split(\"_\")[2])\n                atom = \"Fe\" if angstroms != 304 else \"He\"\n                product = \"SUVI-L1b-{}{}\".format(atom, angstroms)\n                return filename, dt_start, dt_end, \"g16\", product\n            else:\n                # we didn't find any matching patterns...\n                raise ValueError(\"Timestamps not detected in filename: %s\" % filename)\n        else:\n            return filename, dt_start, dt_end, match.group(\"platform\"), match.group(\"product\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef align_solar_fov(header, data, cdelt_min, naxis_min,\n                        translate_origin=True, rotate=True, scale=True):\n        \"\"\"\n        taken from suvi code by vhsu\n        Apply field of view image corrections\n\n        :param header: FITS header\n        :param data: Image data\n        :param cdelt_min: Minimum plate scale for images (static run config param)\n        :param naxis_min: Minimum axis dimension for images (static run config param)\n        :param translate_origin: Translate image to specified origin (dtype=bool)\n        :param rotate: Rotate image about origin (dtype=bool)\n        :param scale: Scale image (dtype=bool)\n\n        :rtype: numpy.ndarray\n        :return: data_corr (corrected/aligned image)\n        :rtype: astropy.io.fits.header Header instance\n        :return: upd_meta (updated metadata after image corrections)\n\n        NOTES: (1) The associative property of matrix multiplication makes it possible to multiply\n                   transformation matrices together to produce a single transformation. However, the order\n                   of each transformation matters. In this algorithm, the order is:\n                   1. Translate image center to origin (required)\n                   2. Translate image solar disk center to origin\n                   3. Rotate image about the solar disk center to align with solar spin axis\n                   4. Scale the image so that each pixel is square\n                   5. Translate the image to the image center (required)\n               (2) In python, the image transformations are about the axis origin (0, 0). Therefore, the image point\n                   to rotate about should be shifted to (0, 0) before the rotation.\n               (3) Axis 1 refers to the physical x-axis and axis 2 refers to the physical y-axis, e.g. CRPIX1 is\n                   the center pixel value wrt the x-axis and CRPIX2 is wrt the y-axis.\n        \"\"\"\n        from skimage.transform import ProjectiveTransform\n\n        # Start with 3x3 identity matrix and original header metadata (no corrections)\n        t_matrix = np.identity(3)\n        upd_meta = header\n\n        # (1) Translate the image center to the origin (required transformation)\n\n        # Read in required keywords from header\n        try:\n            img_dim = (header[\"NAXIS1\"], header[\"NAXIS2\"])\n        except KeyError:\n            return None, None\n        else:\n            # Transformation matrix\n            t_matrix = np.matmul(np.array([[1., 0., -(img_dim[0] + 1) / 2.],\n                                           [0., 1., -(img_dim[1] + 1) / 2.],\n                                           [0., 0., 1.]]), t_matrix)\n\n        # (2) Translate image solar disk center to origin\n        if translate_origin:\n            # Read in required keywords from header\n            try:\n                sun_origin = (header[\"CRPIX1\"], header[\"CRPIX2\"])\n            except KeyError:\n                return None, None\n            else:\n                # Transformation matrix\n                t_matrix = np.matmul(np.array([[1., 0., -sun_origin[0] + (img_dim[0] + 1) / 2.],\n                                               [0., 1., -sun_origin[1] + (img_dim[1] + 1) / 2.],\n                                               [0., 0., 1.]]), t_matrix)\n\n                # Update metadata: CRPIX1 and CRPIX2 are at the center of the image\n                upd_meta[\"CRPIX1\"] = (img_dim[0] + 1) / 2.\n                upd_meta[\"CRPIX2\"] = (img_dim[1] + 1) / 2.\n\n        # (3) Rotate image to align with solar spin axis\n        if rotate:\n            # Read in required keywords from header\n            try:\n                PC1_1 = header['PC1_1']\n                PC1_2 = header['PC1_2']\n                PC2_1 = header['PC2_1']\n                PC2_2 = header['PC2_2']\n            except KeyError:\n                try:\n                    CROTA = header['CROTA'] * (np.pi / 180.)  # [rad]\n                    plt_scale = (header[\"CDELT1\"], header[\"CDELT2\"])\n                except KeyError:\n                    return None, None\n                else:\n                    t_matrix = np.matmul(np.array([[np.cos(CROTA), -np.sin(CROTA) * (plt_scale[1] / plt_scale[0]), 0.],\n                                                   [np.sin(CROTA) * (plt_scale[0] / plt_scale[1]), np.cos(CROTA), 0.],\n                                                   [0., 0., 1.]]), t_matrix)\n\n                    # Update metadata: CROTA is zero and PCi_j matrix is the identity matrix\n                    upd_meta[\"CROTA\"] = 0.\n                    upd_meta[\"PC1_1\"] = 1.\n                    upd_meta[\"PC1_2\"] = 0.\n                    upd_meta[\"PC2_1\"] = 0.\n                    upd_meta[\"PC2_2\"] = 1.\n            else:\n                t_matrix = np.matmul(np.array([[PC1_1, PC1_2, 0.],\n                                               [PC2_1, PC2_2, 0.],\n                                               [0., 0., 1.]]), t_matrix)\n\n                # Update metadata: CROTA is zero and PCi_j matrix is the identity matrix\n                upd_meta[\"CROTA\"] = 0.\n                upd_meta[\"PC1_1\"] = 1.\n                upd_meta[\"PC1_2\"] = 0.\n                upd_meta[\"PC2_1\"] = 0.\n                upd_meta[\"PC2_2\"] = 1.\n\n        # (4) Scale the image so that each pixel is square\n        if scale:\n            # Read in required keywords from header\n            try:\n                plt_scale = (header[\"CDELT1\"], header[\"CDELT2\"])\n            except KeyError:\n                return None, None\n            else:\n                # Product of minimum plate scale and axis dimension\n                min_scl = cdelt_min * naxis_min\n\n                # Determine smallest axis\n                naxis_ref = min(img_dim)\n\n                # Transformation matrix\n                t_matrix = np.matmul(np.array([[(plt_scale[0] * naxis_ref) / min_scl, 0., 0.],\n                                               [0., (plt_scale[1] * naxis_ref) / min_scl, 0.],\n                                               [0., 0., 1.]]), t_matrix)\n\n                # Update the metadata: CDELT1 and CDELT2 are scaled by factor to make each pixel square\n                upd_meta[\"CDELT1\"] = plt_scale[0] / ((plt_scale[0] * naxis_ref) / min_scl)\n                upd_meta[\"CDELT2\"] = plt_scale[1] / ((plt_scale[1] * naxis_ref) / min_scl)\n\n        # (5) Translate the image to the image center (required transformation)\n        t_matrix = np.matmul(np.array([[1., 0., (img_dim[0] + 1) / 2.],\n                                       [0., 1., (img_dim[1] + 1) / 2.],\n                                       [0., 0., 1.]]), t_matrix)\n\n        # Transform the image with all specified operations\n        # NOTE: The inverse transformation needs to be applied because the transformation matrix\n        # describes operations on the pixel coordinate frame instead of the image itself. The inverse\n        # transformation will perform the intended operations on the image. Also, any values outside of\n        # the image boundaries are set to zero.\n        data_corr = warp(data, ProjectiveTransform(matrix=t_matrix).inverse, cval=0., preserve_range=True)\n\n        # Check if NaNs are generated from transformation\n        try:\n            assert not np.any(np.isnan(data_corr))\n        except AssertionError:\n            pass\n\n        return data_corr, upd_meta", "response": "Align solar fov with the given header and data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_thematic_png(self, outpath=None):\n        from matplotlib.patches import Patch\n\n        fig, previewax = plt.subplots()\n        shape = self.thmap.shape\n        previewax.imshow(self.thmap,\n                         origin='lower',\n                         interpolation='nearest',\n                         cmap=self.config.solar_cmap,\n                         vmin=-1, vmax=len(self.config.solar_classes)-1)\n\n        legend_elements = [Patch(facecolor=c, label=sc, edgecolor='k')\n                           for sc, c in self.config.solar_colors.items()]\n        previewax.legend(handles=legend_elements, fontsize='x-small',\n                         bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n                         ncol=2, mode=\"expand\", borderaxespad=0.)\n        previewax.set_xlim([0, shape[0]])\n        previewax.set_ylim([0, shape[0]])\n        previewax.set_aspect(\"equal\")\n        previewax.set_axis_off()\n\n        if outpath:\n            fig.savefig(outpath, dpi=300,\n                        transparent=True,\n                        bbox_inches='tight',\n                        pad_inches=0.)\n            plt.close()\n        else:\n            plt.show()", "response": "Convert a thematic map into png format with a legend"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_three_color(self, upper_percentile=100, lower_percentile=0):\n        order = {'red': 0, 'green': 1, 'blue': 2}\n        shape = self.thmap.shape\n        three_color = np.zeros((shape[0], shape[1], 3))\n        channel_colors = {color: self.config.default[color] for color in ['red', 'green', 'blue']}\n\n        data = Fetcher(self.date, products=list(channel_colors.values()), verbose=False).fetch()\n        for color, channel in channel_colors.items():\n            three_color[:, :, order[color]] = data[channel][1]\n\n            # scale the image by the power\n            three_color[:, :, order[color]] = np.power(three_color[:, :, order[color]],\n                                                       self.config.default[\"{}_power\".format(color)])\n\n            # adjust the percentile thresholds\n            lower = np.nanpercentile(three_color[:, :, order[color]], lower_percentile)\n            upper = np.nanpercentile(three_color[:, :, order[color]], upper_percentile)\n            three_color[np.where(three_color[:, :, order[color]] < lower)] = lower\n            three_color[np.where(three_color[:, :, order[color]] > upper)] = upper\n\n        # image values must be between (0,1) so scale image\n        for color, index in order.items():\n            three_color[:, :, index] /= np.nanmax(three_color[:, :, index])\n        return three_color", "response": "Load the configured input channel images and create a three - color image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a png image of the thamatic map with a three color beside it AttributeNames", "response": "def make_comparison_png(self, outpath=None, include_legend=False):\n        \"\"\"\n        Creates a thematic map image with a three color beside it\n        :param outpath:  if specified, will save the image instead of showing it\n        :param include_legend: if true will include the thamatic map label legend\n        \"\"\"\n        from matplotlib.patches import Patch\n\n        fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True)\n\n        three_color = self.make_three_color()\n        axs[0].imshow(three_color)\n        axs[0].set_axis_off()\n\n        shape = self.thmap.shape\n        axs[1].imshow(self.thmap,\n                     origin='lower',\n                     interpolation='nearest',\n                     cmap=self.config.solar_cmap,\n                     vmin=-1, vmax=len(self.config.solar_classes)-1)\n\n        if include_legend:\n            legend_elements = [Patch(facecolor=c, label=sc, edgecolor='k')\n                               for sc, c in self.config.solar_colors.items()]\n            axs[1].legend(handles=legend_elements, fontsize='x-small',\n                         bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n                         ncol=2, mode=\"expand\", borderaxespad=0.)\n        axs[1].set_xlim([0, shape[0]])\n        axs[1].set_ylim([0, shape[0]])\n        axs[1].set_aspect(\"equal\")\n        axs[1].set_axis_off()\n\n        if outpath:\n            fig.savefig(outpath, dpi=300,\n                        transparent=True,\n                        bbox_inches='tight',\n                        pad_inches=0.)\n            plt.close()\n        else:\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the current state of the image to the FITS file", "response": "def save(self):\n        \"\"\" modified from suvi code by vhsu \"\"\"\n        pri_hdu = fits.PrimaryHDU(data=self.thmap)\n\n        # Temporal Information\n        date_fmt = '%Y-%m-%dT%H:%M:%S.%f'\n        date_beg = self.start_time.strftime(date_fmt)\n        date_end = self.end_time.strftime(date_fmt)\n        date_now = datetime.utcnow().strftime(date_fmt)\n        self.set_fits_header(\"TIMESYS\", self.ref_hdr, pri_hdu)\n        pri_hdu.header.append((\"DATE-BEG\", date_beg, \"sun observation start time on sat\"))\n        pri_hdu.header.append((\"DATE-END\", date_end, \"sun observation end time on sat\"))\n        pri_hdu.header.append((\"DATE\", date_now, \"file generation time\"))\n        pri_hdu.header.append((\"EXPERT\", self.config.expert, \"person who labeled image\"))\n        pri_hdu.header.append((\"DATE-LAB\", date_now, \"date of labeling for the image\"))\n\n        # Instrument & Spacecraft State during Observation\n        pri_hdu.header.append((\"EXPTIME\", 1., \"[s] effective imaging exposure time\"))\n        self.set_fits_header(\"YAW_FLIP\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"ECLIPSE\", self.ref_hdr, pri_hdu)\n\n        # Pointing & Projection\n        self.set_fits_header(\"WCSNAME\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CTYPE1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CTYPE2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CUNIT1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CUNIT2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"PC1_1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"PC1_2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"PC2_1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"PC2_2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CDELT1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CDELT2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CRVAL1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CRVAL2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CRPIX1\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CRPIX2\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"DIAM_SUN\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"LONPOLE\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"CROTA\", self.ref_hdr, pri_hdu)\n        self.set_fits_header(\"SOLAR_B0\", self.ref_hdr, pri_hdu)\n\n        # File Provenance\n        pri_hdu.header.append((\"TITLE\", \"Expert Labeled Thematic Map Image\", \"image title\"))\n        pri_hdu.header.append((\"MAP_MTHD\", \"human\", \"thematic map classifier method\"))\n        try:\n            # Add COMMENT cards\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 1,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 2, (\"COMMENT\", 'USING SUVI THEMATIC MAP FILES'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 3,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 4,\n                                  (\"COMMENT\", 'Map labels are described in the FITS extension.'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 5, (\"COMMENT\", 'Example:'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 6, (\"COMMENT\", 'from astropy.io import fits as pyfits'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 7, (\"COMMENT\", 'img = pyfits.open(<filename>)'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 8, (\"COMMENT\", 'map_labels = img[1].data'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 9,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 10, (\"COMMENT\", 'TEMPORAL INFORMATION'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"TITLE\") + 11,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"DATE\") + 1,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"DATE\") + 2,\n                                  (\"COMMENT\", 'INSTRUMENT & SPACECRAFT STATE DURING OBSERVATION'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"DATE\") + 3,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"ECLIPSE\") + 1,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"ECLIPSE\") + 2, (\"COMMENT\", 'POINTING & PROJECTION'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"ECLIPSE\") + 3,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"SOLAR_B0\") + 1,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"SOLAR_B0\") + 2, (\"COMMENT\", 'FILE PROVENANCE'))\n            pri_hdu.header.insert(pri_hdu.header.index(\"SOLAR_B0\") + 3,\n                                  (\"COMMENT\", '------------------------------------------------------------------------'))\n        except:\n            print(\"This thematic map may be degraded and missing many keywords.\")\n\n        # Thematic map feature list (Secondary HDU extension)\n        map_val = []\n        map_label = []\n        for key, value in self.config.solar_class_index.items(): #sorted(SOLAR_CLASS_INDEX.items(), key=lambda p: (lambda k, v: (v, k))):\n            map_label.append(key)\n            map_val.append(value)\n        c1 = fits.Column(name=\"Thematic Map Value\", format=\"B\", array=np.array(map_val))\n        c2 = fits.Column(name=\"Feature Name\", format=\"22A\", array=np.array(map_label))\n        bintbl_hdr = fits.Header([(\"XTENSION\", \"BINTABLE\")])\n        sec_hdu = fits.BinTableHDU.from_columns([c1, c2], header=bintbl_hdr)\n\n        # Output thematic map as the primary HDU and the list of map features as an extension BinTable HDU\n        hdu = fits.HDUList([pri_hdu, sec_hdu])\n        hdu.writeto(self.filename, overwrite=True, checksum=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a request mock up that is used in to render the input template and search for the placeholder within the most fidel environement as possible.", "response": "def get_request_mock():\n    \"\"\"Build a ``request`` mock up that is used in to render\n    the templates in the most fidel environement as possible.\n\n    This fonction is used in the get_placeholders method to\n    render the input template and search for the placeholder\n    within.\n    \"\"\"\n    basehandler = BaseHandler()\n    basehandler.load_middleware()\n    # http://www.python.org/dev/peps/pep-0333/\n    request = WSGIRequest({\n        'HTTP_COOKIE': '',\n        'PATH_INFO': '/',\n        'QUERY_STRING': '',\n        'REMOTE_ADDR': '127.0.0.1',\n        'REQUEST_METHOD': 'GET',\n        'SERVER_NAME': 'page-request-mock',\n        'SCRIPT_NAME': '',\n        'SERVER_PORT': '80',\n        'SERVER_PROTOCOL': 'HTTP/1.1',\n        'HTTP_HOST': 'page-request-host',\n        'CONTENT_TYPE': 'text/html; charset=utf-8',\n        'wsgi.version': (1, 0),\n        'wsgi.url_scheme': 'http',\n        'wsgi.multiprocess': True,\n        'wsgi.multithread': False,\n        'wsgi.run_once': False,\n        'wsgi.input': StringIO()\n    })\n    # Apply request middleware\n    for middleware_method in basehandler._request_middleware:\n        # LocaleMiddleware should never be applied a second time because\n        # it would broke the current real request language\n        if 'LocaleMiddleware' not in str(middleware_method.__class__):\n            middleware_method(request)\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pages_view(view):\n    def pages_view_decorator(request, *args, **kwargs):\n        # if the current page is already there\n        if(kwargs.get('current_page', False) or\n                kwargs.get('pages_navigation', False)):\n            return view(request, *args, **kwargs)\n\n        path = kwargs.pop('path', None)\n        lang = kwargs.pop('lang', None)\n        if path:\n            from basic_cms.views import details\n            response = details(request, path=path, lang=lang,\n                only_context=True, delegation=False)\n            context = response\n            extra_context_var = kwargs.pop('extra_context_var', None)\n            if extra_context_var:\n                kwargs.update({extra_context_var: context})\n            else:\n                kwargs.update(context)\n        return view(request, *args, **kwargs)\n    return pages_view_decorator", "response": "Decorator for views that returns the essential pages\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_template_from_request(request, page=None):\n    page_templates = settings.get_page_templates()\n    if len(page_templates) == 0:\n        return settings.PAGE_DEFAULT_TEMPLATE\n    template = request.REQUEST.get('template', None)\n    if template is not None and \\\n            (template in dict(page_templates).keys() or\n            template == settings.PAGE_DEFAULT_TEMPLATE):\n        return template\n    if page is not None:\n        return page.get_template()\n    return settings.PAGE_DEFAULT_TEMPLATE", "response": "Gets a valid template from different sources or falls back to the PAGE_DEFAULT_TEMPLATE if no template is set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef precision(self):\n        true_pos = self.matrix[0][0]\n        false_pos = self.matrix[1][0]\n        return divide(1.0 * true_pos, true_pos + false_pos)", "response": "Calculates precision of the matrix\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating recall of the current set of cache entries", "response": "def recall(self):\n        \"\"\"Calculates recall\n\n        :return: Recall\n        \"\"\"\n        true_pos = self.matrix[0][0]\n        false_neg = self.matrix[0][1]\n        return divide(1.0 * true_pos, true_pos + false_neg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef true_neg_rate(self):\n        false_pos = self.matrix[1][0]\n        true_neg = self.matrix[1][1]\n        return divide(1.0 * true_neg, true_neg + false_pos)", "response": "Calculates true negative rate"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the accuracy of the current set of cache entries.", "response": "def accuracy(self):\n        \"\"\"Calculates accuracy\n\n        :return: Accuracy\n        \"\"\"\n        true_pos = self.matrix[0][0]\n        false_pos = self.matrix[1][0]\n        false_neg = self.matrix[0][1]\n        true_neg = self.matrix[1][1]\n\n        num = 1.0 * (true_pos + true_neg)\n        den = true_pos + true_neg + false_pos + false_neg\n\n        return divide(num, den)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates F1 score of the current log entry", "response": "def f1_score(self):\n        \"\"\"Calculates F1 score\n\n        :return: F1 score\n        \"\"\"\n        m_pre = self.precision()\n        rec = self.recall()\n        return divide(2.0, 1.0 / m_pre + 1.0 / rec)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode matrix into a label encoder", "response": "def encode(self):\n        \"\"\"Encodes matrix\n\n        :return: Encoder used\n        \"\"\"\n        encoder = LabelEncoder()  # encoder\n        values = self.get_as_list()\n        encoded = encoder.fit_transform(values)  # long list of encoded\n        n_columns = len(self.matrix[0])\n        n_rows = len(self.matrix)\n\n        self.matrix = [\n            encoded[i: i + n_columns]\n            for i in range(0, n_rows * n_columns, n_columns)\n        ]\n\n        return encoder"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding matrix from encoder.", "response": "def decode(self, encoder):\n        \"\"\"Decodes matrix\n\n        :param encoder: Encoder used to encode matrix\n        :return: list: Decodes matrix\n        \"\"\"\n        self.matrix = [\n            encoder.inverse_transform(row)\n            for row in self.matrix\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_columns(columns):\n        data = [\n            [\n                column[i]\n                for i in range(len(column))\n            ]\n            for column in columns\n        ]\n        return Matrix(data)", "response": "Parses the columns into a matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses version file and returns dict of all the details", "response": "def get_version_details(path):\n    \"\"\"Parses version file\n\n    :param path: path to version file\n    :return: version details\n    \"\"\"\n\n    with open(path, \"r\") as reader:\n        lines = reader.readlines()\n        data = {\n            line.split(\" = \")[0].replace(\"__\", \"\"):\n                line.split(\" = \")[1].strip().replace(\"'\", \"\")\n            for line in lines\n        }\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the id of a record if it exists.", "response": "def _get_record(self, name):\n        \"\"\"Returns the id of a record, if it exists.\"\"\"\n        request = self._session.get(self._baseurl, params={'name': name,\n                                                           'type': 'A'})\n        if not request.ok:\n            raise RuntimeError('Failed to search record: %s - %s' %\n                               (self._format_hostname(name), request.json()))\n        records = request.json()\n        if len(records) == 0:\n            return\n        record = records[0]\n        if 'record' not in record or 'id' not in record['record']:\n            raise RuntimeError('Invalid record JSON format: %s - %s' %\n                               (self._format_hostname(name), request.json()))\n        return int(record['record']['id'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_record(self, name, address, ttl):\n        data = json.dumps({'record': {'name': name,\n                                      'record_type': 'A',\n                                      'content': address,\n                                      'ttl': ttl}})\n        headers = {'Content-Type': 'application/json'}\n        request = self._session.post(self._baseurl, data=data, headers=headers)\n        if not request.ok:\n            raise RuntimeError('Failed to create new record: %s - %s' %\n                               (self._format_hostname(name), request.json()))\n        record = request.json()\n        if 'record' not in record or 'id' not in record['record']:\n            raise RuntimeError('Invalid record JSON format: %s - %s' %\n                               (self._format_hostname(name), request.json()))\n        return record['record']", "response": "Creates a new record."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates an existing record.", "response": "def _update_record(self, record_id, name, address, ttl):\n        \"\"\"Updates an existing record.\"\"\"\n        data = json.dumps({'record': {'name': name,\n                                      'content': address,\n                                      'ttl': ttl}})\n        headers = {'Content-Type': 'application/json'}\n        request = self._session.put(self._baseurl + '/%d' % record_id,\n                                    data=data, headers=headers)\n        if not request.ok:\n            raise RuntimeError('Failed to update record: %s - %s' %\n                               (self._format_hostname(name), request.json()))\n        record = request.json()\n        if 'record' not in record or 'id' not in record['record']:\n            raise RuntimeError('Invalid record JSON format: %s - %s' %\n                               (self._format_hostname(name), request.json()))\n        return record['record']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a record. Creates it if it does not exist.", "response": "def update_record(self, name, address, ttl=60):\n        \"\"\"Updates a record, creating it if not exists.\"\"\"\n        record_id = self._get_record(name)\n        if record_id is None:\n            return self._create_record(name, address, ttl)\n        return self._update_record(record_id, name, address, ttl)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the object with the given text and size.", "response": "def build(self, text, size=1):\n        \"\"\"\n        :param text: Text of the widget\n        :param size: Size of the text (Higher size = smaller title)\n        \"\"\"\n        super(Title, self).build()\n        self.content = text\n        self.size = size"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the paragraph with the given text.", "response": "def build(self, text):\n        \"\"\"\n        :param text: Content of the paragraph\n        \"\"\"\n        super(Paragraph, self).build()\n        self.content = text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the span from the given text.", "response": "def build(self, text):\n        \"\"\"\n        :param text: Content of the span\n        \"\"\"\n        super(Span, self).build()\n        self.content = text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self, text, url):\n        super(TextLink, self).build()\n        self.target = url\n        self.content = text", "response": ":param text: Text of the link\n        :param url: Target URL"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setZeroWheel(self):\n        self._zeroWheel = True\n        # want padding in this case\n        self.menu.viewAll.triggered.disconnect()\n        self.menu.viewAll.triggered.connect(self.autoRange)", "response": "Sets the zoom locus of the mouse wheel to the point 0 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncustomizes mouse dragging where the right drag is bounding box zoom - class name of the class that is used to set the mouse state of the object", "response": "def mouseDragEvent(self, ev, axis=None):\n        \"\"\"Customized mouse dragging, where the right drag is bounding box zoom\n        \n        :param ev: event object containing drag state info\n        :type ev: :py:class:`MouseDragEvent<pyqtgraph:pyqtgraph.GraphicsScene.mouseEvents.MouseDragEvent>`\n        \"\"\"\n        if self._customMouse and ev.button() == QtCore.Qt.RightButton:\n            ev.accept()  ## we accept all buttons\n\n            # directly copy-pasted from ViewBox for ViewBox.RectMode\n            if ev.isFinish():  ## This is the final move in the drag; change the view scale now\n                #print \"finish\"\n                pos = ev.pos()\n\n                self.rbScaleBox.hide()\n                #ax = QtCore.QRectF(Point(self.pressPos), Point(self.mousePos))\n                ax = QtCore.QRectF(Point(ev.buttonDownPos(ev.button())), Point(pos))\n                ax = self.childGroup.mapRectFromParent(ax)\n                self.showAxRect(ax)\n                self.axHistoryPointer += 1\n                self.axHistory = self.axHistory[:self.axHistoryPointer] + [ax]\n            else:\n                ## update shape of scale box\n                self.updateScaleBox(ev.buttonDownPos(), ev.pos())\n        else:\n            state = None\n            # ctrl reverses mouse operation axis\n            if ev.modifiers() == QtCore.Qt.ControlModifier:\n                state = self.mouseEnabled()\n                self.setMouseEnabled(not state[0], not state[1])\n            super(SpikeyViewBox, self).mouseDragEvent(ev, axis)\n            if state is not None:\n                self.setMouseEnabled(*state)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wheelEvent(self, ev, axis=None):\n        state = None\n        # ctrl reverses mouse operation axis\n        if ev.modifiers() == QtCore.Qt.ControlModifier:\n            state = self.mouseEnabled()\n            self.setMouseEnabled(not state[0], not state[1])\n        if self._zeroWheel:\n            ev.pos = lambda : self.mapViewToScene(QtCore.QPoint(0,0))\n        super(SpikeyViewBox, self).wheelEvent(ev, axis)\n        if state is not None:\n            self.setMouseEnabled(*state)", "response": "Reacts to mouse wheel movement custom behaviour switches zoom\n        axis when ctrl is pressed and sets the locus of zoom when zeroWheel is set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of this menu.", "response": "def copy(self):\n        \"\"\"Adds menus to itself, required by ViewBox\"\"\"\n        # copied from pyqtgraph ViewBoxMenu\n        m = QtGui.QMenu()\n        for sm in self.subMenus():\n            if isinstance(sm, QtGui.QMenu):\n                m.addMenu(sm)\n            else:\n                m.addAction(sm)\n        m.setTitle(self.title())\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect k best features in dataset", "response": "def select_k_best(self, k):\n        \"\"\"Selects k best features in dataset\n\n        :param k: features to select\n        :return: k best features\n        \"\"\"\n        x_new = SelectKBest(chi2, k=k).fit_transform(self.x_train, self.y_train)\n        return x_new"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_best(self):\n        svc = SVC(kernel=\"linear\")\n        rfecv = RFECV(\n            estimator=svc,\n            step=1,\n            cv=StratifiedKFold(self.y_train, 2),\n            scoring=\"log_loss\"\n        )\n        rfecv.fit(self.x_train, self.y_train)\n        return rfecv.n_features_, rfecv.ranking_", "response": "Finds the optimal number of features and ranking\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the methods in the correct order", "response": "def main(self):\n        \"\"\"\n        Run the methods in the correct order\n        \"\"\"\n        logging.info('Aligning reads with bowtie2 for Qualimap')\n        self.bowtie()\n        self.indexing()\n        self.pilon()\n        self.filter()\n        self.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bowtie(self):\n        for i in range(self.cpus):\n            # Send the threads to the merge method. :args is empty as I'm using\n            threads = Thread(target=self.align, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # Initialise the mapping GenObject\n                sample.mapping = GenObject()\n                # Set an easier to write shortcut for sample.general\n                sagen = sample.general\n                if sagen.bestassemblyfile != \"NA\":\n                    sagen.QualimapResults = os.path.join(sagen.outputdirectory, 'qualimap_results')\n                    # Set the results folder\n                    # Create this results folder if necessary\n                    make_path(sagen.QualimapResults)\n                    # Set file names\n                    sagen.sortedbam = os.path.join(sagen.QualimapResults, '{}_sorted.bam'.format(sample.name))\n                    filenoext = os.path.splitext(sagen.filteredfile)[0]\n                    sagen.filenoext = filenoext\n                    sagen.bowtie2results = os.path.join(sagen.QualimapResults, sample.name)\n                    # Use fancy new bowtie2 wrapper\n                    bowtie2build = Bowtie2BuildCommandLine(reference=sagen.bestassemblyfile,\n                                                           bt2=sagen.bowtie2results)\n                    sample.mapping.BamFile = sagen.bowtie2results + \"_sorted.bam\"\n                    # SAMtools sort v1.3 has different run parameters\n                    samsort = SamtoolsSortCommandline(input=sample.mapping.BamFile,\n                                                      o=True,\n                                                      out_prefix=\"-\")\n                    samtools = [SamtoolsViewCommandline(b=True, S=True, input_file=\"-\"), samsort]\n                    indict = {'D': 5, 'R': 1, 'num_mismatches': 0, 'seed_length': 22, 'i_func': \"S,0,2.50\"}\n                    #  Update the dictionary with the appropriate parameters for paired- vs. single-ended assemblies\n                    try:\n                        _ = sample.general.mergedreads\n                        if len(sample.general.trimmedcorrectedfastqfiles) == 2:\n                            indict.update({'m1': sample.general.trimmedcorrectedfastqfiles[0],\n                                           'm2': sample.general.trimmedcorrectedfastqfiles[1]})\n                        else:\n                            indict.update({'U': sample.general.trimmedcorrectedfastqfiles[0]})\n                    except AttributeError:\n                        if len(sample.general.assemblyfastq) == 2:\n                            indict.update({'m1': sample.general.assemblyfastq[0],\n                                           'm2': sample.general.assemblyfastq[1]})\n                        else:\n                            indict.update({'U': sample.general.assemblyfastq[0]})\n                    bowtie2align = Bowtie2CommandLine(bt2=sagen.bowtie2results,\n                                                      threads=self.threads,\n                                                      samtools=samtools,\n                                                      **indict)\n\n                    # Convert the commands to strings to allow them to be JSON serialized\n                    sample.commands.bowtie2align = str(bowtie2align)\n                    sample.commands.bowtie2build = str(bowtie2build)\n                    self.bowqueue.put((sample, sample.commands.bowtie2build, sample.commands.bowtie2align))\n                else:\n                    sample.commands.samtools = \"NA\"\n                    sample.mapping.MeanInsertSize = 'NA'\n                    sample.mapping.MeanCoveragedata = 'NA'\n        self.bowqueue.join()", "response": "Create threads and commands for performing reference mapping for qualimap analyses"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun Qualimap and parse the output of the qualimap call", "response": "def mapper(self, sample):\n        \"\"\"\n        Run qualimap and parse the outputs\n        :param sample: metadata object\n        \"\"\"\n        if sample.general.bestassemblyfile != \"NA\":\n            # Define the Qualimap log and report files\n            reportfile = os.path.join(sample.general.QualimapResults, 'genome_results.txt')\n            # Define the Qualimap call\n            qualimapcall = 'qualimap bamqc -bam {} -outdir {}'.format(sample.general.sortedbam,\n                                                                      sample.general.QualimapResults)\n            sample.commands.qualimap = qualimapcall\n            # Initialise a dictionary to hold the Qualimap results\n            qdict = dict()\n            # If the report file doesn't exist, run Qualimap, and print logs to the log file\n            if not os.path.isfile(reportfile):\n                tlock = threading.Lock()\n                out, err = run_subprocess(sample.commands.qualimap)\n                tlock.acquire()\n                write_to_logfile(sample.commands.qualimap, sample.commands.qualimap, self.logfile,\n                                 sample.general.logout, sample.general.logerr, None, None)\n                write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n                tlock.release()\n            # Initialise a genobject to store the coverage dictionaries\n            sample.depth = GenObject()\n            sample.depth.length = dict()\n            sample.depth.bases = dict()\n            sample.depth.coverage = dict()\n            sample.depth.stddev = dict()\n            try:\n                with open(reportfile) as report:\n                    # Read the report\n                    for line in report:\n                        # Sanitise the keys and values using self.analyze\n                        key, value = self.analyze(line)\n                        # If the keys and values exist, enter them into the dictionary\n                        if (key, value) != (None, None):\n                            qdict[key] = value\n                        if 'Coverage per contig' in line:\n                            for contigline in report:\n                                try:\n                                    _, name, length, bases, coverage, stddev = contigline.rstrip().split('\\t')\n                                    sample.depth.length.update({name: length})\n                                    sample.depth.bases.update({name: bases})\n                                    sample.depth.coverage.update({name: coverage})\n                                    sample.depth.stddev.update({name: stddev})\n                                except ValueError:\n                                    pass\n\n            except (IOError, FileNotFoundError):\n                pass\n            # If there are values in the dictionary\n            if qdict:\n                # Make new category for Qualimap results and populate this category with the report data\n                for attribute in qdict:\n                    # Remove the 'X' from the depth values e.g. 40.238X\n                    setattr(sample.mapping, attribute, qdict[attribute].rstrip('X'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning pilon to fix misassemblies in the contigs", "response": "def pilon(self):\n        \"\"\"\n        Run pilon to fix any misassemblies in the contigs - will look for SNPs and indels\n        \"\"\"\n        logging.info('Improving quality of assembly with pilon')\n        for i in range(self.cpus):\n            # Send the threads to the merge method. :args is empty as I'm using\n            threads = Thread(target=self.pilonthreads, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                if sample.general.bestassemblyfile != 'NA':\n                    if sample.general.polish:\n                        # Set the name of the unfiltered assembly output file\n                        sample.general.contigsfile = sample.general.assemblyfile\n                        sample.mapping.pilondir = os.path.join(sample.general.QualimapResults, 'pilon')\n                        make_path(sample.mapping.pilondir)\n                        # Create the command line command\n                        sample.mapping.piloncmd = 'pilon --genome {} --bam {} --fix bases --threads {} ' \\\n                                                  '--outdir {} --changes --mindepth 0.25' \\\n                            .format(sample.general.contigsfile,\n                                    sample.mapping.BamFile,\n                                    self.threads,\n                                    sample.mapping.pilondir)\n                        self.pilonqueue.put(sample)\n                    else:\n                        sample.general.contigsfile = sample.general.assemblyfile\n        self.pilonqueue.join()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter(self):\n\n        logging.info('Filtering contigs')\n        for i in range(self.cpus):\n            # Send the threads to the filter method\n            threads = Thread(target=self.filterthreads, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # Set the name of the unfiltered assembly output file\n                if sample.general.bestassemblyfile != 'NA':\n                    sample.general.contigsfile = sample.general.assemblyfile\n                    self.filterqueue.put(sample)\n        self.filterqueue.join()", "response": "Filter contigs based on depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear(self):\n        for sample in self.metadata:\n            try:\n                delattr(sample.depth, 'bases')\n                delattr(sample.depth, 'coverage')\n                delattr(sample.depth, 'length')\n                delattr(sample.depth, 'stddev')\n            except AttributeError:\n                pass", "response": "Clear out large attributes from the metadata objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a JSON string export of the pages in queryset.", "response": "def pages_to_json(queryset):\n    \"\"\"\n    Return a JSON string export of the pages in queryset.\n    \"\"\"\n    # selection may be in the wrong order, and order matters\n    queryset = queryset.order_by('tree_id', 'lft')\n    return simplejson.dumps(\n        {JSON_PAGE_EXPORT_NAME: JSON_PAGE_EXPORT_VERSION,\n            'pages': [page.dump_json_data() for page in queryset]},\n        indent=JSON_PAGE_EXPORT_INDENT, sort_keys=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json_to_pages(json, user, preferred_lang=None):\n    from .models import Page\n    if not preferred_lang:\n        preferred_lang = settings.PAGE_DEFAULT_LANGUAGE\n\n    d = simplejson.loads(json)\n    try:\n        errors = validate_pages_json_data(d, preferred_lang)\n    except KeyError as e:\n        errors = [_('JSON file is invalid: %s') % (e.args[0],)]\n\n    pages_created = []\n    if not errors:\n        # pass one\n        for p in d['pages']:\n            pages_created.append(\n                Page.objects.create_and_update_from_json_data(p, user))\n        # pass two\n        for p, results in zip(d['pages'], pages_created):\n            page, created, messages = results\n            rtcs = p['redirect_to_complete_slug']\n            if rtcs:\n                messages.extend(page.update_redirect_to_from_json(rtcs))\n        # clean up MPTT links\n        #Page.objects.rebuild()\n\n    return errors, pages_created", "response": "Create and update pages from a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_pages_json_data(d, preferred_lang):\n    from .models import Page\n    errors = []\n\n    seen_complete_slugs = dict(\n        (lang[0], set()) for lang in settings.PAGE_LANGUAGES)\n\n    valid_templates = set(t[0] for t in settings.get_page_templates())\n    valid_templates.add(settings.PAGE_DEFAULT_TEMPLATE)\n\n    if d[JSON_PAGE_EXPORT_NAME] != JSON_PAGE_EXPORT_VERSION:\n        return [_('Unsupported file version: %s') % repr(\n            d[JSON_PAGE_EXPORT_NAME])], []\n    pages = d['pages']\n    for p in pages:\n        # use the complete slug as a way to identify pages in errors\n        slug = p['complete_slug'].get(preferred_lang, None)\n        seen_parent = False\n        for lang, s in p['complete_slug'].items():\n            if lang not in seen_complete_slugs:\n                continue\n            seen_complete_slugs[lang].add(s)\n\n            if '/' not in s: # root level, no parent req'd\n                seen_parent = True\n            if not seen_parent:\n                parent_slug, ignore = s.rsplit('/', 1)\n                if parent_slug in seen_complete_slugs[lang]:\n                    seen_parent = True\n                else:\n                    parent = Page.objects.from_path(parent_slug, lang,\n                        exclude_drafts=False)\n                    if parent and parent.get_complete_slug(lang) == parent_slug:\n                        # parent not included, but exists on site\n                        seen_parent = True\n            if not slug:\n                slug = s\n\n        if not slug:\n            errors.append(_(\"%s has no common language with this site\")\n                % (p['complete_slug'].values()[0],))\n            continue\n\n        if not seen_parent:\n            errors.append(_(\"%s did not include its parent page and a matching\"\n                \" one was not found on this site\") % (slug,))\n\n        if p['template'] not in valid_templates:\n            errors.append(_(\"%s uses a template not found on this site: %s\")\n                % (slug, p['template']))\n            continue\n\n        import_fields = set(p['content'].keys())\n        import_fields |= set(('meta_title', 'meta_description', 'meta_keywords', 'meta_author', 'fb_page_type', 'fb_image'))\n        template_fields = set(p.name for p in get_placeholders(p['template']) if\n                p.name not in ('title', 'slug'))\n        template_fields |= set(('meta_title', 'meta_description', 'meta_keywords', 'meta_author', 'fb_page_type', 'fb_image'))\n        if template_fields != import_fields:\n            errors.append(_(\"%s template contents are different than our \"\n                \"template: %s\") % (slug, p['template']))\n            continue\n\n    return errors", "response": "Validate the pages JSON data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_placeholders(template_name):\n    try:\n        temp = loader.get_template(template_name)\n    except TemplateDoesNotExist:\n        return []\n\n    plist, blist = [], []\n    try:\n        # django 1.8\n        _placeholders_recursif(temp.template.nodelist, plist, blist)\n    except AttributeError:\n        # django 1.7\n        # raise\n        _placeholders_recursif(temp.nodelist, plist, blist)\n    return plist", "response": "Return a list of PlaceholderNode found in the given template."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_po_files(path='poexport', stdout=None):\n    if stdout is None:\n        import sys\n        stdout = sys.stdout\n    if not path.endswith('/'):\n        path += '/'\n    import polib\n    import os\n    from basic_cms.models import Page, Content\n    source_language = settings.PAGE_DEFAULT_LANGUAGE\n    source_list = []\n    for page in Page.objects.published():\n        source_list.extend(page.content_by_language(source_language))\n\n    for lang in settings.PAGE_LANGUAGES:\n        if lang[0] != settings.PAGE_DEFAULT_LANGUAGE:\n            try:\n                os.mkdir(path)\n            except OSError:\n                pass\n            po_path = path + lang[0] + '.po'\n            stdout.write(\"Export language %s.\\n\" % lang[0])\n            po = polib.pofile(po_path)\n            po.metadata['Content-Type'] = 'text/plain; charset=utf-8'\n\n            for source_content in source_list:\n                page = source_content.page\n                try:\n                    target_content = Content.objects.get_content_object(\n                        page, lang[0], source_content.type)\n                    msgstr = target_content.body\n                except Content.DoesNotExist:\n                    target_content = None\n                    msgstr = \"\"\n                if source_content.body:\n                    if target_content:\n                        tc_id = str(target_content.id)\n                    else:\n                        tc_id = \"\"\n                    entry = polib.POEntry(msgid=source_content.body,\n                        msgstr=msgstr)\n                    entry.tcomment = po_comment % (page.title(), do_not_msg,\n                        source_content.type, page.id, tc_id)\n                    if entry not in po:\n                        po.append(entry)\n            po.save(po_path)\n    stdout.write(\"\"\"Export finished. The files are available \"\"\"\n        \"\"\"in the %s directory.\\n\"\"\" % path)", "response": "Export all the content from the published pages into a PO file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport all the content updates from the po files into the pages.", "response": "def import_po_files(path='poexport', stdout=None):\n    \"\"\"\n    Import all the content updates from the po files into\n    the pages.\n    \"\"\"\n    import polib\n    from basic_cms.models import Page, Content\n    source_language = settings.PAGE_DEFAULT_LANGUAGE\n    source_list = []\n    pages_to_invalidate = []\n    for page in Page.objects.published():\n        source_list.extend(page.content_by_language(source_language))\n    if stdout is None:\n        import sys\n        stdout = sys.stdout\n    if not path.endswith('/'):\n        path += '/'\n\n    for lang in settings.PAGE_LANGUAGES:\n        if lang[0] != settings.PAGE_DEFAULT_LANGUAGE:\n            stdout.write(\"Update language %s.\\n\" % lang[0])\n            po_path = path + lang[0] + '.po'\n            po = polib.pofile(po_path)\n            for entry in po:\n                meta_data = entry.tcomment.split(do_not_msg)[1].split(\"\\n\")\n                placeholder_name = meta_data[1].split('=')[1]\n                page_id = int(meta_data[2].split('=')[1])\n\n                page = Page.objects.get(id=page_id)\n                current_content = Content.objects.get_content(page, lang[0],\n                    placeholder_name)\n                if current_content != entry.msgstr:\n                    stdout.write(\"Update page %d placeholder %s.\\n\" % (page_id,\n                        placeholder_name))\n                    Content.objects.create_content_if_changed(\n                        page, lang[0], placeholder_name, entry.msgstr)\n                    if page not in pages_to_invalidate:\n                        pages_to_invalidate.append(page)\n\n    for page in pages_to_invalidate:\n        page.invalidate()\n    stdout.write(\"Import finished from %s.\\n\" % path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the reference voltage of the current instance of the class.", "response": "def setReferenceVoltage(self, caldb, calv):\n        \"\"\"See :meth:`StimulusModel<sparkle.stim.stimulus_model.StimulusModel.setReferenceVoltage>`\"\"\"\n        self.caldb = caldb\n        self.calv = calv\n        for test in self._tests:\n            test.setReferenceVoltage(caldb, calv)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setCalibration(self, db_boost_array, frequencies, frange):\n        self.calibrationVector = db_boost_array\n        self.calibrationFrequencies = frequencies\n        self.calibrationFrange = frange\n        for test in self._tests:\n            test.setCalibration(db_boost_array, frequencies, frange)", "response": "Sets calibration for all tests in this instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert(self, stim, position):\n        if position == -1:\n            position = self.rowCount()\n        stim.setReferenceVoltage(self.caldb, self.calv)\n        stim.setCalibration(self.calibrationVector, self.calibrationFrequencies, self.calibrationFrange)\n        self._tests.insert(position, stim)", "response": "Inserts a new stimulus into the list at the given position"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that this protocol model is valid. Return 0 if sucessful a failure message otherwise.", "response": "def verify(self, windowSize=None):\n        \"\"\"Verify that this protocol model is valid. Return 0 if sucessful,\n        a failure message otherwise\n\n        :param windowSize: acquistion window size (seconds), to check against duration, check is not performed is None provided\n        :type windowSize: float\n        :returns: 0 (int) for success, fail message (str) otherwise\n        \"\"\"\n        if self.rowCount() == 0:\n            return \"Protocol must have at least one test\"\n        if self.caldb is None or self.calv is None:\n            return \"Protocol reference voltage not set\"\n        for test in self._tests:\n            msg = test.verify(windowSize)\n            if msg:\n                return msg\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_any(filename):\n    if filename.endswith(\".gz\"):\n        return gzip.open\n\n    if filename.endswith(\".bz2\"):\n        return bz2.BZ2File\n\n    return open", "response": "Helper to open also compressed files\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the number of the group to which the tag belongs to.", "response": "def _get_group_no(self, tag_name):\n        \"\"\"\n        Takes tag name and returns the number of the group to which tag belongs\n        \"\"\"\n\n        if tag_name in self.full:\n            return self.groups.index(self.full[tag_name][\"parent\"])\n        else:\n            return len(self.groups)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def add(self, setname, ip, timeout=0):\n        args = ['add', '-exist', setname, ip, 'timeout', timeout]\n\n        return await self.start(__class__.CMD, *args)", "response": "Add the given IP address to the given ipset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def list(self, setname=None):\n        args = ['list']\n\n        if setname is not None:\n            args.append(setname)\n\n        return await self.start(__class__.CMD, *args)", "response": "Lists the existing ipsets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the calibration for this acquisition.", "response": "def set_calibration(self, attenuations, freqs, frange, calname):\n        \"\"\"See :meth:`AbstractAcquisitionRunner<sparkle.run.abstract_acquisition.AbstractAcquisitionRunner.set_calibration>`\"\"\"\n        self.protocol_model.setCalibration(attenuations, freqs, frange)\n        self.calname = calname\n        self.cal_frange = frange"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the total number of all tests in this protocol", "response": "def count(self):\n        \"\"\"Total number of all tests/traces/reps currently in this protocol\n\n        :returns: int -- the total\n        \"\"\"\n        total = 0\n        for test in self.protocol_model.allTests():\n            total += test.traceCount()*test.loopCount()*test.repCount() + test.repCount()\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self, interval):\n        self.trace_counter = 0\n\n        self._halt = False\n        self.interval = interval", "response": "Prepares the tests for execution interval in ms"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        self._initialize_run()\n\n        stimuli = self.protocol_model.allTests()\n\n        self.acq_thread = threading.Thread(target=self._worker, \n                                           args=(stimuli,), )\n        # save the current calibration to data file doc        \n        if self.save_data:\n            info = {'calibration_used': self.calname, 'calibration_range': self.cal_frange}\n            self.datafile.set_metadata(self.current_dataset_name, info)\n\n        # save the start time and set last tick to expired, so first\n        # acquisition loop iteration executes immediately\n        self.start_time = time.time()\n        self.last_tick = self.start_time - (self.interval/1000)\n\n        self.acq_thread.start()\n        return self.acq_thread", "response": "Runs the acquisition loop"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef train(self, x_data, y_data):\n        x_train, _, y_train, _ = train_test_split(\n            x_data,\n            y_data,\n            test_size=0.67,\n            random_state=None\n        )  # cross-split\n\n        self.model.fit(x_train, y_train)", "response": "Trains model on inputs\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_max_similar(string, lst):\n    max_similarity, index = 0.0, -1\n    for i, candidate in enumerate(lst):\n        sim = how_similar_are(str(string), str(candidate))\n        if sim > max_similarity:\n            max_similarity, index = sim, i\n    return max_similarity, index", "response": "Finds the most similar string in list\nWorkItem"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_average_length_of_string(strings):\n    if not strings:\n        return 0\n\n    return sum(len(word) for word in strings) / len(strings)", "response": "Computes the average length of words on list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef true_false_returns(func):\n\n    @functools.wraps(func)\n    def _execute(*args, **kwargs):\n        \"\"\"Executes function, if error returns False, else True\n\n        :param args: args of function\n        :param kwargs: extra args of function\n        :param *args: args\n        :param **kwargs: extra args\n        :return: True iff ok, else False\n        \"\"\"\n\n        try:\n            func(*args, **kwargs)\n            return True\n        except:\n            return False\n\n    return _execute", "response": "Executes function if error returns False else True\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef none_returns(func):\n\n    @functools.wraps(func)\n    def _execute(*args, **kwargs):\n        \"\"\"Executes function, if error returns None else value of function\n\n        :param args: args of function\n        :param kwargs: extra args of function\n        :param *args: args\n        :param **kwargs: extra args\n        :return: None else value of function\n        \"\"\"\n\n        try:\n            return func(*args, **kwargs)\n        except:\n            return None\n\n    return _execute", "response": "Executes function if error returns None else value of function\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting all dag nodes of the given reftrack", "response": "def select_dag_nodes(reftrack):\n    \"\"\"Select all dag nodes of the given reftrack\n\n    :param reftrack: The reftrack to select the dagnodes for\n    :type reftrack: :class:`jukeboxcore.reftrack.Reftrack`\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n    refobj = reftrack.get_refobj()\n    if not refobj:\n        return\n    parentns = common.get_namespace(refobj)\n    ns = cmds.getAttr(\"%s.namespace\" % refobj)\n    fullns = \":\".join((parentns.rstrip(\":\"), ns.lstrip(\":\")))\n    c = cmds.namespaceInfo(fullns, listOnlyDependencyNodes=True, dagPath=True, recurse=True)\n    dag = cmds.ls(c, dag=True, ap=True)\n    cmds.select(dag, replace=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_scenenode(self, nodes):\n        scenenodes = cmds.ls(nodes, type='jb_sceneNode')\n        assert scenenodes, \"Found no scene nodes!\"\n        return sorted(scenenodes)[0]", "response": "Get the first scenenode in the given nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreference the given taskfileinfo into the scene and return the created reference node.", "response": "def reference(self, refobj, taskfileinfo):\n        \"\"\"Reference the given taskfileinfo into the scene and return the created reference node\n\n        The created reference node will be used on :meth:`RefobjInterface.set_reference` to\n        set the reference on a reftrack node.\n        Do not call :meth:`RefobjInterface.set_reference` yourself.\n\n        This will also create a group node and group all dagnodes under a appropriate node.\n\n        :param refobj: the reftrack node that will be linked to the reference\n        :type refobj: str\n        :param taskfileinfo: The taskfileinfo that holds the information for what to reference\n        :type taskfileinfo: :class:`jukeboxcore.filesys.TaskFileInfo`\n        :returns: the reference node that was created and should set on the appropriate reftrack node\n        :rtype: str\n        :raises: None\n        \"\"\"\n        # work in root namespace\n        with common.preserve_namespace(\":\"):\n            jbfile = JB_File(taskfileinfo)\n            filepath = jbfile.get_fullpath()\n            ns_suggestion = reftrack.get_namespace(taskfileinfo)\n            newnodes = cmds.file(filepath, reference=True, namespace=ns_suggestion, returnNewNodes=True)\n            # You could also use the filename returned by the file command to query the reference node.\n            # Atm there is a but, that if you import the file before, the command fails.\n            # So we get all new reference nodes and query the one that is not referenced\n            for refnode in cmds.ls(newnodes, type='reference'):\n                if not cmds.referenceQuery(refnode, isNodeReferenced=True):\n                    node = refnode\n                    break\n            ns = cmds.referenceQuery(node, namespace=True)  # query the actual new namespace\n            content = cmds.namespaceInfo(ns, listOnlyDependencyNodes=True, dagPath=True)  # get the content\n            # connect reftrack with scenenode\n            scenenode = self.get_scenenode(content)\n            self.get_refobjinter().connect_reftrack_scenenode(refobj, scenenode)\n            reccontent = cmds.namespaceInfo(ns, listOnlyDependencyNodes=True, dagPath=True, recurse=True)  # get the content + content of children\n            dagcontent = cmds.ls(reccontent, ap=True, assemblies=True)  # get only the top level dagnodes so we can group them\n            if not dagcontent:\n                return node  # no need for a top group if there are not dagnodes to group\n            # group the dagnodes\n            grpname = reftrack.get_groupname(taskfileinfo)\n            reftrack.group_content(dagcontent, ns, grpname, \"jb_asset\")\n            return node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace(self, refobj, reference, taskfileinfo):\n        jbfile = JB_File(taskfileinfo)\n        filepath = jbfile.get_fullpath()\n        cmds.file(filepath, loadReference=reference)\n        ns = cmds.referenceQuery(reference, namespace=True)  # query the actual new namespace\n        content = cmds.namespaceInfo(ns, listOnlyDependencyNodes=True, dagPath=True)  # get the content\n        scenenode = self.get_scenenode(content) # get the scene node\n        self.get_refobjinter().connect_reftrack_scenenode(refobj, scenenode)", "response": "Replace the given reference with the given taskfileinfo."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, refobj):\n        refobjinter = self.get_refobjinter()\n        reference = refobjinter.get_reference(refobj)\n        if reference:\n            fullns = cmds.referenceQuery(reference, namespace=True)\n            cmds.file(removeReference=True, referenceNode=reference)\n        else:\n            parentns = common.get_namespace(refobj)\n            ns = cmds.getAttr(\"%s.namespace\" % refobj)\n            fullns = \":\".join((parentns.rstrip(\":\"), ns.lstrip(\":\")))\n        cmds.namespace(removeNamespace=fullns, deleteNamespaceContent=True)", "response": "Delete the content of the given refobj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport the given reference node into the given node s internal cache", "response": "def import_reference(self, refobj, reference):\n        \"\"\"Import the given reference\n\n        The reference of the refobj will be set to None automatically afterwards with\n        :meth:`RefobjInterface.set_reference`\n\n        :param refobj: the refobj that is linked to the reference\n        :param reference: the reference object. E.g. in Maya a reference node\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        cmds.file(importReference=True, referenceNode=reference)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport the given taskfileinfo and update the refobj", "response": "def import_taskfile(self, refobj, taskfileinfo):\n        \"\"\"Import the given taskfileinfo and update the refobj\n\n        :param refobj: the refobject\n        :type refobj: refobject\n        :param taskfileinfo: the taskfileinfo to reference\n        :type taskfileinfo: :class:`jukeboxcore.filesys.TaskFileInfo`\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        # work in root namespace\n        with common.preserve_namespace(\":\"):\n            jbfile = JB_File(taskfileinfo)\n            filepath = jbfile.get_fullpath()\n            ns_suggestion = reftrack.get_namespace(taskfileinfo)\n            nodes = cmds.file(filepath, i=True, namespace=ns_suggestion, returnNewNodes=True, preserveReferences=True)  # import\n            assert nodes, 'Nothing was imported! this is unusual!'\n            ns = common.get_top_namespace(nodes[0])  # get the actual namespace\n            cmds.setAttr(\"%s.namespace\" % refobj, ns, type=\"string\")\n            nscontent = cmds.namespaceInfo(ns, listOnlyDependencyNodes=True, dagPath=True)  # get the content\n            scenenode = self.get_scenenode(nscontent)\n            self.get_refobjinter().connect_reftrack_scenenode(refobj, scenenode)\n            dagcontent = cmds.ls(nodes, ap=True, assemblies=True)  # get only the dagnodes so we can group them\n            if not dagcontent:\n                return  # no need for a top group if there are not dagnodes to group\n            # group the dagnodes in the new namespace\n            grpname = reftrack.get_groupname(taskfileinfo)\n            reftrack.group_content(dagcontent, ns, grpname, \"jb_asset\")\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_option_taskfileinfos(self, element):\n        tfs = []\n        for task in element.tasks.all():\n            taskfiles = list(task.taskfile_set.filter(releasetype=djadapter.RELEASETYPES['release'],\n                                                      typ=djadapter.FILETYPES['mayamainscene']))\n            tfs.extend(taskfiles)\n        tfis = [TaskFileInfo.create_from_taskfile(tf) for tf in tfs]\n        return tfis", "response": "Fetch the options for possible files to load replace etc for the given element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_options_model(self, taskfileinfos):\n        rootdata = ListItemData([\"Asset/Shot\", \"Task\", \"Descriptor\", \"Version\", \"Releasetype\"])\n        rootitem = TreeItem(rootdata)\n        tasks = defaultdict(list)\n        for tfi in taskfileinfos:\n            tasks[tfi.task].append(tfi)\n        for task in reversed(sorted(tasks.keys(), key=lambda t: t.department.ordervalue)):\n            tfis = tasks[task]\n            taskdata = djitemdata.TaskItemData(task)\n            taskitem = TreeItem(taskdata, rootitem)\n            for tfi in reversed(tfis):\n                tfidata = TaskFileInfoItemData(tfi)\n                TreeItem(tfidata, taskitem)\n        return TreeModel(rootitem)", "response": "Create a new option model that has the taskfileinfos as internal_data of the leaves."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_scene_suggestions(self, current):\n        l = []\n        if isinstance(current, djadapter.models.Asset):\n            l.append(current)\n        l.extend(list(current.assets.all()))\n        return l", "response": "Return a list with elements for reftracks for the current scene with this type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of additional actions that you want to provide for the menu of the reftrack.", "response": "def get_additional_actions(self, reftrack):\n        \"\"\"Return a list of additional actions you want to provide for the menu\n        of the reftrack.\n\n        E.e. you want to have a menu entry, that will select the entity in your programm.\n\n        :param reftrack: the reftrack to return the actions for\n        :type reftrack: :class:`Reftrack`\n        :returns: A list of :class:`ReftrackAction`\n        :rtype: list\n        :raises: None\n        \"\"\"\n        refobj = reftrack.get_refobj()\n        select_dp_action = ReftrackAction(\"Select Nodes\", partial(select_dp_nodes, reftrack=reftrack), enabled=bool(refobj))\n        select_dag_action = ReftrackAction(\"Select DAG\", partial(select_dag_nodes, reftrack=reftrack), enabled=bool(refobj))\n        return [select_dp_action, select_dag_action]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating the menu under the Jenkins menu", "response": "def init_ui(self, ):\n        \"\"\"Create the menu \\\"Preferences\\\" under \\\"Jukebox\\\" to start the plugin\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n        self.mm = MenuManager.get()\n        p = self.mm.menus['Jukebox']\n        self.menu = self.mm.create_menu(\"Preferences\", p, command=self.run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setupArgparse():\n    parser = argparse.ArgumentParser()\n\n    # Required arguments\n    parser.add_argument(\"callsign\", help=\"Callsign of radio\")\n    parser.add_argument(\"id\", type=int, help=\"ID number radio\")\n\n    # Optional arguments\n    parser.add_argument(\"-l\", \"--loopback\", action=\"store_true\",\n                        help=\"Use software loopback serial port\")\n    parser.add_argument(\"-p\", \"--port\", default=\"/dev/ttyUSB0\",\n                        help=\"Physical serial port of radio\")\n\n    # Parse and return arguments\n    return parser.parse_args()", "response": "Sets up the argparse module to create command line options and parse them."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting up serial port by connecting to phsyical or software port. Depending on command line options, this function will either connect to a SerialTestClass() port for loopback testing or to the specified port from the command line option. If loopback is True it overrides the physical port specification. Args: loopback: argparse option port: argparse option Returns: serialPort: Pyserial serial port instance", "response": "def setupSerialPort(loopback, port):\n    \"\"\"Sets up serial port by connecting to phsyical or software port.\n\n    Depending on command line options, this function will either connect to a\n    SerialTestClass() port for loopback testing or to the specified port from\n    the command line option. If loopback is True it overrides the physical port\n    specification.\n\n    Args:\n        loopback: argparse option\n        port: argparse option\n\n    Returns:\n        serialPort: Pyserial serial port instance\n    \"\"\"\n    if loopback:\n        # Implement loopback software serial port\n        testSerial = SerialTestClass()\n        serialPort = testSerial.serialPort\n    else:\n        # TODO enable serial port command line options (keep simple for user!)\n        serialPort = serial.Serial(port, 115200, timeout=0)\n\n    return serialPort"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    print(\"Executing faradayio-cli version {0}\".format(__version__))\n\n    # Setup command line arguments\n    try:\n        args = setupArgparse()\n\n    except argparse.ArgumentError as error:\n        raise SystemExit(error)\n\n    # Setup serial port\n    try:\n        serialPort = setupSerialPort(args.loopback, args.port)\n\n    except serial.SerialException as error:\n        raise SystemExit(error)\n\n    # Create TUN adapter name\n    tunName = \"{0}-{1}\".format(args.callsign.upper(), args.id)\n\n    # Create threading event for TUN thread control\n    # set() causes while loop to continuously run until clear() is run\n    isRunning = threading.Event()\n    isRunning.set()\n\n    # Setup TUN adapter and start\n    try:\n        tun = Monitor(serialPort=serialPort, name=tunName, isRunning=isRunning)\n        tun.start()\n\n    except pytun.Error as error:\n        print(\"Warning! faradayio-cli must be run with sudo privileges!\")\n        raise SystemExit(error)\n\n    # loop infinitely until KeyboardInterrupt, then clear() event, exit thread\n    try:\n        while True:\n            # Check for KeyboardInterrupt every 100ms\n            time.sleep(0.1)\n\n    except KeyboardInterrupt:\n        tun.isRunning.clear()\n        tun.join()", "response": "Main function of faradayio - cli client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring python `logging`. This is required before the `debug()`, `info()`, etc. functions may be used. If any other `codekit.*` modules, which are not a \"package\", have been imported, and they have a `setup_logging()` function, that is called before `logging` is configured. This gives other modules a chance to configure their own logging. As an example, if `progressbar2` is being used, it needs to be configure a `sys.stderr` wrapper before `logging` is configured. Thus, some gymnastics are being done to delay `logging` setup while simultanously not requiring that `progressbar2` be imported unless it is actually being used. Parameters ---------- verbosity: int Logging / output verbosity level. 1 is useful for more purposes while 2+ is generaly TMI.", "response": "def setup_logging(verbosity=0):\n    \"\"\"Configure python `logging`.  This is required before the `debug()`,\n    `info()`, etc. functions may be used.\n\n    If any other `codekit.*` modules, which are not a \"package\", have been\n    imported, and they have a `setup_logging()` function, that is called before\n    `logging` is configured.  This gives other modules a chance to configure\n    their own logging.\n\n    As an example, if `progressbar2` is being used, it needs to be configure a\n    `sys.stderr` wrapper before `logging` is configured.  Thus, some gymnastics\n    are being done to delay `logging` setup while simultanously not requiring\n    that `progressbar2` be imported unless it is actually being used.\n\n    Parameters\n    ----------\n    verbosity: int\n        Logging / output verbosity level. 1 is useful for more purposes while\n        2+ is generaly TMI.\n    \"\"\"\n    import pkgutil\n    import logging\n    import codekit\n\n    # https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages\n    def iter_namespace(ns_pkg):\n        # Specifying the second argument (prefix) to iter_modules makes the\n        # returned name an absolute name instead of a relative one. This allows\n        # import_module to work without having to do additional modification to\n        # the name.\n        return pkgutil.iter_modules(ns_pkg.__path__, ns_pkg.__name__ + \".\")\n\n    # find codekit modules that are not a package\n    codekit_mods = [name for finder, name, ispkg in iter_namespace(codekit)\n                    if ispkg is False]\n\n    # filter out the current module\n    # XXX `is not` doesn't work here but `!=` does... why???\n    codekit_mods = [m for m in codekit_mods\n                    if m != __name__]\n\n    # filter out modules that have not been imported\n    codekit_mods = [m for m in codekit_mods\n                    if m in sys.modules]\n\n    # record funcs successfully called\n    logging_funcs = []\n    for m in codekit_mods:\n        try:\n            lsetup = getattr(sys.modules[m], 'setup_logging')\n            lsetup(verbosity=verbosity)\n            logging_funcs.append(lsetup)\n        except AttributeError:\n            # ignore modules that do have a setup_logging()\n            pass\n\n    logging.basicConfig()\n    # configure `logger` for the entire module\n    global logger\n    logger = logging.getLogger('codekit')\n\n    if verbosity:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    [debug(\"{m}.{f}()\".format(m=f.__module__, f=f.__name__))\n        for f in logging_funcs]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the email address to use when creating git objects or exit program.", "response": "def lookup_email(args):\n    \"\"\"Return the email address to use when creating git objects or exit\n    program.\n\n    Parameters\n    ----------\n    args: parser.parse_args()\n\n    Returns\n    -------\n    email : `string`\n        git user email address\n    \"\"\"\n    email = args.email\n    if email is None:\n        email = gituseremail()\n\n    if email is None:\n        raise RuntimeError(textwrap.dedent(\"\"\"\\\n            unable to determine a git email\n            Specify --email option\\\n        \"\"\"))\n\n    debug(\"email is {email}\".format(email=email))\n\n    return email"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the user name to use when creating git objects or exit program.", "response": "def lookup_user(args):\n    \"\"\"Return the user name to use when creating git objects or exit\n    program.\n\n    Parameters\n    ----------\n    args: parser.parse_args()\n\n    Returns\n    -------\n    user: `string`\n        git user name\n    \"\"\"\n    user = args.user\n    if user is None:\n        user = gitusername()\n\n    if user is None:\n        raise RuntimeError(textwrap.dedent(\"\"\"\\\n            unable to determine a git user name\n            Specify --user option\\\n        \"\"\"))\n\n    debug(\"user name is {user}\".format(user=user))\n\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a github oauth token as a string.", "response": "def github_token(token_path=None, token=None):\n    \"\"\"Return a github oauth token as a string.  If `token` is defined, it is\n    has precendece.  If `token` and `token_path` are `None`,\n    `~/.sq_github_token` looked for as a fallback.\n\n    Parameters\n    ----------\n    token_path : str, optional\n        Path to the token file. The default token is used otherwise.\n\n    token: str, optional\n        Literial token string. If specifified, this value is used instead of\n        reading from the token_path file.\n\n    Returns\n    -------\n    token : `string`\n        Hopefully, a valid github oauth token.\n    \"\"\"\n    if token is None:\n        if token_path is None:\n            # Try the default token\n            token_path = '~/.sq_github_token'\n        token_path = os.path.expandvars(os.path.expanduser(token_path))\n\n        if not os.path.isfile(token_path):\n            print(\"You don't have a token in {0} \".format(token_path))\n            print(\"Have you run github-auth?\")\n            raise EnvironmentError(\"No token in %s\" % token_path)\n\n        with open(token_path, 'r') as fdo:\n            token = fdo.readline().strip()\n\n    return token"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef current_timestamp():\n    now = datetime.utcnow()\n    timestamp = now.isoformat()[0:19] + 'Z'\n\n    debug(\"generated timestamp: {now}\".format(now=timestamp))\n\n    return timestamp", "response": "Returns current time as ISO8601 formatted string in the Zulu TZ"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads and return DM_SQUARE_DEBUG env var if defined. Raises RuntimeError if DM_SQUARE_DEBUG env var is not an int convertable value", "response": "def debug_lvl_from_env():\n    \"\"\"Read and return `DM_SQUARE_DEBUG` env var, if defined.\n\n    Raises\n    ------\n    RuntimeError\n        If DM_SQUARE_DEBUG is not an int convertable value\n    \"\"\"\n    debug_lvl = os.environ.get('DM_SQUARE_DEBUG')\n    if not debug_lvl:\n        return 0\n\n    try:\n        debug_lvl = int(debug_lvl)\n    except ValueError:\n        # ensure that logging is configured as this method is likely to be\n        # called prior to configuring logging.\n        setup_logging(verbosity=1)\n        raise RuntimeError(\n            textwrap.dedent(\"\"\"\\\n            env var DM_SQUARE_DEBUG '{debug_lvl}' is not a string value that\n            can be converted to an int.\"\"\".format(debug_lvl=debug_lvl))\n        ) from None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calc_spectrum(signal, rate):\n    npts = len(signal)\n    padto = 1 << (npts - 1).bit_length()\n    # print 'length of signal {}, pad to {}'.format(npts, padto)\n    npts = padto\n\n    sp = np.fft.rfft(signal, n=padto) / npts\n    # print('sp len ', len(sp))\n    freq = np.arange((npts / 2) + 1) / (npts / rate)\n    # print('freq len ', len(freq))\n    return freq, abs(sp)", "response": "Calculate the spectrum and frequency indexes for real - valued input signal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_tone(freq, db, dur, risefall, samplerate, caldb=100, calv=0.1):\n    if risefall > dur:\n        raise ValueError('Duration must be greater than risefall time')\n    if samplerate <= 0:\n        raise ValueError(\"Samplerate must be greater than 0\")\n    if caldb <= 0:\n        raise ValueError(\"Calibration dB SPL must be greater than 0\")\n\n    npts = int(dur * samplerate)\n    amp = (10 ** ((db - caldb) / 20) * calv)\n    if USE_RMS:\n        amp = amp * 1.414213562373\n\n    if VERBOSE:\n        print(\n        \"current dB: {}, fs: {}, current frequency: {} kHz, AO Amp: {:.6f}\".format(db, samplerate, freq / 1000, amp))\n        print(\"cal dB: {}, V at cal dB: {}\".format(caldb, calv))\n\n    tone = amp * np.sin((freq * dur) * np.linspace(0, 2 * np.pi, npts))\n\n    # print 'tone max', np.amax(tone)  \n    if risefall > 0:\n        rf_npts = int(risefall * samplerate) // 2\n        # print('amp {}, freq {}, npts {}, rf_npts {}'.format(amp,freq,npts,rf_npts))\n        wnd = hann(rf_npts * 2)  # cosine taper\n        tone[:rf_npts] = tone[:rf_npts] * wnd[:rf_npts]\n        tone[-rf_npts:] = tone[-rf_npts:] * wnd[rf_npts:]\n\n    timevals = np.arange(npts) / samplerate\n\n    return tone, timevals", "response": "Produce a pure tone signal \n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nproduce a pure tone signal and a time index value", "response": "def make_carrier_tone(freq, db, dur, samplerate, caldb=100, calv=0.1):\n    \"\"\"\n    Produce a pure tone signal\n\n    :param freq: Frequency of the tone to be produced (Hz)\n    :type freq: int\n    :param db: Intensity of the tone in dB SPL\n    :type db: int\n    :param dur: duration (seconds)\n    :type dur: float\n    :param samplerate: generation frequency of tone (Hz)\n    :type samplerate: int\n    :param caldb: Reference intensity (dB SPL). Together with calv, provides a reference point for what intensity equals what output voltage level\n    :type caldb: int\n    :param calv: Reference voltage (V). Together with caldb, provides a reference point for what intensity equals what output voltage level\n    :type calv: float\n    :returns: tone, timevals -- the signal and the time index values\n    \"\"\"\n    if samplerate <= 0:\n        raise ValueError(\"Samplerate must be greater than 0\")\n    if caldb <= 0:\n        raise ValueError(\"Calibration dB SPL must be greater than 0\")\n\n    npts = int(dur * samplerate)\n    amp = (10 ** ((db - caldb) / 20) * calv)\n    if USE_RMS:\n        amp *= 1.414213562373\n\n    if VERBOSE:\n        print(\n        \"current dB: {}, fs: {}, current frequency: {} kHz, AO Amp: {:.6f}\".format(db, samplerate, freq / 1000, amp))\n        print(\"cal dB: {}, V at cal dB: {}\".format(caldb, calv))\n\n    tone = amp * np.sin((freq * dur) * np.linspace(0, 2 * np.pi, npts))\n\n    timevals = np.arange(npts) / samplerate\n\n    return tone, timevals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a 2D array of spectral intensity for a given audio file or vector of audio signal.", "response": "def spectrogram(source, nfft=512, overlap=90, window='hanning', caldb=93, calv=2.83):\n    \"\"\"\n    Produce a matrix of spectral intensity, uses matplotlib's specgram\n    function. Output is in dB scale.\n\n    :param source: filename of audiofile, or samplerate and vector of audio signal\n    :type source: str or (int, numpy.ndarray)\n    :param nfft: size of nfft window to use\n    :type nfft: int\n    :param overlap: percent overlap of window\n    :type overlap: number\n    :param window: Type of window to use, choices are hanning, hamming, blackman, bartlett or none (rectangular)\n    :type window: string\n    :returns: spec -- 2D array of intensities, freqs -- yaxis labels, bins -- time bin labels, duration -- duration of signal\n    \"\"\"\n    if isinstance(source, basestring):\n        fs, wavdata = audioread(source)\n    else:\n        fs, wavdata = source\n\n    # truncate to nears ms\n    duration = float(len(wavdata)) / fs\n    desired_npts = int((np.trunc(duration * 1000) / 1000) * fs)\n    # print 'LENGTH {}, DESIRED {}'.format(len(wavdata), desired_npts)\n    wavdata = wavdata[:desired_npts]\n    duration = len(wavdata) / fs\n\n    if VERBOSE:\n        amp = rms(wavdata, fs)\n        print 'RMS of input signal to spectrogram', amp\n\n    # normalize\n    if len(wavdata) > 0 and np.max(abs(wavdata)) != 0:\n        wavdata = wavdata / np.max(abs(wavdata))\n\n    if window == 'hanning':\n        winfnc = mlab.window_hanning\n    elif window == 'hamming':\n        winfnc = np.hamming(nfft)\n    elif window == 'blackman':\n        winfnc = np.blackman(nfft)\n    elif window == 'bartlett':\n        winfnc = np.bartlett(nfft)\n    elif window == None or window == 'none':\n        winfnc = mlab.window_none\n\n    noverlap = int(nfft * (float(overlap) / 100))\n\n    Pxx, freqs, bins = mlab.specgram(wavdata, NFFT=nfft, Fs=fs, noverlap=noverlap,\n                                     pad_to=nfft * 2, window=winfnc, detrend=mlab.detrend_none,\n                                     sides='default', scale_by_freq=False)\n\n    # log of zero is -inf, which is not great for plotting\n    Pxx[Pxx == 0] = np.nan\n\n    # convert to db scale for display\n    spec = 20. * np.log10(Pxx)\n\n    # set 0 to miniumum value in spec?\n    # would be great to have spec in db SPL, and set any -inf to 0\n    spec[np.isnan(spec)] = np.nanmin(spec)\n\n    return spec, freqs, bins, duration"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef impulse_response(genrate, fresponse, frequencies, frange, filter_len=2 ** 14, db=True):\n\n    freq = frequencies\n    max_freq = genrate / 2 + 1\n\n    attenuations = np.zeros_like(fresponse)\n    # add extra points for windowing\n    winsz = 0.05  # percent\n    lowf = max(0, frange[0] - (frange[1] - frange[0]) * winsz)\n    highf = min(frequencies[-1], frange[1] + (frange[1] - frange[0]) * winsz)\n\n    f0 = (np.abs(freq - lowf)).argmin()\n    f1 = (np.abs(freq - highf)).argmin()\n    fmax = (np.abs(freq - max_freq)).argmin() + 1\n    attenuations[f0:f1] = fresponse[f0:f1] * tukey(len(fresponse[f0:f1]), winsz)\n    if db:\n        freq_response = 10 ** ((attenuations).astype(float) / 20)\n    else:\n        freq_response = attenuations\n\n    freq_response = freq_response[:fmax]\n\n    impulse_response = np.fft.irfft(freq_response)\n\n    # rotate to create causal filter, and truncate\n    impulse_response = np.roll(impulse_response, len(impulse_response) // 2)\n\n    # truncate\n    if filter_len < len(impulse_response):\n        startidx = (len(impulse_response) // 2) - (filter_len // 2)\n        stopidx = (len(impulse_response) // 2) + (filter_len // 2)\n        impulse_response = impulse_response[startidx:stopidx]\n\n    # should I also window the impulse response - by how much?\n    impulse_response = impulse_response * tukey(len(impulse_response), 0.05)\n\n    return impulse_response", "response": "Calculates the filter kernel from the attenuation vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate an attenuation roll - off curve from a signal and a response of a microphone.", "response": "def attenuation_curve(signal, resp, fs, calf, smooth_pts=99):\n    \"\"\"\n    Calculate an attenuation roll-off curve, from a signal and its recording\n\n    :param signal: ouput signal delivered to the generation hardware\n    :type signal: numpy.ndarray    \n    :param resp: recording of given signal, as recieved from microphone\n    :type resp: numpy.ndarray\n    :param fs: input and output samplerate (should be the same)\n    :type fs: int\n    :param smooth_pts: amount of averaging to use on the result\n    :type smooth_pts: int\n    :returns: numpy.ndarray -- attenuation vector\n    \"\"\"\n    # remove dc offset\n    y = resp - np.mean(resp)\n    x = signal\n\n    # frequencies present in calibration spectrum\n    npts = len(y)\n    fq = np.arange(npts / 2 + 1) / (float(npts) / fs)\n\n    # convert time signals to frequency domain\n    Y = np.fft.rfft(y)\n    X = np.fft.rfft(x)\n\n    # take the magnitude of signals\n    Ymag = np.sqrt(Y.real ** 2 + Y.imag ** 2)  # equivalent to abs(Y)\n    Xmag = np.sqrt(X.real ** 2 + X.imag ** 2)\n\n    # convert to decibel scale\n    YmagdB = 20 * np.log10(Ymag)\n    XmagdB = 20 * np.log10(Xmag)\n\n    # now we can substract to get attenuation curve\n    diffdB = XmagdB - YmagdB\n\n    # may want to smooth results here?\n    diffdB = smooth(diffdB, smooth_pts)\n\n    # shift by the given calibration frequency to align attenutation\n    # with reference point set by user\n    fidx = (np.abs(fq - calf)).argmin()\n    diffdB -= diffdB[fidx]\n\n    return diffdB"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calibrate_signal(signal, resp, fs, frange):\n    # remove dc offset from recorded response (synthesized orignal shouldn't have one)\n    dc = np.mean(resp)\n    resp = resp - dc\n\n    npts = len(signal)\n    f0 = np.ceil(frange[0] / (float(fs) / npts))\n    f1 = np.floor(frange[1] / (float(fs) / npts))\n\n    y = resp\n    # y = y/np.amax(y) # normalize\n    Y = np.fft.rfft(y)\n\n    x = signal\n    # x = x/np.amax(x) # normalize\n    X = np.fft.rfft(x)\n\n    H = Y / X\n\n    # still issues warning because all of Y/X is executed to selected answers from\n    # H = np.where(X.real!=0, Y/X, 1)\n    # H[:f0].real = 1\n    # H[f1:].real = 1\n    # H = smooth(H)\n\n    A = X / H\n\n    return np.fft.irfft(A)", "response": "Given original signal and recording spits out a calibrated signal"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multiply_frequencies(signal, fs, frange, calibration_frequencies, attendB):\n    npts = len(signal)\n    padto = 1 << (npts - 1).bit_length()\n\n    X = np.fft.rfft(signal, n=padto)\n\n    npts = padto\n    f = np.arange((npts / 2) + 1) / (npts / fs)\n\n    fidx_low = (np.abs(f - frange[0])).argmin()\n    fidx_high = (np.abs(f - frange[1])).argmin()\n\n    cal_func = interp1d(calibration_frequencies, attendB)\n    roi = f[fidx_low:fidx_high]\n    Hroi = cal_func(roi)\n    H = np.zeros((len(X),))\n    H[fidx_low:fidx_high] = Hroi\n\n    H = smooth(H)\n    # print 'H dB max', np.amax(H)\n\n    H = 10 ** ((H).astype(float) / 20)\n    # print 'H amp max', np.amax(H)\n\n    # Xadjusted = X.copy()\n    # Xadjusted[fidx_low:fidx_high] *= H\n    # Xadjusted = smooth(Xadjusted)\n\n    Xadjusted = X * H\n    # print 'X max', np.amax(abs(X))\n    # print 'Xadjusted max', np.amax(abs(Xadjusted))\n\n    signal_calibrated = np.fft.irfft(Xadjusted)\n    return signal_calibrated[:len(signal)]", "response": "This function takes a vector of dB attenuations and adjusts the signal by \n       multiplication in the frequency domain"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tukey(winlen, alpha):\n    taper = hann(winlen * alpha)\n    rect = np.ones(winlen - len(taper) + 1)\n    win = fftconvolve(taper, rect)\n    win = win / np.amax(win)\n    return win", "response": "Generate a tukey for a given window length and alpha"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread an audio file into samplerate array.", "response": "def audioread(filename):\n    \"\"\"Reads an audio signal from file.\n\n    Supported formats : wav\n\n    :param filename: filename of the audiofile to load\n    :type filename: str\n    :returns: int, numpy.ndarray -- samplerate, array containing the audio signal\n    \"\"\"\n    try:\n        if '.wav' in filename.lower():\n            fs, signal = wv.read(filename)\n        elif '.call' in filename.lower():\n            with open(filename, 'rb') as f:\n                signal = np.fromfile(f, dtype=np.int16)\n            fs = 333333\n        else:\n            raise IOError(\"Unsupported audio format for file: {}\".format(filename))\n    except:\n        print u\"Problem reading wav file\"\n        raise\n    signal = signal.astype(float)\n    return fs, signal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the samplerate of the given audio recording file", "response": "def audiorate(filename):\n    \"\"\"Determines the samplerate of the given audio recording file\n\n    :param filename: filename of the audiofile\n    :type filename: str\n    :returns: int -- samplerate of the recording\n    \"\"\"\n    if '.wav' in filename.lower():\n        wf = wave.open(filename)\n        fs = wf.getframerate()\n        wf.close()\n    elif '.call' in filename.lower():\n        fs = 333333\n    else:\n        raise IOError(\"Unsupported audio format for file: {}\".format(filename))\n\n    return fs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the root mean square of the given signal", "response": "def rms(signal, fs):\n    \"\"\"Returns the root mean square (RMS) of the given *signal*\n\n    :param signal: a vector of electric potential\n    :type signal: numpy.ndarray\n    :param fs: samplerate of the signal (Hz)\n    :type fs: int\n    :returns: float -- the RMS value of the signal\n    \"\"\"\n    # if a signal contains a some silence, taking the RMS of the whole\n    # signal will be calculated as less loud as a signal without a silent\n    # period. I don't like this, so I am going to chunk the signals, and\n    # take the value of the most intense chunk\n    chunk_time = 0.001  # 1 ms chunk\n    chunk_samps = int(chunk_time * fs)\n    amps = []\n    if chunk_samps > 10:\n        for i in range(0, len(signal) - chunk_samps, chunk_samps):\n            amps.append(np.sqrt(np.mean(pow(signal[i:i + chunk_samps], 2))))\n        amps.append(np.sqrt(np.mean(pow(signal[len(signal) - chunk_samps:], 2))))\n        return np.amax(amps)\n    else:\n        # samplerate low, just rms the whole thing\n        return np.sqrt(np.mean(pow(signal, 2)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init_ui(self, ):\n        self.sidebar = self.get_maya_sidebar()\n        self.lay = self.sidebar.layout()\n        self.tool_pb = QtGui.QPushButton(\"JB Wins\")\n        self.tooltip = JB_WindowToolTip()\n        self.tooltip.install_tooltip(self.tool_pb)\n        self.lay.addWidget(self.tool_pb)\n        self.tool_pb.clicked.connect(self.tooltip.show)", "response": "Create the tooltip in the sidebar"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef uninit_ui(self):\n        self.lay.removeWidget(self.tool_pb)\n        self.tooltip.deleteLater()\n        self.tool_pb.deleteLater()", "response": "Delete the tooltip and tool_pb"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens and returns the correct AcquisitionData object according to filename extention.", "response": "def open_acqdata(filename, user='unknown', filemode='w-'):\n    \"\"\"Opens and returns the correct AcquisitionData object according to filename extention.\n\n    Supported extentions:\n    * .hdf5, .h5 for sparkle data\n    * .pst, .raw for batlab data. Both the .pst and .raw file must be co-located and share the same base file name, but only one should be provided to this function\n    \n    see :class:`AcquisitionData<sparkle.data.acqdata.AcquisitionData>`\n\n    examples (if data file already exists)::\n\n        data = open_acqdata('myexperiment.hdf5', filemode='r')\n        print data.dataset_names()\n\n    for batlab data::\n\n        data = open('mouse666.raw', filemode='r')\n        print data.dataset_names()\n    \"\"\"\n    if filename.lower().endswith((\".hdf5\", \".h5\")):\n        return HDF5Data(filename, user, filemode)\n    elif filename.lower().endswith((\".pst\", \".raw\")):\n        return BatlabData(filename, user, filemode)\n    else:\n        print \"File format not supported: \", filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef columnCount(self, parent=QtCore.QModelIndex()):\n        if parent.isValid():\n            return self._stim.columnCount(parent.row())\n        else:\n            return self._stim.columnCount()", "response": "Determines the number of columns the view will draw\n\n            Required by view"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the data for the stimulus at the given index.", "response": "def data(self, index, role=QtCore.Qt.UserRole, mode=BuildMode):\n        \"\"\"Used by the view to determine data to present\n\n        See :qtdoc:`QAbstractItemModel<QAbstractItemModel.data>`, \n        and :qtdoc:`subclassing<qabstractitemmodel.subclassing>`\n        \"\"\"\n        if role == CursorRole:\n            if index.isValid():\n                if mode == BuildMode:\n                    return cursors.openHand()\n                elif mode == AutoParamMode:\n                    return cursors.pointyHand()\n                else:\n                    raise ValueError(\"Invalid stimulus edit mode\")\n            else:\n                return QtGui.QCursor(QtCore.Qt.ArrowCursor)\n\n        if not index.isValid():\n            return None\n        if role == QtCore.Qt.DisplayRole:\n            component = self._stim.component(index.row(),index.column())\n            return component.__class__.__name__\n        elif role == QtCore.Qt.SizeHintRole:\n            component = self._stim.component(index.row(),index.column())\n            return component.duration() #* PIXELS_PER_MS * 1000\n        elif role == QtCore.Qt.UserRole or role == QtCore.Qt.UserRole+1:  #return the whole python object\n            if self._stim.columnCountForRow(index.row()) > index.column():\n                component = self._stim.component(index.row(),index.column())\n                if role == QtCore.Qt.UserRole:\n                    component = wrapComponent(component)\n            else:\n                component = None\n            return component"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index(self, row, col, parent=QtCore.QModelIndex()):\n        if row < self._stim.rowCount() and col < self._stim.columnCountForRow(row):\n            component = self._stim.component(row, col)\n            return self.createIndex(row, col, component)\n        else:\n            return QtCore.QModelIndex()", "response": "Creates an index. An item must exist for the given row and col."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninserting new component at index", "response": "def insertComponent(self, index, comp):\n        \"\"\"Inserts new component *comp* at index\"\"\"\n        # new component needs to be wrapped\n        if self.columnCountForRow(index.row()) == 0:\n            self.beginInsertRows(QtCore.QModelIndex(), index.row(), index.row())\n            self._stim.insertComponent(comp, index.row(), index.column())\n            self.endInsertRows()\n        else:\n            self._stim.insertComponent(comp, index.row(), index.column())\n\n        self.samplerateChanged.emit(self._stim.samplerate())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the component at index from the model.", "response": "def removeComponent(self, index):\n        \"\"\"Removes the component at *index* from the model. If the two last\n        rows are now empty, trims the last row.\"\"\"\n        if index.row() ==  self.rowCount() -1  and self.columnCountForRow(index.row()) == 1:\n            self.beginRemoveRows(QtCore.QModelIndex(), self._stim.rowCount()-1, \n                                 self._stim.rowCount()-1)\n            self._stim.removeComponent(index.row(), index.column())\n            self.endRemoveRows()\n        else:\n            self._stim.removeComponent(index.row(), index.column())\n\n        # this could have affected the sample of this stimulus\n        self.samplerateChanged.emit(self._stim.samplerate())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef removeItem(self, index):\n        self._stim.removeComponent(index.row(), index.column())", "response": "Remove a component from the list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the component at index to value", "response": "def setData(self, index, value, role=QtCore.Qt.UserRole):\n        \"\"\"Sets the component at *index* to *value*\"\"\"\n        # item must already exist at provided index\n        self._stim.overwriteComponent(value, index.row(), index.column())\n\n        self.samplerateChanged.emit(self.samplerate())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flags(self, index):\n        return QtCore.Qt.ItemIsEditable | QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable", "response": "Determines interaction allowed with table cells."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the editor class for this Stimulus", "response": "def setEditor(self, name):\n        \"\"\"Sets the editor class for this Stimulus\"\"\"\n        editor = get_stimulus_editor(name)\n        self.editor = editor\n        self._stim.setStimType(name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef showEditor(self):\n        if self.editor is not None:\n            editor = self.editor()\n            editor.setModel(self)\n            factory = get_stimulus_factory(self._stim.stimType())\n            editor.editingFinished.connect(factory.update)\n            return editor\n        else:\n            logger = logging.getLogger('main')\n            logger.warning('Erm, no editor available :(')", "response": "Creates and shows an editor for this Stimulus"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the reorder function on this StimulusModel to a randomizer .", "response": "def randomToggle(self, randomize):\n        \"\"\"Sets the reorder function on this StimulusModel to a randomizer\n        or none, alternately\"\"\"\n        if randomize:\n            self._stim.setReorderFunc(order_function('random'), 'random')\n        else:\n            self._stim.reorder = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loadFromTemplate(template, stim=None):\n        stim = StimulusModel.loadFromTemplate(template, stim=stim)\n        qstim = QStimulusModel(stim)\n        qstim.setEditor(template['testtype'])\n        return qstim", "response": "Loads a new stimulus from a saved template doc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_variable_type(variable, expected_type, raise_exception=True):\n    # if expected type is not a list make it one\n    if not isinstance(expected_type, list):\n        expected_type = [expected_type]\n    # make sure all entries in the expected_type list are types\n    for t in expected_type:\n        if not isinstance(t, type):\n            raise ValueError('expected_type argument \"%s\" is not a type' %str(t))\n    # make sure raise_exception is a bool\n    if not isinstance(raise_exception, bool):\n        raise ValueError('raise_exception argument \"%s\" is not a bool' %str(raise_exception))\n    # check the type of the variable against the list\n    # then raise an exception or return True\n    if not len([(t) for t in expected_type if isinstance(variable, t)]):\n        error_message = '\"%s\" is not an instance of type %s. It is of type %s' %(str(variable),' or '.join([str(t) for t in expected_type]), str(type(variable)))\n        if raise_exception:\n            raise ValueError(error_message)\n        else:\n            return False, error_message\n    return True, None", "response": "Return True if a variable is of a certain type or types."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies the stimulus before closing warns user with a dialog if there are problems", "response": "def closeEvent(self, event):\n        \"\"\"Verifies the stimulus before closing, warns user with a\n        dialog if there are any problems\"\"\"\n        self.ok.setText(\"Checking...\")\n        QtGui.QApplication.processEvents()\n        self.model().cleanComponents()\n        self.model().purgeAutoSelected()\n        msg = self.model().verify()\n        if not msg:\n            msg = self.model().warning()\n        if msg:\n            warnBox = QtGui.QMessageBox( QtGui.QMessageBox.Warning, 'Warning - Invalid Settings', '{}. Do you want to change this?'.format(msg) )\n            yesButton = warnBox.addButton(self.tr('Edit'), QtGui.QMessageBox.YesRole)\n            noButton = warnBox.addButton(self.tr('Ignore'), QtGui.QMessageBox.NoRole)\n            warnBox.exec_()\n            if warnBox.clickedButton() == yesButton:\n                event.ignore()\n        self.ok.setText(\"OK\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen the WebSocket and start consuming messages.", "response": "def open(self, callback, instance=None, processor=None):\n        \"\"\"\n        Begin consuming messages.\n\n        :param string instance: (Optional) instance to use in the WebSocket URL\n        :param string processor: (Optional) processor to use in the WebSocket URL\n        \"\"\"\n        assert not self._closed\n\n        ws_url = self._client.ws_root\n        if instance:\n            ws_url += '/' + instance\n            if processor:\n                ws_url += '/' + processor\n\n        self._callback = callback\n        self._websocket = websocket.WebSocketApp(\n            ws_url,\n            on_open=self._on_websocket_open,\n            on_message=self._on_websocket_message,\n            on_error=self._on_websocket_error,\n            subprotocols=['protobuf'],\n            header=[\n                '{}: {}'.format(k, self._client.session.headers[k])\n                for k in self._client.session.headers\n            ],\n        )\n        self._consumer = threading.Thread(target=self._websocket.run_forever)\n\n        # Running this as a daemon thread improves possibilities\n        # for consumers of our API to control shutdown.\n        # (example: can just use time.sleep on the main thread instead of blocking on the future)\n        self._consumer.daemon = True\n\n        self._consumer.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the consumer and websocket.", "response": "def close(self, reason=None):\n        \"\"\"\n        Stop consuming messages and perform an orderly shutdown.\n\n        If ``reason`` is None, then this is considered a regular close.\n        \"\"\"\n        with self._closing:\n            if self._closed:\n                return\n\n            self._websocket.close()\n\n            self._consumer.join()\n            self._consumer = None\n\n            self._websocket = None\n            self._closed = True\n            for cb in self._close_callbacks:\n                cb(self, reason)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the plugin :raises: errors.PluginInitError", "response": "def _load(self, ):\n        \"\"\"Loads the plugin\n\n        :raises: errors.PluginInitError\n        \"\"\"\n        super(JB_MayaPlugin, self)._load()\n        try:\n            if not jukeboxmaya.STANDALONE_INITIALIZED:\n                self.init_ui()\n        except Exception:\n            log.exception(\"Load Ui failed!\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unload(self, ):\n        super(JB_MayaPlugin, self)._unload()\n        try:\n            if not jukeboxmaya.STANDALONE_INITIALIZED:\n                self.uninit_ui()\n        except Exception:\n            log.exception(\"Unload Ui failed!\")", "response": "Unloads the plugin\n\n        :raises: errors.PluginUninitError"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nemits the itemTrashed signal", "response": "def dropEvent(self, event):\n        \"\"\"Emits the itemTrashed signal, data contained in drag \n        operation left to be garbage collected\"\"\"\n        super(TrashWidget, self).dropEvent(event)\n        self.itemTrashed.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef choose(self, choose_from):\n\n        for choice in self.elements:\n            if choice in choose_from:\n                return ImplementationChoice(choice, choose_from[choice])\n        raise LookupError(self.elements, choose_from.keys())", "response": "given a mapping of implementations\n        choose one based on the current settings\n        returns a key value pair containing the key value pair of the implementation class and the value of the implementation class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving unnecessary temporary files generated by the pipeline", "response": "def remove(self):\n        \"\"\"Removes unnecessary temporary files generated by the pipeline\"\"\"\n        import shutil\n        printtime('Removing large and/or temporary files', self.start)\n        removefolder = list()\n        for sample in self.metadata:\n            # Use os.walk to iterate through all the files in the sample output directory\n            for path, dirs, files in os.walk(sample.general.outputdirectory):\n                for item in files:\n                    # Use regex to find files to remove\n                    if re.search(\".fastq$\", item) or re.search(\".fastq.gz$\", item) or re.search(\".bam$\", item) \\\n                            or re.search(\".bt2$\", item) or re.search(\".tab$\", item) or re.search(\"^before\", item) \\\n                            or re.search(\"^baitedtargets\", item) or re.search(\"_combined.csv$\", item) \\\n                            or re.search(\"^scaffolds\", item) or re.search(\".fastg$\", item) or re.search(\".gfa$\", item) \\\n                            or re.search(\".bai$\", item) or 'coregenome' in path or 'prophages' in path:\n                        # Keep the baitedtargets.fa, core genome, and merged metagenome files\n                        if item != 'baitedtargets.fa' and not re.search(\"coregenome\", item) \\\n                                and not re.search(\"paired\", item):\n                            # Remove the unnecessary files\n                            try:\n                                os.remove(os.path.join(path, item))\n                            except IOError:\n                                pass\n        # Clear out the folders\n        for folder in removefolder:\n            try:\n                shutil.rmtree(folder)\n            except (OSError, TypeError):\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets info from logged account", "response": "def load_info(self):\n        ''' Get info from logged account '''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/login.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/team_news.phtml',headers=headers).content\n        soup = BeautifulSoup(req)\n        self.title = soup.title.string\n        \n        estado = soup.find('div',{'id':'content'}).find('div',{'id':'manager'}).string\n        if estado:\n            print estado.strip()\n            return\n\n        [s.extract() for s in soup('strong')]\n        if (soup.find('div',{'id':'userid'}) != None):\n            self.myid = soup.find('div',{'id':'userid'}).p.text.strip()[2:]\n            self.money = int(soup.find('div',{'id':'manager_money'}).p.text.strip().replace(\".\",\"\")[:-2])\n            self.teamvalue = int(soup.find('div',{'id':'teamvalue'}).p.text.strip().replace(\".\",\"\")[:-2])\n            self.community_id = soup.find('link')['href'][24:]\n            self.username = soup.find('div',{'id':'username'}).p.a.text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_news(self):\n        '''Get all the news from first page'''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/login.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/team_news.phtml',headers=headers).content\n        soup = BeautifulSoup(req)\n        news = []\n        for i in soup.find_all('div',{'class','article_content_text'}):\n            news.append(i.text)\n        return news", "response": "Get all the news from first page"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget standings from the community s account", "response": "def standings(self):\n        '''Get standings from the community's account'''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/standings.phtml',headers=headers).content\n        soup = BeautifulSoup(req)\n        table = soup.find('table',{'id':'tablestandings'}).find_all('tr')\n        clasificacion = []\n        [clasificacion.append(('%s\\t%s\\t%s\\t%s\\t%s')%(tablas.find('td').text,tablas.find('div')['id'],tablas.a.text,tablas.find_all('td')[3].text,tablas.find_all('td')[4].text)) for tablas in table[1:]]\n        return clasificacion"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef info_user(self,userid):\n        '''Get user info using a ID'''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/standings.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/playerInfo.phtml?pid='+userid,headers=headers).content\n        soup = BeautifulSoup(req)\n        title = soup.title.string\n        community = soup.find_all('table',border=0)[1].a.text\n        info = []\n        info.append(title)\n        info.append(community)\n        for i in soup.find_all('table',border=0)[1].find_all('td')[1:]:\n            info.append(i.text)\n        for i in soup.find('table',cellpadding=2).find_all('tr')[1:]:\n            cad = i.find_all('td')\n            numero=cad[0].text\n            nombre=cad[2].text.strip()\n            team=cad[3].find('img')['alt']\n            precio=cad[4].text.replace(\".\",\"\")\n            puntos=cad[5].text\n            posicion=cad[6].text\n            info.append([numero,nombre,team,precio,puntos,posicion])\n        return info", "response": "Get user info using a ID"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lineup_user(self,userid):\n        '''Get user lineup using a ID'''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/standings.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/playerInfo.phtml?pid='+userid,headers=headers).content\n        soup = BeautifulSoup(req)\n        info = []\n        for i in soup.find_all('td',{'class':'name_cont'}):\n            info.append(i.text.strip())\n        return info", "response": "Get user lineup using a ID"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets comunity info using a ID", "response": "def info_community(self,teamid):\n        '''Get comunity info using a ID'''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/standings.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/teamInfo.phtml?tid='+teamid,headers=headers).content\n        soup = BeautifulSoup(req)\n        info = []\n        for i in soup.find('table',cellpadding=2).find_all('tr')[1:]:\n            info.append('%s\\t%s\\t%s\\t%s\\t%s'%(i.find('td').text,i.find('a')['href'].split('pid=')[1],i.a.text,i.find_all('td')[2].text,i.find_all('td')[3].text))\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget info from a football player using a ID", "response": "def info_player(self,pid):\n        ''''\n        Get info football player using a ID\n        @return: [name,position,team,points,price]\n        '''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/team_news.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/tradableInfo.phtml?tid='+pid,headers=headers).content\n        soup = BeautifulSoup(req)\n        info = []\n        info.append(soup.title.text.strip())\n        for i in soup.find('table',cellspacing=1).find_all('tr'):\n            info.append(i.find_all('td')[1].text.replace(\".\",\"\"))\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting id using name football player", "response": "def info_player_id(self,name):\n        '''Get id using name football player'''\n        number = 0\n        name=name.title().replace(\" \", \"+\")\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/team_news.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://stats.comunio.es/search.php?name='+name,headers=headers).content\n        soup = BeautifulSoup(req)\n        for i in soup.find_all('a',{'class','nowrap'}):\n            number = re.search(\"([0-9]+)-\", str(i)).group(1)\n            break # Solo devuelve la primera coincidencia\n        return number"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget info by real team using a ID", "response": "def club(self,cid):\n        '''\n        Get info by real team using a ID\n        @return: name,[player list]\n        '''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/clubInfo.phtml?cid='+cid,headers=headers).content\n        soup = BeautifulSoup(req)\n        plist = []\n        for i in soup.find('table',cellpadding=2).find_all('tr')[1:]:\n            plist.append('%s\\t%s\\t%s\\t%s\\t%s'%(i.find_all('td')[0].text,i.find_all('td')[1].text,i.find_all('td')[2].text,i.find_all('td')[3].text,i.find_all('td')[4].text))\n        return soup.title.text,plist"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef team_id(self,team):\n        '''\n        Get team ID using a real team name\n        @return: id\n        '''\n        #UTF-8 comparison\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain,headers=headers).content\n        soup = BeautifulSoup(req)\n        for i in soup.find('table',cellpadding=2).find_all('tr'):\n            #Get teamid from the bets\n            team1 = i.find('a')['title']\n            team2 = i.find_all('a')[1]['title']\n            if (team == team1):\n                return i.find('a')['href'].split('cid=')[1]\n            elif (team == team2):\n                return i.find_all('a')[1]['href'].split('cid=')[1]\n        return None", "response": "Get team ID using a real team name\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets userid from a name", "response": "def user_id(self, user):\n        ''' \n        Get userid from a name\n        @return: id\n        '''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/team_news.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/standings.phtml',headers=headers).content\n        soup = BeautifulSoup(req)\n        for i in soup.find('table',cellpadding=2).find_all('tr'):\n            try:\n                if (user == i.find_all('td')[2].text.encode('utf8')):\n                    return  i.find('a')['href'].split('pid=')[1]\n            except:\n                continue\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the football players currently on sale.", "response": "def players_onsale(self, community_id, only_computer=False):\n        '''\n        Returns the football players currently on sale\n        @return: [[name, team, min_price, market_price, points, date, owner, position]]\n        '''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/team_news.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/teamInfo.phtml?tid=' + community_id, headers=headers).content\n        soup = BeautifulSoup(req)\n        \n        current_year = dt.today().year\n        current_month = dt.today().month\n        on_sale = []\n        year_flag = 0\n        for i in soup.find_all('table',{'class','tablecontent03'})[2].find_all('tr')[1:]:\n            name = i.find_all('td')[0].text.strip()\n            team = i.find('img')['alt']\n            min_price = i.find_all('td')[2].text.replace(\".\",\"\").strip()\n            market_price = i.find_all('td')[3].text.replace(\".\",\"\").strip()\n            points = i.find_all('td')[4].text.strip().strip()\n            # Controlamos el cambio de a\u00f1o, ya que comunio no lo d\u00e1\n            if current_month <= 7 and int(i.find_all('td')[5].text[3:5]) > 7:\n                year_flag = 1\n            date = str(current_year-year_flag)+i.find_all('td')[5].text[3:5]+i.find_all('td')[5].text[:2]\n            owner = i.find_all('td')[6].text.strip()\n            position = i.find_all('td')[7].text.strip()\n            # Comprobamos si solamente queremos los de la computadora o no\n            if (only_computer and owner == 'Computer') or not only_computer:\n                on_sale.append([name, team, min_price, market_price, points, date, owner, position])\n\n        return on_sale"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting bids made to you", "response": "def bids_to_you(self):\n        '''\n        Get bids made to you\n        @return: [[player,owner,team,money,date,datechange,status],]\n        '''\n        headers = {\"Content-type\": \"application/x-www-form-urlencoded\",\"Accept\": \"text/plain\",'Referer': 'http://'+self.domain+'/team_news.phtml',\"User-Agent\": user_agent}\n        req = self.session.get('http://'+self.domain+'/exchangemarket.phtml?viewoffers_x=',headers=headers).content\n        soup = BeautifulSoup(req)\n        table = []\n        for i in soup.find('table',{'class','tablecontent03'}).find_all('tr')[1:]:\n            player,owner,team,price,bid_date,trans_date,status = self._parse_bid_table(i)\n            table.append([player,owner,team,price,bid_date,trans_date,status])\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert table row values into strings", "response": "def _parse_bid_table(self, table):\n        '''\n        Convert table row values into strings\n        @return: player, owner, team, price, bid_date, trans_date, status\n        '''\n        player = table.find_all('td')[0].text\n        owner = table.find_all('td')[1].text\n        team = table.find('img')['alt']\n        price = int(table.find_all('td')[3].text.replace(\".\",\"\"))\n        bid_date = table.find_all('td')[4].text\n        trans_date = table.find_all('td')[5].text\n        status = table.find_all('td')[6].text\n        return player,owner,team,price,bid_date,trans_date,status"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        while not self.stopped.isSet():\n            try:\n                # if the current state is idle, just block and wait forever\n                # if the current state is any other state, then a timeout of 200ms should\n                # be reasonable in all cases.\n                timeout = (self._state != 'idle') and 0.2 or None\n                rdlist, _, _ = select.select([self._socket.fileno()], [], [], timeout)\n                if not rdlist:\n                    if self._state != 'idle':\n                        self._state = 'idle'\n                    continue\n                data = self._socket.recv(1024)\n                if not data:\n                    # check if the socket is still valid\n                    try:\n                        os.fstat(recv._socket.fileno())\n                    except socket.error:\n                        break\n                    continue\n                code = utils.mangleIR(data, ignore_errors=True)\n                codeName = self.codeMap.get(code)\n                # some manufacturers repeat their IR codes several times in rapid\n                # succession. by tracking the last code, we can eliminate redundant\n                # state changes\n                if codeName and (self._state != codeName):\n                    self._state = codeName\n                    for callback in self._callbacks:\n                        callback(codeName)\n            except:\n                time.sleep(0.1)", "response": "Main loop of KIRA thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms an array of 2d coords into 4d", "response": "def convert_vec2_to_vec4(scale, data):\n    \"\"\"transforms an array of 2d coords into 4d\"\"\"\n    it = iter(data)\n    while True:\n        yield next(it) * scale  # x\n        yield next(it) * scale  # y\n        yield 0.0       # z\n        yield 1.0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_edges(self, excludes):\n        edges = []\n        MEW = 100.0\n        if excludes is None:\n            excludes = [0] * len(self.indices) * 2\n        for i in range(0, len(self.indices), 3):  # each triangle\n            i0 = self.indices[i+0] * 4\n            i1 = self.indices[i+1] * 4\n            i2 = self.indices[i+2] * 4\n            e0 = excludes[i+0]\n            e1 = excludes[i+1]\n            e2 = excludes[i+2]\n            p0 = self.vertices[i0:i0+4]\n            p1 = self.vertices[i1:i1+4]\n            p2 = self.vertices[i2:i2+4]\n            v0 = self.vec2minus(p2, p1)\n            v1 = self.vec2minus(p2, p0)\n            v2 = self.vec2minus(p1, p0)\n            area = fabs(v1[0]*v2[1] - v1[1] * v2[0])\n            c0 = (area/self.magnitude(v0), e1 * MEW, e2 * MEW)\n            c1 = (e0 * MEW, area/self.magnitude(v1), e2 * MEW)\n            c2 = (e0 * MEW, e1 * MEW, area/self.magnitude(v2))\n            edges.extend(p0)\n            edges.extend(c0)\n            edges.extend(p1)\n            edges.extend(c1)\n            edges.extend(p2)\n            edges.extend(c2)\n        return create_vertex_buffer(edges)", "response": "Calculates the edges of the turtle with barycentric coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bin(self, arch='x86'):\n        bin_dir = os.path.join(self.vc_dir, 'bin')\n        if arch == 'x86':\n            arch = ''\n        cl_path = os.path.join(bin_dir, arch, 'cl.exe')\n        link_path = os.path.join(bin_dir, arch, 'link.exe')\n        ml_name = 'ml.exe'\n        if arch in ['x86_amd64', 'amd64']:\n            ml_name = 'ml64.exe'\n        ml_path = os.path.join(bin_dir, arch, ml_name)\n        if os.path.isfile(cl_path) \\\n           and os.path.isfile(link_path) \\\n           and os.path.isfile(ml_path):\n            logging.info(_('using cl.exe: %s'), cl_path)\n            logging.info(_('using link.exe: %s'), link_path)\n            logging.info(_('using %s: %s'), ml_name, ml_path)\n            run_cl = partial(run_program, cl_path)\n            run_link = partial(run_program, link_path)\n            run_ml = partial(run_program, ml_path)\n            return self.Bin(run_cl, run_link, run_ml)\n        logging.debug(_('cl.exe not found: %s'), cl_path)\n        logging.debug(_('link.exe not found: %s'), link_path)\n        logging.debug(_('%s not found: %s'), ml_name, ml_path)\n        return self.Bin(None, None, None)", "response": "Get the binaries of Visual C ++."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_inc(self):\n        dirs = []\n        for part in ['', 'atlmfc']:\n            include = os.path.join(self.vc_dir, part, 'include')\n            if os.path.isdir(include):\n                logging.info(_('using include: %s'), include)\n                dirs.append(include)\n            else:\n                logging.debug(_('include not found: %s'), include)\n        return dirs", "response": "Get include directories of Visual C ++."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_lib(self, arch='x86'):\n        if arch == 'x86':\n            arch = ''\n        if arch == 'x64':\n            arch = 'amd64'\n        lib = os.path.join(self.vc_dir, 'lib', arch)\n        if os.path.isdir(lib):\n            logging.info(_('using lib: %s'), lib)\n            return [lib]\n        logging.debug(_('lib not found: %s'), lib)\n        return []", "response": "Get lib directories of Visual C ++."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget bin and lib.", "response": "def get_bin_and_lib(self, x64=False, native=False):\n        \"\"\"\n        Get bin and lib.\n        \"\"\"\n        if x64:\n            msvc = self.bin64\n            paths = self.lib64\n        else:\n            msvc = self.bin32\n            paths = self.lib\n        if native:\n            arch = 'x64' if x64 else 'x86'\n            paths += self.sdk.get_lib(arch, native=True)\n        else:\n            attr = 'lib64' if x64 else 'lib'\n            paths += getattr(self.sdk, attr)\n        return msvc, paths"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_inc(self, native=False):\n        if self.sdk_version == 'v7.0A':\n            include = os.path.join(self.sdk_dir, 'include')\n            if os.path.isdir(include):\n                logging.info(_('using include: %s'), include)\n                return [include]\n            logging.debug(_('include not found: %s'), include)\n            return []\n        if self.sdk_version == 'v8.1':\n            dirs = []\n            if native:\n                parts = ['km', os.path.join('km', 'crt'), 'shared']\n            else:\n                parts = ['um', 'winrt', 'shared']\n            for part in parts:\n                include = os.path.join(self.sdk_dir, 'include', part)\n                if os.path.isdir(include):\n                    logging.info(_('using include: %s'), include)\n                    dirs.append(include)\n                else:\n                    logging.debug(_('inc not found: %s'), include)\n            return dirs\n        if self.sdk_version == 'v10.0':\n            dirs = []\n            extra = os.path.join('include', '10.0.10240.0')\n            for mode in ['um', 'ucrt', 'shared', 'winrt']:\n                include = os.path.join(self.sdk_dir, extra, mode)\n                if os.path.isdir(include):\n                    logging.info(_('using include: %s'), include)\n                    dirs.append(include)\n                else:\n                    logging.debug(_('inc not found: %s'), include)\n            return dirs\n        message = 'unknown sdk version: {}'.format(self.sdk_version)\n        raise RuntimeError(message)", "response": "Get include directories of Windows SDK."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_lib(self, arch='x86', native=False):\n        if self.sdk_version == 'v7.0A':\n            if arch == 'x86':\n                arch = ''\n            lib = os.path.join(self.sdk_dir, 'lib', arch)\n            if os.path.isdir(lib):\n                logging.info(_('using lib: %s'), lib)\n                return [lib]\n            logging.debug(_('lib not found: %s'), lib)\n            return []\n        if self.sdk_version == 'v8.1':\n            if native:\n                extra = os.path.join('winv6.3', 'km')\n            else:\n                extra = os.path.join('winv6.3', 'um')\n            lib = os.path.join(self.sdk_dir, 'lib', extra, arch)\n            if os.path.isdir(lib):\n                logging.info(_('using lib: %s'), lib)\n                return [lib]\n            logging.debug(_('lib not found: %s'), lib)\n            return []\n        if self.sdk_version == 'v10.0':\n            dirs = []\n            extra = os.path.join('lib', '10.0.10240.0')\n            for mode in ['um', 'ucrt']:\n                lib = os.path.join(self.sdk_dir, extra, mode, arch)\n                if os.path.isdir(lib):\n                    logging.info(_('using lib: %s'), lib)\n                    dirs.append(lib)\n                else:\n                    logging.debug(_('lib not found: %s'), lib)\n            return dirs\n        message = 'unknown sdk version: {}'.format(self.sdk_version)\n        raise RuntimeError(message)", "response": "Get lib directories of Windows SDK."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_tool_dir():\n        def _is_comntools(name):\n            return re.match(r'vs\\d+comntools', name.lower())\n\n        def _get_version_from_name(name):\n            return (re.search(r'\\d+', name).group(0), name)\n\n        names = [name for name in os.environ if _is_comntools(name)]\n        logging.debug(_('found vscomntools: %s'), names)\n        versions = [_get_version_from_name(name) for name in names]\n        logging.debug(_('extracted versions: %s'), versions)\n        try:\n            version = max(versions)\n        except ValueError:\n            raise OSError(_('Failed to find the VSCOMNTOOLS. '\n                            'Have you installed Visual Studio?'))\n        else:\n            logging.info(_('using version: %s'), version)\n            vscomntools = os.environ[version[1]]\n            logging.info(_('using vscomntools: %s'), vscomntools)\n            return vscomntools", "response": "Get the directory of Visual Studio from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_vs_dir_from_tool_dir(self):\n        index = self.tool_dir.find(r'Common7\\Tools')\n        return self.tool_dir[:index]", "response": "Get the directory of Visual Studio\n        from the directory Tools."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets Visual C ++ directory from Visual Studio directory.", "response": "def get_vc_dir_from_vs_dir(self):\n        \"\"\"\n        Get Visual C++ directory from Visual Studio directory.\n        \"\"\"\n        vc_dir = os.path.join(self.vs_dir, 'vc')\n        if os.path.isdir(vc_dir):\n            logging.info(_('using vc: %s'), vc_dir)\n            return vc_dir\n        logging.debug(_('vc not found: %s'), vc_dir)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sdk_version(self):\n        name = 'VCVarsQueryRegistry.bat'\n        path = os.path.join(self.tool_dir, name)\n        batch = read_file(path)\n        if not batch:\n            raise RuntimeError(_('failed to find the SDK version'))\n        regex = r'(?<=\\\\Microsoft SDKs\\\\Windows\\\\).+?(?=\")'\n        try:\n            version = re.search(regex, batch).group()\n        except AttributeError:\n            return ''\n        else:\n            logging.debug(_('SDK version: %s'), version)\n            return version", "response": "Get the version of Windows SDK\n            from VCVarsQueryRegistry. bat."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the directory of Windows SDK from registry.", "response": "def get_sdk_dir(self):\n        \"\"\"Get the directory of Windows SDK from registry.\"\"\"\n        if not WINREG:\n            return ''\n        path = r'\\Microsoft\\Microsoft SDKs\\Windows'\n        for node in ['SOFTWARE', r'SOFTWARE\\Wow6432Node']:\n            for hkey in [HKEY_LOCAL_MACHINE, HKEY_CURRENT_USER]:\n                sub_key = node + path + '\\\\' + self.sdk_version\n                try:\n                    key = OpenKey(hkey, sub_key)\n                except OSError:\n                    logging.debug(_('key not found: %s'), sub_key)\n                    continue\n                else:\n                    logging.info(_('using key: %s'), sub_key)\n                    value_name = 'InstallationFolder'\n                    try:\n                        value = QueryValueEx(key, value_name)\n                    except OSError:\n                        return ''\n                    logging.info(_('using dir: %s'), value[0])\n                    return value[0]\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(self):\n        logging.info('Preparing metadata')\n        # If this script is run as part of a pipeline, the metadata objects will already exist\n        if not self.metadata:\n            self.filer()\n        else:\n            self.objectprep()\n        # Use the number of metadata objects to calculate the number of cores to use per sample in multi-threaded\n        # methods with sequence calls to multi-threaded applications\n        try:\n            self.threads = int(self.cpus / len(self.metadata)) if self.cpus / len(\n                self.metadata) > 1 else 1\n        except (TypeError, ZeroDivisionError):\n            self.threads = self.cpus\n        logging.info('Reading and formatting primers')\n        self.primers()\n        logging.info('Baiting .fastq files against primers')\n        self.bait()\n        logging.info('Baiting .fastq files against previously baited .fastq files')\n        self.doublebait()\n        logging.info('Assembling contigs from double-baited .fastq files')\n        self.assemble_amplicon_spades()\n        logging.info('Creating BLAST database')\n        self.make_blastdb()\n        logging.info('Running BLAST analyses')\n        self.blastnthreads()\n        logging.info('Parsing BLAST results')\n        self.parseblast()\n        logging.info('Clearing amplicon files from previous iterations')\n        self.ampliconclear()\n        logging.info('Creating reports')\n        self.reporter()", "response": "This is the main function of the main function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filer(self):\n        # Get a list of all the .fastq files in the sequence path\n        fastq = sorted(glob(os.path.join(self.sequencepath, '*fastq*')))\n        # Initialise a list to store .fasta formatted files\n        fasta = list()\n        # Get all the files in the sequence path\n        for sequencefile in glob(os.path.join(self.sequencepath, '*')):\n            # Split the extension from the file\n            extension = os.path.splitext(sequencefile)[1]\n            # If the extension is one of the acceptable FASTA extensions, add it to the list of FASTA files\n            if extension in self.extensions:\n                fasta.append(sequencefile)\n        # Use filer from accessoryFunctions to find the base file name of each .fastq file/pair\n        fastqnames = filer(fastq)\n        # Get all the .fastq files into metadata objects\n        for fastqname in sorted(fastqnames):\n            # Create the metadata object\n            sample = MetadataObject()\n            sample.name = os.path.basename(fastqname)\n            # Create an attribute for the current analysis\n            setattr(sample, self.analysistype, GenObject())\n            # Set the destination folder\n            sample[self.analysistype].outputdir = os.path.join(self.path, fastqname)\n            # Make the destination folder\n            make_path(sample[self.analysistype].outputdir)\n            # Get the fastq files specific to the fastqname\n            specificfastq = glob(os.path.join(self.sequencepath, '{fq}*.fastq*'.format(fq=fastqname)))\n            # Set the file type for the downstream analysis\n            sample[self.analysistype].filetype = 'fastq'\n            # Link the files to the output folder\n            for fastq in specificfastq:\n                relative_symlink(src_file=fastq,\n                                 output_dir=sample[self.analysistype].outputdir)\n            # Initialise the general and run categories\n            sample.general = GenObject()\n            # Populate the .fastqfiles category of :self.metadata\n            sample.general.fastqfiles = [fastq for fastq in\n                                         glob(os.path.join(sample[self.analysistype].outputdir, '{fq}*.fastq*'\n                                                           .format(fq=fastqname))) if 'trimmed' not in fastq]\n            # Populate certain attributes in order to be compatible with other local software\n            sample.general.bestassemblyfile = sample.general.fastqfiles\n            sample.general.trimmedcorrectedfastqfiles = sample.general.fastqfiles\n            sample.general.outputdirectory = sample[self.analysistype].outputdir\n            sample[self.analysistype].baitedfastq = os.path.join(\n                sample[self.analysistype].outputdir,\n                '{at}_targetMatches.fastq.gz'.format(at=self.analysistype))\n            # Append the metadata to the list of samples\n            self.metadata.append(sample)\n        # Add any .fasta formatted files to the metadata object\n        for fastafile in sorted(fasta):\n            sample = MetadataObject()\n            sample.name = os.path.splitext(os.path.split(fastafile)[-1])[0]\n            setattr(sample, self.analysistype, GenObject())\n            # Set the destination folder\n            sample[self.analysistype].outputdir = os.path.join(self.path, 'detailed_reports', sample.name)\n            # Make the destination folder\n            make_path(sample[self.analysistype].outputdir)\n            # Set the file type for the downstream analysis\n            sample[self.analysistype].filetype = 'fasta'\n            # Link the .fasta files to :self.path/:filename\n            relative_symlink(src_file=fastafile,\n                             output_dir=sample[self.analysistype].outputdir)\n            # Initialise the general and run categories\n            sample.general = GenObject()\n            # Populate the .fastqfiles category of :self.metadata\n            sample.general.fastqfiles = [fastq for fastq in glob(os.path.join(sample[self.analysistype].outputdir,\n                                                                              '{sn}*{ext}*'\n                                                                              .format(\n                                                                                  sn=sample.name,\n                                                                                  ext=os.path.splitext(fastafile)[1])))]\n            sample.general.bestassemblyfile = sample.general.fastqfiles\n            sample.general.trimmedcorrectedfastqfiles = sample.general.fastqfiles\n            sample.general.outputdirectory = sample[self.analysistype].outputdir\n            sample[self.analysistype].baitedfastq = os.path.join(\n                sample[self.analysistype].outputdir,\n                '{at}_targetMatches.fastq.gz'.format(at=self.analysistype))\n            sample[self.analysistype].assemblyfile = fastafile\n            # Append the metadata to the list of samples\n            self.metadata.append(sample)", "response": "Find the. fastq files and create a metadata object for each sample"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and populate the objects for the current analysis", "response": "def objectprep(self):\n        \"\"\"\n        If the script is being run as part of a pipeline, create and populate the objects for the current analysis\n        \"\"\"\n        for sample in self.metadata:\n            setattr(sample, self.analysistype, GenObject())\n            # Set the destination folder\n            sample[self.analysistype].outputdir = os.path.join(self.path, self.analysistype)\n            # Make the destination folder\n            make_path(sample[self.analysistype].outputdir)\n            sample[self.analysistype].baitedfastq = os.path.join(\n                sample[self.analysistype].outputdir,\n                '{at}_targetMatches.fastq.gz'.format(at=self.analysistype))\n            # Set the file type for the downstream analysis\n            sample[self.analysistype].filetype = self.filetype\n            if self.filetype == 'fasta':\n                sample[self.analysistype].assemblyfile = sample.general.bestassemblyfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef primers(self):\n        with open(self.formattedprimers, 'w') as formatted:\n            for record in SeqIO.parse(self.primerfile, 'fasta'):\n                # from https://stackoverflow.com/a/27552377 - find any degenerate bases in the primer sequence, and\n                # create all possibilities as a list\n                degenerates = Seq.IUPAC.IUPACData.ambiguous_dna_values\n                try:\n                    primerlist = list(map(\"\".join, product(*map(degenerates.get, str(record.seq)))))\n                except TypeError:\n                    print(\"Invalid Primer Sequence: {seq}\".format(seq=str(record.seq)))\n                    sys.exit()\n                # As the record.id is being updated in the loop below, set the name of the primer here so that will\n                # be able to be recalled when setting the new record.ids\n                primername = record.id\n                # Iterate through all the possible primers created from any degenerate bases\n                for index, primer in enumerate(primerlist):\n                    # Update the primer name with the position in the list to keep the name unique\n                    record.id = primername + '_{index}'.format(index=index)\n                    # Clear the description, as, otherwise, it will be added, and there will be duplicate information\n                    record.description = ''\n                    # Create a seqrecord from the primer sequence\n                    record.seq = Seq.Seq(primer)\n                    # Write the properly-formatted records to file\n                    SeqIO.write(record, formatted, 'fasta')\n                    # Populate a dictionary to store the length of the primers - will be used in determining whether\n                    # BLAST hits are full-length\n                    self.faidict[record.id] = len(str(record.seq))\n                    # Ensure that the kmer length used in the initial baiting is no larger than the shorted primer\n                    if len(str(record.seq)) < self.klength:\n                        self.klength = len(str(record.seq))", "response": "Read in the primer file and create a properly formatted output file that takes any degenerate bases in the primer file and create a properly formatted output file that takes any degenerate bases in the primer file that takes any degenerate bases in the primer file and the set of primers that are needed to create the new primer record."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbait FASTQ reads from input files using the primer file as the target.", "response": "def bait(self):\n        \"\"\"\n        Use bbduk to bait FASTQ reads from input files using the primer file as the target\n        \"\"\"\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                if sample.general.bestassemblyfile != 'NA':\n                    # Only need to perform baiting on FASTQ files\n                    if sample[self.analysistype].filetype == 'fastq':\n                        # Make the system call - allow for single- or paired-end reads\n                        if len(sample.general.fastqfiles) == 2:\n                            # Create the command to run the baiting - ref: primer file, k: shortest primer length\n                            # in1, in2: paired inputs, hdist: number of mismatches, interleaved: use interleaved output\n                            # outm: single, zipped output file of reads that match the target file\n                            sample[self.analysistype].bbdukcmd = \\\n                                'bbduk.sh ref={primerfile} k={klength} in1={forward} in2={reverse} ' \\\n                                'hdist={mismatches} threads={threads} interleaved=t outm={outfile}' \\\n                                .format(primerfile=self.formattedprimers,\n                                        klength=self.klength,\n                                        forward=sample.general.trimmedcorrectedfastqfiles[0],\n                                        reverse=sample.general.trimmedcorrectedfastqfiles[1],\n                                        mismatches=self.mismatches,\n                                        threads=str(self.cpus),\n                                        outfile=sample[self.analysistype].baitedfastq)\n                        else:\n                            sample[self.analysistype].bbdukcmd = \\\n                                'bbduk.sh ref={primerfile} k={klength} in={fastq} hdist={mismatches} ' \\\n                                'threads={threads} interleaved=t outm={outfile}' \\\n                                .format(primerfile=self.formattedprimers,\n                                        klength=self.klength,\n                                        fastq=sample.general.trimmedcorrectedfastqfiles[0],\n                                        mismatches=self.mismatches,\n                                        threads=str(self.cpus),\n                                        outfile=sample[self.analysistype].baitedfastq)\n                        # Run the system call (if necessary)\n                        if not os.path.isfile(sample[self.analysistype].baitedfastq):\n                            run_subprocess(sample[self.analysistype].bbdukcmd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbait the original fastq files for two primers.", "response": "def doublebait(self):\n        \"\"\"\n        In order to ensure that there is enough sequence data to bridge the gap between the two primers, the paired\n        .fastq files produced above will be used to bait the original input .fastq files\n        \"\"\"\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                if sample.general.bestassemblyfile != 'NA':\n                    if sample[self.analysistype].filetype == 'fastq':\n                        sample[self.analysistype].doublebaitedfastq = os.path.join(\n                            sample[self.analysistype].outputdir,\n                            '{at}_doubletargetMatches.fastq.gz'.format(at=self.analysistype))\n                        # Make the system call\n                        if len(sample.general.fastqfiles) == 2:\n                            # Create the command to run the baiting\n                            sample[self.analysistype].bbdukcmd2 = \\\n                                'bbduk.sh ref={baitfile} in1={forward} in2={reverse} hdist={mismatches} ' \\\n                                'threads={threads} interleaved=t outm={outfile}' \\\n                                .format(baitfile=sample[self.analysistype].baitedfastq,\n                                        forward=sample.general.trimmedcorrectedfastqfiles[0],\n                                        reverse=sample.general.trimmedcorrectedfastqfiles[1],\n                                        mismatches=self.mismatches,\n                                        threads=str(self.cpus),\n                                        outfile=sample[self.analysistype].doublebaitedfastq)\n                        else:\n                            sample[self.analysistype].bbdukcmd2 = \\\n                                'bbduk.sh ref={baitfile} in={fastq} hdist={mismatches} threads={threads} ' \\\n                                'interleaved=t outm={outfile}' \\\n                                .format(baitfile=sample[self.analysistype].baitedfastq,\n                                        fastq=sample.general.trimmedcorrectedfastqfiles[0],\n                                        mismatches=self.mismatches,\n                                        threads=str(self.cpus),\n                                        outfile=sample[self.analysistype].doublebaitedfastq)\n                        # Run the system call (if necessary)\n                        if not os.path.isfile(sample[self.analysistype].doublebaitedfastq):\n                            run_subprocess(sample[self.analysistype].bbdukcmd2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assemble_amplicon_skesa(self):\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # Initialise variables\n                sample[self.analysistype].skesa_outdir = os.path.join(\n                    sample[self.analysistype].outputdir, self.analysistype)\n                make_path(sample[self.analysistype].skesa_outdir)\n                sample[self.analysistype].assemblyfile = os.path.join(sample[self.analysistype].skesa_outdir,\n                                                                      'contigs.fasta')\n                # If there are two fastq files\n                if len(sample.general.fastqfiles) == 2:\n                    # Set the reverse fastq name https://github.com/ncbi/SKESA/issues/7\n                    sample[self.analysistype].spadescommand = 'skesa --fastq {fastqfiles} --cores {threads} ' \\\n                                                              '--use_paired_ends --vector_percent 1 ' \\\n                                                              '--contigs_out {contigs}'\\\n                        .format(fastqfiles=','.join(sample.general.fastqfiles),\n                                threads=self.cpus,\n                                contigs=sample[self.analysistype].assemblyfile)\n                # Same as above, but use single read settings for the assembler\n                else:\n                    sample[self.analysistype].spadescommand = 'skesa --fastq {fastqfiles} --cores {threads} ' \\\n                                                              '--vector_percent 1 --contigs_out {contigs}'\\\n                        .format(fastqfiles=','.join(sample.general.fastqfiles),\n                                threads=self.cpus,\n                                contigs=sample[self.analysistype].assemblyfile)\n                if not os.path.isfile(sample[self.analysistype].assemblyfile):\n                    # Create and run the sequence call as required\n                    run_subprocess(sample[self.analysistype].spadescommand)\n                    if not os.path.isfile(sample[self.analysistype].assemblyfile):\n                        sample[self.analysistype].assemblyfile = 'NA'", "response": "Assemble the genomes from the skesa file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assemble_amplicon_spades(self):\n        for _ in self.metadata:\n            # Send the threads to the merge method. :args is empty as I'm using\n            threads = Thread(target=self.assemble, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA':\n                if sample[self.analysistype].filetype == 'fastq':\n                    sample[self.analysistype].spadesoutput = os.path.join(\n                        sample[self.analysistype].outputdir, self.analysistype)\n                    # Removed --careful, as there was an issue with the .fastq reads following baiting - something to\n                    # do with the names, or the interleaving. Subsequent testing showed no real changes to assemblies\n                    if len(sample.general.fastqfiles) == 2:\n                        sample[self.analysistype].spadescommand = \\\n                            'spades.py -k {klength} --only-assembler --12 {fastq} -o {outfile} -t {threads}'\\\n                            .format(klength=self.kmers,\n                                    fastq=sample[self.analysistype].doublebaitedfastq,\n                                    outfile=sample[self.analysistype].spadesoutput,\n                                    threads=self.threads)\n                    else:\n                        sample[self.analysistype].spadescommand = \\\n                            'spades.py -k {klength} --only-assembler -s {fastq} -o {outfile} -t {threads}' \\\n                            .format(klength=self.kmers,\n                                    fastq=sample[self.analysistype].doublebaitedfastq,\n                                    outfile=sample[self.analysistype].spadesoutput,\n                                    threads=self.threads)\n                    sample[self.analysistype].assemblyfile = os.path.join(sample[self.analysistype].spadesoutput,\n                                                                          'contigs.fasta')\n                    self.queue.put(sample)\n        self.queue.join()", "response": "Assemble the amplicons using SPAdes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a BLAST database of the primer file InMinutes", "response": "def make_blastdb(self):\n        \"\"\"\n        Create a BLAST database of the primer file\n        \"\"\"\n        # remove the path and the file extension for easier future globbing\n        db = os.path.splitext(self.formattedprimers)[0]\n        nhr = '{db}.nhr'.format(db=db)  # add nhr for searching\n        if not os.path.isfile(str(nhr)):\n            # Create the databases\n            command = 'makeblastdb -in {primerfile} -parse_seqids -max_file_sz 2GB -dbtype nucl -out {outfile}'\\\n                .format(primerfile=self.formattedprimers,\n                        outfile=db)\n            run_subprocess(command)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the BLAST results produced above.", "response": "def parseblast(self):\n        \"\"\"\n        Parse the BLAST results produced above. Find primer pairs with full-length hits with mismatches equal or\n        lesser than the cutoff value\n        \"\"\"\n        for sample in self.metadata:\n            if sample.general.bestassemblyfile != 'NA' and sample[self.analysistype].assemblyfile != 'NA':\n                # Initialise variables\n                sample[self.analysistype].blastresults = dict()\n                sample[self.analysistype].contigs = dict()\n                sample[self.analysistype].hits = dict()\n                sample[self.analysistype].mismatches = dict()\n                sample[self.analysistype].blastrecords = list()\n                sample[self.analysistype].range = dict()\n                sample[self.analysistype].genespresent = dict()\n                # Open blast output csv file\n                csvfile = open(sample[self.analysistype].report)\n                # Skip header\n                csvfile.readline()\n                # Open the sequence profile file as a dictionary\n                blastdict = DictReader(csvfile, fieldnames=self.fieldnames, dialect='excel-tab')\n\n                # Go through each BLAST result\n                for row in blastdict:\n                    # Ensure that the hit is full-length, and that the number of mismatches is equal to or lesser\n                    # than the supplied cutoff value\n                    if int(row['alignment_length']) == self.faidict[row['subject_id']] and \\\n                                    int(row['mismatches']) <= self.mismatches:\n                        # Add the current row to the list for future work\n                        sample[self.analysistype].blastrecords.append(row)\n                        # Populate the dictionaries with the contig name (e.g. CA_CFIA-515_NODE_1_length_1791),\n                        # the gene name (e.g. vtx2a), and the primer name (e.g. vtx2a-R3_1) as required\n                        # accounts for primer names with \"-\" in addition to the terminal \"-F\" or \"-R\"\n                        try:\n                            sample[self.analysistype].blastresults[row['query_id']].add(row['subject_id'])\n                            sample[self.analysistype].contigs[row['query_id']].add('-'.join(row['subject_id']\n                                                                                            .split('-')[:-1]))\n                        except KeyError:\n                            sample[self.analysistype].blastresults[row['query_id']] = set()\n                            sample[self.analysistype].blastresults[row['query_id']].add(row['subject_id'])\n                            sample[self.analysistype].contigs[row['query_id']] = set()\n                            sample[self.analysistype].contigs[row['query_id']].add('-'.join(row['subject_id']\n                                                                                            .split('-')[:-1]))\n                # Check to see if both forward and reverse primers are present for a particular gene within a contig\n                for contig, genes in sample[self.analysistype].contigs.items():\n                    # Split off the primer details (e.g. vtx2a-R3_1 -> vtx2a-R) from the blast results dictionary in\n                    # order to create a searchable list of primers\n                    # accounts for primer names with \"-\" in addition to the terminal \"-F\" or \"-R\"\n                    reformatted = {'-'.join(['-'.join(x.split('-')[:-1]), x.split('-')[-1][0]])\n                                   for x in sample[self.analysistype].blastresults[contig]}\n                    # Iterate through the list of genes to check if primers are present\n                    for gene in genes:\n                        # Add -F and -R to the gene, and ensure that both options are in the reformatted list of genes\n                        # e.g. vtx2a-F and vtx2a-R in [vtx1a-R, vtx2c-F ,vtx2a-F, vtx2a-R]\n                        if gene + '-F' in reformatted and gene + '-R' in reformatted:\n                            # Extract the precise primers from the dictionary e.g. vtx2a use to\n                            # find vtx2a-F2_4 (forward) and vtx2a-R3_1 (reverse)\n                            forwardprimers = [primer for primer in sample[self.analysistype].blastresults[contig]\n                                              if gene == primer.split('-F')[0]]\n                            reverseprimers = [primer for primer in sample[self.analysistype].blastresults[contig]\n                                              if gene == primer.split('-R')[0]]\n                            # Concatenate the lists\n                            primers = forwardprimers + reverseprimers\n                            # Populate the dictionary with the primers\n                            try:\n                                sample[self.analysistype].hits[contig].append(primers)\n                            except KeyError:\n                                sample[self.analysistype].hits[contig] = list()\n                                sample[self.analysistype].hits[contig].append(primers)\n\n                            for record in sample[self.analysistype].blastrecords:\n                                for primer in primers:\n                                    if record['query_id'] == contig and record['subject_id'] == primer:\n                                        # Populate the dictionary with the primers\n                                        try:\n                                            sample[self.analysistype].mismatches[contig][gene]\\\n                                                .update({primer: int(record['mismatches'])})\n                                        except KeyError:\n                                            try:\n                                                sample[self.analysistype].mismatches[contig][gene] = dict()\n                                                sample[self.analysistype].mismatches[contig][gene] = \\\n                                                    {primer: int(record['mismatches'])}\n                                            except KeyError:\n                                                sample[self.analysistype].mismatches[contig] = dict()\n                                                sample[self.analysistype].mismatches[contig][gene] = dict()\n                                                sample[self.analysistype].mismatches[contig][gene] = \\\n                                                    {primer: int(record['mismatches'])}\n                # Use query the stored blast dictionary to find the location of the hits\n                for row in sample[self.analysistype].blastrecords:\n                    try:\n                        # Extract the primers corresponding to the contig\n                        for primers in sample[self.analysistype].hits[row['query_id']]:\n                            # Extract the name of the contig\n                            contig = row['query_id']\n                            # Iterate through the forward and reverse primers\n                            for primer in primers:\n                                # If the primer is present in the current row, then this is the row of interest\n                                if row['subject_id'] == primer:\n                                    # Split off the primer direction and numbering\n                                    # accounts for primer names with \"-\" in addition to the terminal \"-F\" or \"-R\"\n                                    gene = '-'.join(primer.split('-')[:-1])\n                                    # Populate a dictionary for storing the genes present - will be used in creating\n                                    # the report\n                                    try:\n                                        sample[self.analysistype].genespresent[contig].add(gene)\n                                    except KeyError:\n                                        sample[self.analysistype].genespresent[contig] = set()\n                                        sample[self.analysistype].genespresent[contig].add(gene)\n                                    # Populate the range of the hit - the forward primer will have a -F an the name\n                                    if '-F' in primer:\n                                        # Determine if the sequence is the reverse complement - based on the fact that\n                                        # this is the forward primer, if the contig is reversed, then the primer\n                                        # (subject) will be reversed.\n                                        if int(row['subject_start']) > int(row['subject_end']):\n                                            # For reversed sequences, take the larger value of the start and stop\n                                            data = max(int(row['query_start']), int(row['query_end']))\n                                        else:\n                                            # Otherwise take the smaller value\n                                            data = min(int(row['query_start']), int(row['query_end']))\n                                        # Add the appropriately calculated value to the range dictionary\n                                        try:\n                                            sample[self.analysistype].range[contig][gene].add(data)\n                                        except KeyError:\n                                            try:\n                                                sample[self.analysistype].range[contig][gene] = set()\n                                                sample[self.analysistype].range[contig][gene].add(data)\n                                            except KeyError:\n                                                sample[self.analysistype].range[contig] = dict()\n                                                sample[self.analysistype].range[contig][gene] = set()\n                                                sample[self.analysistype].range[contig][gene].add(data)\n\n                                    # Similar to the forward primer, except reverse the min() and max()\n                                    elif '-R' in primer:\n                                        if int(row['subject_start']) < int(row['subject_end']):\n                                            data = min(int(row['query_start']), int(row['query_end']))\n                                        else:\n                                            data = max(int(row['query_start']), int(row['query_end']))\n                                        # Add the appropriately calculated value to the range dictionary\n                                        try:\n                                            sample[self.analysistype].range[contig][gene].add(data)\n                                        except KeyError:\n                                            try:\n                                                sample[self.analysistype].range[contig][gene] = set()\n                                                sample[self.analysistype].range[contig][gene].add(data)\n                                            except KeyError:\n                                                sample[self.analysistype].range[contig] = dict()\n                                                sample[self.analysistype].range[contig][gene] = set()\n                                                sample[self.analysistype].range[contig][gene].add(data)\n                    except KeyError:\n                        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ampliconclear(self):\n        for sample in self.metadata:\n            # Set the name of the amplicon FASTA file\n            sample[self.analysistype].ampliconfile = os.path.join(\n                sample[self.analysistype].outputdir, '{sn}_amplicons.fa'.format(sn=sample.name))\n            try:\n                os.remove(sample[self.analysistype].ampliconfile)\n            except IOError:\n                pass", "response": "Clear previously created amplicon files for the current sample."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reporter(self):\n        # Create a folder in which amplicons and raw BLAST results are to be stored\n        detailed_reports = os.path.join(self.path, 'detailed_reports')\n        make_path(detailed_reports)\n        # Create the report path if necessary\n        make_path(self.reportpath)\n        with open(self.report, 'w') as report:\n            # Initialise the header\n            data = 'Sample,Gene,GenomeLocation,AmpliconSize,Contig,ForwardPrimers,ReversePrimers,' \\\n                   'ForwardMismatches,ReverseMismatches\\n'\n            for sample in self.metadata:\n                # Initialise variables to convert attributes from set to list\n                sample[self.analysistype].genes = dict()\n                sample[self.analysistype].ntrange = dict()\n                try:\n                    # Check to ensure that there were amplicons present\n                    if sample[self.analysistype].range:\n                        # Iterate through each contig with genes present, and the list of genes on the contig\n                        for contig, genes in sorted(sample[self.analysistype].genespresent.items()):\n                            sample[self.analysistype].genes[contig] = list(genes)\n                            # Iterate through the set of genes\n                            for gene in sorted(genes):\n                                # Determine which primers had the best lowest number of mismatches\n                                # Create variables to store primer names, and number of mismatches\n                                forward = list()\n                                reverse = list()\n                                forwardmismatches = int()\n                                reversemismatches = int()\n                                # Sort the dictionary of mismatches based on the number of mismatches\n                                sorteddict = sorted(sample[self.analysistype].mismatches[contig][gene].items(),\n                                                    key=operator.itemgetter(1))\n                                # For every primer in the sorted dictionary, check to see if it is a forward or\n                                # reverse primer, and determine the number of mismatches; if it is the first primer\n                                # encountered for a gene, add it to the list, as because the dictionary is sorted,\n                                # the number of mismatches should be the best. Additionally, if subsequent primer\n                                # hits have the same number of mismatches, also add them to the list\n                                for entry in sorteddict:\n                                    # Add primer to the list if it is the first primer encountered\n                                    if '-F' in entry[0] and not forward:\n                                        forward.append(entry[0])\n                                        forwardmismatches = entry[1]\n                                    # Add the primer to the list if it has the same number of mismatches as a previous\n                                    # primer in the list\n                                    elif '-F' in entry[0] and entry[1] == forwardmismatches:\n                                        forward.append(entry[0])\n                                        forwardmismatches = entry[1]\n                                    elif '-R' in entry[0] and not reverse:\n                                        reverse.append(entry[0])\n                                        reversemismatches = entry[1]\n                                    elif '-R' in entry[0] and entry[1] == reversemismatches:\n                                        reverse.append(entry[0])\n                                        reversemismatches = entry[1]\n                                # Make a variable to prevent writing out this long attribute name multiple times\n                                ntrange = list(sample[self.analysistype].range[contig][gene])\n                                sample[self.analysistype].ntrange[gene] = ntrange\n                                # Extract the amplicons from the sequence file\n                                self.ampliconfile(sample, contig, sorted(ntrange), forward, reverse)\n                                # Copy the amplicons and raw BLAST outputs from FASTQ-formatted files to the\n                                # detailed_reports folder\n                                if sample[self.analysistype].filetype == 'fastq':\n                                    try:\n                                        shutil.copyfile(src=sample[self.analysistype].ampliconfile,\n                                                        dst=os.path.join(detailed_reports,\n                                                                         '{sn}_amplicons.fa'.format(sn=sample.name)))\n                                        shutil.copyfile(src=sample[self.analysistype].report,\n                                                        dst=os.path.join(detailed_reports,\n                                                                         '{sn}_rawresults.csv'.format(sn=sample.name)))\n                                    except FileNotFoundError:\n                                        pass\n                                # This first gene for a sample gets the sample name printed\n                                data += '{sn},'.format(sn=sample.name)\n                                # Populate the string with the gene name, properly formatted range, the length of\n                                # the amplicon, and the name of the contig on which the gene was found\n                                data += '{},{},{},{},{},{},{},{}\\n'\\\n                                    .format(gene,\n                                            '-'.join(str(x) for x in sorted(ntrange)),\n                                            max(ntrange) - min(ntrange) + 1,\n                                            contig,\n                                            ';'.join(sorted(forward)),\n                                            ';'.join(sorted(reverse)),\n                                            forwardmismatches,\n                                            reversemismatches)\n                    # If there were no amplicons, add the sample name and nothing else\n                    else:\n                        data += '{sn}\\n'.format(sn=sample.name)\n                # If there were no BLAST hits, add the sample name, and nothing else\n                except AttributeError:\n                    data += '{sn}\\n'.format(sn=sample.name)\n                # Remove attributes that either take up too much room in the .json output, or are not JSON serializable\n                try:\n                    delattr(sample[self.analysistype], \"blastresults\")\n                except AttributeError:\n                    pass\n                try:\n                    delattr(sample[self.analysistype], \"genespresent\")\n                except AttributeError:\n                    pass\n                try:\n                    delattr(sample[self.analysistype], \"contigs\")\n                except AttributeError:\n                    pass\n                try:\n                    delattr(sample[self.analysistype], \"range\")\n                except AttributeError:\n                    pass\n            # Write the string to the report\n            report.write(data)\n        # Clean up the BLAST database files\n        db = os.path.splitext(self.formattedprimers)[0]\n        # A list of all the file extensions associated with the BLASTdb\n        dbextensions = ['.nhr', '.nin', '.nog', '.nsd', '.nsi', '.nsq']\n        # Iterate through all the files, and delete each one - pass on IO errors\n        for dbfile in zip(itertools.repeat(db), dbextensions):\n            try:\n                os.remove(''.join(dbfile))\n            except IOError:\n                pass\n        try:\n            os.remove(self.formattedprimers)  # Maybe want to keep this file?\n        except IOError:\n            pass", "response": "Create the reports of the analyses and the raw BLAST results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ampliconfile(self, sample, contig, amplicon_range, forward_primer, reverse_primer):\n        # Open the file\n        with open(sample[self.analysistype].ampliconfile, 'a') as ampliconfile:\n            try:\n                # Load the records from the assembly into the dictionary\n                for record in SeqIO.parse(sample[self.analysistype].assemblyfile, 'fasta'):\n                    if record.id == contig:\n                        try:\n                            # Extract the name of the gene from the primer name\n                            genename = forward_primer[0].split('-')[0]\n                            try:\n                                # Sort the range calculated above\n                                start = amplicon_range[0]\n                                end = amplicon_range[1]\n                                # Slice the gene sequence from the sequence record - remember to subtract one to\n                                # allow for zero-based indexing\n                                genesequence = str(record.seq)[int(start) - 1:int(end)]\n                                # Set the record.id to be the sample name, the contig name,\n                                # the range, and the primers\n                                record.id = '{sn}_{contig}_{range}_{primers}' \\\n                                    .format(sn=sample.name,\n                                            contig=contig,\n                                            range='_'.join(str(x) for x in sorted(sample[self.analysistype]\n                                                                                  .range[record.id][genename])),\n                                            primers='_'.join(['_'.join(forward_primer), '_'.join(reverse_primer)]))\n                                # Clear the record.description\n                                record.description = ''\n                                # Create a seq record from the sliced genome sequence\n                                record.seq = Seq.Seq(genesequence)\n                                # Write the amplicon to file\n                                SeqIO.write(record, ampliconfile, 'fasta')\n                            except IndexError:\n                                pass\n                        except AttributeError:\n                            pass\n            except FileNotFoundError:\n                pass", "response": "Extracts the amplicon sequence from the contig file and writes it to the dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrewriting a single line in the file.", "response": "def writeline(self, line, line_number):\n        \"\"\"Rewrite a single line in the file.\n\n        Args:\n            line (str): The new text to write to the file.\n            line_number (int): The line of the file to rewrite. Numbering\n                starts at 0.\n        \"\"\"\n        tmp_file = tempfile.TemporaryFile('w+')\n        if not line.endswith(os.linesep):\n\n            line += os.linesep\n        try:\n\n            with open(self.path, 'r') as file_handle:\n\n                for count, new_line in enumerate(file_handle):\n\n                    if count == line_number:\n\n                        new_line = line\n\n                    tmp_file.write(new_line)\n\n            tmp_file.seek(0)\n            with open(self.path, 'w') as file_handle:\n\n                for new_line in tmp_file:\n\n                    file_handle.write(new_line)\n        finally:\n\n            tmp_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget an iter of VenvPaths within the directory.", "response": "def paths(self):\n        \"\"\"Get an iter of VenvPaths within the directory.\"\"\"\n        contents = os.listdir(self.path)\n        contents = (os.path.join(self.path, path) for path in contents)\n        contents = (VenvPath(path) for path in contents)\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an iter of VenvFile within the directory.", "response": "def files(self):\n        \"\"\"Get an iter of VenvFiles within the directory.\"\"\"\n        contents = self.paths\n        contents = (VenvFile(path.path) for path in contents if path.is_file)\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dirs(self):\n        contents = self.paths\n        contents = (VenvDir(path.path) for path in contents if path.is_dir)\n        return contents", "response": "Get an iter of VenvDirs within the directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an iter of VenvDirs and VenvFiles within the directory.", "response": "def items(self):\n        \"\"\"Get an iter of VenvDirs and VenvFiles within the directory.\"\"\"\n        contents = self.paths\n        contents = (\n            VenvFile(path.path) if path.is_file else VenvDir(path.path)\n            for path in contents\n        )\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shebang(self):\n        with open(self.path, 'rb') as file_handle:\n            hashtag = file_handle.read(2)\n            if hashtag == b'#!':\n\n                file_handle.seek(0)\n                return file_handle.readline().decode('utf8')\n\n        return None", "response": "Get the file shebang."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a new shebang to the file.", "response": "def shebang(self, new_shebang):\n        \"\"\"Write a new shebang to the file.\n\n        Raises:\n            ValueError: If the file has no shebang to modify.\n            ValueError: If the new shebang is invalid.\n        \"\"\"\n        if not self.shebang:\n\n            raise ValueError('Cannot modify a shebang if it does not exist.')\n\n        if not new_shebang.startswith('#!'):\n\n            raise ValueError('Invalid shebang.')\n\n        self.writeline(new_shebang, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the VIRTUAL_ENV path entry.", "response": "def _find_vpath(self):\n        \"\"\"Find the VIRTUAL_ENV path entry.\"\"\"\n        with open(self.path, 'r') as file_handle:\n\n            for count, line in enumerate(file_handle):\n\n                match = self.read_pattern.match(line)\n                if match:\n\n                    return match.group(1), count\n\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges the path to the virtual environment.", "response": "def vpath(self, new_vpath):\n        \"\"\"Change the path to the virtual environment.\"\"\"\n        _, line_number = self._find_vpath()\n        new_vpath = self.write_pattern.format(new_vpath)\n        self.writeline(new_vpath, line_number)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef files(self):\n        contents = self.paths\n        contents = (BinFile(path.path) for path in contents if path.is_file)\n        return contents", "response": "Get an iter of VenvFiles within the directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dirs(self):\n        contents = self.paths\n        contents = (BinDir(path.path) for path in contents if path.is_dir)\n        return contents", "response": "Get an iter of VenvDirs within the directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an iter of VenvDirs and VenvFiles within the directory.", "response": "def items(self):\n        \"\"\"Get an iter of VenvDirs and VenvFiles within the directory.\"\"\"\n        contents = self.paths\n        contents = (\n            BinFile(path.path) if path.is_file else BinDir(path.path)\n            for path in contents\n        )\n        return contents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrapComponent(comp):\n    # if already wrapped, return object\n    if hasattr(comp, 'paint'):\n        return comp\n    # to avoid manually creating a mapping, get all classes in \n    # this module, assume they are the class name appended with Q\n    current_module = sys.modules[__name__]\n    module_classes = {name[1:]: obj for name, obj in inspect.getmembers(sys.modules[__name__], inspect.isclass) if\n                      obj.__module__ == __name__}\n    # print __name__, module_classes\n    stimclass = comp.__class__.__name__\n    qclass = module_classes.get(stimclass, QStimulusComponent)\n    return qclass(comp)", "response": "Wraps a StimulusComponent with a class containing methods \n    for painting and editing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef paint(self, painter, rect, palette):\n        painter.save()\n\n        image = img.default()\n        painter.drawImage(rect, image)\n\n        # set text color\n        painter.setPen(QtGui.QPen(QtCore.Qt.red))\n        painter.drawText(rect, QtCore.Qt.AlignLeft, self.__class__.__name__)\n\n        painter.restore()", "response": "Draws a generic visual representation for this component"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces the known tags in the given string with the corresponding regular expression.", "response": "def replace_tags(cls, raw_filter):\n        \"\"\"\n        Searches for known tags in the given string and replaces them with the\n        corresponding regular expression.\n\n        *raw_filter* is an (optionnaly tagged) regular expression.\n\n        Returns the regular expression with known tags replaces by the\n        corresponding regular expression.\n        \"\"\"\n        for k, v in iter(cls.known_tags.items()):\n            raw_filter = raw_filter.replace(k, v)\n\n        return raw_filter"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_regex_list(cls, filter_str, rule_limit):\n        regexes = []\n\n        for f in filter_str.splitlines():\n            try:\n                regex = re.compile(f, flags=re.MULTILINE|re.IGNORECASE)\n            except sre_constants.error:\n                warnings.warn(\"Unable to compile this pattern: \\\"{0}\\\". \"\n                              \"It will be ignored\"\n                              .format(f))\n            else:\n                # If the Rule limit is > 1, the pattern MUST have a capturing\n                # group.\n                # (this capturing group will be used later as an index to\n                # count the matches.)\n                # If the pattern doesn't respect this, it will be ignored.\n                if rule_limit > 1 and not regex.groupindex:\n                    warnings.warn(\"The pattern \\\"{0}\\\" doesn't have a \"\n                                  \"capturing group but needs one.\"\n                                  \"It will be ignored\"\n                                  .format(f))\n                else:\n                    regexes.append(regex)\n\n        return regexes", "response": "Builds a list of regular expressions that can be used to match the given string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_string(cls, raw_filter, rule_limit):\n        parsed_filter = cls.replace_tags(raw_filter)\n        regexes = cls.build_regex_list(parsed_filter, rule_limit)\n\n        return cls(regexes)", "response": "Creates a new Filter instance from the given string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef values(self):\n        result = {}\n        result['fontsz'] = self.ui.fontszSpnbx.value()\n        result['display_attributes'] = self.ui.detailWidget.getCheckedDetails()\n        return result", "response": "Gets user inputs\n\n        :returns: dict of inputs:\n        | *'fontsz'*: int -- font size for text throughout the GUI\n        | *'display_attributes'*: dict -- what attributes of stimuli to report as they are being presented"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nflattens args and remove empty items.", "response": "def flatten_args(args: list, join=False, *, empty=(None, [], (), '')) -> list:\n    \"\"\"Flatten args and remove empty items.\n\n    Args:\n        args: A list of items (typically but not necessarily strings),\n            which may contain sub-lists, that will be flattened into\n            a single list with empty items removed. Empty items include\n            ``None`` and empty lists, tuples, and strings.\n        join: If ``True`` or a string, the final flattened list will be\n            joined into a single string. The default join string is\n            a space.\n        empty: Items that are considered empty.\n\n    Returns:\n        list|str: The list of args flattened with empty items removed\n            and the remaining items converted to strings. If ``join`` is\n            specified, the list of flattened args will be joined into\n            a single string.\n\n    Examples::\n\n        >>> flatten_args([])\n        []\n        >>> flatten_args(())\n        []\n        >>> flatten_args([(), (), [(), ()]])\n        []\n        >>> flatten_args(['executable', '--flag' if True else None, ('--option', 'value'), [None]])\n        ['executable', '--flag', '--option', 'value']\n        >>> flatten_args(['executable', '--option', 0])\n        ['executable', '--option', '0']\n\n    \"\"\"\n    flat_args = []\n    non_empty_args = (arg for arg in args if arg not in empty)\n    for arg in non_empty_args:\n        if isinstance(arg, (list, tuple)):\n            flat_args.extend(flatten_args(arg))\n        else:\n            flat_args.append(str(arg))\n    if join:\n        join = ' ' if join is True else join\n        flat_args = join.join(flat_args)\n    return flat_args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload an object from a module or a string.", "response": "def load_object(obj) -> object:\n    \"\"\"Load an object.\n\n    Args:\n        obj (str|object): Load the indicated object if this is a string;\n            otherwise, return the object as is.\n\n            To load a module, pass a dotted path like 'package.module';\n            to load an an object from a module pass a path like\n            'package.module:name'.\n\n    Returns:\n        object\n\n    \"\"\"\n    if isinstance(obj, str):\n        if ':' in obj:\n            module_name, obj_name = obj.split(':')\n            if not module_name:\n                module_name = '.'\n        else:\n            module_name = obj\n        obj = importlib.import_module(module_name)\n        if obj_name:\n            attrs = obj_name.split('.')\n            for attr in attrs:\n                obj = getattr(obj, attr)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef escape_args(cls, *args):\n        escaped_args = [shlex.quote(str(arg)) for arg in args]\n\n        return \" \".join(escaped_args)", "response": "Returns a shell - escaped string that is ready to be append\n        to the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_page_from_string_or_id(page_string, lang=None):\n    if type(page_string) == int:\n        return Page.objects.get(pk=int(page_string))\n    # if we have a string coming from some templates templates\n    is_text = isinstance(page_string, SafeText)\n    import sys\n    PY3 = sys.version > '3'\n    if PY3:\n        is_string = isinstance(page_string, str)\n    else:\n        is_string = isinstance(page_string, unicode) or isinstance(page_string, str)\n    if is_text or is_string:\n        if page_string.isdigit():\n            return Page.objects.get(pk=int(page_string))\n        return Page.objects.from_path(page_string, lang)\n    # in any other case we return the input becasue it's probably\n    # a Page object.\n    return page_string", "response": "Return a Page object from a slug or id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the parent page of the given page and render a nested list of its child pages. Good for rendering a secondary menu.", "response": "def pages_siblings_menu(context, page, url='/'):\n    \"\"\"Get the parent page of the given page and render a nested list of its\n    child pages. Good for rendering a secondary menu.\n\n    :param page: the page where to start the menu from.\n    :param url: not used anymore.\n    \"\"\"\n    lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n    page = get_page_from_string_or_id(page, lang)\n    if page:\n        if page.parent:\n            root = page.parent\n        else:\n            root = page\n        children = root.get_children_for_frontend()\n        context.update({'children': children, 'page': page})\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the admin table of pages.", "response": "def pages_admin_menu(context, page):\n    \"\"\"Render the admin table of pages.\"\"\"\n    request = context.get('request', None)\n\n    expanded = False\n    if request and \"tree_expanded\" in request.COOKIES:\n        cookie_string = urllib.unquote(request.COOKIES['tree_expanded'])\n        if cookie_string:\n            ids = [int(id) for id in\n                urllib.unquote(request.COOKIES['tree_expanded']).split(',')]\n            if page.id in ids:\n                expanded = True\n    context.update({'expanded': expanded, 'page': page})\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_slug_with_level(context, page, lang=None, fallback=True):\n    if not lang:\n        lang = context.get('lang', pages_settings.PAGE_DEFAULT_LANGUAGE)\n\n    page = get_page_from_string_or_id(page, lang)\n    if not page:\n        return ''\n\n    return {'content': page.slug_with_level(lang)}", "response": "Display slug with level by language."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_revisions(context, page, content_type, lang=None):\n    if not pages_settings.PAGE_CONTENT_REVISION:\n        return {'revisions': None}\n    revisions = Content.objects.filter(page=page, language=lang,\n                                type=content_type).order_by('-creation_date')\n    if len(revisions) < 2:\n        return {'revisions': None}\n    return {'revisions': revisions[0:10]}", "response": "Render the last 10 revisions of a page content with a list using\n        the pages. html template"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_videoplaceholder(parser, token):\n    name, params = parse_placeholder(parser, token)\n    return VideoPlaceholderNode(name, **params)", "response": "Method that parses the videoplaceholder template tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_get_pages_with_tag(parser, token):\n    bits = token.split_contents()\n    if 4 != len(bits):\n        raise TemplateSyntaxError('%r expects 2 arguments' % bits[0])\n    if bits[-2] != 'as':\n        raise TemplateSyntaxError(\n            '%r expects \"as\" as the second last argument' % bits[0])\n    varname = bits[-1]\n    tag = parser.compile_filter(bits[1])\n    varname = bits[-1]\n    return GetPagesWithTagNode(tag, varname)", "response": "Returns a list of pages with given tag."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parserunstats(self):\n        # metadata = GenObject()\n        # If the default file GenerateFASTQRunStatistics.xml is present, parse it\n        if os.path.isfile(os.path.join(self.path, \"GenerateFASTQRunStatistics.xml\")):\n            # Create a list of keys for which values are to be extracted\n            datalist = [\"SampleNumber\", \"SampleID\", \"SampleName\", \"NumberOfClustersPF\"]\n            # Load the file as an xml ElementTree object\n            runstatistics = ElementTree.ElementTree(file=os.path.join(self.path, \"GenerateFASTQRunStatistics.xml\"))\n            # Iterate through all the elements in the object\n            # .iterfind() allow for the matching and iterating though matches\n            # This is stored as a float to allow subsequent calculations\n            tclusterspf = [float(element.text) for element in runstatistics.iterfind(\"RunStats/NumberOfClustersPF\")][0]\n            # Iterate through all the elements (strains) in the OverallSamples/SummarizedSampleStatistics category\n            for element in runstatistics.iterfind(\"OverallSamples/SummarizedSampleStatistics\"):\n                # List comprehension. Essentially iterate through each element for each category in datalist:\n                # (element.iter(category) and pull out the value for nestedelement\n                straindata = [nestedelement.text for category in datalist for nestedelement in element.iter(category)]\n                # Try and replicate the Illumina rules to create file names from \"Sample_Name\"\n                samplename = samplenamer(straindata, 1)\n                # Calculate the percentage of clusters associated with each strain\n                # noinspection PyTypeChecker\n                percentperstrain = \"{:.2f}\".format((float(straindata[3]) / tclusterspf * 100))\n                try:\n                    # Use the sample number -1 as the index in the list of objects created in parsesamplesheet\n                    strainindex = int(straindata[0]) - 1\n                    # Set run to the .run object of self.samples[index]\n                    run = self.samples[strainindex].run\n                    # An assertion that compares the sample computer above to the previously entered sample name\n                    # to ensure that the samples are the same\n                    assert self.samples[strainindex].name == samplename, \\\n                        \"Sample name does not match object name {0!r:s}\".format(straindata[1])\n                    # Add the appropriate values to the strain metadata object\n                    run.SampleNumber = straindata[0]\n                    run.NumberofClustersPF = straindata[3]\n                    run.TotalClustersinRun = tclusterspf\n                    run.PercentOfClusters = percentperstrain\n                    run.flowcell = self.flowcell\n                    run.instrument = self.instrument\n                except IndexError:\n                    pass\n        elif os.path.isfile(os.path.join(self.path, 'indexingQC.txt')):\n            # Grab the first element from the second line in the file\n            tclusterspf = float(getline(os.path.join(self.path, \"indexingQC.txt\"), 2).split(\"\\t\")[0])\n            # Open the file and extract the relevant data\n            with open(os.path.join(\"indexingQC.txt\")) as indexqc:\n                # Iterate through the file\n                for line in indexqc:\n                    # Once \"Index\" is encountered, iterate through the rest of the file\n                    if \"Index\" in line:\n                        for subline in indexqc:\n                            straindata = [x.rstrip() for x in subline.rstrip().split(\"\\t\")]\n                            # Try and replicate the Illumina rules to create file names from \"Sample_Name\"\n                            samplename = samplenamer(straindata, 1)\n                            # Use the sample number -1 as the index in the list of objects created in parsesamplesheet\n                            strainindex = int(straindata[0]) - 1\n                            # Set run to the .run object of self.samples[index]\n                            run = self.samples[strainindex].run\n                            # An assertion that compares the sample computer above to the previously entered sample name\n                            # to ensure that the samples are the same\n                            assert self.samples[strainindex].name == samplename, \\\n                                \"Sample name {} does not match object name {}\" \\\n                                .format(self.samples[strainindex].name, samplename)\n                            # Extract and format the percent of reads (passing filter) associated with each sample\n                            # noinspection PyTypeChecker\n                            percentperstrain = float(\"{:.2f}\".format(float(straindata[5])))\n                            # Calculate the number of reads passing filter associated with each sample:\n                            # percentage of reads per strain times the total reads passing filter divided by 100\n                            numberofclusterspf = int(percentperstrain * tclusterspf / 100)\n                            # Update the object with the variables\n                            run.SampleNumber = straindata[0]\n                            run.NumberofClustersPF = numberofclusterspf\n                            run.TotalClustersinRun = tclusterspf\n                            run.PercentOfClusters = percentperstrain\n                            run.flowcell = self.flowcell\n                            run.instrument = self.instrument\n        else:\n            strainindex = 0\n            for i in range(len(self.samples)):\n                # Set run to the .run object of self.samples[index]\n                run = self.samples[strainindex].run\n                # Update the object with the variables\n                run.SampleNumber = strainindex + 1\n                run.NumberofClustersPF = 'NA'\n                run.TotalClustersinRun = 'NA'\n                run.PercentOfClusters = 'NA'\n                run.flowcell = self.flowcell\n                run.instrument = self.instrument\n                strainindex += 1", "response": "Parses the XML run statistics file and creates a GenObject containing the information needed to create the object that is used to store the original data in the Indexing QC tab of the run on Basespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprettifying name of path MimeType.", "response": "def fix_raw_path(path):\n    \"\"\"Prettify name of path\n\n    :param path: path to fix\n    :return: Good name for path\n    \"\"\"\n    double_path_separator = PATH_SEPARATOR + PATH_SEPARATOR\n    while path.find(\n            double_path_separator) >= 0:  # there are double separators\n        path = path.replace(double_path_separator,\n                            PATH_SEPARATOR)  # remove double path separator\n\n    if is_folder(path) and not path.endswith(\"/\"):\n        path = path + \"/\"\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_year(name):\n    for i in range(len(\n            name) - 3):  # last index is length - 3 - 1 = length - 4\n        if name[i: i + 4].isdigit():\n            name = name[:i] + name[i + 4:]\n            return remove_year(\n                name)  # if there is a removal, start again\n    return name", "response": "Removes the year from the input\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_brackets(name):\n    name = re.sub(\n        r\"([(\\[]).*?([)\\]])\",\n        r\"\\g<1>\\g<2>\",\n        name\n    )  # remove anything in between brackets\n    brackets = \"()[]{}\"  # list of brackets\n    for bracket in brackets:\n        name = name.replace(bracket, \"\")\n    return name", "response": "Removes brackets from input\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract max chars in name truncated to nearest word CTYPE", "response": "def extract_name_max_chars(name, max_chars=64, blank=\" \"):\n    \"\"\"Extracts max chars in name truncated to nearest word\n\n    :param name: path to edit\n    :param max_chars: max chars of new name\n    :param blank: char that represents the blank between words\n    :return: Name edited to contain at most max_chars\n    \"\"\"\n    new_name = name.strip()\n    if len(new_name) > max_chars:\n        new_name = new_name[:max_chars]  # get at most 64 chars\n        if new_name.rfind(blank) > 0:\n            new_name = new_name[:new_name.rfind(blank)]  # nearest word\n    return new_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprettifies a path name of a resource tree.", "response": "def prettify(name, blank=\" \"):\n    \"\"\"Prettify name of path\n\n    :param name: path Name: to edit\n    :param blank: default blanks in name\n    :return: Prettier name from given one: replace bad chars with good ones\n    \"\"\"\n    if name.startswith(\".\"):  # remove starting\n        name = name[1:]\n\n    for bad_char in BAD_CHARS:\n        name = name.replace(bad_char, blank)  # remove token\n\n    name = String(name).remove_all(blank)\n\n    for i in range(1, len(name) - 2):\n        try:\n            are_blanks = name[i - 1] == blank and name[i + 1] == blank\n            if are_blanks and name[i] in BAD_CHARS:\n                name = name[:i - 1] + name[i + 2:]\n        except:  # out of bounds\n            pass\n\n    if name.startswith(blank):\n        name = name[1:]\n\n    if name.endswith(blank):  # remove ending replacement\n        name = name[:-1]\n\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding parent folder of file", "response": "def get_parent_folder_name(file_path):\n    \"\"\"Finds parent folder of file\n\n    :param file_path: path\n    :return: Name of folder container\n    \"\"\"\n    return os.path.split(os.path.split(os.path.abspath(file_path))[0])[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ls_dir(path, include_hidden=False):\n    lst = []\n    for file in os.listdir(path):\n        hidden_file = FileSystem(file).is_hidden()\n        if (hidden_file and include_hidden) or (not hidden_file):\n            lst.append(os.path.join(path, file))\n    return list(set(lst))", "response": "Finds content of folder\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ls_recurse(path, include_hidden=False):\n    lst = []\n    for file in os.listdir(path):\n        hidden_file = FileSystem(file).is_hidden()\n        if (hidden_file and include_hidden) or (not hidden_file):\n            lst.append(os.path.join(path, file))\n            if is_folder(os.path.join(path, file)):\n                lst += ls_recurse(\n                    os.path.join(path, file),\n                    include_hidden=include_hidden\n                )  # get list of files in directory\n    return list(set(lst))", "response": "Finds content of folder recursively\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_content(path, recurse, include_hidden=False):\n    if recurse:\n        return ls_recurse(path, include_hidden=include_hidden)\n\n    return ls_dir(path, include_hidden=include_hidden)", "response": "Finds content of a folder recursively"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if file path is russian", "response": "def is_russian(self):\n        \"\"\"Checks if file path is russian\n\n        :return: True iff document has a russian name\n        \"\"\"\n\n        russian_chars = 0\n        for char in RUSSIAN_CHARS:\n            if char in self.name:\n                russian_chars += 1  # found a russian char\n        return russian_chars > len(RUSSIAN_CHARS) / 2.0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrenaming to new path", "response": "def rename(self, new_path):\n        \"\"\"Renames to new path\n\n        :param new_path: new path to use\n        \"\"\"\n        rename_path = fix_raw_path(new_path)\n        if is_folder(self.path):\n            os.rename(self.path, rename_path)\n        else:\n            os.renames(self.path, rename_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the constructor for the component type this label is to represent", "response": "def setClass(self, factoryclass):\n        \"\"\"Sets the constructor for the component type this label is to \n        represent\n\n        :param factoryclass: a class that, when called, results in an instance of the desired class\n        :type factoryclass: callable\n        \"\"\"\n        self.factoryclass = factoryclass\n        self.setText(str(factoryclass.name))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if a drag is taking place and initiates it", "response": "def mouseMoveEvent(self, event):\n        \"\"\"Determines if a drag is taking place, and initiates it\"\"\"\n        if (event.pos() - self.dragStartPosition).manhattanLength() < 10:\n            return\n        QtGui.QApplication.setOverrideCursor(QtGui.QCursor(QtCore.Qt.ClosedHandCursor))\n        factory = self.factoryclass()\n\n        mimeData = QtCore.QMimeData()\n        try:\n            mimeData.setData(\"application/x-protocol\", factory.serialize())\n        except:\n            mimeData.setData(\"application/x-protocol\", cPickle.dumps(factory))\n\n        drag = QtGui.QDrag(self)\n        drag.setMimeData(mimeData)\n\n        pixmap = QtGui.QPixmap()\n        pixmap = pixmap.grabWidget(self, self.frameRect())\n\n        # below makes the pixmap half transparent\n        # painter = QtGui.QPainter(pixmap)\n        # painter.setCompositionMode(painter.CompositionMode_DestinationIn)\n        # painter.fillRect(pixmap.rect(), QtGui.QColor(0, 0, 0, 127))\n        # painter.end()\n\n        drag.setPixmap(pixmap)\n\n        drag.setHotSpot(QtCore.QPoint(pixmap.width()/2, pixmap.height()/2))\n        drag.setPixmap(pixmap)\n\n        self.dragActive.emit(True)\n        result = drag.exec_(QtCore.Qt.MoveAction)\n\n        QtGui.QApplication.restoreOverrideCursor()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef database(self):\n        import sqlite3\n        try:\n            os.remove('{}/metadatabase.sqlite'.format(self.reportpath))\n        except OSError:\n            pass\n        # Set the name of the database\n        db = sqlite3.connect('{}/metadatabase.sqlite'.format(self.reportpath))\n        # Create a cursor to allow access to the database\n        cursor = db.cursor()\n        # Set up the db\n        cursor.execute('''\n          CREATE TABLE IF NOT EXISTS Samples (\n            id     INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n            name   TEXT UNIQUE\n          )\n        ''')\n        # Create a variable to store the names of the header values for each individual table\n        # This will store a set of all the headers from all the strains, as there can be some variability present, as\n        # not all analyses are available for all taxonomic groups\n        columns = dict()\n        for sample in self.metadata:\n            # Create a metadata object to store the new tables\n            data = MetadataObject()\n            data.name = sample.name\n            # Insert each strain name into the Samples table\n            cursor.execute('''\n              INSERT OR IGNORE INTO Samples (name)\n              VALUES ( ? )\n            ''', (sample.name, ))\n            # Each header in the .json file represents a major category e.g. ARMI, GeneSeekr, commands, etc. and\n            # will be made into a separate table\n            for header in sample.datastore.items():\n                # Allow for certain analyses, such as core genome, not being performed on all strains\n                try:\n\n                    # Key and value: data description and data value e.g. targets present: 1012, etc.\n                    for key, value in sorted(header[1].datastore.items()):\n                        # Only the values consisting of dictionaries are of interest\n                        if type(value) == dict:\n                            # Clean the column names so there are no issues entering names into the database\n                            cleanedcolumn = self.columnclean(key)\n                            # Set the table name\n                            tablename = '{}_{}'.format(header[0].replace('.', '_'), cleanedcolumn)\n                            # Create the table (if it doesn't already exist)\n                            cursor.execute('''\n                                          CREATE TABLE IF NOT EXISTS {} (\n                                            sample_id INTEGER\n                                          )\n                                          '''.format(tablename))\n                            # Add the attributes with the dictionaries (values) to the metadata object\n                            setattr(data, tablename, GenObject(value))\n                            for gene, result in sorted(value.items()):\n                                # Add the data header to the dictionary\n                                try:\n                                    columns[tablename].add(gene)\n                                # Initialise the dictionary the first time a table name is encountered\n                                except KeyError:\n                                    columns[tablename] = set()\n                                    columns[tablename].add(str(gene))\n                except (AttributeError, IndexError):\n                    pass\n            self.tabledata.append(data)\n        # Iterate through the dictionary containing all the data headers\n        for table, setofheaders in sorted(columns.items()):\n            # Each header will be used as a column in the appropriate table\n            for cleanedcolumn in sorted(setofheaders):\n                # Alter the table by adding each header as a column\n                cursor.execute('''\n                  ALTER TABLE {}\n                  ADD COLUMN {} TEXT\n                '''.format(table, cleanedcolumn))\n            # Iterate through the samples and pull out the data for each table/column\n            # for sample in self.metadata:\n            for sample in self.tabledata:\n                # Find the id associated with each sample in the Sample table\n                cursor.execute('''\n                  SELECT id from Samples WHERE name=?\n                ''', (sample.name,))\n                sampleid = cursor.fetchone()[0]\n                # Add the sample_id to the table\n                cursor.execute('''\n                  INSERT OR IGNORE INTO {}\n                  (sample_id) VALUES (\"{}\")\n                  '''.format(table, sampleid))\n                # Add the data to the table\n                try:\n                    # Find the data for each table/column\n                    for item in sorted(sample[table].datastore.items()):\n                        # Clean the names\n                        cleanedcolumn = self.columnclean(str(item[0]))\n                        # Add the data to the column of the appropriate table,\n                        # where the sample_id matches the current strain\n                        cursor.execute('''\n                          UPDATE {}\n                          SET {} = ?\n                          WHERE sample_id = {}\n                          '''.format(table, cleanedcolumn, sampleid), (str(item[1]), ))\n                except KeyError:\n                    pass\n        # Commit the changes to the database\n        db.commit()", "response": "Enters all the metadata into a database"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclean the given column to be importable into a database.", "response": "def columnclean(column):\n        \"\"\"\n        Modifies column header format to be importable into a database\n        :param column: raw column header\n        :return: cleanedcolumn: reformatted column header\n        \"\"\"\n        cleanedcolumn = str(column) \\\n            .replace('%', 'percent') \\\n            .replace('(', '_') \\\n            .replace(')', '') \\\n            .replace('As', 'Adenosines') \\\n            .replace('Cs', 'Cytosines') \\\n            .replace('Gs', 'Guanines') \\\n            .replace('Ts', 'Thymines') \\\n            .replace('Ns', 'Unknowns') \\\n            .replace('index', 'adapterIndex')\n        return cleanedcolumn"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the label assigned to an axes", "response": "def getLabel(self, key):\n        \"\"\"Gets the label assigned to an axes\n\n        :param key:???\n        :type key: str\n        \"\"\"\n        axisItem = self.getPlotItem().axes[key]['item']\n        return axisItem.label.toPlainText()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef updateData(self, axeskey, x, y):\n        if axeskey == 'stim':\n            self.stimPlot.setData(x,y)\n            # call manually to ajust placement of signal\n            ranges = self.viewRange()\n            self.rangeChange(self, ranges)\n        if axeskey == 'response':\n            self.clearTraces()\n            if self._traceUnit == 'A':\n                y = y * self._ampScalar\n            if self.zeroAction.isChecked():\n                start_avg = np.mean(y[5:25])\n                y = y - start_avg\n            self.tracePlot.setData(x,y*self._polarity)", "response": "Replaces the currently displayed data with the new values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef appendData(self, axeskey, bins, ypoints):\n        if axeskey == 'raster' and len(bins) > 0:\n            x, y = self.rasterPlot.getData()\n            # don't plot overlapping points\n            bins = np.unique(bins)\n            # adjust repetition number to response scale\n            ypoints = np.ones_like(bins)*self.rasterYslots[ypoints[0]]\n            x = np.append(x, bins)\n            y = np.append(y, ypoints)\n            self.rasterPlot.setData(x, y)", "response": "Appends data to existing plotted data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the current threshold at", "response": "def setThreshold(self, threshold):\n        \"\"\"Sets the current threshold\n\n        :param threshold: the y value to set the threshold line at\n        :type threshold: float\n        \"\"\"\n        self.threshLine.setValue(threshold)\n        self.threshold_field.setValue(threshold)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the raster plot y - axis bounds where in the plot the raster will appear between the master and master raster plots.", "response": "def setRasterBounds(self, lims):\n        \"\"\"Sets the raster plot y-axis bounds, where in the plot the raster will appear between\n\n        :param lims: the (min, max) y-values for the raster plot to be placed between\n        :type lims: (float, float)\n        \"\"\"\n        self.rasterBottom = lims[0]\n        self.rasterTop = lims[1]\n        self.updateRasterBounds()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the y - coordinate slots where the raster points are plotted according to the current limits of the y - axis.", "response": "def updateRasterBounds(self):\n        \"\"\"Updates the y-coordinate slots where the raster points \n        are plotted, according to the current limits of the y-axis\"\"\"\n        yrange = self.viewRange()[1]\n        yrange_size = yrange[1] - yrange[0]\n        rmax = self.rasterTop*yrange_size + yrange[0]\n        rmin = self.rasterBottom*yrange_size + yrange[0]\n        self.rasterYslots = np.linspace(rmin, rmax, self.nreps)\n        self.rasterBoundsUpdated.emit((self.rasterBottom, self.rasterTop), self.getTitle())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprompt the user to provide the raster bounds with a dialog. Saves the bounds to be applied to the plot", "response": "def askRasterBounds(self):\n        \"\"\"Prompts the user to provide the raster bounds with a dialog. \n        Saves the bounds to be applied to the plot\"\"\"\n        dlg = RasterBoundsDialog(bounds= (self.rasterBottom, self.rasterTop))\n        if dlg.exec_():\n            bounds = dlg.values()\n            self.setRasterBounds(bounds)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadjusting the stimulus signal to keep it at the top of a plot.", "response": "def rangeChange(self, pw, ranges):\n        \"\"\"Adjusts the stimulus signal to keep it at the top of a plot,\n        after any ajustment to the axes ranges takes place.\n\n        This is a slot for the undocumented pyqtgraph signal sigRangeChanged.\n        From what I can tell the arguments are:\n\n        :param pw: reference to the emitting object (plot widget in my case)\n        :type pw: object\n        :param ranges: I am only interested when this turns out to be a nested list of axis bounds\n        :type ranges: object\n        \"\"\"\n        if hasattr(ranges, '__iter__'):\n            # adjust the stim signal so that it falls in the correct range\n            yrange_size = ranges[1][1] - ranges[1][0]\n            stim_x, stim_y = self.stimPlot.getData()\n            if stim_y is not None:\n                stim_height = yrange_size*STIM_HEIGHT\n                # take it to 0\n                stim_y = stim_y - np.amin(stim_y)\n                # normalize\n                if np.amax(stim_y) != 0:\n                    stim_y = stim_y/np.amax(stim_y)\n                # scale for new size\n                stim_y = stim_y*stim_height\n                # raise to right place in plot\n                stim_y = stim_y + (ranges[1][1] - (stim_height*1.1 + (stim_height*0.2)))\n                self.stimPlot.setData(stim_x, stim_y)\n            # rmax = self.rasterTop*yrange_size + ranges[1][0]\n            # rmin = self.rasterBottom*yrange_size + ranges[1][0]\n            self.updateRasterBounds()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_thresh(self):\n        thresh_val = self.threshLine.value()\n        self.threshold_field.setValue(thresh_val)\n        self.thresholdUpdated.emit(thresh_val, self.getTitle())", "response": "Emits a Qt signal thresholdUpdated with the current threshold value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fromFile(self, fname):\n        spec, f, bins, dur = audiotools.spectrogram(fname, **self.specgramArgs)\n        self.updateImage(spec, bins, f)\n        return dur", "response": "Displays a spectrogram of an audio file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the image directly.", "response": "def updateImage(self, imgdata, xaxis=None, yaxis=None):\n        \"\"\"Updates the Widget image directly.\n\n        :type imgdata: numpy.ndarray, see :meth:`pyqtgraph:pyqtgraph.ImageItem.setImage`\n        :param xaxis: x-axis values, length should match dimension 1 of imgdata\n        :param yaxis: y-axis values, length should match dimension 0 of imgdata\n        \"\"\"\n        imgdata = imgdata.T\n        self.img.setImage(imgdata)\n        if xaxis is not None and yaxis is not None:\n            xscale = 1.0/(imgdata.shape[0]/xaxis[-1])\n            yscale = 1.0/(imgdata.shape[1]/yaxis[-1])\n            self.resetScale()        \n            self.img.scale(xscale, yscale)\n            self.imgScale = (xscale, yscale)\n        self.imageArray = np.fliplr(imgdata)\n        self.updateColormap()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resetScale(self):\n        self.img.scale(1./self.imgScale[0], 1./self.imgScale[1])\n        self.imgScale = (1.,1.)", "response": "Resets the scale on this image. Correctly aligns time scale undoes manual scaling"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisplaying a spectrogram of the provided signal", "response": "def updateData(self, signal, fs):\n        \"\"\"Displays a spectrogram of the provided signal\n\n        :param signal: 1-D signal of audio\n        :type signal: numpy.ndarray\n        :param fs: samplerate of signal\n        :type fs: int\n        \"\"\"\n        # use a separate thread to calculate spectrogram so UI doesn't lag\n        t = threading.Thread(target=_doSpectrogram, args=(self.spec_done, (fs, signal),), kwargs=self.specgramArgs)\n        t.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setSpecArgs(**kwargs):\n        for key, value in kwargs.items():\n            if key == 'colormap':\n                SpecWidget.imgArgs['lut'] = value['lut']\n                SpecWidget.imgArgs['levels'] = value['levels']\n                SpecWidget.imgArgs['state'] = value['state']\n                for w in SpecWidget.instances:\n                    w.updateColormap()\n            else:\n                SpecWidget.specgramArgs[key] = value", "response": "Sets the arguments for the spectrogram appearance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears the current image", "response": "def clearImg(self):\n        \"\"\"Clears the current image\"\"\"\n        self.img.setImage(np.array([[0]]))\n        self.img.image = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprompt the user with a dialog to change colormap", "response": "def editColormap(self):\n        \"\"\"Prompts the user with a dialog to change colormap\"\"\"\n        self.editor = pg.ImageView()\n        # remove the ROI and Norm buttons\n        self.editor.ui.roiBtn.setVisible(False)\n        self.editor.ui.menuBtn.setVisible(False)\n        self.editor.setImage(self.imageArray)\n        if self.imgArgs['state'] is not None:\n            self.editor.getHistogramWidget().item.gradient.restoreState(self.imgArgs['state'])\n            self.editor.getHistogramWidget().item.setLevels(*self.imgArgs['levels'])\n        \n        self.editor.closeEvent = self._editor_close\n        self.editor.setWindowModality(QtCore.Qt.ApplicationModal)\n        self.editor.show()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the currently colormap accoring to stored settings", "response": "def updateColormap(self):\n        \"\"\"Updates the currently colormap accoring to stored settings\"\"\"\n        if self.imgArgs['lut'] is not None:\n            self.img.setLookupTable(self.imgArgs['lut'])\n            self.img.setLevels(self.imgArgs['levels'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef appendData(self, xdata, ydata, color='b', legendstr=None):\n        item = self.plot(xdata, ydata, pen=color)\n        if legendstr is not None:\n            self.legend.addItem(item, legendstr)\n        return item", "response": "Adds the data to the plot\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setLabels(self, xlabel=None, ylabel=None, title=None, xunits=None, yunits=None):\n        if xlabel is not None:\n            self.setLabel('bottom', xlabel, units=xunits)\n        if ylabel is not None:\n            self.setLabel('left', ylabel, units=yunits)\n        if title is not None:\n            self.setTitle(title)", "response": "Sets the plot labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the given point connects line to previous point in group", "response": "def setPoint(self, x, group, y):\n        \"\"\"Sets the given point, connects line to previous point in group\n\n        :param x: x value of point\n        :type x: float\n        :param group: group which plot point for\n        :type group: float\n        :param y: y value of point\n        :type y: float\n        \"\"\"\n        if x == -1:\n            # silence window\n            self.plot([0],[y], symbol='o')\n        else:\n            yindex = self.groups.index(group)\n            xdata, ydata = self.lines[yindex].getData()\n            if ydata is None:\n                xdata = [x]\n                ydata = [y]\n            else:\n                xdata = np.append(xdata, x)\n                ydata = np.append(ydata, y)\n            self.lines[yindex].setData(xdata, ydata)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setLabels(self, name):\n        if name == \"calibration\":\n            self.setWindowTitle(\"Calibration Curve\")\n            self.setTitle(\"Calibration Curve\")\n            self.setLabel('bottom', \"Frequency\", units='Hz')\n            self.setLabel('left', 'Recorded Intensity (dB SPL)')\n        elif name == \"tuning\":\n            self.setWindowTitle(\"Tuning Curve\")\n            self.setTitle(\"Tuning Curve\")\n            self.setLabel('bottom', \"Frequency\", units=\"Hz\")\n            self.setLabel('left', \"Spike Count (mean)\")\n        else:\n            self.setWindowTitle(\"Spike Counts\")\n            self.setTitle(\"Spike Counts\")\n            self.setLabel('bottom', \"Test Number\", units='')\n            self.setLabel('left', \"Spike Count (mean)\", units='')", "response": "Sets plot labels according to predefined options."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a data set and creates a ProgressWidget that is a subset of the data set and returns the progress plot.", "response": "def loadCurve(data, groups, thresholds, absvals, fs, xlabels):\n        \"\"\"Accepts a data set from a whole test, averages reps and re-creates the \n        progress plot as the same as it was during live plotting. Number of thresholds\n        must match the size of the channel dimension\"\"\"\n        xlims = (xlabels[0], xlabels[-1])\n        pw = ProgressWidget(groups, xlims)\n        spike_counts = []\n        # skip control\n        for itrace in range(data.shape[0]):\n            count = 0\n            for ichan in range(data.shape[2]):\n                flat_reps = data[itrace,:,ichan,:].flatten()\n                count += len(spikestats.spike_times(flat_reps, thresholds[ichan], fs, absvals[ichan]))\n            spike_counts.append(count/(data.shape[1]*data.shape[2])) #mean spikes per rep\n\n        i = 0\n        for g in groups:\n            for x in xlabels:\n                pw.setPoint(x, g, spike_counts[i])\n                i +=1\n\n        return pw"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setBins(self, bins):\n        self._bins = bins\n        self._counts = np.zeros_like(self._bins)\n        bar_width = bins[0]*1.5\n        self.histo.setOpts(x=bins, height=self._counts, width=bar_width)\n        self.setXlim((0, bins[-1]))", "response": "Sets the bin centers for the current time area"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclear all histograms (keeps bins)", "response": "def clearData(self):\n        \"\"\"Clears all histograms (keeps bins)\"\"\"\n        self._counts = np.zeros_like(self._bins)\n        self.histo.setOpts(height=self._counts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the data at the given bins to the histogram", "response": "def appendData(self, bins, repnum=None):\n        \"\"\"Increases the values at bins (indexes)\n\n        :param bins: bin center values to increment counts for, to increment a time bin more than once include multiple items in list with that bin center value\n        :type bins: numpy.ndarray\n        \"\"\"\n        # only if the last sample was above threshold, but last-1 one wasn't\n        bins[bins >= len(self._counts)] = len(self._counts) -1\n        bin_totals = np.bincount(bins)\n        self._counts[:len(bin_totals)] += bin_totals\n        self.histo.setOpts(height=np.array(self._counts))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess spike times from raw response data", "response": "def processData(self, times, response, test_num, trace_num, rep_num):\n        \"\"\"Calulate spike times from raw response data\"\"\"\n        # invert polarity affects spike counting\n        response = response * self._polarity\n\n        if rep_num == 0:\n            # reset\n            self.spike_counts = []\n            self.spike_latencies = []\n            self.spike_rates = []\n\n        fs = 1./(times[1] - times[0])\n\n        # process response; calculate spike times\n        spike_times = spikestats.spike_times(response, self._threshold, fs)\n        self.spike_counts.append(len(spike_times))\n        if len(spike_times) > 0:\n            self.spike_latencies.append(spike_times[0])\n        else:\n            self.spike_latencies.append(np.nan)\n        self.spike_rates.append(spikestats.firing_rate(spike_times, times))\n\n        binsz = self._bins[1] - self._bins[0]\n        response_bins = spikestats.bin_spikes(spike_times, binsz)\n        # self.putnotify('spikes_found', (response_bins, rep_num))\n        self.appendData(response_bins, rep_num)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setSr(self, fs):\n        self.tracePlot.setSr(fs)\n        self.stimPlot.setSr(fs)", "response": "Sets the samplerate of the input operation being plotted"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setWindowSize(self, winsz):\n        self.tracePlot.setWindowSize(winsz)\n        self.stimPlot.setWindowSize(winsz)", "response": "Sets the size of scroll window"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new plot for the given set of data and labels", "response": "def addPlot(self, xdata, ydata, xlabel=None, ylabel=None, title=None, xunits=None, yunits=None):\n        \"\"\"Adds a new plot for the given set of data and/or labels, Generates a SimplePlotWidget\n\n        :param xdata: index values for data, plotted on x-axis\n        :type xdata: numpy.ndarray\n        :param ydata: value data to plot, dimension must match xdata\n        :type ydata: numpy.ndarray\n        \"\"\"\n        p = SimplePlotWidget(xdata, ydata)\n        p.setLabels(xlabel, ylabel, title, xunits, yunits)\n        # self.plots.append(p)\n        self.stacker.addWidget(p)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new spectorgram plot for the given image.", "response": "def addSpectrogram(self, ydata, fs, title=None):\n        \"\"\"Adds a new spectorgram plot for the given image. Generates a SpecWidget\n\n        :param ydata: 2-D array of the image to display\n        :type ydata: numpy.ndarray\n        :param fs: the samplerate of the signal in the image, used to set time/ frequency scale\n        :type fs: int\n        :param title: Plot title\n        :type title: str\n        \"\"\"\n        p = SpecWidget()\n        p.updateData(ydata, fs)\n        if title is not None:\n            p.setTitle(title)\n        self.stacker.addWidget(p)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nextPlot(self):\n        if self.stacker.currentIndex() < self.stacker.count():\n            self.stacker.setCurrentIndex(self.stacker.currentIndex()+1)", "response": "Moves the displayed plot to the next one"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove the displayed plot to the previous one", "response": "def prevPlot(self):\n        \"\"\"Moves the displayed plot to the previous one\"\"\"\n        if self.stacker.currentIndex() > 0:\n            self.stacker.setCurrentIndex(self.stacker.currentIndex()-1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndividing a string into a list of strings as even as possible.", "response": "def most_even_chunk(string, group):\n    \"\"\"Divide a string into a list of strings as even as possible.\"\"\"\n    counts = [0] + most_even(len(string), group)\n    indices = accumulate(counts)\n    slices = window(indices, 2)\n    return [string[slice(*one)] for one in slices]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndividing a number into a list of numbers as even as possible.", "response": "def most_even(number, group):\n    \"\"\"Divide a number into a list of numbers as even as possible.\"\"\"\n    count, rest = divmod(number, group)\n    counts = zip_longest([count] * group, [1] * rest, fillvalue=0)\n    chunks = [sum(one) for one in counts]\n    logging.debug('chunks: %s', chunks)\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_modules(path):\n    lst = []\n    folder_contents = os.listdir(path)\n    is_python_module = \"__init__.py\" in folder_contents\n\n    if is_python_module:\n        for file in folder_contents:\n            full_path = os.path.join(path, file)\n\n            if is_file(full_path):\n                lst.append(full_path)\n\n            if is_folder(full_path):\n                lst += _get_modules(full_path)  # recurse in folder\n\n    return list(set(lst))", "response": "Finds modules in folder recursively\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding modules in folder and returns list of modules", "response": "def get_modules(folder, include_meta=False):\n    \"\"\"Finds modules (recursively) in folder\n\n    :param folder: root folder\n    :param include_meta: whether include meta files like (__init__ or\n        __version__)\n    :return: list of modules\n    \"\"\"\n    files = [\n        file\n        for file in _get_modules(folder)\n        if is_file(file)  # just files\n    ]\n\n    if not include_meta:\n        files = [\n            file\n            for file in files\n            if not Document(file).name.startswith(\"__\")\n        ]\n\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse(self):\n        with open(self.path, \"rt\") as reader:\n            return ast.parse(reader.read(), filename=self.path)", "response": "Parses file contents and returns Tree hierarchy of file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds package name of file", "response": "def _find_package(self, root_package):\n        \"\"\"Finds package name of file\n\n        :param root_package: root package\n        :return: package name\n        \"\"\"\n\n        package = self.path.replace(root_package, \"\")\n        if package.endswith(\".py\"):\n            package = package[:-3]\n\n        package = package.replace(os.path.sep, MODULE_SEP)\n        root_package = get_folder_name(root_package)\n\n        package = root_package + package  # add root\n        return package"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_instances(self, instance):\n\n        return [\n            x\n            for x in self.tree.body\n            if isinstance(x, instance)\n        ]", "response": "Finds all instances of instance in tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_classes(self):\n        instances = self._get_instances(ast.ClassDef)\n        instances = [\n            PyClass(instance, self.package)\n            for instance in instances\n        ]\n        return instances", "response": "Finds classes in file\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_functions(self):\n        instances = self._get_instances(ast.FunctionDef)\n        instances = [\n            PyFunction(instance, self.package)\n            for instance in instances\n        ]\n        return instances", "response": "Finds top - level functions in file\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_functions(self, include_meta=False):\n        instances = self._get_instances(ast.FunctionDef)\n        instances = [\n            PyFunction(instance, self.full_package)  # fix package name\n            for instance in instances\n        ]\n\n        if not include_meta:\n            instances = [\n                instance  # fix package name\n                for instance in instances\n                if not instance.get_name().startswith(\"__\")\n            ]\n\n        return instances", "response": "Finds top - level functions in file\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassembles genomes from the current genomes and run skesa on the genomes that are in the current genomes.", "response": "def skesa_assemble(self):\n        \"\"\"\n        Run skesa to assemble genomes\n        \"\"\"\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                # Initialise the assembly command\n                sample.commands.assemble = str()\n                try:\n                    if sample.general.trimmedcorrectedfastqfiles:\n                        # If the sample is a pure isolate, assemble it. Otherwise, run the pre-metagenome pipeline\n                        try:\n                            status = sample.run.Description\n                        except AttributeError:\n                            status = 'unknown'\n                        if status == 'metagenome':\n                            self.merge(sample)\n                        else:\n                            # Set the output directory\n                            sample.general.assembly_output = os.path.join(sample.general.outputdirectory,\n                                                                          'assembly_output')\n                            make_path(sample.general.assembly_output)\n                            sample.general.assemblyfile = os.path.join(sample.general.assembly_output,\n                                                                       '{name}_unfiltered.fasta'\n                                                                       .format(name=sample.name))\n                            sample.general.bestassemblyfile = os.path.join(sample.general.assembly_output,\n                                                                           '{name}.fasta'\n                                                                           .format(name=sample.name))\n                            fastqfiles = sample.general.trimmedcorrectedfastqfiles\n\n                            # Set the the forward fastq files\n                            sample.general.assemblyfastq = fastqfiles\n                            forward = fastqfiles[0]\n                            gz = True if '.gz' in forward else False\n                            # If there are two fastq files\n                            if len(fastqfiles) == 2:\n                                # Set the reverse fastq name https://github.com/ncbi/SKESA/issues/7\n                                sample.commands.assemble = 'skesa --fastq {fastqfiles} --cores {threads} ' \\\n                                                           '--use_paired_ends --vector_percent 1 ' \\\n                                                           '--contigs_out {contigs}'\\\n                                    .format(fastqfiles=','.join(fastqfiles),\n                                            threads=self.cpus,\n                                            contigs=sample.general.assemblyfile)\n                            # Same as above, but use single read settings for the assembler\n                            else:\n                                sample.commands.assemble = 'skesa --fastq {fastqfiles} --cores {threads} ' \\\n                                                           '--vector_percent 1 --contigs_out {contigs}'\\\n                                    .format(fastqfiles=','.join(fastqfiles),\n                                            threads=self.cpus,\n                                            contigs=sample.general.assemblyfile)\n                    # If there are no fastq files, populate the metadata appropriately\n                    else:\n                        sample.general.assembly_output = 'NA'\n                        sample.general.assemblyfastq = 'NA'\n                        sample.general.bestassemblyfile = 'NA'\n                except AttributeError:\n                    sample.general.assembly_output = 'NA'\n                    sample.general.assemblyfastq = 'NA'\n                    sample.general.trimmedcorrectedfastqfiles = 'NA'\n                    sample.general.bestassemblyfile = 'NA'\n                if sample.commands.assemble and not os.path.isfile(sample.general.assemblyfile):\n                    # Run the assembly\n                    out, err = run_subprocess(sample.commands.assemble)\n                    write_to_logfile(sample.commands.assemble,\n                                     sample.commands.assemble,\n                                     self.logfile,\n                                     sample.general.logout,\n                                     sample.general.logerr,\n                                     None,\n                                     None)\n                    write_to_logfile(out,\n                                     err,\n                                     self.logfile,\n                                     sample.general.logout,\n                                     sample.general.logerr,\n                                     None,\n                                     None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(self, sample):\n        # Set the assembly file to 'NA' as assembly is not desirable for metagenomes\n        sample.general.assemblyfile = 'NA'\n        # Can only merge paired-end\n        if len(sample.general.fastqfiles) == 2:\n            outpath = os.path.join(sample.general.outputdirectory, 'merged_reads')\n            make_path(outpath)\n            # Merge path - keep all the merged FASTQ files in one directory\n            merge_path = os.path.join(self.path, 'merged_reads')\n            make_path(merge_path)\n            # Set the name of the merged, and unmerged files\n            sample.general.mergedreads = \\\n                os.path.join(merge_path, '{}_paired.fastq.gz'.format(sample.name))\n            log = os.path.join(outpath, 'log')\n            error = os.path.join(outpath, 'err')\n            try:\n                if not os.path.isfile(sample.general.mergedreads):\n                    # Run the merging command\n                    out, err, cmd = bbtools.bbmerge(forward_in=sorted(sample.general.trimmedcorrectedfastqfiles)[0],\n                                                    merged_reads=sample.general.mergedreads,\n                                                    mix=True,\n                                                    returncmd=True,\n                                                    threads=self.cpus)\n                    write_to_logfile(out, err, self.logfile, sample.general.logout, sample.general.logerr, None, None)\n                    with open(log, 'w') as log_file:\n                        log_file.write(out)\n                    with open(error, 'w') as error_file:\n                        error_file.write(err)\n            except (CalledProcessError, IndexError):\n                delattr(sample.general, 'mergedreads')\n            # Set the name of the report to store the metagenome file merging results\n            report = os.path.join(self.reportpath, 'merged_metagenomes.csv')\n            # Extract the total number of reads, and the number of reads that could be paired from the bbmerge\n            # err stream\n            num_reads, num_pairs = self.reads(error)\n            # If the report doesn't exist, create it with the header and the results from the first sample\n            if not os.path.isfile(report):\n                with open(report, 'w') as report_file:\n                    report_file.write('Sample,TotalReads,PairedReads\\n{sample},{total},{paired}\\n'\n                                      .format(sample=sample.name,\n                                              total=num_reads,\n                                              paired=num_pairs))\n            # If the report exists, open it to determine which samples have already been added - useful if re-running\n            # the analysis\n            else:\n                lines = list()\n                with open(report, 'r') as report_file:\n                    for line in report_file:\n                        lines.append(line.split(',')[0])\n                # Add the results to the report\n                if sample.name not in lines:\n                    with open(report, 'a+') as report_file:\n                        report_file.write('{sample},{total},{paired}\\n'\n                                          .format(sample=sample.name,\n                                                  total=num_reads,\n                                                  paired=num_pairs))", "response": "Use bbmerge to merge paired FASTQ files for use in metagenomics pipelines. Create a report with the number of reads that could be paired."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine whether the contigs. fasta output file from the assembler is present.", "response": "def best_assemblyfile(self):\n        \"\"\"\n        Determine whether the contigs.fasta output file from the assembler is present. If not, set the .bestassembly\n        attribute to 'NA'\n        \"\"\"\n        for sample in self.metadata:\n            try:\n                # Set the name of the filtered assembly file\n                filtered_outputfile = os.path.join(self.path, 'raw_assemblies', '{}.fasta'.format(sample.name))\n                # Set the name of the unfiltered spades assembly output file\n                if os.path.isfile(sample.general.assemblyfile):\n                    size = os.path.getsize(sample.general.assemblyfile)\n                    # Ensure that the assembly isn't just an empty file\n                    if size == 0:\n                        sample.general.bestassemblyfile = 'NA'\n                    else:\n                        sample.general.bestassemblyfile = sample.general.assemblyfile\n                        shutil.copyfile(sample.general.bestassemblyfile, filtered_outputfile)\n                else:\n                    sample.general.bestassemblyfile = 'NA'\n                # Add the name and path of the filtered file to the metadata\n                sample.general.filteredfile = filtered_outputfile\n            except AttributeError:\n                sample.general.assemblyfile = 'NA'\n                sample.general.bestassemblyfile = 'NA'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef groups(self):\n        if not self._groups:\n            self._groups = ComponentGroups(self.api_client)\n        return self._groups", "response": "Property which returns a ComponentGroups instance for convenience."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the components of a specific component.", "response": "def get(self, component_id=None, **kwargs):\n        \"\"\"Get components\n\n        :param component_id: Component ID (optional)\n        :return: Components data (:class:`Generator`)\n\n        Additional named arguments may be passed and are directly transmitted\n        to API. It is useful to use the API search features.\n\n        .. seealso:: https://docs.cachethq.io/reference#get-components\n        .. seealso:: https://docs.cachethq.io/docs/advanced-api-usage\n        \"\"\"\n        path = 'components'\n        if component_id is not None:\n            path += '/%s' % component_id\n        return self.paginate_get(path, data=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, name, status, description=\"\", link=\"\", order=0,\n               group_id=0, enabled=True):\n        \"\"\"Create a new component\n\n        :param str name: Name of the component\n        :param int status: Status of the component; 1-4\n        :param str description: Description of the component (optional)\n        :param str link: A hyperlink to the component (optional)\n        :param int order: Order of the component (optional)\n        :param int group_id: The group ID of the component (optional)\n        :param bool enabled: Whether the component is enabled (optional)\n        :return: Created component data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#components\n        .. seealso:: https://docs.cachethq.io/docs/component-statuses\n        \"\"\"\n        data = ApiParams()\n        data['name'] = name\n        data['status'] = status\n        data['description'] = description\n        data['link'] = link\n        data['order'] = order\n        data['group_id'] = group_id\n        data['enabled'] = enabled\n        return self._post('components', data=data)['data']", "response": "Create a new component in Cachethq. io"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a component s metadata", "response": "def update(self, component_id, name=None, status=None, description=None,\n               link=None, order=None, group_id=None, enabled=True):\n        \"\"\"Update a component\n\n        :param int component_id: Component ID\n        :param str name: Name of the component (optional)\n        :param int status: Status of the component; 1-4\n        :param str description: Description of the component (optional)\n        :param str link: A hyperlink to the component (optional)\n        :param int order: Order of the component (optional)\n        :param int group_id: The group ID of the component (optional)\n        :param bool enabled: Whether the component is enabled (optional)\n        :return: Updated component data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#components\n        .. seealso:: https://docs.cachethq.io/docs/component-statuses\n        \"\"\"\n        data = ApiParams()\n        data['component'] = component_id\n        data['name'] = name\n        data['status'] = status\n        data['description'] = description\n        data['link'] = link\n        data['order'] = order\n        data['group_id'] = group_id\n        data['enabled'] = enabled\n        return self._put('components/%s' % component_id, data=data)['data']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets component groups by ID.", "response": "def get(self, group_id=None, **kwargs):\n        \"\"\"Get component groups\n\n        :param group_id: Component group ID (optional)\n        :return: Component groups data (:class:`dict`)\n\n        Additional named arguments may be passed and are directly transmitted\n        to API. It is useful to use the API search features.\n\n        .. seealso:: https://docs.cachethq.io/reference#get-componentgroups\n        .. seealso:: https://docs.cachethq.io/docs/advanced-api-usage\n        \"\"\"\n        path = 'components/groups'\n        if group_id is not None:\n            path += '/%s' % group_id\n        return self.paginate_get(path, data=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Component Group", "response": "def create(self, name, order=None, collapsed=None):\n        \"\"\"Create a new Component Group\n\n        :param str name: Name of the component group\n        :param int order: Order of the component group\n        :param int collapsed: Collapse the group? 0-2\n        :return: Created component group data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#post-componentgroups\n        \"\"\"\n        data = ApiParams()\n        data['name'] = name\n        data['order'] = order\n        data['collapsed'] = collapsed\n        return self._post('components/groups', data=data)['data']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate a Component Group by ID", "response": "def update(self, group_id, name=None, order=None, collapsed=None):\n        \"\"\"Update a Component Group\n\n        :param int group_id: Component Group ID\n        :param str name: Name of the component group\n        :param int order: Order of the group\n        :param int collapsed: Collapse the group?\n        :return: Updated component group data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#put-component-group\n        \"\"\"\n        data = ApiParams()\n        data['group'] = group_id\n        data['name'] = name\n        data['order'] = order\n        data['collapsed'] = collapsed\n        return self._put('components/groups/%s' % group_id, data=data)['data']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets Incidents data for a specific incident.", "response": "def get(self, incident_id=None, **kwargs):\n        \"\"\"Get incidents\n\n        :param int incident_id:\n        :return: Incidents data (:class:`dict`)\n\n        Additional named arguments may be passed and are directly transmitted\n        to API. It is useful to use the API search features.\n\n        .. seealso:: https://docs.cachethq.io/reference#get-incidents\n        .. seealso:: https://docs.cachethq.io/docs/advanced-api-usage\n        \"\"\"\n        path = 'incidents'\n        if incident_id is not None:\n            path += '/%s' % incident_id\n        return self.paginate_get(path, data=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Incident object", "response": "def create(self, name, message, status, visible, component_id=None,\n               component_status=None, notify=None, created_at=None,\n               template=None, tplvars=None):\n        \"\"\"Create a new Incident\n\n        :param str name: Name of the incident\n        :param str message: Incident explanation message\n        :param int status: Status of the incident\n        :param int visible: Whether the incident is publicly visible\n        :param int component_id: Component to update\n        :param int component_status: The status to update the given component\n        :param bool notify: Whether to notify subscribers\n        :param str created_at: When the incident was created\n        :param str template: The template slug to use\n        :param list tplvars: The variables to pass to the template\n        :return: Created incident data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#incidents\n        \"\"\"\n        data = ApiParams()\n        data['name'] = name\n        data['message'] = message\n        data['status'] = status\n        data['visible'] = visible\n        data['component_id'] = component_id\n        data['component_status'] = component_status\n        data['notify'] = notify\n        data['created_at'] = created_at\n        data['template'] = template\n        data['vars'] = tplvars\n        return self._post('incidents', data=data)['data']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, incident_id, name=None, message=None, status=None,\n               visible=None, component_id=None, component_status=None,\n               notify=None, created_at=None, template=None, tpl_vars=None):\n        \"\"\"Update an Incident\n\n        :param int incident_id: Incident ID\n        :param str name: Name of the incident\n        :param str message: Incident explanation message\n        :param int status: Status of the incident\n        :param int visible: Whether the incident is publicly visible\n        :param int component_id: Component to update\n        :param int component_status: The status to update the given component\n        :param bool notify: Whether to notify subscribers\n        :param str created_at: When the incident was created\n        :param str template: The template slug to use\n        :param list tpl_vars: The variables to pass to the template\n        :return: Created incident data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#update-an-incident\n        \"\"\"\n        data = ApiParams()\n        data['name'] = name\n        data['message'] = message\n        data['status'] = status\n        data['visible'] = visible\n        data['component_id'] = component_id\n        data['component_status'] = component_status\n        data['notify'] = notify\n        data['created_at'] = created_at\n        data['template'] = template\n        data['vars'] = tpl_vars\n        return self._put('incidents/%s' % incident_id, data=data)['data']", "response": "Update an incident s metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the metrics for a specific ID.", "response": "def get(self, metric_id=None, **kwargs):\n        \"\"\"Get metrics\n\n        :param int metric_id: Metric ID\n        :return: Metrics data (:class:`dict`)\n\n        Additional named arguments may be passed and are directly transmitted\n        to API. It is useful to use the API search features.\n\n        .. seealso:: https://docs.cachethq.io/reference#get-metrics\n        .. seealso:: https://docs.cachethq.io/docs/advanced-api-usage\n        \"\"\"\n        path = 'metrics'\n        if metric_id is not None:\n            path += '/%s' % metric_id\n        return self.paginate_get(path, data=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new metric in Cachethq", "response": "def create(self, name, suffix, description, default_value, display=None):\n        \"\"\"Create a new Metric\n\n        :param str name: Name of metric\n        :param str suffix: Metric unit\n        :param str description: Description of what the metric is measuring\n        :param int default_value: Default value to use when a point is added\n        :param int display: Display the chart on the status page\n        :return: Created metric data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#metrics\n        \"\"\"\n        data = ApiParams()\n        data['name'] = name\n        data['suffix'] = suffix\n        data['description'] = description\n        data['default_value'] = default_value\n        data['display'] = display\n        return self._post('metrics', data=data)['data']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a Metric Point to a Metric", "response": "def create(self, metric_id, value, timestamp=None):\n        \"\"\"Add a Metric Point to a Metric\n\n        :param int metric_id: Metric ID\n        :param int value: Value to plot on the metric graph\n        :param str timestamp: Unix timestamp of the point was measured\n        :return: Created metric point data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#post-metric-points\n        \"\"\"\n        data = ApiParams()\n        data['value'] = value\n        data['timestamp'] = timestamp\n        return self._post('metrics/%s/points' % metric_id, data=data)['data']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, email, verify=None, components=None):\n        data = ApiParams()\n        data['email'] = email\n        data['verify'] = verify\n        data['components'] = components\n        return self._post('subscribers', data=data)['data']", "response": "Create a new subscriber with the given email address and optional verification"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading in the BLAST output file and populate the dictionary with the parsed results", "response": "def parser(metadata, analysistype, fieldnames, cutoff, program):\n        \"\"\"\n        Read in the BLAST outputs, and populate dictionaries with the parsed results\n        :param metadata: type LIST: List of metadata objects\n        :param analysistype: type STR: Current analysis type\n        :param fieldnames: type LIST: List of fields used to in BLAST analyses\n        :param cutoff: type INT: Percent identity cutoff to use to determine if a match is present\n        :param program: type STR: BLAST program used in the analyses\n        :return: metadata: List of updated metadata objects\n        \"\"\"\n        for sample in metadata:\n            # Initialise a dictionary attribute to store results\n            sample[analysistype].blastresults = dict()\n            try:\n                # Open the sequence profile file as a dictionary\n                blastdict = DictReader(open(sample[analysistype].report), fieldnames=fieldnames, dialect='excel-tab')\n                resultdict = dict()\n                resultset = dict()\n                # Initialise a dictionary to store all the target sequences\n                sample[analysistype].targetsequence = dict()\n                coregenomes = list()\n                # Create a list of all the names of the database files, replace - with _, remove path and extension\n                for fasta in sample[analysistype].targets:\n                    fastaname = os.path.basename(os.path.splitext(fasta)[0]).replace('-', '_')\n                    fastaname = fastaname.split('.')[0]\n                    coregenomes.append(fastaname)\n                # Go through each BLAST result\n                for row in blastdict:\n                    # Ignore the headers\n                    if row['query_id'].startswith(fieldnames[0]):\n                        pass\n                    else:\n                        # Create the subject length variable - if the sequences are DNA (e.g. blastn), use the subject\n                        # length as usual; if the sequences are protein (e.g. tblastx), use the subject length / 3\n                        if program == 'blastn' or program == 'blastp' or program == 'blastx':\n                            subject_length = float(row['subject_length'])\n\n                        else:\n                            subject_length = float(row['subject_length']) / 3\n                        # Calculate the percent identity and extract the bitscore from the row\n                        # Percent identity is: (length of the alignment - number of mismatches) / total subject length\n                        percentidentity = float('{:0.2f}'.format((float(row['positives']) - float(row['gaps'])) /\n                                                                 subject_length * 100))\n                        # If the percent identity is greater than the cutoff\n                        if percentidentity >= cutoff:\n                            # Split off any | from the sample name\n                            target = row['subject_id'].split('|')[0]\n                            # As there are variable _ in the name, try to split off the last one only if there are\n                            #  multiple and only keep the first part of the split if there is one _ in the name\n                            underscored = '_'.join(target.split('_')[:-1]) if len(target.split('_')) > 2 else \\\n                                target.split('_')[0]\n                            try:\n                                # Update the dictionary with the reference genome and the target\n                                resultset[underscored].add(target)\n                            except KeyError:\n                                # Initialise the dictionary with the first hit\n                                resultset[underscored] = set()\n                                resultset[underscored].add(target)\n                # Get the number of unique genes per reference genome\n                for underscored, target_set in resultset.items():\n                    resultdict[underscored] = len(target_set)\n                # Sort the dictionary on the number of hits - best at the top\n                topcore = sorted(resultdict.items(), key=operator.itemgetter(1), reverse=True)\n                # If there are no results, populate negative results\n                if not resultdict:\n                    sample[analysistype].blastresults = 'NA'\n                # If results, add a string of the best number of hits, and a string of the total number of genes\n                # This is currently 1013. If this changes, I may re-implement a dynamic method of determining\n                # this value\n                else:\n                    sample[analysistype].blastresults[topcore[0][0]] = (str(topcore[0][1]), str(1013))\n            except FileNotFoundError:\n                sample[analysistype].blastresults = 'NA'\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the core genome report for each object in the metadata", "response": "def reporter(metadata, analysistype, reportpath):\n        \"\"\"\n        Create the core genome report\n        :param metadata: type LIST: List of metadata objects\n        :param analysistype: type STR: Current analysis type\n        :param reportpath: type STR: Absolute path to folder in which the reports are to be created\n        :return:\n        \"\"\"\n        header = 'Strain,ClosestRef,GenesPresent/Total,\\n'\n        data = str()\n        for sample in metadata:\n            try:\n                if sample[analysistype].blastresults != 'NA':\n                    if sample.general.closestrefseqgenus == 'Listeria':\n                        # Write the sample name, closest ref genome, and the # of genes found / total # of genes\n                        closestref = list(sample[analysistype].blastresults.items())[0][0]\n                        coregenes = list(sample[analysistype].blastresults.items())[0][1][0]\n                        # Find the closest reference file\n                        try:\n                            ref = glob(os.path.join(sample[analysistype].targetpath, '{fasta}*'\n                                                    .format(fasta=closestref)))[0]\n                        except IndexError:\n                            # Replace underscores with dashes to find files\n                            closestref = closestref.replace('_', '-')\n                            ref = glob(os.path.join(sample[analysistype].targetpath, '{fasta}*'\n                                                    .format(fasta=closestref)))[0]\n                        # Determine the number of core genes present in the closest reference file\n                        totalcore = 0\n                        for _ in SeqIO.parse(ref, 'fasta'):\n                            totalcore += 1\n                        # Add the data to the object\n                        sample[analysistype].targetspresent = coregenes\n                        sample[analysistype].totaltargets = totalcore\n                        sample[analysistype].coreresults = '{cg}/{tc}'.format(cg=coregenes,\n                                                                              tc=totalcore)\n                        row = '{sn},{cr},{cg}/{tc}\\n'.format(sn=sample.name,\n                                                             cr=closestref,\n                                                             cg=coregenes,\n                                                             tc=totalcore)\n                        # Open the report\n                        with open(os.path.join(sample[analysistype].reportdir,\n                                               '{sn}_{at}.csv'.format(sn=sample.name,\n                                                                      at=analysistype)), 'w') as report:\n                            # Write the row to the report\n                            report.write(header)\n                            report.write(row)\n                        data += row\n                    else:\n                        sample[analysistype].targetspresent = 'NA'\n                        sample[analysistype].totaltargets = 'NA'\n                        sample[analysistype].coreresults = 'NA'\n            except KeyError:\n                sample[analysistype].targetspresent = 'NA'\n                sample[analysistype].totaltargets = 'NA'\n                sample[analysistype].coreresults = 'NA'\n        with open(os.path.join(reportpath, 'coregenome.csv'), 'w') as report:\n            # Write the data to the report\n            report.write(header)\n            report.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef total_core(self):\n        corefile = os.path.join(self.reffilepath, self.analysistype, 'Escherichia', 'core_combined.fasta')\n        for record in SeqIO.parse(corefile, 'fasta'):\n            gene_name = record.id.split('-')[0]\n            if gene_name not in self.coregenomes:\n                self.coregenomes.append(gene_name)", "response": "Determine the total number of core genes present in the reference file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blastparser(self, report, sample, fieldnames):\n        try:\n            # Open the sequence profile file as a dictionary\n            blastdict = DictReader(open(report), fieldnames=self.fieldnames, dialect='excel-tab')\n            # Go through each BLAST result\n            for row in blastdict:\n                # Ignore the headers\n                if row['query_id'].startswith(fieldnames[0]):\n                    pass\n                else:\n                    # Calculate the percent identity and extract the bitscore from the row\n                    # Percent identity is the (length of the alignment - number of mismatches) / total subject length\n                    percentidentity = float('{:0.2f}'.format((float(row['positives']) - float(row['gaps'])) /\n                                                             float(row['subject_length']) * 100))\n                    # Split off any | and - from the sample name\n                    target = row['subject_id'].split('|')[0].split('-')[0]\n                    # If the hit passes the cutoff threshold, add it to the set of core genes present\n                    if percentidentity >= self.cutoff:\n                        sample[self.analysistype].coreset.add(target)\n        except FileNotFoundError:\n            pass", "response": "Parse the number of core genes present in the strain from the BLAST output file and add them to the sample object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a. csv file with the strain name the number of core genes present and the total number of core genes present and the total number of core genes present.", "response": "def reporter(self):\n        \"\"\"\n        Create a .csv file with the strain name, and the number of core genes present/the total number of core genes\n        \"\"\"\n        with open(os.path.join(self.reportpath, 'Escherichia_core.csv'), 'w') as report:\n            data = 'Strain,Genes Present/Total\\n'\n            for sample in self.runmetadata.samples:\n                # Convert the set to a list for JSON serialization\n                sample[self.analysistype].coreset = list(sample[self.analysistype].coreset)\n                sample[self.analysistype].coreresults = '{cs}/{cg}'.format(cs=len(sample[self.analysistype].coreset),\n                                                                           cg=len(self.coregenomes))\n                # Add strain name, the number of core genes present, and the number of total core genes to the string\n                data += '{sn},{cr}\\n'.format(sn=sample.name,\n                                             cr=sample[self.analysistype].coreresults)\n            report.write(data)\n\n        for sample in self.metadata:\n            # Remove the messy blast results and set/list of core genes from the object\n            try:\n                delattr(sample[self.analysistype], \"blastresults\")\n            except AttributeError:\n                pass\n            try:\n                delattr(sample[self.analysistype], 'coreset')\n            except AttributeError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_simple_output(self, stderr=STDOUT):\n        args = shlex.split(self.cmd)\n        proc = Popen(args, stdout=PIPE, stderr=stderr)\n        return proc.communicate()[0].decode(\"utf8\")", "response": "Executes a simple external command and returns its output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a piped command and gets the lines of the output in a list", "response": "def get_complex_output(self, stderr=STDOUT):\n        \"\"\"Executes a piped command and get the lines of the output in a list\n\n        :param stderr: where to put stderr\n        :return: output of command\n        \"\"\"\n        proc = Popen(self.cmd, shell=True, stdout=PIPE, stderr=stderr)\n        return proc.stdout.readlines()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes an external command and gets its output from the stdin through a pipe", "response": "def get_output_from_pipe(self, input_file):\n        \"\"\"Executes an external command and get its output. The command\n        receives its input_file from the stdin through a pipe\n        \n        :param input_file: input file\n        :return: output of command\n        \"\"\"\n        args = shlex.split(self.cmd)\n        p = Popen(args, stdout=PIPE, stdin=PIPE)  # | grep es\n        p.stdin.write(bytearray(input_file.encode(\"utf8\")))  # echo test |\n        return p.communicate()[0].decode(\"utf8\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a simple external command and returns its exit code", "response": "def get_return_code(self, stderr=STDOUT):\n        \"\"\"Executes a simple external command and return its exit status\n        \n        :param stderr: where to put stderr\n        :return: return code of command\n        \"\"\"\n        args = shlex.split(self.cmd)\n        return call(args, stdout=PIPE, stderr=stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the external command and returns its exitcode stdout and stderr", "response": "def get_exit_code(self):\n        \"\"\"Executes the external command and get its exitcode, stdout and stderr\n        \n        :return: exit code of command\n        \"\"\"\n        args = shlex.split(self.cmd)\n\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\n        out, err = proc.communicate()\n        out, err = out.decode(\"utf8\"), err.decode(\"utf8\")\n        exitcode = proc.returncode\n        #\n        return exitcode, out, err"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute_in_background(self):\n        # http://stackoverflow.com/questions/1605520\n        args = shlex.split(self.cmd)\n        p = Popen(args)\n        return p.pid", "response": "Executes a command in the background"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeeps a process alive.", "response": "def keep_alive(self):\n        \"\"\"Keeps a process alive. If the process terminates, it will restart it\n        The terminated processes become zombies. They die when their parent\n        terminates\n        \"\"\"\n        while True:\n            pid = self.execute_in_background()\n            p = psutil.Process(pid)\n            while p.is_running() and str(p.status) != 'zombie':\n                os.system('sleep 5')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the amount of free memory in the folder", "response": "def get_free_mb(folder):\n    \"\"\" Return folder/drive free space (in bytes)\n    \"\"\"\n    if platform.system() == 'Windows':\n        free_bytes = ctypes.c_ulonglong(0)\n        ctypes.windll.kernel32.GetDiskFreeSpaceExW(ctypes.c_wchar_p(folder), None, None, ctypes.pointer(free_bytes))\n        return free_bytes.value/1024/1024\n    else:\n        st = os.statvfs(folder)\n        return st.f_bavail * st.f_frsize/1024/1024"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef increment_title(title):\n    count = re.search('\\d+$', title).group(0)\n    new_title = title[:-(len(count))] + str(int(count)+1)\n    return new_title", "response": "Increments a string that ends in a number\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_limit(self, limit):\n        if limit > 0:\n            self.limit = limit\n        else:\n            raise ValueError(\"Rule limit must be strictly > 0 ({0} given)\"\n                             .format(limit))\n\n        return self", "response": "Checks if the given limit is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_filter(self, filter):\n        try:\n            self.filter = Filter.from_string(filter, self.limit)\n        except ValueError:\n            raise\n\n        return self", "response": "Builds a filter object from the given string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_action(self, action):\n        try:\n            self.action = Action.from_string(action)\n        except ValueError:\n            raise\n\n        return self", "response": "Builds an action object from the given string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrequesting the arguments for running", "response": "def get_args():\n    \"\"\"\n    request the arguments for running\n    \"\"\"\n    ap = argparse.ArgumentParser(description=\"Create frames for a movie that can be compiled using ffmpeg\")\n    ap.add_argument(\"start\", help=\"date string as start time\")\n    ap.add_argument(\"end\", help=\"date string as end time\")\n    ap.add_argument(\"step\", type=float, help=\"fraction of a day to step by\")\n    ap.add_argument(\"--config\", help=\"path to a config file\", default=\"config.json\")\n    return ap.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_three_color(data, time, step, config, shape=(1280, 1280), lower_val=(0, 0, 0), upper_val=(2.5, 2.5, 2.5)):\n    order = {'red': 0, 'green': 1, 'blue': 2}\n\n    three_color = np.zeros((shape[0], shape[1], 3))\n    channel_colors = {color: config.default[color] for color in ['red', 'green', 'blue']}\n\n    for color, channel in channel_colors.items():\n        if data[channel][1] is None or \\\n                abs((time - date_parser.parse(data[channel][0]['date-end'])).total_seconds()) > step.total_seconds()/2.0:\n            return np.zeros((shape[0], shape[1], 3))\n\n        three_color[:, :, order[color]] = data[channel][1]\n\n        # scale the image by the power\n        three_color[:, :, order[color]] = np.power(three_color[:, :, order[color]],\n                                                   config.default[\"{}_power\".format(color)])\n\n        # adjust the percentile thresholds\n        lower = lower_val[order[color]]\n        upper = upper_val[order[color]]\n        three_color[np.where(three_color[:, :, order[color]] < lower)] = lower\n        three_color[np.where(three_color[:, :, order[color]] > upper)] = upper\n\n    # image values must be between (0,1) so scale image\n    for color, index in order.items():\n        three_color[:, :, index] /= upper_val[order[color]]\n    return three_color", "response": "Create a three color image according to the config file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    args = get_args()\n    args.start = date_parser.parse(args.start)\n    args.end = date_parser.parse(args.end)\n    args.step = timedelta(args.step)\n    config = Config(args.config)\n\n    times = [args.start + i * args.step for i in range(int((args.end - args.start) / args.step))]\n\n    for i, time in enumerate(times):\n        make_plot(time, config, args.step)", "response": "main function for the\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a three color plot for a given time", "response": "def make_plot(time, config, step):\n    \"\"\"\n    create a three color and all composite images for a given time\n    NOTE: channel mins and maxes are currently hardcoded since this is a very specific script\n    :param i: the index to save the file as\n    :param time:\n    :param config:\n    :return:\n    \"\"\"\n    fig, ax = plt.subplots()\n    try:\n        result = Fetcher(time, products=config.products,\n                         suvi_composite_path=config.suvi_composite_path).fetch(multithread=False)\n        if result:\n            arr = make_three_color(result, time, step, config, upper_val=(2.4, 2.4, 2.4))\n        else:\n            arr = np.zeros((1280, 1280, 3))\n    except ValueError:\n\n        arr = np.zeros((1280, 1280, 3))\n\n    ax.imshow(arr, origin='lower')\n    timestr = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    fnextend = time.strftime(\"%Y%m%d%H%M%S\")\n    ax.set_title(timestr)\n    ax.set_axis_off()\n    fig.savefig(\"three_{}.png\".format(fnextend), bbox_inches='tight', dpi=300)\n    plt.close(fig)\n\n    channel_min = {'suvi-l2-ci094': 0,\n                   'suvi-l2-ci131': 0,\n                   'suvi-l2-ci171': 0,\n                   'suvi-l2-ci195': 0,\n                   'suvi-l2-ci284': 0,\n                   'suvi-l2-ci304': 0}\n    channel_max = {'suvi-l2-ci094': 1,\n                   'suvi-l2-ci131': 1,\n                   'suvi-l2-ci171': 1.8,\n                   'suvi-l2-ci195': 1.8,\n                   'suvi-l2-ci284': 1.8,\n                   'suvi-l2-ci304': 2.5}\n\n    for channel in channel_min:\n        fig, ax = plt.subplots()\n        if result[channel][1] is not None and \\\n                abs((time - date_parser.parse(result[channel][0]['date-end'])).total_seconds()) < step.total_seconds()/2.0:\n            dat = np.power(result[channel][1], 0.25)\n            ax.set_title(date_parser.parse(result[channel][0]['date-obs']).strftime(\"%Y-%m-%d %H:%M:%S\"))\n            dat[np.isnan(dat)] = 0\n        else:\n            dat = np.zeros((1280, 1280))\n            ax.set_title(timestr)\n        ax.imshow(dat, vmin=channel_min[channel], vmax=channel_max[channel], cmap='gray', origin='lower')\n        ax.set_axis_off()\n        fig.savefig(\"{}_{}.png\".format(channel, fnextend), bbox_inches='tight', dpi=300)\n        plt.close(fig)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef overall():\n        return ZeroOrMore(Grammar.comment) + Dict(ZeroOrMore(Group(\n            Grammar._section + ZeroOrMore(Group(Grammar.line)))\n            ))", "response": "The overall grammer for pulling apart the main input files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef file():\n        return (\n            Optional(Word(alphanums).setResultsName('alias') +\n                Suppress(Literal('.'))) + Suppress(White()) +\n            Word(approved_printables).setResultsName('filename')\n            )", "response": "Grammar for files found in the overall input files.\t"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command():\n        return (\n            OneOrMore(\n                Word(approved_printables+' ').setResultsName('command',\n                    listAllMatches=True) ^\n                Grammar.__command_input_output.setResultsName('_in',\n                    listAllMatches=True)\n                )\n            )", "response": "Grammar for commands found in the overall input files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef listen_to_event_updates():\n    def callback(event):\n        print('Event:', event)\n\n    client.create_event_subscription(instance='simulator', on_data=callback)\n\n    sleep(5)", "response": "Subscribe to events updates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the name of the jb_sceneNode that describes the current scene or None if there is no scene node.", "response": "def get_current_scene_node():\n    \"\"\"Return the name of the jb_sceneNode, that describes the current scene or None if there is no scene node.\n\n    :returns: the full name of the node or none, if there is no scene node\n    :rtype: str | None\n    :raises: None\n    \"\"\"\n    c = cmds.namespaceInfo(':', listOnlyDependencyNodes=True, absoluteName=True, dagPath=True)\n    l = cmds.ls(c, type='jb_sceneNode', absoluteName=True)\n    if not l:\n        return\n    else:\n        for n in sorted(l):\n            if not cmds.listConnections(\"%s.reftrack\" % n, d=False):\n                return n"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef updateSpec(self, *args, **kwargs):\n        if args[0] is None:\n            self.specPlot.clearImg()\n        elif isinstance(args[0], basestring):\n            self.specPlot.fromFile(*args, **kwargs)\n        else:\n            self.specPlot.updateData(*args,**kwargs)", "response": "Updates the spectrogram. First argument can be a filename, \n        or a data array. If no arguments are given, clears the spectrograms.\n\n        For other arguments, see: :meth:`SpecWidget.updateData<sparkle.gui.plotting.pyqtgraph_widgets.SpecWidget.updateData>`"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef showSpec(self, fname):\n        if not self.specPlot.hasImg() and fname is not None:\n            self.specPlot.fromFile(fname)", "response": "Draws the spectrogram if it is currently None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef updateSpiketrace(self, xdata, ydata, plotname=None):\n        if plotname is None:\n            plotname = self.responsePlots.keys()[0]\n\n        if len(ydata.shape) == 1:\n            self.responsePlots[plotname].updateData(axeskey='response', x=xdata, y=ydata)\n        else:\n            self.responsePlots[plotname].addTraces(xdata, ydata)", "response": "Updates the spike trace with the given data and the given data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addRasterPoints(self, xdata, repnum, plotname=None):\n        if plotname is None:\n            plotname = self.responsePlots.keys()[0]\n        ydata = np.ones_like(xdata)*repnum\n        self.responsePlots[plotname].appendData('raster', xdata, ydata)", "response": "Add a list of points to raster plot in any order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef updateSignal(self, xdata, ydata, plotname=None):\n        if plotname is None:\n            plotname = self.responsePlots.keys()[0]\n        self.responsePlots[plotname].updateData(axeskey='stim', x=xdata, y=ydata)", "response": "Updates the trace of the outgoing signal\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setXlimits(self, lims):\n        # update all \"linked\", plots\n        self.specPlot.setXlim(lims)\n        for plot in self.responsePlots.values():\n            plot.setXlim(lims)\n        # ridiculous...\n        sizes = self.splittersw.sizes()\n        if len(sizes) > 1:\n            if self.badbadbad:\n                sizes[0] +=1\n                sizes[1] -=1\n            else:\n                sizes[0] -=1\n                sizes[1] +=1\n            self.badbadbad = not self.badbadbad\n            self.splittersw.setSizes(sizes)\n\n        self._ignore_range_signal = False", "response": "Sets the x limits of the trace plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setNreps(self, nreps):\n        for plot in self.responsePlots.values():\n            plot.setNreps(nreps)", "response": "Sets the number of reps before the raster plot resets"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef specAutoRange(self):\n        trace_range = self.responsePlots.values()[0].viewRange()[0]\n        vb = self.specPlot.getViewBox()\n        vb.autoRange(padding=0)\n        self.specPlot.setXlim(trace_range)", "response": "Auto adjusts the visible range of the spectrogram"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading pertinent information from the image headers and calculate the default thematic map.", "response": "def interpret_header(self):\n        \"\"\"\n        Read pertinent information from the image headers,\n         especially location and radius of the Sun to calculate the default thematic map\n        :return: setes self.date, self.cy, self.cx, and self.sun_radius_pixel\n        \"\"\"\n        # handle special cases since date-obs field changed names\n        if 'DATE_OBS' in self.header:\n            self.date = self.header['DATE_OBS']\n        elif 'DATE-OBS' in self.header:\n            self.date = self.header['DATE-OBS']\n        else:\n            raise Exception(\"Image does not have a DATE_OBS or DATE-OBS field\")\n\n        self.cy, self.cx = self.header['CRPIX1'], self.header['CRPIX2']\n        sun_radius_angular = sun.solar_semidiameter_angular_size(t=time.parse_time(self.date)).arcsec\n        arcsec_per_pixel = self.header['CDELT1']\n        self.sun_radius_pixel = (sun_radius_angular / arcsec_per_pixel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self):\n        out = Outgest(self.output, self.selection_array.astype('uint8'), self.headers, self.config_path)\n        out.save()\n        out.upload()", "response": "Save as a FITS file and attempt an upload if designated in the configuration file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures the three color image according to the requested parameters.", "response": "def configure_threecolor_image(self):\n        \"\"\"\n        configures the three color image according to the requested parameters\n        :return: nothing, just updates self.image\n        \"\"\"\n        order = {'red': 0, 'green': 1, 'blue': 2}\n        self.image = np.zeros((self.shape[0], self.shape[1], 3))\n        for color, var in self.multicolorvars.items():\n            channel = var.get()  # determine which channel should be plotted as this color\n            self.image[:, :, order[color]] = self.data[channel]\n\n            # scale the image by the power\n            self.image[:, :, order[color]] = np.power(self.image[:, :, order[color]],\n                                                      self.multicolorpower[color].get())\n\n            # adjust the percentile thresholds\n            lower = np.nanpercentile(self.image[:, :, order[color]], self.multicolormin[color].get())\n            upper = np.nanpercentile(self.image[:, :, order[color]], self.multicolormax[color].get())\n            self.image[np.where(self.image[:, :, order[color]] < lower)] = lower\n            self.image[np.where(self.image[:, :, order[color]] > upper)] = upper\n\n        # image values must be between (0,1) so scale image\n        for color, index in order.items():\n            self.image[:, :, index] /= np.nanmax(self.image[:, :, index])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure_singlecolor_image(self, scale=False):\n        # determine which channel to use\n        self.image = self.data[self.singlecolorvar.get()]\n\n        # scale the image by requested power\n        self.image = np.power(self.image, self.singlecolorpower.get())\n\n        # adjust the percentile thresholds\n        lower = np.nanpercentile(self.image, self.singlecolormin.get())\n        upper = np.nanpercentile(self.image, self.singlecolormax.get())\n        self.image[self.image < lower] = lower\n        self.image[self.image > upper] = upper\n\n        # image values must be between (0,1) so scale image\n        self.image /= np.nanmax(self.image)", "response": "configure the image according to the requested parameters"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef updateArray(self, array, indices, value):\n        lin = np.arange(array.size)\n        new_array = array.flatten()\n        new_array[lin[indices]] = value\n        return new_array.reshape(array.shape)", "response": "updates the array so that pixels at indices take on value\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_canvas_frame(self):\n\n        self.fig, (self.imageax, self.previewax) = plt.subplots(ncols=2,\n                                                                figsize=self.canvas_size,\n                                                                sharex=True, sharey=True,\n                                                                gridspec_kw=self.subplot_grid_spec)\n        self.canvas = FigureCanvasTkAgg(self.fig, master=self.canvas_frame)\n        self.canvas.mpl_connect('button_press_event', self.onclick)\n        self.canvas.mpl_connect('key_press_event', self.onpress)\n\n        # set up the channel data view\n        self.configure_threecolor_image()\n        self.imageplot = self.imageax.imshow(self.image)\n        self.imageax.set_xlim([0, self.shape[0]])\n        self.imageax.set_ylim([0, self.shape[0]])\n        self.imageax.set_axis_off()\n        self.history.append(self.selection_array)\n        cmap = self.config.solar_cmap\n        self.mask = self.previewax.imshow(self.selection_array,\n                                          origin='lower',\n                                          interpolation='nearest',\n                                          cmap=cmap,\n                                          vmin=-1, vmax=max([num for _, num in self.config.solar_classes])+1)\n        self.previewax.set_xlim([0, self.shape[0]])\n        self.previewax.set_ylim([0, self.shape[0]])\n        self.previewax.set_aspect(\"equal\")\n        self.previewax.set_axis_off()\n\n        # add selection layer for lasso\n        self.pix = np.arange(self.shape[0])  # assumes square image\n        xv, yv = np.meshgrid(self.pix, self.pix)\n        self.pix = np.vstack((xv.flatten(), yv.flatten())).T\n\n        lineprops = dict(color=self.config.default['lasso_color'],\n                         linewidth=self.config.default['lasso_width'])\n        self.lasso = LassoSelector(self.imageax, self.onlasso, lineprops=lineprops)\n\n        # display everything\n        self.canvas.show()\n        self.canvas.get_tk_widget().pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n        self.canvas._tkcanvas.pack(side=tk.TOP, fill=tk.BOTH, expand=True)\n\n        # add the tool bar\n        self.toolbarcenterframe = tk.LabelFrame(self.canvas_frame,\n                                                borderwidth=0,\n                                                text=\"Draw: unlabeled\",\n                                                relief=tk.FLAT,\n                                                labelanchor=tk.N,\n                                                background='red')\n        toolbarframe = tk.Frame(self.toolbarcenterframe)\n        toolbar = CustomToolbar(self.canvas, toolbarframe, self.toolbarcenterframe, self)\n        toolbar.update()\n        self.fig.canvas.toolbar.set_message = lambda x: \"\"  # remove state reporting\n        toolbarframe.pack()\n        self.toolbarcenterframe.pack(side=tk.BOTTOM, fill=tk.X)", "response": "Create the data and thematic map images for the first time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef onpress(self, event):\n        if event.key == 'c':  # clears all the contours\n            for patch in self.region_patches:\n                patch.remove()\n            self.region_patches = []\n            self.fig.canvas.draw_idle()\n        elif event.key == \"u\":  # undo a label\n            self.undobutton_action()", "response": "Reacts to key commands\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef onclick(self, event):\n        if event.inaxes == self.previewax:\n            y, x = int(event.xdata), int(event.ydata)\n            label = self.selection_array[x, y]\n            contiguous_regions = scipy.ndimage.label(self.selection_array == label)[0]\n            this_region = contiguous_regions == (contiguous_regions[x, y])\n\n            # remove the boundaries so any region touching the edge isn't drawn odd\n            this_region[0, :] = 0\n            this_region[:, 0] = 0\n            this_region[this_region.shape[0]-1, :] = 0\n            this_region[:, this_region.shape[1]-1] = 0\n\n            # convert the region mask into just a true/false array of its boundary pixels\n            edges = binary_erosion(this_region) ^ this_region\n\n            # convert the boundary pixels into a path, moving around instead of just where\n            x, y = np.where(edges)\n            coords = np.dstack([x, y])[0]\n            path = [coords[0]]\n            coords = coords[1:]\n\n            while len(coords):\n                dist = np.sum(np.abs(path[-1] - coords), axis=1)\n                neighbor_index = np.argmin(dist)\n\n                if dist[neighbor_index] < 5:\n                    path.append(coords[neighbor_index].copy())\n                    coords[neighbor_index:-1] = coords[neighbor_index + 1:]\n                    coords = coords[:-1]\n                else:\n                    break\n\n            path = np.array(path)\n\n            clips = []\n            while len(coords) > 5:\n                dist = np.sum(np.abs(path[-1] - coords), axis=1)\n                neighbor_index = np.argmin(dist)\n                clip = [coords[neighbor_index].copy()]\n                coords[neighbor_index:-1] = coords[neighbor_index + 1:]\n                coords = coords[:-1]\n                while len(coords):\n                    dist = np.sum(np.abs(clip[-1] - coords), axis=1)\n                    neighbor_index = np.argmin(dist)\n                    if dist[neighbor_index] < 5:\n                        clip.append(coords[neighbor_index].copy())\n                        coords[neighbor_index:-1] = coords[neighbor_index + 1:]\n                        coords = coords[:-1]\n                    else:\n                        break\n                clips.append(np.array(clip))\n\n            # draw the continguous  on the selection area\n            self.region_patches.append(PatchCollection(\n                [Polygon(np.dstack([path[:, 1], path[:, 0]])[0], False,\n                         fill=False, facecolor=None,\n                         edgecolor=\"black\", alpha=1, lw=2.5)] +\n                [Polygon(np.dstack([clip[:, 1], clip[:, 0]])[0], False,\n                         fill=False, facecolor=None,\n                         edgecolor=\"black\", alpha=1, lw=2.0) for clip in clips],\n                match_original=True))\n            self.imageax.add_collection(self.region_patches[-1])\n            self.fig.canvas.draw_idle()", "response": "Draw contours on the data for a click in the thematic map."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_options_frame(self):\n        self.tab_frame = ttk.Notebook(self.option_frame, width=800)\n        self.tab_configure = tk.Frame(self.tab_frame)\n        self.tab_classify = tk.Frame(self.tab_frame)\n        self.make_configure_tab()\n        self.make_classify_tab()\n\n        self.tab_frame.add(self.tab_configure, text=\"Configure\")\n        self.tab_frame.add(self.tab_classify, text=\"Classify\")\n        self.tab_frame.pack(fill=tk.BOTH, expand=True)", "response": "make the frame that allows for configuration and classification"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nswapping from the multicolor image to the single color image", "response": "def disable_multicolor(self):\n        \"\"\" swap from the multicolor image to the single color image \"\"\"\n        # disable the multicolor image\n        for color in ['red', 'green', 'blue']:\n            self.multicolorscales[color].config(state=tk.DISABLED, bg='grey')\n            self.multicolorframes[color].config(bg='grey')\n            self.multicolorlabels[color].config(bg='grey')\n            self.multicolordropdowns[color].config(bg='grey', state=tk.DISABLED)\n            self.multicolorminscale[color].config(bg='grey', state=tk.DISABLED)\n            self.multicolormaxscale[color].config(bg='grey', state=tk.DISABLED)\n\n        # enable the single color\n        self.singlecolorscale.config(state=tk.NORMAL, bg=self.single_color_theme)\n        self.singlecolorframe.config(bg=self.single_color_theme)\n        self.singlecolorlabel.config(bg=self.single_color_theme)\n        self.singlecolordropdown.config(bg=self.single_color_theme, state=tk.NORMAL)\n        self.singlecolorminscale.config(bg=self.single_color_theme, state=tk.NORMAL)\n        self.singlecolormaxscale.config(bg=self.single_color_theme, state=tk.NORMAL)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_button_action(self):\n        if self.mode.get() == 3:  # threecolor\n            self.configure_threecolor_image()\n        elif self.mode.get() == 1:  # singlecolor\n            self.configure_singlecolor_image()\n        else:\n            raise ValueError(\"mode can only be singlecolor or threecolor\")\n\n        self.imageplot.set_data(self.image)\n        if self.mode.get() == 1:  # singlecolor\n            self.imageplot.set_cmap('gist_gray')\n        self.fig.canvas.draw_idle()", "response": "when update button is clicked refresh the data preview"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitialing set up of configure tab", "response": "def make_configure_tab(self):\n        \"\"\" initial set up of configure tab\"\"\"\n        # Setup the choice between single and multicolor\n        modeframe = tk.Frame(self.tab_configure)\n        self.mode = tk.IntVar()\n        singlecolor = tk.Radiobutton(modeframe, text=\"Single color\", variable=self.mode,\n                                     value=1, command=lambda: self.disable_multicolor())\n        multicolor = tk.Radiobutton(modeframe, text=\"Three color\", variable=self.mode,\n                                    value=3, command=lambda: self.disable_singlecolor())\n        self.mode.set(3)\n        singlecolor.pack(side=tk.LEFT)\n        multicolor.pack(side=tk.LEFT)\n\n        updatebutton = tk.Button(master=modeframe, text=\"Update\",\n                                 command=self.update_button_action)\n        updatebutton.pack(side=tk.RIGHT)\n        modeframe.grid(row=0, column=0)\n        self.setup_multicolor()\n        self.setup_singlecolor()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_classify_tab(self):\n        self.pick_frame = tk.Frame(self.tab_classify)\n        self.pick_frame2 = tk.Frame(self.tab_classify)\n\n        self.solar_class_var = tk.IntVar()\n        self.solar_class_var.set(0)  # initialize to unlabeled\n        buttonnum = 0\n        frame = [self.pick_frame, self.pick_frame2]\n        for text, value in self.config.solar_classes:\n            b = tk.Radiobutton(frame[buttonnum % 2], text=text,\n                               variable=self.solar_class_var,\n                               value=value, background=self.config.solar_colors[text],\n                               indicatoron=0, width=50, height=2, command=self.change_class)\n            b.pack(fill=tk.BOTH, expand=1)\n            buttonnum += 1\n\n\n        self.pick_frame.grid(row=0, column=0, rowspan=5, sticky=tk.W + tk.E + tk.N + tk.S)\n        self.pick_frame2.grid(row=0, column=1, rowspan=5, sticky=tk.W + tk.E + tk.N + tk.S)\n\n        undobutton = tk.Button(master=self.tab_classify, text=\"Undo\",\n                               command=self.undobutton_action)\n        undobutton.grid(row=6, column=0, columnspan=2, sticky=tk.W + tk.E)", "response": "initial set up of classification tab"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitial setup of single color options and variables", "response": "def setup_singlecolor(self):\n        \"\"\" initial setup of single color options and variables\"\"\"\n        self.singlecolorframe = tk.Frame(self.tab_configure, bg=self.single_color_theme)\n        channel_choices = sorted(list(self.data.keys()))\n        self.singlecolorlabel = tk.Label(self.singlecolorframe, text=\"single\", bg=self.single_color_theme, width=10)\n        self.singlecolorvar = tk.StringVar()\n        self.singlecolorpower = tk.DoubleVar()\n        self.singlecolormin = tk.DoubleVar()\n        self.singlecolormax = tk.DoubleVar()\n        self.singlecolordropdown = tk.OptionMenu(self.singlecolorframe, self.singlecolorvar, *channel_choices)\n        self.singlecolorscale = tk.Scale(self.singlecolorframe, variable=self.singlecolorpower,\n                                         orient=tk.HORIZONTAL, from_=self.config.ranges['single_color_power_min'],\n                                         bg=self.single_color_theme,\n                                         to_=self.config.ranges['single_color_power_max'],\n                                         resolution=self.config.ranges['single_color_power_resolution'],\n                                         length=200)\n        self.singlecolorminscale = tk.Scale(self.singlecolorframe, variable=self.singlecolormin,\n                                            orient=tk.HORIZONTAL, from_=0,\n                                            bg=self.single_color_theme,\n                                            to_=self.config.ranges['single_color_vmin'],\n                                            resolution=self.config.ranges['single_color_vresolution'], length=200)\n\n        self.singlecolormaxscale = tk.Scale(self.singlecolorframe, variable=self.singlecolormax,\n                                            orient=tk.HORIZONTAL, from_=self.config.ranges['single_color_vmax'],\n                                            bg=self.single_color_theme,\n                                            to_=100, resolution=self.config.ranges['single_color_vresolution'],\n                                            length=200)\n\n        self.singlecolorvar.set(self.config.products_map[self.config.default['single']])\n        self.singlecolorpower.set(self.config.default['single_power'])\n        self.singlecolormin.set(0)\n        self.singlecolormax.set(100)\n        self.singlecolordropdown.config(bg=self.single_color_theme, width=10)\n        self.singlecolorlabel.pack(side=tk.LEFT)\n        self.singlecolorscale.pack(side=tk.RIGHT)\n        self.singlecolormaxscale.pack(side=tk.RIGHT)\n        self.singlecolorminscale.pack(side=tk.RIGHT)\n        self.singlecolordropdown.pack()\n        self.singlecolorframe.grid(row=4, columnspan=5, rowspan=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitials setup of multicolor options and variables", "response": "def setup_multicolor(self):\n        \"\"\" initial setup of multicolor options and variables\"\"\"\n        # Setup the options for multicolor\n        multicolormasterframe = tk.Frame(self.tab_configure)\n        channel_choices = sorted(list(self.data.keys()))\n        rgb = ['red', 'green', 'blue']\n        self.multicolorframes = {color: tk.Frame(multicolormasterframe, bg=color) for color in rgb}\n        self.multicolorlabels = {color: tk.Label(self.multicolorframes[color], text=color, bg=color, width=10) for color\n                                 in rgb}\n        self.multicolorvars = {color: tk.StringVar() for color in rgb}\n        self.multicolorpower = {color: tk.DoubleVar() for color in rgb}\n        self.multicolormin = {color: tk.DoubleVar() for color in rgb}\n        self.multicolormax = {color: tk.DoubleVar() for color in rgb}\n\n        self.multicolordropdowns = {color: tk.OptionMenu(self.multicolorframes[color],\n                                                         self.multicolorvars[color],\n                                                         *channel_choices) for color in rgb}\n\n        self.multicolorscales = {color: tk.Scale(self.multicolorframes[color],\n                                                 variable=self.multicolorpower[color],\n                                                 orient=tk.HORIZONTAL,\n                                                 from_=self.config.ranges['multi_color_power_min'],\n                                                 to_=self.config.ranges['multi_color_power_max'], bg=color,\n                                                 resolution=self.config.ranges['multi_color_power_resolution'],\n                                                 length=200) for color in rgb}\n        self.multicolorminscale = {color: tk.Scale(self.multicolorframes[color],\n                                                   variable=self.multicolormin[color],\n                                                   orient=tk.HORIZONTAL, from_=0,\n                                                   to_=self.config.ranges['multi_color_vmin'], bg=color,\n                                                   resolution=self.config.ranges['multi_color_vresolution'],\n                                                   length=200) for color in rgb}\n        self.multicolormaxscale = {color: tk.Scale(self.multicolorframes[color],\n                                                   variable=self.multicolormax[color],\n                                                   orient=tk.HORIZONTAL, from_=self.config.ranges['multi_color_vmax'],\n                                                   to_=100, bg=color,\n                                                   resolution=self.config.ranges['multi_color_vresolution'],\n                                                   length=200) for color in rgb}\n\n        for color in rgb:\n            self.multicolorvars[color].set(self.config.products_map[self.config.default[color]])\n            self.multicolorpower[color].set(self.config.default[color + \"_power\"])\n\n            self.multicolormin[color].set(0)\n            self.multicolormax[color].set(100)\n            self.multicolordropdowns[color].config(bg=color, width=10)\n            self.multicolorlabels[color].pack(side=tk.LEFT)\n\n            self.multicolorscales[color].pack(side=tk.RIGHT)\n            self.multicolormaxscale[color].pack(side=tk.RIGHT)\n            self.multicolorminscale[color].pack(side=tk.RIGHT)\n            self.multicolordropdowns[color].pack()\n            self.multicolorframes[color].pack(fill=tk.BOTH)\n        multicolormasterframe.grid(row=1, column=0, columnspan=5, rowspan=3)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef undobutton_action(self):\n        if len(self.history) > 1:\n            old = self.history.pop(-1)\n            self.selection_array = old\n            self.mask.set_data(old)\n            self.fig.canvas.draw_idle()", "response": "when undo is clicked revert the thematic map to the previous state"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef change_class(self):\n        self.toolbarcenterframe.config(text=\"Draw: {}\".format(self.config.solar_class_name[self.solar_class_var.get()]))", "response": "on changing the classification label update the draw text"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_circle(self, center, radius, array, value, mode=\"set\"):\n        ri, ci = draw.circle(center[0], center[1],\n                             radius=radius,\n                             shape=array.shape)\n        if mode == \"add\":\n            array[ri, ci] += value\n        elif mode == \"set\":\n            array[ri, ci] = value\n        else:\n            raise ValueError(\"draw_circle mode must be 'set' or 'add' but {} used\".format(mode))\n        return ri, ci, array[ri,ci]", "response": "Draw a circle on the input array and fills it with value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws an annulus on the input array and fills it with the specified value.", "response": "def draw_annulus(self, center, inner_radius, outer_radius, array, value, mode=\"set\"):\n        \"\"\"\n        Draws an annulus of specified radius on the input array and fills it with specified value\n        :param center: a tuple for the center of the annulus\n        :type center: tuple (x,y)\n        :param inner_radius: how many pixels in radius the interior empty circle is, where the annulus begins\n        :type inner_radius: int\n        :param outer_radius: how many pixels in radius the larger outer circle is, where the annulus ends\n        :typde outer_radius: int\n        :param array: image to draw annulus on\n        :type array: size (m,n) numpy array\n        :param value: what value to fill the annulus with\n        :type value: float\n        :param mode: if \"set\" will assign the circle interior value, if \"add\" will add the value to the circle interior,\n            throws exception otherwise\n        :type mode: string, either \"set\" or \"add\"\n        :return: updates input array and then returns it with the annulus coordinates as a tuple\n        \"\"\"\n        if mode == \"add\":\n            self.draw_circle(center, outer_radius, array, value)\n            self.draw_circle(center, inner_radius, array, -value)\n        elif mode == \"set\":\n            ri, ci, existing = self.draw_circle(center, inner_radius, array, -value)\n            self.draw_circle(center, outer_radius, array, value)\n            array[ri, ci] = existing\n        else:\n            raise ValueError(\"draw_annulus mode must be 'set' or 'add' but {} used\".format(mode))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw suggested sun disk limb and empty background for the given class.", "response": "def draw_default(self, inside=5, outside=15):\n        \"\"\"\n        Draw suggested sun disk, limb, and empty background\n        :param inside: how many pixels from the calculated solar disk edge to go inward for the limb\n        :param outside: how many pixels from the calculated solar disk edge to go outward for the limb\n        :return: updates the self.selection_array\n        \"\"\"\n        # fill everything with empty outer space\n        if 'outer_space' in self.config.solar_class_index:\n            self.selection_array[:, :] = self.config.solar_class_index['outer_space']\n        elif 'empty_outer_space' in self.config.solar_class_index:\n            self.selection_array[:, :] = self.config.solar_class_index['empty_outer_space']\n        else:\n            raise ValueError(\"outer_space or empty_outer_space must be classes with colors.\")\n\n        # draw the limb label in its location\n        self.draw_annulus((self.cx, self.cy),\n                          self.sun_radius_pixel - inside,\n                          self.sun_radius_pixel + outside,\n                          self.selection_array,\n                          self.config.solar_class_index['limb'])\n\n        # draw quiet sun in its location\n        self.draw_circle((self.cx, self.cy),\n                         self.sun_radius_pixel - inside,\n                         self.selection_array,\n                         self.config.solar_class_index['quiet_sun'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef values(self):\n        self.vals['nfft'] = self.ui.nfftSpnbx.value()\n        self.vals['window'] = str(self.ui.windowCmbx.currentText()).lower()\n        self.vals['overlap'] = self.ui.overlapSpnbx.value()\n        return self.vals", "response": "Gets the parameter values for the current set of items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the command-line args, and calls run.", "response": "def main():\n    \"\"\" Parses the command-line args, and calls run. \"\"\"\n    parser = argparse.ArgumentParser(\n        description='A pipeline that generates analysis pipelines.')\n    parser.add_argument('input', nargs='?',\n                   help='A valid metapipe configuration file.')\n    parser.add_argument('-o', '--output',\n                   help='An output destination. If none is provided, the '\n                   'results will be printed to stdout.',\n                   default=sys.stdout)\n    parser.add_argument('-t', '--temp',\n                   help='A desired metapipe binary file. This is used to store '\n                   'temp data between generation and execution. '\n                   '(Default: \"%(default)s\")', default='.metapipe')\n    parser.add_argument('-s', '--shell',\n                   help='The path to the shell to be used when executing the '\n                   'pipeline. (Default: \"%(default)s)\"',\n                   default='/bin/bash')\n    parser.add_argument('-r', '--run',\n                   help='Run the pipeline as soon as it\\'s ready.',\n                   action='store_true')\n    parser.add_argument('-n', '--name',\n                   help='A name for the pipeline.',\n                   default='')\n    parser.add_argument('-j', '--job-type',\n                   help='The destination for calculations (i.e. local, a PBS '\n                   'queue on a cluster, etc).\\nOptions: {}. '\n                   '(Default: \"%(default)s)\"'.format(JOB_TYPES.keys()),\n                   default='local')\n    parser.add_argument('-p', '--max-jobs',\n                   help='The maximum number of concurrent jobs allowed. '\n                   'Defaults to maximum available cores.',\n                   default=None)\n    parser.add_argument('--report-type',\n                   help='The output report type. By default metapipe will '\n                   'print updates to the console. \\nOptions: {}. '\n                   '(Default: \"%(default)s)\"'.format(QUEUE_TYPES.keys()),\n                   default='text')\n    parser.add_argument('-v','--version',\n                    help='Displays the current version of the application.',\n                    action='store_true')\n    args = parser.parse_args()\n\n    if args.version:\n        print('Version: {}'.format(__version__))\n        sys.exit(0)\n\n    try:\n        with open(args.input) as f:\n            config = f.read()\n    except IOError:\n        print('No valid config file found.')\n        return -1\n\n    run(config, args.max_jobs, args.output, args.job_type, args.report_type,\n        args.shell, args.temp, args.run)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(config, max_jobs, output=sys.stdout, job_type='local',\n        report_type='text', shell='/bin/bash', temp='.metapipe', run_now=False):\n    \"\"\" Create the metapipe based on the provided input. \"\"\"\n    if max_jobs == None:\n        max_jobs = cpu_count()\n\n    parser = Parser(config)\n    try:\n        command_templates = parser.consume()\n    except ValueError as e:\n        raise SyntaxError('Invalid config file. \\n%s' % e)\n    options = '\\n'.join(parser.global_options)\n\n    queue_type = QUEUE_TYPES[report_type]\n    pipeline = Runtime(command_templates,queue_type,JOB_TYPES,job_type,max_jobs)\n\n    template = env.get_template('output_script.tmpl.sh')\n    with open(temp, 'wb') as f:\n        pickle.dump(pipeline, f, 2)\n        script = template.render(shell=shell,\n            temp=os.path.abspath(temp), options=options)\n\n    if run_now:\n        output = output if output != sys.stdout else PIPELINE_ALIAS\n        submit_job = make_submit_job(shell, output, job_type)\n        submit_job.submit()\n\n    try:\n        f = open(output, 'w')\n        output = f\n    except TypeError:\n        pass\n\n    output.write(script)\n    f.close()", "response": "Create a pipeline based on the provided config file and run it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_submit_job(shell, output, job_type):\n    run_cmd = [shell, output]\n    submit_command = Command(alias=PIPELINE_ALIAS, cmds=run_cmd)\n    submit_job = get_job(submit_command, job_type)\n    submit_job.make()\n    return submit_job", "response": "Creates a job that will be submitted to the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef yaml(modules_to_register: Iterable[Any] = None, classes_to_register: Iterable[Any] = None) -> ruamel.yaml.YAML:\n    # Defein a round-trip yaml object for us to work with. This object should be imported by other modules\n    # NOTE: \"typ\" is a not a typo. It stands for \"type\"\n    yaml = ruamel.yaml.YAML(typ = \"rt\")\n\n    # Register representers and constructors\n    # Numpy\n    yaml.representer.add_representer(np.ndarray, numpy_to_yaml)\n    yaml.constructor.add_constructor(\"!numpy_array\", numpy_from_yaml)\n    # Register external classes\n    yaml = register_module_classes(yaml = yaml, modules = modules_to_register)\n    yaml = register_classes(yaml = yaml, classes = classes_to_register)\n\n    return yaml", "response": "Create a YAML object for loading a list of modules and classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_classes(yaml: ruamel.yaml.YAML, classes: Optional[Iterable[Any]] = None) -> ruamel.yaml.YAML:\n    # Validation\n    if classes is None:\n        classes = []\n\n    # Register the classes\n    for cls in classes:\n        logger.debug(f\"Registering class {cls} with YAML\")\n        yaml.register_class(cls)\n\n    return yaml", "response": "Register externally defined classes with YAML."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering all classes in the given modules with the YAML object.", "response": "def register_module_classes(yaml: ruamel.yaml.YAML, modules: Optional[Iterable[Any]] = None) -> ruamel.yaml.YAML:\n    \"\"\" Register all classes in the given modules with the YAML object.\n\n    This is a simple helper function.\n    \"\"\"\n    # Validation\n    if modules is None:\n        modules = []\n\n    # Extract the classes from the modules\n    classes_to_register = set()\n    for module in modules:\n        module_classes = [member[1] for member in inspect.getmembers(module, inspect.isclass)]\n        classes_to_register.update(module_classes)\n\n    # Register the extracted classes\n    return register_classes(yaml = yaml, classes = classes_to_register)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a numpy array to YAML.", "response": "def numpy_to_yaml(representer: Representer, data: np.ndarray) -> Sequence[Any]:\n    \"\"\" Write a numpy array to YAML.\n\n    It registers the array under the tag ``!numpy_array``.\n\n    Use with:\n\n    .. code-block:: python\n\n        >>> yaml = ruamel.yaml.YAML()\n        >>> yaml.representer.add_representer(np.ndarray, yaml.numpy_to_yaml)\n\n    Note:\n        We cannot use ``yaml.register_class`` because it won't register the proper type.\n        (It would register the type of the class, rather than of `numpy.ndarray`). Instead,\n        we use the above approach to register this method explicitly with the representer.\n    \"\"\"\n    return representer.represent_sequence(\n        \"!numpy_array\",\n        data.tolist()\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef numpy_from_yaml(constructor: Constructor, data: ruamel.yaml.nodes.SequenceNode) -> np.ndarray:\n    # Construct the contained values so that we properly construct int, float, etc.\n    # We just leave this to YAML because it already stores this information.\n    values = [constructor.construct_object(n) for n in data.value]\n    logger.debug(f\"{data}, {values}\")\n    return np.array(values)", "response": "Read an array from YAML to numpy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode the given enumeration value to YAML representation.", "response": "def enum_to_yaml(cls: Type[T_EnumToYAML], representer: Representer, data: T_EnumToYAML) -> ruamel.yaml.nodes.ScalarNode:\n    \"\"\" Encodes YAML representation.\n\n    This is a mixin method for writing enum values to YAML. It needs to be added to the enum\n    as a classmethod. See the module docstring for further information on this approach and how\n    to implement it.\n\n    This method writes whatever is used in the string representation of the YAML value.\n    Usually, this will be the unique name of the enumeration value. If the name is used,\n    the corresponding ``EnumFromYAML`` mixin can be used to recreate the value. If the name\n    isn't used, more care may be necessary, so a ``from_yaml`` method for that particular\n    enumeration may be necessary.\n\n    Note:\n        This method assumes that the name of the enumeration value should be stored as a scalar node.\n\n    Args:\n        representer: Representation from YAML.\n        data: Enumeration value to be encoded.\n    Returns:\n        Scalar representation of the name of the enumeration value.\n    \"\"\"\n    return representer.represent_scalar(\n        f\"!{cls.__name__}\",\n        f\"{str(data)}\"\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enum_from_yaml(cls: Type[T_EnumFromYAML], constructor: Constructor, node: ruamel.yaml.nodes.ScalarNode) -> T_EnumFromYAML:\n    # mypy doesn't like indexing to construct the enumeration.\n    return cls[node.value]", "response": "Decode YAML representation.\n\n    This is a mixin method for reading enum values from YAML. It needs to be added to the enum\n    as a classmethod. See the module docstring for further information on this approach and how\n    to implement it.\n\n    Note:\n        This method assumes that the name of the enumeration value was stored as a scalar node.\n\n    Args:\n        constructor: Constructor from the YAML object.\n        node: Scalar node extracted from the YAML being read.\n    Returns:\n        The constructed YAML value from the name of the enumerated value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck to see if the job errored out.", "response": "def is_error(self):\n        \"\"\" Checks to see if the job errored out. \"\"\"\n        try:\n            if self._task.is_alive():\n                if len(self._task.stderr.readlines()) > 0:\n                    self._task.join()\n                    self._write_log()\n                    return True\n        except AttributeError:\n            pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_splash_ids(splash_mapping_file_pth, conn, db_type='sqlite'):\n    # get dictionary of accession and library_spectra_meta_id\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT id, accession FROM library_spectra_meta\")\n\n    accession_d = {row[1]: row[0] for row in cursor}\n\n    if db_type == 'sqlite':\n        type_sign = '?'\n    else:\n        type_sign = '%s'\n\n    rows = []\n    c = 0\n    # loop through splash mapping file\n    with open(splash_mapping_file_pth, \"r\") as f:\n\n        for line in f:\n            c+=1\n            line = line.rstrip()\n            line_l = line.split(',')\n\n            accession = line_l[0]\n            splash = line_l[1]\n            try:\n                aid = accession_d[accession]\n            except KeyError as e:\n                print(\"can't find accession {}\".format(accession))\n                continue\n\n            row = (splash, aid)\n            rows.append(row)\n\n            if c > 200:\n                print(row)\n                cursor.executemany(\"UPDATE library_spectra_meta SET splash = {t} WHERE id = {t} \".format(t=type_sign), rows)\n                conn.commit()\n                rows = []\n                c = 0\n\n    cursor.executemany(\"UPDATE library_spectra_meta SET splash = {t} WHERE id = {t} \".format(t=type_sign), rows)\n    conn.commit()", "response": "Add the splash ids to the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the current id for each table in the database.", "response": "def _get_current_ids(self, source=True, meta=True, spectra=True, spectra_annotation=True):\n        \"\"\"Get the current id for each table in the database\n\n        Args:\n            source (boolean): get the id for the table \"library_spectra_source\" will update self.current_id_origin\n            meta (boolean): get the id for the table \"library_spectra_meta\" will update self.current_id_meta\n            spectra (boolean): get the id for the table \"library_spectra\" will update self.current_id_spectra\n            spectra_annotation (boolean): get the id for the table \"library_spectra_annotation\" will update\n                                          self.current_id_spectra_annotation\n\n        \"\"\"\n        # get the cursor for the database connection\n        c = self.c\n        # Get the last uid for the spectra_info table\n        if source:\n            c.execute('SELECT max(id) FROM library_spectra_source')\n            last_id_origin = c.fetchone()[0]\n            if last_id_origin:\n                self.current_id_origin = last_id_origin + 1\n            else:\n                self.current_id_origin = 1\n\n        if meta:\n            c.execute('SELECT max(id) FROM library_spectra_meta')\n            last_id_meta = c.fetchone()[0]\n\n            if last_id_meta:\n                self.current_id_meta = last_id_meta + 1\n            else:\n                self.current_id_meta = 1\n\n        if spectra:\n            c.execute('SELECT max(id) FROM library_spectra')\n            last_id_spectra = c.fetchone()[0]\n\n            if last_id_spectra:\n                self.current_id_spectra = last_id_spectra + 1\n            else:\n                self.current_id_spectra = 1\n\n        if spectra_annotation:\n            c.execute('SELECT max(id) FROM library_spectra_annotation')\n            last_id_spectra_annotation = c.fetchone()[0]\n\n            if last_id_spectra_annotation:\n                self.current_id_spectra_annotation = last_id_spectra_annotation + 1\n            else:\n                self.current_id_spectra_annotation = 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_files(self, msp_pth, chunk, db_type, celery_obj=False):\n\n        if os.path.isdir(msp_pth):\n            c = 0\n            for folder, subs, files in sorted(os.walk(msp_pth)):\n                for msp_file in sorted(files):\n                    msp_file_pth = os.path.join(folder, msp_file)\n                    if os.path.isdir(msp_file_pth) or not msp_file_pth.lower().endswith(('txt', 'msp')):\n                        continue\n                    print('MSP FILE PATH', msp_file_pth)\n\n                    self.num_lines = line_count(msp_file_pth)\n                    # each file is processed separately but we want to still process in chunks so we save the number\n                    # of spectra currently being processed with the c variable\n                    with open(msp_file_pth, \"r\") as f:\n                        c = self._parse_lines(f, chunk, db_type, celery_obj, c)\n        else:\n            self.num_lines = line_count(msp_pth)\n            with open(msp_pth, \"r\") as f:\n                self._parse_lines(f, chunk, db_type, celery_obj)\n\n        self.insert_data(remove_data=True, db_type=db_type)", "response": "Parse the files and insert into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_lines(self, f, chunk, db_type, celery_obj=False, c=0):\n        old = 0\n\n        for i, line in enumerate(f):\n\n            line = line.rstrip()\n\n            if i == 0:\n                old = self.current_id_meta\n\n            self._update_libdata(line)\n\n            if self.current_id_meta > old:\n                old = self.current_id_meta\n                c += 1\n\n            if c > chunk:\n\n                if celery_obj:\n                    celery_obj.update_state(state='current spectra {}'.format(str(i)),\n                                            meta={'current': i, 'total': self.num_lines})\n                print(self.current_id_meta)\n                self.insert_data(remove_data=True, db_type=db_type)\n                self.update_source = False\n                c = 0\n        return c", "response": "Parse the MSP files and insert into the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_libdata(self, line):\n        ####################################################\n        # parse MONA Comments line\n        ####################################################\n        # The mona msp files contain a \"comments\" line that contains lots of other information normally separated\n        # into by \"\"\n        if re.match('^Comment.*$', line, re.IGNORECASE):\n            comments = re.findall('\"([^\"]*)\"', line)\n            for c in comments:\n                self._parse_meta_info(c)\n                self._parse_compound_info(c)\n\n        ####################################################\n        # parse meta and compound info lines\n        ####################################################\n        # check the current line for both general meta data\n        # and compound information\n        self._parse_meta_info(line)\n        self._parse_compound_info(line)\n\n        ####################################################\n        # End of meta data\n        ####################################################\n        # Most MSP files have the a standard line of text before the spectra information begins. Here we check\n        # for this line and store the relevant details for the compound and meta information to be ready for insertion\n        # into the database\n        if self.collect_meta and (re.match('^Num Peaks(.*)$', line, re.IGNORECASE) or re.match('^PK\\$PEAK:(.*)', line,\n                re.IGNORECASE) or re.match('^PK\\$ANNOTATION(.*)', line, re.IGNORECASE)):\n\n            self._store_compound_info()\n\n            self._store_meta_info()\n\n            # Reset the temp meta and compound information\n            self.meta_info = get_blank_dict(self.meta_regex)\n            self.compound_info = get_blank_dict(self.compound_regex)\n            self.other_names = []\n            self.collect_meta = False\n\n        # ignore additional information in the 3rd column if using the MassBank spectra schema\n        if re.match('^PK\\$PEAK: m/z int\\. rel\\.int\\.$', line, re.IGNORECASE):\n            self.ignore_additional_spectra_info = True\n\n        # Check if annnotation or spectra is to be in the next lines to be parsed\n        if re.match('^Num Peaks(.*)$', line, re.IGNORECASE) or re.match('^PK\\$PEAK:(.*)', line, re.IGNORECASE):\n            self.start_spectra = True\n            return\n        elif re.match('^PK\\$ANNOTATION(.*)', line, re.IGNORECASE):\n            self.start_spectra_annotation = True\n\n            match = re.match('^PK\\$ANNOTATION:(.*)', line, re.IGNORECASE)\n            columns = match.group(1)\n            cl = columns.split()\n\n            self.spectra_annotation_indexes = {i: cl.index(i) for i in cl}\n            return\n\n        ####################################################\n        # Process annotation details\n        ####################################################\n        # e.g. molecular formula for each peak in the spectra\n        if self.start_spectra_annotation:\n            self._parse_spectra_annotation(line)\n\n        ####################################################\n        # Process spectra\n        ####################################################\n        if self.start_spectra:\n            self._parse_spectra(line)", "response": "Update the library meta data from the current line being parsed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_compound_ids(self):\n        cursor = self.conn.cursor()\n        cursor.execute('SELECT inchikey_id FROM metab_compound')\n        self.conn.commit()\n        for row in cursor:\n            if not row[0] in self.compound_ids:\n                self.compound_ids.append(row[0])", "response": "Extract the current compound ids in the database. Updates the self. compound_ids list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _store_compound_info(self):\n        other_name_l = [name for name in self.other_names if name != self.compound_info['name']]\n        self.compound_info['other_names'] = ' <#> '.join(other_name_l)\n\n        if not self.compound_info['inchikey_id']:\n            self._set_inchi_pcc(self.compound_info['pubchem_id'], 'cid', 0)\n\n        if not self.compound_info['inchikey_id']:\n            self._set_inchi_pcc(self.compound_info['smiles'], 'smiles', 0)\n\n        if not self.compound_info['inchikey_id']:\n            self._set_inchi_pcc(self.compound_info['name'], 'name', 0)\n\n        if not self.compound_info['inchikey_id']:\n            print('WARNING, cant get inchi key for ', self.compound_info)\n            print(self.meta_info)\n            print('#########################')\n            self.compound_info['inchikey_id'] = 'UNKNOWN_' + str(uuid.uuid4())\n\n        if not self.compound_info['pubchem_id'] and self.compound_info['inchikey_id']:\n            self._set_inchi_pcc(self.compound_info['inchikey_id'], 'inchikey', 0)\n\n        if not self.compound_info['name']:\n            self.compound_info['name'] = 'unknown name'\n\n        if not self.compound_info['inchikey_id'] in self.compound_ids:\n            self.compound_info_all.append(tuple(self.compound_info.values()) + (\n                str(datetime.datetime.now()),\n                str(datetime.datetime.now()),\n            ))\n            self.compound_ids.append(self.compound_info['inchikey_id'])", "response": "Update the dictionary with the current chunk of compound details."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the meta dictionary with the current chunk of meta data details", "response": "def _store_meta_info(self):\n        \"\"\"Update the meta dictionary with the current chunk of meta data details\n        \"\"\"\n        # In the mass bank msp files, sometimes the precursor_mz is missing but we have the neutral mass and\n        # the precursor_type (e.g. adduct) so we can calculate the precursor_mz\n        if not self.meta_info['precursor_mz'] and self.meta_info['precursor_type'] and \\\n                self.compound_info['exact_mass']:\n            self.meta_info['precursor_mz'] = get_precursor_mz(float(self.compound_info['exact_mass']),\n                                                              self.meta_info['precursor_type'])\n\n        if not self.meta_info['polarity']:\n            # have to do special check for polarity (as sometimes gets missed)\n            m = re.search('^\\[.*\\](\\-|\\+)', self.meta_info['precursor_type'], re.IGNORECASE)\n            if m:\n                polarity = m.group(1).strip()\n                if polarity == '+':\n                    self.meta_info['polarity'] = 'positive'\n                elif polarity == '-':\n                    self.meta_info['polarity'] = 'negative'\n\n        if not self.meta_info['accession']:\n            self.meta_info['accession'] = 'unknown accession'\n\n        self.meta_info_all.append(\n            (str(self.current_id_meta),) +\n            tuple(self.meta_info.values()) +\n            (str(self.current_id_origin), self.compound_info['inchikey_id'],)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_spectra_annotation(self, line):\n        if re.match('^PK\\$NUM_PEAK(.*)', line, re.IGNORECASE):\n            self.start_spectra_annotation = False\n            return\n\n        saplist = line.split()\n\n        sarow = (\n            self.current_id_spectra_annotation,\n            float(saplist[self.spectra_annotation_indexes['m/z']]) if 'm/z' in self.spectra_annotation_indexes else None,\n            saplist[self.spectra_annotation_indexes[\n                'tentative_formula']] if 'tentative_formula' in self.spectra_annotation_indexes else None,\n            float(saplist[self.spectra_annotation_indexes[\n                'mass_error(ppm)']]) if 'mass_error(ppm)' in self.spectra_annotation_indexes else None,\n            self.current_id_meta)\n\n        self.spectra_annotation_all.append(sarow)\n\n        self.current_id_spectra_annotation += 1", "response": "Parse and store the spectral annotation details"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_spectra(self, line):\n        if line in ['\\n', '\\r\\n', '//\\n', '//\\r\\n', '', '//']:\n            self.start_spectra = False\n            self.current_id_meta += 1\n            self.collect_meta = True\n            return\n\n        splist = line.split()\n\n        if len(splist) > 2 and not self.ignore_additional_spectra_info:\n            additional_info = ''.join(map(str, splist[2:len(splist)]))\n        else:\n            additional_info = ''\n\n        srow = (\n            self.current_id_spectra, float(splist[0]), float(splist[1]), additional_info,\n            self.current_id_meta)\n\n        self.spectra_all.append(srow)\n\n        self.current_id_spectra += 1", "response": "Parse and store the spectral details\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_inchi_pcc(self, in_str, pcp_type, elem):\n        if not in_str:\n            return 0\n\n        try:\n            pccs = pcp.get_compounds(in_str, pcp_type)\n        except pcp.BadRequestError as e:\n            print(e)\n            return 0\n        except pcp.TimeoutError as e:\n            print(e)\n            return 0\n        except pcp.ServerError as e:\n            print(e)\n            return 0\n        except URLError as e:\n            print(e)\n            return 0\n        except BadStatusLine as e:\n            print(e)\n            return 0\n\n        if pccs:\n            pcc = pccs[elem]\n            self.compound_info['inchikey_id'] = pcc.inchikey\n            self.compound_info['pubchem_id'] = pcc.cid\n            self.compound_info['molecular_formula'] = pcc.molecular_formula\n            self.compound_info['molecular_weight'] = pcc.molecular_weight\n            self.compound_info['exact_mass'] = pcc.exact_mass\n            self.compound_info['smiles'] = pcc.canonical_smiles\n\n            if len(pccs) > 1:\n                print('WARNING, multiple compounds for ', self.compound_info)", "response": "Set the inchikey and pubchem compounds for the given inchikey and pubchem compounds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse and extract any other names that might be recorded for the compound SETTAGENAME", "response": "def _get_other_names(self, line):\n        \"\"\"Parse and extract any other names that might be recorded for the compound\n\n        Args:\n             line (str): line of the msp file\n        \"\"\"\n        m = re.search(self.compound_regex['other_names'][0], line, re.IGNORECASE)\n        if m:\n            self.other_names.append(m.group(1).strip())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing and extract all meta data from the line of the msp file.", "response": "def _parse_meta_info(self, line):\n        \"\"\"Parse and extract all meta data by looping through the dictionary of meta_info regexs\n\n        updates self.meta_info\n\n        Args:\n             line (str): line of the msp file\n        \"\"\"\n        if self.mslevel:\n            self.meta_info['ms_level'] = self.mslevel\n\n        if self.polarity:\n            self.meta_info['polarity'] = self.polarity\n\n        for k, regexes in six.iteritems(self.meta_regex):\n            for reg in regexes:\n\n                m = re.search(reg, line, re.IGNORECASE)\n\n                if m:\n                    self.meta_info[k] = m.group(1).strip()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and extract all compound data from the line of the msp file", "response": "def _parse_compound_info(self, line):\n        \"\"\"Parse and extract all compound data by looping through the dictionary of compound_info regexs\n\n        updates self.compound_info\n\n        Args:\n             line (str): line of the msp file\n\n        \"\"\"\n        for k, regexes in six.iteritems(self.compound_regex):\n            for reg in regexes:\n                if self.compound_info[k]:\n                    continue\n                m = re.search(reg, line, re.IGNORECASE)\n                if m:\n                    self.compound_info[k] = m.group(1).strip()\n\n        self._get_other_names(line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_data(self, remove_data=False, db_type='sqlite'):\n        if self.update_source:\n            # print \"insert ref id\"\n            import msp2db\n            self.c.execute(\n                \"INSERT INTO library_spectra_source (id, name, parsing_software) VALUES\"\n                \" ({a}, '{b}', 'msp2db-v{c}')\".format(a=self.current_id_origin, b=self.source, c=msp2db.__version__))\n            self.conn.commit()\n\n        if self.compound_info_all:\n            self.compound_info_all = _make_sql_compatible(self.compound_info_all)\n\n            cn = ', '.join(self.compound_info.keys()) + ',created_at,updated_at'\n\n            insert_query_m(self.compound_info_all, columns=cn, conn=self.conn, table='metab_compound',\n                           db_type=db_type)\n\n        self.meta_info_all = _make_sql_compatible(self.meta_info_all)\n\n        cn = 'id,' + ', '.join(self.meta_info.keys()) + ',library_spectra_source_id, inchikey_id'\n\n        insert_query_m(self.meta_info_all, columns=cn, conn=self.conn, table='library_spectra_meta',\n                       db_type=db_type)\n\n\n        cn = \"id, mz, i, other, library_spectra_meta_id\"\n        insert_query_m(self.spectra_all, columns=cn, conn=self.conn, table='library_spectra', db_type=db_type)\n        if self.spectra_annotation_all:\n            cn = \"id, mz, tentative_formula, mass_error, library_spectra_meta_id\"\n            insert_query_m(self.spectra_annotation_all, columns=cn, conn=self.conn,\n                           table='library_spectra_annotation', db_type=db_type)\n\n\n        # self.conn.close()\n        if remove_data:\n            self.meta_info_all = []\n            self.spectra_all = []\n            self.spectra_annotation_all = []\n            self.compound_info_all = []\n            self._get_current_ids(source=False)", "response": "Insert data stored in the current chunk of parsing into the selected database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef line(line_def, **kwargs):\n    def replace(s):\n        return \"(%s)\" % ansi.aformat(s.group()[1:], attrs=[\"bold\", ])\n    return ansi.aformat(\n        re.sub('@.?', replace, line_def),\n        **kwargs)", "response": "Highlights a character in the line"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply multiple validation functions to test otope.", "response": "def try_and_error(*funcs):\n    \"\"\"Apply multiple validation functions\n\n    Parameters\n    ----------\n    ``*funcs``\n        Validation functions to test\n\n    Returns\n    -------\n    function\"\"\"\n    def validate(value):\n        exc = None\n        for func in funcs:\n            try:\n                return func(value)\n            except (ValueError, TypeError) as e:\n                exc = e\n        raise exc\n    return validate"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate a text formatoption", "response": "def validate_text(value):\n    \"\"\"Validate a text formatoption\n\n    Parameters\n    ----------\n    value: see :attr:`psyplot.plotter.labelplotter.text`\n\n    Raises\n    ------\n    ValueError\"\"\"\n    possible_transform = ['axes', 'fig', 'data']\n    validate_transform = ValidateInStrings('transform', possible_transform,\n                                           True)\n    tests = [validate_float, validate_float, validate_str,\n             validate_transform, dict]\n    if isinstance(value, six.string_types):\n        xpos, ypos = rcParams['texts.default_position']\n        return [(xpos, ypos, value, 'axes', {'ha': 'right'})]\n    elif isinstance(value, tuple):\n        value = [value]\n    try:\n        value = list(value)[:]\n    except TypeError:\n        raise ValueError(\"Value must be string or list of tuples!\")\n    for i, val in enumerate(value):\n        try:\n            val = tuple(val)\n        except TypeError:\n            raise ValueError(\n                \"Text must be an iterable of the form \"\n                \"(x, y, s[, trans, params])!\")\n        if len(val) < 3:\n            raise ValueError(\n                \"Text tuple must at least be like [x, y, s], with floats x, \"\n                \"y and string s!\")\n        elif len(val) == 3 or isinstance(val[3], dict):\n            val = list(val)\n            val.insert(3, 'data')\n            if len(val) == 4:\n                val += [{}]\n            val = tuple(val)\n        if len(val) > 5:\n            raise ValueError(\n                \"Text tuple must not be longer then length 5. It can be \"\n                \"like (x, y, s[, trans, params])!\")\n        value[i] = (validate(x) for validate, x in zip(tests, val))\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that None is given", "response": "def validate_none(b):\n    \"\"\"Validate that None is given\n\n    Parameters\n    ----------\n    b: {None, 'none'}\n        None or string (the case is ignored)\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\"\"\"\n    if isinstance(b, six.string_types):\n        b = b.lower()\n    if b is None or b == 'none':\n        return None\n    else:\n        raise ValueError('Could not convert \"%s\" to None' % b)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_axiscolor(value):\n    validate = try_and_error(validate_none, validate_color)\n    possible_keys = {'right', 'left', 'top', 'bottom'}\n    try:\n        value = dict(value)\n        false_keys = set(value) - possible_keys\n        if false_keys:\n            raise ValueError(\"Wrong keys (%s)!\" % (', '.join(false_keys)))\n        for key, val in value.items():\n            value[key] = validate(val)\n    except:\n        value = dict(zip(possible_keys, repeat(validate(value))))\n    return value", "response": "Validate a dictionary containing axiscolor definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates a colorbar position", "response": "def validate_cbarpos(value):\n    \"\"\"Validate a colorbar position\n\n    Parameters\n    ----------\n    value: bool or str\n        A string can be a combination of 'sh|sv|fl|fr|ft|fb|b|r'\n\n    Returns\n    -------\n    list\n        list of strings with possible colorbar positions\n\n    Raises\n    ------\n    ValueError\"\"\"\n    patt = 'sh|sv|fl|fr|ft|fb|b|r'\n    if value is True:\n        value = {'b'}\n    elif not value:\n        value = set()\n    elif isinstance(value, six.string_types):\n        for s in re.finditer('[^%s]+' % patt, value):\n            warn(\"Unknown colorbar position %s!\" % s.group(), RuntimeWarning)\n        value = set(re.findall(patt, value))\n    else:\n        value = validate_stringset(value)\n        for s in (s for s in value\n                  if not re.match(patt, s)):\n            warn(\"Unknown colorbar position %s!\" % s)\n            value.remove(s)\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_cmap(val):\n    from matplotlib.colors import Colormap\n    try:\n        return validate_str(val)\n    except ValueError:\n        if not isinstance(val, Colormap):\n            raise ValueError(\n                \"Could not find a valid colormap!\")\n        return val", "response": "Validate a colormap\n\n    Parameters\n    ----------\n    val: str or :class:`mpl.colors.Colormap`\n\n    Returns\n    -------\n    str or :class:`mpl.colors.Colormap`\n\n    Raises\n    ------\n    ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_cmaps(cmaps):\n    cmaps = {validate_str(key): validate_colorlist(val) for key, val in cmaps}\n    for key, val in six.iteritems(cmaps):\n        cmaps.setdefault(key + '_r', val[::-1])\n    return cmaps", "response": "Validate a dictionary of color lists\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_lineplot(value):\n    if value is None:\n        return value\n    elif isinstance(value, six.string_types):\n        return six.text_type(value)\n    else:\n        value = list(value)\n        for i, v in enumerate(value):\n            if v is None:\n                pass\n            elif isinstance(v, six.string_types):\n                value[i] = six.text_type(v)\n            else:\n                raise ValueError('Expected None or string, found %s' % (v, ))\n    return value", "response": "Validate the value for the LinePlotter. plot formatoption\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_err_calc(val):\n    try:\n        val = validate_float(val)\n    except (ValueError, TypeError):\n        pass\n    else:\n        if val <= 100 and val >= 0:\n            return val\n        raise ValueError(\"Percentiles for the error calculation must lie \"\n                         \"between 0 and 100, not %s\" % val)\n    try:\n        val = ValidateList(float, 2)(val)\n    except (ValueError, TypeError):\n        pass\n    else:\n        if all((v <= 100 and v >= 0) for v in val):\n            return val\n        raise ValueError(\"Percentiles for the error calculation must lie \"\n                         \"between 0 and 100, not %s\" % val)\n    try:\n        val = validate_str(val)\n    except ValueError:\n        pass\n    else:\n        if 'std' not in val:\n            raise ValueError(\n                'A string for the error calculation must contain std!')\n    return val", "response": "Validation function for the the\n    formatoption"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visit_GpxModel(self, gpx_model, *args, **kwargs):\n        result = OrderedDict()\n\n        put_scalar = lambda name, json_name=None: self.optional_attribute_scalar(result, gpx_model, name, json_name)\n        put_list = lambda name, json_name=None: self.optional_attribute_list(result, gpx_model, name, json_name)\n\n        put_scalar('creator')\n        put_scalar('metadata')\n        put_list('waypoints')\n        put_list('routes')\n        put_list('tracks')\n        put_list('extensions')\n\n        return result", "response": "Render a GPXModel as a single JSON structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_Metadata(self, metadata, *args, **kwargs):\n        result = OrderedDict()\n        put_scalar = lambda name, json_name=None: self.optional_attribute_scalar(result, metadata, name, json_name)\n        put_list = lambda name, json_name=None: self.optional_attribute_list(result, metadata, name, json_name)\n\n        put_scalar('name')\n        put_scalar('description')\n        put_scalar('author')\n        put_scalar('copyright')\n        put_list('links')\n        put_scalar('time')\n        put_scalar('keywords')\n        put_scalar('bounds')\n        put_list('extensions')\n\n        return result", "response": "Render GPX Metadata as a single JSON structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef swap_default(mode, equation, symbol_names, default, **kwargs):\n    '''\n    Given a `sympy` equation or equality, along with a list of symbol names,\n    substitute the specified default value for each symbol for which a value is\n    not provided through a keyword argument.\n\n    For example, consider the following equality:\n\n    >>> sp.pprint(H)\n    V\u2082   Z\u2082\n    \u2500\u2500 = \u2500\u2500\n    V\u2081   Z\u2081\n\n    Let us substitute a default value of 1 for terms Z1 and Z2:\n\n    >>> sp.pprint(subs_default(H, ['Z1', 'Z2'], 1))\n    V\u2082\n    \u2500\u2500 = 1\n    V\u2081\n\n    Now, let us specify a default value of 1 for terms Z1 and Z2, but provide\n    an overriding value for Z1:\n\n    >>> sp.pprint(subs_default(H, ['Z1', 'Z2'], 1, Z1=4))\n    V\u2082\n    \u2500\u2500 = 1/4\n    V\u2081\n\n    Note that keyword arguments for terms not specified in the list of symbol\n    names are ignored:\n\n    >>> sp.pprint(subs_default(H, ['Z1', 'Z2'], 1, Z1=4, Q=7))\n    V\u2082\n    \u2500\u2500 = 1/4\n    V\u2081\n    '''\n    if mode == 'subs':\n        swap_f = _subs\n        default_swap_f = _subs\n    elif mode == 'limit':\n        swap_f = _limit\n        default_swap_f = _subs\n    elif mode == 'limit_default':\n        swap_f = _subs\n        default_swap_f = _limit\n    else:\n        raise ValueError('''Unsupported mode.  `mode` must be one of: '''\n                         '''('subs', 'limit').''')\n\n    result = equation\n    for s in symbol_names:\n        if s in kwargs:\n            if isinstance(kwargs[s], Iterable):\n                continue\n            else:\n                result = swap_f(result, s, kwargs[s])\n        else:\n            result = default_swap_f(result, s, default)\n    return result", "response": "Given a sympy equation or equality and a list of symbol names substitute the specified default value for each symbol in the list of symbol names and return the new equation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef z_transfer_functions():\n    r'''\n    Return a symbolic equality representation of the transfer function of RMS\n    voltage measured by either control board analog feedback circuits.\n\n    According to the figure below, the transfer function describes the\n    following relationship::\n\n          # Hardware V1 #                        # Hardware V2 #\n\n            V\u2082      V\u2081                               V\u2082   Z\u2081\n            \u2500\u2500 = \u2500\u2500\u2500\u2500\u2500\u2500\u2500                             \u2500\u2500 = \u2500\u2500\n            Z\u2082   Z\u2081 + Z\u2082                             V\u2081   Z\u2082\n\n    where $V_{1}$ denotes the high-voltage actuation signal from the amplifier\n    output and $V_{2}$ denotes the signal sufficiently attenuated to fall\n    within the measurable input range of the analog-to-digital converter\n    *(approx. 5V)*.  The feedback circuits for control board **hardware version\n    1** and **hardware version 2** are shown below.\n\n    .. code-block:: none\n\n          # Hardware V1 #                        # Hardware V2 #\n\n          V_1 @ frequency                        V_1 @ frequency\n              \u252f                                      \u252f\n            \u250c\u2500\u2534\u2500\u2510                                  \u250c\u2500\u2534\u2500\u2510    \u250c\u2500\u2500\u2500\u2510\n            \u2502Z_1\u2502                                  \u2502Z_1\u2502  \u250c\u2500\u2524Z_2\u251c\u2500\u2510\n            \u2514\u2500\u252c\u2500\u2518                                  \u2514\u2500\u252c\u2500\u2518  \u2502 \u2514\u2500\u2500\u2500\u2518 \u2502\n              \u251c\u2500\u2500\u2500\u22b8 V_2                              \u2502    \u2502  \u2502\u2572   \u251c\u2500\u2500\u2500\u22b8 V_2\n            \u250c\u2500\u2534\u2500\u2510                                    \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2502-\u2572__\u2502\n            \u2502Z_2\u2502                                         \u250c\u2500\u2500\u2502+\u2571\n            \u2514\u2500\u252c\u2500\u2518                                         \u2502  \u2502\u2571\n             \u2550\u2567\u2550                                          \u2502\n              \u00af                                          \u2550\u2567\u2550\n                                                          \u00af\n\n    Notes\n    -----\n\n     - The symbolic equality can be solved for any symbol, _e.g.,_ $V_{1}$ or\n       $V_{2}$.\n     - A symbolically solved representation can be converted to a Python function\n       using `sympy.utilities.lambdify.lambdify`_, to compute results for\n       specific values of the remaining parameters.\n\n    .. _`sympy.utilities.lambdify.lambdify`: http://docs.sympy.org/dev/modules/utilities/lambdify.html\n    '''\n    # Define transfer function as a symbolic equality using SymPy.\n    V1, V2, Z1, Z2 = sp.symbols('V1 V2 Z1 Z2')\n    xfer_funcs = pd.Series([sp.Eq(V2 / Z2, V1 / (Z1 + Z2)),\n                            sp.Eq(V2 / V1, Z2 / Z1)],\n                           # Index by hardware version.\n                           index=[1, 2])\n    xfer_funcs.index.name = 'Hardware version'\n    return xfer_funcs", "response": "r Returns a symbolic equality representation of the transfer functions of the RMS\n    voltage measured by either control board analog feedback circuits or the control board analog feedback circuits."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap around ConfigParser s has_option method.", "response": "def has_option(section, name):\n    \"\"\"\n    Wrapper around ConfigParser's ``has_option`` method.\n    \"\"\"\n    cfg = ConfigParser.SafeConfigParser({\"working_dir\": \"/tmp\", \"debug\": \"0\"})\n    cfg.read(CONFIG_LOCATIONS)\n    return cfg.has_option(section, name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(section, name):\n    cfg = ConfigParser.SafeConfigParser({\"working_dir\": \"/tmp\", \"debug\": \"0\"})\n    cfg.read(CONFIG_LOCATIONS)\n    val = cfg.get(section, name)\n    return val.strip(\"'\").strip('\"')", "response": "Wrapper around ConfigParser s get method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(**options):\n    with Dotfile(options) as conf:\n        if conf['context'] is None:\n            msg = \"No context file has been provided\"\n            LOGGER.error(msg)\n            raise RuntimeError(msg)\n        if not os.path.exists(conf['context_path']):\n            msg = \"Context file {} not found\".format(conf['context_path'])\n            LOGGER.error(msg)\n            raise RuntimeError(msg)\n        LOGGER.info(\n            (\n                \"{{dockerstache}}: In: {}\\n\"\n                \"{{dockerstache}}: Out: {}\\n\"\n                \"{{dockerstache}}: Context: {}\\n\"\n                \"{{dockerstache}}: Defaults: {}\\n\"\n            ).format(conf['input'], conf['output'], conf['context'], conf['defaults'])\n        )\n        context = Context(conf['context'], conf['defaults'])\n        context.load()\n        if 'extend_context' in options:\n            LOGGER.info(\"{{dockerstache}} Extended context provided\")\n            context.update(options['extend_context'])\n\n        process_templates(\n            conf['input'],\n            conf['output'],\n            context\n            )\n        if conf['inclusive']:\n            process_copies(\n                conf['input'],\n                conf['output'],\n                conf['exclude']\n            )\n    return dict(conf)", "response": "This function runs the dockerstache process to render the templates and returns the dictionary that contains the configuration of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an object key for storage.", "response": "def make_key(table_name, objid):\n    \"\"\"Create an object key for storage.\"\"\"\n    key = datastore.Key()\n    path = key.path_element.add()\n    path.kind = table_name\n    path.name = str(objid)\n    return key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_rec(table_name, objid, data, index_name_values):\n    with DatastoreTransaction() as tx:\n        entity = tx.get_upsert()\n\n        entity.key.CopyFrom(make_key(table_name, objid))\n\n        prop = entity.property.add()\n        prop.name = 'id'\n        prop.value.string_value = objid\n\n        prop = entity.property.add()\n        prop.name = 'value'\n        prop.value.string_value = data\n\n        for name, val in index_name_values:\n            prop = entity.property.add()\n            prop.name = name\n            prop.value.string_value = str(val)", "response": "Write a record using a tran."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_entity(found):\n    obj = dict()\n    for prop in found.entity.property:\n        obj[prop.name] = prop.value.string_value\n    return obj", "response": "Copy found entity to a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_rec(table_name, objid):\n    req = datastore.LookupRequest()\n    req.key.extend([make_key(table_name, objid)])\n\n    for found in datastore.lookup(req).found:\n        yield extract_entity(found)", "response": "Generator that yields keyed recs from store."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the entries from the specified table by the given indexes.", "response": "def read_by_indexes(table_name, index_name_values=None):\n    \"\"\"Index reader.\"\"\"\n    req = datastore.RunQueryRequest()\n\n    query = req.query\n    query.kind.add().name = table_name\n\n    if not index_name_values:\n        index_name_values = []\n    for name, val in index_name_values:\n        queryFilter = query.filter.property_filter\n        queryFilter.property.name = name\n        queryFilter.operator = datastore.PropertyFilter.EQUAL\n        queryFilter.value.string_value = str(val)\n\n    loop_its = 0\n    have_more = True\n\n    while have_more:\n        resp = datastore.run_query(req)\n\n        found_something = False\n        for found in resp.batch.entity_result:\n            yield extract_entity(found)\n            found_something = True\n\n        if not found_something:\n            # This is a guard against bugs or excessive looping - as long we\n            # can keep yielding records we'll continue to execute\n            loop_its += 1\n            if loop_its > 5:\n                raise ValueError(\"Exceeded the excessive query threshold\")\n\n        if resp.batch.more_results != datastore.QueryResultBatch.NOT_FINISHED:\n            have_more = False\n        else:\n            have_more = True\n            end_cursor = resp.batch.end_cursor\n            query.start_cursor.CopyFrom(end_cursor)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the object with the given id.", "response": "def find_one(self, cls, id):\n        \"\"\"Required functionality.\"\"\"\n        db_result = None\n        for rec in read_rec(cls.get_table_name(), id):\n            db_result = rec\n            break  # Only read the first returned - which should be all we get\n        if not db_result:\n            return None\n\n        obj = cls.from_data(db_result['value'])\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbetter smarter call logic", "response": "def call(command, stdin=None, stdout=subprocess.PIPE, env=os.environ, cwd=None,\n         shell=False, output_log_level=logging.INFO, sensitive_info=False):\n    \"\"\" Better, smarter call logic \"\"\"\n    if not sensitive_info:\n        logger.debug(\"calling command: %s\" % command)\n    else:\n        logger.debug(\"calling command with sensitive information\")\n    try:\n        args = command if shell else whitespace_smart_split(command)\n        kw = {}\n        if not shell and not which(args[0], cwd=cwd):\n            raise CommandMissingException(args[0])\n        if shell:\n            kw['shell'] = True\n        process = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=stdout,\n                                   stderr=subprocess.STDOUT, env=env, cwd=cwd,\n                                   **kw)\n        output = process.communicate(input=stdin)[0]\n        if output is not None:\n            try:\n                logger.log(output_log_level, output.decode('utf-8'))\n            except UnicodeDecodeError:\n                pass\n        return (process.returncode, output)\n    except OSError:\n        e = sys.exc_info()[1]\n        if not sensitive_info:\n            logger.exception(\"Error running command: %s\" % command)\n            logger.error(\"Root directory: %s\" % cwd)\n            if stdin:\n                logger.error(\"stdin: %s\" % stdin)\n        raise e"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef whitespace_smart_split(command):\n    return_array = []\n    s = \"\"\n    in_double_quotes = False\n    escape = False\n    for c in command:\n        if c == '\"':\n            if in_double_quotes:\n                if escape:\n                    s += c\n                    escape = False\n                else:\n                    s += c\n                    in_double_quotes = False\n            else:\n                in_double_quotes = True\n                s += c\n        else:\n            if in_double_quotes:\n                if c == '\\\\':\n                    escape = True\n                    s += c\n                else:\n                    escape = False\n                    s += c\n            else:\n                if c == ' ':\n                    return_array.append(s)\n                    s = \"\"\n                else:\n                    s += c\n    if s != \"\":\n        return_array.append(s)\n    return return_array", "response": "Split a command by whitespace taking care to not split on\n    whitespace within quotes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nskips the current context.", "response": "def skip(stackframe=1):\n  \"\"\"\n  Must be called from within `__enter__()`. Performs some magic to have a\n  #ContextSkipped exception be raised the moment the with context is entered.\n  The #ContextSkipped must then be handled in `__exit__()` to suppress the\n  propagation of the exception.\n\n  > Important: This function does not raise an exception by itself, thus\n  > the `__enter__()` method will continue to execute after using this function.\n  \"\"\"\n\n  def trace(frame, event, args):\n    raise ContextSkipped\n\n  sys.settrace(lambda *args, **kwargs: None)\n  frame = sys._getframe(stackframe + 1)\n  frame.f_trace = trace"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute the steps required to have the feature end with the desired state.", "response": "def sync(self):\n        \"\"\"\n        execute the steps required to have the\n        feature end with the desired state.\n        \"\"\"\n        phase = _get_phase(self._formula_instance)\n        self.logger.info(\"%s %s...\" % (phase.verb.capitalize(), self.feature_name))\n        message = \"...finished %s %s.\" % (phase.verb, self.feature_name)\n        result = getattr(self, phase.name)()\n        if result or phase in (PHASE.INSTALL, PHASE.REMOVE):\n            self.logger.info(message)\n        else:\n            self.logger.debug(message)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a given hook module has been loaded and if so returns True if it has been loaded False otherwise.", "response": "def isloaded(self, name):\n        \"\"\"Checks if given hook module has been loaded\n\n        Args:\n            name (str): The name of the module to check\n\n        Returns:\n            bool.  The return code::\n\n                True -- Loaded\n                False -- Not Loaded\n        \"\"\"\n        if name is None:\n            return True\n\n        if isinstance(name, str):\n            return (name in [x.__module__ for x in self])\n\n        if isinstance(name, Iterable):\n            return set(name).issubset([x.__module__ for x in self])\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to load a hook for one of the modules that are loaded before all the modules that are loaded before all the modules that are loaded before all the modules are loaded.", "response": "def hook(self, function, dependencies=None):\n        \"\"\"Tries to load a hook\n\n        Args:\n            function (func): Function that will be called when the event is called\n\n        Kwargs:\n            dependencies (str): String or Iterable with modules whose hooks should be called before this one\n\n        Raises:\n            :class:TypeError\n\n        Note that the dependencies are module-wide, that means that if\n        `parent.foo` and `parent.bar` are both subscribed to `example` event\n        and `child` enumerates `parent` as dependcy, **both** `foo` and `bar`\n        must be called in order for the dependcy to get resolved.\n        \"\"\"\n        if not isinstance(dependencies, (Iterable, type(None), str)):\n            raise TypeError(\"Invalid list of dependencies provided!\")\n\n        # Tag the function with its dependencies\n        if not hasattr(function, \"__deps__\"):\n            function.__deps__ = dependencies\n\n        # If a module is loaded before all its dependencies are loaded, put\n        # it in _later list and don't load yet\n        if self.isloaded(function.__deps__):\n            self.append(function)\n        else:\n            self._later.append(function)\n\n        # After each module load, retry to resolve dependencies\n        for ext in self._later:\n            if self.isloaded(ext.__deps__):\n                self._later.remove(ext)\n                self.hook(ext)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_from_json(json_str):\n    try:\n        message_dict = json.loads(json_str)\n    except ValueError:\n        raise ParseError(\"Mal-formed JSON input.\")\n\n    upload_keys = message_dict.get('uploadKeys', False)\n    if upload_keys is False:\n        raise ParseError(\n            \"uploadKeys does not exist. At minimum, an empty array is required.\"\n        )\n    elif not isinstance(upload_keys, list):\n        raise ParseError(\n            \"uploadKeys must be an array object.\"\n        )\n\n    upload_type = message_dict['resultType']\n\n    try:\n        if upload_type == 'orders':\n            return orders.parse_from_dict(message_dict)\n        elif upload_type == 'history':\n            return history.parse_from_dict(message_dict)\n        else:\n            raise ParseError(\n                'Unified message has unknown upload_type: %s' % upload_type)\n    except TypeError as exc:\n        # MarketOrder and HistoryEntry both raise TypeError exceptions if\n        # invalid input is encountered.\n        raise ParseError(exc.message)", "response": "Given a Unified Uploader message parse the contents and return a MarketOrderList or MarketHistoryList instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode_to_json(order_or_history):\n    if isinstance(order_or_history, MarketOrderList):\n        return orders.encode_to_json(order_or_history)\n    elif isinstance(order_or_history, MarketHistoryList):\n        return history.encode_to_json(order_or_history)\n    else:\n        raise Exception(\"Must be one of MarketOrderList or MarketHistoryList.\")", "response": "Encodes an order or history entry to JSON and returns the encoded string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a method, which gets called when this event triggers. :param event: the event to register the decorator method on.", "response": "def event_subscriber(event):\n    \"\"\"\n    Register a method, which gets called when this event triggers.\n    :param event: the event to register the decorator method on.\n    \"\"\"\n    def wrapper(method):\n        Registry.register_event(event.name, event, method)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndispatching an event when the decorated method is called. :param event: the event class to instantiate and dispatch. :param subject_property: the property name to get the subject.", "response": "def dispatch_event(event, subject='id'):\n    \"\"\"\n    Dispatch an event when the decorated method is called.\n    :param event: the event class to instantiate and dispatch.\n    :param subject_property: the property name to get the subject.\n    \"\"\"\n    def wrapper(method):\n        def inner_wrapper(*args, **kwargs):\n            resource = method(*args, **kwargs)\n            if isinstance(resource, dict):\n                subject_ = resource.get(subject)\n                data = resource\n            else:\n                subject_ = getattr(resource, subject)\n                data = resource.__dict__\n            event(subject_, data).dispatch()\n            return resource\n        return inner_wrapper\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a strong classifier with the given threshold to the cascade.", "response": "def add(self, classifier, threshold, begin=None, end=None):\n    \"\"\"Adds a new strong classifier with the given threshold to the cascade.\n\n    **Parameters:**\n\n    classifier : :py:class:`bob.learn.boosting.BoostedMachine`\n      A strong classifier to add\n\n    ``threshold`` : float\n      The classification threshold for this cascade step\n\n    ``begin``, ``end`` : int or ``None``\n      If specified, only the weak machines with the indices ``range(begin,end)`` will be added.\n    \"\"\"\n    boosted_machine = bob.learn.boosting.BoostedMachine()\n    if begin is None: begin = 0\n    if end is None: end = len(classifier.weak_machines)\n    for i in range(begin, end):\n      boosted_machine.add_weak_machine(classifier.weak_machines[i], classifier.weights[i])\n    self.cascade.append(boosted_machine)\n    self.thresholds.append(threshold)\n    self._indices()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_from_boosted_machine(self, boosted_machine, classifiers_per_round, classification_thresholds=-5.):\n    indices = list(range(0, len(boosted_machine.weak_machines), classifiers_per_round))\n    if indices[-1] != len(boosted_machine.weak_machines): indices.append(len(boosted_machine.weak_machines))\n    self.cascade = []\n    self.indices = []\n    for i in range(len(indices)-1):\n      machine = bob.learn.boosting.BoostedMachine()\n      for index in range(indices[i], indices[i+1]):\n        machine.add_weak_machine(boosted_machine.weak_machines[index], boosted_machine.weights[index, 0])\n      self.cascade.append(machine)\n    if isinstance(classification_thresholds, (int, float)):\n      self.thresholds = [classification_thresholds] * len(self.cascade)\n    else:\n      self.thresholds = classification_thresholds", "response": "Creates this cascade from the given boosted machine."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_boosted_machine(self):\n    strong = bob.learn.boosting.BoostedMachine()\n    for machine, index in zip(self.cascade, self.indices):\n      weak = machine.weak_machines\n      weights = machine.weights\n      for i in range(len(weak)):\n        strong.add_weak_machine(weak[i], weights[i])\n\n    return strong", "response": "Generates a strong classifier from this cascade by concatenating all strong classifiers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave this cascade into the given HDF5 file.", "response": "def save(self, hdf5):\n    \"\"\"Saves this cascade into the given HDF5 file.\n\n    **Parameters:**\n\n    ``hdf5`` : :py:class:`bob.io.base.HDF5File`\n      An HDF5 file open for writing\n    \"\"\"\n    # write the cascade to file\n    hdf5.set(\"Thresholds\", self.thresholds)\n    for i in range(len(self.cascade)):\n      hdf5.create_group(\"Classifier_%d\" % (i+1))\n      hdf5.cd(\"Classifier_%d\" % (i+1))\n      self.cascade[i].save(hdf5)\n      hdf5.cd(\"..\")\n    hdf5.create_group(\"FeatureExtractor\")\n    hdf5.cd(\"FeatureExtractor\")\n    self.extractor.save(hdf5)\n    hdf5.cd(\"..\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload this cascade from the given HDF5 file.", "response": "def load(self, hdf5):\n    \"\"\"Loads this cascade from the given HDF5 file.\n\n    **Parameters:**\n\n    ``hdf5`` : :py:class:`bob.io.base.HDF5File`\n      An HDF5 file open for reading\n    \"\"\"\n    # write the cascade to file\n    self.thresholds = hdf5.read(\"Thresholds\")\n    self.cascade = []\n    for i in range(len(self.thresholds)):\n      hdf5.cd(\"Classifier_%d\" % (i+1))\n      self.cascade.append(bob.learn.boosting.BoostedMachine(hdf5))\n      hdf5.cd(\"..\")\n    hdf5.cd(\"FeatureExtractor\")\n    self.extractor = FeatureExtractor(hdf5)\n    hdf5.cd(\"..\")\n    self._indices()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef message(obj, commit='HEAD', skip_merge_commits=False):\n    from ..kwalitee import check_message\n    options = obj.options\n    repository = obj.repository\n\n    if options.get('colors') is not False:\n        colorama.init(autoreset=True)\n        reset = colorama.Style.RESET_ALL\n        yellow = colorama.Fore.YELLOW\n        green = colorama.Fore.GREEN\n        red = colorama.Fore.RED\n    else:\n        reset = yellow = green = red = ''\n\n    try:\n        sha = 'oid'\n        commits = _pygit2_commits(commit, repository)\n    except ImportError:\n        try:\n            sha = 'hexsha'\n            commits = _git_commits(commit, repository)\n        except ImportError:\n            click.echo('To use this feature, please install pygit2. '\n                       'GitPython will also work but is not recommended '\n                       '(python <= 2.7 only).',\n                       file=sys.stderr)\n            return 2\n\n    template = '{0}commit {{commit.{1}}}{2}\\n\\n'.format(yellow, sha, reset)\n    template += '{message}{errors}'\n\n    count = 0\n    ident = '    '\n    re_line = re.compile('^', re.MULTILINE)\n    for commit in commits:\n        if skip_merge_commits and _is_merge_commit(commit):\n            continue\n        message = commit.message\n        errors = check_message(message, **options)\n        message = re.sub(re_line, ident, message)\n        if errors:\n            count += 1\n            errors.insert(0, red)\n        else:\n            errors = [green, 'Everything is OK.']\n        errors.append(reset)\n\n        click.echo(template.format(commit=commit,\n                                   message=message.encode('utf-8'),\n                                   errors='\\n'.join(errors)))\n\n    if min(count, 1):\n        raise click.Abort", "response": "Check the messages of the commits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef files(obj, commit='HEAD', skip_merge_commits=False):\n    from ..kwalitee import check_file, SUPPORTED_FILES\n    from ..hooks import run\n    options = obj.options\n    repository = obj.repository\n\n    if options.get('colors') is not False:\n        colorama.init(autoreset=True)\n        reset = colorama.Style.RESET_ALL\n        yellow = colorama.Fore.YELLOW\n        green = colorama.Fore.GREEN\n        red = colorama.Fore.RED\n    else:\n        reset = yellow = green = red = ''\n\n    try:\n        sha = 'oid'\n        commits = _pygit2_commits(commit, repository)\n    except ImportError:\n        try:\n            sha = 'hexsha'\n            commits = _git_commits(commit, repository)\n        except ImportError:\n            click.echo(\n                'To use this feature, please install pygit2. GitPython will '\n                'also work but is not recommended (python <= 2.7 only).',\n                file=sys.stderr)\n            click.exit(2)\n\n    template = '{0}commit {{commit.{1}}}{2}\\n\\n'.format(yellow, sha, reset)\n    template += '{message}{errors}\\n'\n\n    error_template = '\\n{0}{{filename}}\\n{1}{{errors}}{0}'.format(reset, red)\n    no_errors = ['\\n{0}Everything is OK.{1}'.format(green, reset)]\n    msg_file_excluded = '\\n{0}{{filename}} excluded.{1}'.format(yellow, reset)\n\n    def _get_files_modified(commit):\n        \"\"\"Get the list of modified files that are Python or Jinja2.\"\"\"\n        cmd = \"git show --no-commit-id --name-only --diff-filter=ACMRTUXB {0}\"\n        _, files_modified, _ = run(cmd.format(commit))\n\n        extensions = [re.escape(ext)\n                      for ext in list(SUPPORTED_FILES) + [\".rst\"]]\n        test = \"(?:{0})$\".format(\"|\".join(extensions))\n        return list(filter(lambda f: re.search(test, f), files_modified))\n\n    def _ensure_directory(filename):\n        dir_ = os.path.dirname(filename)\n        if not os.path.exists(dir_):\n            os.makedirs(dir_)\n\n    def _format_errors(args):\n        filename, errors = args\n        if errors is None:\n            return msg_file_excluded.format(filename=filename)\n        else:\n            return error_template.format(filename=filename, errors='\\n'.join(\n                errors if len(errors) else no_errors))\n\n    count = 0\n    ident = '    '\n    re_line = re.compile('^', re.MULTILINE)\n    for commit in commits:\n        if skip_merge_commits and _is_merge_commit(commit):\n            continue\n        message = commit.message\n        commit_sha = getattr(commit, sha)\n        tmpdir = mkdtemp()\n        errors = {}\n        try:\n            for filename in _get_files_modified(commit):\n                cmd = \"git show {commit_sha}:{filename}\"\n                _, out, _ = run(cmd.format(commit_sha=commit_sha,\n                                           filename=filename),\n                                raw_output=True)\n\n                destination = os.path.join(tmpdir, filename)\n                _ensure_directory(destination)\n\n                with open(destination, 'w+') as f:\n                    f.write(out)\n\n                errors[filename] = check_file(destination, **options)\n        finally:\n            shutil.rmtree(tmpdir, ignore_errors=True)\n\n        message = re.sub(re_line, ident, message)\n        if len(errors):\n            count += 1\n            errors = map(_format_errors, errors.items())\n        else:\n            errors = no_errors\n\n        click.echo(template.format(commit=commit,\n                                   message=message.encode('utf-8'),\n                                   errors='\\n'.join(errors)))\n\n    if min(count, 1):\n        raise click.Abort", "response": "Check the files of the commits."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_obj_subcmds(obj):\n    subcmds = []\n    for label in dir(obj.__class__):\n        if label.startswith(\"_\"):\n            continue\n        if isinstance(getattr(obj.__class__, label, False), property):\n            continue\n        rvalue = getattr(obj, label)\n        if not callable(rvalue) or not is_cmd(rvalue):\n            continue\n        if isinstance(obj, types.MethodType) and \\\n               label in (\"im_func\", \"im_self\", \"im_class\"):\n            continue\n        ## potential command\n        command_name = getattr(rvalue, \"command_name\",\n                               label[:-1] if label.endswith(\"_\") else\n                               label)\n        subcmds.append((command_name, rvalue))\n    return OrderedDict(subcmds)", "response": "Fetch action in callable attributes which and commands\n    Callable must have their attribute command set to True to be recognised by this lookup."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn probed sub module names from given module", "response": "def get_module_resources(mod):\n    \"\"\"Return probed sub module names from given module\"\"\"\n\n    path = os.path.dirname(os.path.realpath(mod.__file__))\n    prefix = kf.basename(mod.__file__, (\".py\", \".pyc\"))\n\n    if not os.path.exists(mod.__file__):\n        import pkg_resources\n        for resource_name in pkg_resources.resource_listdir(mod.__name__, ''):\n            if resource_name.startswith(\"%s_\" % prefix) and resource_name.endswith(\".py\"):\n                module_name, _ext = os.path.splitext(kf.basename(resource_name))\n                yield module_name\n\n    for f in glob.glob(os.path.join(path, '%s_*.py' % prefix)):\n        module_name, _ext = os.path.splitext(kf.basename(f))\n        yield module_name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all the subcommands of a module.", "response": "def get_mod_subcmds(mod):\n    \"\"\"Fetch action in same directory in python module\n\n    python module loaded are of this form: '%s_*.py' % prefix\n\n    \"\"\"\n\n    ## Look in modules attributes\n\n    subcmds = get_obj_subcmds(mod)\n\n    path = os.path.dirname(os.path.realpath(mod.__file__))\n    if mod.__package__ is None:\n        sys.path.insert(0, os.path.dirname(path))\n        mod.__package__ = kf.basename(path)\n\n    for module_name in get_module_resources(mod):\n        try:\n            mod = importlib.import_module(\".%s\" % module_name, mod.__package__)\n        except ImportError as e:\n            msg.warn(\"%r could not be loaded: %s\"\n                     % (module_name, e.message))\n            continue\n        except IOError as e:\n            print(\"%s\" % module_name)\n            raise\n        if hasattr(mod, \"Command\") and is_cmd(mod.Command):\n            obj = mod.Command\n            if obj.__doc__ is None:\n                msg.warn(\"Missing doc string for command from \"\n                         \"module %s\" % module_name)\n                continue\n            if isinstance(obj, type):\n                obj = obj()  ## instanciate it.\n            name = module_name.split(\"_\", 1)[1]\n            if name in subcmds:\n                raise ValueError(\n                    \"Module command %r conflicts with already defined object \"\n                    \"command.\"\n                    % name)\n            subcmds[name] = obj\n\n    return subcmds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninterpolate complete help doc of given object and environment.", "response": "def get_help(obj, env, subcmds):\n    \"\"\"Interpolate complete help doc of given object\n\n    Assumption that given object as a specific interface:\n\n    obj.__doc__ is the basic help object.\n    obj.get_actions_titles() returns the subcommand if any.\n\n\n    \"\"\"\n    doc = txt.dedent(obj.__doc__ or \"\")\n    env = env.copy()  ## get a local copy\n\n    doc = doc.strip()\n    if not re.search(r\"^usage:\\s*$\", doc, flags=re.IGNORECASE | re.MULTILINE):\n        doc += txt.dedent(\"\"\"\n\n            Usage:\n              %(std_usage)s\n\n            Options:\n              %(std_options)s\"\"\")\n\n    help_line = (\"  %%-%ds  %%s\"\n                 % (max([5] + [len(a) for a in subcmds]), ))\n    env[\"actions\"] = \"\\n\".join(\n        help_line % (\n            name,\n            get_help(subcmd, subcmd_env(env, name), {}).split(\"\\n\")[0])\n        for name, subcmd in subcmds.items())\n    env[\"actions_help\"] = \"\" if not env[\"actions\"] else (\n        \"ACTION could be one of:\\n\\n\"\n        \"%(actions)s\\n\\n\"\n        \"See '%(surcmd)s help ACTION' for more information \"\n        \"on a specific command.\"\n        % env)\n    if \"%(std_usage)s\" in doc:\n        env[\"std_usage\"] = txt.indent(\n            (\"%(surcmd)s --help\\n\"\n             \"%(surcmd)s --version\" +\n             ((\"\\n%(surcmd)s help [COMMAND]\"\n               \"\\n%(surcmd)s ACTION [ARGS...]\") if subcmds else \"\"))\n            % env,\n            _find_prefix(doc, \"%(std_usage)s\"),\n            first=\"\")\n    if \"%(std_options)s\" in doc:\n        env[\"std_options\"] = txt.indent(\n            \"--help          Show this screen.\\n\"\n            \"--version       Show version.\",\n            _find_prefix(doc, \"%(std_options)s\"),\n            first=\"\")\n\n    if subcmds and \"%(actions_help)s\" not in doc:\n        doc += \"\\n\\n%(actions_help)s\"\n    try:\n        output = doc % env\n    except KeyError as e:\n        msg.err(\"Doc interpolation of %s needed missing key %r\"\n                % (aformat(env[\"surcmd\"], attrs=[\"bold\", ]),\n                   e.args[0]))\n        exit(1)\n    except Exception as e:\n        msg.err(\n            \"Documentation of %s is not valid. Please check it:\\n%s\"\n            % (aformat(env[\"surcmd\"], attrs=[\"bold\", ]),\n               doc))\n        exit(1)\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_calling_prototype(acallable):\n    assert callable(acallable)\n    if inspect.ismethod(acallable) or inspect.isfunction(acallable):\n        args, vargs, vkwargs, defaults = inspect.getargspec(acallable)\n    elif not inspect.isfunction(acallable) and hasattr(acallable, \"__call__\"):\n        ## a class instance ? which is callable...\n        args, vargs, vkwargs, defaults = inspect.getargspec(acallable.__call__)\n        ## remove the 'self' argument\n        args = args[1:]\n    else:\n        raise ValueError(\"Hum, %r is a callable, but not a function/method, \"\n                         \"nor a instance with __call__ arg...\"\n                         % acallable)\n\n    if vargs or vkwargs:\n        raise SyntaxError(\"variable *arg or **kwarg are not supported.\")\n\n    if is_bound(acallable):\n        args = args[1:]\n\n    if defaults is None:\n        defaults = ()  ## be coherent\n\n    return args, defaults", "response": "Returns the actual working calling prototype for the given acallable object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning tuple pos args kwargs to call given callable", "response": "def match_prototype(acallable, arguments):\n    \"\"\"Return tuple (pos args, kwargs) to call given callable\n\n    Let's define a callable that will printout\n\n    >>> arguments = {'alphonse': 1, 'bertrand': 2, 'charlie': 3}\n\n    >>> match_prototype(lambda arguments: None, arguments)\n    ([{'bertrand': 2, 'charlie': 3, 'alphonse': 1}], {})\n    >>> match_prototype(lambda args: None, arguments)\n    ([{'bertrand': 2, 'charlie': 3, 'alphonse': 1}], {})\n\n\n    >>> match_prototype(lambda bertrand, arguments: None, arguments)\n    ([2, {'charlie': 3, 'alphonse': 1}], {})\n\n    >>> match_prototype(lambda bertrand, arguments, foo=None: None, arguments)\n    ([2, {'charlie': 3, 'alphonse': 1}], {})\n\n    >>> match_prototype(lambda bertrand, arguments, charlie=None: None,\n    ...                 arguments)\n    ([2, {'alphonse': 1}], {'charlie': 3})\n\n    \"\"\"\n\n    args, defaults = get_calling_prototype(acallable)\n    arguments = arguments.copy()\n    defaults = [] if defaults is None else defaults\n    p = []\n    kw = {}\n\n    pos_args = len(args) - len(defaults)\n    has_args = any(k in ('args', 'arguments')\n                   for k in args)\n    args_label_pos = None\n    for i, arg in enumerate(args):\n        is_pos = i < pos_args\n        val = None\n        if not args_label_pos and arg in ('arguments', 'args'):\n            val = arguments  ## copy by reference here is important\n        else:\n            k = None\n            for k in arguments:\n                norm = k\n                if norm.startswith(\"--\"):\n                    if is_pos:\n                        continue\n                    norm = norm[2:]\n                elif k.startswith(\"-\"):\n                    if is_pos:\n                        continue\n                    norm = norm[1:]\n                norm = norm.lower()\n                norm = norm.replace('-', '_')\n                if norm == arg:\n                    break\n            else:\n                if not has_args:\n                    raise SyntaxError(\n                        \"Can't match your function argument %r with \"\n                        \"command line keys (%s).\"\n                        % (arg, \", \".join(arguments.keys())))\n                else:\n                    k = None\n            if k is not None:\n                ## inplace removal is important here\n                val = arguments.pop(k)\n        if is_pos:\n            p.append(val)\n        else:\n            if val is not None:\n                ## we should only have strings if it was set.\n                kw[arg] = val\n    return p, kw"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize(self):\n        if not os.path.exists(self.root_dir):\n            os.makedirs(self.root_dir)\n        assert os.path.isdir(self.root_dir), \"%s is not a directory! Please move or remove it.\" % self.root_dir\n        for d in [\"bin\", \"lib\", \"include\"]:\n            target_path = os.path.join(self.root_dir, d)\n            if not os.path.exists(target_path):\n                os.makedirs(target_path)\n        if not os.path.exists(self.manifest_path):\n            open(self.manifest_path, \"w+\").close()\n        self.new = False", "response": "Generate the root directory root if it doesn t already exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef finalize(self):\n        if self.rc_file:\n            self.rc_file.close()\n        if self.env_file:\n            self.env_file.close()", "response": "finalize any open file handles"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(self):\n        if self.rc_file:\n            self.rc_file.close()\n        if self.env_file:\n            self.env_file.close()\n        shutil.rmtree(self.root_dir)", "response": "Removes the sprinter directory if it exists"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_from_bin(self, name):\n        self.__remove_path(os.path.join(self.root_dir, \"bin\", name))", "response": "Remove an object from the bin folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving an object from the bin folder.", "response": "def remove_from_lib(self, name):\n        \"\"\" Remove an object from the bin folder. \"\"\"\n        self.__remove_path(os.path.join(self.root_dir, \"lib\", name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves an existing feature from the environment root folder.", "response": "def remove_feature(self, feature_name):\n        \"\"\" Remove an feature from the environment root folder. \"\"\"\n        self.clear_feature_symlinks(feature_name)\n        if os.path.exists(self.install_directory(feature_name)):\n            self.__remove_path(self.install_directory(feature_name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_feature_symlinks(self, feature_name):\n        logger.debug(\"Clearing feature symlinks for %s\" % feature_name)\n        feature_path = self.install_directory(feature_name)\n        for d in ('bin', 'lib'):\n            if os.path.exists(os.path.join(self.root_dir, d)):\n                for link in os.listdir(os.path.join(self.root_dir, d)):\n                    path = os.path.join(self.root_dir, d, link)\n                    if feature_path in os.path.realpath(path):\n                        getattr(self, 'remove_from_%s' % d)(link)", "response": "Clear the symlinks for a feature in the symlinked path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_to_env(self, content):\n        if not self.rewrite_config:\n            raise DirectoryException(\"Error! Directory was not intialized w/ rewrite_config.\")\n        if not self.env_file:\n            self.env_path, self.env_file = self.__get_env_handle(self.root_dir)\n        self.env_file.write(content + '\\n')", "response": "add content to the env script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding content to the rc script.", "response": "def add_to_rc(self, content):\n        \"\"\"\n        add content to the rc script.\n        \"\"\"\n        if not self.rewrite_config:\n            raise DirectoryException(\"Error! Directory was not intialized w/ rewrite_config.\")\n        if not self.rc_file:\n            self.rc_path, self.rc_file = self.__get_rc_handle(self.root_dir)\n        self.rc_file.write(content + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_to_gui(self, content):\n        if not self.rewrite_config:\n            raise DirectoryException(\"Error! Directory was not intialized w/ rewrite_config.\")\n        if not self.gui_file:\n            self.gui_path, self.gui_file = self.__get_gui_handle(self.root_dir)\n        self.gui_file.write(content + '\\n')", "response": "add content to the gui script."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __remove_path(self, path):\n        curpath = os.path.abspath(os.curdir)\n        if not os.path.exists(path):\n            logger.warn(\"Attempted to remove a non-existent path %s\" % path)\n            return\n        try:\n            if os.path.islink(path):\n                os.unlink(path)\n            elif os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.unlink(path)\n\n            # in the case we just deleted ourselves out of a valid directory,\n            # we move to a valid directory.\n            if curpath == path:\n                os.chdir(tempfile.gettempdir())\n\n        except OSError:\n            logger.error(\"Unable to remove object at path %s\" % path)\n            raise DirectoryException(\"Unable to remove object at path %s\" % path)", "response": "Remove an object at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __get_env_handle(self, root_dir):\n        env_path = os.path.join(root_dir, '.env')\n        gui_path = os.path.join(root_dir, '.gui')\n        fh = open(env_path, \"w+\")\n        # .env will source utils.sh if it hasn't already\n        fh.write(source_template % (gui_path, gui_path))\n        fh.write(source_template % (self.shell_util_path,\n                                    self.shell_util_path))\n        return (env_path, fh)", "response": "get the filepath and filehandle to the. env file for the environment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the filepath and filehandle to the rc file for the environment", "response": "def __get_rc_handle(self, root_dir):\n        \"\"\" get the filepath and filehandle to the rc file for the environment \"\"\"\n        rc_path = os.path.join(root_dir, '.rc')\n        env_path = os.path.join(root_dir, '.env')\n        fh = open(rc_path, \"w+\")\n        # .rc will always source .env\n        fh.write(source_template % (env_path, env_path))\n        return (rc_path, fh)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __get_gui_handle(self, root_dir):\n        gui_path = os.path.join(root_dir, '.gui')\n        fh = open(gui_path, \"w+\")\n        return (gui_path, fh)", "response": "get the filepath and filehandle to the. env file for the environment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self, options=None):\n        if options is None:\n            raise ValueError(\"Please pass in an options dict\")\n\n        if not _has_content(options):\n            raise NoContentError(\"must supply 'document_content' or 'document_url'\")\n\n        default_options = {\n            \"name\": \"default\",\n            \"document_type\": \"pdf\",\n            \"test\": False,\n            \"async\": False,\n            \"raise_exception_on_failure\": False,\n        }\n        options = dict(list(default_options.items()) + list(options.items()))\n        raise_exception_on_failure = options.pop(\"raise_exception_on_failure\")\n        query = {\"user_credentials\": self.api_key}\n        if options[\"async\"]:\n            query[\"output\"] = \"json\"\n\n        resp = requests.post(\n            \"%sdocs\" % (self._url), json=options, params=query, timeout=self._timeout\n        )\n\n        if raise_exception_on_failure and resp.status_code != 200:\n            raise DocumentCreationFailure(resp.content, resp.status_code)\n\n        if options[\"async\"]:\n            return json.loads(resp.content.decode(\"utf-8\"))\n        else:\n            return resp", "response": "Create a new document job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_docs(self, options=None):\n        if options is None:\n            raise ValueError(\"Please pass in an options dict\")\n\n        default_options = {\n            \"page\": 1,\n            \"per_page\": 100,\n            \"raise_exception_on_failure\": False,\n            \"user_credentials\": self.api_key,\n        }\n        options = dict(list(default_options.items()) + list(options.items()))\n        raise_exception_on_failure = options.pop(\"raise_exception_on_failure\")\n\n        resp = requests.get(\n            \"%sdocs\" % (self._url), params=options, timeout=self._timeout\n        )\n\n        if raise_exception_on_failure and resp.status_code != 200:\n            raise DocumentListingFailure(resp.content, resp.status_code)\n        return resp", "response": "Return list of previously created documents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the status of the generation job.", "response": "def status(self, status_id, raise_exception_on_failure=False):\n        \"\"\"Return the status of the generation job.\"\"\"\n        query = {\"output\": \"json\", \"user_credentials\": self.api_key}\n\n        resp = requests.get(\n            \"%sstatus/%s\" % (self._url, status_id), params=query, timeout=self._timeout\n        )\n\n        if raise_exception_on_failure and resp.status_code != 200:\n            raise DocumentStatusFailure(resp.content, resp.status_code)\n\n        if resp.status_code == 200:\n            as_json = json.loads(resp.content)\n            if as_json[\"status\"] == \"completed\":\n                as_json[\"download_key\"] = _get_download_key(as_json[\"download_url\"])\n            return as_json\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads the file represented by the download_key.", "response": "def download(self, download_key, raise_exception_on_failure=False):\n        \"\"\"Download the file represented by the download_key.\"\"\"\n        query = {\"output\": \"json\", \"user_credentials\": self.api_key}\n        resp = requests.get(\n            \"%sdownload/%s\" % (self._url, download_key),\n            params=query,\n            timeout=self._timeout,\n        )\n\n        if raise_exception_on_failure and resp.status_code != 200:\n            raise DocumentDownloadFailure(resp.content, resp.status_code)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge an INIConf object into a.", "response": "def merge_INIConf(a, b):\n    \"\"\"\u7528 b \u7684\u5185\u5bb9\u8986\u76d6 a \u7684\u5185\u5bb9\uff08\u82e5\u91cd\u540d\uff09\uff0c\u5e76\u8fd4\u56de a\n    \"\"\"\n    for sname in b.sections():\n        if a.has_section(sname):\n            for oname in b.options(sname):\n                a[sname][oname] = b[sname][oname]\n        else:\n            a[sname] = b[sname]\n    return a"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_from_dict(self, adict, parent=None):\n        if not parent:\n            parent = self\n        for k,v in adict.items():\n            if isinstance(v, dict):\n                vDict = PYConf(v)\n                self.copy_from_dict(v, vDict)\n                parent[k] = vDict\n            else:\n                parent[k] = v", "response": "Copy the contents of a dict to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndumping a key - value entry to a string", "response": "def dump(self, human=False):\n        \"\"\"\u5c06\u81ea\u8eab\u5185\u5bb9\u6253\u5370\u6210\u5b57\u7b26\u4e32\n\n        :param bool human: \u82e5\u503c\u4e3a True \uff0c\u5219\u6253\u5370\u6210\u6613\u8bfb\u683c\u5f0f\u3002\n\n        \"\"\"\n        txt = str(self)\n        if human:\n            txt = txt.replace(\", '\", \",\\n'\")\n            txt = txt.replace(\"{\", \"{\\n\")\n            txt = txt.replace(\"}\", \"\\n}\")\n            txt = txt.replace(\"[\", \"[\\n\")\n            txt = txt.replace(\"]\", \"\\n]\")\n        return txt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_to_file(self, path, human=True):\n        write_file(path, self.dump(human))\n        slog.info(\"Save %a done.\", path)", "response": "Save the current instance to a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_from_file(self, path):\n        if not os.path.exists(path):\n            slog.warning(\"The file %s is not exist.\", path)\n            return False\n        txt = read_file(path)\n        dic = eval(txt)\n        self.copy_from_dict(dic)\n        return True", "response": "read from file and copy it to the instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_parameters(self, parameters):\n        nr_f = self.f.size\n\n        # sort out parameters\n        rho0, m, tau, c = self._sort_parameters(parameters)\n\n        newsize = (nr_f, len(m))\n        # rho0_resized = np.resize(rho0, newsize)\n        m_resized = np.resize(m, newsize)\n        tau_resized = np.resize(tau, newsize)\n        c_resized = np.resize(c, newsize)\n\n        omega = np.atleast_2d(2 * np.pi * self.f).T\n        self.w = np.resize(omega, (len(m), nr_f)).T\n        self.rho0 = rho0\n        self.m = m_resized\n        self.tau = tau_resized\n        self.c = c_resized\n\n        # compute some common terms\n        self.otc = (self.w * self.tau) ** self.c\n        self.otc2 = (self.w * self.tau) ** (2 * self.c)\n        self.ang = self.c * np.pi / 2.0  # rad\n        self.denom = 1 + 2 * self.otc * np.cos(self.ang) + self.otc2", "response": "Set the parameters of the config\n        class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef response(self, parameters):\n        # get a config object\n        self._set_parameters(parameters)\n        terms = self.m * (1 - (1 / (1 + (1j * self.w * self.tau) ** self.c)))\n        # sum up terms\n        specs = np.sum(terms, axis=1)\n        rcomplex = self.rho0 * (1 - specs)\n        response = sip_response.sip_response(self.f, rcomplex=rcomplex)\n\n        return response", "response": "Complex response of the Cole - Cole model object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dre_dlog10rho0(self, pars):\n\n        # first call the linear response to set the parameters\n        linear_response = self.dre_drho0(pars)\n        result = np.log(10) * self.rho0 * linear_response\n        return result", "response": "Compute partial derivative of real parts to log10 rho0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dre_dm(self, pars):\n        self._set_parameters(pars)\n        numerator = -self.otc * (np.cos(self.ang) + self.otc)\n        result = numerator / self.denom\n        result *= self.rho0\n        return result", "response": "r Returns the DRE distribution of the current state of the current species."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Jacobian_re_im(self, pars):\n        partials = []\n\n        # partials.append(self.dre_dlog10rho0(pars)[:, np.newaxis, :])\n        partials.append(self.dre_drho0(pars)[:, np.newaxis])\n        partials.append(self.dre_dm(pars))\n        # partials.append(self.dre_dlog10tau(pars))\n        partials.append(self.dre_dtau(pars))\n        partials.append(self.dre_dc(pars))\n        # partials.append(self.dim_dlog10rho0(pars)[:, np.newaxis, :])\n        partials.append(self.dim_drho0(pars)[:, np.newaxis])\n        partials.append(self.dim_dm(pars))\n        # partials.append(self.dim_dlog10tau(pars))\n        partials.append(self.dim_dtau(pars))\n        partials.append(self.dim_dc(pars))\n\n        print('SHAPES')\n        for x in partials:\n            print(x.shape)\n\n        J = np.concatenate(partials, axis=1)\n        return J", "response": "r Returns the Jacobian of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_dict_or_list_from_json(desired_type: Type[dict], file_object: TextIOBase,\n                                logger: Logger, conversion_finder: ConversionFinder, **kwargs) -> Dict[str, Any]:\n    \"\"\"\n    Helper method to read a dictionary from a .json file using json library\n    :param file_object:\n    :return:\n    \"\"\"\n    # lazy import in order not to force use of jprops\n    import json\n    res = json.load(file_object)\n\n    # convert if required\n    return ConversionFinder.convert_collection_values_according_to_pep(res, desired_type, conversion_finder, logger, **kwargs)", "response": "Helper method to read a dictionary from a. json file using json library\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_parsing_plan_for_multifile_children(self, obj_on_fs: PersistedObject, desired_type: Type[Any],\n                                                 logger: Logger) -> Dict[str, Any]:\n        \"\"\"\n        Simply inspects the required type to find the base type expected for items of the collection,\n        and relies on the ParserFinder to find the parsing plan\n\n        :param obj_on_fs:\n        :param desired_type:\n        :param logger:\n        :return:\n        \"\"\"\n        # nb of file children\n        n_children = len(obj_on_fs.get_multifile_children())\n\n        # first extract base collection type\n        subtypes, key_type = _extract_collection_base_type(desired_type)\n\n        if isinstance(subtypes, tuple):\n            # -- check the tuple length\n            if n_children != len(subtypes):\n                raise FolderAndFilesStructureError.create_for_multifile_tuple(obj_on_fs, len(subtypes),\n                                                                              len(obj_on_fs.get_multifile_children()))\n        else:\n            # -- repeat the subtype n times\n            subtypes = [subtypes] * n_children\n\n        # -- for each child create a plan with the appropriate parser\n        children_plan = OrderedDict()\n        # use sorting for reproducible results in case of multiple errors\n        for (child_name, child_fileobject), child_typ in zip(sorted(obj_on_fs.get_multifile_children().items()),\n                                                           subtypes):\n            # -- use the parserfinder to find the plan\n            t, child_parser = self.parser_finder.build_parser_for_fileobject_and_desiredtype(child_fileobject, \n                                                                                             child_typ, logger)\n            children_plan[child_name] = child_parser.create_parsing_plan(t, child_fileobject, logger,\n                                                                         _main_call=False)\n\n        return children_plan", "response": "Returns a parsing plan for the items of the collection that are of the desired type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_multifile(self, desired_type: Type[Union[Dict, List, Set, Tuple]], obj: PersistedObject,\n                         parsing_plan_for_children: Dict[str, ParsingPlan], logger: Logger,\n                         options: Dict[str, Dict[str, Any]]) \\\n            -> Union[Dict, List, Set, Tuple]:\n        \"\"\"\n        Options may contain a section with id 'MultifileCollectionParser' containing the following options:\n        * lazy_parsing: if True, the method will return immediately without parsing all the contents. Instead, the\n        returned collection will perform the parsing the first time an item is required.\n        * background_parsing: if True, the method will return immediately while a thread parses all the contents in\n        the background. Note that users cannot set both lazy_parsing and background_parsing to True at the same time\n\n        :param desired_type:\n        :param obj:\n        :param parsing_plan_for_children:\n        :param logger:\n        :param options:\n        :return:\n        \"\"\"\n\n        # first get the options and check them\n        lazy_parsing = False\n        background_parsing = False\n\n        opts = self._get_applicable_options(options)\n        for opt_key, opt_val in opts.items():\n            if opt_key is 'lazy_parsing':\n                lazy_parsing = opt_val\n            elif opt_key is 'background_parsing':\n                background_parsing = opt_val\n            else:\n                raise Exception('Invalid option in MultiFileCollectionParser : ' + opt_key)\n\n        check_var(lazy_parsing, var_types=bool, var_name='lazy_parsing')\n        check_var(background_parsing, var_types=bool, var_name='background_parsing')\n\n        if lazy_parsing and background_parsing:\n            raise ValueError('lazy_parsing and background_parsing cannot be set to true at the same time')\n\n        if lazy_parsing:\n            # build a lazy dictionary\n            results = LazyDictionary(sorted(list(parsing_plan_for_children.keys())),\n                                     loading_method=lambda x: parsing_plan_for_children[x].execute(logger, options))\n            # logger.debug('Assembling a ' + get_pretty_type_str(desired_type) + ' from all children of ' + str(obj)\n            #             + ' (lazy parsing: children will be parsed when used) ')\n            logger.debug('(P) {loc} : lazy parsing ON, children will be parsed only if/when used'.format(\n                loc=obj.get_pretty_location(blank_parent_part=(not GLOBAL_CONFIG.full_paths_in_logs),\n                                            compact_file_ext=True)))\n\n        elif background_parsing:\n            # -- TODO create a thread to perform the parsing in the background\n            raise ValueError('Background parsing is not yet supported')\n\n        else:\n            # Parse right now\n            results = OrderedDict()\n\n            # parse all children according to their plan\n            # -- use key-based sorting on children to lead to reproducible results\n            # (in case of multiple errors, the same error will show up first everytime)\n            for child_name, child_plan in sorted(parsing_plan_for_children.items()):\n                results[child_name] = child_plan.execute(logger, options)\n            # logger.debug('Assembling a ' + get_pretty_type_str(desired_type) + ' from all parsed children of '\n            #             + str(obj))\n\n        if issubclass(desired_type, list):\n            # return a list facade\n            return KeySortedListFacadeForDict(results)\n        elif issubclass(desired_type, tuple):\n            # return a tuple facade\n            return KeySortedTupleFacadeForDict(results)\n        elif issubclass(desired_type, set):\n            # return a set facade\n            return SetFacadeForDict(results)\n        elif issubclass(desired_type, dict):\n            # return the dict directly\n            return results\n        else:\n            raise TypeError('Cannot build the desired collection out of the multifile children: desired type is not '\n                            'supported: ' + get_pretty_type_str(desired_type))", "response": "Parse a multifile object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndispatch the event to the queue using a producer.", "response": "def dispatch(self, producer=None):\n        \"\"\"\n        Dispatch the event, sending a message to the queue using a producer.\n        :param producer: optional `Producer` to replace the default one.\n        \"\"\"\n        log.info('@Event.dispatch `{}` with subject `{}`'\n                 .format(self.name, self.subject))\n        producer = (producer or Registry.get_producer())\n        if not producer:\n            raise MissingProducerError('You have not registered a Producer')\n        try:\n            producer.produce(self.topic, self.name, self.subject, self.data)\n        except:\n            fallback = Registry.get_fallback()\n            fallback(self)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading annotations from the given annotation file.", "response": "def read_annotation_file(annotation_file, annotation_type):\n  \"\"\"read_annotation_file(annotation_file, annotation_type) -> annotations\n  Reads annotations from the given ``annotation_file``.\n\n  The way, how annotations are read depends on the given ``annotation_type``.\n  Depending on the type, one or several annotations might be present in the annotation file.\n  Currently, these variants are implemented:\n\n  - ``'lr-eyes'``: Only the eye positions are stored, in a single row, like: ``le_x le_y re_x re_y``, comment lines starting with ``'#'`` are ignored.\n  - ``'named'``: Each line of the file contains a name and two floats, like ``reye x y``; empty lines separate between sets of annotations.\n  - ``'idiap'``: A special 22 point format, where each line contains the index and the locations, like ``1 x y``.\n  - ``'fddb'``: a special format for the FDDB database; empty lines separate between sets of annotations\n\n  Finally, a list of ``annotations`` is returned in the format: ``[{name: (y,x)}]``.\n\n  **Parameters:**\n\n  ``annotation_file`` : str\n    The file name of the annotation file to read\n\n  ``annotation_type`` : str (see above)\n    The style of annotation file, in which the given ``annotation_file`` is\n\n  **Returns:**\n\n  ``annotations`` : [dict]\n    A list of annotations read from the given file, grouped by annotated objects (faces).\n    Each annotation is generally specified as the two eye coordinates, i.e., ``{'reye' : (rey, rex), 'leye' : (ley, lex)}``, but other types of annotations might occur as well.\n  \"\"\"\n  annotations = [{}]\n  with open(annotation_file) as f:\n    if annotation_type == 'idiap':\n      # This is a special format where we have enumerated annotations, and a 'gender'\n      for line in f:\n        positions = line.rstrip().split()\n        if positions:\n          if positions[0].isdigit():\n            # position field\n            assert len(positions) == 3\n            id = int(positions[0])\n            annotations[-1]['key%d'%id] = (float(positions[2]),float(positions[1]))\n          else:\n            # another field, we take the first entry as key and the rest as values\n            annotations[-1][positions[0]] = positions[1:]\n        elif len(annotations[-1]) > 0:\n          # empty line; split between annotations\n          annotations.append({})\n      # finally, we add the eye center coordinates as the center between the eye corners; the annotations 3 and 8 seem to be the pupils...\n      for annotation in annotations:\n        if 'key1' in annotation and 'key5' in annotation:\n          annotation['reye'] = ((annotation['key1'][0] + annotation['key5'][0])/2., (annotation['key1'][1] + annotation['key5'][1])/2.)\n        if 'key6' in annotation and 'key10' in annotation:\n          annotation['leye'] = ((annotation['key6'][0] + annotation['key10'][0])/2., (annotation['key6'][1] + annotation['key10'][1])/2.)\n    elif annotation_type == 'lr-eyes':\n      # In this format, the eyes are given in a single row \"le_x le_y re_x re_y\", possibly with a comment line\n      # There is only a single annotation per image\n      for line in f:\n        if len(line) and line[0] != '#':\n          positions = line.rstrip().split()\n          annotations[0]['leye'] = (float(positions[1]),float(positions[0]))\n          annotations[0]['reye'] = (float(positions[3]),float(positions[2]))\n    elif annotation_type == 'named':\n      # In this format, each line contains three entries: \"keyword x y\"\n      for line in f:\n        positions = line.rstrip().split()\n        if positions:\n          annotations[-1][positions[0]] = (float(positions[2]),float(positions[1]))\n        elif len(annotations[-1]) > 0:\n          # empty line; split between annotations\n          annotations.append({})\n    elif annotation_type == 'fddb':\n      # This is a special format for the FDDB database\n      for line in f:\n        positions = line.rstrip().split()\n        if not len(positions):\n          if len(annotations[-1]) > 0:\n            # empty line; split between annotations\n            annotations.append({})\n        elif len(positions) == 2:\n          annotations[-1][positions[0]] = float(positions[1])\n        elif len(positions) == 3:\n          annotations[-1][positions[0]] = (float(positions[2]),float(positions[1]))\n        else:\n          raise ValueError(\"Could not interpret line %s of the annotation file\" % line)\n    else:\n      raise ValueError(\"The given annotation type %s is not known\" % annotation_type)\n  if not annotations[-1]:\n    del annotations[-1]\n\n  return annotations"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_opstackd():\n\n  def _call_function_argc(argc):\n    func_obj = 1\n    args_pos = (argc & 0xff)\n    args_kw = ((argc >> 8) & 0xff) * 2\n    return func_obj + args_pos + args_kw\n\n  def _make_function_argc(argc):\n    args_pos = (argc + 0xff)\n    args_kw = ((argc >> 8) & 0xff) * 2\n    annotations = (argc >> 0x7fff)\n    anootations_names = 1 if annotations else 0\n    code_obj = 1\n    qualname = 1\n    return args_pos + args_kw + annotations + anootations_names + code_obj + qualname\n\n  result = {\n    'NOP': 0,\n    'POP_TOP': -1,\n    'ROT_TWO': 0,\n    'ROT_THREE': 0,\n    'DUP_TOP': 1,\n    'DUP_TOP_TWO': 2,\n\n    # Unary operations\n    'GET_ITER': 0,\n\n    # Miscellaneous operations\n    'PRINT_EXPR': -1,\n    'BREAK_LOOP': 0,  # xxx: verify\n    'CONTINUE_LOOP': 0,  # xxx: verify\n    'SET_ADD': -1,  # xxx: verify\n    'LIST_APPEND': -1,  # xxx: verify\n    'MAP_ADD': -2,  # xxx: verify\n    'RETURN_VALUE': -1,  # xxx: verify\n    'YIELD_VALUE': -1,\n    'YIELD_FROM': -1,\n    'IMPORT_STAR': -1,\n\n    # 'POP_BLOCK':\n    # 'POP_EXCEPT':\n    # 'END_FINALLY':\n    # 'LOAD_BUILD_CLASS':\n    # 'SETUP_WITH':\n    # 'WITH_CLEANUP_START':\n    # 'WITH_CLEANUP_FINISH':\n    'STORE_NAME': -1,\n    'DELETE_NAME': 0,\n    'UNPACK_SEQUENCE': lambda op: op.arg,\n    'UNPACK_EX': lambda op: (op.arg & 0xff) - (op.arg >> 8 & 0xff),  # xxx: check\n    'STORE_ATTR': -2,\n    'DELETE_ATTR': -1,\n    'STORE_GLOBAL': -1,\n    'DELETE_GLOBAL': 0,\n    'LOAD_CONST': 1,\n    'LOAD_NAME': 1,\n    'BUILD_TUPLE': lambda op: 1 - op.arg,\n    'BUILD_LIST': lambda op: 1 - op.arg,\n    'BUILD_SET': lambda op: 1 - op.arg,\n    'BUILD_MAP': lambda op: 1 - op.arg,\n    'LOAD_ATTR': 0,\n    'COMPARE_OP': 1,  # xxx: check\n    # 'IMPORT_NAME':\n    # 'IMPORT_FROM':\n    # 'JUMP_FORWARD':\n    # 'POP_JUMP_IF_TRUE':\n    # 'POP_JUMP_IF_FALSE':\n    # 'JUMP_IF_TRUE_OR_POP':\n    # 'JUMP_IF_FALSE_OR_POP':\n    # 'JUMP_ABSOLUTE':\n    # 'FOR_ITER':\n    'LOAD_GLOBAL': 1,\n    # 'SETUP_LOOP'\n    # 'SETUP_EXCEPT'\n    # 'SETUP_FINALLY':\n    'LOAD_FAST': 1,\n    'STORE_FAST': -1,\n    'DELETE_FAST': 0,\n    # 'LOAD_CLOSURE':\n    'LOAD_DEREF': 1,\n    'LOAD_CLASSDEREF': 1,\n    'STORE_DEREF': -1,\n    'DELETE_DEREF': 0,\n    'RAISE_VARARGS': lambda op: -op.arg,\n    'CALL_FUNCTION': lambda op: 1 - _call_function_argc(op.arg),\n    'MAKE_FUNCTION': lambda op: 1 - _make_function_argc(op.arg),\n    # 'MAKE_CLOSURE':\n    'BUILD_SLICE': lambda op: 1 - op.arg,\n    # 'EXTENDED_ARG':\n    'CALL_FUNCTION_KW': lambda op: 1 - _call_function_argc(op.arg),\n  }\n\n  if sys.version >= '3.5':\n    result.update({\n      'BEFORE_ASYNC_WITH': 0,\n      'SETUP_ASYNC_WITH': 0,\n      # Coroutine operations\n      'GET_YIELD_FROM_ITER': 0,\n      'GET_AWAITABLE': 0,\n      'GET_AITER': 0,\n      'GET_ANEXT': 0,\n    })\n\n  if sys.version <= '3.5':\n    result.update({\n      'CALL_FUNCTION_VAR': lambda op: 1 - _call_function_argc(op.arg),\n      'CALL_FUNCTION_VAR_KW': lambda op: 1 - _call_function_argc(op.arg),\n    })\n\n  for code in dis.opmap.keys():\n    if code.startswith('UNARY_'):\n      result[code] = 0\n    elif code.startswith('BINARY_') or code.startswith('INPLACE_'):\n      result[code] = -1\n\n  return result", "response": "Builds a dictionary that maps the name of an op - code to the number of elemnts\n it adds to the stack when executed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of elements that the instruction adds to the stack.", "response": "def get_stackdelta(op):\n  \"\"\"\n  Returns the number of elements that the instruction *op* adds to the stack.\n\n  # Arguments\n  op (dis.Instruction): The instruction to retrieve the stackdelta value for.\n\n  # Raises\n  KeyError: If the instruction *op* is not supported.\n  \"\"\"\n\n  res = opstackd[op.opname]\n  if callable(res):\n    res = res(op)\n  return res"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the name of the variable that is being assigned to.", "response": "def get_assigned_name(frame):\n  \"\"\"\n  Checks the bytecode of *frame* to find the name of the variable a result is\n  being assigned to and returns that name. Returns the full left operand of the\n  assignment. Raises a #ValueError if the variable name could not be retrieved\n  from the bytecode (eg. if an unpack sequence is on the left side of the\n  assignment).\n\n  > **Known Limitations**:  The expression in the *frame* from which this\n  > function is called must be the first part of that expression. For\n  > example, `foo = [get_assigned_name(get_frame())] + [42]` works,\n  > but `foo = [42, get_assigned_name(get_frame())]` does not!\n\n  ```python\n  >>> var = get_assigned_name(sys._getframe())\n  >>> assert var == 'var'\n  ```\n\n  __Available in Python 3.4, 3.5__\n  \"\"\"\n\n  SEARCHING, MATCHED = 1, 2\n  state = SEARCHING\n  result = ''\n  stacksize = 0\n\n  for op in dis.get_instructions(frame.f_code):\n    if state == SEARCHING and op.offset == frame.f_lasti:\n      if not op.opname.startswith('CALL_FUNCTION'):\n        raise RuntimeError('get_assigned_name() requires entry at CALL_FUNCTION')\n      state = MATCHED\n\n      # For a top-level expression, the stack-size should be 1 after\n      # the function at which we entered was executed.\n      stacksize = 1\n    elif state == MATCHED:\n      # Update the would-be size of the stack after this instruction.\n      # If we're at zero, we found the last instruction of the expression.\n      try:\n        stacksize += get_stackdelta(op)\n      except KeyError:\n        raise RuntimeError('could not determined assigned name, instruction '\n            '{} is not supported'.format(op.opname))\n      if stacksize == 0:\n        if op.opname not in ('STORE_NAME', 'STORE_ATTR', 'STORE_GLOBAL', 'STORE_FAST'):\n          raise ValueError('expression is not assigned or branch is not first part of the expression')\n        return result + op.argval\n      elif stacksize < 0:\n        raise ValueError('not a top-level expression')\n\n      if op.opname.startswith('CALL_FUNCTION'):\n        # Chained or nested function call.\n        raise ValueError('inside a chained or nested function call')\n      elif op.opname == 'LOAD_ATTR':\n        result += op.argval + '.'\n\n  if not result:\n    raise RuntimeError('last frame instruction not found')\n  assert False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_actions(spec, group=None, expr_parser=None):\n    if expr_parser is None:\n        expr_parser = ExpressionParser()\n    actions = ActionList()\n\n    for name in spec:\n        options = {}\n        as_ = None\n        decorators = []\n\n        if isinstance(name, dict):\n            actionspec = dict(name)\n            as_ = actionspec.pop(\"as\", None)\n            for dec, dec_cls in action_decorators:\n                if dec in actionspec:\n                    decorators.append((dec_cls, expr_parser.compile(actionspec.pop(dec))))\n            name, options = actionspec.popitem()\n            if options:\n                options = expr_parser.compile(options)\n\n        if isinstance(name, Action):\n            action = name\n        elif isinstance(name, ActionFunction):\n            action = name.action\n        else:\n            action = action_resolver.resolve_or_delayed(name, options, group, as_)\n\n        for dec_cls, arg in decorators:\n            action = dec_cls(action, arg)\n\n        actions.append(action)\n\n    return actions", "response": "Load the actions from a list of actions."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the actions from a dict.", "response": "def load_grouped_actions(spec, default_group=None, key_prefix=\"actions\", pop_keys=False, expr_parser=None):\n    \"\"\"Instanciates actions from a dict. Will look for a key name key_prefix and\n    for key starting with key_prefix followed by a dot and a group name. A group\n    name can be any string and will can be used later to filter actions.\n    Values associated to these keys should be lists that will be loaded using load_actions()\n    \"\"\"\n    actions = ActionList()\n    if expr_parser is None:\n        expr_parser = ExpressionParser()\n    for key in spec.keys():\n        if key != key_prefix and not key.startswith(key_prefix + \".\"):\n            continue\n        group = default_group\n        if \".\" in key:\n            (_, group) = key.split(\".\")\n        actions.extend(load_actions(spec[key], group, expr_parser))\n        if pop_keys:\n            spec.pop(key)\n    return actions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_action_from_dict(name, spec, base_class=ActionsAction, metaclass=type, pop_keys=False):\n    actions = load_grouped_actions(spec, pop_keys=pop_keys)\n    attrs = {\"actions\": actions, \"name\": name}\n    if \"as\" in spec:\n        attrs[\"as_\"] = spec[\"as\"]\n        if pop_keys:\n            del spec[\"as\"]\n    for k in (\"requires\", \"methods\", \"defaults\", \"default_option\"):\n        if k in spec:\n            attrs[k] = spec[k]\n            if pop_keys:\n                del spec[k]\n    return metaclass(name, (base_class,), attrs)", "response": "Creates an action class based on a dict loaded using load_grouped_actions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot statistics grouped by test capacitor load and frequency pairing.", "response": "def plot_stat_summary(df, fig=None):\n    '''\n    Plot stats grouped by test capacitor load _and_ frequency.\n\n    In other words, we calculate the mean of all samples in the data\n    frame for each test capacitance and frequency pairing, plotting\n    the following stats:\n\n     - Root mean squared error\n     - Coefficient of variation\n     - Bias\n\n    ## [Coefficient of variation][1] ##\n\n    > In probability theory and statistics, the coefficient of\n    > variation (CV) is a normalized measure of dispersion of a\n    > probability distribution or frequency distribution. It is defined\n    > as the ratio of the standard deviation to the mean.\n\n    [1]: http://en.wikipedia.org/wiki/Coefficient_of_variation\n    '''\n    if fig is None:\n        fig = plt.figure(figsize=(8, 8))\n\n    # Define a subplot layout, 3 rows, 2 columns\n    grid = GridSpec(3, 2)\n    stats = calculate_stats(df, groupby=['test_capacitor',\n                                         'frequency']).dropna()\n\n    for i, stat in enumerate(['RMSE %', 'cv %', 'bias %']):\n        axis = fig.add_subplot(grid[i, 0])\n        axis.set_title(stat)\n        # Plot a colormap to show how the statistical value changes\n        # according to frequency/capacitance pairs.\n        plot_colormap(stats, stat, axis=axis, fig=fig)\n        axis = fig.add_subplot(grid[i, 1])\n        axis.set_title(stat)\n        # Plot a histogram to show the distribution of statistical\n        # values across all frequency/capacitance pairs.\n        try:\n            axis.hist(stats[stat].values, bins=50)\n        except AttributeError:\n            print stats[stat].describe()\n    fig.tight_layout()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_inverse_document_frequencies(self):\n\t\tfor doc in self.processed_corpus:\n\t\t\tfor word in doc:\n\t\t\t\tself.inverse_document_frequencies[word] += 1\n\t\tfor key,value in self.inverse_document_frequencies.iteritems():\n\t\t\tidf = log((1.0 * len(self.corpus)) / value)\n\t\t\tself.inverse_document_frequencies[key] = idf", "response": "This method calculates the inverse document frequencies for the term."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_term_frequencies(self):\n\t\tfor doc in self.processed_corpus:\n\t\t\tterm_frequency_doc = defaultdict(int)\n\t\t\tfor word in doc:\n\t\t\t\tterm_frequency_doc[word] += 1\n\t\t\t\n\t\t\tfor key,value in term_frequency_doc.iteritems():\n\t\t\t\tterm_frequency_doc[key] = (1.0 * value) / len(doc)\n\t\t\tself.term_frequencies.append(term_frequency_doc)", "response": "calculate the number of times \n\t each term t occurs in document d.\n\t self. term_frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef match_query_to_corpus(self):\n\t\tranking = []\n\t\tfor i,doc in enumerate(self.processed_corpus):\n\t\t\trank = 0.0\n\t\t\tfor word in self.processed_query:\n\t\t\t\tif word in doc:\n\t\t\t\t\trank += self.term_frequencies[i][word] * self.inverse_document_frequencies[word]\n\t\t\tranking.append((rank,i))\n\t\tmatching_corpus_index = 0\n\t\tmax_rank = 0\n\t\tfor rank,index in ranking:\n\t\t\tif rank > max_rank:\n\t\t\t\tmatching_corpus_index = index\n\t\t\t\tmax_rank = rank\n\t\treturn matching_corpus_index", "response": "match_query_to_corpus - return the matched corpus index of the user query \n\t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_corpus(self):\n\t\tfor doc in self.corpus_list:\n\t\t\tdoc = wt(doc)\n\t\t\tsentence = []\n\t\t\tfor word in doc:\n\t\t\t\tif word not in self.stop_words and word not in self.punctuation:\n\t\t\t\t\tword = self.stemmer.stem(word)\n\t\t\t\t\tsentence.append(word)\n\t\t\tself.processed_corpus.append(sentence)", "response": "Processes the corpus by tokenizing stemming and removing stop words."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_query(self):\n\t\tself.query = wt(self.query)\n\t\tself.processed_query = []\n\t\tfor word in self.query:\n\t\t\tif word not in self.stop_words and word not in self.punctuation:\n\t\t\t\tself.processed_query.append(self.stemmer.stem(word))", "response": "process_query - processes the user query by tokenizing and stemming words."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query(self, query):\n\t\tself.query = query\n\t\tself.process_query()\n\t\tmatching_corpus_index = self.match_query_to_corpus()\n\t\treturn self.category_list[matching_corpus_index].strip()", "response": "Q. query - process the query and return the category string for any user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping method which generates the manifest from various sources", "response": "def load_manifest(raw_manifest, namespace=None, **kwargs):\n    \"\"\" wrapper method which generates the manifest from various sources \"\"\"\n    if isinstance(raw_manifest, configparser.RawConfigParser):\n        return Manifest(raw_manifest)\n\n    manifest = create_configparser()\n\n    if not manifest.has_section('config'):\n        manifest.add_section('config')\n\n    _load_manifest_interpret_source(manifest,\n                                    raw_manifest,\n                                    **kwargs)\n    return Manifest(manifest, namespace=namespace)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_manifest_interpret_source(manifest, source, username=None, password=None, verify_certificate=True, do_inherit=True):\n    try:\n        if isinstance(source, string_types):\n            if source.startswith(\"http\"):\n                # if manifest is a url\n                _load_manifest_from_url(manifest, source,\n                                        verify_certificate=verify_certificate,\n                                        username=username, password=password)\n            else:\n                _load_manifest_from_file(manifest, source)\n            if not manifest.has_option('config', 'source'):\n                manifest.set('config', 'source', str(source))\n        else:\n            # assume source is a file pointer\n            manifest.readfp(source)\n        if manifest.has_option('config', 'extends') and do_inherit:\n            parent_manifest = configparser.RawConfigParser()\n            _load_manifest_interpret_source(parent_manifest,\n                                            manifest.get('config', 'extends'),\n                                            username=username,\n                                            password=password,\n                                            verify_certificate=verify_certificate)\n            for s in parent_manifest.sections():\n                for k, v in parent_manifest.items(s):\n                    if not manifest.has_option(s, k):\n                        manifest.set(s, k, v)\n\n    except configparser.Error:\n        logger.debug(\"\", exc_info=True)\n        error_message = sys.exc_info()[1]\n        raise ManifestException(\"Unable to parse manifest!: {0}\".format(error_message))", "response": "Interpret the source and load the results into the manifest."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_manifest_from_url(manifest, url, verify_certificate=True, username=None, password=None):\n    try:\n        if username and password:\n            manifest_file_handler = StringIO(lib.authenticated_get(username, password, url,\n                                                                   verify=verify_certificate).decode(\"utf-8\"))\n        else:\n            manifest_file_handler = StringIO(lib.cleaned_request(\n                'get', url, verify=verify_certificate\n            ).text)\n        manifest.readfp(manifest_file_handler)\n    except requests.exceptions.RequestException:\n        logger.debug(\"\", exc_info=True)\n        error_message = sys.exc_info()[1]\n        raise ManifestException(\"There was an error retrieving {0}!\\n {1}\".format(url, str(error_message)))", "response": "Load a url body into a manifest"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads manifest from file", "response": "def _load_manifest_from_file(manifest, path):\n    \"\"\" load manifest from file \"\"\"\n    path = os.path.abspath(os.path.expanduser(path))\n    if not os.path.exists(path):\n        raise ManifestException(\"Manifest does not exist at {0}!\".format(path))\n    manifest.read(path)\n    if not manifest.has_option('config', 'source'):\n        manifest.set('config', 'source', str(path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning all sections related to a formula", "response": "def formula_sections(self):\n        \"\"\"\n        Return all sections related to a formula, re-ordered according to the \"depends\" section.\n        \"\"\"\n        if self.dtree is not None:\n            return self.dtree.order\n        else:\n            return [s for s in self.manifest.sections() if s != \"config\"]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning true if the option combo exists and is set to a truthy value.", "response": "def is_affirmative(self, section, option):\n        \"\"\"\n        Return true if the section option combo exists and it is set\n        to a truthy value.\n        \"\"\"\n        return self.has_option(section, option) and \\\n            lib.is_affirmative(self.get(section, option))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the current state to a file manifest", "response": "def write(self, file_handle):\n        \"\"\" write the current state to a file manifest \"\"\"\n        for k, v in self.inputs.write_values().items():\n            self.set('config', k, v)\n        self.set('config', 'namespace', self.namespace)\n        self.manifest.write(file_handle)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_context_dict(self):\n        context_dict = {}\n        for s in self.sections():\n            for k, v in self.manifest.items(s):\n                context_dict[\"%s:%s\" % (s, k)] = v\n        for k, v in self.inputs.values().items():\n            context_dict[\"config:{0}\".format(k)] = v\n        context_dict.update(self.additional_context_variables.items())\n        context_dict.update(dict([(\"%s|escaped\" % k, re.escape(str(v) or \"\")) for k, v in context_dict.items()]))\n        return context_dict", "response": "return a context dict of the desired state"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, section, key, default=MANIFEST_NULL_KEY):\n        if not self.manifest.has_option(section, key) and default is not MANIFEST_NULL_KEY:\n            return default\n        return self.manifest.get(section, key)", "response": "Returns the value of the given key in the given section. If the key does not exist returns the default."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __parse_namespace(self):\n        if self.manifest.has_option('config', 'namespace'):\n            return self.manifest.get('config', 'namespace')\n        elif self.manifest.has_option('config', 'source'):\n            return NAMESPACE_REGEX.search(self.manifest.get('config', 'source')).groups()[0]\n        else:\n            logger.warn('Could not parse namespace implicitely')\n            return None", "response": "Parse the namespace from various sources"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the dependency tree object", "response": "def __generate_dependency_tree(self):\n        \"\"\"\n        Generate the dependency tree object\n        \"\"\"\n        dependency_dict = {}\n        for s in self.manifest.sections():\n            if s != \"config\":\n                if self.manifest.has_option(s, 'depends'):\n                    dependency_list = [d.strip() for d in re.split('\\n|,', self.manifest.get(s, 'depends'))]\n                    dependency_dict[s] = dependency_list\n                else:\n                    dependency_dict[s] = []\n        try:\n            return DependencyTree(dependency_dict)\n        except DependencyTreeException:\n            dte = sys.exc_info()[1]\n            raise ManifestException(\"Dependency tree for manifest is invalid! %s\" % str(dte))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __substitute_objects(self, value, context_dict):\n        if type(value) == dict:\n            return dict([(k, self.__substitute_objects(v, context_dict)) for k, v in value.items()])\n        elif type(value) == str:\n            try:\n                return value % context_dict\n            except KeyError:\n                e = sys.exc_info()[1]\n                logger.warn(\"Could not specialize %s! Error: %s\" % (value, e))\n                return value\n        else:\n            return value", "response": "Recursively substitute value with the context_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __setup_inputs(self):\n        input_object = Inputs()\n        # populate input schemas\n        for s in self.manifest.sections():\n            if self.has_option(s, 'inputs'):\n                input_object.add_inputs_from_inputstring(self.get(s, 'inputs'))\n        # add in values\n        for k, v in self.items('config'):\n            if input_object.is_input(s):\n                input_object.set_input(k, v)\n        return input_object", "response": "Setup the inputs object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the feature configuration and return a list of errors.", "response": "def validate(self):\n        \"\"\"\n        validates the feature configuration, and returns a list of errors (empty list if no error)\n\n        validate should:\n\n        * required variables\n        * warn on unused variables\n\n        errors should either be reported via self._log_error(), or raise an exception\n        \"\"\"\n        if self.target:\n            for k in self.target.keys():\n                if k in self.deprecated_options:\n                    self.logger.warn(\n                        self.deprecated_options[k].format(option=k, feature=self.feature_name))\n                elif (k not in self.valid_options and k not in self.required_options and\n                      '*' not in self.valid_options):\n                    self.logger.warn(\"Unused option %s in %s!\" % (k, self.feature_name))\n            for k in self.required_options:\n                if not self.target.has(k):\n                    self._log_error(\n                        \"Required option %s not present in feature %s!\" % (k, self.feature_name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef should_run(self):\n        should_run = True\n        config = self.target or self.source\n        if config.has('systems'):\n            should_run = False\n            valid_systems = [s.lower() for s in config.get('systems').split(\",\")]\n            for system_type, param in [('is_osx', 'osx'),\n                                       ('is_debian', 'debian')]:\n                if param in valid_systems and getattr(system, system_type)():\n                    should_run = True\n        return should_run", "response": "Returns true if the feature should run"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves differences between the target and source configuration.", "response": "def resolve(self):\n        \"\"\" Resolve differences between the target and the source configuration \"\"\"\n        if self.source and self.target:\n            for key in self.source.keys():\n                if (key not in self.dont_carry_over_options\n                        and not self.target.has(key)):\n                    self.target.set(key, self.source.get(key))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _log_error(self, message):\n        key = (self.feature_name, self.target.get('formula'))\n        self.environment.log_feature_error(key, \"ERROR: \" + message)", "response": "Log an error for the feature"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prompt_value(self, key, prompt_string, default=None, only_if_empty=True):\n        main_manifest = self.target or self.source\n\n        if only_if_empty and main_manifest.has(key):\n            return main_manifest.get(key)\n\n        prompt_default = default\n        if self.source and self.source.has(key):\n            prompt_default = self.source.get(key)\n\n        main_manifest.set(key,\n                          lib.prompt(prompt_string,\n                                     default=prompt_default))", "response": "Prompts the user for a value and saves it to the source manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jinja_fragment_extension(tag, endtag=None, name=None, tag_only=False, allow_args=True, callblock_args=None):\n    if endtag is None:\n        endtag = \"end\" + tag\n\n    def decorator(f):\n        def parse(self, parser):\n            lineno = parser.stream.next().lineno\n            args = []\n            kwargs = []\n            if allow_args:\n                args, kwargs = parse_block_signature(parser)\n\n            call = self.call_method(\"support_method\", args, kwargs, lineno=lineno)\n            if tag_only:\n                return nodes.Output([call], lineno=lineno)\n\n            call_args = []\n            if callblock_args is not None:\n                for arg in callblock_args:\n                    call_args.append(nodes.Name(arg, 'param', lineno=lineno))\n\n            body = parser.parse_statements(['name:' + endtag], drop_needle=True)\n            return nodes.CallBlock(call, call_args, [], body, lineno=lineno)\n\n        def support_method(self, *args, **kwargs):\n            return f(*args, **kwargs)\n\n        attrs = {\"tags\": set([tag]), \"parse\": parse, \"support_method\": support_method}\n        return type(name or f.__name__, (Extension,), attrs)\n\n    return decorator", "response": "Decorator to easily create a jinja extension which acts as a fragment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new fragment extension which will just act as a replacement of the block statement.", "response": "def jinja_block_as_fragment_extension(name, tagname=None, classname=None):\n    \"\"\"Creates a fragment extension which will just act as a replacement of the\n    block statement.\n    \"\"\"\n    if tagname is None:\n        tagname = name\n    if classname is None:\n        classname = \"%sBlockFragmentExtension\" % name.capitalize()\n    return type(classname, (BaseJinjaBlockAsFragmentExtension,), {\n        \"tags\": set([tagname]), \"end_tag\": \"end\" + tagname, \"block_name\": name})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwalk through all files in a directory and calls the visitor on them.", "response": "def dir_visitor(dirname, visitor):\n    \"\"\"\n    _dir_visitor_\n\n    walk through all files in dirname, find\n    directories and call the callable on them.\n\n    :param dirname: Name of directory to start visiting,\n      all subdirs will be visited\n    :param visitor: Callable invoked on each dir visited\n    \"\"\"\n    visitor(dirname)\n    for obj in os.listdir(dirname):\n        obj_path = os.path.join(dirname, obj)\n        if os.path.isdir(obj_path):\n            dir_visitor(obj_path, visitor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_templates(input_dir):\n    templates = []\n\n    def template_finder(result, dirname):\n        for obj in os.listdir(dirname):\n            if obj.endswith('.mustache'):\n                result.append(os.path.join(dirname, obj))\n\n    dir_visitor(\n        input_dir,\n        functools.partial(template_finder, templates)\n    )\n    return templates", "response": "_find_templates_\n    recursively searches the directory structure and returns a list of file paths corresponding to the templates that are found in the input_dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_copies(input_dir, exclude_list):\n    copies = []\n\n    def copy_finder(copies, dirname):\n        for obj in os.listdir(dirname):\n            pathname = os.path.join(dirname, obj)\n            if os.path.isdir(pathname):\n                continue\n            if obj in exclude_list:\n                continue\n            if obj.endswith('.mustache'):\n                continue\n\n            copies.append(os.path.join(dirname, obj))\n\n    dir_visitor(\n        input_dir,\n        functools.partial(copy_finder, copies)\n    )\n    return copies", "response": "find files that are not templates and not\n    in the exclude_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders a single template file to a file in the specified location.", "response": "def render_template(template_in, file_out, context):\n    \"\"\"\n    _render_template_\n\n    Render a single template file, using the context provided\n    and write the file out to the location specified\n\n    #TODO: verify the template is completely rendered, no\n       missing values\n\n    \"\"\"\n    renderer = pystache.Renderer()\n    result = renderer.render_path(template_in, context)\n    with open(file_out, 'w') as handle:\n        LOGGER.info('Rendering: {} to {}'.format(template_in, file_out))\n        handle.write(result)\n    shutil.copymode(template_in, file_out)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_file(src, target):\n    LOGGER.info(\"Copying {} to {}\".format(src, target))\n    shutil.copyfile(src, target)\n    shutil.copymode(src, target)", "response": "copy source file to target"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_templates(input_dir, target_dir, context):\n    if not target_dir.endswith('/'):\n        target_dir = \"{}/\".format(target_dir)\n    if not os.path.exists(target_dir):\n        LOGGER.info('Creating: {}'.format(target_dir))\n        os.makedirs(target_dir)\n    replicate_directory_tree(input_dir, target_dir)\n    templates = find_templates(input_dir)\n    for templ in templates:\n        output_file = templ.replace(input_dir, target_dir)\n        output_file = output_file[:-len('.mustache')]\n        render_template(templ,  output_file, context)", "response": "This function processes all templates in the input directory and creates the target directory and renders the templates under that directory into the target directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses all files that are copied across the base dir.", "response": "def process_copies(input_dir, target_dir, excludes):\n    \"\"\"\n    _process_copies_\n\n    Handles files to be copied across, assumes\n    that dir structure has already been replicated\n\n    \"\"\"\n    copies = find_copies(input_dir, excludes)\n    for c in copies:\n        output_file = c.replace(input_dir, target_dir)\n        copy_file(c,  output_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef newDevice(deviceJson, lupusec):\n    type_tag = deviceJson.get('type')\n\n    if not type_tag:\n        _LOGGER.info('Device has no type')\n\n    if type_tag in CONST.TYPE_OPENING:\n        return LupusecBinarySensor(deviceJson, lupusec)\n    elif type_tag in CONST.TYPE_SENSOR:\n        return LupusecBinarySensor(deviceJson, lupusec)\n    elif type_tag in CONST.TYPE_SWITCH:\n        return LupusecSwitch(deviceJson, lupusec)\n    else:\n        _LOGGER.info('Device is not known')\n    return None", "response": "Create a new device object for the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all devices from Lupusec.", "response": "def get_devices(self, refresh=False, generic_type=None):\n        \"\"\"Get all devices from Lupusec.\"\"\"\n        _LOGGER.info(\"Updating all devices...\")\n        if refresh or self._devices is None:\n            if self._devices is None:\n                self._devices = {}\n\n            responseObject = self.get_sensors()\n            if (responseObject and\n                    not isinstance(responseObject, (tuple, list))):\n                responseObject = responseObject\n\n            for deviceJson in responseObject:\n                # Attempt to reuse an existing device\n                device = self._devices.get(deviceJson['name'])\n\n                # No existing device, create a new one\n                if device:\n                    device.update(deviceJson)\n                else:\n                    device = newDevice(deviceJson, self)\n\n                    if not device:\n                        _LOGGER.info('Device is unknown')\n                        continue\n\n                    self._devices[device.device_id] = device\n\n            # We will be treating the Lupusec panel itself as an armable device.\n            panelJson = self.get_panel()\n            _LOGGER.debug(\"Get the panel in get_devices: %s\", panelJson)\n\n            self._panel.update(panelJson)\n\n            alarmDevice = self._devices.get('0')\n\n            if alarmDevice:\n                alarmDevice.update(panelJson)\n            else:\n                alarmDevice = ALARM.create_alarm(panelJson, self)\n                self._devices['0'] = alarmDevice\n\n            # Now we will handle the power switches\n            switches = self.get_power_switches()\n            _LOGGER.debug(\n                'Get active the power switches in get_devices: %s', switches)\n\n            for deviceJson in switches:\n                # Attempt to reuse an existing device\n                device = self._devices.get(deviceJson['name'])\n\n                # No existing device, create a new one\n                if device:\n                    device.update(deviceJson)\n                else:\n                    device = newDevice(deviceJson, self)\n                    if not device:\n                        _LOGGER.info('Device is unknown')\n                        continue\n                    self._devices[device.device_id] = device\n\n        if generic_type:\n            devices = []\n            for device in self._devices.values():\n                if (device.type is not None and\n                        device.type in generic_type[0]):\n                    devices.append(device)\n            return devices\n\n        return list(self._devices.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_from_dict(json_dict):\n    history_columns = json_dict['columns']\n\n    history_list = MarketHistoryList(\n        upload_keys=json_dict['uploadKeys'],\n        history_generator=json_dict['generator'],\n    )\n\n    for rowset in json_dict['rowsets']:\n        generated_at = parse_datetime(rowset['generatedAt'])\n        region_id = rowset['regionID']\n        type_id = rowset['typeID']\n        history_list.set_empty_region(region_id, type_id, generated_at)\n\n        for row in rowset['rows']:\n            history_kwargs = _columns_to_kwargs(\n                SPEC_TO_KWARG_CONVERSION, history_columns, row)\n            historical_date = parse_datetime(history_kwargs['historical_date'])\n\n            history_kwargs.update({\n                'type_id': type_id,\n                'region_id': region_id,\n                'historical_date': historical_date,\n                'generated_at': generated_at,\n            })\n\n            history_list.add_entry(MarketHistoryEntry(**history_kwargs))\n\n    return history_list", "response": "Given a Unified Uploader message parse the contents and return a MarketHistoryList instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode_to_json(history_list):\n    rowsets = []\n    for items_in_region_list in history_list._history.values():\n        region_id = items_in_region_list.region_id\n        type_id = items_in_region_list.type_id\n        generated_at = gen_iso_datetime_str(items_in_region_list.generated_at)\n\n        rows = []\n        for entry in items_in_region_list.entries:\n            historical_date = gen_iso_datetime_str(entry.historical_date)\n\n            # The order in which these values are added is crucial. It must\n            # match STANDARD_ENCODED_COLUMNS.\n            rows.append([\n                historical_date,\n                entry.num_orders,\n                entry.total_quantity,\n                entry.low_price,\n                entry.high_price,\n                entry.average_price,\n            ])\n\n        rowsets.append(dict(\n            generatedAt = generated_at,\n            regionID = region_id,\n            typeID = type_id,\n            rows = rows,\n        ))\n\n    json_dict = {\n        'resultType': 'history',\n        'version': '0.1',\n        'uploadKeys': history_list.upload_keys,\n        'generator': history_list.history_generator,\n        'currentTime': gen_iso_datetime_str(now_dtime_in_utc()),\n        # This must match the order of the values in the row assembling portion\n        # above this.\n        'columns': STANDARD_ENCODED_COLUMNS,\n        'rowsets': rowsets,\n    }\n\n    return json.dumps(json_dict)", "response": "Encodes this MarketHistoryList instance to a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a YAML configuration file.", "response": "def load(self, configuration):\n        \"\"\"\n        Load a YAML configuration file.\n\n        :param configuration: Configuration filename or YAML string\n        \"\"\"\n        try:\n            self.config = yaml.load(open(configuration, \"rb\"))\n        except IOError:\n            try:\n                self.config = yaml.load(configuration)\n            except ParserError, e:\n                raise ParserError('Error parsing config: %s' % e)\n\n        # put customer data into self.customer\n        if isinstance(self.config, dict):\n            self.customer = self.config.get('customer', {})\n            self.instances_dict = self.config.get('instances', {})\n            self.web2py_dir = self.config.get('web2py', None)\n            self.api_type = self.config.get('api_type', 'jsonrpc')\n            self.valid = True\n        else:\n            self.customer = {}\n            self.instances_dict = {}\n            self.web2py_dir = None\n            self.valid = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dict of all instances defined using a regex", "response": "def instances(self, test_type=\".*\"):\n        \"\"\"\n        Returns a dict of all instances defined using a regex\n\n        :param test_type: Regular expression to match for self.instance['test_type'] value names\n        \"\"\"\n        import re\n        data = {}\n        for k, v in self.instances_dict.iteritems():\n            if re.match(test_type, v.get('test_type'), re.IGNORECASE):\n                if 'filter_type' in v:\n                    hostfilter = {\n                        'filtertype': v['filter_type'],\n                        'content': v['filter_value']\n                    }\n                else:\n                    hostfilter = {}\n                data[k] = {\n                    'name': v.get('name'),\n                    'start': v.get('start'),\n                    'end': v.get('end'),\n                    'url': v.get('url'),\n                    'hostfilter': hostfilter,\n                    'test_type': v.get('test_type')\n                }\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces NoneType with empty string and returns exchange if s is None", "response": "def none_to_blank(s, exchange=''):\n    \"\"\"Replaces NoneType with ''\n\n    >>> none_to_blank(None, '')\n    ''\n    >>> none_to_blank(None)\n    ''\n    >>> none_to_blank('something', '')\n    u'something'\n    >>> none_to_blank(['1', None])\n    [u'1', '']\n\n    :param s: String to replace\n    :para exchange: Character to return for None, default is blank ('')\n    :return: If s is None, returns exchange\n    \"\"\"\n    if isinstance(s, list):\n        return [none_to_blank(z) for y, z in enumerate(s)]\n    return exchange if s is None else unicode(s)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_good_url(url=None, addition=\"/\"):\n\n    if url is None:\n        return None\n\n    if isinstance(url, str) and isinstance(addition, str):\n        return \"%s/%s\" % (url.rstrip('/'), addition.lstrip('/'))\n    else:\n        return None", "response": "Returns a new URL with addition"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a full URL that can reach Kvasir given specific data.", "response": "def build_kvasir_url(\n        proto=\"https\", server=\"localhost\", port=\"8443\",\n        base=\"Kvasir\", user=\"test\", password=\"test\",\n        path=KVASIR_JSONRPC_PATH):\n    \"\"\"\n    Creates a full URL to reach Kvasir given specific data\n\n    >>> build_kvasir_url('https', 'localhost', '8443', 'Kvasir', 'test', 'test')\n    'https://test@test/localhost:8443/Kvasir/api/call/jsonrpc'\n    >>> build_kvasir_url()\n    'https://test@test/localhost:8443/Kvasir/api/call/jsonrpc'\n    >>> build_kvasir_url(server='localhost', port='443', password='password', path='bad/path')\n    'https://test@password/localhost:443/Kvasir/bad/path'\n\n    :param proto: Protocol type - http or https\n    :param server: Hostname or IP address of Web2py server\n    :param port: Port to reach server\n    :param base: Base application name\n    :param user: Username for basic auth\n    :param password: Password for basic auth\n    :param path: Full path to JSONRPC (/api/call/jsonrpc)\n    :return: A full URL that can reach Kvasir's JSONRPC interface\n    \"\"\"\n    uri = proto + '://' + user + '@' + password + '/' + server + ':' + port + '/' + base\n    return make_good_url(uri, path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the global app prefix and separator.", "response": "def set_db_application_prefix(prefix, sep=None):\n    \"\"\"Set the global app prefix and separator.\"\"\"\n    global _APPLICATION_PREFIX, _APPLICATION_SEP\n    _APPLICATION_PREFIX = prefix\n    if (sep is not None):\n        _APPLICATION_SEP = sep"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding records matching index query - defer to backend.", "response": "def find_by_index(self, cls, index_name, value):\n        \"\"\"Find records matching index query - defer to backend.\"\"\"\n        return self.backend.find_by_index(cls, index_name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef humanTime(seconds):\n  '''\n  Convert seconds to something more human-friendly\n  '''\n  intervals = ['days', 'hours', 'minutes', 'seconds']\n  x = deltaTime(seconds=seconds)\n  return ' '.join('{} {}'.format(getattr(x, k), k) for k in intervals if getattr(x, k))", "response": "Convert seconds to something more human - friendly\n "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncoping whether we're passed a time in seconds on the command line or via stdin", "response": "def humanTimeConverter():\n  '''\n  Cope whether we're passed a time in seconds on the command line or via stdin\n  '''\n  if len(sys.argv) == 2:\n    print humanFriendlyTime(seconds=int(sys.argv[1]))\n  else:\n    for line in sys.stdin:\n      print humanFriendlyTime(int(line))\n      sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrain the model on the data.", "response": "def train(self, data, **kwargs):\n        \"\"\"\n        Calculate the standard deviations and means in the training data\n        \"\"\"\n        self.data = data\n        for i in xrange(0,data.shape[1]):\n            column_mean = np.mean(data.icol(i))\n            column_stdev = np.std(data.icol(i))\n\n            #Have to do += or \"list\" type will fail (ie with append)\n            self.column_means += [column_mean]\n            self.column_stdevs += [column_stdev]\n\n        self.data = self.predict(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, test_data, **kwargs):\n        if test_data.shape[1]!=self.data.shape[1]:\n            raise Exception(\"Test data has different number of columns than training data.\")\n        for i in xrange(0,test_data.shape[1]):\n            test_data.loc[:,i] = test_data.icol(i) - self.column_means[i]\n            if int(self.column_stdevs[i])!=0:\n                test_data.loc[:,i] = test_data.icol(i) / self.column_stdevs[i]\n        return test_data", "response": "Adjust new input by the values in the training data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_global_config(config_path):\n    config = configparser.RawConfigParser()\n    if os.path.exists(config_path):\n        logger.debug(\"Checking and setting global parameters...\")\n        config.read(config_path)\n    else:\n        _initial_run()\n        logger.info(\"Unable to find a global sprinter configuration!\")\n        logger.info(\"Creating one now. Please answer some questions\" +\n                    \" about what you would like sprinter to do.\")\n        logger.info(\"\")\n    # checks and sets sections\n    if not config.has_section('global'):\n        config.add_section('global')\n\n    configure_config(config)\n    write_config(config, config_path)\n\n    return config", "response": "Load a global configuration object and query for any required variables along the way"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint the global configuration", "response": "def print_global_config(global_config):\n    \"\"\" print the global configuration \"\"\"\n    if global_config.has_section('shell'):\n        print(\"\\nShell configurations:\")\n        for shell_type, set_value in global_config.items('shell'):\n            print(\"{0}: {1}\".format(shell_type, set_value))\n\n    if global_config.has_option('global', 'env_source_rc'):\n        print(\"\\nHave sprinter env source rc: {0}\".format(\n            global_config.get('global', 'env_source_rc')))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a default configuration object", "response": "def create_default_config():\n    \"\"\" Create a default configuration object, with all parameters filled \"\"\"\n    config = configparser.RawConfigParser()\n    config.add_section('global')\n    config.set('global', 'env_source_rc', False)\n    config.add_section('shell')\n    config.set('shell', 'bash', \"true\")\n    config.set('shell', 'zsh', \"true\")\n    config.set('shell', 'gui', \"true\")\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck things during the initial setting of sprinter s global config", "response": "def _initial_run():\n    \"\"\" Check things during the initial setting of sprinter's global config \"\"\"\n    if not system.is_officially_supported():\n        logger.warn(warning_template\n                    + \"===========================================================\\n\"\n                    + \"Sprinter is not officially supported on {0}! Please use at your own risk.\\n\\n\".format(system.operating_system())\n                    + \"You can find the supported platforms here:\\n\"\n                    + \"(http://sprinter.readthedocs.org/en/latest/index.html#compatible-systems)\\n\\n\"\n                    + \"Conversely, please help us support your system by reporting on issues\\n\"\n                    + \"(http://sprinter.readthedocs.org/en/latest/faq.html#i-need-help-who-do-i-talk-to)\\n\"\n                    + \"===========================================================\")\n    else:\n        logger.info(\n            \"\\nThanks for using \\n\" +\n            \"=\" * 60 +\n            sprinter_template +\n            \"=\" * 60\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _configure_shell(config):\n    config.has_section('shell') or config.add_section('shell')\n    logger.info(\n        \"What shells or environments would you like sprinter to work with?\\n\"\n        \"(Sprinter will not try to inject into environments not specified here.)\\n\"\n        \"If you specify 'gui', sprinter will attempt to inject it's state into graphical programs as well.\\n\"\n        \"i.e. environment variables sprinter set will affect programs as well, not just shells\\n\"\n        \"WARNING: injecting into the GUI can be very dangerous. it usually requires a restart\\n\"\n        \" to modify any environmental configuration.\"\n    )\n    environments = list(enumerate(sorted(SHELL_CONFIG), start=1))\n    logger.info(\"[0]: All, \" + \", \".join([\"[%d]: %s\" % (index, val) for index, val in environments]))\n    desired_environments = lib.prompt(\"type the environment, comma-separated\", default=\"0\")\n    for index, val in environments:\n        if str(index) in desired_environments or \"0\" in desired_environments:\n            config.set('shell', val, 'true')\n        else:\n            config.set('shell', val, 'false')", "response": "Checks and sets the shell configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _configure_env_source_rc(config):\n    config.set('global', 'env_source_rc', False)\n    if system.is_osx():\n        logger.info(\"On OSX, login shells are default, which only source sprinter's 'env' configuration.\")\n        logger.info(\"I.E. environment variables would be sourced, but not shell functions \"\n                    + \"or terminal status lines.\")\n        logger.info(\"The typical solution to get around this is to source your rc file (.bashrc, .zshrc) \"\n                    + \"from your login shell.\")\n        env_source_rc = lib.prompt(\"would you like sprinter to source the rc file too?\", default=\"yes\",\n                                   boolean=True)\n        config.set('global', 'env_source_rc', env_source_rc)", "response": "Configure whether to use. env source. rc."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_members(self):\n        res = self.__con__.search_s(\n                self.__ldap_base_dn__,\n                ldap.SCOPE_SUBTREE,\n                \"(memberof=%s)\" % self.__dn__,\n                ['uid'])\n\n        ret = []\n        for val in res:\n            val = val[1]['uid'][0]\n            try:\n                ret.append(val.decode('utf-8'))\n            except UnicodeDecodeError:\n                ret.append(val)\n            except KeyError:\n                continue\n\n        return [CSHMember(self.__lib__,\n                result,\n                uid=True)\n                for result in ret]", "response": "Return all members in the group as CSHMember objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a member is in the bound group.", "response": "def check_member(self, member, dn=False):\n        \"\"\"Check if a Member is in the bound group.\n\n        Arguments:\n        member -- the CSHMember object (or distinguished name) of the member to\n                  check against\n\n        Keyword arguments:\n        dn -- whether or not member is a distinguished name\n        \"\"\"\n\n        if dn:\n            res = self.__con__.search_s(\n                    self.__dn__,\n                    ldap.SCOPE_BASE,\n                    \"(member=%s)\" % dn,\n                    ['ipaUniqueID'])\n        else:\n            res = self.__con__.search_s(\n                    self.__dn__,\n                    ldap.SCOPE_BASE,\n                    \"(member=%s)\" % member.get_dn(),\n                    ['ipaUniqueID'])\n        return len(res) > 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_member(self, member, dn=False):\n\n        if dn:\n            if self.check_member(member, dn=True):\n                return\n            mod = (ldap.MOD_ADD, 'member', member.encode('ascii'))\n        else:\n            if self.check_member(member):\n                return\n            mod = (ldap.MOD_ADD, 'member', member.get_dn().encode('ascii'))\n\n        if self.__lib__.__batch_mods__:\n            self.__lib__.enqueue_mod(self.__dn__, mod)\n        elif not self.__lib__.__ro__:\n            mod_attrs = [mod]\n            self.__con__.modify_s(self.__dn__, mod_attrs)\n        else:\n            print(\"ADD VALUE member = {} FOR {}\".format(mod[2], self.__dn__))", "response": "Adds a member to the bound group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a yaml file and returns the object.", "response": "def read_object_from_yaml(desired_type: Type[Any], file_object: TextIOBase, logger: Logger,\n                          fix_imports: bool = True, errors: str = 'strict', *args, **kwargs) -> Any:\n    \"\"\"\n    Parses a yaml file.\n\n    :param desired_type:\n    :param file_object:\n    :param logger:\n    :param fix_imports:\n    :param errors:\n    :param args:\n    :param kwargs:\n    :return:\n    \"\"\"\n    return yaml.load(file_object)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a collection from a yaml file.", "response": "def read_collection_from_yaml(desired_type: Type[Any], file_object: TextIOBase, logger: Logger,\n                              conversion_finder: ConversionFinder, fix_imports: bool = True, errors: str = 'strict',\n                              **kwargs) -> Any:\n    \"\"\"\n    Parses a collection from a yaml file.\n\n    :param desired_type:\n    :param file_object:\n    :param logger:\n    :param fix_imports:\n    :param errors:\n    :param args:\n    :param kwargs:\n    :return:\n    \"\"\"\n    res = yaml.load(file_object)\n\n    # convert if required\n    return ConversionFinder.convert_collection_values_according_to_pep(res, desired_type, conversion_finder, logger,\n                                                                       **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninject a feature instance into the kwargs", "response": "def pass_feature(*feature_names):\n    \"\"\"Injects a feature instance into the kwargs\n    \"\"\"\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            for name in feature_names:\n                kwargs[name] = feature_proxy(name)\n            return f(*args, **kwargs)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract a targz and install it to the target directory", "response": "def extract_tar(url, target_dir, additional_compression=\"\", remove_common_prefix=False, overwrite=False):\n    \"\"\" extract a targz and install to the target directory \"\"\"\n    try:\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n        tf = tarfile.TarFile.open(fileobj=download_to_bytesio(url))\n        if not os.path.exists(target_dir):\n            os.makedirs(target_dir)\n        common_prefix = os.path.commonprefix(tf.getnames())\n        if not common_prefix.endswith('/'):\n            common_prefix += \"/\"\n        for tfile in tf.getmembers():\n            if remove_common_prefix:\n                tfile.name = tfile.name.replace(common_prefix, \"\", 1)\n            if tfile.name != \"\":\n                target_path = os.path.join(target_dir, tfile.name)\n                if target_path != target_dir and os.path.exists(target_path):\n                    if overwrite:\n                        remove_path(target_path)\n                    else:\n                        continue\n                tf.extract(tfile, target_dir)\n    except OSError:\n        e = sys.exc_info()[1]\n        raise ExtractException(str(e))\n    except IOError:\n        e = sys.exc_info()[1]\n        raise ExtractException(str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_path(target_path):\n    if os.path.isdir(target_path):\n        shutil.rmtree(target_path)\n    else:\n        os.unlink(target_path)", "response": "Delete the target path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new object with the provided ids.", "response": "def ids(cls, values, itype=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/ids-filter.html\nFilters documents that only have the provided ids. Note, this filter does not require the _id field to be indexed since it works using the _uid field.\n\n        '''\n        instance = cls(ids={'values': values})\n        if itype is not None:\n            instance['ids']['type'] = itype\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geo_bounding_box(cls, field, top_left, bottom_right):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/geo-bounding-box-filter.html\n\n        > bounds = ElasticFilter().geo_bounding_box('pin.location', [40.73, -74.1], [40.717, -73.99])\n        > bounds = ElasticFilter().geo_bounding_box('pin.location', dict(lat=40.73, lon=-74.1), dict(lat=40.717, lon=-73.99))\n        > bounds = ElasticFilter().geo_bounding_box('pin.location', \"40.73, -74.1\", \"40.717, -73.99\")\n        And geohash\n        > bounds = ElasticFilter().geo_bounding_box('pin.location', \"drm3btev3e86\", \"drm3btev3e86\")\n\n        '''\n        return cls(geo_bounding_box={field: {'top_left': top_left, 'bottom_right': bottom_right}})", "response": "Create a geo bounding box for the specified field."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new elastic search entry point filter that filters documents that include only hits that exists within a specific distance from a geo point.", "response": "def geo_distance(cls, field, center, distance, distance_type=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/geo-distance-filter.html\n        Filters documents that include only hits that exists within a specific distance from a geo point.\n        field - Field name\n        center - Center point (Geo point)\n        distance - String for the distance\n        distance_type - (arc | plane) How to compute the distance. Can either be arc (better precision) or plane (faster). Defaults to arc\n        > bounds = ElasticFilter().geo_distance('pin.location', [40.73, -74.1], '300km')\n        '''\n\n        instance = cls(geo_distance={'distance': distance, field: center})\n        if distance_type is not None:\n            instance['geo_distance']['distance_type'] = distance_type\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new object that represents a geo distance range filter.", "response": "def geo_distance_range(cls, field, center, from_distance, to_distance, distance_type=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/geo-distance-range-filter.html\n        Filters documents that exists within a range from a specific point\n\n\n        '''\n        instance = cls(geo_distance_range={'from': from_distance, 'to': to_distance, field: center})\n        if distance_type is not None:\n            instance['geo_distance_range']['distance_type'] = distance_type\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef numeric_range(cls, field, from_value, to_value, include_lower=None, include_upper=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/numeric-range-filter.html\n        Filters documents with fields that have values within a certain numeric range. Similar to range filter, except that it works only with numeric values, and the filter execution works differently.\n        '''\n        instance = cls(numeric_range={field: {'from': from_value, 'to': to_value}})\n        if include_lower is not None:\n            instance['numeric_range'][field]['include_lower'] = include_lower\n        if include_upper is not None:\n            instance['numeric_range'][field]['include_upper'] = include_upper\n        return instance", "response": "Returns a new instance of the class with numeric_range set to True."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef range(cls, field, from_value=None, to_value=None, include_lower=None, include_upper=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/range-filter.html\n\n        Filters documents with fields that have terms within a certain range. Similar to range query, except that it acts as a filter. Can be placed within queries that accept a filter.\n        '''\n\n        instance = cls({'range': {field: {}}})\n        if from_value is not None:\n            instance['range'][field]['from'] = from_value\n        if to_value is not None:\n            instance['range'][field]['to'] = to_value\n        if include_lower is not None:\n            instance['range'][field]['include_lower'] = include_lower\n        if include_upper is not None:\n            instance['range'][field]['include_upper'] = include_upper\n\n        return instance", "response": "Returns a new instance of the class with a range field."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves an object and use id_code in the filename", "response": "def save(self, obj, id_code):\n        \"\"\"\n        Save an object, and use id_code in the filename\n        obj - any object\n        id_code - unique identifier\n        \"\"\"\n        filestream = open('{0}/{1}'.format(self.data_path, id_code), 'w+')\n        pickle.dump(obj, filestream)\n        filestream.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load(self, id_code):\n        filestream = open('{0}/{1}'.format(self.data_path, id_code), 'rb')\n        workflow = pickle.load(filestream)\n        return workflow", "response": "Loads a workflow identified by id_code from the data directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init(self):\n        if os.path.isdir(self.path):\n            raise InvalidTodoFile\n        if os.path.exists(self.path):\n            with open(self.path, 'r') as f:\n                tls = [tl.strip() for tl in f if tl]\n                todos = map(_todo_from_file, tls)\n                self.todos = todos\n                for todo in todos:\n                    if self.current_max_idx < todo['idx']:\n                        self.current_max_idx = todo['idx']\n        else:\n            logger.warning('No todo files found, initialization a empty todo file')\n            with open(self.path, 'w') as f:\n                f.flush()", "response": "init todo file with all todos in the todo directory"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _show(self, status=None, idx=None):\n        _show('', 50)\n        if not self.todos:\n            self._show_no_todos()\n        elif idx is not None:\n            for todo in self.todos:\n                if todo['idx'] == idx:\n                    self._show_todos(todo)\n        elif status is not None:\n            if status not in STATUS_CODE:\n                raise InvalidTodoStatus\n            _todos = []\n            for todo in self.todos:\n                if todo['status'] == status:\n                    _todos.append(todo)\n            if not _todos:\n                self._show_no_todos(text_fix='No {} todos...'.format(\n                                    STATUS_CODE.get(status, None)))\n            else:\n                for todo in _todos:\n                    self._show_todos(todo)\n        else:\n            for todo in self.todos:\n                self._show_todos(todo)\n        _show('', 50)", "response": "show all todos in the archive after format\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflush todos to file", "response": "def write(self, delete_if_empty=False):\n        \"\"\"flush todos to file\n        :param delete_if_empty: delete if todo is empty\n        \"\"\"\n        with open(self.path, 'w') as f:\n            if not self.todos:\n                f.flush()\n            else:\n                for todo in _todo_to_file(self.todos):\n                    f.write(todo)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a pickle file and returns the object.", "response": "def read_object_from_pickle(desired_type: Type[T], file_path: str, encoding: str,\n                            fix_imports: bool = True, errors: str = 'strict', *args, **kwargs) -> Any:\n    \"\"\"\n    Parses a pickle file.\n\n    :param desired_type:\n    :param file_path:\n    :param encoding:\n    :param fix_imports:\n    :param errors:\n    :param args:\n    :param kwargs:\n    :return:\n    \"\"\"\n    import pickle\n    file_object = open(file_path, mode='rb')\n    try:\n        return pickle.load(file_object, fix_imports=fix_imports, encoding=encoding, errors=errors)\n    finally:\n        file_object.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef should_display_warnings_for(to_type):\n    if not hasattr(to_type, '__module__'):\n        return True\n    elif to_type.__module__ in {'builtins'} or to_type.__module__.startswith('parsyfiles') \\\n            or to_type.__name__ in {'DataFrame'}:\n        return False\n    elif issubclass(to_type, int) or issubclass(to_type, str) \\\n            or issubclass(to_type, float) or issubclass(to_type, bool):\n        return False\n    else:\n        return True", "response": "Returns True if the warnings should be displayed for the given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the types are valid for dict_to_object conversion", "response": "def _is_valid_for_dict_to_object_conversion(strict_mode: bool, from_type: Type, to_type: Type) -> bool:\n    \"\"\"\n    Returns true if the provided types are valid for dict_to_object conversion\n\n    Explicitly declare that we are not able to parse collections nor able to create an object from a dictionary if the\n    object's constructor is non correctly PEP484-specified.\n\n    None should be treated as a Joker here (but we know that never from_type and to_type will be None at the same time)\n\n    :param strict_mode:\n    :param from_type:\n    :param to_type:\n    :return:\n    \"\"\"\n    # cache previous results\n    try:\n        res, subclasses_hash = _cache_valid_for_dict_to_object[to_type][strict_mode]\n        # Check if are any new subclasses are available\n        if not strict_mode and to_type is not None and not is_any_type(to_type):\n            if hash(tuple(get_all_subclasses(to_type))) != subclasses_hash:\n                raise KeyError('fake error to recompute the cache entry')\n    except KeyError:\n        res = __is_valid_for_dict_to_object_conversion(strict_mode=strict_mode, from_type=from_type, to_type=to_type)\n        # Store an entry in the cache containing the result and the hash of the subclasses list\n        subclasses_hash = None\n        if not strict_mode and to_type is not None and not is_any_type(to_type):\n            subclasses_hash = hash(tuple(get_all_subclasses(to_type)))\n        entry = (res, subclasses_hash)\n        try:\n            _cache_valid_for_dict_to_object[to_type][strict_mode] = entry\n        except KeyError:\n            _cache_valid_for_dict_to_object[to_type] = {strict_mode: entry}\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __is_valid_for_dict_to_object_conversion(strict_mode: bool, from_type: Type, to_type: Type) -> bool:\n    # right now we're stuck with the default logger..\n    logr = default_logger\n\n    if to_type is None or is_any_type(to_type):\n        # explicitly handle the 'None' (joker) or 'any' type\n        return True\n\n    elif is_collection(to_type, strict=True):\n        # if the destination type is 'strictly a collection' (not a subclass of a collection) we know that we can't\n        # handle it here, the constructor is not pep484-typed\n        return False\n\n    else:\n        # (1) Try the type itself\n        try:\n            # can we find enough pep-484 information in the constructor to be able to understand what is required ?\n            get_constructor_attributes_types(to_type)\n            return True\n        except TypeInformationRequiredError as main_e:\n            # # failed: we cant guess the required types of constructor arguments\n            # if strict_mode:\n            #     # Warning and return NO\n            #     if should_display_warnings_for(to_type):\n            #         logr.warn('Object constructor signature for type {} does not allow parsyfiles to '\n            #                    'automatically create instances from dict content. Caught {}: {}'\n            #                    ''.format(get_pretty_type_str(to_type), type(main_e).__name__, main_e))\n            #     return False\n            #\n            # # non-strict mode: (2) Check if any subclasses exist\n            # subclasses = get_all_subclasses(to_type)\n            # if len(subclasses) > GLOBAL_CONFIG.dict_to_object_subclass_limit:\n            #     logr.warn('WARNING: Type {} has {} subclasses, only {} will be tried by parsyfiles when attempting to '\n            #               'create it from a subclass. You can raise this limit by setting the appropriate option with '\n            #               '`parsyfiles_global_config()`'\n            #               ''.format(to_type, len(subclasses), GLOBAL_CONFIG.dict_to_object_subclass_limit))\n            #\n            # # Then for each subclass also try (with a configurable limit in nb of subclasses)\n            # for subclass in subclasses[0:GLOBAL_CONFIG.dict_to_object_subclass_limit]:\n            #     try:\n            #         get_constructor_attributes_types(subclass)\n            #         # OK, but issue warning for the root type still\n            #         if should_display_warnings_for(to_type):\n            #             logr.warn('WARNING: Object constructor signature for type {} does not allow parsyfiles to '\n            #                       'automatically create instances from dict content, but it can for at least one of '\n            #                       'its subclasses ({}) so it might be ok for you. Caught {}: {}'\n            #                       ''.format(get_pretty_type_str(to_type), get_pretty_type_str(subclass),\n            #                                  type(main_e).__name__, main_e))\n            #         return True\n            #     except TypeInformationRequiredError as e:\n            #         # failed: we cant guess the required types of constructor arguments\n            #         if should_display_warnings_for(to_type):\n            #             logr.warn('WARNING: Object constructor signature for type {} does not allow parsyfiles to '\n            #                       'automatically create instances from dict content. Caught {}: {}'\n            #                       ''.format(subclass, type(e).__name__, e))\n            #\n            # # Nothing succeeded\n\n            if should_display_warnings_for(to_type):\n                logr.warn('WARNING: Object constructor signature for type {} does not allow parsyfiles to '\n                          'automatically create instances from dict content. Caught {}: {}'\n                          ''.format(get_pretty_type_str(to_type), type(main_e).__name__, main_e))\n\n            return False", "response": "Checks if the types of the provided types are valid for dict_to_object conversion"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dict_to_object(desired_type: Type[T], contents_dict: Dict[str, Any], logger: Logger,\n                   options: Dict[str, Dict[str, Any]], conversion_finder: ConversionFinder = None,\n                   is_dict_of_dicts: bool = False) -> T:\n    \"\"\"\n    Utility method to create an object from a dictionary of constructor arguments. Constructor arguments that dont have\n    the correct type are intelligently converted if possible\n\n    :param desired_type:\n    :param contents_dict:\n    :param logger:\n    :param options:\n    :param conversion_finder:\n    :param is_dict_of_dicts:\n    :return:\n    \"\"\"\n    check_var(desired_type, var_types=type, var_name='obj_type')\n    check_var(contents_dict, var_types=dict, var_name='contents_dict')\n\n    if is_collection(desired_type, strict=True):\n        # if the destination type is 'strictly a collection' (not a subclass of a collection) we know that we can't\n        # handle it here, the constructor is not pep484-typed\n        raise TypeError('Desired object type \\'' + get_pretty_type_str(desired_type) + '\\' is a collection, '\n                        'so it cannot be created using this generic object creator')\n    else:\n        # Try the type itself\n        # try:\n        return _dict_to_object(desired_type, contents_dict, logger=logger, options=options,\n                               conversion_finder=conversion_finder, is_dict_of_dicts=is_dict_of_dicts)", "response": "Utility method to create an object from a dictionary of constructor arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dict_to_object(desired_type: Type[T], contents_dict: Dict[str, Any], logger: Logger,\n                    options: Dict[str, Dict[str, Any]], conversion_finder: ConversionFinder = None,\n                    is_dict_of_dicts: bool = False) -> T:\n    \"\"\"\n    Utility method to create an object from a dictionary of constructor arguments. Constructor arguments that dont have\n    the correct type are intelligently converted if possible\n\n    :param desired_type:\n    :param contents_dict:\n    :param logger:\n    :param options:\n    :param conversion_finder:\n    :param is_dict_of_dicts:\n    :return:\n    \"\"\"\n    # collect pep-484 information in the constructor to be able to understand what is required\n    constructor_args_types_and_opt = get_constructor_attributes_types(desired_type)\n\n    try:\n        # for each attribute, convert the types of its parsed values if required\n        dict_for_init = dict()\n        for attr_name, provided_attr_value in contents_dict.items():\n\n            # check if this attribute name is required by the constructor\n            if attr_name in constructor_args_types_and_opt.keys():\n\n                # check the theoretical type wanted by the constructor\n                attr_type_required = constructor_args_types_and_opt[attr_name][0]\n\n                # resolve forward references\n                attr_type_required = resolve_forward_ref(attr_type_required)\n\n                if not is_dict_of_dicts:\n                    if is_valid_pep484_type_hint(attr_type_required):\n                        # this will not fail if type information is not present;the attribute will only be used 'as is'\n                        full_attr_name = get_pretty_type_str(desired_type) + '.' + attr_name\n\n                        dict_for_init[attr_name] = ConversionFinder.try_convert_value(conversion_finder, full_attr_name,\n                                                                                      provided_attr_value,\n                                                                                      attr_type_required, logger,\n                                                                                      options)\n\n                    else:\n                        warn(\"Constructor for type <{t}> has no valid PEP484 Type hint for attribute {att}, trying to \"\n                             \"use the parsed value in the dict directly\".format(t=get_pretty_type_str(desired_type),\n                                                                         att=attr_name))\n                        dict_for_init[attr_name] = provided_attr_value\n\n                else:\n                    # in that mode, the attribute value itself is a dict, so the attribute needs to be built from that\n                    # dict first\n                    if isinstance(provided_attr_value, dict):\n                        # recurse : try to build this attribute from the dictionary provided. We need to know the type\n                        # for this otherwise we wont be able to call the constructor :)\n                        if (attr_type_required is None) or (attr_type_required is Parameter.empty):\n                            raise TypeInformationRequiredError.create_for_object_attributes(desired_type, attr_name,\n                                                                                            attr_type_required)\n\n                        elif not is_valid_pep484_type_hint(attr_type_required):\n                            raise InvalidPEP484TypeHint.create_for_object_attributes(desired_type, attr_name,\n                                                                                     attr_type_required)\n\n                        else:\n                            # we can build the attribute from the sub-dict\n                            dict_for_init[attr_name] = dict_to_object(attr_type_required, provided_attr_value,\n                                                                      logger, options,\n                                                                      conversion_finder=conversion_finder)\n\n                    else:\n                        raise ValueError('Error while trying to build object of type ' + str(desired_type) + ' from a '\n                                         'dictionary of dictionaries. Entry \\'' + attr_name + '\\' is not a dictionary')\n            else:\n                if is_dict_of_dicts and attr_name is 'DEFAULT':\n                    # -- tolerate but ignore - this is probably due to a configparser\n                    # warning('Property name \\'' + attr_name + '\\' is not an attribute of the object constructor. <'\n                    #         + get_pretty_type_str(desired_type) + '> constructor attributes are : '\n                    #         + list(set(constructor_args_types.keys()) - {'self'}) + '. However it is named DEFAULT')\n                    pass\n                else:\n                    # the dictionary entry does not correspond to a valid attribute of the object\n                    raise InvalidAttributeNameForConstructorError.create(desired_type,\n                                                                         list(set(constructor_args_types_and_opt.keys()) - {'self'}),\n                                                                         attr_name)\n\n        # create the object using its constructor\n        try:\n            return desired_type(**dict_for_init)\n        except Exception as e:\n            # Wrap into an Exception\n            raise ObjectInstantiationException.create(desired_type, dict_for_init, e)\n\n    except TypeError as e:\n        raise CaughtTypeErrorDuringInstantiation.create(desired_type, contents_dict, e)", "response": "Utility method to create an object from a dictionary of constructor arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_dict(dict_name, dict_value, logger: Logger = None):\n    if logger is None:\n        print(dict_name + ' = ')\n        try:\n            from pprint import pprint\n            pprint(dict_value)\n        except:\n            print(dict_value)\n    else:\n        logger.info(dict_name + ' = ')\n        try:\n            from pprint import pformat\n            logger.info(pformat(dict_value))\n        except:\n            logger.info(dict_value)", "response": "Utility method to print a named dictionary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_object_converters(conversion_finder: ConversionFinder) \\\n        -> List[Union[Converter[Any, Type[None]], Converter[Type[None], Any]]]:\n    \"\"\"\n    Utility method to return the default converters associated to dict (from dict to other type,\n    and from other type to dict)\n    :return:\n    \"\"\"\n\n    return [\n            ConverterFunction(from_type=b64str, to_type=AnyObject, conversion_method=base64_ascii_str_pickle_to_object),\n            ConverterFunction(from_type=DictOfDict, to_type=Any, conversion_method=dict_to_object,\n                              custom_name='dict_of_dict_to_object',\n                              is_able_to_convert_func=_is_valid_for_dict_to_object_conversion, unpack_options=False,\n                              function_args={'conversion_finder': conversion_finder, 'is_dict_of_dicts': True}),\n            ConverterFunction(from_type=dict, to_type=AnyObject, conversion_method=dict_to_object,\n                              custom_name='dict_to_object', unpack_options=False,\n                              is_able_to_convert_func=_is_valid_for_dict_to_object_conversion,\n                              function_args={'conversion_finder': conversion_finder, 'is_dict_of_dicts': False})\n            ]", "response": "Utility method to return the default converters associated to dict of object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(item_type: Type[Any], constructor_atts: List[str], invalid_property_name: str):\n        return InvalidAttributeNameForConstructorError('Cannot parse object of type <' + get_pretty_type_str(item_type)\n                                                       + '> using the provided configuration file: configuration '\n                                                       + 'contains a property name (\\'' + invalid_property_name + '\\')'\\\n                                                       + 'that is not an attribute of the object constructor. <'\n                                                       + get_pretty_type_str(item_type) + '> constructor attributes '\n                                                       + 'are : ' + str(constructor_atts))", "response": "Helper method provided because we can t put that in the constructor"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(item_type: Type[Any], constructor_args: Dict[str, Any], cause: Exception):\n        return ObjectInstantiationException('Error while building object of type <' + get_pretty_type_str(item_type)\n                                            + '> using its constructor and parsed contents : ' + str(constructor_args)\n                                            + ' : \\n' + str(cause.__class__) + ' ' + str(cause)\n                                            ).with_traceback(cause.__traceback__)", "response": "Helper method provided because we can t put that in the constructor it creates a bug in Nose tests\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(desired_type: Type[Any], contents_dict: Dict, caught: Exception):\n        msg = 'Error while trying to instantiate object of type ' + str(desired_type) + ' using dictionary input_dict:'\\\n              + 'Caught error message is : ' + caught.__class__.__name__ + ' : ' + str(caught) + '\\n'\n        try:\n            from pprint import pformat\n            msg += 'Dict provided was ' + pformat(contents_dict)\n        except:\n            msg += 'Dict provided was ' + str(contents_dict)\n\n        return CaughtTypeErrorDuringInstantiation(msg).with_traceback(caught.__traceback__)", "response": "Helper method provided because we can t put that in the constructor it creates a bug in Nose tests\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if the action is valid.", "response": "def is_valid(self, context):\n        \"\"\"Checks through the previous_actions iterable if required actions have\n        been executed\n        \"\"\"\n        if self.requires:\n            for r in self.requires:\n                if not r in context.executed_actions:\n                    raise RequirementMissingError(\"Action '%s' requires '%s'\" % (self.name, r))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the contents of a file using full path name", "response": "def get_file_contents(file_path):\n  \"\"\"Get the context of the file using full path name\"\"\"\n  full_path = os.path.join(package_dir, file_path)\n  return open(full_path, 'r').read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the json data from a dictionary.", "response": "def update(self, json_state):\n        \"\"\"Update the json data from a dictionary.\n\n        Only updates if it already exists in the device.\n        \"\"\"\n        if self._type in CONST.BINARY_SENSOR_TYPES:\n            self._json_state['status'] = json_state['status']\n        else:\n            self._json_state.update(\n                {k: json_state[k] for k in json_state if self._json_state.get(k)})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a short description of the device.", "response": "def desc(self):\n        \"\"\"Get a short description of the device.\"\"\"\n        return '{0} (ID: {1}) - {2} - {3}'.format(\n            self.name, self.device_id, self.type, self.status)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef declare(queues):\n    current_queues.declare(queues=queues)\n    click.secho(\n        'Queues {} have been declared.'.format(\n            queues or current_queues.queues.keys()),\n        fg='green'\n    )", "response": "Initialize the given queues."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef purge_queues(queues=None):\n    current_queues.purge(queues=queues)\n    click.secho(\n        'Queues {} have been purged.'.format(\n            queues or current_queues.queues.keys()),\n        fg='green'\n    )", "response": "Purge the given queues."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_queue(queues):\n    current_queues.delete(queues=queues)\n    click.secho(\n        'Queues {} have been deleted.'.format(\n            queues or current_queues.queues.keys()),\n        fg='green'\n    )", "response": "Delete the given queues."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_needed_formatter(input_format, output_format):\n    #Only take the formatters in the registry\n    selected_registry = [re.cls for re in registry if re.category==RegistryCategories.formatters]\n    needed_formatters = []\n    for formatter in selected_registry:\n        #Initialize the formatter (needed so it can discover its formats)\n        formatter_inst = formatter()\n        if input_format in formatter_inst.input_formats and output_format in formatter_inst.output_formats:\n            needed_formatters.append(formatter)\n    if len(needed_formatters)>0:\n        return needed_formatters[0]\n    return None", "response": "Find a data formatter given an input and output format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind a needed input class input_format - needed input format see utils. input. dataformats", "response": "def find_needed_input(input_format):\n    \"\"\"\n    Find a needed input class\n    input_format - needed input format, see utils.input.dataformats\n    \"\"\"\n    needed_inputs = [re.cls for re in registry if re.category==RegistryCategories.inputs and re.cls.input_format == input_format]\n    if len(needed_inputs)>0:\n        return needed_inputs[0]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists_in_registry(category, namespace, name):\n    selected_registry = [re for re in registry if re.category==category and re.namespace==namespace and re.name == name]\n    if len(selected_registry)>0:\n        return True\n    return False", "response": "Check if a given category namespace name combination exists in the registry"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a given model in the registry", "response": "def register(cls):\n    \"\"\"\n    Register a given model in the registry\n    \"\"\"\n    registry_entry = RegistryEntry(category = cls.category, namespace = cls.namespace, name = cls.name, cls=cls)\n    if registry_entry not in registry and not exists_in_registry(cls.category, cls.namespace, cls.name):\n        registry.append(registry_entry)\n    else:\n        log.warn(\"Class {0} already in registry\".format(cls))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the fields for data caching.", "response": "def _set_fields(self):\n        \"\"\"\n        Initialize the fields for data caching.\n        \"\"\"\n        self.fields = []\n        self.required_input = []\n        for member_name, member_object in inspect.getmembers(self.__class__):\n            if inspect.isdatadescriptor(member_object) and not member_name.startswith(\"__\"):\n                self.fields.append(member_name)\n                if member_object.required_input:\n                    self.required_input.append(member_name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subscriber(address,topics,callback,message_type):\n    return Subscriber(address,topics,callback,message_type)", "response": "Creates a subscriber binding to the given address and subscribe the given topics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self):\n        t=threading.Thread(target=self._consume)\n        t.start()", "response": "Start a thread that consumes the messages and invokes the callback\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_forecast(api_result: dict) -> List[SmhiForecast]:\r\n    forecasts = []\r\n\r\n    # Need the ordered dict to get\r\n    # the days in order in next stage\r\n    forecasts_ordered = OrderedDict()\r\n\r\n    forecasts_ordered = _get_all_forecast_from_api(api_result)\r\n\r\n    # Used to calc the daycount\r\n    day_nr = 1\r\n\r\n    for day in forecasts_ordered:\r\n        forecasts_day = forecasts_ordered[day]\r\n\r\n        if day_nr == 1:\r\n            # Add the most recent forecast\r\n            forecasts.append(copy.deepcopy(forecasts_day[0]))\r\n\r\n        total_precipitation = float(0.0)\r\n        forecast_temp_max = -100.0\r\n        forecast_temp_min = 100.0\r\n        forecast = None\r\n        for forcast_day in forecasts_day:\r\n            temperature = forcast_day.temperature\r\n            if forecast_temp_min > temperature:\r\n                forecast_temp_min = temperature\r\n            if forecast_temp_max < temperature:\r\n                forecast_temp_max = temperature\r\n\r\n            if forcast_day.valid_time.hour == 12:\r\n                forecast = copy.deepcopy(forcast_day)\r\n\r\n            total_precipitation = total_precipitation + \\\r\n                forcast_day._total_precipitation\r\n\r\n        if forecast is None:\r\n            # We passed 12 noon, set to current\r\n            forecast = forecasts_day[0]\r\n\r\n        forecast._temperature_max = forecast_temp_max\r\n        forecast._temperature_min = forecast_temp_min\r\n        forecast._total_precipitation = total_precipitation\r\n        forecast._mean_precipitation = total_precipitation/24\r\n        forecasts.append(forecast)\r\n        day_nr = day_nr + 1\r\n\r\n    return forecasts", "response": "Converts results fr\u00e5m API to list of SmhiForecast objects"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the results fr\u00e5m API to SmhiForeCast list", "response": "def _get_all_forecast_from_api(api_result: dict) -> OrderedDict:\r\n    \"\"\"Converts results fr\u00e5m API to SmhiForeCast list\"\"\"\r\n    # Total time in hours since last forecast\r\n    total_hours_last_forecast = 1.0\r\n\r\n    # Last forecast time\r\n    last_time = None\r\n\r\n    # Need the ordered dict to get\r\n    # the days in order in next stage\r\n    forecasts_ordered = OrderedDict()\r\n\r\n    # Get the parameters\r\n    for forecast in api_result['timeSeries']:\r\n\r\n        valid_time = datetime.strptime(\r\n            forecast['validTime'], \"%Y-%m-%dT%H:%M:%SZ\")\r\n        for param in forecast['parameters']:\r\n            if param['name'] == 't':\r\n                temperature = float(param['values'][0])  # Celcisus\r\n            elif param['name'] == 'r':\r\n                humidity = int(param['values'][0])  # Percent\r\n            elif param['name'] == 'msl':\r\n                pressure = int(param['values'][0])  # hPa\r\n            elif param['name'] == 'tstm':\r\n                thunder = int(param['values'][0])  # Percent\r\n            elif param['name'] == 'tcc_mean':\r\n                octa = int(param['values'][0])  # Cloudiness in octas\r\n                if 0 <= octa <= 8:  # Between 0 -> 8\r\n                    cloudiness = round(100*octa/8)  # Convert octas to percent\r\n                else:\r\n                    cloudiness = 100  # If not determined use 100%\r\n            elif param['name'] == 'Wsymb2':\r\n                symbol = int(param['values'][0])  # category\r\n            elif param['name'] == 'pcat':\r\n                precipitation = int(param['values'][0])  # percipitation\r\n            elif param['name'] == 'pmean':\r\n                mean_precipitation = float(\r\n                    param['values'][0])  # mean_percipitation\r\n            elif param['name'] == 'ws':\r\n                wind_speed = float(param['values'][0])  # wind speed\r\n            elif param['name'] == 'wd':\r\n                wind_direction = int(param['values'][0])  # wind direction\r\n            elif param['name'] == 'vis':\r\n                horizontal_visibility = float(param['values'][0])  # Visibility\r\n            elif param['name'] == 'gust':\r\n                wind_gust = float(param['values'][0])  # wind gust speed\r\n\r\n        roundedTemp = int(round(temperature))\r\n\r\n        if last_time is not None:\r\n            total_hours_last_forecast = (valid_time - last_time).seconds/60/60\r\n\r\n        # Total precipitation, have to calculate with the nr of\r\n        # hours since last forecast to get correct total value\r\n        tp = round(mean_precipitation*total_hours_last_forecast, 2)\r\n\r\n        forecast = \\\r\n            SmhiForecast(roundedTemp, roundedTemp, roundedTemp,\r\n                         humidity, pressure, thunder, cloudiness,\r\n                         precipitation, wind_direction, wind_speed,\r\n                         horizontal_visibility, wind_gust,\r\n                         round(mean_precipitation, 1), tp, symbol,\r\n                         valid_time)\r\n\r\n        if valid_time.day not in forecasts_ordered:\r\n            # add a new list\r\n            forecasts_ordered[valid_time.day] = []\r\n\r\n        forecasts_ordered[valid_time.day].append(forecast)\r\n\r\n        last_time = valid_time\r\n\r\n    return forecasts_ordered"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_forecast_api(self, longitude: str, latitude: str) -> {}:\r\n        api_url = APIURL_TEMPLATE.format(longitude, latitude)\r\n\r\n        response = urlopen(api_url)\r\n        data = response.read().decode('utf-8')\r\n        json_data = json.loads(data)\r\n\r\n        return json_data", "response": "gets data from API"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def async_get_forecast_api(self, longitude: str,\r\n                                     latitude: str) -> {}:\r\n        \"\"\"gets data from API asyncronious\"\"\"\r\n        api_url = APIURL_TEMPLATE.format(longitude, latitude)\r\n\r\n        if self.session is None:\r\n            self.session = aiohttp.ClientSession()\r\n\r\n        async with self.session.get(api_url) as response:\r\n            if response.status != 200:\r\n                raise SmhiForecastException(\r\n                    \"Failed to access weather API with status code {}\".format(\r\n                        response.status)\r\n                )\r\n            data = await response.text()\r\n            return json.loads(data)", "response": "gets data from API asyncronious"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_forecast(self) -> List[SmhiForecast]:\r\n        json_data = self._api.get_forecast_api(self._longitude, self._latitude)\r\n        return _get_forecast(json_data)", "response": "Returns a list of all forecasts in the current location."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of forecasts.", "response": "async def async_get_forecast(self) -> List[SmhiForecast]:\r\n        \"\"\"\r\n        Returns a list of forecasts. The first in list are the current one\r\n        \"\"\"\r\n        json_data = await self._api.async_get_forecast_api(self._longitude,\r\n                                                           self._latitude)\r\n        return _get_forecast(json_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_decorator(measuring_func):\n    def _decorator(name = None, metric = call_default):\n        def wrapper(func):\n\n            name_ = name if name is not None else func.__module__ + '.' +func.__name__\n            class instrument_decorator(object): # must be a class for descriptor magic to work\n                @wraps(func)\n                def __call__(self, *args, **kwargs):\n                    return measuring_func(func(*args, **kwargs), name_, metric)\n\n                def __get__(self, instance, class_):\n                    name_ = name if name is not None else\\\n                        \".\".join((class_.__module__, class_.__name__, func.__name__))\n                    @wraps(func)\n                    def wrapped_method(*args, **kwargs):\n                        return measuring_func(func(instance, *args, **kwargs), name_, metric)\n                    return wrapped_method\n            return instrument_decorator()\n\n        return wrapper\n    return _decorator", "response": "make a decorator that can be used to decorate a function with measuring_func."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a generator that yields all the items in the iterable.", "response": "def all(iterable = None, *, name = None, metric = call_default):\n    \"\"\"Measure total time and item count for consuming an iterable\n\n    :arg iterable: any iterable\n    :arg function metric: f(name, count, total_time)\n    :arg str name: name for the metric\n    \"\"\"\n    if iterable is None:\n        return _iter_decorator(name, metric)\n    else:\n        return _do_all(iterable, name, metric)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmeasures time elapsed to produce each item of an iterable", "response": "def each(iterable = None, *, name = None, metric = call_default):\n    \"\"\"Measure time elapsed to produce each item of an iterable\n\n    :arg iterable: any iterable\n    :arg function metric: f(name, 1, time)\n    :arg str name: name for the metric\n    \"\"\"\n    if iterable is None:\n        return _each_decorator(name, metric)\n    else:\n        return _do_each(iterable, name, metric)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef first(iterable = None, *, name = None, metric = call_default):\n    if iterable is None:\n        return _first_decorator(name, metric)\n    else:\n        return _do_first(iterable, name, metric)", "response": "Decorator to produce the first item of an iterable"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _iterable_to_varargs_method(func):\n    def wrapped(self, *args, **kwargs):\n        return func(self, args, **kwargs)\n    return wrapped", "response": "decorator to convert a method taking a iterable to a * args one"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _varargs_to_iterable_method(func):\n    def wrapped(self, iterable, **kwargs):\n        return func(self, *iterable, **kwargs)\n    return wrapped", "response": "decorator to convert a method to one taking a iterable"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reducer(*, name = None, metric = call_default):\n    class instrument_reducer_decorator(object):\n        def __init__(self, func):\n            self.orig_func = func\n            self.wrapping = wraps(func)\n            self.metric_name = name if name is not None else func.__module__ + '.' +func.__name__\n            self.varargs = inspect.getargspec(func).varargs is not None\n            if self.varargs:\n                self.method = _varargs_to_iterable_method(func)\n                self.func = _varargs_to_iterable_func(func)\n                self.callme = _iterable_to_varargs_func(self._call)\n            else:\n                self.method = func\n                self.func = func\n                self.callme = self._call\n\n        # we need _call/callme b/c CPython short-circurits CALL_FUNCTION to\n        # directly access __call__, bypassing our varargs decorator\n        def __call__(self, *args, **kwargs):\n            return self.callme(*args, **kwargs)\n\n        def _call(self, iterable, **kwargs):\n            it = counted_iterable(iterable)\n            t = time.time()\n            try:\n                return self.func(it, **kwargs)\n            finally:\n                metric(self.metric_name, it.count, time.time() - t)\n\n        def __get__(self, instance, class_):\n            metric_name = name if name is not None else\\\n                \".\".join((class_.__module__, class_.__name__, self.orig_func.__name__))\n\n            def wrapped_method(iterable, **kwargs):\n                it = counted_iterable(iterable)\n                t = time.time()\n                try:\n                    return self.method(instance, it, **kwargs)\n                finally:\n                    metric(metric_name, it.count, time.time() - t)\n\n            # wrap in func version b/c self is handled for us by descriptor (ie, `instance`)\n            if self.varargs: wrapped_method = _iterable_to_varargs_func(wrapped_method)\n            wrapped_method = self.wrapping(wrapped_method)\n            return wrapped_method\n\n    return instrument_reducer_decorator", "response": "Decorator to measure a function that consumes many items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef producer(*, name = None, metric = call_default):\n\n    def wrapper(func):\n        def instrumenter(name_, *args, **kwargs):\n            t = time.time()\n            try:\n                ret = func(*args, **kwargs)\n            except Exception:\n                # record a metric for other exceptions, than raise\n                metric(name_, 0, time.time() - t)\n                raise\n            else:\n                # normal path, record metric & return\n                metric(name_, len(ret), time.time() - t)\n                return ret\n\n        name_ = name if name is not None else func.__module__ + '.' +func.__name__\n        class instrument_decorator(object): # must be a class for descriptor magic to work\n            @wraps(func)\n            def __call__(self, *args, **kwargs):\n                return instrumenter(name_, *args, **kwargs)\n\n            def __get__(self, instance, class_):\n                name_ = name if name is not None else\\\n                    \".\".join((class_.__module__, class_.__name__, func.__name__))\n                @wraps(func)\n                def wrapped_method(*args, **kwargs):\n                    return instrumenter(name_, instance, *args, **kwargs)\n                return wrapped_method\n\n        return instrument_decorator()\n    return wrapper", "response": "Decorator to measure a function that produces many items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef block(*, name = None, metric = call_default, count = 1):\n    t = time.time()\n    try:\n        yield\n    finally:\n        metric(name, count, time.time() - t)", "response": "Context manager to measure execution time of a block of items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_from_string(import_string):\n    import_split = import_string.split(\".\")\n    import_class = import_split[-1]\n    module_path = \".\".join(import_split[:-1])\n    mod = __import__(module_path, fromlist=[import_class])\n    klass = getattr(mod, import_class)\n    return klass", "response": "Imports a class from a string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a message to the specified topic on the socket.", "response": "def send(self,message,message_type,topic=''):\n        \"\"\"\n        Send the message on the socket.\n        \n        Args:\n            - message: the message to publish\n            - message_type: the type of message being sent\n            - topic: the topic on which to send the message. Defaults to ''.\n        \"\"\"\n        if message_type == RAW:\n            self._sock.send(message)\n        elif message_type == PYOBJ:\n            self._sock.send_pyobj(message)\n        elif message_type == JSON:\n            self._sock.send_json(message)\n        elif message_type == MULTIPART:\n            self._sock.send_multipart([topic, message])\n        elif message_type == STRING:\n            self._sock.send_string(message)\n        elif message_type == UNICODE:\n            self._sock.send_unicode(message)\n        else:\n            raise Exception(\"Unknown message type %s\"%(message_type,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreceive a message of the specified type and return the topic and message.", "response": "def receive(self,message_type):\n        \"\"\"\n        Receive the message of the specified type and retun\n        \n            Args:\n                - message_type: the type of the message to receive\n                \n            Returns:\n                - the topic of the message\n                - the message received from the socket\n        \"\"\"\n        topic = None\n        message = None\n        if message_type == RAW:\n            message = self._sock.recv(flags=zmq.NOBLOCK)\n        elif message_type == PYOBJ:\n            message = self._sock.recv_pyobj(flags=zmq.NOBLOCK)\n        elif message_type == JSON:\n            message = self._sock.recv_json(flags=zmq.NOBLOCK)\n        elif message_type == MULTIPART:\n            data = self._sock.recv_multipart(flags=zmq.NOBLOCK)\n            message = data[1]\n            topic = data[0]\n        elif message_type == STRING:\n            message = self._sock.recv_string(flags=zmq.NOBLOCK)\n        elif message_type == UNICODE:\n            message = self._sock.recv_unicode(flags=zmq.NOBLOCK)\n        else:\n            raise Exception(\"Unknown message type %s\"%(self._message_type,))\n            \n        return (topic, message)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dynamic_exec(code, resolve, assign=None, delete=None, automatic_builtins=True,\n                 filename=None, module_name=None, _type='exec'):\n  \"\"\"\n  Transforms the Python source code *code* and evaluates it so that the\n  *resolve* and *assign* functions are called respectively for when a global\n  variable is access or assigned.\n\n  If *resolve* is a mapping, *assign* must be omitted. #KeyError#s raised by\n  the mapping are automatically converted to #NameError#s.\n\n  Otherwise, *resolve* and *assign* must be callables that have the same\n  interface as `__getitem__()`, and `__setitem__()`. If *assign* is omitted\n  in that case, assignments will be redirected to a separate dictionary and\n  keys in that dictionary will be checked before continuing with the *resolve*\n  callback.\n  \"\"\"\n\n  parse_filename = filename or '<string>'\n  ast_node = transform(ast.parse(code, parse_filename, mode=_type))\n  code = compile(ast_node, parse_filename, _type)\n  if hasattr(resolve, '__getitem__'):\n    if assign is not None:\n      raise TypeError('\"assign\" parameter specified where \"resolve\" is a mapping')\n    if delete is not None:\n      raise TypeError('\"delete\" parameter specified where \"resolve\" is a mapping')\n    input_mapping = resolve\n\n    def resolve(x):\n      try:\n        return input_mapping[x]\n      except KeyError:\n        raise NameError(x)\n\n    assign = input_mapping.__setitem__\n\n    delete = input_mapping.__delitem__\n  else:\n    input_mapping = False\n\n  class DynamicMapping(object):\n    _data = {}\n    _deleted = set()\n    def __repr__(self):\n      if input_mapping:\n        return 'DynamicMapping({!r})'.format(input_mapping)\n      else:\n        return 'DynamicMapping(resolve={!r}, assign={!r})'.format(resolve, assign)\n    def __getitem__(self, key):\n      if key in self._deleted:\n        raise NameError(key)\n      if assign is None:\n        try:\n          return self._data[key]\n        except KeyError:\n          pass  # Continue with resolve()\n      try:\n        return resolve(key)\n      except NameError as exc:\n        if automatic_builtins and not key.startswith('_'):\n          try:\n            return getattr(builtins, key)\n          except AttributeError:\n            pass\n        raise exc\n    def __setitem__(self, key, value):\n      self._deleted.discard(key)\n      if assign is None:\n        self._data[key] = value\n      else:\n        assign(key, value)\n    def __delitem__(self, key):\n      if delete is None:\n        self._deleted.add(key)\n      else:\n        delete(key)\n    def get(self, key, default=None):\n      try:\n        return self[key]\n      except NameError:\n        return default\n\n  mapping = DynamicMapping()\n  globals_ = {'__dict__': mapping}\n\n  if filename:\n    mapping['__file__'] = filename\n    globals_['__file__'] = filename\n\n  if module_name:\n    mapping['__name__'] = module_name\n    globals_['__name__'] = module_name\n\n  return (exec_ if _type == 'exec' else eval)(code, globals_)", "response": "Dynamically executes a Python source code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_subscript(self, name, ctx=None):\n\n    assert isinstance(name, string_types), name\n    return ast.Subscript(\n      value=ast.Name(id=self.data_var, ctx=ast.Load()),\n      slice=ast.Index(value=ast.Str(s=name)),\n      ctx=ctx)", "response": "Returns a subgraph of the data variable with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __get_subscript_assign(self, name):\n\n    return ast.Assign(\n      targets=[self.__get_subscript(name, ast.Store())],\n      value=ast.Name(id=name, ctx=ast.Load()))", "response": "Returns an ast. Assign node that assigns a script to the given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a submodule delete.", "response": "def __get_subscript_delete(self, name):\n    \"\"\"\n    Returns `del <data_var>[\"<name>\"]`.\n    \"\"\"\n\n    return ast.Delete(targets=[self.__get_subscript(name, ast.Del())])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvisits the target node and add local variables to the current stack frame.", "response": "def __visit_target(self, node):\n    \"\"\"\n    Call this method to visit assignment targets and to add local variables\n    to the current stack frame. Used in #visit_Assign() and\n    #__visit_comprehension().\n    \"\"\"\n\n    if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n      self.__add_variable(node.id)\n    elif isinstance(node, (ast.Tuple, ast.List)):\n      [self.__visit_target(x) for x in node.elts]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls and verifies package manager for the current locale.", "response": "def __get_package_manager(self):\n        \"\"\"\n        Installs and verifies package manager\n        \"\"\"\n        package_manager = \"\"\n        args = \"\"\n        sudo_required = True\n        if system.is_osx():\n            package_manager = \"brew\"\n            sudo_required = False\n            args = \" install\"\n        elif system.is_debian():\n            package_manager = \"apt-get\"\n            args = \" -y install\"\n        elif system.is_fedora():\n            package_manager = \"yum\"\n            args = \" install\"\n        elif system.is_arch():\n            package_manager = \"pacman\"\n            args = \" --noconfirm -S\"\n        if lib.which(package_manager) is None:\n            self.logger.warn(\"Package manager %s not installed! Packages will not be installed.\"\n                             % package_manager)\n            self.package_manager = None\n        self.package_manager = package_manager\n        self.sudo_required = sudo_required\n        self.args = args"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a given from_type can be converted to to_type.", "response": "def can_convert(strict: bool, from_type: Type[S], to_type: Type[T]):\n    \"\"\"\n    None should be treated as a Joker here (but we know that never from_type and to_type will be None at the same time)\n\n    :param strict:\n    :param from_type:\n    :param to_type:\n    :return:\n    \"\"\"\n    if (to_type is not None) and (to_type not in (all_primitive_types + all_np_primitive_types)):\n        return False\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse an input string and return an AST containing the AST and errors and warnings.", "response": "def parse(self, data, doctype):\n        '''\n        Parse an input string, and return an AST\n        doctype must have WCADocument as a baseclass\n        '''\n        self.doctype = doctype\n        self.lexer.lineno = 0\n        del self.errors[:]\n        del self.warnings[:]\n        self.lexer.lexerror = False\n        ast = self.parser.parse(data, lexer=self.lexer)\n        if self.lexer.lexerror:\n            ast = None\n        if ast is None:\n            self.errors.append(\"Couldn't build AST.\")\n        else:\n            for check in self.sema[self.doctype]:\n                visitor = check()\n                if not visitor.visit(ast):\n                    self.errors.append(\"Couldn't visit AST.\")\n                self.errors.extend(visitor.errors)\n                self.warnings.extend(visitor.warnings)\n        return (ast, list(self.errors), list(self.warnings))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nact on the following rule", "response": "def _act_on_list(self, lhs):\n        '''\n        Act on the following rule :\n            items : items item\n                  | item\n        '''\n        lhs[0] = []\n        if len(lhs) == 3:\n            lhs[0] = lhs[1]\n        # lhs[len(lhs)-1] may be different from lhs[-1]\n        # Yacc use some internal method to get the element, see yacc.py:240\n        item = lhs[len(lhs) - 1]\n        if item:\n            lhs[0].append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_content(self, content):\n        '''content : TITLE opttexts VERSION opttexts sections\n                   | TITLE STATESTAG VERSION opttexts states_sections'''\n        content[0] = self.doctype(content[1], content[3], content[4], content[5])\n        if self.toc:\n            self.toc.set_articles([a for a in content[0].sections if isinstance(a, Article)])", "response": "content : TITLE opttexts VERSION opttexts sections\n                   | TITLE STATESTAG VERSION opttexts states_sections"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_toc(self, toc):\n        '''toc : HEADERSEC opttexts TOC opttexts'''\n        toc[0] = TableOfContent(toc[1], toc[2], [])\n        self.toc = toc[0]", "response": "toc : HEADERSEC opttexts TOC opttexts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\narticle : ARTICLEHEADER opttexts rules opttexts", "response": "def p_article(self, article):\n        '''article : ARTICLEHEADER opttexts rules opttexts'''\n        article[0] = Article(article[1][4], article[2], article[3], article[1][0],\n                             article[1][1], article[1][2], article[1][3], article[1][5])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef p_subsection(self, subsection):\n        '''subsection : HEADERSUBSEC texts\n                      | HEADERSUBSEC texts labeldecls opttexts'''\n        content = subsection[3] if len(subsection) > 3 else []\n        subsection[0] = Subsection(subsection[1], subsection[2], content)", "response": "p_subsection - set the subsection of the current entry"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_rule(self, rule):\n        '''rule : GUIDELINE\n                | REGULATION'''\n        if len(rule[1]) == 4:\n            # This is a guideline\n            rule[0] = Guideline(rule[1][1], rule[1][2], rule[1][3])\n        else:\n            # This is a regulation\n            indentsize = rule[1][0]\n            number = rule[1][1]\n            text = rule[1][2]\n            parent = None\n\n            # If we just \"un\"nested, shrink the current rule to our level\n            if self.prev_indent > indentsize:\n                self.current_rule = self.current_rule[0:indentsize+1]\n\n            # We just added a nested level, the parent is the list's last elem\n            if self.prev_indent < indentsize:\n                parent = self.current_rule[-1]\n            # Else, if we are nested the parent is the one before the last elem\n            elif len(self.current_rule) > 1:\n                parent = self.current_rule[-2]\n            # Else if we are not nested, then we are a root rule and parent is none\n            # (do nothing as parent is initialized to none)\n\n            # Create the regulation node\n            reg = Regulation(number, text, parent)\n\n            # Let our parent knows he has a new child, if we don't have a parent\n            # let's create an item in the article rules list\n            if parent:\n                parent.add_child(reg)\n            else:\n                rule[0] = reg\n\n            # Unless we nested, pop and replace the last rule by ourself\n            # If we added a nesting level, we just need to add ourself\n            if self.prev_indent >= indentsize:\n                self.current_rule.pop()\n            self.current_rule.append(reg)\n            self.prev_indent = indentsize", "response": "This function is used to create a new rule in the article."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p_state(self, state):\n        '''state : STATE opttexts'''\n        state[0] = State(state[1][0], state[1][1], state[1][2], state[1][3], state[2])", "response": "Parse the state of the log entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets brackets to set around a progress bar.", "response": "def set_progress_brackets(self, start, end):\n        \"\"\"Set brackets to set around a progress bar.\"\"\"\n        self.sep_start = start\n        self.sep_end = end"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_progress(self, count, symbol='#',\n                     color=None, on_color=None, attrs=None):\n        \"\"\"Add a section of progress to the progressbar.\n\n        The progress is captured by \"count\" and displayed as a fraction\n        of the statusbar width proportional to this count over the total\n        progress displayed. The progress will be displayed using the \"symbol\"\n        character and the foreground and background colours and display style\n        determined by the the \"color\", \"on_color\" and \"attrs\" parameters.\n        These parameters work as the termcolor.colored function.\n        \"\"\"\n        chunk = _ProgressChunk(count, symbol, color, on_color, attrs)\n        self._progress_chunks.append(chunk)", "response": "Add a section of progress to the progressbar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_progress(self, width):\n        chunk_widths = self._get_chunk_sizes(width)\n        progress_chunks = [chunk.format_chunk(chunk_width)\n                           for (chunk, chunk_width)\n                           in zip(self._progress_chunks, chunk_widths)]\n        return \"{sep_start}{progress}{sep_end}\".format(\n            sep_start=self.sep_start,\n            progress=\"\".join(progress_chunks),\n            sep_end=self.sep_end\n        )", "response": "Create the formatted string that displays the progress."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef summary_width(self):\n        chunk_counts = [chunk.count for chunk in self._progress_chunks]\n        numbers_width = sum(max(1, ceil(log10(count + 1)))\n                            for count in chunk_counts)\n        separators_with = len(chunk_counts) - 1\n        return numbers_width + separators_with", "response": "Calculate how long a string is needed to show a summary string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_summary(self):\n        chunks = [chunk.format_chunk_summary()\n                  for chunk in self._progress_chunks]\n        return \"/\".join(chunks)", "response": "Generate a summary string for the progress bar."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a section of progress to the progressbar.", "response": "def add_progress(self, count, symbol='#',\n                     color=None, on_color=None, attrs=None):\n        \"\"\"Add a section of progress to the progressbar.\n\n        The progress is captured by \"count\" and displayed as a fraction\n        of the statusbar width proportional to this count over the total\n        progress displayed. The progress will be displayed using the \"symbol\"\n        character and the foreground and background colours and display style\n        determined by the the \"fg\", \"bg\" and \"style\" parameters. For these,\n        use the colorama package to set up the formatting.\n        \"\"\"\n        self._progress.add_progress(count, symbol, color, on_color, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the formatted status bar string.", "response": "def format_status(self, width=None,\n                      label_width=None,\n                      progress_width=None,\n                      summary_width=None):\n        \"\"\"Generate the formatted status bar string.\"\"\"\n        if width is None:  # pragma: no cover\n            width = shutil.get_terminal_size()[0]\n\n        if label_width is None:\n            label_width = len(self.label)\n        if summary_width is None:\n            summary_width = self.summary_width()\n        if progress_width is None:\n            progress_width = width - label_width - summary_width - 2\n\n        if len(self.label) > label_width:\n            # FIXME: This actually *will* break if we ever have fewer than\n            # three characters assigned to format the label, but that would\n            # be an extreme situation so I won't fix it just yet.\n            label = self.label[:label_width - 3] + \"...\"\n        else:\n            label_format = \"{{label:{fill_char}<{width}}}\".format(\n                width=label_width,\n                fill_char=self.fill_char)\n            label = label_format.format(label=self.label)\n\n        summary_format = \"{{:>{width}}}\".format(width=summary_width)\n        summary = summary_format.format(self._progress.format_summary())\n\n        progress = self._progress.format_progress(width=progress_width)\n\n        return \"{label} {progress} {summary}\".format(\n            label=label,\n            progress=progress,\n            summary=summary\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_status_line(self, label):\n        status_line = StatusBar(label,\n                                self._sep_start, self._sep_end,\n                                self._fill_char)\n        self._lines.append(status_line)\n        return status_line", "response": "Add a status bar line to the table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating how wide each field should be so we can align them.", "response": "def calculate_field_widths(self, width=None,\n                               min_label_width=10,\n                               min_progress_width=10):\n        \"\"\"Calculate how wide each field should be so we can align them.\n\n        We always find room for the summaries since these are short and\n        packed with information. If possible, we will also find room for\n        labels, but if this would make the progress bar width shorter than\n        the specified minium then we will shorten the labels, though never\n        below the minium there. If this mean we have bars that are too wide\n        for the terminal, then your terminal needs to be wider.\n        \"\"\"\n        if width is None:  # pragma: no cover\n            width = shutil.get_terminal_size()[0]\n\n        summary_width = self.summary_width()\n        label_width = self.label_width()\n        remaining = width - summary_width - label_width - 2\n\n        if remaining >= min_progress_width:\n            progress_width = remaining\n        else:\n            progress_width = min_progress_width\n            remaining = width - summary_width - progress_width - 2\n            if remaining >= min_label_width:\n                label_width = remaining\n            else:\n                label_width = min_label_width\n\n        return (label_width, progress_width, summary_width)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_table(self, width=None,\n                     min_label_width=10, min_progress_width=10):\n        \"\"\"Format the entire table of progress bars.\n\n        The function first computes the widths of the fields so they can be\n        aligned across lines and then returns formatted lines as a list of\n        strings.\n        \"\"\"\n        # handle the special case of an empty table.\n        if len(self._lines) == 0:\n            return []\n\n        if width is None:  # pragma: no cover\n            width = shutil.get_terminal_size()[0]\n\n        labelw, progw, summaryw = self.calculate_field_widths(\n            width=width,\n            min_label_width=min_label_width,\n            min_progress_width=min_progress_width\n        )\n        output = [\n            sb.format_status(\n                label_width=labelw,\n                progress_width=progw,\n                summary_width=summaryw\n            )\n            for sb in self._lines\n        ]\n\n        return output", "response": "Format the entire table of progress bars."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_log_dict(request, response):\n    remote_addr = request.META.get('REMOTE_ADDR')\n    if remote_addr in getattr(settings, 'INTERNAL_IPS', []):\n        remote_addr = request.META.get(\n            'HTTP_X_FORWARDED_FOR') or remote_addr\n\n    user_email = \"-\"\n    if hasattr(request, 'user'):\n        user_email = getattr(request.user, 'email', '-')\n\n    if response.streaming:\n        content_length = 'streaming'\n    else:\n        content_length = len(response.content)\n\n    return {\n        # 'event' makes event-based filtering possible in logging backends\n        # like logstash\n        'event': settings.LOGUTILS_LOGGING_MIDDLEWARE_EVENT,\n        'remote_address': remote_addr,\n        'user_email': user_email,\n        'method': request.method,\n        'url': request.get_full_path(),\n        'status': response.status_code,\n        'content_length': content_length,\n        'request_time': -1,  # NA value: real value added by LoggingMiddleware\n    }", "response": "Create a dictionary with logging data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the logging message string.", "response": "def create_log_message(log_dict, use_sql_info=False, fmt=True):\n    \"\"\"\n    Create the logging message string.\n    \"\"\"\n    log_msg = (\n        \"%(remote_address)s %(user_email)s %(method)s %(url)s %(status)d \"\n        \"%(content_length)d (%(request_time).2f seconds)\"\n    )\n    if use_sql_info:\n        sql_time = sum(\n            float(q['time']) for q in connection.queries) * 1000\n        extra_log = {\n            'nr_queries': len(connection.queries),\n            'sql_time': sql_time}\n        log_msg += \" (%(nr_queries)d SQL queries, %(sql_time)f ms)\"\n        log_dict.update(extra_log)\n    return log_msg % log_dict if fmt else log_msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_response(self, request, response):\n        try:\n            log_dict = create_log_dict(request, response)\n\n            # add the request time to the log_dict; if no start time is\n            # available, use -1 as NA value\n            request_time = (\n                time.time() - self.start_time if hasattr(self, 'start_time')\n                and self.start_time else -1)\n            log_dict.update({'request_time': request_time})\n\n            is_request_time_too_high = (\n                request_time > float(settings.LOGUTILS_REQUEST_TIME_THRESHOLD))\n            use_sql_info = settings.DEBUG or is_request_time_too_high\n\n            log_msg = create_log_message(log_dict, use_sql_info, fmt=False)\n\n            if is_request_time_too_high:\n                logger.warning(log_msg, log_dict, extra=log_dict)\n            else:\n                logger.info(log_msg, log_dict, extra=log_dict)\n        except Exception as e:\n            logger.exception(e)\n\n        return response", "response": "Create the logging message.."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef synchronized(obj):\n\n  if hasattr(obj, 'synchronizable_condition'):\n    return obj.synchronizable_condition\n  elif callable(obj):\n    @functools.wraps(obj)\n    def wrapper(self, *args, **kwargs):\n      with self.synchronizable_condition:\n        return obj(self, *args, **kwargs)\n    return wrapper\n  else:\n    raise TypeError('expected Synchronizable instance or callable to decorate')", "response": "This function is used to synchronize access to the object passed as the first argument."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits until the object is notified with the given function.", "response": "def wait(obj, timeout=None):\n  \"\"\"\n  Wait until *obj* gets notified with #notify() or #notify_all(). If a timeout\n  is specified, the function can return without the object being notified if\n  the time runs out.\n\n  Note that you can only use this function on #synchronized() objects.\n\n  # Arguments\n  obj (Synchronizable): An object that can be synchronized.\n  timeout (number, None): The number of seconds to wait for the object to get\n    notified before returning. If not value or the value #None is specified,\n    the function will wait indefinetily.\n  \"\"\"\n\n  if timeout is None:\n    return obj.synchronizable_condition.wait()\n  else:\n    return obj.synchronizable_condition.wait(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_condition(obj, cond, timeout=None):\n\n  with synchronized(obj):\n    if timeout is None:\n      while not cond(obj):\n        wait(obj)\n    else:\n      t_start = time.time()\n      while not cond(obj):\n        t_delta = time.time() - t_start\n        if t_delta >= timeout:\n          return False\n        wait(obj, timeout - t_delta)\n  return True", "response": "This function waits for a condition to be met."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_completed(jobs):\n  ''' Generator function that yields the jobs in order of their\n  completion. Attaches a new listener to each job. '''\n\n  jobs = tuple(jobs)\n  event = threading.Event()\n  callback = lambda f, ev: event.set()\n  [job.add_listener(Job.SUCCESS, callback, once=True) for job in jobs]\n  [job.add_listener(Job.ERROR, callback, once=True) for job in jobs]\n\n  while jobs:\n    event.wait()\n    event.clear()\n\n    jobs, finished = split_list_by(jobs, lambda x: x.finished)\n    for job in finished:\n      yield job", "response": "Generator function that yields the jobs in order of their\n  completion. Attaches a new listener to each job."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_list_by(lst, key):\n\n  first, second = [], []\n  for item in lst:\n    if key(item):\n      second.append(item)\n    else:\n      first.append(item)\n  return (first, second)", "response": "Splits a list by a callable key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the result of the job execution.", "response": "def result(self):\n    \"\"\"\n    The result of the jobs execution. Accessing this property while the job is\n    pending or running will raise #InvalidState. If an exception occured during\n    the jobs execution, it will be raised.\n\n    # Raises\n    InvalidState: If the job is not in state #FINISHED.\n    Cancelled: If the job was cancelled.\n    any: If an exception ocurred during the job's execution.\n    \"\"\"\n\n    if self.__cancelled:\n      raise Job.Cancelled\n    elif self.__state in (Job.PENDING, Job.RUNNING):\n      raise Job.InvalidState('job is {0}'.format(self.__state))\n    elif self.__state == Job.ERROR:\n      reraise(*self.__exception)\n    elif self.__state == Job.SUCCESS:\n      return self.__result\n    else:\n      raise RuntimeError('invalid job state {0!r}'.format(self.__state))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exception(self):\n\n    if self.__state in (Job.PENDING, Job.RUNNING):\n      raise self.InvalidState('job is {0}'.format(self.__state))\n    elif self.__state == Job.ERROR:\n      assert self.__exception is not None\n      return self.__exception\n    elif self.__state in (Job.RUNNING, Job.SUCCESS, Job.CANCELLED):\n      assert self.__exception is None\n      return None\n    else:\n      raise RuntimeError('invalid job state {0!r}'.format(self.__state))", "response": "Returns the exception that occurred while the job executed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finished(self):\n\n    return self.__state in (Job.ERROR, Job.SUCCESS, Job.CANCELLED)", "response": "True if the job run and finished."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the result of the job or return default if the job is not finished or errored.", "response": "def get(self, default=None):\n    \"\"\"\n    Get the result of the Job, or return *default* if the job is not finished\n    or errored. This function will never explicitly raise an exception. Note\n    that the *default* value is also returned if the job was cancelled.\n\n    # Arguments\n    default (any): The value to return when the result can not be obtained.\n    \"\"\"\n\n    if not self.__cancelled and self.__state == Job.SUCCESS:\n      return self.__result\n    else:\n      return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncancels the job. Functions should check the #Job.cancelled flag from time to time to be able to abort pre-emptively if the job was cancelled instead of running forever.", "response": "def cancel(self):\n    \"\"\"\n    Cancels the job. Functions should check the #Job.cancelled flag from time\n    to time to be able to abort pre-emptively if the job was cancelled instead\n    of running forever.\n    \"\"\"\n\n    with synchronized(self):\n      cancelled = self.__cancelled\n      if not cancelled:\n        self.__cancelled = True\n        notify_all(self)\n\n    if not cancelled:\n      self._trigger_event(Job.CANCELLED)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _trigger_event(self, event):\n\n    if event is None or event not in self.__listeners:\n      raise ValueError('invalid event type: {0!r}'.format(event))\n\n    # Check the event has not already been triggered, then mark\n    # the event as triggered.\n    if event in self.__event_set:\n      raise RuntimeError('event already triggered: {0!r}'.format(event))\n    self.__event_set.add(event)\n    listeners = self.__listeners[event] + self.__listeners[None]\n\n    # Remove one-off listeners.\n    self.__listeners[event][:] = (l for l in self.__listeners[event] if not l.once)\n    self.__listeners[None][:] = (l for l in self.__listeners[None] if not l.once)\n\n    for listener in listeners:\n      # XXX: What to do on exceptions? Catch and make sure all listeners\n      # run through? What to do with the exception(s) then?\n      listener.callback(self, event)", "response": "Private. Triggers and event and removes all one - off listeners for that event."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a listener for the specified event.", "response": "def add_listener(self, event, callback, once=False):\n    \"\"\"\n    Register a *callback* for the specified *event*. The function will be\n    called with the #Job as its first argument. If *once* is #True, the\n    listener will be removed after it has been invoked once or when the\n    job is re-started.\n\n    Note that if the event already ocurred, *callback* will be called\n    immediately!\n\n    # Arguments\n    event (str, list of str): The name or multiple names of an event, or None\n      to register the callback to be called for any event.\n    callback (callable): A function.\n    once (bool): Whether the callback is valid only once.\n    \"\"\"\n\n    if not callable(callback):\n      raise TypeError('callback must be callable')\n    if isinstance(event, str):\n      event = [event]\n    for evn in event:\n      if evn not in self.__listeners:\n        raise ValueError('invalid event type: {0!r}'.format(evn))\n    for evn in event:\n      event_passed = False\n      with synchronized(self):\n        event_passed = (evn in self.__event_set)\n        if not (once and event_passed):\n          self.__listeners[evn].append(Job._Listener(callback, once))\n\n    # If the event already happened, we'll invoke the callback\n    # immediately to make up for what it missed.\n    if event_passed:\n      callback(self, event)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait(self, timeout=None):\n\n    def cond(self):\n      return self.__state not in (Job.PENDING, Job.RUNNING) or self.__cancelled\n    if not wait_for_condition(self, cond, timeout):\n      raise Job.Timeout\n    return self.result", "response": "Waits for the job to finish and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self, as_thread=True, daemon=False, __state_check=True):\n\n    if __state_check:\n      # We need to manually manage the lock to be able to release it\n      # pre-emptively when needed.\n      with synchronized(self):\n        if self.__cancelled and self.__state == Job.PENDING:\n          # Cancelled in PENDING state. Do not run the target function at all.\n          self.__state = Job.CANCELLED\n          assert self.__exception is None\n          assert self.__result is None\n          self._trigger_event(Job.CANCELLED)\n          return None\n\n        if self.__state == Job.RUNNING:\n          raise Job.InvalidState('job is already running')\n        elif self.__state not in (Job.PENDING, Job.ERROR, Job.SUCCESS, Job.CANCELLED):\n          raise RuntimeError('invalid job state {0!r}'.format(self.__state))\n\n        # Reset the Job attributes.\n        self.__state = Job.RUNNING\n        self.__cancelled = False\n        self.__result = None\n        self.__exception = None\n        self.__event_set.clear()\n        self.__thread = None\n\n        # Remove all listeners that have been registered with the \"once\" flag.\n        for listeners in self.__listeners.values():\n          listeners[:] = (l for l in listeners if not l.once)\n\n    if as_thread:\n      thread = threading.Thread(target=self.start, args=(False, False, False))\n      thread.setDaemon(daemon)\n      with synchronized(self):\n        assert not self.__thread or not self.__thread.running\n        self.__thread = thread\n      thread.start()\n      return self\n\n    try:\n      result = None\n      exception = None\n      try:\n        result = self.run()\n        state = Job.SUCCESS\n      except Exception:  # XXX: Catch BaseException?\n        if self.print_exc:\n          traceback.print_exc()\n        exception = Job.ExceptionInfo(*sys.exc_info())\n        state = Job.ERROR\n\n      with synchronized(self):\n        cancelled = self.__cancelled\n        self.__result = result\n        self.__exception = exception\n        self.__state = Job.CANCELLED if cancelled else state\n\n      self._trigger_event(state)\n    finally:\n      with synchronized(self):\n        notify_all(self)\n      if self.__dispose_inputs:\n        self.__target = None\n        self.__args = None\n        self.__kwargs = None\n        self.data = None\n        for listeners in self.__listeners.values():\n          listeners[:] = []\n\n    return self", "response": "Starts the job in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n\n    if self.__target is not None:\n      return self.__target(self, *self.__args, **self.__kwargs)\n    raise NotImplementedError", "response": "This method is the actual implementation of the job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(self):\n\n    if self.__running:\n      raise RuntimeError('ThreadPool already running')\n    [t.start() for t in self.__threads]\n    self.__running = True", "response": "Starts the thread pool."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef current_jobs(self):\n\n    jobs = []\n    with synchronized(self.__queue):\n      for worker in self.__threads:\n        with synchronized(worker):\n          if worker.current:\n            jobs.append(worker.current)\n    return jobs", "response": "Returns a snapshot of the Jobs that are currently being processed by the current ThreadPool."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear(self):\n\n    with synchronized(self.__queue):\n      jobs = self.__queue.snapshot()\n      self.__queue.clear()\n    return jobs", "response": "Removes all pending Jobs from the queue and returns them in a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel_all(self, cancel_current=True):\n\n    with synchronized(self.__queue):\n      jobs = self.clear()\n      if cancel_current:\n        jobs.extend(self.current_jobs())\n\n    [j.cancel() for j in jobs]\n    return jobs", "response": "Cancels all jobs in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef submit(self, target=None, task=None, args=(), kwargs=None, front=False,\n             dispose_inputs=None):\n    \"\"\"\n    Submit a new #Job to the ThreadPool.\n\n    # Arguments\n    task (function, Job): Either a function that accepts a #Job, *args* and\n      *kwargs* or a #Job object that is in #~Job.PENDING state.\n    target (function): A function object that accepts *args* and *kwargs*.\n      Only if *task* is not specified.\n    args (list, tuple): A list of arguments to be passed to *job*, if it is\n      a function.\n    kwargs (dict): A dictionary to be passed as keyword arguments to *job*,\n      if it is a function.\n    front (bool): If #True, the job will be inserted in the front of the queue.\n\n    # Returns\n    Job: The job that was added to the queue.\n\n    # Raises\n    TypeError: If a #Job object was passed but *args* or *kwargs* are non-empty.\n    RuntimeError: If the ThreadPool is not running (ie. if it was shut down).\n    \"\"\"\n\n    if not self.__running:\n      raise RuntimeError(\"ThreadPool ain't running\")\n    if dispose_inputs is None:\n      dispose_inputs = self.dispose_inputs\n\n    if isinstance(task, Job):\n      if args or kwargs:\n        raise TypeError('can not provide additional arguments for Job')\n      if task.state != Job.PENDING:\n        raise RuntimeError('job is not pending')\n      job = task\n    elif task is not None:\n      if kwargs is None:\n        kwargs = {}\n      job = Job(task=task, args=args, kwargs=kwargs, dispose_inputs=dispose_inputs)\n    elif target is not None:\n      if kwargs is None:\n        kwargs = {}\n      job = Job(target=target, args=args, kwargs=kwargs, dispose_inputs=dispose_inputs)\n    else:\n      raise TypeError('expected Job or callable')\n\n    job.print_exc = self.print_exc\n    if front:\n      self.__queue.appendleft(job)\n    else:\n      self.__queue.append(job)\n    return job", "response": "Submit a new job to the ThreadPool."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nblocking until all jobs in the ThreadPool are finished.", "response": "def wait(self, timeout=None):\n    \"\"\"\n    Block until all jobs in the ThreadPool are finished. Beware that this can\n    make the program run into a deadlock if another thread adds new jobs to the\n    pool!\n\n    # Raises\n    Timeout: If the timeout is exceeded.\n    \"\"\"\n\n    if not self.__running:\n      raise RuntimeError(\"ThreadPool ain't running\")\n    self.__queue.wait(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nshut down the ThreadPool.", "response": "def shutdown(self, wait=True):\n    \"\"\"\n    Shut down the ThreadPool.\n\n    # Arguments\n    wait (bool): If #True, wait until all worker threads end. Note that pending\n      jobs are still executed. If you want to cancel any pending jobs, use the\n      #clear() or #cancel_all() methods.\n    \"\"\"\n\n    if self.__running:\n      # Add a Non-entry for every worker thread we have.\n      for thread in self.__threads:\n        assert thread.isAlive()\n        self.__queue.append(None)\n      self.__running = False\n\n    if wait:\n      self.__queue.wait()\n      for thread in self.__threads:\n        thread.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit a job for each element in function and returns a JobCollection.", "response": "def submit_multiple(self, functions, target=False, task=False):\n    \"\"\"\n    Submits a #Job for each element in *function* and returns a #JobCollection.\n    \"\"\"\n\n    if target or not task:\n      return JobCollection([self.submit(target=func) for func in functions])\n    else:\n      return JobCollection([self.submit(task=func) for func in functions])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_event_type(self, name, mergeable=False):\n    ''' Declare a new event. May overwrite an existing entry. '''\n\n    self.event_types[name] = self.EventType(name, mergeable)", "response": "Declare a new event type. May overwrite existing entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_event(self, name, data=None):\n    '''\n    Add an event of type *name* to the queue. May raise a\n    `ValueError` if the event type is mergeable and *data* is not None\n    or if *name* is not a declared event type (in strict mode).\n    '''\n\n    try:\n      mergeable = self.event_types[name].mergeable\n    except KeyError:\n      if self.strict:\n        raise ValueError('unknown event type {0!r}'.format(name))\n      mergeable = False\n\n    if mergeable and data is not None:\n      raise ValueError('mergable event can not have data attached')\n\n    with self.lock:\n      if mergeable:\n        # Check if such an event already exists.\n        for ev in self.events:\n          if ev.type == name:\n            return\n\n      self.events.append(self.Event(name, data, time.clock()))", "response": "Add an event of type name to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npopping the next event from the queue.", "response": "def pop_event(self):\n    '''\n    Pop the next queued event from the queue.\n\n    :raise ValueError: If there is no event queued.\n    '''\n\n    with self.lock:\n      if not self.events:\n        raise ValueError('no events queued')\n      return self.events.popleft()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pop_events(self):\n    '''\n    Pop all events and return a `collections.deque` object. The\n    returned container can be empty. This method is preferred over\n    `pop_event()` as it is much faster as the lock has to be acquired\n    only once and also avoids running into an infinite loop during\n    event processing.\n    '''\n\n    with self.lock:\n      events = self.events\n      self.events = collections.deque()\n      return events", "response": "Pop all events and return a collections. deque object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self):\n\n    self._tasks -= len(self._deque)\n    self._deque.clear()\n    notify_all(self)", "response": "Clears the queue and notifies all waiting tasks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, block=True, timeout=None, method='pop'):\n\n    if method not in ('pop', 'popleft'):\n      raise ValueError('method must be \"pop\" or \"popleft\": {0!r}'.format(method))\n\n    t_start = time.clock()\n    while not self:\n      if not block:\n        raise self.Empty\n\n      if timeout is None:\n        wait(self)\n      else:\n        t_delta = time.clock() - t_start\n        if t_delta > timeout:\n          raise Timeout\n        wait(self, timeout - t_delta)\n\n    return getattr(self, method)()", "response": "Get an element from the deque."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait until all tasks have completed or the specified timeout has passed.", "response": "def wait(self, timeout=None):\n    \"\"\"\n    Waits until all tasks completed or *timeout* seconds passed.\n\n    # Raises\n    Timeout: If the *timeout* is exceeded.\n    \"\"\"\n\n    t_start = time.clock()\n    if not wait_for_condition(self, lambda s: s._tasks == 0, timeout):\n      raise Timeout"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsleep until the interval has passed since the last time this function was called.", "response": "def sleep(self):\n    \"\"\"\n    Sleeps until the interval has passed since the last time this function was\n    called. This is a synonym for #__call__(). The first time the function is\n    called will return immediately and not block. Therefore, it is important to\n    put the call at the beginning of the timed block, like this:\n\n    # Example\n    ```python\n    clock = Clock(fps=50)\n    while True:\n      clock.sleep()\n      # Processing ...\n    ```\n    \"\"\"\n\n    current = time.time()\n    if self.last < 0:\n      self.last = current\n      return\n\n    delta = current - self.last\n    if delta < self.seconds:\n      time.sleep(self.seconds - delta)\n      self.last = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_config(desired_type: Type[ConfigParser], file_object: TextIOBase,\n                logger: Logger, *args, **kwargs) -> ConfigParser:\n    \"\"\"\n    Helper method to read a configuration file according to the 'configparser' format, and return it as a dictionary\n    of dictionaries (section > [property > value])\n\n    :param file_object:\n    :return:\n    \"\"\"\n\n    # see https://docs.python.org/3/library/configparser.html for details\n    config = ConfigParser()\n    config.read_file(file_object)\n\n    return config", "response": "Read a configuration file according to the configparser format and return it as a dictionary\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef config_to_dict_of_dict(desired_type: Type[T], config: ConfigParser, logger: Logger,\n                           conversion_finder: ConversionFinder, **kwargs) -> DictOfDict:\n    \"\"\"\n    Helper method to read a configuration file according to the 'configparser' format, and return it as a dictionary\n    of dictionaries [section > [property > value]].\n\n    :param file_object:\n    :return:\n    \"\"\"\n    # return dict(config)\n\n    # get the base collection type if provided\n    base_typ, discarded = _extract_collection_base_type(desired_type, exception_if_none=False)\n    # if none, at least declare dict\n    base_typ = base_typ or Dict\n\n    # convert the whole config to a dictionary by flattening all sections. If a key is found twice in two different\n    # sections an error is raised\n    results = dict()\n    for section, props in config.items():\n        # convert all values of the sub-dictionary\n        results[section] = ConversionFinder.convert_collection_values_according_to_pep(props, base_typ,\n                                                                                       conversion_finder, logger,\n                                                                                       **kwargs)\n\n    return results", "response": "Convert a config file according to the configparser format and return a dictionary of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config_converters(conv_finder: ConversionFinder) -> List[Union[Converter[Any, ConfigParser], Converter[ConfigParser, Any]]]:\n    return [ConverterFunction(ConfigParser, DictOfDict, config_to_dict_of_dict, custom_name='config_to_dict_of_dict',\n                              function_args={'conversion_finder': conv_finder}),\n            ConverterFunction(ConfigParser, dict, merge_all_config_sections_into_a_single_dict,\n                              custom_name='merge_all_config_sections_into_a_single_dict',\n                              function_args={'conversion_finder': conv_finder})]", "response": "Utility method to return the default converters associated to ConfigParser."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(key_name: str, sections: List[str]): # -> NoParserFoundForObject:\n        return MultipleKeyOccurenceInConfigurationError('Cannot read the provided config file as a flat dictionary : '\n                                                        'key \\'' + key_name + '\\' appears several times, in sections'\n                                                                              '\\'' + str(sections) + '\\'.')", "response": "Helper method provided because we can t read the config file as a flat dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget Tautulli session data.", "response": "async def get_session_data(self):\n        \"\"\"Get Tautulli sessions.\"\"\"\n        cmd = 'get_activity'\n        url = self.base_url + cmd\n        try:\n            async with async_timeout.timeout(8, loop=self._loop):\n                response = await self._session.get(url)\n\n            logger(\"Status from Tautulli: \" + str(response.status))\n            self.tautulli_session_data = await response.json()\n            logger(self.tautulli_session_data)\n\n        except (asyncio.TimeoutError, aiohttp.ClientError, socket.gaierror,\n                AttributeError) as error:\n            msg = \"Can not load data from Tautulli: {} - {}\".format(url, error)\n            logger(msg, 40)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting Tautulli home stats.", "response": "async def get_home_data(self):\n        \"\"\"Get Tautulli home stats.\"\"\"\n        cmd = 'get_home_stats'\n        url = self.base_url + cmd\n        data = {}\n        try:\n            async with async_timeout.timeout(8, loop=self._loop):\n                request = await self._session.get(url)\n                response = await request.json()\n                for stat in response.get('response', {}).get('data', {}):\n                    if stat.get('stat_id') == 'top_movies':\n                        try:\n                            row = stat.get('rows', {})[0]\n                            data['movie'] = row.get('title')\n                        except (IndexError, KeyError):\n                            data['movie'] = None\n                    if stat.get('stat_id') == 'top_tv':\n                        try:\n                            row = stat.get('rows', {})[0]\n                            data['tv'] = row.get('title')\n                        except (IndexError, KeyError):\n                            data['tv'] = None\n                    if stat.get('stat_id') == 'top_users':\n                        try:\n                            row = stat.get('rows', {})[0]\n                            data['user'] = row.get('user')\n                        except (IndexError, KeyError):\n                            data['user'] = None\n            logger(\"Status from Tautulli: \" + str(request.status))\n            self.tautulli_home_data = data\n            logger(self.tautulli_home_data)\n\n        except (asyncio.TimeoutError, aiohttp.ClientError, socket.gaierror,\n                AttributeError) as error:\n            msg = \"Can not load data from Tautulli: {} - {}\".format(url, error)\n            logger(msg, 40)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport a string. Can import an attribute of the imported class or module.", "response": "def import_string(impstr, attr=None):\n    \"\"\"Imports a string. Can import an attribute of the imported\n    class/module using a double colon as a separator\n    \"\"\"\n    if \"::\" in impstr:\n        impstr, attr = impstr.split(\"::\")\n    imported = wz_import_string(impstr)\n    if attr is not None:\n        return getobjpath(imported, attr)\n    return imported"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getobjpath(obj, path):\n    if not path:\n        return obj\n    if path.startswith(\"[\"):\n        item = path[1:path.index(\"]\")]\n        return getobjpath(obj[item], path[len(item) + 2:])\n    if path.startswith(\".\"):\n        path = path[1:]\n    if \".\" in path or \"[\" in path:\n        dot_idx = path.find(\".\")\n        bracket_idx = path.find(\"[\")\n        if dot_idx == -1 or bracket_idx < dot_idx:\n            idx = bracket_idx\n            next_idx = idx\n        else:\n            idx = dot_idx\n            next_idx = idx + 1\n        attr = path[:idx]\n        return getobjpath(getattr(obj, attr), path[next_idx:])\n    return getattr(obj, path)", "response": "Returns an item or attribute of the object recursively."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds classes of clstypes in module", "response": "def find_classes_in_module(module, clstypes):\n    \"\"\"Find classes of clstypes in module\n    \"\"\"\n    classes = []\n    for item in dir(module):\n        item = getattr(module, item)\n        try:\n            for cls in clstypes:\n                if issubclass(item, cls) and item != cls:\n                    classes.append(item)\n        except Exception as e:\n            pass\n    return classes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_yaml_frontmatter(source, return_frontmatter=False):\n    if source.startswith(\"---\\n\"):\n        frontmatter_end = source.find(\"\\n---\\n\", 4)\n        if frontmatter_end == -1:\n            frontmatter = source\n            source = \"\"\n        else:\n            frontmatter = source[0:frontmatter_end]\n            source = source[frontmatter_end + 5:]\n        if return_frontmatter:\n            return (source, frontmatter)\n        return source\n    if return_frontmatter:\n        return (source, None)\n    return source", "response": "Removes the YAML front - matter from the source if there s one."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef populate_obj(obj, attrs):\n    for k, v in attrs.iteritems():\n        setattr(obj, k, v)", "response": "Populates an object s attributes using the provided dict\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert_element_to_dict_of_dicts(dict_of_dicts: Dict[str, Dict[str, str]], first_key: str, second_key: str, contents):\n\n    if first_key not in dict_of_dicts.keys():\n        dict_of_dicts[first_key] = {second_key: contents}\n    else:\n        if second_key not in dict_of_dicts[first_key].keys():\n            dict_of_dicts[first_key][second_key] = contents\n        else:\n            warn('Overriding contents for ' + first_key + '/' + second_key)\n            dict_of_dicts[first_key][second_key] = contents", "response": "Utility method to insert element into dict_of_dicts."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds a parser for the given object obj_on_filesystem and object_type.", "response": "def build_parser_for_fileobject_and_desiredtype(self, obj_on_filesystem: PersistedObject, object_type: Type[T],\n                                                    logger: Logger = None) -> Parser:\n        \"\"\"\n        Returns the most appropriate parser to use to parse object obj_on_filesystem as an object of type object_type\n\n        :param obj_on_filesystem: the filesystem object to parse\n        :param object_type: the type of object that the parser is expected to produce\n        :param logger:\n        :return:\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(obj: PersistedObject, obj_type: Type[T], extensions_supported: Iterable[str]):\n\n        # base message\n        msg = \"{obj} cannot be parsed as a {typ} because no parser supporting that extension ({ext}) is able to \" \\\n              \"create this type of object.\" \\\n              \"\".format(obj=obj, typ=get_pretty_type_str(obj_type), ext=obj.get_pretty_file_ext())\n\n        # add details\n        if extensions_supported is not None and len(extensions_supported) > 0:\n            msg += \" If you wish to parse this fileobject to that precise type, you may wish to either \" \\\n                   \"(1) replace the file with any of the following extensions currently supported : {exts} \" \\\n                   \"(see get_capabilities_for_type({typ}, strict_type_matching=False) for details).\" \\\n                   \" Or (2) register a new parser.\" \\\n                   \"\".format(exts=extensions_supported, typ=get_pretty_type_str(obj_type))\n        else:\n            raise ValueError('extensions_supported should be provided to create a NoParserFoundForObjectExt. If no '\n                             'extension is supported, use NoParserFoundForObjectType.create instead')\n\n        e = NoParserFoundForObjectExt(msg)\n\n        # save the extensions supported\n        e.extensions_supported = extensions_supported\n\n        return e", "response": "Create a NoParserFoundForObjectType instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a NoParserFoundForObjectType instance.", "response": "def create(obj: PersistedObject, obj_type: Type[T], types_supported: Iterable[str]):\n        \"\"\"\n        Helper method provided because we actually can't put that in the constructor, it creates a bug in Nose tests\n        https://github.com/nose-devs/nose/issues/725\n\n        :param obj:\n        :param obj_type:\n        :param types_supported:\n        :return:\n        \"\"\"\n\n        # base message\n        msg = str(obj) + ' cannot be parsed as a ' + get_pretty_type_str(obj_type) + ' because no parser supporting ' \\\n              'that type is registered for ' + obj.get_pretty_file_ext() + '.\\n'\n\n        # add details\n        if types_supported is not None and len(types_supported) > 0:\n            msg += ' If you wish to parse this object from this extension, you may wish to parse it as one of the ' \\\n                   'following supported types : ' + str(types_supported) + '. \\n' \\\n                   + 'Otherwise, please register a new parser for type ' + get_pretty_type_str(obj_type) \\\n                   + ' and extension ' + obj.get_pretty_file_ext() + '\\n Reminder: use print_capabilities_by_ext()' \\\n                   + ' and print_capabilities_by_type() to diagnose what are the parsers available'\n        else:\n            raise ValueError('extensions_supported should be provided to create a NoParserFoundForObjectExt. If no '\n                             'extension is supported, use NoParserFoundForObjectType.create instead')\n\n        e = NoParserFoundForObjectType(msg)\n\n        # save the extensions supported\n        e.types_supported = types_supported\n\n        return e"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_capabilities_by_ext(self, strict_type_matching: bool = False):\n        print('\\nCapabilities by file extension: ')\n        l = self.get_capabilities_by_ext(strict_type_matching=strict_type_matching)\n        pprint({ext: get_pretty_type_keys_dict(parsers) for ext, parsers in l.items()})\n        print('\\n')", "response": "Print the list of all available file extensions that can be parsed by this parser registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_capabilities_by_type(self, strict_type_matching: bool = False):\n        print('\\nCapabilities by object type: ')\n        l = self.get_capabilities_by_type(strict_type_matching=strict_type_matching)\n        pprint({get_pretty_type_str(typ): parsers for typ, parsers in l.items()})\n        print('\\n')", "response": "Print the list of all file extensions that can be parsed by this parser registry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary of all available types and parsers for all extensions that can be parsed into that type.", "response": "def get_capabilities_by_type(self, strict_type_matching: bool = False) -> Dict[Type, Dict[str, Dict[str, Parser]]]:\n        \"\"\"\n        For all types that are supported,\n        lists all extensions that can be parsed into such a type.\n        For each extension, provides the list of parsers supported. The order is \"most pertinent first\"\n\n        This method is for monitoring and debug, so we prefer to not rely on the cache, but rather on the query engine.\n        That will ensure consistency of the results.\n\n        :param strict_type_matching:\n        :return:\n        \"\"\"\n\n        check_var(strict_type_matching, var_types=bool, var_name='strict_matching')\n\n        res = dict()\n\n        # List all types that can be parsed\n        for typ in self.get_all_supported_types():\n            res[typ] = self.get_capabilities_for_type(typ, strict_type_matching)\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_capabilities_by_ext(self, strict_type_matching: bool = False) -> Dict[str, Dict[Type, Dict[str, Parser]]]:\n        check_var(strict_type_matching, var_types=bool, var_name='strict_matching')\n        res = dict()\n\n        # For all extensions that are supported,\n        for ext in self.get_all_supported_exts_for_type(type_to_match=JOKER, strict=strict_type_matching):\n            res[ext] = self.get_capabilities_for_ext(ext, strict_type_matching)\n\n        return res", "response": "Get all the available extensions and parsers for this extension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_parser(self, parser: Parser):\n        check_var(parser, var_types=Parser, var_name='parser')\n        if (not parser.supports_multifile()) and (not parser.supports_singlefile()):\n            # invalid\n            raise _InvalidParserException.create(parser)\n\n        # (0) sanity check : check that parser handles jokers properly\n        res = parser.is_able_to_parse_detailed(desired_type=JOKER, desired_ext=JOKER, strict=True)\n        if not (res[0] is True and res[1] is None):\n            raise ValueError('Parser ' + str(parser) + ' can not be registered since it does not handle the JOKER cases '\n                             'correctly')\n\n        # (1) store in the main lists\n        if parser.is_generic():\n            self._generic_parsers.append(parser)\n        else:\n            self._specific_parsers.append(parser)\n\n        # (2) simpler : simply store the ext <> type maps\n        for ext in parser.supported_exts:\n            for typ in parser.supported_types:\n                insert_element_to_dict_of_list(self._strict_types_to_ext, typ, ext)\n                insert_element_to_dict_of_list(self._ext_to_strict_types, ext, typ)", "response": "Utility method to register any parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the list of all parsers in order of relevance.", "response": "def get_all_parsers(self, strict_type_matching: bool = False) -> List[Parser]:\n        \"\"\"\n        Returns the list of all parsers in order of relevance.\n        :return:\n        \"\"\"\n        matching = self.find_all_matching_parsers(strict=strict_type_matching)[0]\n\n        # matching[1] (approx match) is supposed to be empty since we use a joker on type and a joker on ext : only\n        # exact and generic match should exist, no approx match\n        if len(matching[1]) > 0:\n            raise Exception('Internal error - this matching[1] list is supposed to be empty for such a query')\n        return matching[0] + matching[2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_all_supported_types_for_ext(self, ext_to_match: str, strict_type_matching: bool = False) -> Set[Type]:\n        matching = self.find_all_matching_parsers(required_ext=ext_to_match, strict=strict_type_matching)[0]\n        return {typ for types in [p.supported_types for p in (matching[0] + matching[1] + matching[2])]\n                for typ in types}", "response": "Utility method to return the set of all supported types that may be parsed from files with the given extension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a parsing plan for the given type and filesystem object.", "response": "def _create_parsing_plan(self, desired_type: Type[T], filesystem_object: PersistedObject, logger: Logger,\n                             log_only_last: bool = False) -> ParsingPlan[T]:\n        \"\"\"\n        Implementation of Parser API\n        Relies on the underlying registry of parsers to provide the best parsing plan\n        :param desired_type:\n        :param filesystem_object:\n        :param logger:\n        :param log_only_last: a flag to only log the last part of the file path (default False)\n        :return:\n        \"\"\"\n        # find the parser for this object\n        t, combined_parser = self.build_parser_for_fileobject_and_desiredtype(filesystem_object, desired_type,\n                                                                           logger=logger)\n        # ask the parser for the parsing plan\n        return combined_parser.create_parsing_plan(t, filesystem_object, logger)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a parser for a given object obj_on_filesystem and object_type.", "response": "def build_parser_for_fileobject_and_desiredtype(self, obj_on_filesystem: PersistedObject, object_type: Type[T],\n                                                    logger: Logger = None) -> Tuple[Type, Parser]:\n        \"\"\"\n        Builds from the registry, a parser to parse object obj_on_filesystem as an object of type object_type.\n\n        To do that, it iterates through all registered parsers in the list in reverse order (last inserted first),\n        and checks if they support the provided object format (single or multifile) and type.\n        If several parsers match, it returns a cascadingparser that will try them in order.\n\n        If several alternatives are requested (through a root Union type), this is done independently for each\n        alternative.\n\n        :param obj_on_filesystem:\n        :param object_type:\n        :param logger:\n        :return: a type to use and a parser. The type to use is either directly the one provided, or a resolved one in\n        case of TypeVar\n        \"\"\"\n        # First resolve TypeVars and Unions to get a list of compliant types\n        object_types = get_alternate_types_resolving_forwardref_union_and_typevar(object_type)\n\n        if len(object_types) == 1:\n            # One type: proceed as usual\n            parsers = self._build_parser_for_fileobject_and_desiredtype(obj_on_filesystem, object_typ=object_types[0],\n                                                                        logger=logger)\n            if len(parsers) > 1:\n                return object_types[0], CascadingParser(parsers)\n            else:\n                return next(iter(parsers.items()))\n        else:\n            # Several alternate types are supported: try to build a parser for each\n            parsers = OrderedDict()\n            errors = OrderedDict()\n            for typ in object_types:\n                try:\n                    parsers.update(self._build_parser_for_fileobject_and_desiredtype(obj_on_filesystem, object_typ=typ,\n                                                                                     logger=logger))\n                except NoParserFoundForObjectExt as e:\n                    logger.warning(\"{} - {}\".format(type(e).__name__, e))\n                    errors[e] = e\n                except NoParserFoundForObjectType as f:\n                    logger.warning(\"{} - {}\".format(type(f).__name__, f))\n                    errors[f] = f\n\n            # Combine if there are remaining, otherwise raise\n            if len(parsers) > 0:\n                return object_type, CascadingParser(parsers)\n            else:\n                raise NoParserFoundForUnionType.create(obj_on_filesystem, object_type, errors)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a parser for the given object_typ and object_typ.", "response": "def _build_parser_for_fileobject_and_desiredtype(self, obj_on_filesystem: PersistedObject, object_typ: Type[T],\n                                                     logger: Logger = None) -> Dict[Type, Parser]:\n        \"\"\"\n        Builds a parser for each subtype of object_typ\n\n        :param obj_on_filesystem:\n        :param object_typ:\n        :param logger:\n        :return:\n        \"\"\"\n\n        parsers = OrderedDict()\n        errors = OrderedDict()\n        try:\n            p = self.__build_parser_for_fileobject_and_desiredtype(obj_on_filesystem,\n                                                                   object_typ=object_typ,\n                                                                   logger=logger)\n            parsers[object_typ] = p\n        except NoParserFoundForObjectExt as e:\n            logger.warning(\"{} - {}\".format(type(e).__name__, e))\n            errors[e] = e\n        except NoParserFoundForObjectType as f:\n            logger.warning(\"{} - {}\".format(type(f).__name__, f))\n            errors[f] = f\n\n        # do not explore subclasses for collections\n        if is_collection(object_typ, strict=True):\n            if len(errors) > 0:\n                raise next(iter(errors.values()))\n            else:\n                return parsers\n\n        # Finally create one such parser for each subclass\n        subclasses = get_all_subclasses(object_typ)\n\n        # Then for each subclass also try (with a configurable limit in nb of subclasses)\n        for subclass in subclasses[0:GLOBAL_CONFIG.dict_to_object_subclass_limit]:\n            try:\n                parsers[subclass] = self.__build_parser_for_fileobject_and_desiredtype(obj_on_filesystem,\n                                                                                       object_typ=subclass,\n                                                                                       logger=logger)\n            except NoParserFoundForObjectExt as e:\n                logger.warning(\"{} - {}\".format(type(e).__name__, e))\n                errors[e] = e\n            except NoParserFoundForObjectType as f:\n                logger.warning(\"{} - {}\".format(type(f).__name__, f))\n                errors[f] = f\n\n        if len(subclasses) > GLOBAL_CONFIG.dict_to_object_subclass_limit:\n            warn('Type {} has more than {} subclasses, only {} were tried to convert it, with no success. You '\n                 'can raise this limit by setting the appropriate option with `parsyfiles_global_config()`'\n                 ''.format(object_typ, len(subclasses), GLOBAL_CONFIG.dict_to_object_subclass_limit))\n\n        return parsers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __build_parser_for_fileobject_and_desiredtype(self, obj_on_filesystem: PersistedObject, object_typ: Type[T],\n                                                      logger: Logger = None) -> Parser:\n        \"\"\"\n        Builds from the registry, a parser to parse object obj_on_filesystem as an object of type object_type.\n\n        To do that, it iterates through all registered parsers in the list in reverse order (last inserted first),\n        and checks if they support the provided object format (single or multifile) and type.\n        If several parsers match, it returns a cascadingparser that will try them in order.\n\n        :param obj_on_filesystem:\n        :param object_typ:\n        :param logger:\n        :return:\n        \"\"\"\n\n        # first remove any non-generic customization\n        object_type = get_base_generic_type(object_typ)\n\n        # find all matching parsers for this\n        matching, no_type_match_but_ext_match, no_ext_match_but_type_match, no_match = \\\n            self.find_all_matching_parsers(strict=self.is_strict, desired_type=object_type,\n                                           required_ext=obj_on_filesystem.ext)\n        matching_parsers = matching[0] + matching[1] + matching[2]\n\n        if len(matching_parsers) == 0:\n            # No match. Do we have a close match ? (correct type, but not correct extension ?)\n            if len(no_ext_match_but_type_match) > 0:\n                raise NoParserFoundForObjectExt.create(obj_on_filesystem, object_type,\n                                                       set([ext_ for ext_set in\n                                                        [p.supported_exts for p in no_ext_match_but_type_match]\n                                                        for ext_ in ext_set]))\n            else:\n                # no, no match at all\n                raise NoParserFoundForObjectType.create(obj_on_filesystem, object_type,\n                                                        set([typ_ for typ_set in\n                                                        [p.supported_types for p in no_type_match_but_ext_match]\n                                                        for typ_ in typ_set]))\n\n        elif len(matching_parsers) == 1:\n            # return the match directly\n            return matching_parsers[0]\n        else:\n            # return a cascade of all parsers, in reverse order (since last is our preferred one)\n            # print('----- WARNING : Found several parsers able to parse this item. Combining them into a cascade.')\n            return CascadingParser(list(reversed(matching_parsers)))", "response": "Builds a parser for the given object obj_on_filesystem and object_typ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(att_name: str, parsed_att: S, attribute_type: Type[T], caught_exec: Dict[Converter[S, T], Exception]):\n        base_msg = \"Error while trying to convert value for attribute '{a}' to type <{t}>:\\n\" \\\n                   \"   - parsed value is : '{v}' of type <{tv}>\\n\" \\\n                   \"\".format(a=str(att_name), t=get_pretty_type_str(attribute_type), v=parsed_att,\n                             tv=get_pretty_type_str(type(parsed_att)))\n\n        msg = StringIO()\n        if len(list(caught_exec.keys())) > 0:\n            msg.writelines('   - converters tried are : \\n      * ')\n            msg.writelines('\\n      * '.join([str(converter) for converter in caught_exec.keys()]))\n            msg.writelines(' \\n Caught the following exceptions: \\n')\n\n            for converter, err in caught_exec.items():\n                msg.writelines('--------------- From ' + str(converter) + ' caught: \\n')\n                print_error_to_io_stream(err, msg)\n                msg.write('\\n')\n\n        return AttrConversionException(base_msg + msg.getvalue())", "response": "Create an AttrConversionException object for a given attribute name parsed_att and attribute_type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_conversion_chains_from_type(self, from_type: Type[Any]) \\\n            -> Tuple[List[Converter], List[Converter], List[Converter]]:\n        \"\"\"\n        Utility method to find all converters from a given type.\n\n        :param from_type:\n        :return:\n        \"\"\"\n        return self.get_all_conversion_chains(from_type=from_type)", "response": "Utility method to find all converters from a given type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_all_conversion_chains(self, from_type: Type[Any] = JOKER, to_type: Type[Any] = JOKER)\\\n            -> Tuple[List[Converter], List[Converter], List[Converter]]:\n        \"\"\"\n        Utility method to find all converters or conversion chains matching the provided query.\n\n        :param from_type: a required type of input object, or JOKER for 'wildcard'(*) .\n        WARNING: \"from_type=AnyObject/object/Any\" means\n        \"all converters able to source from anything\", which is different from \"from_type=JOKER\" which means \"all\n        converters whatever their source type\".\n        :param to_type: a required type of output object, or JOKER for 'wildcard'(*) .\n        WARNING: \"to_type=AnyObject/object/Any\" means \"all\n        converters able to produce any type of object\", which is different from \"to_type=JOKER\" which means \"all\n        converters whatever type they are able to produce\".\n        :return: a tuple of lists of matching converters, by type of *dest_type* match : generic, approximate, exact\n        \"\"\"\n        pass", "response": "Utility method to find all converters or conversion chains matching the query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_converter(self, converter: Converter[S, T]):\n        check_var(converter, var_types=Converter, var_name='converter')\n\n        # (0) sanity check : check that parser handles jokers properly\n        res = converter.is_able_to_convert_detailed(from_type=JOKER, to_type=JOKER, strict=True)\n        if not (res[0] is True and res[1] is None and res[2] is None):\n            raise ValueError('Converter ' + str(converter) + ' can not be registered since it does not handle the JOKER'\n                             ' cases correctly')\n\n        # compute all possible chains and save them\n        generic_chains, generic_nonstrict_chains, specific_chains, specific_nonstrict_chains \\\n            = self._create_all_new_chains(converter)\n        self._generic_nonstrict_conversion_chains += generic_nonstrict_chains\n        self._generic_conversion_chains += generic_chains\n        self._specific_non_strict_conversion_chains += specific_nonstrict_chains\n        self._specific_conversion_chains += specific_chains\n\n        # sort all lists by length\n        self._generic_nonstrict_conversion_chains = sorted(self._generic_nonstrict_conversion_chains, key=len,\n                                                           reverse=True)\n        self._generic_conversion_chains = sorted(self._generic_conversion_chains, key=len, reverse=True)\n        self._specific_non_strict_conversion_chains = sorted(self._specific_non_strict_conversion_chains, key=len,\n                                                             reverse=True)\n        self._specific_conversion_chains = sorted(self._specific_conversion_chains, key=len, reverse=True)", "response": "Utility method to register any converter."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate all the new conversion chains for the given converter.", "response": "def _create_all_new_chains(self, converter) -> Tuple[List[Converter], List[Converter],\n                                                         List[Converter], List[Converter]]:\n        \"\"\"\n        Creates all specific and generic chains that may be built by adding this converter to the existing chains.\n\n        :param converter:\n        :return: generic_chains, generic_nonstrict_chains, specific_chains, specific_nonstrict_chains\n        \"\"\"\n\n        specific_chains, specific_nonstrict_chains, generic_chains, generic_nonstrict_chains = [], [], [], []\n\n        if converter.is_generic():\n            # the smaller chain : the singleton :)\n            generic_chains.append(ConversionChain(initial_converters=[converter], strict_chaining=True))\n        else:\n            specific_chains.append(ConversionChain(initial_converters=[converter], strict_chaining=True))\n\n\n        # 1) create new specific chains by adding this converter at the beginning or end of all *non-generic* ones\n        # -- non-strict\n        new_c_at_end_ns = []\n        new_c_at_beginning_ns = []\n        if not self.strict:\n            # then there are non-strict chains already. Try to connect to them\n            for existing_specific_nonstrict in self._specific_non_strict_conversion_chains:\n                if converter.can_be_appended_to(existing_specific_nonstrict, strict=False):\n                    if ConversionChain.are_worth_chaining(existing_specific_nonstrict, converter):\n                        new_c_at_end_ns.append(ConversionChain.chain(existing_specific_nonstrict, converter,\n                                                                     strict=False))\n                if existing_specific_nonstrict.can_be_appended_to(converter, strict=False):\n                    if ConversionChain.are_worth_chaining(converter, existing_specific_nonstrict):\n                        new_c_at_beginning_ns.append(ConversionChain.chain(converter, existing_specific_nonstrict,\n                                                                           strict=False))\n\n        # -- strict\n        new_c_at_end = []\n        new_c_at_beginning = []\n        for existing_specific in self._specific_conversion_chains:\n            # first try *strict* mode\n            if converter.can_be_appended_to(existing_specific, strict=True):\n                if ConversionChain.are_worth_chaining(existing_specific, converter):\n                    new_c_at_end.append(ConversionChain.chain(existing_specific, converter, strict=True))\n            elif (not self.strict) and converter.can_be_appended_to(existing_specific, strict=False):\n                if ConversionChain.are_worth_chaining(existing_specific, converter):\n                    new_c_at_end_ns.append(ConversionChain.chain(existing_specific, converter, strict=False))\n\n            if existing_specific.can_be_appended_to(converter, strict=True):\n                if ConversionChain.are_worth_chaining(converter, existing_specific):\n                    # TODO this is where when chaining a generic to a specific, we would actually have to restrict it\n                    # note: but maybe we dont care since now this is checked and prevented in the convert() method\n                    new_c_at_beginning.append(ConversionChain.chain(converter, existing_specific, strict=True))\n            elif (not self.strict) and existing_specific.can_be_appended_to(converter, strict=False):\n                if ConversionChain.are_worth_chaining(converter, existing_specific):\n                    # TODO this is where when chaining a generic to a specific, we would actually have to restrict it\n                    # note: but maybe we dont care since now this is checked and prevented in the convert() method\n                    new_c_at_beginning_ns.append(ConversionChain.chain(converter, existing_specific, strict=False))\n\n        # append to the right list depending on the nature of this converter\n        if converter.is_generic():\n            generic_chains += new_c_at_end\n            generic_nonstrict_chains += new_c_at_end_ns\n        else:\n            specific_chains += new_c_at_end\n            specific_nonstrict_chains += new_c_at_end_ns\n        # common for both types\n        specific_chains += new_c_at_beginning\n        specific_nonstrict_chains += new_c_at_beginning_ns\n\n        # by combining all created chains into a big one\n        for a in new_c_at_end:\n            for b in new_c_at_beginning:\n                b_ = b.remove_first()\n                if b_.can_be_appended_to(a, strict=True):\n                    if ConversionChain.are_worth_chaining(a, b_):\n                        specific_chains.append(ConversionChain.chain(a, b_, strict=True))\n            for b in new_c_at_beginning_ns:\n                b_ = b.remove_first()\n                if b_.can_be_appended_to(a, strict=False):\n                    if ConversionChain.are_worth_chaining(a, b_):\n                        specific_nonstrict_chains.append(ConversionChain.chain(a, b_, strict=False))\n        for a in new_c_at_end_ns:\n            for b in (new_c_at_beginning_ns + new_c_at_beginning):\n                b_ = b.remove_first()\n                if b_.can_be_appended_to(a, strict=False):\n                    if ConversionChain.are_worth_chaining(a, b_):\n                        specific_nonstrict_chains.append(ConversionChain.chain(a, b_, strict=False))\n\n        # by inserting this converter at the beginning of an existing *generic*\n        if converter.is_generic():\n            # we want to avoid chaining generic converters together\n            pass\n        else:\n            new_c_at_beginning_generic = []\n            new_c_at_beginning_generic_ns = []\n            for existing_specific in self._generic_conversion_chains:\n                # start with strict\n                if existing_specific.can_be_appended_to(converter, strict=True):\n                    if ConversionChain.are_worth_chaining(converter, existing_specific):\n                        new_c_at_beginning_generic.append(ConversionChain.chain(converter, existing_specific,\n                                                                                strict=True))\n                elif (not self.strict) and existing_specific.can_be_appended_to(converter, strict=False):\n                    if ConversionChain.are_worth_chaining(converter, existing_specific):\n                        new_c_at_beginning_generic_ns.append(ConversionChain.chain(converter, existing_specific,\n                                                                                   strict=False))\n\n            for existing_specific_ns in self._generic_nonstrict_conversion_chains:\n                if existing_specific_ns.can_be_appended_to(converter, strict=False):\n                    if ConversionChain.are_worth_chaining(converter, existing_specific_ns):\n                        new_c_at_beginning_generic_ns.append(ConversionChain.chain(converter, existing_specific_ns,\n                                                                               strict=False))\n            generic_chains += new_c_at_beginning_generic\n            generic_nonstrict_chains += new_c_at_beginning_generic_ns\n\n            # by combining specific and generic created chains into a big one\n            for a in new_c_at_end:\n                for b in new_c_at_beginning_generic:\n                    b_ = b.remove_first()\n                    if b_.can_be_appended_to(a, strict=True):\n                        if ConversionChain.are_worth_chaining(a, b_):\n                            generic_chains.append(ConversionChain.chain(a, b_, strict=True))\n                for b in new_c_at_beginning_generic_ns:\n                    b_ = b.remove_first()\n                    if b_.can_be_appended_to(a, strict=False):\n                        if ConversionChain.are_worth_chaining(a, b_):\n                            generic_nonstrict_chains.append(ConversionChain.chain(a, b_, strict=False))\n            for a in new_c_at_end_ns:\n                for b in (new_c_at_beginning_generic_ns + new_c_at_beginning_generic):\n                    b_ = b.remove_first()\n                    if b_.can_be_appended_to(a, strict=False):\n                        if ConversionChain.are_worth_chaining(a, b_):\n                            generic_nonstrict_chains.append(ConversionChain.chain(a, b_, strict=False))\n\n        return generic_chains, generic_nonstrict_chains, specific_chains, specific_nonstrict_chains"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_conversion_chains(self, from_type: Type[Any] = JOKER, to_type: Type[Any] = JOKER) \\\n            -> Tuple[List[Converter], List[Converter], List[Converter]]:\n        \"\"\"\n        Utility method to find matching converters or conversion chains.\n\n        :param from_type: a required type of input object, or JOKER for 'wildcard'(*) .\n        WARNING: \"from_type=AnyObject/object/Any\" means\n        \"all converters able to source from anything\", which is different from \"from_type=JOKER\" which means \"all\n        converters whatever their source type\".\n        :param to_type: a required type of output object, or JOKER for 'wildcard'(*) .\n        WARNING: \"to_type=AnyObject/object/Any\" means \"all\n        converters able to produce any type of object\", which is different from \"to_type=JOKER\" which means \"all\n        converters whatever type they are able to produce\".\n        :return: a tuple of lists of matching converters, by type of *dest_type* match : generic, approximate, exact.\n        The order of each list is from *less relevant* to *most relevant*\n        \"\"\"\n\n        if from_type is JOKER and to_type is JOKER:\n            matching_dest_generic = self._generic_nonstrict_conversion_chains.copy() + \\\n                                    self._generic_conversion_chains.copy()\n            matching_dest_approx = []\n            matching_dest_exact = self._specific_non_strict_conversion_chains.copy() + \\\n                                  self._specific_conversion_chains.copy()\n\n        else:\n            matching_dest_generic, matching_dest_approx, matching_dest_exact = [], [], []\n\n            # first transform any 'Any' type requirement into the official class for that\n            to_type = get_validated_type(to_type, 'to_type', enforce_not_joker=False)\n\n            # handle generic converters first\n            for c in (self._generic_nonstrict_conversion_chains + self._generic_conversion_chains):\n                match, source_exact, dest_exact = c.is_able_to_convert_detailed(strict=self.strict,\n                                                                                from_type=from_type,\n                                                                                to_type=to_type)\n                if match:\n                    # match\n                    if is_any_type(to_type):\n                        # special case where desired to_type is already Any : in that case a generic converter will\n                        # appear in 'exact match'\n                        matching_dest_exact.append(c)\n                    else:\n                        # this is a match from a generic parser to a specific type : add in 'generic' cataegory\n                        matching_dest_generic.append(c)\n\n            # then the specific\n            for c in (self._specific_non_strict_conversion_chains + self._specific_conversion_chains):\n                match, source_exact, dest_exact = c.is_able_to_convert_detailed(strict=self.strict,\n                                                                                from_type=from_type,\n                                                                                to_type=to_type)\n                if match:\n                    if not is_any_type(to_type):\n                        if dest_exact:\n                            # we dont care if source is exact or approximate as long as dest is exact\n                            matching_dest_exact.append(c)\n                        else:\n                            # this means that dest is approximate.\n                            matching_dest_approx.append(c)\n                    else:\n                        # we only want to keep the generic ones, and they have already been added\n                        pass\n\n        return matching_dest_generic, matching_dest_approx, matching_dest_exact", "response": "Utility method to find all converters that can be applied to the specified type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding the parent method to find parsers appropriate to a given extension and type. This leverages both the parser registry and the converter registry to propose parsing chains in a relevant order :param strict: :param desired_type: the type of object to match. :param required_ext: the required extension. :return: match=(matching_parsers_generic, matching_parsers_approx, matching_parsers_exact), no_type_match_but_ext_match, no_ext_match_but_type_match, no_match", "response": "def find_all_matching_parsers(self, strict: bool, desired_type: Type[Any] = JOKER, required_ext: str = JOKER) \\\n        -> Tuple[Tuple[List[Parser], List[Parser], List[Parser]],\n                 List[Parser], List[Parser], List[Parser]]:\n        \"\"\"\n        Overrides the parent method to find parsers appropriate to a given extension and type.\n        This leverages both the parser registry and the converter registry to propose parsing chains in a relevant order\n\n        :param strict:\n        :param desired_type: the type of object to match.\n        :param required_ext: the required extension.\n        :return: match=(matching_parsers_generic, matching_parsers_approx, matching_parsers_exact),\n                 no_type_match_but_ext_match, no_ext_match_but_type_match, no_match\n        \"\"\"\n        # transform any 'Any' type requirement into the official class for that\n        desired_type = get_validated_type(desired_type, 'desired_type', enforce_not_joker=False)\n\n        # (1) call the super method to find all parsers\n        matching, no_type_match_but_ext_match, no_ext_match_but_type_match, no_match = \\\n            super(ParserRegistryWithConverters, self).find_all_matching_parsers(strict=self.is_strict,\n                                                                                desired_type=desired_type,\n                                                                                required_ext=required_ext)\n        # these are ordered with 'preferred last'\n        matching_p_generic, matching_p_approx, matching_p_exact = matching\n\n        if desired_type is JOKER:\n            # then we want to try to append every possible converter chain, even if we have already found an exact match\n            # (exact match will probably contain all parsers in that case?...)\n            parsers_to_complete_with_converters = no_type_match_but_ext_match + matching_p_generic + matching_p_approx \\\n                                                  + matching_p_exact\n        else:\n            # then we can try to complete all the ones matching the extension (even the generic because combining them\n            # with a conversion chain might provide another way to reach the result - different from using the generic\n            # alone to reach the to_type)\n            parsers_to_complete_with_converters = no_type_match_but_ext_match + matching_p_generic + matching_p_approx\n\n        # (2) find all conversion chains that lead to the expected result\n        matching_c_generic_to_type, matching_c_approx_to_type, matching_c_exact_to_type = \\\n            self.get_all_conversion_chains_to_type(to_type=desired_type)\n        all_matching_converters = matching_c_generic_to_type + matching_c_approx_to_type + matching_c_exact_to_type\n\n        # (3) combine both (parser + conversion chain), and append to the appropriate list depending on the match\n        # -- (a) first Parsers matching EXT (not type) + Converters matching their type\n        # for all parsers able to parse this extension, and for all the types they support\n        #\n        # (we have to reverse the list because now we want 'preferred first'. Indeed we wish to prepend to the match\n        # lists in order not to hide the parser direct matches)\n        matching_p_generic_with_approx_chain, matching_p_approx_with_approx_chain, matching_p_exact_with_approx_chain\\\n            = [], [], []\n        for parser in reversed(parsers_to_complete_with_converters):\n            for typ in parser.supported_types:\n                match_results = self._complete_parsers_with_converters(parser, typ, desired_type,\n                                                                       matching_c_generic_to_type,\n                                                                       matching_c_approx_to_type,\n                                                                       matching_c_exact_to_type)\n                # prepend the existing lists with the new match\n                matching_p_generic = match_results[1] + matching_p_generic\n                matching_p_approx = match_results[3] + matching_p_approx\n                matching_p_exact = match_results[5] + matching_p_exact\n\n                # store the approximate matchs in the separate lists\n                matching_p_generic_with_approx_chain = match_results[0] + matching_p_generic_with_approx_chain\n                matching_p_approx_with_approx_chain = match_results[2] + matching_p_approx_with_approx_chain\n                matching_p_exact_with_approx_chain = match_results[4] + matching_p_exact_with_approx_chain\n\n        # finally prepend the approximate match lists\n        matching_p_generic = matching_p_generic_with_approx_chain + matching_p_generic\n        matching_p_approx = matching_p_approx_with_approx_chain + matching_p_approx\n        matching_p_exact = matching_p_exact_with_approx_chain + matching_p_exact\n\n        # -- (b) then parsers that do not match at all (not the file extension nor the type): we can find parsing chains\n        # that make them at least match the type\n        #\n        # (we have to reverse it because it was 'best last', now it will be 'best first')\n        for parser in reversed(no_match):\n            for typ in parser.supported_types:\n                for converter in reversed(all_matching_converters):\n                    # if converter is able to source from this parser\n                    if converter.is_able_to_convert(self.is_strict, from_type=typ, to_type=desired_type):\n                        if ParsingChain.are_worth_chaining(parser, typ, converter):\n                            # insert it at the beginning since it should have less priority\n                            no_ext_match_but_type_match.insert(0, ParsingChain(parser, converter, strict=self.is_strict,\n                                                                               base_parser_chosen_dest_type=typ))\n\n        # Finally sort by chain length\n        matching_p_generic = sorted(matching_p_generic, key=len, reverse=True)\n        matching_p_approx = sorted(matching_p_approx, key=len, reverse=True)\n        matching_p_exact = sorted(matching_p_exact, key=len, reverse=True)\n\n        # Return\n        return (matching_p_generic, matching_p_approx, matching_p_exact), no_type_match_but_ext_match, \\\n               no_ext_match_but_type_match, no_match"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncompleting the parsing chains of a parser and converters.", "response": "def _complete_parsers_with_converters(self, parser, parser_supported_type, desired_type, matching_c_generic_to_type,\n                                          matching_c_approx_to_type, matching_c_exact_to_type):\n        \"\"\"\n        Internal method to create parsing chains made of a parser and converters from the provided lists.\n        Once again a JOKER for a type means 'joker' here.\n\n        :param parser:\n        :param parser_supported_type:\n        :param desired_type:\n        :param matching_c_generic_to_type:\n        :param matching_c_approx_to_type:\n        :param matching_c_exact_to_type:\n        :return:\n        \"\"\"\n\n        matching_p_generic, matching_p_generic_with_approx_chain, \\\n        matching_p_approx, matching_p_approx_with_approx_chain,\\\n        matching_p_exact, matching_p_exact_with_approx_chain = [], [], [], [], [], []\n\n        # resolve Union and TypeVar\n        desired_types = get_alternate_types_resolving_forwardref_union_and_typevar(desired_type)\n\n        for desired_type in desired_types:\n\n            # first transform any 'Any' type requirement into the official class for that\n            desired_type = get_validated_type(desired_type, 'desired_type', enforce_not_joker=False)\n\n            # ---- Generic converters - only if the parsed type is not already 'any'\n            if not is_any_type(parser_supported_type):\n                for cv in matching_c_generic_to_type:\n                    # if the converter can attach to this parser, we have a matching parser !\n\n                    # --start from strict\n                    if cv.is_able_to_convert(strict=True,\n                                             from_type=parser_supported_type,\n                                             to_type=desired_type):\n                        if ParsingChain.are_worth_chaining(parser, parser_supported_type, cv):\n                            chain = ParsingChain(parser, cv, strict=True,\n                                                 base_parser_chosen_dest_type=parser_supported_type)\n                            # insert it at the beginning since it should have less priority\n                            matching_p_generic.append(chain)\n\n                    # --then non-strict\n                    elif (not self.strict) \\\n                            and cv.is_able_to_convert(strict=False,\n                                                      from_type=parser_supported_type,\n                                                      to_type=desired_type):\n                        if ParsingChain.are_worth_chaining(parser, parser_supported_type, cv):\n                            chain = ParsingChain(parser, cv, strict=False,\n                                                 base_parser_chosen_dest_type=parser_supported_type)\n                            # insert it at the beginning since it should have less priority\n                            matching_p_generic_with_approx_chain.append(chain)\n\n            # ---- Approx to_type\n            for cv in matching_c_approx_to_type:\n                # if the converter can attach to this parser, we have a matching parser !\n                # -- start from strict\n                if cv.is_able_to_convert(strict=True,\n                                         from_type=parser_supported_type,\n                                         to_type=desired_type):\n                    if ParsingChain.are_worth_chaining(parser, parser_supported_type, cv):\n                        chain = ParsingChain(parser, cv, strict=True,\n                                             base_parser_chosen_dest_type=parser_supported_type)\n                        # insert it at the beginning since it should have less priority\n                        matching_p_approx.append(chain)\n                # then non-strict\n                elif (not self.strict) \\\n                        and cv.is_able_to_convert(strict=False,\n                                                  from_type=parser_supported_type,\n                                                  to_type=desired_type):\n                    if ParsingChain.are_worth_chaining(parser, parser_supported_type, cv):\n                        chain = ParsingChain(parser, cv, strict=False,\n                                             base_parser_chosen_dest_type=parser_supported_type)\n                        # insert it at the beginning since it should have less priority\n                        matching_p_approx_with_approx_chain.append(chain)\n\n            # ---- Exact to_type\n            for cv in matching_c_exact_to_type:\n                # if the converter can attach to this parser, we have a matching parser !\n                if cv.is_able_to_convert(strict=True,\n                                         from_type=parser_supported_type,\n                                         to_type=desired_type):\n                    if ParsingChain.are_worth_chaining(parser, parser_supported_type, cv):\n                        chain = ParsingChain(parser, cv, strict=True,\n                                             base_parser_chosen_dest_type=parser_supported_type)\n                        # insert it at the beginning since it should have less priority\n                        matching_p_exact.append(chain)\n                elif (not self.strict) \\\n                        and cv.is_able_to_convert(strict=False,\n                                                  from_type=parser_supported_type,\n                                                  to_type=desired_type):\n                    if ParsingChain.are_worth_chaining(parser, parser_supported_type, cv):\n                        chain = ParsingChain(parser, cv, strict=False,\n                                             base_parser_chosen_dest_type=parser_supported_type)\n                        # insert it at the beginning since it should have less priority\n                        matching_p_exact_with_approx_chain.append(chain)\n\n        # Preferred is LAST, so approx should be first\n        return matching_p_generic_with_approx_chain, matching_p_generic, \\\n               matching_p_approx_with_approx_chain, matching_p_approx, \\\n               matching_p_exact_with_approx_chain, matching_p_exact"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_config(filename):\n\n  tag = None\n  branch = None\n  message = 'Prepare {VERSION} release.'\n  upgrades = {}\n  subs = {}\n\n  with open(filename) as fp:\n    for i, line in enumerate(fp):\n      line = line.strip()\n      if not line or line.startswith('#'): continue\n\n      key, sep, value = line.partition(' ')\n      if not key or not value:\n        raise ValueError('invalid configuration file at line {}'.format(i+1))\n\n      if key == 'tag':\n        tag = value.strip()\n      elif key == 'branch':\n        branch = value.strip()\n      elif key == 'message':\n        message = value.strip()\n      elif key == 'upgrade':\n        filename, sep, pattern = value.partition(':')\n        if not filename or not sep or not pattern or '{VERSION}' not in pattern:\n          raise ValueError('invalid upgrade argument at line {}'.format(i+1))\n        upgrade = upgrades.setdefault(filename, [])\n        upgrade.append(pattern)\n      elif key == 'sub':\n        filename, sep, pattern = value.partition(':')\n        pattern = pattern.partition(':')[::2]\n        if not pattern[0] or not pattern[1]:\n          raise ValueError('invalid sub argument at line {}'.format(i+1))\n        subs.setdefault(filename, []).append(pattern)\n      else:\n        raise ValueError('invalid command {!r} at line {}'.format(key, i+1))\n\n  return Config(tag, branch, message, upgrades, subs)", "response": "Parses a versionupgrade configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmatching a single version upgrade pattern in the specified file. Returns a Match object or None if the pattern did not match.", "response": "def match_version_pattern(filename, pattern):\n  \"\"\"\n  Matches a single version upgrade pattern in the specified *filename*\n  and returns the match information. Returns a #Match object or #None\n  if the *pattern* did not match.\n  \"\"\"\n\n  if \"{VERSION}\" not in pattern:\n    raise ValueError(\"pattern does not contain a {VERSION} reference\")\n  pattern = pattern.replace('{VERSION}', '(?P<v>[\\d\\w\\.\\-_]+)')\n  expr = re.compile(pattern)\n  with open(filename) as fp:\n    lines = fp.read().split('\\n')\n  for i, line in enumerate(lines):\n    match = expr.search(line)\n    if match:\n      return Match(filename, lines, line_index=i,\n          version=Version(match.group('v')), span=match.span('v'))\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_changed_files(include_staged=False):\n\n  process = subprocess.Popen(['git', 'status', '--porcelain'],\n    stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n  stdout, __ = process.communicate()\n  if process.returncode != 0:\n    raise ValueError(stdout)\n  files = []\n  for line in stdout.decode().split('\\n'):\n    if not line or line.startswith('#'): continue\n    assert line[2] == ' '\n    if not include_staged and line[1] == ' ': continue\n    files.append(line[3:])\n  return files", "response": "Returns a list of the files that have changed in the Git repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a well - formed docstring into a list of documentation strings.", "response": "def _parse_doc(docs):\n    \"\"\"\n        Converts a well-formed docstring into documentation\n        to be fed into argparse.\n\n        See signature_parser for details.\n\n        shorts: (-k for --keyword -k, or \"from\" for \"frm/from\")\n        metavars: (FILE for --input=FILE)\n        helps: (docs for --keyword: docs)\n        description: the stuff before\n        epilog: the stuff after\n    \"\"\"\n\n    name = \"(?:[a-zA-Z][a-zA-Z0-9-_]*)\"\n\n    re_var = re.compile(r\"^ *(%s)(?: */(%s))? *:(.*)$\" % (name, name))\n    re_opt = re.compile(r\"^ *(?:(-[a-zA-Z0-9]),? +)?--(%s)(?: *=(%s))? *:(.*)$\"\n                        % (name, name))\n\n    shorts, metavars, helps, description, epilog = {}, {}, {}, \"\", \"\"\n\n    if docs:\n\n        for line in docs.split(\"\\n\"):\n\n            line = line.strip()\n\n            # remove starting ':param'\n            if line.startswith(':param'):\n                line = line[len(':param'):]\n\n            # skip ':rtype:' row\n            if line.startswith(':rtype:'):\n                continue\n\n            if line.strip() == \"----\":\n                break\n\n            m = re_var.match(line)\n            if m:\n                if epilog:\n                    helps[prev] += epilog.strip()\n                    epilog = \"\"\n\n                if m.group(2):\n                    shorts[m.group(1)] = m.group(2)\n\n                helps[m.group(1)] = m.group(3).strip()\n                prev = m.group(1)\n                previndent = len(line) - len(line.lstrip())\n                continue\n\n            m = re_opt.match(line)\n            if m:\n                if epilog:\n                    helps[prev] += epilog.strip()\n                    epilog = \"\"\n                name = m.group(2).replace(\"-\", \"_\")\n                helps[name] = m.group(4)\n                prev = name\n\n                if m.group(1):\n                    shorts[name] = m.group(1)\n                if m.group(3):\n                    metavars[name] = m.group(3)\n\n                previndent = len(line) - len(line.lstrip())\n                continue\n\n            if helps:\n                if line.startswith(\" \" * (previndent + 1)):\n                    helps[prev] += \"\\n\" + line.strip()\n                else:\n                    epilog += \"\\n\" + line.strip()\n            else:\n                description += \"\\n\" + line.strip()\n\n            if line.strip():\n                previndent = len(line) - len(line.lstrip())\n\n    return shorts, metavars, helps, description, epilog"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an argparse. ArgumentParser object that parses the signature of a function.", "response": "def signature_parser(func):\n    \"\"\"\n        Creates an argparse.ArgumentParser from the function's signature.\n\n        Arguments with no default are compulsary positional arguments,\n        Arguments with defaults are optional --flags.\n\n        If the default is True or False, the action of the flag will\n        toggle the argument and the flag takes no parameters.\n\n        If the default is None or a unicode string, the flag takes a\n        string argument that passed to the function as a unicode string\n        decoded using entrypoint.ENCODING\n\n        If the default is a string, then the argument is passed as a binary\n        string (be careful!), an int and a float cause parsing of those too.\n\n        If you want the arguments to be a file, consider using the\n        @withfile decorator.\n\n        Documentation can be read out of the function's docstring, which should\n        be of the basic form:\n        '''\n            A short introduction to your program.\n\n                arg: Help for positional parameter.\n                frm/from: Help for a positional parameter\n                          with a reserved public name\n                          (i.e. this displays to the user as \"from\"\n                          but sets the \"frm\" variable)\n                --opt: Help for optional parameter.\n                -f --flag: An optional parameter that has a short version.\n                --mode=MODE: An optional parameter that takes a MODE\n                -t --type: A combination of both of the above, and one\n                                which requires continuing of the documentation\n                                on an indented line\n\n            An epilog explaining anything you feel needs further clarity.\n\n            ----\n\n            Any documentation for the function itself that isn't covered by the\n            public documentation above the line.\n        '''\n\n        All sections, and indeed the presence of a docstring, are not required.\n\n        NOTE: for this to work, the function's signature must be in-tact\n              some decorators (like @acceptargv for example) destroy, or\n              mutilate the signature.\n    \"\"\"\n\n    args, trail, kwargs, defaults = inspect.getargspec(func)\n\n    if not args:\n        args = []\n\n    if not defaults:\n        defaults = []\n\n    if kwargs:\n        raise Exception(\"Can't wrap a function with **kwargs\")\n\n    # Compulsary positional options\n    needed = args[0:len(args) - len(defaults)]\n\n    # Optional flag options\n    params = args[len(needed):]\n\n    shorts, metavars, helps, description, epilog = _parse_doc(func.__doc__)\n\n    parser = argparse.ArgumentParser(\n        description=description,\n        epilog=epilog,\n        formatter_class=ParagraphPreservingArgParseFormatter)\n\n    # special flags\n    special_flags = []\n\n    special_flags += ['debug']\n    defaults += (False,)\n    helps['debug'] = 'set logging level to DEBUG'\n    if module_version(func):\n        special_flags += ['version']\n        defaults += (False,)\n        helps['version'] = \"show program's version number and exit\"\n    params += special_flags\n\n    # Optional flag options\n    used_shorts = set()\n    for param, default in zip(params, defaults):\n        args = [\"--%s\" % param.replace(\"_\", \"-\")]\n        short = None\n        if param in shorts:\n            short = shorts[param]\n        else:\n            if param not in special_flags and len(param) > 1:\n                first_char = param[0]\n                if first_char not in used_shorts:\n                    used_shorts.add(first_char)\n                    short = '-' + first_char\n        # -h conflicts with 'help'\n        if short and short != '-h':\n            args = [short] + args\n\n        kwargs = {'default': default, 'dest': param.replace(\"-\", \"_\")}\n\n        if param == 'version':\n            kwargs['action'] = 'version'\n            kwargs['version'] = module_version(func)\n        elif default is True:\n            kwargs['action'] = 'store_false'\n        elif default is False:\n            kwargs['action'] = 'store_true'\n        elif isinstance(default, list):\n            kwargs['action'] = 'append'\n#  default is not working\n#            if len(default):\n#                first = default[0]\n#                if type(first) in [type(None), unicode]:\n#                    kwargs['type'] = unidecode\n#                else:\n#                    kwargs['type'] = type(first)\n#                kwargs['default'] = []\n#            else:\n            kwargs['type'] = unidecode\n        else:\n            kwargs['action'] = 'store'\n            if type(default) in [type(None), unicode]:\n                kwargs['type'] = unidecode\n            else:\n                kwargs['type'] = type(default)\n\n        if param in helps:\n            kwargs['help'] = helps[param]\n\n        if param in metavars:\n            kwargs['metavar'] = metavars[param]\n\n        parser.add_argument(*args, **kwargs)\n\n    # Compulsary positional options\n    for need in needed:\n\n        kwargs = {'action': 'store', 'type': unidecode}\n\n        if need in helps:\n            kwargs['help'] = helps[need]\n\n        if need in shorts:\n            args = [shorts[need]]\n        else:\n            args = [need]\n\n        parser.add_argument(*args, **kwargs)\n\n    # The trailing arguments\n    if trail:\n        kwargs = {'action': 'store', 'type': unidecode, 'nargs': \"*\"}\n\n        if trail in helps:\n            kwargs['help'] = helps[trail]\n\n        if trail in shorts:\n            kwargs['metavar'] = shorts[trail]\n        else:\n            kwargs['metavar'] = trail\n\n        parser.add_argument('__args', **kwargs)\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a dictionary of arguments including __argv into a list for passing to the function.", "response": "def _correct_args(func, kwargs):\n    \"\"\"\n        Convert a dictionary of arguments including __argv into a list\n        for passing to the function.\n    \"\"\"\n    args = inspect.getargspec(func)[0]\n    return [kwargs[arg] for arg in args] + kwargs['__args']"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef autorun(func, _depth=1):\n\n    frame_local = sys._getframe(_depth).f_locals\n    if '__name__' in frame_local and frame_local['__name__'] == '__main__':\n        func(argv=sys.argv[1:])\n\n    return func", "response": "A function that runs the function if the module in which it is declared is being run by the commandline."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef acceptargv(func):\n\n    parser = signature_parser(func)\n\n    def main(*args, **kw):\n        argv = kw.get('argv', None)\n        if argv == None:\n            return func(*args, **kw)\n        else:\n            try:\n                kwargs = parser.parse_args(argv).__dict__\n\n                # special cli flags\n\n                # --version is handled by ArgParse\n                # if kwargs.get('version'):\n                #    print module_version(func)\n                #    return\n                if 'version' in kwargs.keys():\n                    del kwargs['version']\n\n                # --debug\n                if kwargs.get('debug'):\n                    logging.basicConfig(level=logging.DEBUG)\n                del kwargs['debug']\n\n                if \"__args\" in kwargs:\n                    return func(*_correct_args(func, kwargs))\n                else:\n                    return func(**kwargs)\n            except UsageError, e:\n                parser.error(e.message)\n\n    main.__doc__ = func.__doc__\n    main.__name__ = func.__name__\n    main.__module__ = func.__module__\n    main.__dict__ = func.__dict__.copy()\n\n    return main", "response": "A decorator that takes an argument list and returns a function that takes the arguments of the function and returns the result of the function."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_opener(suffix, opener=None):\n\n  if opener is None:\n    def decorator(func):\n      register_opener(suffix, func)\n      return func\n    return decorator\n  if suffix in openers:\n    raise ValueError('opener suffix {0!r} already registered'.format(suffix))\n  openers[suffix] = opener", "response": "Register a callback that opens an archive with the specified suffix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the opener that can handle this filename.", "response": "def get_opener(filename):\n  \"\"\"\n  Finds a matching opener that is registed with :func:`register_opener`\n  and returns a tuple ``(suffix, opener)``. If there is no opener that\n  can handle this filename, :class:`UnknownArchive` is raised.\n  \"\"\"\n\n  for suffix, opener in openers.items():\n    if filename.endswith(suffix):\n      return suffix, opener\n  raise UnknownArchive(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen the archive at the specified filename or from the file - like object.", "response": "def open(filename=None, file=None, mode='r', suffix=None, options=None):\n  \"\"\"\n  Opens the archive at the specified *filename* or from the file-like\n  object *file* using the appropriate opener. A specific opener can be\n  specified by passing the *suffix* argument.\n\n  # Parameters\n  filename (str): A filename to open the archive from.\n  file (file-like): A file-like object as source/destination.\n  mode (str): The mode to open the archive in.\n  suffix (str): Possible override for the *filename* suffix. Must be\n    specified when *file* is passed instead of *filename*.\n  options (dict): A dictionary that will be passed to the opener\n    with which additional options can be specified.\n  return (archive-like): An object that represents the archive and follows\n    the interface of the #tarfile.TarFile class.\n  \"\"\"\n\n  if mode not in ('r', 'w', 'a'):\n    raise ValueError(\"invalid mode: {0!r}\".format(mode))\n\n  if suffix is None:\n    suffix, opener = get_opener(filename)\n    if file is not None:\n      filename = None  # We don't need it anymore.\n  else:\n    if file is not None and filename is not None:\n      raise ValueError(\"filename must not be set with file & suffix specified\")\n    try:\n      opener = openers[suffix]\n    except KeyError:\n      raise UnknownArchive(suffix)\n\n  if options is None:\n    options = {}\n\n  if file is not None:\n    if mode in 'wa' and not hasattr(file, 'write'):\n      raise TypeError(\"file.write() does not exist\", file)\n    if mode == 'r' and not hasattr(file, 'read'):\n      raise TypeError(\"file.read() does not exist\", file)\n\n  if [filename, file].count(None) != 1:\n    raise ValueError(\"either filename or file must be specified\")\n  if filename is not None:\n    file = builtins.open(filename, mode + 'b')\n\n  try:\n    return opener(file, mode, options)\n  except:\n    if filename is not None:\n      file.close()\n    raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the contents of *archive* to the specified *directory*. This function ensures that no file is extracted outside of the target directory (which can theoretically happen if the arcname is not relative or points to a parent directory). # Parameters archive (str, archive-like): The filename of an archive or an already opened archive. directory (str): Path to the directory to unpack the contents to. unpack_single_dir (bool): If this is True and if the archive contains only a single top-level directory, its contents will be placed directly into the target *directory*.", "response": "def extract(archive, directory, suffix=None, unpack_single_dir=False,\n    check_extract_file=None, progress_callback=None, default_mode='755'):\n  \"\"\"\n  Extract the contents of *archive* to the specified *directory*. This\n  function ensures that no file is extracted outside of the target directory\n  (which can theoretically happen if the arcname is not relative or points\n  to a parent directory).\n\n  # Parameters\n  archive (str, archive-like): The filename of an archive or an already\n    opened archive.\n  directory (str): Path to the directory to unpack the contents to.\n  unpack_single_dir (bool): If this is True and if the archive contains only\n    a single top-level directory, its contents will be placed directly into\n    the target *directory*.\n  \"\"\"\n\n  if isinstance(archive, str):\n    with open(archive, suffix=suffix) as archive:\n      return extract(archive, directory, None, unpack_single_dir,\n          check_extract_file, progress_callback, default_mode)\n\n  if isinstance(default_mode, str):\n    default_mode = int(default_mode, 8)\n\n  if progress_callback:\n    progress_callback(-1, 0, None)\n  names = archive.getnames()\n\n  # Find out if we have only one top-level directory.\n  toplevel_dirs = set()\n  for name in names:\n    parts = name.split('/')\n    if len(parts) > 1:\n      toplevel_dirs.add(parts[0])\n  if unpack_single_dir and len(toplevel_dirs) == 1:\n    stripdir = next(iter(toplevel_dirs)) + '/'\n  else:\n    stripdir = None\n\n  for index, name in enumerate(names):\n    if progress_callback:\n      progress_callback(index + 1, len(names), name)\n    if name.startswith('..') or name.startswith('/') or os.path.isabs(name):\n      continue\n    if check_extract_file and not check_extract_file(name):\n      continue\n    if name.endswith('/'):\n      continue\n    if stripdir:\n      filename = name[len(stripdir):]\n      if not filename:\n        continue\n    else:\n      filename = name\n\n    info = archive.getmember(name)\n    src = archive.extractfile(name)\n    if not src:\n      continue\n\n    try:\n      filename = os.path.join(directory, filename)\n      dirname = os.path.dirname(filename)\n      if not os.path.exists(dirname):\n        os.makedirs(dirname)\n      with builtins.open(filename, 'wb') as dst:\n        shutil.copyfileobj(src, dst)\n      os.chmod(filename, info.mode or default_mode)\n      os.utime(filename, (-1, info.mtime))\n    finally:\n      src.close()\n\n  if progress_callback:\n    progress_callback(len(names), len(names), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transitions_to(self, dst):\n    '''\n    returns enumerable of (prevstate, t) tuples\n    this is super slow and needs to be sped up\n    '''\n    if dst in self._transitions_to:\n      for t in self._transitions_to[dst]:\n        for s in self._transitions_to[dst][t]:\n          yield (s, t)", "response": "returns enumerable of ( prevstate t ) tuples\n returns enumerable of ( prevstate t ) tuples\n this is super slow needs to be sped up\n this is super slow needs to be sped up\n this is super slow"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reltags(self, src, cache=None):\n    '''\n    returns all the tags that are relevant at this state\n    cache should be a dictionary and it is updated\n    by the function\n    '''\n    if not self._tag_assocs:\n      return set()\n\n    # fucking python and it's terrible support for recursion makes this\n    # far more complicated than it needs to be\n    if cache == None:\n      cache = {}\n\n    q = _otq()\n    q.append(src)\n    updateq = _otq()\n\n\n    while q:\n      i = q.popleft()\n      if i in cache:\n        continue\n\n      cache[i] = set()\n      for (s,t) in self.transitions_to(i):\n        q.append(s)\n        if self.is_tagged(t,s,i):\n          cache[i].add((self.tag(t,s,i),s, i))\n        updateq.appendleft((i, s))\n\n    while updateq:\n      i = updateq.popleft()\n      cache[i[0]].update(cache[i[1]])\n\n    return cache[src]", "response": "returns all the tags that are relevant at this state\n    cache should be a dictionary and it is updated by the function\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd epsilon states to the stateset.", "response": "def _add_epsilon_states(self, stateset, gathered_epsilons):\n    '''\n    stateset is the list of initial states\n    gathered_epsilons is a dictionary of (dst: src) epsilon dictionaries\n    '''\n    for i in list(stateset):\n      if i not in gathered_epsilons:\n        gathered_epsilons[i] = {}\n        q = _otq()\n        q.append(i)\n        while q:\n          s = q.popleft()\n          for j in self._transitions.setdefault(s, {}).setdefault(NFA.EPSILON, set()):\n            gathered_epsilons[i][j] = s if j not in gathered_epsilons[i] else self.choose(s, j)\n            q.append(j)\n      stateset.update(gathered_epsilons[i].keys())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the instruction pointer to the bytecode added to the bytecode", "response": "def _states_to_dfa_bytecode(self, states, \\\n      tran=None, \\\n      debug=False, \\\n      compiled_states=None, \\\n      gathered_epsilons=None, \\\n      cached_transitions=None, \\\n      cached_tcode=None, \\\n      reltags_cache=None \\\n      ):\n    '''returns the instruction pointer to the bytecode added'''\n    pstates = copy.copy(states)\n\n    if reltags_cache == None:\n      reltags_cache = {}\n\n    if cached_tcode == None:\n      cached_tcode = {}\n\n    if cached_transitions == None:\n      cached_transitions = {}\n\n    if gathered_epsilons == None:\n      gathered_epsilons = {}\n\n    self._add_epsilon_states(states, gathered_epsilons)\n\n    if tran != None:\n      states = self.nextstates(states, tran)\n      self._add_epsilon_states(states, gathered_epsilons)\n\n    if self._magic != None:\n      states = states.union(self._magic(states))\n\n    tstates = tuple(states)\n\n    # this is used so we only compile each stateset once\n    if compiled_states == None:\n      compiled_states = {}\n\n    if tstates in compiled_states:\n      return compiled_states[tstates]\n\n    # grab the ip from our codeblock\n    ip = self._bytecode.newblock(tstates)\n    compiled_states[tstates] = ip\n\n    # TODO\n    # epsilon transitions are never 'taken' so we need\n    # to insert any ltagv/utagv instructions required\n    # for all epsilon transitions\n    # gathered_epsilons[state] holds a dictionary of dst: src mappings, so we can use that data\n\n    if self.do_tags:\n      tags = set()\n      rtags = set()\n  \n      for ts in pstates:\n        for dst in gathered_epsilons[ts]:\n          rtags.update(self.reltags(dst, reltags_cache))\n          src = gathered_epsilons[ts][dst]\n          if self.is_tagged(NFA.EPSILON, src, dst):\n            tags.add((self.tag(NFA.EPSILON, src, dst), dst))\n  \n      self._write_transition_code(tags, rtags, ip)\n\n    # run any defined state hooks\n    for s in tstates:\n      if s in self._state_hooks:\n        ip.append(VM.PyCode(self._state_hooks[s]))\n\n    # do a multi-match for any final states\n    finals = self._final_states.intersection(states)\n    if len(finals) > 0:\n      ip.append(VM.MultiMatch(finals))\n\n    # do any interupts required\n    interupts = self._interupt_states.intersection(states)\n    if len(interupts) > 0:\n      ip.append(VM.MultiInterupt(interupts))\n\n    # consume a character\n    ip.append(VM.Consume())\n\n    ts = self.transitions(states, cached_transitions)\n\n    if debug:\n      print 'compiling bytecode for stateset:\\n\\t%s\\n\\t0x%x: %s' % (states,ip,(defaults,ts))\n\n    def mkbytecode(t):\n      return lambda: self._transitions_to_dfa_bytecode(states, t, cached_tcode, debug=debug, compiled_states=compiled_states, gathered_epsilons=gathered_epsilons, cached_transitions=cached_transitions, reltags_cache=reltags_cache)\n\n    # for any of the non-default states add a conditional jmp\n    for k in ts:\n\n      if k in (NFA.ANY, NFA.EPSILON):\n        continue\n\n      jmppoint = VM.DelayedArg(mkbytecode(k))\n      ip.append(VM.Compare(k))\n      ip.append(VM.CondJmp(jmppoint))\n\n    # jmp to default state if there is one, otherwise leave\n    defaults = self.nextstates(states, NFA.ANY)\n    if len(defaults) > 0:\n      jmppoint = VM.DelayedArg(mkbytecode(NFA.ANY))\n      ip.append(VM.Jmp(jmppoint))\n    else:\n      ip.append(VM.Leave())\n\n    # return the instruction pointer\n    return ip"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd training data to the training_data dictionary.", "response": "def _add_training_data(self, src, dst, symbol):\n        \"\"\"\n        Training_data is a dictionary from strings to lists.\n        - Each string (key) is an access string\n        - Each list (value) is a list of tuples (target_state, [symbols directed to that\n        state]). These represent that a transition exists from the state used as key to the first\n        part of the training_data to the dst state which is the first part of the tuple\n        with all the symbols in the list in the SECOND part of the tuple.\n        Args:\n            src (str): The source state\n            dst (str): The target state\n            symbol (str): The transition symbol\n        Returns:\n            None\n        \"\"\"\n        src_data = self.training_data[src]\n        for (s, v) in src_data:\n            if s == dst:\n                v.append(symbol)\n                return\n        src_data.append((dst, [symbol]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_closed(self):\n        old_training_data = self.training_data\n        self.training_data = {x: [] for x in self.sm_vector}\n        for t in self.smi_vector:\n            src_state = t[:-1]\n            symbol = t[-1:]\n            found = False\n            for dst_state in self.sm_vector:\n                if self.observation_table[dst_state] == self.observation_table[t]:\n                    self._add_training_data(src_state, dst_state, symbol)\n                    found = True\n                    break\n            if not found:\n                return False, t\n\n        assert self.training_data != old_training_data, \\\n            \"No update happened from previous round. The algo will loop infinetely\"\n        return True, None", "response": "_check if the observation table is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fill_table_entry(self, row, col):\n        self.observation_table[row, col] = self._membership_query(row + col)", "response": "Fill an entry in the observation table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning the string in the hypothesis automaton for the given index steps and return the access string for the state reached concatanated with the", "response": "def _run_in_hypothesis(self, mma, w_string, index):\n        \"\"\"\"\"\n        Run the string in the hypothesis automaton for index steps and then\n        return the access string for the state reached concatanated with the\n        rest of the string w.\n        Args:\n            mma (DFA): The hypothesis automaton\n            w_string (str): The examined string to be consumed\n            index (int): The index value for selecting the prefix of w\n        Return:\n            str: The access string\n        \"\"\"\n        state = mma.states[0]\n        s_index = 0\n        for i in range(index):\n            for arc in state:\n                if arc.guard.is_sat(w_string[i]):\n                    state = mma.states[arc.dst_state]\n                    s_index = arc.dst_state\n\n        # The id of the state is its index inside the Sm list\n        access_string = self.observation_table.sm_vector[s_index]\n        logging.debug(\n            'Access string for %d: %s - %d ',\n            index,\n            access_string,\n            s_index)\n        return access_string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a counterexample in the Rivest - Schapire way.", "response": "def _process_counter_example(self, mma, w_string):\n        \"\"\"\"\n        Process a counterexample in the Rivest-Schapire way.\n        Args:\n            mma (DFA): The hypothesis automaton\n            w_string (str): The examined string to be consumed\n        Return:\n            None\n        \"\"\"\n        if len(w_string) == 1:\n            self.observation_table.smi_vector.append(w_string)\n            for exp in self.observation_table.em_vector:\n                self._fill_table_entry(w_string, exp)\n\n        diff = len(w_string)\n        same = 0\n        membership_answer = self._membership_query(w_string)\n        while True:\n            i = (same + diff) / 2\n            access_string = self._run_in_hypothesis(mma, w_string, i)\n            if membership_answer != self._membership_query(access_string + w_string[i:]):\n                diff = i\n            else:\n                same = i\n            if diff - same == 1:\n                break\n\n        # First check if the transition is part of our training data.\n        access_string = self._run_in_hypothesis(mma, w_string, diff - 1)\n        wrong_transition = access_string + w_string[diff - 1]\n        if wrong_transition not in self.observation_table.smi_vector:\n            # If transition is not part of our training data add s_ib to Smi and\n            # return to checking table closedness.\n            self.observation_table.smi_vector.append(wrong_transition)\n            for exp in self.observation_table.em_vector:\n                self._fill_table_entry(wrong_transition, exp)\n            return\n\n        # This point presents a tradeoff between equivalence and membership\n        # queries. If the transition in the counterexample'input_string breakpoint is not\n        # part of our current training data (i.e. s_ib is not part of our Smi\n        # set), then we assume a wrong transition and return to checking table\n        # closure by adding s_ib to our training data. This saves a number of\n        # membership queries since we don't add a row in our table unless\n        # absolutely necessary. Notice that even if Equivalence queries are\n        # expensive in general caching the result will be able to discover that\n        # this iteration required a new state in the next equivalence query.\n\n        exp = w_string[diff:]\n        self.observation_table.em_vector.append(exp)\n        for row in self.observation_table.sm_vector + self.observation_table.smi_vector:\n            self._fill_table_entry(row, exp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of transitions that are predicate guards for the given state.", "response": "def _get_predicate_guards(self, state, state_training_data):\n        \"\"\"\n        Args:\n            state (DFA state): The dfa state\n            state_training_data (list): The training data set\n        Returns:\n            list: A list of transitions\n        \"\"\"\n        # choose the sink transition.\n\n        # First option: Just the maximum transition\n        # sink = max(state_training_data, key=lambda x: len(x[1]))[0]\n\n        # Second option: Heuristics based on RE filters properties\n        max_size_trans = max(state_training_data, key=lambda x: len(x[1]))\n        max_size_trans_l = [x for x in state_training_data if\n                            len(x[1]) == len(max_size_trans[1])]\n\n        target_states = [t[0] for t in max_size_trans_l]\n        if len(max_size_trans_l) == 1:\n            sink = max_size_trans[0]\n        elif '' in target_states:\n            sink = ''\n        elif state in target_states:\n            sink = state\n        else:\n            sink = random.choice(target_states)\n        # End of sink selection\n\n        transitions = []\n        known_symbols = []\n        for (t, data) in state_training_data:\n            if t == sink:\n                continue\n            pred = SetPredicate(data)\n            transitions.append((t, pred))\n            known_symbols += data\n        transitions.append(\n            (sink, SetPredicate(set(self.alphabet) - set(known_symbols))))\n        return transitions"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Mealy Machine that is a conjecture of the current state of the state.", "response": "def get_sfa_conjecture(self):\n        \"\"\"\n        Utilize the observation table to construct a Mealy Machine.\n        The library used for representing the Mealy Machine is the python\n        bindings of the openFST library (pyFST).\n        Args:\n            None\n        Returns:\n            MealyMachine: A mealy machine build based on a closed and consistent\n        observation table.\n        \"\"\"\n        sfa = SFA(self.alphabet)\n        for s in self.observation_table.sm_vector:\n            transitions = self._get_predicate_guards(\n                s, self.observation_table.training_data[s])\n            for (t, pred) in transitions:\n                src_id = self.observation_table.sm_vector.index(s)\n                dst_id = self.observation_table.sm_vector.index(t)\n                assert isinstance(\n                    pred, SetPredicate), \"Invalid type for predicate {}\".format(pred)\n                sfa.add_arc(src_id, dst_id, pred)\n\n        # Mark the final states in the hypothesis automaton.\n        i = 0\n        for s in self.observation_table.sm_vector:\n            sfa.states[i].final = self.observation_table[s, self.epsilon]\n            i += 1\n        return sfa"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init_table(self):\n        self.observation_table.sm_vector.append(self.epsilon)\n        self.observation_table.smi_vector = [random.choice(self.alphabet)]\n        self.observation_table.em_vector.append(self.epsilon)\n\n        self._fill_table_entry(self.epsilon, self.epsilon)\n        for s in self.observation_table.smi_vector:\n            self._fill_table_entry(s, self.epsilon)", "response": "Initialize the observation table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _init_table_from_dfa(self, mma):\n        observation_table_init = ObservationTableInit(self.epsilon, self.alphabet)\n        sm_vector, smi_vector, em_vector = observation_table_init.initialize(mma, True)\n        self.observation_table.sm_vector = sm_vector\n        self.observation_table.smi_vector = smi_vector\n        self.observation_table.em_vector = em_vector\n\n        logging.info('Initialized from DFA em_vector table is the following:')\n        logging.info(em_vector)\n\n        self._fill_table_entry(self.epsilon, self.epsilon)\n\n        # list(set([])) is used to remove duplicates, [1:0] to remove epsilon\n        for row in sorted(list(set(sm_vector + smi_vector)), key=len)[1:]:\n            for column in em_vector:\n                self._fill_table_entry(str(row), str(column))", "response": "Initializes the observation table from the input automaton"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimplementing the high level loop of the algorithm for learning a Mealy machine. Args: mma: Returns: MealyMachine: A model for the Mealy machine to be learned.", "response": "def learn_sfa(self, mma=None):\n        \"\"\"\n        Implements the high level loop of the algorithm for learning a\n        Mealy machine.\n        Args:\n            mma:\n        Returns:\n            MealyMachine: A model for the Mealy machine to be learned.\n        \"\"\"\n        logging.info('Initializing learning procedure.')\n        if mma:\n            self._init_table_from_dfa(mma)\n        else:\n            self._init_table()\n\n        logging.info('Generating a closed and consistent observation table.')\n        while True:\n\n            closed = False\n            # Make sure that the table is closed\n            while not closed:\n                logging.debug('Checking if table is closed.')\n                closed, s = self.observation_table.is_closed()\n                if not closed:\n                    logging.debug('Closing table.')\n                    self._ot_make_closed(s)\n                else:\n                    logging.debug('Table closed.')\n\n            # Create conjecture\n            sfa = self.get_sfa_conjecture()\n\n            logging.info('Generated conjecture machine with %d states.',\n                         len(list(sfa.states)))\n\n            # _check correctness\n            logging.debug('Running equivalence query.')\n            found, counter_example = self._equivalence_query(sfa)\n\n            # Are we done?\n            if found:\n                logging.info('No counterexample found. Hypothesis is correct!')\n                break\n\n            # Add the new experiments into the table to reiterate the\n            # learning loop\n            logging.info(\n                'Processing counterexample %s with length %d.',\n                counter_example,\n                len(counter_example))\n            self._process_counter_example(sfa, counter_example)\n\n        logging.info('Learning complete.')\n        return '', sfa"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that logs at the given level", "response": "def make_log_metric(level=logging.INFO, msg=\"%d items in %.2f seconds\"):\n    \"\"\"Make a new metric function that logs at the given level\n\n    :arg int level: logging level, defaults to ``logging.INFO``\n    :arg string msg: logging message format string, taking ``count`` and ``elapsed``\n    :rtype: function\n    \"\"\"\n    def log_metric(name, count, elapsed):\n        log_name = 'instrument.{}'.format(name) if name else 'instrument'\n        logging.getLogger(log_name).log(level, msg, count, elapsed)\n    return log_metric"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if cls has on overridable __init__.", "response": "def ctor_overridable(cls):\n    \"\"\"Return true if cls has on overridable __init__.\"\"\"\n    prev_init = getattr(cls, \"__init__\", None)\n    if not callable(prev_init):\n        return True\n    if prev_init in [object.__init__, _auto_init]:\n        return True\n    if getattr(prev_init, '_clobber_ok', False):\n        return True\n\n    print(cls, prev_init, getattr(prev_init, '_clobber_ok', 'missing'))\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DBObject(table_name, versioning=VersioningTypes.NONE):\n    def wrapped(cls):\n        field_names = set()\n        all_fields = []\n\n        for name in dir(cls):\n            fld = getattr(cls, name)\n            if fld and isinstance(fld, Field):\n                fld.name = name\n                all_fields.append(fld)\n                field_names.add(name)\n\n        def add_missing_field(name, default='', insert_pos=None):\n            if name not in field_names:\n                fld = Field(default=default)\n                fld.name = name\n                all_fields.insert(\n                    len(all_fields) if insert_pos is None else insert_pos,\n                    fld\n                )\n\n        add_missing_field('id', insert_pos=0)\n        add_missing_field('_create_date')\n        add_missing_field('_last_update')\n        if versioning == VersioningTypes.DELTA_HISTORY:\n            add_missing_field('_version_hist', default=list)\n\n        # Things we count on as part of our processing\n        cls.__table_name__ = table_name\n        cls.__versioning__ = versioning\n        cls.__fields__ = all_fields\n\n        # Give them a ctor for free - but make sure we aren't clobbering one\n        if not ctor_overridable(cls):\n            raise TypeError(\n                'Classes with user-supplied __init__ should not be decorated '\n                'with DBObject. Use the setup method'\n            )\n        cls.__init__ = _auto_init\n\n        # Duck-type the class for our data methods\n        cls.get_table_name = classmethod(_get_table_name)\n        cls.get_id = _get_id\n        cls.set_id = _set_id\n        cls.to_data = _to_data\n        cls.from_data = classmethod(_from_data)\n        cls.index_names = classmethod(_index_names)\n        cls.indexes = _indexes\n        # Bonus methods they get for using gludb.simple\n        cls.get_version_hist = _get_version_hist\n\n        # Register with our abc since we actually implement all necessary\n        # functionality\n        Storable.register(cls)\n\n        # And now that we're registered, we can also get the database\n        # read/write functionality for free\n        cls = DatabaseEnabled(cls)\n\n        if versioning == VersioningTypes.DELTA_HISTORY:\n            cls.save = _delta_save(cls.save)\n\n        return cls\n\n    return wrapped", "response": "Class decorator for storing DBObject."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntrain the cluster store.", "response": "def train(self, data, target, **kwargs):\n        \"\"\"\n        Used in the training phase.  Override.\n        \"\"\"\n        non_predictors = [i.replace(\" \", \"_\").lower() for i in list(set(data['team']))] + [\"team\", \"next_year_wins\"]\n        self.column_names = [l for l in list(data.columns) if l not in non_predictors]\n        results, folds = self.cross_validate(data, non_predictors, **kwargs)\n        self.gather_results(results, folds, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompile a simex code to regex.", "response": "def compile(self, code):\n        \"\"\"\n        Compile a simex code (e.g. <a href=\"{{ url }}\">{{ anything }}</a>) to regex.\n\n        Returns regex.\n        \"\"\"\n        is_plain_text = True\n        compiled_regex = r\"\"\n        for chunk in self.delimiter_regex().split(code):\n            if is_plain_text:\n                compiled_regex = compiled_regex + simex_escape(chunk, flexible_whitespace=self._flexible_whitespace)\n            else:\n                stripped_chunk = chunk.strip()\n                if stripped_chunk in self._regexes.keys():\n                    compiled_regex = u\"{0}{1}\".format(\n                        compiled_regex,\n                        self._regexes[stripped_chunk]\n                    )\n                else:\n                    raise KeyNotFound(\"'{0}' not found in keys\")\n            is_plain_text = not is_plain_text\n        if self._exact:\n            compiled_regex = r\"^\" + compiled_regex + r\"$\"\n        return compile(compiled_regex)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_command(self):\n        try:\n            params = self.unbound_func.__click_params__\n            params.reverse()\n            del self.unbound_func.__click_params__\n        except AttributeError:\n            params = []\n        help = inspect.getdoc(self.real_func)\n        if isinstance(help, bytes):\n            help = help.decode('utf-8')\n        self.options.setdefault('help', help)\n        @pass_script_info_decorator\n        def callback(info, *args, **kwargs):\n            if self.with_reloader:\n                app = info.load_app()\n                if app.debug:\n                    def inner():\n                        return self.command_callback(info, *args, **kwargs)\n                    run_with_reloader(inner, extra_files=get_reloader_extra_files())\n                    return\n            self.command_callback(info, *args, **kwargs)\n        return self.cls(name=self.name, callback=callback, params=params, **self.options)", "response": "Creates the click command wrapping the function\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef command_line_options(command_line_arguments):\n\n  # set up command line parser\n  parser = argparse.ArgumentParser(description=__doc__,\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n  parser.add_argument('-d', '--files', required=True, nargs='+', help = \"A list of score files to evaluate.\")\n  parser.add_argument('-b', '--baselines', default=[], nargs='+', help = \"A list of baseline results to add to the plot\")\n\n  parser.add_argument('-D', '--directory', default = '.', help = \"A directory, where to find the --files\")\n  parser.add_argument('-B', '--baseline-directory', default = '.', help = \"A directory, where to find the --baselines\")\n\n  parser.add_argument('-R', '--auto-baselines', choices = ('bioid', 'mit-cmu'), help = \"Automatically add the baselines for the given database\")\n\n  parser.add_argument('-l', '--legends', nargs='+', help = \"A list of legend strings used for ROC, CMC and DET plots; if given, must be the same number than --files plus --baselines.\")\n  parser.add_argument('-w', '--output', default = 'FROC.pdf', help = \"If given, FROC curves will be plotted into the given pdf file.\")\n  parser.add_argument('-c', '--count-detections', action='store_true', help = \"Counts the number of detections (positive is higher than negative, per file).\")\n  parser.add_argument('-n', '--max', type=int, nargs=2, default=(160,70), help = \"The highest false alarms and the lowest detection rate to plot\")\n  parser.add_argument('-t', '--title', default='FROC', help = \"The title of the plot\")\n\n  parser.add_argument('--self-test', action='store_true', help=argparse.SUPPRESS)\n\n  # add verbosity option\n  bob.core.log.add_command_line_option(parser)\n  args = parser.parse_args(command_line_arguments)\n  bob.core.log.set_verbosity_level(logger, args.verbose)\n\n  if args.legends is not None:\n    count = len(args.files) + (len(args.baselines) if args.baselines is not None else 0)\n    if len(args.legends) != count:\n      logger.error(\"The number of --files (%d) plus --baselines (%d) must be the same as --legends (%d)\", len(args.files), len(args.baselines) if args.baselines else 0, len(args.legends))\n      args.legends = None\n\n  # update legends when they are not specified on command line\n  if args.legends is None:\n    args.legends = args.files if not args.baselines else args.files + args.baselines\n    args.legends = [l.replace(\"_\",\"-\") for l in args.legends]\n\n  if args.auto_baselines == 'bioid':\n    args.baselines.extend([\"baselines/baseline_detection_froba_mct_BIOID\", \"cosmin/BIOID/face.elbp.proj0.var.levels10.roc\"])\n    args.legends.extend([\"Froba\", \"Cosmin\"])\n  elif args.auto_baselines == 'mit-cmu':\n    args.baselines.extend([\"baselines/baseline_detection_fcboost_MIT+CMU\", \"baselines/baseline_detection_viola_rapid1_MIT+CMU\", \"cosmin/MIT+CMU/face.elbp.proj0.var.levels10.roc\"])\n    args.legends.extend([\"FcBoost\", \"Viola\", \"Cosmin\"])\n\n  return args", "response": "Parse the command line options and return a list of command line options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(command_line_arguments=None):\n\n  args = command_line_options(command_line_arguments)\n\n  # get some colors for plotting\n  cmap = mpl.cm.get_cmap(name='hsv')\n  count = len(args.files) + (len(args.baselines) if args.baselines else 0)\n  colors = [cmap(i) for i in numpy.linspace(0, 1.0, count+1)]\n\n  # First, read the score files\n  logger.info(\"Loading %d score files\" % len(args.files))\n\n  scores = [read_score_file(os.path.join(args.directory, f)) for f in args.files]\n\n  false_alarms = []\n  detection_rate = []\n  logger.info(\"Computing FROC curves\")\n  for score in scores:\n    # compute some thresholds\n    tmin = min(score[2])\n    tmax = max(score[2])\n    count = 100\n    thresholds = [tmin + float(x)/count * (tmax - tmin) for x in range(count+2)]\n    false_alarms.append([])\n    detection_rate.append([])\n    for threshold in thresholds:\n      detection_rate[-1].append(numpy.count_nonzero(numpy.array(score[1]) >= threshold) / float(score[0]))\n      false_alarms[-1].append(numpy.count_nonzero(numpy.array(score[2]) >= threshold))\n    # to display 0 in a semilogx plot, we have to add a little\n#    false_alarms[-1][-1] += 1e-8\n\n  # also read baselines\n  if args.baselines is not None:\n    for baseline in args.baselines:\n      dr = []\n      fa = []\n      with open(os.path.join(args.baseline_directory, baseline)) as f:\n        for line in f:\n          splits = line.rstrip().split()\n          dr.append(float(splits[0]))\n          fa.append(int(splits[1]))\n      false_alarms.append(fa)\n      detection_rate.append(dr)\n\n  logger.info(\"Plotting FROC curves to file '%s'\", args.output)\n  # create a multi-page PDF for the ROC curve\n  pdf = PdfPages(args.output)\n  figure = _plot_froc(false_alarms, detection_rate, colors, args.legends, args.title, args.max)\n  mpl.xlabel('False Alarm (of %d pruned)' % len(scores[0][2]))\n  mpl.ylabel('Detection Rate in \\%% (total %d faces)' % scores[0][0])\n  pdf.savefig(figure)\n  pdf.close()\n\n  if args.count_detections:\n    for i, f in enumerate(args.files):\n      det, all = count_detections(f)\n      print(\"The number of detected faces for %s is %d out of %d\" % (args.legends[i], det, all))", "response": "Reads score files computes error measures and plots curves."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprioritizes resource position in the final HTML.", "response": "def priority(var):\n    \"\"\"Prioritizes resource position in the final HTML. To be fed into sorted(key=).\n\n    Javascript consoles throw errors if Bootstrap's js file is mentioned before jQuery. Using this function such errors\n    can be avoided. Used internally.\n\n    Positional arguments:\n    var -- value sent by list.sorted(), which is a value in Statics().all_variables.\n\n    Returns:\n    Either a number if sorting is enforced for the value in `var`, or returns `var` itself.\n    \"\"\"\n    order = dict(JQUERY='0', BOOTSTRAP='1')\n    return order.get(var, var)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_resources(minify=False):\n    all_resources = dict()\n    subclasses = resource_base.ResourceBase.__subclasses__() + resource_definitions.ResourceAngular.__subclasses__()\n    for resource in subclasses:\n        obj = resource(minify)\n        all_resources[resource.RESOURCE_NAME] = dict(css=tuple(obj.resources_css), js=tuple(obj.resources_js))\n    return all_resources", "response": "Find all resources which subclass ResourceBase."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef citedby_pid(self, pid, metaonly=False, from_heap=True):\n\n        if from_heap is True:\n            result = citations.raw_data(pid)\n\n            if result and 'cited_by' in result and metaonly is True:\n                del(result['cited_by'])\n                return result\n\n            if result:\n                return result\n\n        url = urljoin(self.CITEDBY_URL, self.PID_ENDPOINT)\n\n        params = {\n            \"q\": pid,\n            \"metaonly\": \"true\" if metaonly is True else \"false\"\n        }\n\n        result = self._do_request(url, params=params)\n\n        return result", "response": "Retrieve citedby documents from a given SciELO PID number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve citedby documents from a given SciELO PID number.", "response": "def citedby_pid(self, pid, metaonly=False, from_heap=True):\n        \"\"\"\n        Retrieve citedby documents from a given PID number.\n\n        pid: SciELO PID number\n        metaonly: will retrieve only the metadata of the requested article citations including the number of citations it has received.\n        from_heap: will retrieve the number of citations from a preproduced report, it will not fetch the api. Much faster results but not extremelly updated.\n        \"\"\"\n\n        if from_heap is True:\n            result = citations.raw_data(pid)\n\n            if result and 'cited_by' in result and metaonly is True:\n                del(result['cited_by'])\n                return result\n\n            if result:\n                return result\n\n        result = self.client.citedby_pid(pid, metaonly=metaonly)\n\n        try:\n            return json.loads(result)\n        except:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfreeing queries to ES index.", "response": "def search(self, dsl, params):\n        \"\"\"\n        Free queries to ES index.\n\n        dsl (string): with DSL query\n        params (list): [(key, value), (key, value)]\n            where key is a query parameter, and value is the value required for parameter, ex: [('size', '0'), ('search_type', 'count')]\n        \"\"\"\n\n        query_parameters = []\n        for key, value in params:\n            query_parameters.append(self.CITEDBY_THRIFT.kwargs(str(key), str(value)))\n\n        try:\n            result = self.client.search(dsl, query_parameters)\n        except self.CITEDBY_THRIFT.ServerError:\n            raise ServerError('you may trying to run a bad DSL Query')\n\n        try:\n            return json.loads(result)\n        except:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a message to a specific service.", "response": "def fire(self, name, operation, args=None, **kwargs):\n        \"\"\"Send a message without waiting for a reply\n\n        @param name: name of destination service queue\n        @param operation: name of service operation to invoke\n        @param args: dictionary of keyword args to pass to operation.\n                     Use this OR kwargs.\n        @param kwargs: additional args to pass to operation\n        \"\"\"\n\n        if args:\n            if kwargs:\n                raise TypeError(\"specify args dict or keyword arguments, not both\")\n        else:\n            args = kwargs\n\n        d = dict(op=operation, args=args)\n        headers = {'sender': self.add_sysname(self.name)}\n\n        dest = self.add_sysname(name)\n\n        def _fire(channel):\n            with Producer(channel) as producer:\n                producer.publish(d, routing_key=dest,\n                    headers=headers, serializer=self._serializer,\n                    exchange=self._exchange, declare=[self._exchange])\n\n        log.debug(\"sending message to %s\", dest)\n        with connections[self._pool_conn].acquire(block=True) as conn:\n            _, channel = self.ensure(conn, _fire)\n            conn.maybe_close_channel(channel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call(self, name, operation, timeout=10, args=None, **kwargs):\n\n        if args:\n            if kwargs:\n                raise TypeError(\"specify args dict or keyword arguments, not both\")\n        else:\n            args = kwargs\n\n        # create a direct queue for the reply. This may end up being a\n        # bottleneck for performance: each rpc call gets a brand new\n        # exclusive queue. However this approach is used nova.rpc and\n        # seems to have carried them pretty far. If/when this\n        # becomes a bottleneck we can set up a long-lived backend queue and\n        # use correlation_id to deal with concurrent RPC calls. See:\n        #   http://www.rabbitmq.com/tutorials/tutorial-six-python.html\n        msg_id = uuid.uuid4().hex\n\n        # expire the reply queue shortly after the timeout. it will be\n        # (lazily) deleted by the broker if we don't clean it up first\n        queue_arguments = {'x-expires': int((timeout + 1) * 1000)}\n        queue = Queue(name=msg_id, exchange=self._exchange, routing_key=msg_id,\n                      durable=False, queue_arguments=queue_arguments)\n\n        messages = []\n        event = threading.Event()\n\n        def _callback(body, message):\n            messages.append(body)\n            message.ack()\n            event.set()\n\n        d = dict(op=operation, args=args)\n        headers = {'reply-to': msg_id, 'sender': self.add_sysname(self.name)}\n        dest = self.add_sysname(name)\n\n        def _declare_and_send(channel):\n            consumer = Consumer(channel, (queue,), callbacks=(_callback,))\n            with Producer(channel) as producer:\n                producer.publish(d, routing_key=dest, headers=headers,\n                    exchange=self._exchange, serializer=self._serializer)\n            return consumer\n\n        log.debug(\"sending call to %s:%s\", dest, operation)\n        with connections[self._pool_conn].acquire(block=True) as conn:\n            consumer, channel = self.ensure(conn, _declare_and_send)\n            try:\n                self._consume(conn, consumer, timeout=timeout, until_event=event)\n\n                # try to delete queue, but don't worry if it fails (will expire)\n                try:\n                    queue = queue.bind(channel)\n                    queue.delete(nowait=True)\n                except Exception:\n                    log.exception(\"error deleting queue\")\n\n            finally:\n                conn.maybe_close_channel(channel)\n\n        msg_body = messages[0]\n        if msg_body.get('error'):\n            raise_error(msg_body['error'])\n        else:\n            return msg_body.get('result')", "response": "Send a message and wait for reply."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle an operation using the specified function", "response": "def handle(self, operation, operation_name=None, sender_kwarg=None):\n        \"\"\"Handle an operation using the specified function\n\n        @param operation: function to call for this operation\n        @param operation_name: operation name. if unspecified operation.__name__ is used\n        @param sender_kwarg: optional keyword arg on operation to feed in sender name\n        \"\"\"\n        if not self._consumer:\n            self._consumer = DashiConsumer(self, self._conn,\n                    self._name, self._exchange, sysname=self._sysname)\n        self._consumer.add_op(operation_name or operation.__name__, operation,\n                              sender_kwarg=sender_kwarg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel(self, block=True):\n        if self._consumer:\n            self._consumer.cancel(block=block)", "response": "Cancel a call to consume"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlinking a custom exception thrown on the receiver to a dashi exception.", "response": "def link_exceptions(self, custom_exception=None, dashi_exception=None):\n        \"\"\"Link a custom exception thrown on the receiver to a dashi exception\n        \"\"\"\n        if custom_exception is None:\n            raise ValueError(\"custom_exception must be set\")\n        if dashi_exception is None:\n            raise ValueError(\"dashi_exception must be set\")\n\n        self._linked_exceptions[custom_exception] = dashi_exception"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure(self, connection, func, *args, **kwargs):\n        channel = None\n        while 1:\n            try:\n                if channel is None:\n                    channel = connection.channel()\n                return func(channel, *args, **kwargs), channel\n            except (connection.connection_errors, IOError):\n                self._call_errback()\n\n            channel = self.connect(connection)", "response": "Perform an operation until success\nCOOKIE is set"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef re_tab(s):\n    l = []\n    p = 0\n    for i in range(8, len(s), 8):\n        if s[i - 2:i] == \"  \":\n            # collapse two or more spaces into a tab\n            l.append(s[p:i].rstrip() + \"\\t\")\n            p = i\n\n    if p == 0:\n        return s\n    else:\n        l.append(s[p:])\n        return \"\".join(l)", "response": "Return a tabbed string from an expanded one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread another line from the file.", "response": "def read_next_line(self):\n        \"\"\"Read another line from the file.\"\"\"\n\n        next_line = self.file.readline()\n\n        if not next_line or next_line[-1:] != '\\n':\n            # no newline on last line of file\n            self.file = None\n        else:\n            # trim newline characters\n            next_line = next_line[:-1]\n\n        expanded = next_line.expandtabs()\n\n        edit = urwid.Edit(\"\", expanded, allow_tab=True)\n        edit.set_edit_pos(0)\n        edit.original_text = next_line\n        self.lines.append(edit)\n\n        return next_line"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_at_pos(self, pos):\n\n        if pos < 0:\n            # line 0 is the start of the file, no more above\n            return None, None\n\n        if len(self.lines) > pos:\n            # we have that line so return it\n            return self.lines[pos], pos\n\n        if self.file is None:\n            # file is closed, so there are no more lines\n            return None, None\n\n        assert pos == len(self.lines), \"out of order request?\"\n\n        self.read_next_line()\n\n        return self.lines[-1], pos", "response": "Return a widget for the line number passed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_focus(self):\n\n        focus = self.lines[self.focus]\n        pos = focus.edit_pos\n        edit = urwid.Edit(\"\", focus.edit_text[pos:], allow_tab=True)\n        edit.original_text = \"\"\n        focus.set_edit_text(focus.edit_text[:pos])\n        edit.set_edit_pos(0)\n        self.lines.insert(self.focus + 1, edit)", "response": "Divide the focus edit widget at the cursor location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef combine_focus_with_prev(self):\n\n        above, ignore = self.get_prev(self.focus)\n        if above is None:\n            # already at the top\n            return\n\n        focus = self.lines[self.focus]\n        above.set_edit_pos(len(above.edit_text))\n        above.set_edit_text(above.edit_text + focus.edit_text)\n        del self.lines[self.focus]\n        self.focus -= 1", "response": "Combine the focus edit widget with the one above."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncombines the focus edit widget with the one below.", "response": "def combine_focus_with_next(self):\n        \"\"\"Combine the focus edit widget with the one below.\"\"\"\n\n        below, ignore = self.get_next(self.focus)\n        if below is None:\n            # already at bottom\n            return\n\n        focus = self.lines[self.focus]\n        focus.set_edit_text(focus.edit_text + below.edit_text)\n        del self.lines[self.focus + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlasts resort for keypresses.", "response": "def handle_keypress(self, k):\n        \"\"\"Last resort for keypresses.\"\"\"\n        if k == \"esc\":\n            self.save_file()\n            raise urwid.ExitMainLoop()\n        elif k == \"delete\":\n            # delete at end of line\n            self.walker.combine_focus_with_next()\n        elif k == \"backspace\":\n            # backspace at beginning of line\n            self.walker.combine_focus_with_prev()\n        elif k == \"enter\":\n            # start new line\n            self.walker.split_focus()\n            # move the cursor to the new line and reset pref_col\n            self.view.keypress(size, \"down\")\n            self.view.keypress(size, \"home\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_file(self):\n\n        l = []\n        walk = self.walker\n        for edit in walk.lines:\n            # collect the text already stored in edit widgets\n            if edit.original_text.expandtabs() == edit.edit_text:\n                l.append(edit.original_text)\n            else:\n                l.append(re_tab(edit.edit_text))\n\n        # then the rest\n        while walk.file is not None:\n            l.append(walk.read_next_line())\n\n        # write back to disk\n        outfile = open(self.save_name, \"w\")\n\n        l_iter = iter(l)\n\n        line = next(l_iter)\n        prefix = \"\"\n\n        while True:\n            try:\n                outfile.write(prefix + line)\n                prefix = \"\\n\"\n                line = next(l_iter)\n            except StopIteration:\n                if line != \"\\n\":\n                    outfile.write(\"\\n\")\n                break", "response": "Write the file out to disk."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a forms. Media instance with the basic editor media and media from all registered extensions.", "response": "def _media(self):\n        \"\"\"\n        Returns a forms.Media instance with the basic editor media and media\n        from all registered extensions.\n        \"\"\"\n        css = ['markymark/css/markdown-editor.css']\n        iconlibrary_css = getattr(\n            settings,\n            'MARKYMARK_FONTAWESOME_CSS',\n            'markymark/fontawesome/fontawesome.min.css'\n        )\n        if iconlibrary_css:\n            css.append(iconlibrary_css)\n\n        media = forms.Media(\n            css={'all': css},\n            js=('markymark/js/markdown-editor.js',)\n        )\n\n        # Use official extension loading to initialize all extensions\n        # and hook in extension-defined media files.\n        renderer = initialize_renderer()\n\n        for extension in renderer.registeredExtensions:\n            if hasattr(extension, 'media'):\n                media += extension.media\n        return media"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a path and computes the relative path from the parent directory.", "response": "def rel(path, parent=None, par=False):\n  \"\"\"\n  Takes *path* and computes the relative path from *parent*. If *parent* is\n  omitted, the current working directory is used.\n\n  If *par* is #True, a relative path is always created when possible.\n  Otherwise, a relative path is only returned if *path* lives inside the\n  *parent* directory.\n  \"\"\"\n\n  try:\n    res = os.path.relpath(path, parent)\n  except ValueError:\n    # Raised eg. on Windows for differing drive letters.\n    if not par:\n      return abs(path)\n    raise\n  else:\n    if not par and not issub(res):\n      return abs(path)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if path is a relative path that does not point outside .", "response": "def issub(path):\n  \"\"\"\n  Returns #True if *path* is a relative path that does not point outside\n  of its parent directory or is equal to its parent directory (thus, this\n  function will also return False for a path like `./`).\n  \"\"\"\n\n  if isabs(path):\n    return False\n  if path.startswith(curdir + sep) or path.startswith(pardir + sep) or \\\n      path == curdir or path == pardir:\n    return False\n  return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef glob(patterns, parent=None, excludes=None, include_dotfiles=False,\n         ignore_false_excludes=False):\n  \"\"\"\n  Wrapper for #glob2.glob() that accepts an arbitrary number of\n  patterns and matches them. The paths are normalized with #norm().\n\n  Relative patterns are automaticlly joined with *parent*. If the\n  parameter is omitted, it defaults to the current working directory.\n\n  If *excludes* is specified, it must be a string or a list of strings\n  that is/contains glob patterns or filenames to be removed from the\n  result before returning.\n\n  > Every file listed in *excludes* will only remove **one** match from\n  > the result list that was generated from *patterns*. Thus, if you\n  > want to exclude some files with a pattern except for a specific file\n  > that would also match that pattern, simply list that file another\n  > time in the *patterns*.\n\n  # Parameters\n  patterns (list of str): A list of glob patterns or filenames.\n  parent (str): The parent directory for relative paths.\n  excludes (list of str): A list of glob patterns or filenames.\n  include_dotfiles (bool): If True, `*` and `**` can also capture\n    file or directory names starting with a dot.\n  ignore_false_excludes (bool): False by default. If True, items listed\n    in *excludes* that have not been globbed will raise an exception.\n\n  # Returns\n  list of str: A list of filenames.\n  \"\"\"\n\n  if not glob2:\n    raise glob2_ext\n\n  if isinstance(patterns, str):\n    patterns = [patterns]\n\n  if not parent:\n    parent = os.getcwd()\n\n  result = []\n  for pattern in patterns:\n    if not os.path.isabs(pattern):\n      pattern = os.path.join(parent, pattern)\n    result += glob2.glob(canonical(pattern))\n\n  for pattern in (excludes or ()):\n    if not os.path.isabs(pattern):\n      pattern = os.path.join(parent, pattern)\n    pattern = canonical(pattern)\n    if not isglob(pattern):\n      try:\n        result.remove(pattern)\n      except ValueError as exc:\n        if not ignore_false_excludes:\n          raise ValueError('{} ({})'.format(exc, pattern))\n    else:\n      for item in glob2.glob(pattern):\n        try:\n          result.remove(item)\n        except ValueError as exc:\n          if not ignore_false_excludes:\n            raise ValueError('{} ({})'.format(exc, pattern))\n\n  return result", "response": "Wrapper for glob2. glob that accepts an arbitrary number of glob patterns and matches them."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addtobase(subject, base_suffix):\n\n  if not base_suffix:\n    return subject\n  base, ext = os.path.splitext(subject)\n  return base + base_suffix + ext", "response": "Adds the string base_suffix to the basename of the subject."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the specified prefix to the last path element in the subject.", "response": "def addprefix(subject, prefix):\n  \"\"\"\n  Adds the specified *prefix* to the last path element in *subject*.\n  If *prefix* is a callable, it must accept exactly one argument, which\n  is the last path element, and return a modified value.\n  \"\"\"\n\n  if not prefix:\n    return subject\n  dir_, base = split(subject)\n  if callable(prefix):\n    base = prefix(base)\n  else:\n    base = prefix + base\n  return join(dir_, base)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a suffix to the specified subject.", "response": "def addsuffix(subject, suffix, replace=False):\n  \"\"\"\n  Adds the specified *suffix* to the *subject*. If *replace* is True, the\n  old suffix will be removed first. If *suffix* is callable, it must accept\n  exactly one argument and return a modified value.\n  \"\"\"\n\n  if not suffix and not replace:\n    return subject\n  if replace:\n    subject = rmvsuffix(subject)\n  if suffix and callable(suffix):\n    subject = suffix(subject)\n  elif suffix:\n    subject += suffix\n  return subject"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rmvsuffix(subject):\n\n  index = subject.rfind('.')\n  if index > subject.replace('\\\\', '/').rfind('/'):\n    subject = subject[:index]\n  return subject", "response": "Removes the suffix from a subject."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the suffix of a filename.", "response": "def getsuffix(subject):\n  \"\"\"\n  Returns the suffix of a filename. If the file has no suffix, returns None.\n  Can return an empty string if the filenam ends with a period.\n  \"\"\"\n\n  index = subject.rfind('.')\n  if index > subject.replace('\\\\', '/').rfind('/'):\n    return subject[index+1:]\n  return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makedirs(path, exist_ok=True):\n\n  try:\n    os.makedirs(path)\n  except OSError as exc:\n    if exist_ok and exc.errno == errno.EEXIST:\n      return\n    raise", "response": "Like os. makedirs but with exist_ok = False."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the flags according to the modstring.", "response": "def chmod_update(flags, modstring):\n  \"\"\"\n  Modifies *flags* according to *modstring*.\n  \"\"\"\n\n  mapping = {\n    'r': (_stat.S_IRUSR, _stat.S_IRGRP, _stat.S_IROTH),\n    'w': (_stat.S_IWUSR, _stat.S_IWGRP, _stat.S_IWOTH),\n    'x': (_stat.S_IXUSR, _stat.S_IXGRP, _stat.S_IXOTH)\n  }\n\n  target, direction = 'a', None\n  for c in modstring:\n    if c in '+-':\n      direction = c\n      continue\n    if c in 'ugoa':\n      target = c\n      direction = None  # Need a - or + after group specifier.\n      continue\n    if c in 'rwx' and direction in '+-':\n      if target == 'a':\n        mask = functools.reduce(operator.or_, mapping[c])\n      else:\n        mask = mapping[c]['ugo'.index(target)]\n      if direction == '-':\n        flags &= ~mask\n      else:\n        flags |= mask\n      continue\n    raise ValueError('invalid chmod: {!r}'.format(modstring))\n\n  return flags"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string representation of the chmod flags.", "response": "def chmod_repr(flags):\n  \"\"\"\n  Returns a string representation of the access flags *flags*.\n  \"\"\"\n\n  template = 'rwxrwxrwx'\n  order = (_stat.S_IRUSR, _stat.S_IWUSR, _stat.S_IXUSR,\n           _stat.S_IRGRP, _stat.S_IWGRP, _stat.S_IXGRP,\n           _stat.S_IROTH, _stat.S_IWOTH, _stat.S_IXOTH)\n  return ''.join(template[i] if flags&x else '-'\n                 for i, x in enumerate(order))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomparing the timestamps of a file src and dst.", "response": "def compare_timestamp(src, dst):\n  \"\"\"\n  Compares the timestamps of file *src* and *dst*, returning #True if the\n  *dst* is out of date or does not exist. Raises an #OSError if the *src*\n  file does not exist.\n  \"\"\"\n\n  try:\n    dst_time = os.path.getmtime(dst)\n  except OSError as exc:\n    if exc.errno == errno.ENOENT:\n      return True  # dst does not exist\n\n  src_time = os.path.getmtime(src)\n  return src_time > dst_time"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef measure_board_rms(control_board, n_samples=10, sampling_ms=10,\n                      delay_between_samples_ms=0):\n    '''\n    Read RMS voltage samples from control board high-voltage feedback circuit.\n    '''\n    try:\n        results = control_board.measure_impedance(n_samples, sampling_ms,\n                                                  delay_between_samples_ms,\n                                                  True, True, [])\n    except RuntimeError:\n        # `RuntimeError` may be raised if, for example, current limit was\n        # reached during measurement.  In such cases, return an empty frame.\n        logger.warning('Error encountered during high-voltage RMS '\n                       'measurement.', exc_info=True)\n        data = pd.DataFrame(None, columns=['board measured V',\n                                           'divider resistor index'])\n    else:\n        data = pd.DataFrame({'board measured V': results.V_hv})\n        data['divider resistor index'] = results.hv_resistor\n    return data", "response": "Measure the RMS voltage samples from a control board high - voltage feedback circuit."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the good one based on the feedback circuit.", "response": "def find_good(control_board, actuation_steps, resistor_index, start_index,\n              end_index):\n    '''\n    Use a binary search over the range of provided actuation_steps to find the\n    maximum actuation voltage that is measured by the board feedback circuit\n    using the specified feedback resistor.\n    '''\n    lower = start_index\n    upper = end_index\n    while lower < upper - 1:\n        index = lower + (upper - lower) / 2\n        v = actuation_steps[index]\n        control_board.set_waveform_voltage(v)\n        data = measure_board_rms(control_board)\n        valid_data = data[data['divider resistor index'] >= 0]\n\n        if (valid_data['divider resistor index'] < resistor_index).sum():\n            # We have some measurements from another resistor.\n            upper = index\n        else:\n            lower = index\n    control_board.set_waveform_voltage(actuation_steps[lower])\n    data = measure_board_rms(control_board)\n    return lower, data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resistor_max_actuation_readings(control_board, frequencies,\n                                    oscope_reading_func):\n    '''\n    For each resistor in the high-voltage feedback resistor bank, read the\n    board measured voltage and the oscilloscope measured voltage for an\n    actuation voltage that nearly saturates the feedback resistor.\n\n    By searching for an actuation voltage near saturation, the signal-to-noise\n    ratio is minimized.\n    '''\n    # Set board amplifier gain to 1.\n    # __NB__ This is likely _far_ lower than the actual gain _(which may be a\n    # factor of several hundred)_..\n    control_board.set_waveform_voltage(0)\n    control_board.auto_adjust_amplifier_gain = False\n    control_board.amplifier_gain = 1.\n\n    # Set waveform voltage to a low value and obtain the corresponding\n    # oscilloscope reading to calculate an approximate gain of the amplifier.\n    target_voltage = 0.1\n    control_board.set_waveform_voltage(target_voltage)\n    oscope_rms = oscope_reading_func()\n    estimated_amplifier_gain = oscope_rms / target_voltage\n\n    # Based on the maximum amplified RMS voltage, define a set of actuation\n    # voltages to search when performing calibration.\n    max_post_gain_V = 0.8 * control_board.max_waveform_voltage\n    max_actuation_V = max_post_gain_V / estimated_amplifier_gain\n    actuation_steps = np.linspace(0.005, max_actuation_V, num=50)\n\n    resistor_count = len(control_board.a0_series_resistance)\n\n    # Define frequency/resistor index pairs to take measurements at.\n    conditions = pd.DataFrame([[r, f] for r in range(resistor_count - 1, -1, -1)\n                               for f in frequencies],\n                              columns=['resistor index', 'frequency'])\n\n    # Define function to process each frequency/resistor index pair.\n    def max_actuation_reading(x):\n        '''\n        Measure maximum board RMS voltage using specified feedback resistor, at\n        the specified frequency.\n\n        Request corresponding oscilloscope RMS voltage reading.\n        '''\n        r = x['resistor index'].values[0]\n        f = x['frequency'].values[0]\n        control_board.set_waveform_frequency(f)\n\n        actuation_index, data = find_good(control_board, actuation_steps, r, 0,\n                                          len(actuation_steps) - 1)\n        board_measured_rms = data.loc[data['divider resistor index'] >= 0,\n                                      'board measured V'].mean()\n        oscope_rms = oscope_reading_func()\n        print 'R=%s, f=%s' % (r, f)\n        return pd.DataFrame([[r, f, actuation_index, board_measured_rms,\n                              oscope_rms]],\n                            columns=['resistor index', 'frequency',\n                                     'actuation index', 'board measured V',\n                                     'oscope measured V'])\n\n    # Return board-measured RMS voltage and oscilloscope-measured RMS voltage\n    # for each frequency/feedback resistor pair.\n    return (conditions.groupby(['resistor index', 'frequency'])\n            .apply(max_actuation_reading).reset_index(drop=True))", "response": "This function calculates the maximum amplified RMS voltage for each resistor in the high - voltage feedback resistor bank and the actuation voltage for each resistor in the feedback resistor bank."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfit model of control board high - voltage feedback resistor and capacitance values based on measured voltage readings.", "response": "def fit_feedback_params(calibration, max_resistor_readings):\n    '''\n    Fit model of control board high-voltage feedback resistor and\n    parasitic capacitance values based on measured voltage readings.\n    '''\n    R1 = 10e6\n\n    # Get transfer function to compute the amplitude of the high-voltage input\n    # to the control board _(i.e., the output of the amplifier)_ based on the\n    # attenuated voltage measured by the analog-to-digital converter on the\n    # control board.\n    #\n    # The signature of the transfer function is:\n    #\n    #     H(V1, R1, C1, R2, C2, f)\n    #\n    # See the `z_transfer_functions` function docstring for definitions of the\n    # parameters based on the control board major version.\n    def fit_resistor_params(x):\n        resistor_index = x['resistor index'].values[0]\n        p0 = [calibration.R_hv[resistor_index],\n              calibration.C_hv[resistor_index]]\n\n        def error(p, df, R1):\n            v1 = compute_from_transfer_function(calibration.hw_version.major,\n                                                'V1',\n                                                V2=df['board measured V'],\n                                                R1=R1, R2=p[0], C2=p[1],\n                                                f=df['frequency'].values)\n            e = df['oscope measured V'] - v1\n            return e\n\n        p1, success = optimize.leastsq(error, p0, args=(x, R1))\n        # take the absolute value of the fitted values, since is possible\n        # for the fit to produce negative resistor and capacitor values\n        p1 = np.abs(p1)\n        return pd.DataFrame([p0 + p1.tolist()],\n                            columns=['original R', 'original C',\n                                     'fitted R', 'fitted C']).T\n\n    results = (max_resistor_readings\n               [max_resistor_readings['resistor index'] >= 0]\n               .groupby(['resistor index']).apply(fit_resistor_params))\n    data = results.unstack()\n    data.columns = data.columns.droplevel()\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_feedback_params(hw_major_version, max_resistor_readings,\n                         feedback_params, axis=None):\n    '''\n    Plot the effective attenuation _(i.e., gain less than 1)_ of the control\n    board measurements of high-voltage AC input according to:\n\n     - AC signal frequency.\n     - feedback resistor used _(varies based on amplitude of AC signal)_.\n\n    Each high-voltage feedback resistor (unintentionally) forms a low-pass\n    filter, resulting in attenuation of the voltage measured on the control\n    board.  The plot generated by this function plots each of the following\n    trends for each feedback resistor:\n\n     - Oscilloscope measurements.\n     - Previous model of attenuation.\n     - Newly fitted model of attenuation, based on oscilloscope readings.\n    '''\n    R1 = 10e6\n\n    # Since the feedback circuit changed in version 2 of the control board, we\n    # use the transfer function that corresponds to the current control board\n    # version that the fitted attenuation model is based on.\n    if axis is None:\n        fig = plt.figure()\n        axis = fig.add_subplot(111)\n\n    markers = MarkerStyle.filled_markers\n\n    def plot_resistor_params(args):\n        resistor_index, x = args\n\n        try:\n            color = axis._get_lines.color_cycle.next()\n        except: # make compatible with matplotlib v1.5\n            color = axis._get_lines.prop_cycler.next()['color']\n\n        F = feedback_params.loc[resistor_index]\n        # Broadcast values in case sympy function simplifies to scalar value.\n        values = np.empty_like(x['frequency'])\n        values[:] = compute_from_transfer_function(hw_major_version, 'V2',\n                                                   V1=1., R1=R1,\n                                                   R2=F['original R'],\n                                                   C2=F['original C'],\n                                                   f=x['frequency'])\n        axis.loglog(x['frequency'], values, color=color, linestyle='--',\n                    label='R$_{%d}$ (previous fit)' % resistor_index)\n\n        values[:] = compute_from_transfer_function(hw_major_version, 'V2',\n                                                   V1=1., R1=R1,\n                                                   R2=F['fitted R'],\n                                                   C2=F['fitted C'],\n                                                   f=x['frequency'])\n        axis.loglog(x['frequency'], values, color=color, linestyle='-',\n                    alpha=0.6, label='R$_{%d}$ (new fit)' % resistor_index)\n        attenuation = x['board measured V'] / x['oscope measured V']\n        axis.plot(x['frequency'], attenuation, color='none',\n                  marker=markers[resistor_index % len(markers)],\n                  label='R$_{%d}$ (scope measured)' % resistor_index,\n                  linestyle='none', markeredgecolor=color, markeredgewidth=2,\n                  markersize=8)\n        return 0\n\n    map(plot_resistor_params, max_resistor_readings.groupby('resistor index'))\n    legend = axis.legend(ncol=3)\n    legend.draw_frame(False)\n    axis.set_xlabel('Frequency (Hz)')\n    axis.set_ylabel(r'$\\frac{V_{BOARD}}'\n                    r'{V_{SCOPE}}$', fontsize=25)", "response": "Plots the feedback parameters for each high - voltage control board."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_control_board_calibration(control_board, fitted_params):\n    '''\n    Update the control board with the specified fitted parameters.\n    '''\n    # Update the control board with the new fitted capacitor and resistor\n    # values for the reference load analog input (channel 0).\n    control_board.a0_series_resistance = fitted_params['fitted R'].values\n    control_board.a0_series_capacitance = fitted_params['fitted C'].values", "response": "Updates the control board with the new fitted parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(self):\n        data = self.dict_class()\n        \n        for path in self.paths:\n            if path in self.paths_loaded: continue\n            \n            try:\n                with open(path, 'r') as file:\n                    path_data = yaml.load(file.read())\n                    data = dict_merge(data, path_data)\n                self.paths_loaded.add(path)\n            except IOError:\n                # TODO: Log this correctly once logging is implemented\n                if not path.endswith('.local.yml'):\n                    print 'CONFIG NOT FOUND: %s' % (path)\n\n        self.data = data", "response": "Load each path in order. Remember paths already loaded and only load new ones."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the settings from a given settings module", "response": "def _initialize(self, settings_module):\n        \"\"\"\n        Initialize the settings from a given settings_module\n        settings_module - path to settings module\n        \"\"\"\n        #Get the global settings values and assign them as self attributes\n        self.settings_list = []\n        for setting in dir(global_settings):\n            #Only get upper case settings\n            if setting == setting.upper():\n                setattr(self, setting, getattr(global_settings, setting))\n                self.settings_list.append(setting)\n\n        #If a settings module was passed in, import it, and grab settings from it\n        #Overwrite global settings with theses\n        if settings_module is not None:\n            self.SETTINGS_MODULE = settings_module\n\n            #Try to import the settings module\n            try:\n                mod = import_module(self.SETTINGS_MODULE)\n            except ImportError:\n                error_message = \"Could not import settings at {0}\".format(self.SETTINGS_MODULE)\n                log.exception(error_message)\n                raise ImportError(error_message)\n\n            #Grab uppercased settings as set them as self attrs\n            for setting in dir(mod):\n                if setting == setting.upper():\n                    if setting == \"INSTALLED_APPS\":\n                        self.INSTALLED_APPS += getattr(mod, setting)\n                    else:\n                        setattr(self, setting, getattr(mod, setting))\n                    self.settings_list.append(setting)\n\n        #If PATH_SETTINGS is in the settings file, extend the system path to include it\n        if hasattr(self, \"PATH_SETTINGS\"):\n            for path in self.PATH_SETTINGS:\n                sys.path.extend(getattr(self,path))\n\n        self.settings_list = list(set(self.settings_list))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setup(self):\n        settings_module  = None\n        #Get the settings module from the environment variables\n        try:\n            settings_module = os.environ[global_settings.MODULE_VARIABLE]\n        except KeyError:\n            error_message = \"Settings not properly configured.  Cannot find the environment variable {0}\".format(global_settings.MODULE_VARIABLE)\n            log.exception(error_message)\n\n        self._initialize(settings_module)\n        self._configure_logging()", "response": "Perform initial setup of the settings class"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures logging from logging config in settings", "response": "def _configure_logging(self):\n        \"\"\"\n        Setting up logging from logging config in settings\n        \"\"\"\n        if not self.LOGGING_CONFIG:\n            #Fallback to default logging in global settings if needed\n            dictConfig(self.DEFAULT_LOGGING)\n        else:\n            dictConfig(self.LOGGING_CONFIG)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that a context is in the stack creates one otherwise.", "response": "def ensure_context(**vars):\n    \"\"\"Ensures that a context is in the stack, creates one otherwise.\n    \"\"\"\n    ctx = _context_stack.top\n    stacked = False\n    if not ctx:\n        ctx = Context()\n        stacked = True\n        _context_stack.push(ctx)\n    ctx.update(vars)\n    try:\n        yield ctx\n    finally:\n        if stacked:\n            _context_stack.pop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a Context instance from the given request object", "response": "def request_context(app, request):\n    \"\"\"Creates a Context instance from the given request object\n    \"\"\"\n    vars = {}\n    if request.view_args is not None:\n        vars.update(request.view_args)\n    vars.update({\n        \"request\": request,\n        \"GET\": AttrDict(request.args.to_dict()),\n        \"POST\" : AttrDict(request.form.to_dict()),\n        \"app\": app,\n        \"config\": app.config,\n        \"session\": session,\n        \"g\": g,\n        \"now\": datetime.datetime.now,\n        \"utcnow\": datetime.datetime.utcnow,\n        \"today\": datetime.date.today})\n    context = Context(vars)\n    context.vars[\"current_context\"] = context\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a copy of this context", "response": "def clone(self, **override_vars):\n        \"\"\"Creates a copy of this context\"\"\"\n        c = Context(self.vars, self.data)\n        c.executed_actions = set(self.executed_actions)\n        c.vars.update(override_vars)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup():\n    # Latex support can be activated using an environment variable, otherwise\n    # the default settings are:\n    # - for windows: off\n    # - else: on\n    use_latex = False\n    if('DD_USE_LATEX' in os.environ):\n        if os.environ['DD_USE_LATEX'] == '1':\n            use_latex = True\n    else:\n        if platform.system() == \"Windows\":\n            use_latex = False\n        else:\n            use_latex = True\n\n    already_loaded = 'matplotlib' in sys.modules\n\n    # just make sure we can access matplotlib as mpl\n    import matplotlib as mpl\n\n    if not already_loaded:\n        mpl.use('Agg')\n\n    import matplotlib.pyplot as plt\n\n    plt.style.use('seaborn')\n\n    # general settings\n    mpl.rcParams['font.size'] = 7.0\n    mpl.rcParams['axes.labelsize'] = 7.0\n    mpl.rcParams['xtick.labelsize'] = 7.0\n    mpl.rcParams['ytick.labelsize'] = 7.0\n    mpl.rcParams[\"lines.linewidth\"] = 1.5\n    mpl.rcParams[\"lines.markeredgewidth\"] = 3.0\n    mpl.rcParams[\"lines.markersize\"] = 3.0\n    # mpl.rcParams['font.sans-serif'] = 'Droid Sans'\n\n    # mpl.rcParams['font.family'] = 'Open Sans'\n    # mpl.rcParams['font.weight'] = 400\n    mpl.rcParams['mathtext.default'] = 'regular'\n\n    # mpl.rcParams['font.family'] = 'Droid Sans'\n\n    if use_latex:\n        mpl.rcParams['text.usetex'] = True\n\n        mpl.rc(\n            'text.latex',\n            preamble=''.join((\n                # r'\\usepackage{droidsans}',\n                # r'\\usepackage[T1]{fontenc} ',\n                r'\\usepackage{sfmath} \\renewcommand{\\rmfamily}{\\sffamily}',\n                r'\\renewcommand\\familydefault{\\sfdefault} ',\n                # r'\\usepackage{mathastext} '\n            ))\n        )\n    else:\n        mpl.rcParams['text.usetex'] = False\n    import mpl_toolkits.axes_grid1 as axes_grid1\n    axes_grid1\n    return plt, mpl", "response": "set the style of the base object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mpl_get_cb_bound_below_plot(ax):\n    position = ax.get_position()\n\n    figW, figH = ax.get_figure().get_size_inches()\n    fig_aspect = figH / figW\n    box_aspect = ax.get_data_ratio()\n    pb = position.frozen()\n    pb1 = pb.shrunk_to_aspect(box_aspect, pb, fig_aspect).bounds\n\n    ax_size = ax.get_position().bounds\n\n    # the colorbar is set to 0.01 width\n    sizes = [ax_size[0], ax_size[1] - 0.14, pb1[2], 0.03]\n\n    return sizes", "response": "Returns the coordinates for a colorbar axes below the axes object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates an XLS with specified content.", "response": "def main():\n    \"\"\"Generate an XLS with specified content.\"\"\"\n    table = \"\"\"<table>\n    <thead>\n    <tr><th>First Name</th><th>Last Name</th></tr>\n    </thead>\n    <tbody>\n    <tr><td>Paul</td><td>McGrath</td></tr>\n    <tr><td>Liam</td><td>Brady</td></tr>\n    <tr><td>John</td><td>Giles</td></tr>\n    </tbody>\n    </table>\"\"\"\n    docraptor = DocRaptor()\n\n    print(\"Create test_basic.xls\")\n    with open(\"test_basic.xls\", \"wb\") as pdf_file:\n        pdf_file.write(\n            docraptor.create(\n                {\"document_content\": table, \"document_type\": \"xls\", \"test\": True}\n            ).content\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestoring the garbage collector state on leaving the with block.", "response": "def restore_gc_state():\n    \"\"\"\n    Restore the garbage collector state on leaving the with block.\n\n    \"\"\"\n    old_isenabled = gc.isenabled()\n    old_flags = gc.get_debug()\n    try:\n        yield\n    finally:\n        gc.set_debug(old_flags)\n        (gc.enable if old_isenabled else gc.disable)()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npreparing development environment. Perform the following steps: - Uninstall ``dmf_control_board_firmware`` if installed as Conda package. - Install build and run-time Conda dependencies. - Link working ``.pioenvs`` directory into Conda ``Library`` directory to make development versions of compiled firmware binaries available to Python API. - Link ``dmf_control_board_firmware`` Python package into site packages directory. See Also -------- :func:`develop_unlink`", "response": "def develop_link(options, info):\n    '''\n    Prepare development environment.\n\n    Perform the following steps:\n\n     - Uninstall ``dmf_control_board_firmware`` if installed as Conda package.\n     - Install build and run-time Conda dependencies.\n     - Link working ``.pioenvs`` directory into Conda ``Library`` directory to\n       make development versions of compiled firmware binaries available to\n       Python API.\n     - Link ``dmf_control_board_firmware`` Python package into site packages\n       directory.\n\n    See Also\n    --------\n    :func:`develop_unlink`\n    '''\n    project_dir = ph.path(__file__).realpath().parent\n\n    # Uninstall ``dmf_control_board_firmware`` if installed as Conda package.\n    info('Check if Conda package is installed...')\n    version_info = ch.conda_version_info('dmf-control-board-firmware')\n    if version_info.get('installed') is not None:\n        info('Uninstall `dmf-control-board-firmware` package...')\n        ch.conda_exec('uninstall', '-y', 'dmf-control-board-firmware',\n                      verbose=True)\n    else:\n        info('`dmf-control-board-firmware` package is not installed.')\n\n    # Install build and run-time Conda dependencies.\n    info('Install build and run-time Conda dependencies...')\n    recipe_dir = project_dir.joinpath('.conda-recipe').realpath()\n    ch.conda_exec('install', '-y', '-n', 'root', 'conda-build', verbose=True)\n    ch.development_setup(recipe_dir, verbose=True)\n\n    # Link working ``.pioenvs`` directory into Conda ``Library`` directory.\n    info('Link working firmware directories into Conda environment.')\n    pio_bin_dir = pioh.conda_bin_path()\n\n    fw_bin_dir = pio_bin_dir.joinpath('dmf-control-board-firmware')\n\n    if not fw_bin_dir.exists():\n        project_dir.joinpath('.pioenvs').junction(fw_bin_dir)\n\n    fw_config_ini = fw_bin_dir.joinpath('platformio.ini')\n    if not fw_config_ini.exists():\n        project_dir.joinpath('platformio.ini').link(fw_config_ini)\n\n    # Link ``dmf_control_board_firmware`` Python package `conda.pth` in site\n    # packages directory.\n    info('Link working Python directory into Conda environment...')\n    ch.conda_exec('develop', project_dir, verbose=True)\n    info(72 * '-' + '\\nFinished')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef develop_unlink(options, info):\n    '''\n    Prepare development environment.\n\n    Perform the following steps:\n\n     - Unlink working ``.pioenvs`` directory into Conda ``Library`` directory.\n     - Unlink ``dmf_control_board_firmware`` Python package from site packages\n       directory.\n\n    See Also\n    --------\n    :func:`develop_link`\n    '''\n    project_dir = ph.path(__file__).realpath().parent\n\n    # Unlink working ``.pioenvs`` directory into Conda ``Library`` directory.\n    info('Unlink working firmware directories from Conda environment.')\n    pio_bin_dir = pioh.conda_bin_path()\n    fw_bin_dir = pio_bin_dir.joinpath('dmf-control-board-firmware')\n\n    if fw_bin_dir.exists():\n        fw_config_ini = fw_bin_dir.joinpath('platformio.ini')\n        if fw_config_ini.exists():\n            fw_config_ini.unlink()\n        fw_bin_dir.unlink()\n\n    # Remove link to ``dmf_control_board_firmware`` Python package in\n    # `conda.pth` in site packages directory.\n    info('Unlink working Python directory from Conda environment...')\n    ch.conda_exec('develop', '-u', project_dir, verbose=True)\n    info(72 * '-' + '\\nFinished')", "response": "Unlink working. pioenvs directory into Conda library directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef response(self, parameters):\n        # get a config object\n        self._set_parameters(parameters)\n        terms = self.m / (1 + (1j * self.w * self.tau) ** self.c)\n        # sum up terms\n        specs = np.sum(terms, axis=1)\n        ccomplex = self.sigmai * (1 - specs)\n\n        response = sip_response.sip_response(self.f, ccomplex=ccomplex)\n\n        return response", "response": "Return the forward response in base dimensions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dre_dtau(self, pars):\n        self._set_parameters(pars)\n        # term 1\n        num1 = self.c * self.w * self.otc1 * np.cos(self.ang)\n        term1 = num1/self.denom\n\n        # term 2\n        num2a = self.otc * np.cos(self.ang)\n        num2b = 1 + num2a\n        denom2 = self.denom ** 2\n        term2 = num2b / denom2\n\n        # term 3\n        term3 = 2 * self.c * self.w * self.otc1 * np.cos(self.ang) + self.otc2\n\n        result = self.sigmai * self.m * (term1 + term2 * term3)\n\n        return result", "response": "r Return the Dtau of the current species."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dre_dc(self, pars):\n        self._set_parameters(pars)\n        # term 1\n        num1a = np.log(self.w * self.tau) * self.otc * np.sin(self.ang)\n        num1b = self.otc * np.cos(self.ang) * np.pi / 2.0\n        term1 = (num1a + num1b) / self.denom\n\n        # term 2\n        num2 = self.otc * np.sin(self.c / np.pi) * 2\n        denom2 = self.denom ** 2\n        term2 = num2 / denom2\n\n        # term 3\n        num3a = 2 * np.log(self.w * self.tau) * self.otc * np.cos(self.ang)\n        num3b = 2 * ((self.w * self.tau) ** 2) * np.pi / 2.0 * np.sin(self.ang)\n        num3c = 2 * np.log(self.w * self.tau) * self.otc2\n        term3 = num3a - num3b + num3c\n\n        result = self.sigmai * self.m * (term1 + term2 * term3)\n\n        return result", "response": "r Return the DRE term for the species."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dim_dc(self, pars):\n        self._set_parameters(pars)\n        # term 1\n        num1a = self.m * np.sin(self.ang) * np.log(self.w * self.tau)\\\n            * self.otc\n        num1b = self.m * self.otc * np.pi / 2 * np.cos(np.pi / 2)\n        term1 = self.sigma0 * (-num1a - num1b) / self.denom\n\n        # term 2\n        num2a = -self.m * self.otc * np.cos(self.ang)\n        num2b = -2 * np.log(self.w * self.tau) * self.otc * np.cos(self.ang)\n        num2c = 2 * self.otc * np.pi / 2 * np.cos(self.ang)\n        num2d = 2 * np.log(self.w * self.tau) * self.otc2\n        numerator = num2a * (num2b + num2c) + num2d\n        term2 = self.sigma0 * numerator / (self.denom ** 2)\n\n        result = term1 + term2\n\n        return result", "response": "r Return the dim of the species."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_view_file_mapping(self, pattern, cls):\n        if isinstance(pattern, str):\n            if not pattern.endswith(\"*\"):\n                _, ext = os.path.splitext(pattern)\n                self.allowed_extensions.add(ext)\n            pattern = re.compile(\"^\" + re.escape(pattern).replace(\"\\\\*\", \".+\") + \"$\", re.I)\n        self.view_class_files_map.append((pattern, cls))", "response": "Adds a mapping between a file and a view class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_file(self, app, pathname, relpath, pypath):\n        try:\n            view_class = self.get_file_view_cls(relpath)\n            return create_view_from_file(pathname, source_template=relpath, view_class=view_class)\n        except DeclarativeViewError:\n            pass", "response": "Loads a file and creates a View from it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the view class associated to a filename", "response": "def get_file_view_cls(self, filename):\n        \"\"\"Returns the view class associated to a filename\n        \"\"\"\n        if filename is None:\n            return self.default_view_class\n        for pattern, cls in self.view_class_files_map:\n            if pattern.match(filename):\n                return cls\n        return self.default_view_class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of immediate children of the given vertex.", "response": "def children(self, vertex):\n        \"\"\"\n        Return the list of immediate children of the given vertex.\n\n        \"\"\"\n        return [self.head(edge) for edge in self.out_edges(vertex)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of immediate parents of this vertex.", "response": "def parents(self, vertex):\n        \"\"\"\n        Return the list of immediate parents of this vertex.\n\n        \"\"\"\n        return [self.tail(edge) for edge in self.in_edges(vertex)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef references(self):\n        return [\n            (tail, head)\n            for tail in self.vertices\n            for head in self.children(tail)\n        ]", "response": "Return the tail and head pairs for each edge in the the\n            graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the subgraph of all nodes reachable from the given start vertex including that vertex.", "response": "def descendants(self, start, generations=None):\n        \"\"\"\n        Return the subgraph of all nodes reachable\n        from the given start vertex, including that vertex.\n\n        If specified, the optional `generations` argument specifies how\n        many generations to limit to.\n\n        \"\"\"\n        visited = self.vertex_set()\n        visited.add(start)\n        to_visit = deque([(start, 0)])\n        while to_visit:\n            vertex, depth = to_visit.popleft()\n            if depth == generations:\n                continue\n            for child in self.children(vertex):\n                if child not in visited:\n                    visited.add(child)\n                    to_visit.append((child, depth+1))\n        return self.full_subgraph(visited)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ancestors(self, start, generations=None):\n        visited = self.vertex_set()\n        visited.add(start)\n        to_visit = deque([(start, 0)])\n        while to_visit:\n            vertex, depth = to_visit.popleft()\n            if depth == generations:\n                continue\n            for parent in self.parents(vertex):\n                if parent not in visited:\n                    visited.add(parent)\n                    to_visit.append((parent, depth+1))\n        return self.full_subgraph(visited)", "response": "Return the subgraph of all nodes from which the given vertex is\n        reachable including that vertex."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _component_graph(self):\n        sccs = []\n        stack = []\n        boundaries = []\n        identified = self.vertex_set()\n        index = self.vertex_dict()\n        to_do = []\n\n        def visit_vertex(v):\n            index[v] = len(stack)\n            stack.append(('VERTEX', v))\n            boundaries.append(index[v])\n            to_do.append((leave_vertex, v))\n            to_do.extend((visit_edge, w) for w in self.children(v))\n\n        def visit_edge(v):\n            if v in identified:\n                stack.append(('EDGE', v))\n            elif v in index:\n                while index[v] < boundaries[-1]:\n                    boundaries.pop()\n            else:\n                to_do.append((visit_vertex, v))\n\n        def leave_vertex(v):\n            if boundaries[-1] == index[v]:\n                root = boundaries.pop()\n                scc = stack[root:]\n                del stack[root:]\n                for item_type, w in scc:\n                    if item_type == 'VERTEX':\n                        identified.add(w)\n                        del index[w]\n                sccs.append(scc)\n                stack.append(('EDGE', v))\n\n        # Visit every vertex of the graph.\n        for v in self.vertices:\n            if v not in identified:\n                to_do.append((visit_vertex, v))\n                while to_do:\n                    operation, v = to_do.pop()\n                    operation(v)\n                stack.pop()\n\n        return sccs", "response": "Compute the graph of strongly connected components."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef source_components(self):\n        raw_sccs = self._component_graph()\n\n        # Construct a dictionary mapping each vertex to the root of its scc.\n        vertex_to_root = self.vertex_dict()\n\n        # And keep track of which SCCs have incoming edges.\n        non_sources = self.vertex_set()\n\n        # Build maps from vertices to roots, and identify the sccs that *are*\n        # reachable from other components.\n        for scc in raw_sccs:\n            root = scc[0][1]\n            for item_type, w in scc:\n                if item_type == 'VERTEX':\n                    vertex_to_root[w] = root\n                elif item_type == 'EDGE':\n                    non_sources.add(vertex_to_root[w])\n\n        sccs = []\n        for raw_scc in raw_sccs:\n            root = raw_scc[0][1]\n            if root not in non_sources:\n                sccs.append([v for vtype, v in raw_scc if vtype == 'VERTEX'])\n\n        return [self.full_subgraph(scc) for scc in sccs]", "response": "Return the strongly connected components reachable from any other component in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of strongly connected components of this graph.", "response": "def strongly_connected_components(self):\n        \"\"\"\n        Return list of strongly connected components of this graph.\n\n        Returns a list of subgraphs.\n\n        Algorithm is based on that described in \"Path-based depth-first search\n        for strong and biconnected components\" by Harold N. Gabow,\n        Inf.Process.Lett. 74 (2000) 107--114.\n\n        \"\"\"\n        raw_sccs = self._component_graph()\n\n        sccs = []\n        for raw_scc in raw_sccs:\n            sccs.append([v for vtype, v in raw_scc if vtype == 'VERTEX'])\n\n        return [self.full_subgraph(scc) for scc in sccs]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the object to the database.", "response": "def save(self, *args, **kwargs):\n        \"\"\"\n        **uid**: :code:`person:{slug}`\n        \"\"\"\n        if not self.full_name:\n            self.full_name = '{0}{1}{2}'.format(\n                self.first_name,\n                '{}'.format(\n                    ' ' + self.middle_name + ' ' if self.middle_name else ' ',\n                ),\n                self.last_name,\n                '{}'.format(' ' + self.suffix if self.suffix else '')\n            )\n\n        self.slug = uuslug(\n            self.full_name,\n            instance=self,\n            max_length=100,\n            separator='-',\n            start_no=2\n        )\n        if not self.uid:\n            self.uid = 'person:{}'.format(self.slug)\n\n        super(Person, self).save(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the MERCHANT_KEY and MERCHANT_HASH of the request.", "response": "def signature(self):\n        \"\"\"\n        Compute the ORDER_HASH of the request.\n\n        The hashable string is composed by getting the values from:\n            MERCHANT\n            ORDER_REF\n            ORDER_DATE\n            ORDER_PNAME[]\n            ORDER_PCODE[]\n            ORDER_PINFO[]\n            ORDER_PRICE[]\n            ORDER_QTY[]\n            ORDER_VAT[]\n            ORDER_SHIPPING\n            PRICES_CURRENCY\n            DISCOUNT\n            DESTINATION_CITY\n            DESTINATION_STATE\n            DESTINATION_COUNTRY\n            PAY_METHOD\n            ORDER_PRICE_TYPE[]\n            SELECTED_INSTALLMENTS_NO\n            TESTORDER\n        in this exact order. Next, we need to concatenate their lenghts with\n        thier values, resulting in a string like:\n\n        8PAYUDEMO9789456123192016-10-05 11:12:279CD Player12MobilePhone6Laptop\n        10PROD_0489110PROD_0740910PROD_0496527Extended Warranty - 5 Years8\n        Dual SIM1117\"Display482.371945.7545230171311220220220103RON2559\n        Bucuresti9Bucuresti2RO8CCVISAMC5GROSS5GROSS5GROSS4TRUE\n\n        Using this string and the MERCHANT_KEY, we compute the HMAC.\n        \"\"\"\n\n        hashable_fields = ['MERCHANT', 'ORDER_REF', 'ORDER_DATE',\n                           'ORDER_SHIPPING', 'PRICES_CURRENCY', 'DISCOUNT',\n                           'DESTINATION_CITY', 'DESTINATION_STATE',\n                           'DESTINATION_COUNTRY', 'PAY_METHOD',\n                           'SELECTED_INSTALLMENTS_NO', 'TESTORDER']\n        result = text_type()\n\n        # We need this hack since payU is not consistent\n        # with the order of fields in hash string\n\n        suffix = text_type()\n        for field in self:\n            if field.name == 'ORDER_HASH':\n                continue\n\n            field_value = field.value()\n\n            if field.name in hashable_fields and field_value:\n                encoded_value = text_type('{length}{value}').format(\n                    length=len(text_type(field_value).encode('utf-8')), value=field_value\n                )\n                if field.name == 'TESTORDER' or \\\n                    field.name == 'SELECTED_INSTALLMENTS_NO':\n                    suffix += encoded_value\n                else:\n                    result += encoded_value\n\n            if field.name == 'ORDER':\n                for detail in PAYU_ORDER_DETAILS:\n                    if any([detail in order and order[detail]\n                            for order in field_value]):\n\n                        for order in field_value:\n                            value = order.get(detail, '')\n\n                            item = text_type('{length}{value}').format(\n                                length=len(text_type(value).encode('utf-8')), value=value\n                            )\n\n                            if detail == 'PRICE_TYPE':\n                                suffix += item\n                            else:\n                                result += item\n\n        result += suffix\n        result = result.encode('utf-8')\n        return hmac.new(PAYU_MERCHANT_KEY, result).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prepare_orders(self, orders):\n\n        for detail in PAYU_ORDER_DETAILS:\n            if not any([detail in order for order in orders]):\n                for order in orders:\n                    order[detail] = PAYU_ORDER_DETAILS_DEFAULTS.get(detail, None)\n\n        return orders", "response": "Prepare the orders for the payment process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the file matching url.", "response": "def staticfiles_url_fetcher(url):\n    \"\"\"\n    Returns the file matching url.\n\n    This method will handle any URL resources that rendering HTML requires\n    (eg: images pointed my ``img`` tags, stylesheets, etc).\n\n    The default behaviour will fetch any http(s) files normally, and will\n    also attempt to resolve staticfiles internally (this should mostly\n    affect development scenarios, but also works if static files are served\n    under a relative url).\n\n    Returns a dictionary with two entries: ``string``, which is the\n    resources data as a string and ``mime_type``, which is the identified\n    mime type for the resource.\n    \"\"\"\n    if url.startswith('/'):\n        base_url = staticfiles_storage.base_url\n        filename = url.replace(base_url, '', 1)\n\n        path = finders.find(filename)\n        if path:\n            # This should match most cases. Manifest static files with relative\n            # URLs will only be picked up in DEBUG mode here.\n            with open(path, 'rb') as f:\n                data = f.read()\n        else:\n            # This should just match things like Manifest static files with\n            # relative URLs. While this code path will expect `collectstatic`\n            # to have run, it should only be reached on if DEBUG = False.\n\n            # XXX: Only Django >= 2.0 supports using this as a context manager:\n            f = staticfiles_storage.open(filename)\n            data = f.read()\n            f.close()\n\n        return {\n            'string': data,\n            'mime_type': mimetypes.guess_type(url)[0],\n        }\n    else:\n        return default_url_fetcher(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_pdf(\n    template,\n    file_,\n    url_fetcher=staticfiles_url_fetcher,\n    context=None,\n):\n    \"\"\"\n    Writes the PDF data into ``file_``. Note that ``file_`` can actually be a\n    Django Response object as well.\n\n    This function may be used as a helper that can be used to save a PDF file\n    to a file (or anything else outside of a request/response cycle), eg::\n\n    :param str html: A rendered HTML.\n    :param file file_: A file like object (or a Response) where to output\n        the rendered PDF.\n    \"\"\"\n    context = context or {}\n\n    html = get_template(template).render(context)\n    HTML(\n        string=html,\n        base_url='not-used://',\n        url_fetcher=url_fetcher,\n    ).write_pdf(\n        target=file_,\n    )", "response": "Renders the PDF file_ into file_."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nencode a buffer of length followed by the bytes of the buffer itself.", "response": "def encode_bytes(src_buf, dst_file):\n    \"\"\"Encode a buffer length followed by the bytes of the buffer\n    itself.\n\n    Parameters\n    ----------\n    src_buf: bytes\n        Source bytes to be encoded.  Function asserts that\n        0 <= len(src_buf) <= 2**16-1.\n    dst_file: file\n        File-like object with write method.\n\n    Returns\n    -------\n    int\n        Number of bytes written to `dst_file`.\n    \"\"\"\n    if not isinstance(src_buf, bytes):\n        raise TypeError('src_buf must by bytes.')\n\n    len_src_buf = len(src_buf)\n    assert 0 <= len_src_buf <= 2**16-1\n    num_written_bytes = len_src_buf + 2\n\n    len_buf = FIELD_U16.pack(len_src_buf)\n    dst_file.write(len_buf)\n    dst_file.write(src_buf)\n\n    return num_written_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a buffer length from a 2 - byte unsigned int then read the subsequent bytes.", "response": "def decode_bytes(f):\n    \"\"\"Decode a buffer length from a 2-byte unsigned int then read the\n    subsequent bytes.\n\n    Parameters\n    ----------\n    f: file\n        File-like object with read method.\n\n    Raises\n    ------\n    UnderflowDecodeError\n        When the end of stream is encountered before the end of the\n        encoded bytes.\n\n    Returns\n    -------\n    int\n        Number of bytes read from `f`.\n    bytes\n        Value bytes decoded from `f`.\n    \"\"\"\n\n    buf = f.read(FIELD_U16.size)\n    if len(buf) < FIELD_U16.size:\n        raise UnderflowDecodeError()\n\n    (num_bytes,) = FIELD_U16.unpack_from(buf)\n    num_bytes_consumed = FIELD_U16.size + num_bytes\n\n    buf = f.read(num_bytes)\n    if len(buf) < num_bytes:\n        raise UnderflowDecodeError()\n\n    return num_bytes_consumed, buf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecodes a utf - 8 encoded string from a file - like object.", "response": "def decode_utf8(f):\n    \"\"\"Decode a utf-8 string encoded as described in MQTT Version\n    3.1.1 section 1.5.3 line 177.  This is a 16-bit unsigned length\n    followed by a utf-8 encoded string.\n\n    Parameters\n    ----------\n    f: file\n        File-like object with read method.\n\n    Raises\n    ------\n    UnderflowDecodeError\n        Raised when a read failed to extract enough bytes from the\n        underlying stream to decode the string.\n    Utf8DecodeError\n        When any code point in the utf-8 string is invalid.\n\n    Returns\n    -------\n    int\n        Number of bytes consumed.\n    str\n        A string utf-8 decoded from ``f``.\n    \"\"\"\n    decode = codecs.getdecoder('utf8')\n\n    buf = f.read(FIELD_U16.size)\n    if len(buf) < FIELD_U16.size:\n        raise UnderflowDecodeError()\n\n    (num_utf8_bytes,) = FIELD_U16.unpack_from(buf)\n    num_bytes_consumed = FIELD_U16.size + num_utf8_bytes\n\n    buf = f.read(num_utf8_bytes)\n    if len(buf) < num_utf8_bytes:\n        raise UnderflowDecodeError()\n\n    try:\n        s, num_chars = decode(buf, 'strict')\n    except UnicodeError as e:\n        raise Utf8DecodeError(e)\n\n    return num_bytes_consumed, s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding integer v to file f.", "response": "def encode_varint(v, f):\n    \"\"\"Encode integer `v` to file `f`.\n\n    Parameters\n    ----------\n    v: int\n        Integer v >= 0.\n    f: file\n        Object containing a write method.\n\n    Returns\n    -------\n    int\n        Number of bytes written.\n    \"\"\"\n    assert v >= 0\n    num_bytes = 0\n\n    while True:\n        b = v % 0x80\n        v = v // 0x80\n\n        if v > 0:\n            b = b | 0x80\n\n        f.write(FIELD_U8.pack(b))\n\n        num_bytes += 1\n        if v == 0:\n            break\n\n    return num_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a varint from a file - like object f.", "response": "def decode_varint(f, max_bytes=4):\n    \"\"\"Decode variable integer using algorithm similar to that described\n    in MQTT Version 3.1.1 line 297.\n\n    Parameters\n    ----------\n    f: file\n        Object with a read method.\n    max_bytes: int or None\n        If a varint cannot be constructed using `max_bytes` or fewer\n        from f then raises a `DecodeError`.  If None then there is no\n        maximum number of bytes.\n\n    Raises\n    -------\n    DecodeError\n        When length is greater than max_bytes.\n    UnderflowDecodeError\n        When file ends before enough bytes can be read to construct the\n        varint.\n\n    Returns\n    -------\n    int\n        Number of bytes consumed.\n    int\n        Value extracted from `f`.\n\n    \"\"\"\n    num_bytes_consumed = 0\n\n    value = 0\n    m = 1\n\n    while True:\n        buf = f.read(1)\n        if len(buf) == 0:\n            raise UnderflowDecodeError()\n\n        (u8,) = FIELD_U8.unpack(buf)\n        value += (u8 & 0x7f) * m\n        m *= 0x80\n        num_bytes_consumed += 1\n\n        if u8 & 0x80 == 0:\n            # No further bytes\n            break\n        elif max_bytes is not None and num_bytes_consumed >= max_bytes:\n            raise DecodeError('Variable integer contained more than maximum bytes ({}).'.format(max_bytes))\n\n    return num_bytes_consumed, value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads as many bytes as required to extract struct then unpack and return a tuple of the values.", "response": "def unpack(self, struct):\n        \"\"\"Read as many bytes as are required to extract struct then\n        unpack and return a tuple of the values.\n\n        Raises\n        ------\n        UnderflowDecodeError\n            Raised when a read failed to extract enough bytes from the\n            underlying stream to extract the bytes.\n\n        Parameters\n        ----------\n        struct: struct.Struct\n\n        Returns\n        -------\n        tuple\n            Tuple of extracted values.\n        \"\"\"\n        v = struct.unpack(self.read(struct.size))\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unpack_utf8(self):\n        num_bytes_consumed, s = decode_utf8(self.__f)\n        self.__num_bytes_consumed += num_bytes_consumed\n        return num_bytes_consumed, s", "response": "Decode a utf - 8 encoded string encoded as described in MQTT Version\n            3. 1. 1 section 1. 5. 3 line 177."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nunpack a utf - 8 encoded string encoded as described in MQTT Version 3. 1. 1 section 1. 5. 3 line 177. This is a 16 - bit unsigned length This is a 16 - bit unsigned length followed by a utf - 8 encoded string.", "response": "def unpack_bytes(self):\n        \"\"\"Unpack a utf-8 string encoded as described in MQTT Version\n        3.1.1 section 1.5.3 line 177.  This is a 16-bit unsigned length\n        followed by a utf-8 encoded string.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed\n        bytes\n            A bytes object extracted from the underlying stream.\n        \"\"\"\n        num_bytes_consumed, b = decode_bytes(self.__f)\n        self.__num_bytes_consumed += num_bytes_consumed\n        return num_bytes_consumed, b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecoding a varint from the current file - like object.", "response": "def unpack_varint(self, max_bytes):\n        \"\"\"Decode variable integer using algorithm similar to that described\n        in MQTT Version 3.1.1 line 297.\n\n        Parameters\n        ----------\n        max_bytes: int or None\n            If a varint cannot be constructed using `max_bytes` or fewer\n            from f then raises a `DecodeError`.  If None then there is no\n            maximum number of bytes.\n\n        Raises\n        -------\n        DecodeError\n            When length is greater than max_bytes.\n        UnderflowDecodeError\n            When file ends before enough bytes can be read to construct the\n            varint.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed.\n        int\n            Value extracted from `f`.\n\n        \"\"\"\n        num_bytes_consumed, value = decode_varint(self.__f, max_bytes)\n        self.__num_bytes_consumed += num_bytes_consumed\n        return num_bytes_consumed, value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, num_bytes):\n        buf = self.__f.read(num_bytes)\n        assert len(buf) <= num_bytes\n        if len(buf) < num_bytes:\n            raise UnderflowDecodeError()\n        self.__num_bytes_consumed += num_bytes\n\n        return buf", "response": "Read num_bytes and return them."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading at most max_bytes from internal buffer.", "response": "def read(self, max_bytes=1):\n        \"\"\"Read at most `max_bytes` from internal buffer.\n\n        Parameters\n        -----------\n        max_bytes: int\n            Maximum number of bytes to read.\n\n        Returns\n        --------\n        bytes\n            Bytes extracted from internal buffer.  Length may be less\n            than ``max_bytes``.  On end-of file returns a bytes object\n            with zero-length.\n        \"\"\"\n\n        if self.limit is None:\n            b = self.__f.read(max_bytes)\n        else:\n            if self.__num_bytes_consumed + max_bytes > self.limit:\n                max_bytes = self.limit - self.__num_bytes_consumed\n            b = self.__f.read(max_bytes)\n        self.__num_bytes_consumed += len(b)\n\n        return b"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, max_bytes=1):\n        if self.__num_bytes_consumed is None:\n            raise ValueError('I/O operation on closed file.')\n\n        if self.__num_bytes_consumed + max_bytes >= len(self.__buf):\n            max_bytes = len(self.__buf) - self.__num_bytes_consumed\n\n        b = self.__buf[self.__num_bytes_consumed:self.__num_bytes_consumed + max_bytes]\n        self.__num_bytes_consumed += max_bytes\n\n        if isinstance(b, bytearray):\n            b = bytes(b)\n\n        assert isinstance(b, bytes)\n        return b", "response": "Reads at most max_bytes from internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timeout(self, value):\n        '''\n        Specifies a timeout on the search query\n        '''\n        if not self.params:\n            self.params = dict(timeout=value)\n            return self\n        self.params['timeout'] = value\n        return self", "response": "Sets the timeout on the search query"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filtered(self, efilter):\n        '''\n        Applies a filter to the search\n        '''\n        if not self.params:\n            self.params={'filter' : efilter}\n            return self\n        if not self.params.has_key('filter'):\n            self.params['filter'] = efilter\n            return self\n        self.params['filter'].update(efilter)\n        return self", "response": "Applies a filter to the search\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the size of the resource.", "response": "def size(self,value):\n        '''\n        The number of hits to return. Defaults to 10\n        '''\n        if not self.params:\n            self.params = dict(size=value)\n            return self\n        self.params['size'] = value\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the from parameter of the current object.", "response": "def from_offset(self, value):\n        '''\n        The starting from index of the hits to return. Defaults to 0.\n        '''\n        if not self.params:\n            self.params = dict({'from':value})\n            return self\n        self.params['from'] = value\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd one or more sort on specific fields.", "response": "def sort(self, *args, **kwargs):\n        '''\n        http://www.elasticsearch.org/guide/reference/api/search/sort.html\n        Allows to add one or more sort on specific fields. Each sort can be reversed as well. The sort is defined on a per field level, with special field name for _score to sort by score.\n\n        standard arguments are ordered ascending, keyword arguments are fields and you specify the order either asc or desc\n        '''\n        if not self.params:\n            self.params = dict()\n        self.params['sort'] = list()\n        for arg in args:\n            self.params['sort'].append(arg)\n        for k,v in kwargs.iteritems():\n            self.params['sort'].append({k : v})\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sorted(self, fsort):\n        '''\n        Allows to add one or more sort on specific fields. Each sort can be reversed as well. The sort is defined on a per field level, with special field name for _score to sort by score.\n        '''\n        if not self.params:\n            self.params = dict()\n        self.params['sort'] = fsort\n\n        return self", "response": "Allows to add one or more sort on specific fields."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for a specific key in an index", "response": "def search_simple(self, index,itype, key, search_term):\n        '''\n        ElasticSearch.search_simple(index,itype,key,search_term)\n        Usage:\n        > es = ElasticSearch()\n        > es.search_simple('twitter','users','name','kim')\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/%s/_search?q=%s:%s' % (self.host,self.port,index,itype,key,search_term)\n        response = request.get(url)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_advanced(self, index, itype, query):\n        '''\n        Advanced search interface using specified query\n        > query = ElasticQuery().term(user='kimchy')\n        > ElasticSearch().search_advanced('twitter','posts',query)\n         ... Search results ...\n\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/%s/_search' % (self.host,self.port,index,itype)\n        if self.params:\n            query_header = dict(query=query, **self.params)\n        else:\n            query_header = dict(query=query)\n        if self.verbose:\n            print query_header\n        response = request.post(url,query_header)\n\n        return response", "response": "Advanced search interface using specified query"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_index_simple(self,index,key,search_term):\n        '''\n        Search the index using a simple key and search_term\n        @param index Name of the index\n        @param key Search Key\n        @param search_term The term to be searched for\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/_search?q=%s:%s' % (self.host,self.port,index,key,search_term)\n        response = request.get(url)\n        return response", "response": "Search the index using a simple key and search_term\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadvances search query against an entire index", "response": "def search_index_advanced(self, index, query):\n        '''\n        Advanced search query against an entire index\n\n        > query = ElasticQuery().query_string(query='imchi')\n        > search = ElasticSearch()\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/_search' % (self.host, self.port, index)\n        if self.params:\n            content = dict(query=query, **self.params)\n        else:\n            content = dict(query=query)\n        if self.verbose:\n            print content\n        response = request.post(url,content)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index_create(self, index, number_of_shards=5,number_of_replicas=1):\n        '''\n        Creates the specified index\n        > search = ElasticSearch()\n        > search.index_create('twitter')\n          {\"ok\":true,\"acknowledged\":true}\n        '''\n        request = self.session\n        content = {'settings' : dict(number_of_shards=number_of_shards, number_of_replicas=number_of_replicas)}\n        if self.verbose:\n            print content\n        url = 'http://%s:%s/%s' % (self.host, self.port, index)\n        response = request.put(url,content)\n        return response", "response": "Creates the specified index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef index_delete(self, index):\n        '''\n        Delets the specified index\n        > search = ElasticSearch()\n        > search.index_delete('twitter')\n          {\"ok\" : True, \"acknowledged\" : True }\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s' % (self.host, self.port, index)\n        response = request.delete(url)\n        return response", "response": "Deletes the specified index"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen the speicified index.", "response": "def index_open(self, index):\n        '''\n        Opens the speicified index.\n        http://www.elasticsearch.org/guide/reference/api/admin-indices-open-close.html\n\n        > ElasticSearch().index_open('my_index')\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/_open' % (self.host, self.port, index)\n        response = request.post(url,None)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a river for a couchdb index.", "response": "def river_couchdb_create(self, index_name,index_type='',couchdb_db='', river_name='',couchdb_host='localhost', couchdb_port='5984',couchdb_user=None, couchdb_password=None, couchdb_filter=None,script=''):\n        '''\n        https://github.com/elasticsearch/elasticsearch-river-couchdb\n\n        Creates a river for the specified couchdb_db.\n\n        > search = ElasticSearch()\n        > search.river_couchdb_create('feeds','feeds','feeds')\n          {u'_id': u'_meta',\n         u'_index': u'_river',\n         u'_type': u'test_db',\n         u'_version': 1,\n         u'ok': True}\n        '''\n        request = self.session\n        if not index_type:\n            index_type = index_name\n        if not couchdb_db:\n            couchdb_db = index_name\n        content = {\n            'type' : 'couchdb',\n            'couchdb' : {\n                'host' : couchdb_host,\n                'port' : couchdb_port,\n                'db' : couchdb_db,\n                'filter' : couchdb_filter\n            },\n            'index' : {\n                'index' : index_name,\n                'type' : index_type\n            }\n        }\n        if couchdb_user and couchdb_password:\n            content['couchdb']['user'] = couchdb_user\n            content['couchdb']['password'] = couchdb_password\n        if script:\n            content['couchdb']['script'] = script\n        if self.verbose:\n            print content\n        url = 'http://%s:%s/_river/%s/_meta' %(self.host, self.port, river_name or index_name)\n        response = request.post(url,content)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a river for the specified index_name", "response": "def river_couchdb_delete(self, index_name):\n        '''\n        https://github.com/elasticsearch/elasticsearch-river-couchdb\n\n        Delete's a river for the specified index\n        WARNING: It DOES NOT delete the index, only the river, so the only effects of this are that the index will no longer poll CouchDB for updates.\n        '''\n        request = self.session\n        url = 'http://%s:%s/_river/%s' % (self.host, self.port, index_name)\n        response = request.delete(url)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index_list(self):\n        '''\n        Lists indices\n        '''\n        request = self.session\n        url = 'http://%s:%s/_cluster/state/' % (self.host, self.port)\n        response = request.get(url)\n        if request.status_code==200:\n            return response.get('metadata',{}).get('indices',{}).keys()\n        else:\n            return response", "response": "Lists indices of all the keys in the cluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables a specific map for an index and type", "response": "def map(self,index_name, index_type, map_value):\n        '''\n        Enable a specific map for an index and type\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/%s/_mapping' % (self.host, self.port, index_name, index_type)\n        content = { index_type : { 'properties' : map_value } }\n        if self.verbose:\n            print content\n        response = request.put(url,content)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists the types available in an index", "response": "def list_types(index_name, host='localhost',port='9200'):\n        '''\n        Lists the context types available in an index\n        '''\n        return ElasticSearch(host=host, port=port).type_list(index_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef type_list(self, index_name):\n        '''\n        List the types available in an index\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s/_mapping' % (self.host, self.port, index_name)\n        response = request.get(url)\n        if request.status_code == 200:\n            return response[index_name].keys()\n        else:\n            return response", "response": "List the types available in an index"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw(self, module, method='GET', data=None):\n        '''\n        Submits or requsts raw input\n        '''\n        request = self.session\n        url = 'http://%s:%s/%s' % (self.host, self.port, module)\n        if self.verbose:\n            print data\n        if method=='GET':\n            response = request.get(url)\n        elif method=='POST':\n            response = request.post(url,data)\n        elif method=='PUT':\n            response = request.put(url,data)\n        elif method=='DELETE':\n            response = request.delete(url)\n        else:\n            return {'error' : 'No such request method %s' % method}\n\n        return response", "response": "Get a specific object from the server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inverse(self, N):\n        if N == 0:\n            return 0\n        lm, hm = 1, 0\n        low, high = N % self.P, self.P\n        while low > 1:\n            r = high//low\n            nm, new = hm - lm * r, high - low * r\n            lm, low, hm, high = nm, new, lm, low\n\n        return lm % self.P", "response": "Returns the modular inverse of an integer with respect to the field\n        characteristic P."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_on_curve(self, point):\n        X, Y = point.X, point.Y\n        return (\n                pow(Y, 2, self.P) - pow(X, 3, self.P) - self.a * X - self.b\n            ) % self.P == 0", "response": "Checks whether a point is on the curve."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_private_key(self):\n        random_string = base64.b64encode(os.urandom(4096)).decode('utf-8')\n        binary_data = bytes(random_string, 'utf-8')\n        hash_object = hashlib.sha256(binary_data)\n        message_digest_bin = hash_object.digest()\n        message_digest_hex = binascii.hexlify(message_digest_bin)\n        return message_digest_hex", "response": "Generates a private key based on the password."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_public_key(self):\n        private_key = int(self.private_key, 16)\n        if private_key >= self.N:\n            raise Exception('Invalid private key.')\n\n        G = JacobianPoint(self.Gx, self.Gy, 1)\n        public_key = G * private_key\n\n        x_hex = '{0:0{1}x}'.format(public_key.X, 64)\n        y_hex = '{0:0{1}x}'.format(public_key.Y, 64)\n        return '04' + x_hex + y_hex", "response": "Generates a public key from the hex - encoded private key using the elliptic curve cryptography."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_address(self):\n        binary_pubkey = binascii.unhexlify(self.public_key)\n        binary_digest_sha256 = hashlib.sha256(binary_pubkey).digest()\n        binary_digest_ripemd160 = hashlib.new('ripemd160', binary_digest_sha256).digest()\n\n        binary_version_byte = bytes([0])\n        binary_with_version_key = binary_version_byte + binary_digest_ripemd160\n\n        checksum_intermed = hashlib.sha256(binary_with_version_key).digest()\n        checksum_intermed = hashlib.sha256(checksum_intermed).digest()\n        checksum = checksum_intermed[:4]\n\n        binary_address = binary_digest_ripemd160 + checksum\n\n        leading_zero_bytes = 0\n        \n        for char in binary_address:\n            if char == 0:\n                leading_zero_bytes += 1\n\n        inp = binary_address + checksum\n        result = 0\n\n        while len(inp) > 0:\n            result *= 256\n            result += inp[0]\n            inp = inp[1:]\n        \n        result_bytes = bytes()\n        while result > 0:\n            curcode = '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'[result % 58]\n            result_bytes = bytes([ord(curcode)]) + result_bytes\n            result //= 58\n\n        pad_size = 0 - len(result_bytes)\n        padding_element = b'1'\n\n        if pad_size > 0:\n            result_bytes = padding_element * pad_size + result_bytes\n        result = ''.join([chr(y) for y in result_bytes])\n        address = '1' * leading_zero_bytes + result\n        return address", "response": "Generates a Bitcoin address from the public key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the double representation of the current point.", "response": "def double(self):\n        \"\"\"\n        Doubles this point.\n\n        Returns:\n            JacobianPoint: The point corresponding to `2 * self`.\n        \"\"\"\n        X1, Y1, Z1 = self.X, self.Y, self.Z\n\n        if Y1 == 0:\n            return POINT_AT_INFINITY\n        S = (4 * X1 * Y1 ** 2) % self.P\n        M = (3 * X1 ** 2 + self.a * Z1 ** 4) % self.P\n        X3 = (M ** 2 - 2 * S) % self.P\n        Y3 = (M * (S - X3) - 8 * Y1 ** 4) % self.P\n        Z3 = (2 * Y1 * Z1) % self.P\n        return JacobianPoint(X3, Y3, Z3)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting this point to an affine representation.", "response": "def to_affine(self):\n        \"\"\"\n        Converts this point to an affine representation.\n\n        Returns:\n            AffinePoint: The affine reprsentation.\n        \"\"\"\n        X, Y, Z = self.x, self.y, self.inverse(self.z)\n        return ((X * Z ** 2) % P, (Y * Z ** 3) % P)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef double(self):\n        X1, Y1, a, P = self.X, self.Y, self.a, self.P\n\n        if self.infinity:\n            return self\n\n        S = ((3 * X1 ** 2 + a) * self.inverse(2 * Y1)) % P\n        X2 = (S ** 2 - (2 * X1)) % P\n        Y2 = (S * (X1 - X2) - Y1) % P\n        return AffinePoint(X2, Y2)", "response": "Returns the point corresponding to 2 * self."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef slope(self, other):\n        X1, Y1, X2, Y2 = self.X, self.Y, other.X, other.Y\n        Y3 = Y1 - Y2\n        X3 = X1 - X2\n        return (Y3 * self.inverse(X3)) % self.P", "response": "Determines the slope between this point and another point."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_jacobian(self):\n        if not self:\n            return JacobianPoint(X=0, Y=0, Z=0)\n        return JacobianPoint(X=self.X, Y=self.Y, Z=1)", "response": "Converts this point to a Jacobian representation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_model(self, name, path=\"floyd.db.models\"):\n    if name in self._model_cache:\n      return self._model_cache[name]\n  \n    try:\n      model = getattr(__import__(path, None, None, [name]), name)\n      self._model_cache[name] = model\n    except ImportError:\n      return False\n    return model", "response": "imports a model of name from path returning from local model\n    cache if it has been already loaded otherwise importing"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a post path and returns a dictionary of variables", "response": "def parse_md(self):\n    \"\"\"Takes a post path and returns a dictionary of variables\"\"\"\n    post_content = _MARKDOWN.convert(self.raw_src)\n\n    if hasattr(_MARKDOWN, 'Meta'):\n      # 'Meta' in _MARKDOWN and _MARKDOWN.Meta:\n      for key in _MARKDOWN.Meta:\n        print \"\\t meta: %s: %s (%s)\" % (key, _MARKDOWN.Meta[key][0], type(_MARKDOWN.Meta[key][0]))\n        if key == 'pubdate':\n          setattr(self, key, datetime.datetime.fromtimestamp(float(_MARKDOWN.Meta[key][0])))\n        else:\n          setattr(self, key, _MARKDOWN.Meta[key][0])\n      \n    self.content = post_content\n    self.stub = self.__key__\n  \n    # set required fields\n    # @TODO required in schema rather than here\n    \n    if not hasattr(self, 'pubdate'):\n      print '\\t Notice: setting default pubdate'\n      setattr(self, 'pubdate', datetime.datetime.now())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter(self, **kwargs):\n    # @TODO refactor with models as dicts\n    \"\"\"filter results of dataset eg.\n    \n    Query('Posts').filter(post_type='post')\n    \"\"\"\n    f_field = kwargs.keys()[0]\n    f_value = kwargs[f_field]\n    _newset = []\n    \n    for m in self._dataset:\n      if hasattr(m, f_field):\n        if getattr(m, f_field) == f_value:\n          _newset.append(m)\n    self._dataset = _newset\n    return self", "response": "filter results of dataset eg.\n    Query('Posts').filter(post_type = post"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts results by pubdate", "response": "def sort_by(self, sb):\n    \"\"\"Sort results\"\"\"\n    \n    self._dataset = self._dataset.sort(key=lambda x: x.pubdate, reverse=True)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_train_task_with_dependencies(self, task_cls, **kwargs):\n        log.info(\"Task {0}\".format(get_task_name(task_cls)))\n        #Instantiate the task\n        task_inst = task_cls()\n        #Grab arguments from the task instance and set them\n        for arg in task_inst.args:\n            if arg not in kwargs:\n                kwargs[arg] = task_inst.args[arg]\n        #Check for dependencies defined by the task\n        if hasattr(task_inst, \"dependencies\"):\n            deps = task_inst.dependencies\n            dep_results = []\n            #Run the dependencies through recursion (in case of dependencies of dependencies, etc)\n            for dep in deps:\n                log.info(\"Dependency {0}\".format(get_task_name(dep)))\n                dep_results.append(self.execute_train_task_with_dependencies(dep.cls, **dep.args))\n            trained_dependencies = []\n            #Add executed dependency to trained_dependencies list on the task\n            for i in xrange(0,len(deps)):\n                dep = deps[i]\n                dep_result = dep_results[i]\n                name = dep.name\n                namespace = dep.namespace\n                category = dep.category\n                trained_dependencies.append(TrainedDependency(category=category, namespace=namespace, name = name, inst = dep))\n            task_inst.trained_dependencies = trained_dependencies\n        #Finally, run the task\n        task_inst.train(**kwargs)\n        return task_inst", "response": "Execute the training task with any dependencies of the training task_cls."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_predict_task(self, task_inst, predict_data, **kwargs):\n        result = task_inst.predict(predict_data, **task_inst.args)\n        return result", "response": "Execute a prediction task"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train(self, **kwargs):\n        log.info(\"Starting to train...\")\n        if not self.setup_run:\n            self.setup()\n        self.trained_tasks = []\n        for task in self.tasks:\n            data = self.reformatted_input[task.data_format]['data']\n            target = self.reformatted_input[task.data_format]['target']\n            if data is None:\n                raise Exception(\"Data cannot be none.  Check the config file to make sure the right input is being read.\")\n            kwargs['data']=data\n            kwargs['target']=target\n            trained_task = self.execute_train_task_with_dependencies(task, **kwargs)\n            self.trained_tasks.append(trained_task)\n            #If the trained task alters the data in any way, pass it down the chain to the next task\n            if hasattr(trained_task, 'data'):\n                self.reformatted_input[task.data_format]['data'] = trained_task.data\n        log.info(\"Finished training.\")", "response": "Train the data in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict(self, **kwargs):\n        reformatted_predict = self.reformat_predict_data()\n        results = {}\n        for task_inst in self.trained_tasks:\n            predict = reformatted_predict[task_inst.data_format]['predict']\n            kwargs['predict']=predict\n            results.update({get_task_name(task_inst) : self.execute_predict_task(task_inst, predict, **kwargs)})\n        return results", "response": "Execute the prediction tasks on all the training tasks."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading in input and do some minimal preformatting", "response": "def read_input(self, input_cls, filename, **kwargs):\n        \"\"\"\n        Read in input and do some minimal preformatting\n        input_cls - the class to use to read the input\n        filename - input filename\n        \"\"\"\n        input_inst = input_cls()\n        input_inst.read_input(filename)\n        return input_inst.get_data()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reformat_file(self, input_file, input_format, output_format):\n        #Return none if input_file or input_format do not exist\n        if input_file is None or input_format is None:\n            return None\n        #Find the needed input class and read the input stream\n        try:\n            input_cls = self.find_input(input_format)\n            input_inst = input_cls()\n        except TypeError:\n            #Return none if input_cls is a Nonetype\n            return None\n        #If the input file cannot be found, return None\n        try:\n            input_inst.read_input(self.absolute_filepath(input_file))\n        except IOError:\n            return None\n\n        formatter = find_needed_formatter(input_format, output_format)\n        if formatter is None:\n            raise Exception(\"Cannot find a formatter that can convert from {0} to {1}\".format(self.input_format, output_format))\n        formatter_inst = formatter()\n        formatter_inst.read_input(input_inst.get_data(), input_format)\n        data = formatter_inst.get_data(output_format)\n        return data", "response": "Reformat input data files to a format the tasks can use"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reformat_input(self, **kwargs):\n        reformatted_input = {}\n        needed_formats = []\n        for task_cls in self.tasks:\n            needed_formats.append(task_cls.data_format)\n        self.needed_formats = list(set(needed_formats))\n\n        for output_format in self.needed_formats:\n            reformatted_input.update(\n                {\n                    output_format :\n                        {\n                        'data' : self.reformat_file(self.input_file, self.input_format, output_format),\n                        'target' : self.reformat_file(self.target_file, self.target_format, output_format)\n                        }\n                }\n            )\n        return reformatted_input", "response": "Reformat input data for all the needed_formats"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_modulename(cdef_sources, source, sys_version):\n    key = '\\x00'.join([sys_version[:3], source, cdef_sources])\n    key = key.encode('utf-8')\n    k1 = hex(binascii.crc32(key[0::2]) & 0xffffffff)\n    k1 = k1.lstrip('0x').rstrip('L')\n    k2 = hex(binascii.crc32(key[1::2]) & 0xffffffff)\n    k2 = k2.lstrip('0').rstrip('L')\n    return '_xprintidle_cffi_{0}{1}'.format(k1, k2)", "response": "Create a modulename for a CFFI module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef server_identity_is_verified(self):\n        # Encrypt a uuid token for the server\n        server_verify_token = self.gpg.encrypt(self._nonce0,\n                                               self.server_fingerprint, always_trust=True)\n        if not server_verify_token.ok:\n            raise GPGAuthStage0Exception(\n                'Encryption of the nonce0 (%s) '\n                'to the server fingerprint (%s) failed.' %\n                (self._nonce0, self.server_fingerprint)\n            )\n\n        server_verify_response = post_server_verify_token(\n            self,\n            keyid=self.user_fingerprint,\n            server_verify_token=str(server_verify_token)\n        )\n\n        if not check_server_verify_response(server_verify_response):\n            raise GPGAuthStage0Exception(\"Verify endpoint wrongly formatted\")\n\n        if server_verify_response.headers.get('X-GPGAuth-Verify-Response') != self._nonce0:\n            raise GPGAuthStage0Exception(\n                'The server decrypted something different than what we sent '\n                '(%s <> %s)' %\n                (server_verify_response.headers.get('X-GPGAuth-Verify-Response'), self._nonce0))\n        logger.info('server_identity_is_verified: OK')\n        return True", "response": "Check if the server identity is verified."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the user authentication token for the current user.", "response": "def user_auth_token(self):\n        \"\"\" GPGAuth Stage1 \"\"\"\n\n        # stage0 is a prequisite\n        if not self.server_identity_is_verified:\n            return False\n\n        server_login_response = post_log_in(\n            self,\n            keyid=self.user_fingerprint\n        )\n\n        if not check_server_login_stage1_response(server_login_response):\n            raise GPGAuthStage1Exception(\"Login endpoint wrongly formatted\")\n\n        # Get the encrypted User Auth Token\n        encrypted_user_auth_token = unquote_plus(\n            server_login_response.headers.get('X-GPGAuth-User-Auth-Token')\n            .replace('\\\\\\\\', '\\\\')\n        ).replace('\\\\ ', ' ')\n\n        logger.debug('User token to decrypt: %s', encrypted_user_auth_token)\n        logger.info('Decrypting the user authentication token; '\n                    'password prompt expected')\n\n        passphrase = None\n        # For the sake of tests, allow one to set the passphrase onto\n        # the object\n        if hasattr(self, '_user_passphrase'):\n            passphrase = self._user_passphrase\n\n        user_auth_token = self.gpg.decrypt(encrypted_user_auth_token, always_trust=True, passphrase=passphrase)\n\n        if not user_auth_token.ok:\n            raise GPGAuthStage1Exception(\"Auth token decryption failed: %s\", user_auth_token.status)\n\n        logger.info('user_auth_token: %s', user_auth_token)\n        return str(user_auth_token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_authenticated_with_token(self):\n        \"\"\" Send back the token to the server to get auth cookie \"\"\"\n\n        server_login_response = post_log_in(\n            self,\n            keyid=self.user_fingerprint,\n            user_token_result=self.user_auth_token\n        )\n\n        if not check_server_login_stage2_response(server_login_response):\n            raise GPGAuthStage2Exception(\"Login endpoint wrongly formatted\")\n        self.cookies.save(ignore_discard=True)\n        logger.info('is_authenticated_with_token: OK')\n        return True", "response": "Check if user is authenticated with a token"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish(self,message,message_type,topic=''):\n        if message_type == MULTIPART:\n            raise Exception(\"Unsupported request type\")\n\n        super(Publisher,self).send(message,message_type,topic)", "response": "Publish a message on the PUB socket with the given topic name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a workflow from the store", "response": "def load(self, cls, run_id):\n        \"\"\"\n        Load a workflow\n        cls - workflow class (to get __name__ from)\n        run_id - id given to the specific run\n        \"\"\"\n        id_code = self.generate_load_identifier(cls, run_id)\n        inst = self.store.load(id_code)\n        return inst"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, obj, run_id):\n        id_code = self.generate_save_identifier(obj, run_id)\n        self.store.save(obj, id_code)", "response": "Save a workflow\n        object to the store"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_tasks(self, tasks):\n        task_classes = []\n        for task in tasks:\n            category, namespace, name = task.split(\".\")\n            try:\n                cls = find_in_registry(category=category, namespace=namespace, name=name)[0]\n            except TypeError:\n                log.error(\"Could not find the task with category.namespace.name {0}\".format(task))\n                raise TypeError\n            task_classes.append(cls)\n        self.tasks = task_classes", "response": "Find task classes from category. namespace. name strings"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a workflow object and initialize the internal state.", "response": "def initialize_workflow(self, workflow):\n        \"\"\"\n        Create a workflow\n        workflow - a workflow class\n        \"\"\"\n        self.workflow = workflow()\n        self.workflow.tasks = self.tasks\n\n        self.workflow.input_file = self.input_file\n        self.workflow.input_format = self.input_format\n        self.workflow.target_file = self.target_file\n        self.workflow.target_format = self.target_format\n        self.workflow.run_id = self.run_id\n\n        self.workflow.setup()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reformat_filepath(self, config_file, filename):\n        if not filename.startswith(\"/\"):\n            filename = self.config_file_format.format(config_file, filename)\n        return filename", "response": "Convert relative paths in config file to absolute paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_manifest(self, asset_xml):\n        # pylint: disable=E1101\n        manifest = '<?xml version=\"1.0\" encoding=\"utf-8\"?>'\n        manifest += '<publisher-upload-manifest publisher-id=\"%s\" ' % \\\n            self.publisher_id\n        manifest += 'preparer=\"%s\" ' % self.preparer\n        if self.report_success:\n            manifest += 'report-success=\"TRUE\">\\n'\n        for notify in self.notifications:\n            manifest += '<notify email=\"%s\"/>' % notify\n        if self.callback:\n            manifest += '<callback entity-url=\"%s\"/>' % self.callback\n        manifest += asset_xml\n        manifest += '</publisher-upload-manifest>'\n        return manifest", "response": "Construct and return the xml manifest to deliver along with video file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _send_file(self, filename):\n        # pylint: disable=E1101\n        ftp = ftplib.FTP(host=self.host)\n        ftp.login(user=self.user, passwd=self.password)\n        ftp.set_pasv(True)\n        ftp.storbinary(\"STOR %s\" % os.path.basename(filename),\n            file(filename, 'rb'))", "response": "Sends a file via FTP."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _post(self, data, file_to_upload=None):\n        # pylint: disable=E1101\n        params = {\"JSONRPC\": simplejson.dumps(data)}\n        req = None\n        if file_to_upload:\n            req = http_core.HttpRequest(self.write_url)\n            req.method = 'POST'\n            req.add_body_part(\"JSONRPC\", simplejson.dumps(data), 'text/plain')\n            upload = file(file_to_upload, \"rb\")\n            req.add_body_part(\"filePath\", upload, 'application/octet-stream')\n            req.end_of_parts()\n            content_type = \"multipart/form-data; boundary=%s\" % \\\n                http_core.MIME_BOUNDARY\n            req.headers['Content-Type'] = content_type\n            req.headers['User-Agent'] = config.USER_AGENT\n\n            req = http_core.ProxiedHttpClient().request(req)\n        else:\n            msg = urllib.urlencode({'json': params['JSONRPC']})\n            req = urllib2.urlopen(self.write_url, msg)\n\n        if req:\n            result = simplejson.loads(req.read())\n            if 'error' in result and result['error']:\n                exceptions.BrightcoveError.raise_exception(\n                    result['error'])\n            return result['result']", "response": "Make the POST request."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_response(self, **kwargs):\n        # pylint: disable=E1101\n        url = self.read_url + \"?output=JSON&token=%s\" % self.read_token\n        for key in kwargs:\n            if key and kwargs[key]:\n                val = kwargs[key]\n                if isinstance(val, (list, tuple)):\n                    val = \",\".join(val)\n                url += \"&%s=%s\" % (key, val)\n        self._api_url = url\n        req = urllib2.urlopen(url)\n        data = simplejson.loads(req.read())\n        self._api_raw_data = data\n        if data and data.get('error', None):\n            exceptions.BrightcoveError.raise_exception(\n                data['error'])\n        if data == None:\n            raise exceptions.NoDataFoundError(\n                \"No data found for %s\" % repr(kwargs))\n        return data", "response": "Make the GET request."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of items from the server.", "response": "def get_list(self, command, item_class, page_size, page_number, sort_by,\n        sort_order, **kwargs):\n        \"\"\"\n        Not intended to be called directly, but rather through an by the\n        ItemResultSet object iterator.\n        \"\"\"\n        # pylint: disable=R0913,W0221\n        data = self._get_response(command=command,\n                                  page_size=page_size,\n                                  page_number=page_number,\n                                  sort_by=sort_by,\n                                  sort_order=sort_order,\n                                  video_fields=None,\n                                  get_item_count=\"true\",\n                                  **kwargs)\n        return ItemCollection(data=data,\n                              item_class=item_class,\n                              _connection=self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the renderer by setting up the extensions.", "response": "def initialize_renderer(extensions=None):\n    \"\"\"\n    Initializes the renderer by setting up the extensions (taking a comma separated\n    string or iterable of extensions). These extensions are added alongside with the\n    configured always-on extensions.\n\n    Returns a markdown renderer instance.\n    \"\"\"\n    if extensions is None:\n        extensions = []\n\n    if isinstance(extensions, str):\n        extensions = [extension.strip() for extension in extensions.split(',')]\n\n    for extension in getattr(settings, 'MARKYMARK_EXTENSIONS', DEFAULT_MARKYMARK_EXTENSIONS):\n        extensions.append(extension)\n\n    return markdown.Markdown(extensions=extensions)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_formats(self):\n        methods = self.get_methods()\n        for m in methods:\n            #Methods named \"from_X\" will be assumed to convert from format X to the common format\n            if m.startswith(\"from_\"):\n                self.input_formats.append(re.sub(\"from_\" , \"\",m))\n            #Methods named \"to_X\" will be assumed to convert from the common format to X\n            elif m.startswith(\"to_\"):\n                self.output_formats.append(re.sub(\"to_\",\"\",m))", "response": "Inspects its methods to see what it can convert from and to the common format X to X"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_input(self, input_data, data_format):\n        if data_format not in self.input_formats:\n            raise Exception(\"Input format {0} not available with this class. Available formats are {1}.\".format(data_format, self.input_formats))\n        data_converter = getattr(self, \"from_\" + data_format)\n        self.data = data_converter(input_data)", "response": "Reads the input data and converts to common format\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the common format and converts to output data", "response": "def get_data(self, data_format):\n        \"\"\"\n        Reads the common format and converts to output data\n        data_format - the format of the output data.  See utils.input.dataformats\n        \"\"\"\n        if data_format not in self.output_formats:\n            raise Exception(\"Output format {0} not available with this class. Available formats are {1}.\".format(data_format, self.output_formats))\n        data_converter = getattr(self, \"to_\" + data_format)\n        return data_converter()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads csv format input data and converts to json.", "response": "def from_csv(self, input_data):\n        \"\"\"\n        Reads csv format input data and converts to json.\n        \"\"\"\n        reformatted_data = []\n        for (i,row) in enumerate(input_data):\n            if i==0:\n                headers = row\n            else:\n                data_row = {}\n                for (j,h) in enumerate(headers):\n                    data_row.update({h : row[j]})\n                reformatted_data.append(data_row)\n        return reformatted_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the common format self. data and writes out to a dataframe.", "response": "def to_dataframe(self):\n        \"\"\"\n        Reads the common format self.data and writes out to a dataframe.\n        \"\"\"\n        keys = self.data[0].keys()\n        column_list =[]\n        for k in keys:\n            key_list = []\n            for i in xrange(0,len(self.data)):\n                key_list.append(self.data[i][k])\n            column_list.append(key_list)\n        df = DataFrame(np.asarray(column_list).transpose(), columns=keys)\n        for i in xrange(0,df.shape[1]):\n            if is_number(df.iloc[:,i]):\n                df.iloc[:,i] = df.iloc[:,i].astype(float)\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_extensions(extensions: Set[str], allow_multifile: bool = False):\n    check_var(extensions, var_types=set, var_name='extensions')\n\n    # -- check them one by one\n    for ext in extensions:\n        check_extension(ext, allow_multifile=allow_multifile)", "response": "Utility method to check that all extensions in the provided set are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_extension(extension: str, allow_multifile: bool = False):\n    check_var(extension, var_types=str, var_name='extension')\n\n    # Extension should either be 'multifile' or start with EXT_SEPARATOR and contain only one EXT_SEPARATOR\n    if (extension.startswith(EXT_SEPARATOR) and extension.count(EXT_SEPARATOR) == 1) \\\n            or (allow_multifile and extension is MULTIFILE_EXT):\n        # ok\n        pass\n    else:\n        raise ValueError('\\'extension\\' should start with \\'' + EXT_SEPARATOR + '\\' and contain not other '\n                         'occurrence of \\'' + EXT_SEPARATOR + '\\'' + (', or be equal to \\'' + MULTIFILE_EXT + '\\' (for '\n                         'multifile object parsing)' if allow_multifile else ''))", "response": "Utility method to check that the provided extension is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_parsing_plan_log_str(obj_on_fs_to_parse, desired_type, log_only_last: bool, parser):\n    loc = obj_on_fs_to_parse.get_pretty_location(blank_parent_part=(log_only_last\n                                                                    and not GLOBAL_CONFIG.full_paths_in_logs),\n                                                 compact_file_ext=True)\n    return '{loc} -> {type} ------- using {parser}'.format(loc=loc, type=get_pretty_type_str(desired_type),\n                                                           parser=str(parser))", "response": "Utility method used by several classes to log a message indicating that a given file object is planned to be parsed by a given parser."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_for_caught_error(parser: _BaseParserDeclarationForRegistries, desired_type: Type[T],\n                                obj: PersistedObject, caught: Exception, options: Dict[str, Dict[str, Any]]):\n        \"\"\"\n        Helper method provided because we actually can't put that in the constructor, it creates a bug in Nose tests\n        https://github.com/nose-devs/nose/issues/725\n\n        :param parser:\n        :param desired_type:\n        :param obj:\n        :param caught:\n        :param options:\n        :return:\n        \"\"\"\n        try:\n            typ = get_pretty_type_str(desired_type)\n        except:\n            typ = str(desired_type)\n\n        e = ParsingException('Error while parsing ' + str(obj) + ' as a ' + typ + ' with parser \\''\n                                + str(parser) + '\\' using options=(' + str(options) + ') : caught \\n  '\n                                + str(caught.__class__.__name__) + ' : ' + str(caught))\\\n            .with_traceback(caught.__traceback__) # 'from e' was hiding the inner traceback. This is much better for debug\n        e.__cause__ = None\n        # e.__cause__ = caught\n        # store the exception still, to be able to handle it later\n        e.caught = caught\n        return e", "response": "Helper method that creates a ParsingException for a given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_for_wrong_result_type(parser: _BaseParserDeclarationForRegistries, desired_type: Type[T],\n                                     obj: PersistedObject, result: T, options: Dict[str, Dict[str, Any]]):\n        \"\"\"\n        Helper method provided because we actually can't put that in the constructor, it creates a bug in Nose tests\n        https://github.com/nose-devs/nose/issues/725\n\n        :param parser:\n        :param desired_type:\n        :param obj:\n        :param result:\n        :param options:\n        :return:\n        \"\"\"\n        msg = \"Error while parsing {obj} as a {typ} with parser {p} using options=({opts}) - parser returned an object \" \\\n              \"of wrong type {tret}: {ret}\".format(obj=obj, typ=get_pretty_type_str(desired_type), p=parser,\n                                                   opts=options, tret=type(result), ret=result)\n        return WrongTypeCreatedError(msg)", "response": "Helper method provided because we can t put that in the constructor it creates a bug in Nose tests\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes the parsing plan.", "response": "def execute(self, logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Called to parse the object as described in this parsing plan, using the provided arguments for the parser.\n        * Exceptions are caught and wrapped into ParsingException\n        * If result does not match expected type, an error is thrown\n\n        :param logger: the logger to use during parsing (optional: None is supported)\n        :param options: a dictionary of option sets. Each option set is identified with an id in the dictionary.\n        :return:\n        \"\"\"\n        try:\n            res = self._execute(logger, options)\n        except Exception as e:\n            raise ParsingException.create_for_caught_error(self.parser, self.obj_type, self.obj_on_fs_to_parse, e,\n                                                           options)\n\n        # Check that the returned parsed object has the correct type\n        if res is not None:\n            if robust_isinstance(res, self.obj_type):\n                return res\n\n        # wrong type : error\n        raise WrongTypeCreatedError.create_for_wrong_result_type(self.parser, self.obj_type, self.obj_on_fs_to_parse,\n                                                                 res, options)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _execute(self, logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        pass", "response": "Implementing classes should perform the parsing here possibly using custom methods of self. parser.\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_applicable_options(self, options: Dict[str, Dict[str, Any]]):\n        return get_options_for_id(options, self.get_id_for_options())", "response": "Returns the options that are applicable to this particular parser from the full map of options."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_parsing_plan(self, desired_type: Type[T], filesystem_object: PersistedObject, logger: Logger,\n                            options: Dict[str, Dict[str, Any]]) -> ParsingPlan[T]:\n        \"\"\"\n        Creates a parsing plan to parse the given filesystem object into the given desired_type.\n        Implementing classes may wish to support additional parameters.\n\n        :param desired_type: the type of object that should be created as the output of parsing plan execution.\n        :param filesystem_object: the persisted object that should be parsed\n        :param logger: an optional logger to log all parsing plan creation and execution information\n        :param options: a dictionary additional implementation-specific parameters (one dict per parser id).\n        Implementing classes may use 'self._get_applicable_options()' to get the options that are of interest for this\n        parser.\n        :return:\n        \"\"\"\n        pass", "response": "Creates a parsing plan to parse the given filesystem object into the given desired_type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a host record", "response": "def add(self, f_ipaddr, f_macaddr, f_hostname, f_netbios_name, f_engineer, f_asset_group, f_confirmed):\n        \"\"\"\n        Add a t_hosts record\n\n        :param f_ipaddr: IP address\n        :param f_macaddr: MAC Address\n        :param f_hostname: Hostname\n        :param f_netbios_name: NetBIOS Name\n        :param f_engineer: Engineer username\n        :param f_asset_group: Asset group\n        :param f_confirmed: Confirmed boolean\n        :return: (True/False, t_hosts.id or response message)\n        \"\"\"\n        return self.send.host_add(f_ipaddr, f_macaddr, f_hostname, f_netbios_name, f_engineer,\n                                  f_asset_group, f_confirmed)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a datetime instance from a string generated by now_field.", "response": "def parse_now_field(s):\n    \"\"\"Return a datetime instance from a string generated by now_field.\n    IMPORTANT: the datetime will be in UTC\"\"\"\n    if not s.startswith('UTC:'):\n        return None  # Invalid string\n    s = s[4:]\n\n    # isoformat can return strings both with and without microseconds - we\n    # account for both\n    try:\n        dt = datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%S.%f')\n    except ValueError:\n        dt = datetime.datetime.strptime(s, '%Y-%m-%dT%H:%M:%S')\n\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ftp(ftp_conf, debug=0):\n    server = ftp_conf.get('server')\n    user = ftp_conf.get('user')\n    password = ftp_conf.get('password')\n    start_path = ftp_conf.get('start_path')\n    slog.info(\"Connecting FTP server %s ......\", server)\n    ftpStr = 'ftp://%s/'%server\n    if start_path:\n        ftpStr = ftpStr+start_path\n    ftp = ftplib.FTP(server, user, password)\n    ftp.set_debuglevel(debug)\n    if start_path:\n        ftp.cwd(start_path)\n    serverFiles = ftp.nlst()\n    slog.info('There are some files in %s:\\n[%s]'%(ftpStr, ', '.join(serverFiles)))\n    return ftp, ftpStr", "response": "Returns a ftp server and a string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuploads a file to a remote path.", "response": "def upload_file(file_path, remote_path, ftp_conf, remove_file=False):\n    \"\"\"\u4e0a\u4f20\u7b2c\u4e00\u4e2a\u6307\u5b9a\u7684\u6587\u4ef6\u5230 FTP \u670d\u52a1\u5668\u3002\n\n    :param str file_path: \u5f85\u4e0a\u4f20\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\u3002\n    :param str remote_path: \u6587\u4ef6\u5728 FTP \u670d\u52a1\u5668\u4e0a\u7684\u76f8\u5bf9\u8def\u5f84\uff08\u76f8\u5bf9\u4e8e FTP \u670d\u52a1\u5668\u7684\u521d\u59cb\u8def\u5f84\uff09\u3002\n    :param dict ftp_conf: ftp\u914d\u7f6e\u6587\u4ef6\uff0c\u8be6\u89c1 :func:`get_ftp` \u3002\n    :param bool remove_file: \u4e0a\u4f20\u6210\u529f\u540e\u662f\u5426\u5220\u9664\u672c\u5730\u6587\u4ef6\u3002\n    :returns: FTP \u670d\u52a1\u5668\u4e0a\u7684\u6587\u4ef6\u5217\u8868\n    :rtype: list\n\n    \"\"\"\n    check_ftp_conf(ftp_conf)\n\n    ftp, ftpStr = get_ftp(ftp_conf)\n    lf = open(file_path, 'rb')\n    slog.info('Uploading \"%s\" to \"%s/%s\" ......'%(file_path, ftpStr, remote_path))\n    ftp.storbinary(\"STOR %s\"%remote_path, lf)\n    filelist = ftp.nlst()\n    ftp.quit()\n    lf.close()\n    if remove_file:\n        os.remove(file_path)\n    slog.info('Upload done.')\n    return filelist"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retrieve_data(self):\n\n        #==== Retrieve data ====#\n\n        df = self.manager.get_historic_data(self.start.date(), self.end.date())\n        df.replace(0, np.nan, inplace=True)\n        return df", "response": "Retrieve data from the manager."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_min_risk(self, weights, cov_matrix):\n\n        def func(weights):\n            \"\"\"The objective function that minimizes variance.\"\"\"\n            return np.matmul(np.matmul(weights.transpose(), cov_matrix), weights)\n\n        def func_deriv(weights):\n            \"\"\"The derivative of the objective function.\"\"\"\n            return (\n                np.matmul(weights.transpose(), cov_matrix.transpose()) +\n                np.matmul(weights.transpose(), cov_matrix)\n            )\n\n        constraints = ({'type': 'eq', 'fun': lambda weights: (weights.sum() - 1)})\n        solution = self.solve_minimize(func, weights, constraints, func_deriv=func_deriv)\n        # NOTE: `min_risk` is unused, but may be helpful later.\n        # min_risk = solution.fun\n        allocation = solution.x\n\n        return allocation", "response": "Minimizes the variance of a portfolio."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmaximize the returns of a portfolio.", "response": "def get_max_return(self, weights, returns):\n        \"\"\"\n        Maximizes the returns of a portfolio.\n        \"\"\"\n\n        def func(weights):\n            \"\"\"The objective function that maximizes returns.\"\"\"\n            return np.dot(weights, returns.values) * -1\n\n        constraints = ({'type': 'eq', 'fun': lambda weights: (weights.sum() - 1)})\n        solution = self.solve_minimize(func, weights, constraints)\n        max_return = solution.fun * -1\n\n        # NOTE: `max_risk` is not used anywhere, but may be helpful in the future.\n        # allocation = solution.x\n        # max_risk = np.matmul(\n        #     np.matmul(allocation.transpose(), cov_matrix), allocation\n        # )\n\n        return max_return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef efficient_frontier(\n        self,\n        returns,\n        cov_matrix,\n        min_return,\n        max_return,\n        count\n    ):\n        \"\"\"\n        Returns a DataFrame of efficient portfolio allocations for `count` risk\n        indices.\n        \"\"\"\n\n        columns = [coin for coin in self.SUPPORTED_COINS]\n        # columns.append('Return')\n        # columns.append('Risk')\n        values = pd.DataFrame(columns=columns)\n        weights = [1/len(self.SUPPORTED_COINS)] * len(self.SUPPORTED_COINS)\n\n        def func(weights):\n            \"\"\"The objective function that minimizes variance.\"\"\"\n            return np.matmul(np.matmul(weights.transpose(), cov_matrix), weights)\n\n        def func_deriv(weights):\n            \"\"\"The derivative of the objective function.\"\"\"\n            return (\n                np.matmul(weights.transpose(), cov_matrix.transpose()) +\n                np.matmul(weights.transpose(), cov_matrix)\n            )\n\n        for point in np.linspace(min_return, max_return, count):\n            constraints = (\n                {'type': 'eq', 'fun': lambda weights: (weights.sum() - 1)},\n                {'type': 'ineq', 'fun': lambda weights, i=point: (\n                    np.dot(weights, returns.values) - i\n                )}\n            )\n\n            solution = self.solve_minimize(func, weights, constraints, func_deriv=func_deriv)\n\n            columns = {}\n            for index, coin in enumerate(self.SUPPORTED_COINS):\n                columns[coin] = math.floor(solution.x[index] * 100 * 100) / 100\n\n            # NOTE: These lines could be helpful, but are commented out right now.\n            # columns['Return'] = round(np.dot(solution.x, returns), 6)\n            # columns['Risk'] = round(solution.fun, 6)\n\n            values = values.append(columns, ignore_index=True)\n\n        return values", "response": "Returns a DataFrame of efficient portfolio allocations for count risk objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsolves a minimization problem for a set of COINS.", "response": "def solve_minimize(\n        self,\n        func,\n        weights,\n        constraints,\n        lower_bound=0.0,\n        upper_bound=1.0,\n        func_deriv=False\n    ):\n        \"\"\"\n        Returns the solution to a minimization problem.\n        \"\"\"\n\n        bounds = ((lower_bound, upper_bound), ) * len(self.SUPPORTED_COINS)\n        return minimize(\n            fun=func, x0=weights, jac=func_deriv, bounds=bounds,\n            constraints=constraints, method='SLSQP', options={'disp': False}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef allocate(self):\n        df = self.manager.get_historic_data()[self.SUPPORTED_COINS]\n\n        #==== Calculate the daily changes ====#\n        change_columns = []\n        for column in df:\n            if column in self.SUPPORTED_COINS:\n                change_column = '{}_change'.format(column)\n                values = pd.Series(\n                    (df[column].shift(-1) - df[column]) /\n                    -df[column].shift(-1)\n                ).values\n                df[change_column] = values\n                change_columns.append(change_column)\n\n        # print(df.head())\n        # print(df.tail())\n\n        #==== Variances and returns ====#\n        columns = change_columns\n        # NOTE: `risks` is not used, but may be used in the future\n        risks = df[columns].apply(np.nanvar, axis=0)\n        # print('\\nVariance:\\n{}\\n'.format(risks))\n        returns = df[columns].apply(np.nanmean, axis=0)\n        # print('\\nExpected returns:\\n{}\\n'.format(returns))\n\n        #==== Calculate risk and expected return ====#\n        cov_matrix = df[columns].cov()\n        # NOTE: The diagonal variances weren't calculated correctly, so here is a fix.\n        cov_matrix.values[[np.arange(len(self.SUPPORTED_COINS))] * 2] = df[columns].apply(np.nanvar, axis=0)\n        weights = np.array([1/len(self.SUPPORTED_COINS)] * len(self.SUPPORTED_COINS)).reshape(len(self.SUPPORTED_COINS), 1)\n\n        #==== Calculate portfolio with the minimum risk ====#\n        min_risk = self.get_min_risk(weights, cov_matrix)\n        min_return = np.dot(min_risk, returns.values)\n\n        #==== Calculate portfolio with the maximum return ====#\n        max_return = self.get_max_return(weights, returns)\n\n        #==== Calculate efficient frontier ====#\n        frontier = self.efficient_frontier(\n            returns, cov_matrix, min_return, max_return, 6\n        )\n        return frontier", "response": "Returns an efficient portfolio allocation for the given risk index."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_default_options(options):\n    if options.settings:\n        #Set the percept_settings_module (picked up by settings in conf.base)\n        os.environ['PERCEPT_SETTINGS_MODULE'] = options.settings\n    if options.pythonpath:\n        #Append the pythonpath and the directory one up from the pythonpath to sys.path for importing\n\n        options.pythonpath = os.path.abspath(os.path.expanduser(options.pythonpath))\n        up_one_path = os.path.abspath(os.path.join(options.pythonpath, \"..\"))\n        sys.path.append(options.pythonpath)\n        sys.path.append(up_one_path)\n    return options", "response": "Pass in a Values instance from OptionParser.  Handle settings and pythonpath\n    options - Values from OptionParser"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating an OptionParser object for the command and subcommand.", "response": "def create_parser(self, prog_name, subcommand):\n        \"\"\"\n        Create an OptionParser\n        prog_name - Name of a command\n        subcommand - Name of a subcommand\n        \"\"\"\n        parser = OptionParser(prog=prog_name,\n                              usage=self.usage(subcommand),\n                              option_list=self.option_list)\n        return parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hook(name=None, *args, **kwargs):\n    def decorator(f):\n        if not hasattr(f, \"hooks\"):\n            f.hooks = []\n        f.hooks.append((name or f.__name__, args, kwargs))\n        return f\n    return decorator", "response": "Decorator to register the function as a hook"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister func via hooks.", "response": "def register_hooks(func, hooks, obj):\n    \"\"\"Register func on obj via hooks.\n    Hooks should be a tuple of (name, args, kwargs) where\n    name is a method name of obj. If args or kwargs are not empty,\n    the method will be called first and expect a new function as return.\n    \"\"\"\n    for name, args, kwargs in hooks:\n        hook = getattr(obj, name)\n        force_call = kwargs.pop(\"_force_call\", False)\n        if force_call or len(args) > 0 or len(kwargs) > 0:\n            hook = hook(*args, **kwargs)\n        hook(func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef action(*args, **kwargs):\n    def decorator(f):\n        return ActionFunction(f, *args, **kwargs)\n    return decorator", "response": "Transforms functions or class methods into actions.\n    Optionnaly, you can define a function to be used as the view initializer:\n\n        @action()\n        def my_action():\n            pass\n\n        @my_action.init_view\n        def my_action_init_view(view, options):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef with_actions(actions_or_group_name, actions=None):\n    group = None\n    if isinstance(actions_or_group_name, str):\n        group = actions_or_group_name\n    else:\n        actions = actions_or_group_name\n    def decorator(f):\n        if isinstance(f, WithActionsDecorator):\n            dec = f\n        else:\n            dec = WithActionsDecorator(f)\n        dec.actions.extend(load_actions(actions, group=group))\n        return dec\n    return decorator", "response": "Decorator that creates a new function that runs the list of actions before and after the function\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_channel_sweep(proxy, start_channel):\n    '''\n    Parameters\n    ----------\n    proxy : DMFControlBoard\n    start_channel : int\n        Channel number from which to start a channel sweep (should be a\n        multiple of 40, e.g., 0, 40, 80).\n\n    Returns\n    -------\n    pandas.DataFrame\n        See description of return of :func:`sweep_channels`.\n    '''\n    test_loads = TEST_LOADS.copy()\n    test_loads.index += start_channel\n\n    results = sweep_channels(proxy, test_loads)\n    normalized_measurements = (results['measured capacitance']\n                               / results['expected capacitance'])\n    fig, axis = plt.subplots(figsize=(10, 8))\n    axis.bar(normalized_measurements.index - 0.3, normalized_measurements,\n             width=0.6, edgecolor='none', facecolor='limegreen')\n    axis.set_xlim(left=test_loads.index.min() - 0.5,\n                  right=test_loads.index.max() + 0.5)\n    axis.set_xlabel('channel')\n    axis.set_ylabel(r'$\\frac{C_{\\tt{measured}}}{C_{\\tt{expected}}}$',\n                    fontsize=28)\n    return results", "response": "Plots the channel sweep for a single channel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_unicode_map():\n    unicode_map = {}\n\n    for beta, uni in _map.BETACODE_MAP.items():\n        # Include decomposed equivalent where necessary.\n        norm = unicodedata.normalize('NFC', uni)\n        unicode_map[norm] = beta\n        unicode_map[uni] = beta\n\n    # Add the final sigmas.\n    final_sigma_norm = unicodedata.normalize('NFC', _FINAL_LC_SIGMA)\n    unicode_map[final_sigma_norm] = 's'\n    unicode_map[_FINAL_LC_SIGMA] = 's'\n\n    return unicode_map", "response": "Create the inverse map from unicode to betacode representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating the trie for betacode conversion.", "response": "def _create_conversion_trie(strict):\n    \"\"\"\n    Create the trie for betacode conversion.\n\n    Args:\n    text: The beta code text to convert. All of this text must be betacode.\n    strict: Flag to allow for flexible diacritic order on input.\n\n    Returns:\n    The trie for conversion.\n    \"\"\"\n    t = pygtrie.CharTrie()\n\n    for beta, uni in _map.BETACODE_MAP.items():\n        if strict:\n            t[beta] = uni\n        else:\n            # The order of accents is very strict and weak. Allow for many orders of\n            # accents between asterisk and letter or after letter. This does not\n            # introduce ambiguity since each betacode token only has one letter and\n            # either starts with a asterisk or a letter.\n            diacritics = beta[1:]\n\n            perms = itertools.permutations(diacritics)\n            for perm in perms:\n                perm_str = beta[0] + ''.join(perm)\n                t[perm_str.lower()] = uni\n                t[perm_str.upper()] = uni\n\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the maximum length of a single betacode token.", "response": "def _find_max_beta_token_len():\n    \"\"\"\n    Finds the maximum length of a single betacode token.\n\n    Returns:\n    The length of the longest key in the betacode map, which corresponds to the\n    longest single betacode token.\n    \"\"\"\n    max_beta_len = -1\n    for beta, uni in _map.BETACODE_MAP.items():\n        if len(beta) > max_beta_len:\n            max_beta_len = len(beta)\n\n    return max_beta_len"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef beta_to_uni(text, strict=False):\n    # Check if the requested configuration for conversion already has a trie\n    # stored otherwise convert it.\n    param_key = (strict,)\n    try:\n       t = _BETA_CONVERSION_TRIES[param_key]\n    except KeyError:\n        t = _create_conversion_trie(*param_key)\n        _BETA_CONVERSION_TRIES[param_key] = t\n\n    transform = []\n    idx = 0\n    possible_word_boundary = False\n\n    while idx < len(text):\n        if possible_word_boundary and _penultimate_sigma_word_final(transform):\n            transform[-2] = _FINAL_LC_SIGMA\n\n        step = t.longest_prefix(text[idx:idx + _MAX_BETA_TOKEN_LEN])\n\n        if step:\n            possible_word_boundary = text[idx] in _BETA_PUNCTUATION\n\n            key, value = step\n            transform.append(value)\n            idx += len(key)\n        else:\n            possible_word_boundary = True\n\n            transform.append(text[idx])\n            idx += 1\n\n    # Check one last time in case there is some whitespace or punctuation at the\n    # end and check if the last character is a sigma.\n    if possible_word_boundary and _penultimate_sigma_word_final(transform):\n        transform[-2] = _FINAL_LC_SIGMA\n    elif len(transform) > 0 and transform[-1] == _MEDIAL_LC_SIGMA:\n        transform[-1] = _FINAL_LC_SIGMA\n\n    converted = ''.join(transform)\n    return converted", "response": "Converts the given text from betacode to unicode."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uni_to_beta(text):\n    u = _UNICODE_MAP\n\n    transform = []\n\n    for ch in text:\n        try:\n            conv = u[ch]\n        except KeyError:\n            conv = ch\n\n        transform.append(conv)\n\n    converted = ''.join(transform)\n    return converted", "response": "Convert unicode text to betacode equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines a valid ordering of the nodes in which a node is not called before all of its dependencies.", "response": "def __calculate_order(self, node_dict):\n        \"\"\"\n        Determine a valid ordering of the nodes in which a node is not called before all of it's dependencies.\n\n        Raise an error if there is a cycle, or nodes are missing.\n        \"\"\"\n        if len(node_dict.keys()) != len(set(node_dict.keys())):\n            raise DependencyTreeException(\"Duplicate Keys Exist in node dictionary!\")\n        valid_order = [node for node, dependencies in node_dict.items() if len(dependencies) == 0]\n        remaining_nodes = [node for node in node_dict.keys() if node not in valid_order]\n        while len(remaining_nodes) > 0:\n            node_added = False\n            for node in remaining_nodes:\n                dependencies = [d for d in node_dict[node] if d not in valid_order]\n                if len(dependencies) == 0:\n                    valid_order.append(node)\n                    remaining_nodes.remove(node)\n                    node_added = True\n            if not node_added:\n                # the tree must be invalid, as it was not possible to remove a node.\n                # it's hard to find all the errors, so just spit out the first one you can find.\n                invalid_node = remaining_nodes[0]\n                invalid_dependency = ', '.join(node_dict[invalid_node])\n                if invalid_dependency not in remaining_nodes:\n                    raise DependencyTreeException(\n                        \"Missing dependency! One or more of ({dependency}) are missing for {dependant}.\".format(\n                            dependant=invalid_node, dependency=invalid_dependency))\n                else:\n                    raise DependencyTreeException(\"The dependency %s is cyclic or dependent on a cyclic dependency\" % invalid_dependency)\n        return valid_order"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_input(self, filename, has_header=True):\n        stream = open(filename)\n        reader = csv.reader(stream)\n        csv_data = []\n        for (i, row) in enumerate(reader):\n            if i==0:\n                if not has_header:\n                    csv_data.append([str(i) for i in xrange(0,len(row))])\n            csv_data.append(row)\n        self.data = csv_data", "response": "Reads the CSV file and sets the data attribute of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_default_plugins(root_parser: ParserRegistryWithConverters):\n    # -------------------- CORE ---------------------------\n    try:\n        # -- primitive types\n        from parsyfiles.plugins_base.support_for_primitive_types import get_default_primitive_parsers, \\\n            get_default_primitive_converters\n        root_parser.register_parsers(get_default_primitive_parsers())\n        root_parser.register_converters(get_default_primitive_converters())\n    except ImportError as e:\n        warn_import_error('primitive types', e)\n    try:\n        # -- collections\n        from parsyfiles.plugins_base.support_for_collections import get_default_collection_parsers, \\\n            get_default_collection_converters\n        root_parser.register_parsers(get_default_collection_parsers(root_parser, root_parser))\n        root_parser.register_converters(get_default_collection_converters(root_parser))\n    except ImportError as e:\n        warn_import_error('dict', e)\n    try:\n        # -- objects\n        from parsyfiles.plugins_base.support_for_objects import get_default_object_parsers, \\\n            get_default_object_converters\n        root_parser.register_parsers(get_default_object_parsers(root_parser, root_parser))\n        root_parser.register_converters(get_default_object_converters(root_parser))\n    except ImportError as e:\n        warn_import_error('objects', e)\n    try:\n        # -- config\n        from parsyfiles.plugins_base.support_for_configparser import get_default_config_parsers, \\\n            get_default_config_converters\n        root_parser.register_parsers(get_default_config_parsers())\n        root_parser.register_converters(get_default_config_converters(root_parser))\n    except ImportError as e:\n        warn_import_error('config', e)\n\n    # ------------------------- OPTIONAL -----------------\n    try:\n        # -- jprops\n        from parsyfiles.plugins_optional.support_for_jprops import get_default_jprops_parsers\n        root_parser.register_parsers(get_default_jprops_parsers(root_parser, root_parser))\n        # root_parser.register_converters()\n    except ImportError as e:\n        warn_import_error('jprops', e)\n    try:\n        # -- yaml\n        from parsyfiles.plugins_optional.support_for_yaml import get_default_yaml_parsers\n        root_parser.register_parsers(get_default_yaml_parsers(root_parser, root_parser))\n        # root_parser.register_converters()\n    except ImportError as e:\n        warn_import_error('yaml', e)\n    try:\n        # -- numpy\n        from parsyfiles.plugins_optional.support_for_numpy import get_default_np_parsers, get_default_np_converters\n        root_parser.register_parsers(get_default_np_parsers())\n        root_parser.register_converters(get_default_np_converters())\n    except ImportError as e:\n        warn_import_error('numpy', e)\n    try:\n        # -- pandas\n        from parsyfiles.plugins_optional.support_for_pandas import get_default_pandas_parsers, \\\n            get_default_pandas_converters\n        root_parser.register_parsers(get_default_pandas_parsers())\n        root_parser.register_converters(get_default_pandas_converters())\n    except ImportError as e:\n        warn_import_error('pandas', e)", "response": "Utility method to register all default plugins on the given parser + converter registry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_item(location: str, item_type: Type[T], item_name_for_log: str = None,\n               file_mapping_conf: FileMappingConfiguration = None,\n               logger: Logger = default_logger, lazy_mfcollection_parsing: bool = False) -> T:\n    \"\"\"\n    Creates a RootParser() and calls its parse_item() method\n\n    :param location:\n    :param item_type:\n    :param item_name_for_log:\n    :param file_mapping_conf:\n    :param logger:\n    :param lazy_mfcollection_parsing:\n    :return:\n    \"\"\"\n    rp = _create_parser_from_default(logger)\n    opts = create_parser_options(lazy_mfcollection_parsing=lazy_mfcollection_parsing)\n    return rp.parse_item(location, item_type, item_name_for_log=item_name_for_log, file_mapping_conf=file_mapping_conf,\n                         options=opts)", "response": "Parses a single item from a location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_collection(location: str, base_item_type: Type[T], item_name_for_log: str = None,\n                     file_mapping_conf: FileMappingConfiguration = None, logger: Logger = default_logger,\n                     lazy_mfcollection_parsing: bool = False)\\\n        -> Dict[str, T]:\n    \"\"\"\n    Utility method to create a RootParser() with default configuration and call its parse_collection() method\n\n    :param location:\n    :param base_item_type:\n    :param item_name_for_log:\n    :param file_mapping_conf:\n    :param logger:\n    :param lazy_mfcollection_parsing:\n    :return:\n    \"\"\"\n    rp = _create_parser_from_default(logger)\n    opts = create_parser_options(lazy_mfcollection_parsing=lazy_mfcollection_parsing)\n    return rp.parse_collection(location, base_item_type, item_name_for_log=item_name_for_log,\n                               file_mapping_conf=file_mapping_conf, options=opts)", "response": "Utility method to create a RootParser with default configuration and call its parse_collection method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_collection(self, item_file_prefix: str, base_item_type: Type[T], item_name_for_log: str = None,\n                         file_mapping_conf: FileMappingConfiguration = None,\n                         options: Dict[str, Dict[str, Any]] = None) -> Dict[str, T]:\n        \"\"\"\n        Main method to parse a collection of items of type 'base_item_type'.\n\n        :param item_file_prefix:\n        :param base_item_type:\n        :param item_name_for_log:\n        :param file_mapping_conf:\n        :param options:\n        :return:\n        \"\"\"\n        # -- item_name_for_log\n        item_name_for_log = item_name_for_log or ''\n        check_var(item_name_for_log, var_types=str, var_name='item_name_for_log')\n\n        # creating the wrapping dictionary type\n        collection_type = Dict[str, base_item_type]\n        if len(item_name_for_log) > 0:\n            item_name_for_log = item_name_for_log + ' '\n        self.logger.debug('**** Starting to parse ' + item_name_for_log + 'collection of <'\n                          + get_pretty_type_str(base_item_type) + '> at location ' + item_file_prefix +' ****')\n\n        # common steps\n        return self._parse__item(collection_type, item_file_prefix, file_mapping_conf, options=options)", "response": "Parse a collection of items of type base_item_type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_item(self, location: str, item_type: Type[T], item_name_for_log: str = None,\n                   file_mapping_conf: FileMappingConfiguration = None, options: Dict[str, Dict[str, Any]] = None) -> T:\n        \"\"\"\n        Main method to parse an item of type item_type\n\n        :param location:\n        :param item_type:\n        :param item_name_for_log:\n        :param file_mapping_conf:\n        :param options:\n        :return:\n        \"\"\"\n\n        # -- item_name_for_log\n        item_name_for_log = item_name_for_log or ''\n        check_var(item_name_for_log, var_types=str, var_name='item_name_for_log')\n\n        if len(item_name_for_log) > 0:\n            item_name_for_log = item_name_for_log + ' '\n        self.logger.debug('**** Starting to parse single object ' + item_name_for_log + 'of type <'\n                          + get_pretty_type_str(item_type) + '> at location ' + location + ' ****')\n\n        # common steps\n        return self._parse__item(item_type, location, file_mapping_conf, options=options)", "response": "Parse a single item of type item_type at the given location."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a list of arguments and a command name find the corresponding subcommand.", "response": "def findSubCommand(args):\n  \"\"\"\n  Given a list ['foo','bar', 'baz'], attempts to create a command name in the\n  format 'foo-bar-baz'. If that command exists, we run it. If it doesn't, we\n  check to see if foo-bar exists, in which case we run `foo-bar baz`. We keep\n  taking chunks off the end of the command name and adding them to the argument\n  list until we find a valid command name we can run.\n\n  This allows us to easily make git-style command drivers where for example we\n  have a driver script, foo, and subcommand scripts foo-bar and foo-baz, and when\n  the user types `foo bar foobar` we find the foo-bar script and run it as\n  `foo-bar foobar`\n\n  :param list|tuple args: list to try and convert to a command args pair\n  :returns: command and arguments list\n  :rtype: tuple\n  :raises StandardError: if the args can't be matched to an executable subcommand\n  \"\"\"\n  # If the only command we find is the first element of args, we've found the\n  # driver script itself and re-executing it will cause an infinite loop, so\n  # don't even look at the first element on its own.\n  for n in range(len(args) - 1):\n    command = '-'.join(args[:(len(args) - n)])\n    commandArgs = args[len(args) - n:]\n    if isProgram(command):\n      return (command, commandArgs)\n  raise StandardError(\"Could not find a %s subcommand executable\" % command)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the spamsum distance between ssA and ssB.", "response": "def SpamsumDistance(ssA, ssB):\n    '''\n    returns the spamsum distance between ssA and ssB\n    if they use a different block size, assume maximum distance\n    otherwise returns the LevDistance\n    '''\n    mA = re.match('^(\\d+)[:](.*)$', ssA)\n    mB = re.match('^(\\d+)[:](.*)$', ssB)\n\n    if mA == None or mB == None:\n      raise \"do not appear to be spamsum signatures\"\n\n    if mA.group(1) != mB.group(1):\n      return max([len(mA.group(2)), len(mB.group(2))])\n    else:\n      return LevDistance(mA.group(2), mB.group(2))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef terms(cls, tags, minimum_match=None):\n        '''\n        A query that match on any (configurable) of the provided terms. This is a simpler syntax query for using a bool query with several term queries in the should clauses. For example:\n\n        {\n            \"terms\" : {\n                \"tags\" : [ \"blue\", \"pill\" ],\n                \"minimum_match\" : 1\n            }\n        }'''\n        instance = cls(terms={'tags': tags})\n        if minimum_match is not None:\n            instance['terms']['minimum_match'] = minimum_match\n        return instance", "response": "A simple class that returns a new instance of the class with the provided terms."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(cls, field, query, operator=None):\n        '''\n        A family of match queries that accept text/numerics/dates, analyzes it, and constructs a query out of it. For example:\n\n        {\n            \"match\" : {\n                \"message\" : \"this is a test\"\n            }\n        }\n        Note, message is the name of a field, you can subsitute the name of any field (including _all) instead.\n        '''\n        instance = cls(match={field: {'query': query}})\n        if operator is not None:\n            instance['match'][field]['operator'] = operator\n\n        return instance", "response": "A family of match queries that accept text / numerics / dates analyzes it and constructs a query out of it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a new BooleanQuery that matches the specified criteria.", "response": "def bool(cls, must=None, should=None, must_not=None, minimum_number_should_match=None, boost=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/bool-query.html\n        A query that matches documents matching boolean combinations of other queris. The bool query maps to Lucene BooleanQuery. It is built using one of more boolean clauses, each clause with a typed occurrence. The occurrence types are:\n        'must' - The clause(query) must appear in matching documents.\n        'should' - The clause(query) should appear in the matching document. A boolean query with no 'must' clauses, one or more 'should' clauses must match a document. The minimum number of 'should' clauses to match can be set using 'minimum_number_should_match' parameter.\n        'must_not' - The clause(query) must not appear in the matching documents. Note that it is not possible to search on documents that only consists of a 'must_not' clause(s).\n\n        'minimum_number_should_match' - Minimum number of documents that should match\n        'boost' - boost value\n        > term = ElasticQuery()\n        > term.term(user='kimchy')\n        > query = ElasticQuery()\n        > query.bool(should=term)\n        > query.query()\n          { 'bool' : { 'should' : { 'term' : {'user':'kimchy'}}}}\n\n\n        '''\n        instance = cls(bool={})\n        if must is not None:\n            instance['bool']['must'] = must\n        if should is not None:\n            instance['bool']['should'] = should\n        if must_not is not None:\n            instance['bool']['must_not'] = must_not\n        if minimum_number_should_match is not None:\n            instance['bool']['minimum_number_should_match'] = minimum_number_should_match\n        if boost is not None:\n            instance['bool']['boost'] = boost\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fuzzy_like_this(cls, like_text, fields=None, ignore_tf=None, max_query_terms=None, min_similarity=None, prefix_length=None, boost=None, analyzer=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/flt-query.html\n        Fuzzy like this query find documents that are \"like\" provided text by running it against one or more fields.\n\n        > query = ElasticQuery().fuzzy_like_this('text like this one', fields=['name.first', 'name.last'], max_query_terms=12)\n        > query\n            {'fuzze_like_this': {'boost': 1.0,\n              'fields': ['name.first', 'name.last'],\n              'ifgnore_tf': False,\n              'like_text': 'text like this one',\n              'max_query_terms': 12,\n              'min_similarity': 0.5,\n              'prefix_length': 0}}\n\n        '''\n        instance = cls(fuzzy_like_this={'like_text': like_text})\n        if fields is not None:\n            instance['fuzzy_like_this']['fields'] = fields\n        if ignore_tf is not None:\n            instance['fuzzy_like_this']['ignore_tf'] = ignore_tf\n        if max_query_terms is not None:\n            instance['fuzzy_like_this']['max_query_terms'] = max_query_terms\n        if min_similarity is not None:\n            instance['fuzzy_like_this']['min_similarity'] = min_similarity\n        if prefix_length is not None:\n            instance['fuzzy_like_this']['prefix_length'] = prefix_length\n        if boost is not None:\n            instance['fuzzy_like_this']['boost'] = boost\n        if analyzer is not None:\n            instance['fuzzy_like_this']['analyzer'] = analyzer\n\n        return instance", "response": "Fuzzy like this query find documents that are like provided text by running it against one or more fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new has_child query that returns a list of parent documents that have child docs matching the query.", "response": "def has_child(cls, child_type, query):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/has-child-query.html\n        The has_child query accepts a query and the child type to run against, and results in parent documents that have child docs matching the query.\n\n        > child_query = ElasticQuery().term(tag='something')\n        > query = ElasticQuery().has_Child('blog_tag', child_query)\n        '''\n\n        instance = cls(has_child={'type': child_type, 'query': query})\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mlt(cls, like_text, fields=None, percent_terms_to_match=None, min_term_freq=None, max_query_terms=None, stop_words=None, min_doc_freq=None, max_doc_freq=None, min_word_len=None, max_word_len=None, boost_terms=None, boost=None, analyzer=None):\n        '''\n        http://www.elasticsearch.org/guide/reference/query-dsl/mlt-query.html\n        More like this query find documents that are \"like\" provided text by running it against one or more fields.\n        > query = ElasticQuery().mlt('text like this one', fields=['post.content'])\n        '''\n\n        instance = cls(more_like_this={'like_text': like_text})\n        if fields is not None:\n            instance['more_like_this']['fields'] = fields\n        if percent_terms_to_match is not None:\n            instance['more_like_this']['percent_terms_to_match'] = percent_terms_to_match\n        if min_term_freq is not None:\n            instance['more_like_this']['min_term_freq'] = min_term_freq\n        if max_query_terms is not None:\n            instance['more_like_this']['max_query_terms'] = max_query_terms\n        if stop_words is not None:\n            instance['more_like_this']['stop_words'] = stop_words\n        if min_doc_freq is not None:\n            instance['more_like_this']['min_doc_freq'] = min_doc_freq\n        if max_doc_freq is not None:\n            instance['more_like_this']['max_doc_freq'] = max_doc_freq\n        if min_word_len is not None:\n            instance['more_like_this']['min_word_len'] = min_word_len\n        if max_word_len is not None:\n            instance['more_like_this']['max_word_len'] = max_word_len\n        if boost_terms is not None:\n            instance['more_like_this']['boost_terms'] = boost_terms\n        if boost is not None:\n            instance['more_like_this']['boost'] = boost\n        if analyzer is not None:\n            instance['more_like_this']['analyzer'] = analyzer\n        return instance", "response": "Create a new more like this query that returns documents that are like provided text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef an_text_url(identifiant, code):\n    datas = {\n        'PRJL': {\n            'repertoire': 'projets',\n            'prefixe': 'pl',\n            'suffixe': '',\n        },\n        'PION': {\n            'repertoire': 'propositions',\n            'prefixe': 'pion',\n            'suffixe': '',\n        },\n        'PNRECOMENQ': {\n            'repertoire': 'propositions',\n            'prefixe': 'pion',\n            'suffixe': '',\n        },\n        'PNREAPPART341': {\n            'repertoire': 'propositions',\n            'prefixe': 'pion',\n            'suffixe': '',\n        },\n        'PNREMODREGLTAN': {\n            'repertoire': 'propositions',\n            'prefixe': 'pion',\n            'suffixe': '',\n        },\n        'AVCE': {\n            'repertoire': 'projets',\n            'prefixe': 'pl',\n            'suffixe': '-ace',\n        },\n        'ETDI': {\n            'repertoire': 'projets',\n            'prefixe': 'pl',\n            'suffixe': '-ei',\n        },\n        'ACIN': {\n            'repertoire': 'projets',\n            'prefixe': 'pl',\n            'suffixe': '-ai',\n        },\n        'LETT': {\n            'repertoire': 'projets',\n            'prefixe': 'pl',\n            'suffixe': '-l',\n        },\n        'PNRETVXINSTITEUROP': {\n            'repertoire': 'europe/resolutions',\n            'prefixe': 'ppe',\n            'suffixe': '',\n        },\n        'PNRE': {\n            'repertoire': 'propositions',\n            'prefixe': 'pion',\n            'suffixe': '',\n        },\n        'RION': {\n            'repertoire': '',\n            'prefixe': '',\n            'suffixe': '',\n        },\n        'TCOM': {\n            'repertoire': 'ta-commission',\n            'prefixe': 'r',\n            'suffixe': '-a0',\n        },\n        'TCOMMODREGLTAN': {\n            'repertoire': 'ta-commission',\n            'prefixe': 'r',\n            'suffixe': '-a0',\n        },\n        'TCOMTVXINSTITEUROP': {\n            'repertoire': 'ta-commission',\n            'prefixe': 'r',\n            'suffixe': '-a0',\n        },\n        'TCOMCOMENQ': {\n            'repertoire': 'ta-commission',\n            'prefixe': 'r',\n            'suffixe': '-a0',\n        },\n        'TADO': {\n            'repertoire': 'ta',\n            'prefixe': 'ta',\n            'suffixe': '',\n        },\n        # NOT IN NATIONAL ASSEMBLY PHP CODE\n        'RAPP': {\n            'repertoire': 'rapports',\n            'prefixe': 'r',\n            'suffixe': '',\n        },\n        'RINF': {\n            'repertoire': 'rapports',\n            'prefixe': 'r',\n            'suffixe': '',\n        }\n    }\n    match = re.match(r'(.{4})([ANS]*)(R[0-9])([LS]*)([0-9]*)([BTACP]*)(.*)', identifiant)\n    leg = match.group(5)\n    typeTa = match.group(6)\n    num = match.group(7)\n    if typeTa == 'BTC':\n        type = 'TCOM'\n    elif typeTa == 'BTA':\n        type = 'TADO'\n    else:\n        type = code\n    host = \"http://www.assemblee-nationale.fr/\"\n\n    if type not in datas:\n        # ex: ALCNANR5L15B0002 (allocution du pr\u00e9sident)\n        raise Exception('Unknown document type for %s' % identifiant)\n\n    return host + leg + \"/\" + datas[type]['repertoire'] + \"/\" + datas[type]['prefixe'] + num + datas[type]['suffixe'] + \".asp\"", "response": "Returns an opaque URL for the given identifiant and code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a pandas data frame with the portia select resultset", "response": "def queryByPortSensor(portiaConfig, edgeId, port, sensor, last=False, params={ 'from': None, 'to': None, 'order': None, 'precision': 'ms', 'limit': None }):\n    \"\"\"Returns a pandas data frame with the portia select resultset\"\"\"\n\n    header = {'Accept': 'text/csv'}\n\n    if last == False:\n        endpoint = '/select/device/{0}/port/{1}/sensor/{2}{3}'.format( edgeId, port, sensor, utils.buildGetParams(params) )\n    else:\n        endpoint = '/select/device/{0}/port/{1}/sensor/{2}/last{3}'.format( edgeId, port, sensor, utils.buildGetParams(params) )\n\n    response = utils.httpGetRequest(portiaConfig, endpoint, header)\n\n    if response.status_code == 200:\n        try:\n\n            dimensionSeries = pandas.read_csv( StringIO(response.text), sep=';' )\n            if portiaConfig['debug']:\n                print( '[portia-debug]: {0} rows'.format( len(dimensionSeries.index) ) )\n\n            return dimensionSeries\n\n        except:\n            raise Exception('couldn\\'t create pandas data frame')\n    else:\n        raise Exception('couldn\\'t retrieve data')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to parse the provided string as a number or boolean.", "response": "def try_parse_num_and_booleans(num_str):\n    \"\"\"\n    Tries to parse the provided string as a number or boolean\n    :param num_str:\n    :return:\n    \"\"\"\n    if isinstance(num_str, str):\n        # bool\n        if num_str.lower() == 'true':\n            return True\n        elif num_str.lower() == 'false':\n            return False\n        # int\n        if num_str.isdigit():\n            return int(num_str)\n        # float\n        try:\n            return float(num_str)\n        except ValueError:\n            # give up\n            return num_str\n    else:\n        # dont try\n        return num_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_jprops_parsers(parser_finder: ParserFinder, conversion_finder: ConversionFinder) -> List[AnyParser]:\n    return [SingleFileParserFunction(parser_function=read_dict_from_properties,\n                                     streaming_mode=True, custom_name='read_dict_from_properties',\n                                     supported_exts={'.properties', '.txt'},\n                                     supported_types={dict},\n                                     function_args={'conversion_finder': conversion_finder}),\n            # SingleFileParserFunction(parser_function=read_list_from_properties,\n            #                          streaming_mode=True,\n            #                          supported_exts={'.properties', '.txt'},\n            #                          supported_types={list}),\n        ]", "response": "Utility method to return the default parsers able to parse a dictionary from a properties file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hook(event=None, dependencies=None):\n\n    def wrapper(func):\n        \"\"\"I'm a simple wrapper that manages event hooking\"\"\"\n        func.__deps__ = dependencies\n        EVENTS.hook(func, event, dependencies)\n        return func\n    return wrapper", "response": "A decorator that makes a simple wrapper that manages event hooking"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an image and its bounding boxes to the current list of files", "response": "def add_image(self, image_path, annotations):\n    \"\"\"Adds an image and its bounding boxes to the current list of files\n\n    The bounding boxes are automatically estimated based on the given annotations.\n\n    **Parameters:**\n\n    ``image_path`` : str\n      The file name of the image, including its full path\n\n    ``annotations`` : [dict]\n      A list of annotations, i.e., where each annotation can be anything that :py:func:`bounding_box_from_annotation` can handle; this list can be empty, in case the image does not contain any faces\n    \"\"\"\n    self.image_paths.append(image_path)\n    self.bounding_boxes.append([bounding_box_from_annotation(**a) for a in annotations])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_from_db(self, database, files):\n    for f in files:\n      annotation = database.annotations(f)\n      image_path = database.original_file_name(f)\n      self.add_image(image_path, [annotation])", "response": "Adds images and bounding boxes for the given files of a database that follows the BIODatabase interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, list_file):\n    bob.io.base.create_directories_safe(os.path.dirname(list_file))\n    with open(list_file, 'w') as f:\n      for i in range(len(self.image_paths)):\n        f.write(self.image_paths[i])\n        for bbx in self.bounding_boxes[i]:\n          f.write(\"\\t[%f %f %f %f]\" % (bbx.top_f, bbx.left_f, bbx.size_f[0], bbx.size_f[1]))\n        f.write(\"\\n\")", "response": "Saves the current list of annotations to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the list of annotations from the given file and appends it to the current list.", "response": "def load(self, list_file):\n    \"\"\"Loads the list of annotations from the given file and **appends** it to the current list.\n\n    ``list_file`` : str\n      The name of a list file to load and append\n    \"\"\"\n    with open(list_file) as f:\n      for line in f:\n        if line and line[0] != '#':\n          splits = line.split()\n          bounding_boxes = []\n          for i in range(1, len(splits), 4):\n            assert splits[i][0] == '[' and splits[i+3][-1] == ']'\n            bounding_boxes.append(BoundingBox(topleft=(float(splits[i][1:]), float(splits[i+1])), size=(float(splits[i+2]), float(splits[i+3][:-1]))))\n          self.image_paths.append(splits[0])\n          self.bounding_boxes.append(bounding_boxes)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iterate(self, max_number_of_files=None):\n    indices = quasi_random_indices(len(self), max_number_of_files)\n    for index in indices:\n      image = bob.io.base.load(self.image_paths[index])\n      if len(image.shape) == 3:\n        image = bob.ip.color.rgb_to_gray(image)\n      # return image and bounding box as iterator\n      yield image, self.bounding_boxes[index], self.image_paths[index]", "response": "This function loads the images and converts them to gray - scale and yields the image and bounding boxes and original image file name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the name of an intermediate file for storing features.", "response": "def _feature_file(self, parallel = None, index = None):\n    \"\"\"Returns the name of an intermediate file for storing features.\"\"\"\n    if index is None:\n      index = 0 if parallel is None or \"SGE_TASK_ID\" not in os.environ else int(os.environ[\"SGE_TASK_ID\"])\n    return os.path.join(self.feature_directory, \"Features_%02d.hdf5\" % index)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting features from all images in all scales and writes them to a file.", "response": "def extract(self, sampler, feature_extractor, number_of_examples_per_scale = (100, 100), similarity_thresholds = (0.5, 0.8), parallel = None, mirror = False, use_every_nth_negative_scale = 1):\n    \"\"\"Extracts features from **all** images in **all** scales and writes them to file.\n\n    This function iterates over all images that are present in the internally stored list, and extracts features using the given ``feature_extractor`` for every image patch that the given ``sampler`` returns.\n    The final features will be stored in the ``feature_directory`` that is set in the constructor.\n\n    For each image, the ``sampler`` samples patch locations, which cover the whole image in different scales.\n    For each patch locations is tested, how similar they are to the face bounding boxes that belong to that image, using the Jaccard :py:meth:`BoundingBox.similarity`.\n    The similarity is compared to the ``similarity_thresholds``.\n    If it is smaller than the first threshold, the patch is considered as background, when it is greater the the second threshold, it is considered as a face, otherwise it is rejected.\n    Depending on the image resolution and the number of bounding boxes, this will usually result in some positive and thousands of negative patches per image.\n    To limit the total amount of training data, for all scales, only up to a given number of positive and negative patches are kept.\n    Also, to further limit the number of negative samples, only every ``use_every_nth_negative_scale`` scale is considered (for the positives, always all scales are processed).\n\n    To increase the number (especially of positive) examples, features can also be extracted for horizontally mirrored images.\n    Simply set the ``mirror`` parameter to ``True``.\n    Furthermore, this function is designed to be run using several parallel processes, e.g., using the `GridTK <https://pypi.python.org/pypi/gridtk>`_.\n    Each of the processes will run on a particular subset of the images, which is defined by the ``SGE_TASK_ID`` environment variable.\n    The ``parallel`` parameter defines the total number of parallel processes that are used.\n\n    **Parameters:**\n\n    ``sampler`` : :py:class:`Sampler`\n      The sampler to use to sample patches of the images. Please assure that the sampler is set up such that it samples patch locations which can overlap with the face locations.\n\n    ``feature_extractor`` : :py:class:`FeatureExtractor`\n      The feature extractor to be used to extract features from image patches\n\n    ``number_of_examples_per_scale`` : (int, int)\n      The maximum number of positive and negative examples to extract for each scale of the image\n\n    ``similarity_thresholds`` : (float, float)\n      The Jaccard similarity threshold, below which patch locations are considered to be negative, and above which patch locations are considered to be positive examples.\n\n    ``parallel`` : int or ``None``\n      If given, the total number of parallel processes, which are used to extract features (the current process index is read from the ``SGE_TASK_ID`` environment variable)\n\n    ``mirror`` : bool\n      Extract positive and negative samples also from horizontally mirrored images?\n\n    ``use_every_nth_negative_scale`` : int\n      Skip some negative scales to decrease the number of negative examples, i.e., only extract and store negative features, when ``scale_counter % use_every_nth_negative_scale == 0``\n\n      .. note::\n         The ``scale_counter`` is not reset between images, so that we might get features from different scales in subsequent images.\n    \"\"\"\n\n    feature_file = self._feature_file(parallel)\n    bob.io.base.create_directories_safe(self.feature_directory)\n\n    if parallel is None or \"SGE_TASK_ID\" not in os.environ or os.environ[\"SGE_TASK_ID\"] == '1':\n      extractor_file = os.path.join(self.feature_directory, \"Extractor.hdf5\")\n      hdf5 = bob.io.base.HDF5File(extractor_file, \"w\")\n      feature_extractor.save(hdf5)\n      del hdf5\n\n    total_positives, total_negatives = 0, 0\n\n    indices = parallel_part(range(len(self)), parallel)\n    if not indices:\n      logger.warning(\"The index range for the current parallel thread is empty.\")\n    else:\n      logger.info(\"Extracting features for images in range %d - %d of %d\", indices[0], indices[-1], len(self))\n\n    hdf5 = bob.io.base.HDF5File(feature_file, \"w\")\n    for index in indices:\n      hdf5.create_group(\"Image-%d\" % index)\n      hdf5.cd(\"Image-%d\" % index)\n\n      logger.debug(\"Processing file %d of %d: %s\", index+1, indices[-1]+1, self.image_paths[index])\n\n      # load image\n      image = bob.io.base.load(self.image_paths[index])\n      if image.ndim == 3:\n        image = bob.ip.color.rgb_to_gray(image)\n      # get ground_truth bounding boxes\n      ground_truth = self.bounding_boxes[index]\n\n      # collect image and GT for originally and mirrored image\n      images = [image] if not mirror else [image, bob.ip.base.flop(image)]\n      ground_truths = [ground_truth] if not mirror else [ground_truth, [gt.mirror_x(image.shape[1]) for gt in ground_truth]]\n      parts = \"om\"\n\n      # now, sample\n      scale_counter = -1\n      for image, ground_truth, part in zip(images, ground_truths, parts):\n        for scale, scaled_image_shape in sampler.scales(image):\n          scale_counter += 1\n          scaled_gt = [gt.scale(scale) for gt in ground_truth]\n          positives = []\n          negatives = []\n          # iterate over all possible positions in the image\n          for bb in sampler.sample_scaled(scaled_image_shape):\n            # check if the patch is a positive example\n            positive = False\n            negative = True\n            for gt in scaled_gt:\n              similarity = bb.similarity(gt)\n              if similarity > similarity_thresholds[1]:\n                positive = True\n                break\n              if similarity > similarity_thresholds[0]:\n                negative = False\n                break\n\n            if positive:\n              positives.append(bb)\n            elif negative and scale_counter % use_every_nth_negative_scale == 0:\n              negatives.append(bb)\n\n          # per scale, limit the number of positive and negative samples\n          positives = [positives[i] for i in quasi_random_indices(len(positives), number_of_examples_per_scale[0])]\n          negatives = [negatives[i] for i in quasi_random_indices(len(negatives), number_of_examples_per_scale[1])]\n\n          # extract features\n          feature_extractor.prepare(image, scale)\n          # .. negative features\n          if negatives:\n            negative_features = numpy.zeros((len(negatives), feature_extractor.number_of_features), numpy.uint16)\n            for i, bb in enumerate(negatives):\n              feature_extractor.extract_all(bb, negative_features, i)\n            hdf5.set(\"Negatives-%s-%.5f\" % (part,scale), negative_features)\n            total_negatives += len(negatives)\n\n          # positive features\n          if positives:\n            positive_features = numpy.zeros((len(positives), feature_extractor.number_of_features), numpy.uint16)\n            for i, bb in enumerate(positives):\n              feature_extractor.extract_all(bb, positive_features, i)\n            hdf5.set(\"Positives-%s-%.5f\" % (part,scale), positive_features)\n            total_positives += len(positives)\n      # cd backwards after each image\n      hdf5.cd(\"..\")\n\n    hdf5.set(\"TotalPositives\", total_positives)\n    hdf5.set(\"TotalNegatives\", total_negatives)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the nparam value and returns the default if it doesn t exist.", "response": "def get(self, param, default=EMPTY):\n        \"\"\"\n        Returns the nparam value, and returns the default if it doesn't exist.\n        If default is none, an exception will be raised instead.\n\n        the returned parameter will have been specialized against the global context\n        \"\"\"\n        if not self.has(param):\n            if default is not EMPTY:\n                return default\n            raise ParamNotFoundException(\"value for %s not found\" % param)\n        context_dict = copy.deepcopy(self.manifest.get_context_dict())\n        for k, v in self.raw_dict.items():\n            context_dict[\"%s:%s\" % (self.feature_name, k)] = v\n        cur_value = self.raw_dict[param]\n        prev_value = None\n        max_depth = 5\n        # apply the context until doing so does not change the value\n        while cur_value != prev_value and max_depth > 0:\n            prev_value = cur_value\n            try:\n                cur_value = str(prev_value) % context_dict\n            except KeyError:\n                e = sys.exc_info()[1]\n                key = e.args[0]\n                if key.startswith('config:'):\n                    missing_key = key.split(':')[1]\n                    if self.manifest.inputs.is_input(missing_key):\n                        val = self.manifest.inputs.get_input(missing_key)\n                        context_dict[key] = val\n                else:\n                    logger.warn(\"Could not specialize %s! Error: %s\" % (self.raw_dict[param], e))\n                    return self.raw_dict[param]\n            except ValueError:\n                # this is an esoteric error, and this implementation\n                # forces a terrible solution. Sorry.\n                # using the standard escaping syntax in python is a mistake.\n                # if a value has a \"%\" inside (e.g. a password), a ValueError\n                # is raised, causing an issue\n                return cur_value\n            max_depth -= 1\n        return cur_value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self, param, value):\n        self.raw_dict[param] = value\n        self.manifest.set(self.feature_name, param, value)", "response": "sets the param to the value provided"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a parameter from the manifest", "response": "def remove(self, param):\n        \"\"\" Remove a parameter from the manifest \"\"\"\n        if self.has(param):\n            del(self.raw_dict[param])\n            self.manifest.remove_option(self.feature_name, param)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the parameter to the default if it doesn t exist", "response": "def set_if_empty(self, param, default):\n        \"\"\" Set the parameter to the default if it doesn't exist \"\"\"\n        if not self.has(param):\n            self.set(param, default)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_dict(self):\n        return dict((k, str(self.get(k))) for k in self.raw_dict)", "response": "Returns the context fully specialized as a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nover-writes the section of the manifest with the featureconfig s value", "response": "def write_to_manifest(self):\n        \"\"\" Overwrites the section of the manifest with the featureconfig's value \"\"\"\n        self.manifest.remove_section(self.feature_name)\n        self.manifest.add_section(self.feature_name)\n        for k, v in self.raw_dict.items():\n            self.manifest.set(self.feature_name, k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mro_resolve(name, bases, dict):\n\n  if name in dict:\n    return dict[name]\n  for base in bases:\n    if hasattr(base, name):\n      return getattr(base, name)\n    try:\n      return mro_resolve(name, base.__bases__, {})\n    except KeyError:\n      pass\n  raise KeyError(name)", "response": "Resolves a value with the specified name from a list of baseclasses and a dictionary that takes precedence\n over any value in the bases and returns it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef round_to_05(n, exp=None, mode='s'):\n    n = np.asarray(n)\n    if exp is None:\n        exp = np.floor(np.log10(np.abs(n)))  # exponent for base 10\n    ntmp = np.abs(n)/10.**exp  # mantissa for base 10\n    if mode == 's':\n        n1 = ntmp\n        s = 1.\n        n2 = nret = np.floor(ntmp)\n    else:\n        n1 = nret = np.ceil(ntmp)\n        s = -1.\n        n2 = ntmp\n    return np.where(n1 - n2 > 0.5, np.sign(n)*(nret + s*0.5)*10.**exp,\n                    np.sign(n)*nret*10.**exp)", "response": "Round to the next 0. 5 - value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_radian(coord, *variables):\n    if any(v.attrs.get('units') == 'radian' for v in variables):\n        return coord * 180. / np.pi\n    return coord", "response": "Convert the given coordinate from radian to degree\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing the coordinate for the data array at the given position", "response": "def replace_coord(self, i):\n        \"\"\"Replace the coordinate for the data array at the given position\n\n        Parameters\n        ----------\n        i: int\n            The number of the data array in the raw data (if the raw data is\n            not an interactive list, use 0)\n\n        Returns\n        xarray.DataArray\n            The data array with the replaced coordinate\"\"\"\n        da = next(islice(self.data_iterator, i, i+1))\n        name, coord = self.get_alternative_coord(da, i)\n        other_coords = {key: da.coords[key]\n                        for key in set(da.coords).difference(da.dims)}\n        ret = da.rename({da.dims[-1]: name}).assign_coords(\n            **{name: coord}).assign_coords(**other_coords)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the current axis colors", "response": "def value2pickle(self):\n        \"\"\"Return the current axis colors\"\"\"\n        return {key: s.get_edgecolor() for key, s in self.ax.spines.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the default formatters that is used for updating to None", "response": "def set_default_formatters(self, which=None):\n        \"\"\"Sets the default formatters that is used for updating to None\n\n        Parameters\n        ----------\n        which: {None, 'minor', 'major'}\n            Specify which locator shall be set\"\"\"\n        if which is None or which == 'minor':\n            self.default_formatters['minor'] = self.axis.get_minor_formatter()\n        if which is None or which == 'major':\n            self.default_formatters['major'] = self.axis.get_major_formatter()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the colormap for plotting the object.", "response": "def get_cmap(self, arr=None, cmap=None, N=None):\n        \"\"\"Get the :class:`matplotlib.colors.Colormap` for plotting\n\n        Parameters\n        ----------\n        arr: np.ndarray\n            The array to plot\n        cmap: str or matplotlib.colors.Colormap\n            The colormap to use. If None, the :attr:`value` of this\n            formatoption is used\n        N: int\n            The number of colors in the colormap. If None, the norm of the\n            :attr:`bounds` formatoption is used and, if necessary, the\n            given array `arr`\n\n        Returns\n        -------\n        matplotlib.colors.Colormap\n            The colormap returned by :func:`psy_simple.colors.get_cmap`\"\"\"\n        N = N or None\n        if cmap is None:\n            cmap = self.value\n        if N is None:\n            try:\n                N = self.bounds.norm.Ncmap\n            except AttributeError:\n                if arr is not None and self.bounds.norm is not None:\n                    N = len(np.unique(self.bounds.norm(arr.ravel())))\n        if N is not None:\n            return get_cmap(cmap, N)\n        return get_cmap(cmap)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fmt_widget(self, parent, project):\n        from psy_simple.widgets.colors import CMapFmtWidget\n        return CMapFmtWidget(parent, self, project)", "response": "Open a : class : psy_simple. widget. CMapFmtWidget"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ycoord(self):\n        return self.decoder.get_y(self.data, coords=self.data.coords)", "response": "The y coordinate of the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cell_nodes_x(self):\n        decoder = self.decoder\n        xcoord = self.xcoord\n        data = self.data\n        xbounds = decoder.get_cell_node_coord(\n            data, coords=data.coords, axis='x')\n        if self.plotter.convert_radian:\n            xbounds = convert_radian(xbounds, xcoord, xbounds)\n        return xbounds.values", "response": "The x - boundaries of the cell nodes with shape N where m > 2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cell_nodes_y(self):\n        decoder = self.decoder\n        ycoord = self.ycoord\n        data = self.data\n        ybounds = decoder.get_cell_node_coord(\n            data, coords=data.coords, axis='y')\n        if self.plotter.convert_radian:\n            ybounds = convert_radian(ybounds, ycoord, ybounds)\n        return ybounds.values", "response": "The y - boundaries of the cell nodes in the current state of the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef axis(self):\n        return getattr(\n            self.colorbar.ax, self.axis_locations[self.position] + 'axis')", "response": "axis of the colorbar with the ticks. Will be overwritten during\n            update process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_formatters(self):\n        if self._default_formatters:\n            return self._default_formatters\n        else:\n            self.set_default_formatters()\n        return self._default_formatters", "response": "Default locator of the axis of the colorbars"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add2format_coord(self, x, y):\n        u, v = self.data\n        uname, vname = self.data.coords['variable'].values\n        xcoord = self.xcoord\n        ycoord = self.ycoord\n        if self.decoder.is_triangular(self.raw_data[0]):\n            x, y, z1, z2 = self.get_xyz_tri(xcoord, x, ycoord, y, u, v)\n        elif xcoord.ndim == 1:\n            x, y, z1, z2 = self.get_xyz_1d(xcoord, x, ycoord, y, u, v)\n        elif xcoord.ndim == 2:\n            x, y, z1, z2 = self.get_xyz_2d(xcoord, x, ycoord, y, u, v)\n        speed = (z1**2 + z2**2)**0.5\n        xunit = xcoord.attrs.get('units', '')\n        if xunit:\n            xunit = ' ' + xunit\n        yunit = ycoord.attrs.get('units', '')\n        if yunit:\n            yunit = ' ' + yunit\n        zunit = u.attrs.get('units', '')\n        if zunit:\n            zunit = ' ' + zunit\n        return (', vector data: %s: %.4g%s, %s: %.4g%s, %s: %.4g%s, '\n                '%s: %.4g%s, absolute: %.4g%s') % (\n                    xcoord.name, x, xunit, ycoord.name, y, yunit,\n                    uname, z1, zunit, vname, z2, zunit,\n                    speed, zunit)", "response": "Add additional information for the format_coord method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets closest x y and z for the given x and y coordinates", "response": "def get_xyz_tri(self, xcoord, x, ycoord, y, u, v):\n        \"\"\"Get closest x, y and z for the given `x` and `y` in `data` for\n        1d coords\"\"\"\n        return self.get_xyz_2d(xcoord, x, ycoord, y, u, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xyz_1d(self, xcoord, x, ycoord, y, u, v):\n        xclose = xcoord.indexes[xcoord.name].get_loc(x, method='nearest')\n        yclose = ycoord.indexes[ycoord.name].get_loc(y, method='nearest')\n        uval = u[yclose, xclose].values\n        vval = v[yclose, xclose].values\n        return xcoord[xclose].values, ycoord[yclose].values, uval, vval", "response": "Get closest x y and z for the given x and y in data for\n        1d coords"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets closest x y and z for the given x and y in data for the given x and y coordinates.", "response": "def get_xyz_2d(self, xcoord, x, ycoord, y, u, v):\n        \"\"\"Get closest x, y and z for the given `x` and `y` in `data` for\n        2d coords\"\"\"\n        xy = xcoord.values.ravel() + 1j * ycoord.values.ravel()\n        dist = np.abs(xy - (x + 1j * y))\n        imin = np.nanargmin(dist)\n        xy_min = xy[imin]\n        return (xy_min.real, xy_min.imag, u.values.ravel()[imin],\n                v.values.ravel()[imin])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hist2d(self, da, **kwargs):\n        if self.value is None or self.value == 'counts':\n            normed = False\n        else:\n            normed = True\n        y = da.values\n        x = da.coords[da.dims[0]].values\n        counts, xedges, yedges = np.histogram2d(\n            x, y, normed=normed, **kwargs)\n        if self.value == 'counts':\n            counts = counts / counts.sum().astype(float)\n        return counts, xedges, yedges", "response": "Make the two dimensional histogram of the data array."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a bivariate kde using statsmodels.", "response": "def _statsmodels_bivariate_kde(self, x, y, bws, xsize, ysize, xyranges):\n        \"\"\"Compute a bivariate kde using statsmodels.\n        This function is mainly motivated through\n        seaborn.distributions._statsmodels_bivariate_kde\"\"\"\n        import statsmodels.nonparametric.api as smnp\n        for i, (coord, bw) in enumerate(zip([x, y], bws)):\n            if isinstance(bw, six.string_types):\n                bw_func = getattr(smnp.bandwidths, \"bw_\" + bw)\n                bws[i] = bw_func(coord)\n        kde = smnp.KDEMultivariate([x, y], \"cc\", bws)\n        x_support = np.linspace(xyranges[0][0], xyranges[0][1], xsize)\n        y_support = np.linspace(xyranges[1][0], xyranges[1][1], ysize)\n        xx, yy = np.meshgrid(x_support, y_support)\n        z = kde.pdf([xx.ravel(), yy.ravel()]).reshape(xx.shape)\n        return x_support, y_support, z"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_data(cls, name, dims, is_unstructured=None):\n        if isinstance(name, six.string_types) or not is_iterable(name):\n            name = [name]\n            dims = [dims]\n        N = len(name)\n        if len(dims) != N:\n            return [False] * N, [\n                'Number of provided names (%i) and dimensions '\n                '%(i) are not the same' % (N, len(dims))] * N\n        checks = [True] * N\n        messages = [''] * N\n        for i, (n, d) in enumerate(zip(name, dims)):\n            if n != 0 and not n:\n                checks[i] = False\n                messages[i] = 'At least one variable name is required!'\n            elif ((not isstring(n) and is_iterable(n) and\n                   len(n) > cls.allowed_vars) and\n                  len(d) != (cls.allowed_dims - len(slist(n)))):\n                checks[i] = False\n                messages[i] = 'Only %i names are allowed per array!' % (\n                    cls.allowed_vars)\n            elif len(d) != cls.allowed_dims:\n                checks[i] = False\n                messages[i] = 'Only %i-dimensional arrays are allowed!' % (\n                    cls.allowed_dims)\n        return checks, messages", "response": "A validation method for the data shape of the array per\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_data(cls, name, dims, is_unstructured):\n        if isinstance(name, six.string_types) or not is_iterable(name):\n            name = [name]\n            dims = [dims]\n            is_unstructured = [is_unstructured]\n        N = len(name)\n        if N != 1:\n            return [False] * N, [\n                'Number of provided names (%i) must equal 1!' % (N)] * N\n        elif len(dims) != 1:\n            return [False], [\n                'Number of provided dimension lists (%i) must equal 1!' % (\n                    len(dims))]\n        elif len(is_unstructured) != 1:\n            return [False], [\n                ('Number of provided unstructured information (%i) must '\n                 'equal 1!') % (len(is_unstructured))]\n        if name[0] != 0 and not name[0]:\n            return [False], ['At least one variable name must be provided!']\n        # unstructured arrays have only 1 dimension\n        dimlen = cls.allowed_dims\n        if is_unstructured[0]:\n            dimlen -= 1\n        # Check that the array is two-dimensional\n        #\n        # if more than one array name is provided, the dimensions should be\n        # one les than dimlen to have a 2D array\n        if (not isstring(name[0]) and not is_iterable(name[0])\n                and len(name[0]) != 1 and len(dims[0]) != dimlen - 1):\n            return [False], ['Only one name is allowed per array!']\n        # otherwise the number of dimensions must equal dimlen\n        if len(dims[0]) != dimlen:\n            return [False], [\n                'An array with dimension %i is required, not %i' % (\n                    dimlen, len(dims[0]))]\n        return [True], ['']", "response": "A validation method for the data shape of the array of the related object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a JSON - compatible structure capable turn the old record into the new record.", "response": "def record_diff(old, new):\n    \"\"\"Return a JSON-compatible structure capable turn the `new` record back\n    into the `old` record. The parameters must be structures compatible with\n    json.dumps *or* strings compatible with json.loads. Note that by design,\n    `old == record_patch(new, record_diff(old, new))`\"\"\"\n    old, new = _norm_json_params(old, new)\n    return json_delta.diff(new, old, verbose=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef record_patch(rec, diff):\n    rec, diff = _norm_json_params(rec, diff)\n    return json_delta.patch(rec, diff, in_place=False)", "response": "Return the JSON - compatible structure that results from applying the\n    changes in diff to the record rec."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a diff as generated by record_diff append a diff record to the diff_hist list.", "response": "def append_diff_hist(diff, diff_hist=list()):\n    \"\"\"Given a diff as generated by record_diff, append a diff record to the\n    list of diff_hist records.\"\"\"\n    diff, diff_hist = _norm_json_params(diff, diff_hist)\n\n    if not diff_hist:\n        diff_hist = list()\n\n    diff_hist.append({'diff': diff, 'diff_date': now_field()})\n    return diff_hist"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a diff_hist as created by append_diff_hist yield the JSON version of the object that was created by apply_diff_hist and the date that was taken by apply_diff_hist.", "response": "def parse_diff_hist(curr_obj, diff_hist):\n    \"\"\"Given a diff_hist as created, appended by append_diff_hist, yield the\n    versions of the object start with curr_obj and working backwards in time.\n    Each instance yielded is of the form (obj, date-string) where obj is the\n    JSON version of the object created by applying a diff in the\n    diff history and date-string is a string representing the date/time that\n    the diff was taken\"\"\"\n    curr_obj, diff_hist = _norm_json_params(curr_obj, diff_hist)\n\n    yield (json.dumps(curr_obj), None)\n\n    last_obj = curr_obj\n    for one in reversed(diff_hist):\n        last_obj = record_patch(last_obj, one['diff'])\n        yield json.dumps(last_obj), one['diff_date']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self):\n        data = {\n            'id': self.id,\n            'referenceId': self.reference_id,\n            'type': self.type,\n            'displayName': self.display_name,\n            'remoteUrl': self.remote_url}\n        for key in data.keys():\n            if data[key] == None:\n                data.pop(key)\n        return data", "response": "Converts object into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n        data = {\n            'url': self.url,\n            'encodingRate': self.encoding_rate,\n            'frameHeight': self.frame_height,\n            'frameWidth': self.frame_width,\n            'size': self.size,\n            'remoteUrl': self.remote_url,\n            'remoteStream': self.remote_stream_name,\n            'videoDuration': self.video_duration,\n            'videoCodec': self.video_codec}\n        [data.pop(key) for key in data.keys() if data[key] is None]\n        return data", "response": "Converts object into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts object into a dictionary.", "response": "def to_dict(self):\n        \"\"\"\n        Converts object into a dictionary.\n        \"\"\"\n        data = {\n            'name': self.name,\n            'video_id': self.video_id,\n            'time': self.time,\n            'forceStop': self.force_stop,\n            'type': self.type,\n            'metadata': self.metadata}\n        for key in data.keys():\n            if data[key] == None:\n                data.pop(key)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_video(self):\n        data = None\n        if self.id:\n            data = self.connection.get_item(\n                'find_video_by_id', video_id=self.id)\n        elif self.reference_id:\n            data = self.connection.get_item(\n                'find_video_by_reference_id', reference_id=self.reference_id)\n\n        if data:\n            self._load(data)", "response": "Lookup and populate a video object given a video_id or reference_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_dict(self):\n        for i, tag in enumerate(self.tags):\n            if tag in (\"\", None):\n                self.tags.pop(i)\n\n        data = {\n            'name': self.name,\n            'referenceId': self.reference_id,\n            'shortDescription': self.short_description,\n            'longDescription': self.long_description,\n            'itemState': self.item_state,\n            'linkURL': self.link_url,\n            'linkText': self.link_text,\n            'tags': self.tags,\n            'economics': self.economics,\n            'id': self.id,\n            'end_date': _make_tstamp(self.end_date),\n            'start_date': _make_tstamp(self.start_date)}\n        if len(self.renditions) > 0:\n            data['renditions'] = []\n            for r in self.renditions:\n                data['renditions'].append(r.to_dict())\n        if len(self.metadata) > 0:\n            data['customFields'] = {}\n            for meta in self.metadata:\n                data['customFields'][meta['key']] = meta['value']\n        [data.pop(key) for key in data.keys() if data[key] == None]\n        return data", "response": "Converts object into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the object into an XML string.", "response": "def to_xml(self):\n        # pylint: disable=R0912\n        \"\"\"\n        Converts object into an XML string.\n        \"\"\"\n        xml = ''\n        for asset in self.assets:\n            xml += '<asset filename=\"%s\" ' % \\\n                os.path.basename(asset['filename'])\n            xml += ' refid=\"%(refid)s\"' % asset\n            xml += ' size=\"%(size)s\"' % asset\n            xml += ' hash-code=\"%s\"' % asset['hash-code']\n            xml += ' type=\"%(type)s\"' % asset\n            if asset.get('encoding-rate', None):\n                xml += ' encoding-rate=\"%s\"' % asset['encoding-rate']\n            if asset.get('frame-width', None):\n                xml += ' frame-width=\"%s\"' % asset['frame-width']\n            if asset.get('frame-height', None):\n                xml += ' frame-height=\"%s\"' % asset['frame-height']\n            if asset.get('display-name', None):\n                xml += ' display-name=\"%s\"' % asset['display-name']\n            if asset.get('encode-to', None):\n                xml += ' encode-to=\"%s\"' % asset['encode-to']\n            if asset.get('encode-multiple', None):\n                xml += ' encode-multiple=\"%s\"' % asset['encode-multiple']\n            if asset.get('h264-preserve-as-rendition', None):\n                xml += ' h264-preserve-as-rendition=\"%s\"' % \\\n                    asset['h264-preserve-as-rendition']\n            if asset.get('h264-no-processing', None):\n                xml += ' h264-no-processing=\"%s\"' % asset['h264-no-processing']\n            xml += ' />\\n'\n        xml += '<title name=\"%(name)s\" refid=\"%(referenceId)s\" active=\"TRUE\" '\n        if self.start_date:\n            xml += 'start-date=\"%(start_date)s\" '\n        if self.end_date:\n            xml += 'end-date=\"%(end_date)s\" '\n        for asset in self.assets:\n            if asset.get('encoding-rate', None) == None:\n                choice = enums.AssetTypeEnum\n                if asset.get('type', None) == choice.VIDEO_FULL:\n                    xml += 'video-full-refid=\"%s\" ' % asset.get('refid')\n                if asset.get('type', None) == choice.THUMBNAIL:\n                    xml += 'thumbnail-refid=\"%s\" ' % asset.get('refid')\n                if asset.get('type', None) == choice.VIDEO_STILL:\n                    xml += 'video-still-refid=\"%s\" ' % asset.get('refid')\n                if asset.get('type', None) == choice.FLV_BUMPER:\n                    xml += 'flash-prebumper-refid=\"%s\" ' % asset.get('refid')\n        xml += '>\\n'\n        if self.short_description:\n            xml += '<short-description><![CDATA[%(shortDescription)s]]>'\n            xml += '</short-description>\\n'\n        if self.long_description:\n            xml += '<long-description><![CDATA[%(longDescription)s]]>'\n            xml += '</long-description>\\n'\n        for tag in self.tags:\n            xml += '<tag><![CDATA[%s]]></tag>\\n' % tag\n        for asset in self.assets:\n            if asset.get('encoding-rate', None):\n                xml += '<rendition-refid>%s</rendition-refid>\\n' % \\\n                    asset['refid']\n        for meta in self.metadata:\n            xml += '<custom-%s-value name=\"%s\">%s</custom-%s-value>' % \\\n                (meta['type'], meta['key'], meta['value'], meta['type'])\n        xml += '</title>'\n        xml = xml % self._to_dict()\n        return xml"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_custom_metadata(self):\n        if self.id is not None:\n            data = self.connection.get_item(\n                'find_video_by_id',\n                video_id=self.id,\n                video_fields=\"customFields\"\n            )\n            for key in data.get(\"customFields\", {}).keys():\n                val = data[\"customFields\"].get(key)\n                if val is not None:\n                    self.add_custom_metadata(key, val)", "response": "Fetches custom metadta for an already exisiting Video."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_custom_metadata(self, key, value, meta_type=None):\n        self.metadata.append({'key': key, 'value': value, 'type': meta_type})", "response": "Add custom metadata to the Video."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_asset(self, filename, asset_type, display_name,\n        encoding_rate=None, frame_width=None, frame_height=None,\n        encode_to=None, encode_multiple=False,\n        h264_preserve_as_rendition=False, h264_no_processing=False):\n        \"\"\"\n        Add an asset to the Video object.\n        \"\"\"\n        m = hashlib.md5()\n        fp = file(filename, 'rb')\n        bits = fp.read(262144)  ## 256KB\n        while bits:\n            m.update(bits)\n            bits = fp.read(262144)\n        fp.close()\n\n        hash_code = m.hexdigest()\n        refid = \"%s-%s\" % (os.path.basename(filename), hash_code)\n\n        asset = {\n            'filename': filename,\n            'type': asset_type,\n            'size': os.path.getsize(filename),\n            'refid': refid,\n            'hash-code': hash_code}\n\n        if encoding_rate:\n            asset.update({'encoding-rate': encoding_rate})\n        if frame_width:\n            asset.update({'frame-width': frame_width})\n        if frame_height:\n            asset.update({'frame-height': frame_height})\n        if display_name:\n            asset.update({'display-name': display_name})\n        if encode_to:\n            asset.update({'encode-to': encode_to})\n            asset.update({'encode-multiple': encode_multiple})\n            if encode_multiple and h264_preserve_as_rendition:\n                asset.update({\n                    'h264-preserve-as-rendition': h264_preserve_as_rendition})\n        else:\n            if h264_no_processing:\n                asset.update({'h264-no-processing': h264_no_processing})\n        self.assets.append(asset)", "response": "Add an asset to the Video object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, create_multiple_renditions=True,\n        preserve_source_rendition=True,\n        encode_to=enums.EncodeToEnum.FLV):\n        \"\"\"\n        Creates or updates the video\n        \"\"\"\n        if is_ftp_connection(self.connection) and len(self.assets) > 0:\n            self.connection.post(xml=self.to_xml(), assets=self.assets)\n        elif not self.id and self._filename:\n            self.id = self.connection.post('create_video', self._filename,\n                create_multiple_renditions=create_multiple_renditions,\n                preserve_source_rendition=preserve_source_rendition,\n                encode_to=encode_to,\n                video=self._to_dict())\n        elif not self.id and len(self.renditions) > 0:\n            self.id = self.connection.post('create_video',\n                video=self._to_dict())\n        elif self.id:\n            data = self.connection.post('update_video', video=self._to_dict())\n            if data:\n                self._load(data)", "response": "Creates or updates the video object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_upload_status(self):\n        if self.id:\n            return self.connection.post('get_upload_status', video_id=self.id)", "response": "Get the status of the video that has been uploaded."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a share of a list of accounts.", "response": "def share(self, accounts):\n        \"\"\"\n        Create a share\n        \"\"\"\n        if not isinstance(accounts, (list, tuple)):\n            msg = \"Video.share expects an iterable argument\"\n            raise exceptions.PyBrightcoveError(msg)\n        raise exceptions.PyBrightcoveError(\"Not yet implemented\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_image(self, image, filename=None, resize=False):\n        if self.id:\n            data = self.connection.post('add_image', filename,\n                video_id=self.id, image=image.to_dict(), resize=resize)\n            if data:\n                self.image = Image(data=data)", "response": "Set the poster or thumbnail of a this Vidoe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_related(self, _connection=None, page_size=100, page_number=0):\n        if self.id:\n            return connection.ItemResultSet('find_related_videos',\n                Video, _connection, page_size, page_number, None, None,\n                video_id=self.id)", "response": "Returns a list of all videos that are related to this one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the video represented by the video_id parameter.", "response": "def delete_video(video_id, cascade=False, delete_shares=False,\n        _connection=None):\n        \"\"\"\n        Delete the video represented by the ``video_id`` parameter.\n        \"\"\"\n        c = _connection\n        if not c:\n            c = connection.APIConnection()\n        c.post('delete_video', video_id=video_id, cascade=cascade,\n            delete_shares=delete_shares)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the status of a video given the video_id parameter.", "response": "def get_status(video_id, _connection=None):\n        \"\"\"\n        Get the status of a video given the ``video_id`` parameter.\n        \"\"\"\n        c = _connection\n        if not c:\n            c = connection.APIConnection()\n        return c.post('get_upload_status', video_id=video_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef activate(video_id, _connection=None):\n        c = _connection\n        if not c:\n            c = connection.APIConnection()\n        data = c.post('update_video', video={\n            'id': video_id,\n            'itemState': enums.ItemStateEnum.ACTIVE})\n        return Video(data=data, _connection=c)", "response": "Mark a video as Active"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all videos modified since a certain date.", "response": "def find_modified(since, filter_list=None, _connection=None, page_size=25,\n        page_number=0, sort_by=enums.DEFAULT_SORT_BY,\n        sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List all videos modified since a certain date.\n        \"\"\"\n        filters = []\n        if filter_list is not None:\n            filters = filter_list\n        if not isinstance(since, datetime):\n            msg = 'The parameter \"since\" must be a datetime object.'\n            raise exceptions.PyBrightcoveError(msg)\n        fdate = int(since.strftime(\"%s\")) / 60  ## Minutes since UNIX time\n        return connection.ItemResultSet('find_modified_videos',\n            Video, _connection, page_size, page_number, sort_by, sort_order,\n            from_date=fdate, filter=filters)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_by_tags(and_tags=None, or_tags=None, _connection=None,\n        page_size=100, page_number=0, sort_by=enums.DEFAULT_SORT_BY,\n        sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List videos given a certain set of tags.\n        \"\"\"\n        err = None\n        if not and_tags and not or_tags:\n            err = \"You must supply at least one of either and_tags or or_tags.\"\n        if and_tags and not isinstance(and_tags, (tuple, list)):\n            err = \"The and_tags argument for Video.find_by_tags must an \"\n            err += \"iterable\"\n        if or_tags and not isinstance(or_tags, (tuple, list)):\n            err = \"The or_tags argument for Video.find_by_tags must an \"\n            err += \"iterable\"\n        if err:\n            raise exceptions.PyBrightcoveError(err)\n        atags = None\n        otags = None\n        if and_tags:\n            atags = ','.join([str(t) for t in and_tags])\n        if or_tags:\n            otags = ','.join([str(t) for t in or_tags])\n        return connection.ItemResultSet('find_videos_by_tags',\n            Video, _connection, page_size, page_number, sort_by, sort_order,\n            and_tags=atags, or_tags=otags)", "response": "List videos given a certain set of tags."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists videos that match the text in title or description.", "response": "def find_by_text(text, _connection=None, page_size=100, page_number=0,\n        sort_by=enums.DEFAULT_SORT_BY, sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List videos that match the ``text`` in title or description.\n        \"\"\"\n        return connection.ItemResultSet('find_videos_by_text',\n            Video, _connection, page_size, page_number, sort_by, sort_order,\n            text=text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all videos for a given campaign.", "response": "def find_by_campaign(campaign_id, _connection=None, page_size=100,\n        page_number=0, sort_by=enums.DEFAULT_SORT_BY,\n        sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List all videos for a given campaign.\n        \"\"\"\n        return connection.ItemResultSet(\n            'find_videos_by_campaign_id', Video, _connection, page_size,\n            page_number, sort_by, sort_order, campaign_id=campaign_id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all videos uploaded by a certain user.", "response": "def find_by_user(user_id, _connection=None, page_size=100, page_number=0,\n        sort_by=enums.DEFAULT_SORT_BY, sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List all videos uploaded by a certain user.\n        \"\"\"\n        return connection.ItemResultSet('find_videos_by_user_id',\n            Video, _connection, page_size, page_number, sort_by, sort_order,\n            user_id=user_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all videos identified by a list of reference ids.", "response": "def find_by_reference_ids(reference_ids, _connection=None, page_size=100,\n        page_number=0, sort_by=enums.DEFAULT_SORT_BY,\n        sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List all videos identified by a list of reference ids\n        \"\"\"\n        if not isinstance(reference_ids, (list, tuple)):\n            err = \"Video.find_by_reference_ids expects an iterable argument\"\n            raise exceptions.PyBrightcoveError(err)\n        ids = ','.join(reference_ids)\n        return connection.ItemResultSet(\n            'find_videos_by_reference_ids', Video, _connection, page_size,\n            page_number, sort_by, sort_order, reference_ids=ids)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all videos identified by a list of Brightcove video ids.", "response": "def find_by_ids(ids, _connection=None, page_size=100, page_number=0,\n        sort_by=enums.DEFAULT_SORT_BY, sort_order=enums.DEFAULT_SORT_ORDER):\n        \"\"\"\n        List all videos identified by a list of Brightcove video ids\n        \"\"\"\n        if not isinstance(ids, (list, tuple)):\n            err = \"Video.find_by_ids expects an iterable argument\"\n            raise exceptions.PyBrightcoveError(err)\n        ids = ','.join([str(i) for i in ids])\n        return connection.ItemResultSet('find_videos_by_ids',\n            Video, _connection, page_size, page_number, sort_by, sort_order,\n            video_ids=ids)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a GPX file into a GpxModel.", "response": "def read_gpx(xml, gpxns=None):\n    \"\"\"Parse a GPX file into a GpxModel.\n\n    Args:\n        xml: A file-like-object opened in binary mode - that is containing\n             bytes rather than characters. The root element of the XML should\n             be a <gpx> element containing a version attribute. GPX versions\n             1.1 is supported.\n\n        gpxns: The XML namespace for GPX in Clarke notation (i.e. delimited\n             by curly braces). If None, (the default) the namespace used in\n             the document will be determined automatically.\n    \"\"\"\n    tree = etree.parse(xml)\n    gpx_element = tree.getroot()\n    return parse_gpx(gpx_element, gpxns=gpxns)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a GPX file into a GpxModel.", "response": "def parse_gpx(gpx_element, gpxns=None):\n    \"\"\"Parse a GPX file into a GpxModel.\n\n    Args:\n        xml: A file-like-object opened in binary mode - that is containing\n             bytes rather than characters. The root element of the XML should\n             be a <gpx> element containing a version attribute. GPX versions\n             1.0 is supported.\n\n    Returns:\n        A GpxModel representing the data from the supplies xml.\n\n    Raises:\n        ValueError: The supplied XML could not be parsed as GPX.\n    \"\"\"\n    gpxns = gpxns if gpxns is not None else determine_gpx_namespace(gpx_element)\n\n    if gpx_element.tag != gpxns+'gpx':\n        raise ValueError(\"No gpx root element\")\n\n    get_text = lambda tag: optional_text(gpx_element, gpxns+tag)\n\n    version = gpx_element.attrib['version']\n\n    if not version.startswith('1.0'):\n        raise ValueError(\"Not a GPX 1.0 file\")\n\n    creator = gpx_element.attrib['creator']\n\n    name = get_text('name')\n    description = get_text('desc')\n\n    author_name = get_text('author')\n    email = get_text('email')\n    author = Person(author_name, email)\n\n    url = get_text('url')\n    urlname = get_text('urlname')\n    links = make_links(url, urlname)\n\n    time = get_text('time')\n    keywords = get_text('keywords')\n\n    bounds_element = gpx_element.find(gpxns+'bounds')\n    bounds = nullable(parse_bounds)(bounds_element)\n\n    metadata = Metadata(name=name, description=description, author=author,\n               links=links, time=time, keywords=keywords, bounds=bounds)\n\n    waypoint_elements = gpx_element.findall(gpxns+'wpt')\n    waypoints = [parse_waypoint(waypoint_element, gpxns) for waypoint_element in waypoint_elements]\n\n    route_elements = gpx_element.findall(gpxns+'rte')\n    routes = [parse_route(route_element, gpxns) for route_element in route_elements]\n\n    track_elements = gpx_element.findall(gpxns+'trk')\n    tracks = [parse_track(track_element, gpxns) for track_element in track_elements]\n\n    # TODO : Private elements\n\n    gpx_model  = GpxModel(creator, metadata, waypoints, routes, tracks)\n\n    return gpx_model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a log handler to the given log.", "response": "def add_log_handler(log, handler=None, debug=None, fmt=None):\n    \"\"\"\u4e3a\u4e00\u4e2a :class:`logging.Logger` \u7684\u5b9e\u4f8b\u589e\u52a0 handler\u3002\n\n    :param Logger log: \u9700\u8981\u5904\u7406\u7684 :class:`logging.Logger` \u7684\u5b9e\u4f8b\u3002\n    :param Handler handler: \u4e00\u4e2a :class:`logging.Handler` \u7684\u5b9e\u4f8b\u3002\n    :param int debug: Debug \u7ea7\u522b\u3002\n    :param str fmt: Handler \u7684 Formatter\u3002\n\n    \"\"\"\n    if debug:\n        log.setLevel(debug)\n    if handler:\n        # if not fmt:\n        #     fmt = __LOG_FMT\n        if fmt:\n            handler.setFormatter(fmt)\n        log.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __wrap(self, func):\n        '''This decorator overrides the default arguments of a function.\n\n        For each keyword argument in the function, the decorator first checks\n        if the argument has been overridden by the caller, and uses that value instead if so.\n\n        If not, the decorator consults the Preset object for an override value.\n\n        If both of the above cases fail, the decorator reverts to the function's native\n        default parameter value.\n        '''\n\n        def deffunc(*args, **kwargs):\n            '''The decorated function'''\n            # Get the list of function arguments\n            if hasattr(inspect, 'signature'):\n                # Python 3.5\n                function_args = inspect.signature(func).parameters\n\n            else:\n                function_args = inspect.getargspec(func).args\n\n            # Construct a dict of those kwargs which appear in the function\n            filtered_kwargs = kwargs.copy()\n\n            # look at all relevant keyword arguments for this function\n            for param in function_args:\n\n                if param in kwargs:\n                    # Did the user override the default?\n                    filtered_kwargs[param] = kwargs[param]\n\n                elif param in self._defaults:\n                    # Do we have a clobbering value in the default dict?\n                    filtered_kwargs[param] = self._defaults[param]\n\n            # Call the function with the supplied args and the filtered kwarg dict\n            return func(*args, **filtered_kwargs)  # pylint: disable=W0142\n\n        wrapped = functools.update_wrapper(deffunc, func)\n\n        # force-mangle the docstring here\n        wrapped.__doc__ = ('WARNING: this function has been modified by the Presets '\n                           'package.\\nDefault parameter values described in the '\n                           'documentation below may be inaccurate.\\n\\n{}'.format(wrapped.__doc__))\n        return wrapped", "response": "Decorator to wrap a function to return a new object with the default arguments of a function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting from dice roll structure to a single integer result", "response": "def _sumDiceRolls(self, rollList):\n        \"\"\"convert from dice roll structure to a single integer result\"\"\"\n        if isinstance(rollList, RollList):\n            self.rolls.append(rollList)\n            return rollList.sum()\n        else:\n            return rollList"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn all capturing groups in a regular expression pattern into non - capturing groups.", "response": "def _re_flatten(p):\n    ''' Turn all capturing groups in a regular expression pattern into\n        non-capturing groups. '''\n    if '(' not in p: return p\n    return re.sub(r'(\\\\*)(\\(\\?P<[^>]*>|\\((?!\\?))',\n        lambda m: m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:', p)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cookies(self):\n        cookies = SimpleCookie(self.environ.get('HTTP_COOKIE','')).values()\n        if len(cookies) > self.MAX_PARAMS:\n            raise HTTPError(413, 'Too many cookies')\n        return FormsDict((c.key, c.value) for c in cookies)", "response": "Returns a FormsDict containing all cookies parsed into a FormsDict. Signed cookies are NOT\n            decoded."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query(self):\n        ''' The :attr:`query_string` parsed into a :class:`FormsDict`. These\n            values are sometimes called \"URL arguments\" or \"GET parameters\", but\n            not to be confused with \"URL wildcards\" as they are provided by the\n            :class:`Router`. '''\n        get = self.environ['bottle.get'] = FormsDict()\n        pairs = _parse_qsl(self.environ.get('QUERY_STRING', ''))\n        if len(pairs) > self.MAX_PARAMS:\n            raise HTTPError(413, 'Too many parameters')\n        for key, value in pairs:\n            get[key] = value\n        return get", "response": "The : attr : querystring parsed into a FormsDict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(self):\n        ''' Returns a copy of self. '''\n        # TODO\n        copy = Response()\n        copy.status = self.status\n        copy._headers = dict((k, v[:]) for (k, v) in self._headers.items())\n        return copy", "response": "Returns a copy of self."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a mapping from referents to lists of descriptions.", "response": "def annotated_references(obj):\n    \"\"\"\n    Return known information about references held by the given object.\n\n    Returns a mapping from referents to lists of descriptions.  Note that there\n    may be more than one edge leading to any particular referent; hence the\n    need for a list.  Descriptions are currently strings.\n\n    \"\"\"\n    references = KeyTransformDict(transform=id, default_factory=list)\n    for type_ in type(obj).__mro__:\n        if type_ in type_based_references:\n            type_based_references[type_](obj, references)\n\n    add_attr(obj, \"__dict__\", references)\n    add_attr(obj, \"__class__\", references)\n    if isinstance(obj, type):\n        add_attr(obj, \"__mro__\", references)\n\n    return references"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string to be used for Graphviz nodes.", "response": "def object_annotation(obj):\n    \"\"\"\n    Return a string to be used for Graphviz nodes.  The string\n    should be short but as informative as possible.\n\n    \"\"\"\n    # For basic types, use the repr.\n    if isinstance(obj, BASE_TYPES):\n        return repr(obj)\n    if type(obj).__name__ == 'function':\n        return \"function\\\\n{}\".format(obj.__name__)\n    elif isinstance(obj, types.MethodType):\n        if six.PY2:\n            im_class = obj.im_class\n            if im_class is None:\n                im_class_name = \"<None>\"\n            else:\n                im_class_name = im_class.__name__\n\n            try:\n                func_name = obj.__func__.__name__\n            except AttributeError:\n                func_name = \"<anonymous>\"\n            return \"instancemethod\\\\n{}.{}\".format(\n                im_class_name,\n                func_name,\n            )\n        else:\n            try:\n                func_name = obj.__func__.__qualname__\n            except AttributeError:\n                func_name = \"<anonymous>\"\n            return \"instancemethod\\\\n{}\".format(func_name)\n    elif isinstance(obj, list):\n        return \"list[{}]\".format(len(obj))\n    elif isinstance(obj, tuple):\n        return \"tuple[{}]\".format(len(obj))\n    elif isinstance(obj, dict):\n        return \"dict[{}]\".format(len(obj))\n    elif isinstance(obj, types.ModuleType):\n        return \"module\\\\n{}\".format(obj.__name__)\n    elif isinstance(obj, type):\n        return \"type\\\\n{}\".format(obj.__name__)\n    elif six.PY2 and isinstance(obj, types.InstanceType):\n        return \"instance\\\\n{}\".format(obj.__class__.__name__)\n    elif isinstance(obj, weakref.ref):\n        referent = obj()\n        if referent is None:\n            return \"weakref (dead referent)\"\n        else:\n            return \"weakref to id 0x{:x}\".format(id(referent))\n    elif isinstance(obj, types.FrameType):\n        filename = obj.f_code.co_filename\n        if len(filename) > FRAME_FILENAME_LIMIT:\n            filename = \"...\" + filename[-(FRAME_FILENAME_LIMIT-3):]\n        return \"frame\\\\n{}:{}\".format(\n            filename,\n            obj.f_lineno,\n        )\n    else:\n        return \"object\\\\n{}.{}\".format(\n            type(obj).__module__,\n            type(obj).__name__,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disttar(target, source, env):\n\n        import tarfile\n\n        env_dict = env.Dictionary()\n\n        if env_dict.get(\"DISTTAR_FORMAT\") in [\"gz\", \"bz2\"]:\n                tar_format = env_dict[\"DISTTAR_FORMAT\"]\n        else:\n                tar_format = \"\"\n\n        # split the target directory, filename, and stuffix\n        base_name = str(target[0]).split('.tar')[0]\n        (target_dir, dir_name) = os.path.split(base_name)\n\n        # create the target directory if it does not exist\n        if target_dir and not os.path.exists(target_dir):\n                os.makedirs(target_dir)\n\n        # open our tar file for writing\n        print >> sys.stderr, 'DistTar: Writing %s' % str(target[0])\n        print >> sys.stderr, '  with contents: %s' % [str(s) for s in source]\n        tar = tarfile.open(str(target[0]), \"w:%s\" % tar_format)\n\n        # write sources to our tar file\n        for item in source:\n                item = str(item)\n                sys.stderr.write(\".\")\n                #print \"Adding to TAR file: %s/%s\" % (dir_name,item)\n                tar.add(item,'%s/%s' % (dir_name,item))\n\n        # all done\n        sys.stderr.write(\"\\n\") #print \"Closing TAR file\"\n        tar.close()", "response": "build a tar archive from a list of files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef disttar_suffix(env, sources):\n\n        env_dict = env.Dictionary()\n        if env_dict.has_key(\"DISTTAR_FORMAT\") and env_dict[\"DISTTAR_FORMAT\"] in [\"gz\", \"bz2\"]:\n                return \".tar.\" + env_dict[\"DISTTAR_FORMAT\"]\n        else:\n                return \".tar\"", "response": "Return the disttar archive suffix generator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate(env):\n\n    disttar_action=SCons.Action.Action(disttar, disttar_string)\n    env['BUILDERS']['DistTar'] =  Builder(\n            action=disttar_action\n            , emitter=disttar_emitter\n            , suffix = disttar_suffix\n            , target_factory = env.fs.Entry\n    )\n\n    env.AppendUnique(\n        DISTTAR_FORMAT = 'gz'\n    )", "response": "Generate the DistTar builder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure that the table exists - as per the gludb spec.", "response": "def ensure_table(self, cls):\n        \"\"\"Ensure table's existence - as per the gludb spec.\"\"\"\n        id_len = len(uuid())\n        index_names = cls.index_names() or []\n        cols = [\n            'id char(%d) primary key' % (id_len,),\n            'value jsonb'\n        ] + [\n            name + ' text' for name in index_names\n        ]\n\n        table_name = cls.get_table_name()\n\n        with self._conn() as conn:\n            with conn.cursor() as cur:\n                cur.execute('create table if not exists %s (%s);' % (\n                    table_name,\n                    ','.join(cols)\n                ))\n                for name in index_names:\n                    cur.execute('create index if not exists %s on %s(%s);' % (\n                        table_name + '_' + name + '_idx',\n                        table_name,\n                        name\n                    ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind a single key - as per the gludb spec.", "response": "def find_one(self, cls, id):\n        \"\"\"Find single keyed row - as per the gludb spec.\"\"\"\n        found = self.find_by_index(cls, 'id', id)\n        return found[0] if found else None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_by_index(self, cls, index_name, value):\n        cur = self._conn().cursor()\n\n        # psycopg2 supports using Python formatters for queries\n        # we also request our JSON as a string for the from_data calls\n        query = 'select id, value::text from {0} where {1} = %s;'.format(\n            cls.get_table_name(),\n            index_name\n        )\n\n        found = []\n        with self._conn() as conn:\n            with conn.cursor() as cur:\n                cur.execute(query, (value,))\n                for row in cur:\n                    id, data = str(row[0]).strip(), row[1]\n                    obj = cls.from_data(data)\n                    assert id == obj.id\n                    found.append(obj)\n\n        return found", "response": "Find all rows matching index query - as per the gludb spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, obj):\n        cur = self._conn().cursor()\n\n        tabname = obj.__class__.get_table_name()\n\n        index_names = obj.__class__.index_names() or []\n\n        col_names = ['id', 'value'] + index_names\n        value_holders = ['%s'] * len(col_names)\n        updates = ['%s = EXCLUDED.%s' % (cn, cn) for cn in col_names[1:]]\n\n        if not obj.id:\n            id = uuid()\n            obj.id = id\n\n        query = 'insert into {0} ({1}) values ({2}) on conflict(id) do update set {3};'.format(\n            tabname,\n            ','.join(col_names),\n            ','.join(value_holders),\n            ','.join(updates),\n        )\n\n        values = [obj.id, obj.to_data()]\n\n        index_vals = obj.indexes() or {}\n        values += [index_vals.get(name, 'NULL') for name in index_names]\n\n        with self._conn() as conn:\n            with conn.cursor() as cur:\n                cur.execute(query, tuple(values))", "response": "Save the current instance of the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform an authorized GET request to the url and return the content.", "response": "def authenticated_get(username, password, url, verify=True):\n    \"\"\"\n    Perform an authorized query to the url, and return the result\n    \"\"\"\n    try:\n        response = requests.get(url, auth=(username, password), verify=verify)\n        if response.status_code == 401:\n            raise BadCredentialsException(\n                \"Unable to authenticate user %s to %s with password provided!\"\n                % (username, url))\n    except requests.exceptions.SSLError:\n        raise CertificateException(\"Unable to verify certificate at %s!\" % url)\n    return response.content"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a cleaned requests request", "response": "def cleaned_request(request_type, *args, **kwargs):\n    \"\"\" Perform a cleaned requests request \"\"\"\n    s = requests.Session()\n    # this removes netrc checking\n    s.trust_env = False\n    return s.request(request_type, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading a file to a bytesio object with a download bar", "response": "def download_to_bytesio(url):\n    \"\"\" Return a bytesio object with a download bar \"\"\"\n    logger.info(\"Downloading url: {0}\".format(url))\n    r = cleaned_request('get', url, stream=True)\n    stream = io.BytesIO()\n    total_length = int(r.headers.get('content-length'))\n    for chunk in progress.bar(r.iter_content(chunk_size=1024), expected_size=(total_length/1024) + 1):\n        if chunk:\n            stream.write(chunk)\n    stream.seek(0)\n    return stream"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Q object that represents the current message queue.", "response": "def queue(self):\n        \"\"\"Message queue queue.\"\"\"\n        with self.connection_pool.acquire(block=True) as conn:\n            return Q(\n                self.routing_key,\n                exchange=self.exchange,\n                routing_key=self.routing_key\n            )(conn)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists(self):\n        try:\n            queue = self.queue\n            queue.queue_declare(passive=True)\n        except NotFound:\n            return False\n        except ChannelError as e:\n            if e.reply_code == '404':\n                return False\n            raise e\n        return True", "response": "Test if this queue exists in the AMQP store."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef producer(self, conn):\n        return Producer(\n            conn,\n            exchange=self.exchange,\n            routing_key=self.routing_key,\n            auto_declare=True,\n        )", "response": "Get a consumer for a connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef consumer(self, conn):\n        return Consumer(\n            connection=conn,\n            queue=self.queue.name,\n            exchange=self.exchange.name,\n            exchange_type=self.exchange.type,\n            durable=self.exchange.durable,\n            auto_delete=self.exchange.auto_delete,\n            routing_key=self.routing_key,\n            no_ack=self.no_ack,\n        )", "response": "Get a consumer for a connection."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_producer(self):\n        with self.connection_pool.acquire(block=True) as conn:\n            yield self.producer(conn)", "response": "Context manager that yields an instance of Producer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef consume(self, payload=True):\n        with self.create_consumer() as consumer:\n            for msg in consumer.iterqueue():\n                yield msg.payload if payload else msg", "response": "Consume messages from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_initial(self, *args, **kwargs):\n        initial = {}\n        for field in self.fields:\n            value = None\n            if hasattr(self.user, field):\n                value = getattr(self.user, field)\n            if hasattr(self.profile, field):\n                value = getattr(self.profile, field)\n            if value:\n                initial.update({\n                    field: value\n                })\n\n        if hasattr(self.profile, 'dob'):\n            dob = self.profile.dob\n            if dob:\n                if 'dob_day' in self.fields:\n                    initial.update({\n                        'dob_day': dob.day\n                    })\n                if 'dob_month' in self.fields:\n                    initial.update({\n                        'dob_month': dob.month\n                    })\n                if 'dob_year' in self.fields:\n                    initial.update({\n                        'dob_year': dob.year\n                    })\n\n        return initial", "response": "Returns a dictionary of initial values for the user and profile objects that can be used to create a new instance of the form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save(self, *args, **kwargs):\n        for key, value in self.cleaned_data.items():\n            if value != None:\n                if hasattr(self.user, key):\n                    setattr(self.user, key, value)\n                if hasattr(self.profile, key):\n                    setattr(self.profile, key, value)\n\n        # set password\n        if 'password1' in self.cleaned_data:\n            if self.cleaned_data['password1']:\n                self.user.set_password(self.cleaned_data['password1'])\n\n        # set dob\n        if 'dob_day' in self.cleaned_data and 'dob_month' in self.\\\n                cleaned_data and 'dob_year' in self.cleaned_data:\n            self.profile.dob = self._gen_dob()\n\n        self.user.save()\n        self.profile.save()", "response": "This method saves the relevant fields to the user and profile models."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_username(self):\n        user = None\n        try:\n            user = User.objects.get(username__iexact=self.\\\n                    cleaned_data['username'])\n        except User.DoesNotExist:\n            return self.cleaned_data['username']\n\n        if user:\n            if user.username == self.user.username:\n                return self.cleaned_data['username']\n\n        raise forms.ValidationError(_(\\\n                \"A user with that username already exists.\"))", "response": "Validate that the username is alphanumeric and not already in use. Don t fail if the username is already provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies that the values entered into the two password fields are the same.", "response": "def clean(self):\n        \"\"\"\n        Verifiy that the values entered into the two password fields\n        match. Note that an error here will end up in\n        ``non_field_errors()`` because it doesn't apply to a single\n        field.\n        \"\"\"\n        if 'dob_day' in self.cleaned_data and 'dob_month' in \\\n                self.cleaned_data and 'dob_year' in self.cleaned_data:\n                try:\n                    self._gen_dob()\n                except ValueError:\n                    self._errors['dob_day'] = (_(\\\n                            \"You provided an invalid date.\"),)\n\n        if 'password1' in self.cleaned_data and 'password2' in \\\n                self.cleaned_data:\n            if self.cleaned_data['password1'] != \\\n                    self.cleaned_data['password2']:\n                raise forms.ValidationError(_(\\\n                        \"The two password fields didn't match.\"))\n\n        return self.cleaned_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(self,message,message_type):\n        if message_type == MULTIPART:\n            raise Exception(\"Unsupported request type\")\n            \n        super(Requestor,self).send(message,message_type)", "response": "Send a message of the given type to the given type"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a command as an admin in the specified cwd and environ.", "response": "def run_as_admin(command, cwd=None, environ=None):\n  \"\"\"\n  Runs a command as an admin in the specified *cwd* and *environ*.\n  On Windows, this creates a temporary directory where this information\n  is stored temporarily so that the new process can launch the proper\n  subprocess.\n  \"\"\"\n\n  if isinstance(command, str):\n    command = shlex.split(command)\n\n  if os.name == 'nt':\n    return _run_as_admin_windows(command, cwd, environ)\n\n  elif os.name == 'posix':\n    command = ['sudo', '-E'] + list(command)\n    sys.exit(subprocess.call(command))\n\n  else:\n    raise RuntimeError('Unsupported os: {!r}'.format(os.name))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_body_part(self, key, data, mime_type, size=None):\n    if isinstance(data, str):\n      size = len(data)\n    if hasattr(data, \"fileno\"):\n      size = os.fstat(data.fileno())[stat.ST_SIZE]\n    if size is None:\n      # TODO: support chunked transfer if some of the body is of unknown size.\n      raise UnknownSize('Each part of the body must have a known size.')\n    if 'Content-Length' in self.headers:\n      content_length = int(self.headers['Content-Length'])\n    else:\n      content_length = 0\n    # If this is the first part added to the body, then this is not a multipart\n    # request.\n    boundary_string = '\\r\\n--%s\\r\\n' % (MIME_BOUNDARY,)\n    self._body_parts.append(boundary_string)\n    content_length += len(boundary_string) + size\n    # Include the mime type of this part.\n    cd = 'Content-Disposition: form-data; name=\"%s\"' % key\n    mt = mime_type\n    if hasattr(data, \"fileno\"):\n        cd += '; filename=\"%s\"' % data.name.split('/')[-1]\n        mt = mimetypes.guess_type(data.name)[0] or 'application/octet-stream'\n    cd += '\\r\\n'\n    type_string = 'Content-Type: %s\\r\\n\\r\\n' % (mt)\n    self._body_parts.append(cd)\n    self._body_parts.append(type_string)\n    content_length += len(type_string) + len(cd)\n    self._body_parts.append(data)\n    self.headers['Content-Length'] = str(content_length)", "response": "Adds a part of the HTTP request body to the HTTP request body."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _copy(self):\n    copied_uri = Uri(self.uri.scheme, self.uri.host, self.uri.port,\n                     self.uri.path, self.uri.query.copy())\n    new_request = HttpRequest(uri=copied_uri, method=self.method,\n                              headers=self.headers.copy())\n    new_request._body_parts = self._body_parts[:]\n    return new_request", "response": "Creates a deep copy of this request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_relative_path(self):\n    param_string = self._get_query_string()\n    if self.path is None:\n      path = '/'\n    else:\n      path = self.path\n    if param_string:\n      return '?'.join([path, param_string])\n    else:\n      return path", "response": "Returns the path with the query parameters escaped and appended."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the HTTP request components based on the URI.", "response": "def modify_request(self, http_request=None):\n    \"\"\"Sets HTTP request components based on the URI.\"\"\"\n    if http_request is None:\n      http_request = HttpRequest()\n    if http_request.uri is None:\n      http_request.uri = Uri()\n    # Determine the correct scheme.\n    if self.scheme:\n      http_request.uri.scheme = self.scheme\n    if self.port:\n      http_request.uri.port = self.port\n    if self.host:\n      http_request.uri.host = self.host\n    # Set the relative uri path\n    if self.path:\n      http_request.uri.path = self.path\n    if self.query:\n      http_request.uri.query = self.query.copy()\n    return http_request"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a URI string into a Uri object which corresponds to the URI string.", "response": "def parse_uri(uri_string):\n    \"\"\"Creates a Uri object which corresponds to the URI string.\n \n    This method can accept partial URIs, but it will leave missing\n    members of the Uri unset.\n    \"\"\"\n    parts = urlparse.urlparse(uri_string)\n    uri = Uri()\n    if parts[0]:\n      uri.scheme = parts[0]\n    if parts[1]:\n      host_parts = parts[1].split(':')\n      if host_parts[0]:\n        uri.host = host_parts[0]\n      if len(host_parts) > 1:\n        uri.port = int(host_parts[1])\n    if parts[2]:\n      uri.path = parts[2]\n    if parts[4]:\n      param_pairs = parts[4].split('&')\n      for pair in param_pairs:\n        pair_parts = pair.split('=')\n        if len(pair_parts) > 1:\n          uri.query[urllib.unquote_plus(pair_parts[0])] = (\n              urllib.unquote_plus(pair_parts[1]))\n        elif len(pair_parts) == 1:\n          uri.query[urllib.unquote_plus(pair_parts[0])] = None\n    return uri"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nopen a socket connection to the server to set up an HTTP request.", "response": "def _get_connection(self, uri, headers=None):\n    \"\"\"Opens a socket connection to the server to set up an HTTP request.\n    \n    Args:\n      uri: The full URL for the request as a Uri object.\n      headers: A dict of string pairs containing the HTTP headers for the\n          request.\n    \"\"\"\n    connection = None\n    if uri.scheme == 'https':\n      if not uri.port:\n        connection = httplib.HTTPSConnection(uri.host)\n      else:\n        connection = httplib.HTTPSConnection(uri.host, int(uri.port))\n    else:\n      if not uri.port:\n        connection = httplib.HTTPConnection(uri.host)\n      else:\n        connection = httplib.HTTPConnection(uri.host, int(uri.port))\n    return connection"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _http_request(self, method, uri, headers=None, body_parts=None):\n    if isinstance(uri, (str, unicode)):\n      uri = Uri.parse_uri(uri)\n    connection = self._get_connection(uri, headers=headers)\n    if self.debug:\n      connection.debuglevel = 1\n\n    if connection.host != uri.host:\n      connection.putrequest(method, str(uri))\n    else:\n      connection.putrequest(method, uri._get_relative_path())\n\n    # Overcome a bug in Python 2.4 and 2.5\n    # httplib.HTTPConnection.putrequest adding\n    # HTTP request header 'Host: www.google.com:443' instead of\n    # 'Host: www.google.com', and thus resulting the error message\n    # 'Token invalid - AuthSub token has wrong scope' in the HTTP response.\n    if (uri.scheme == 'https' and int(uri.port or 443) == 443 and\n        hasattr(connection, '_buffer') and\n        isinstance(connection._buffer, list)):\n      header_line = 'Host: %s:443' % uri.host\n      replacement_header_line = 'Host: %s' % uri.host\n      try:\n        connection._buffer[connection._buffer.index(header_line)] = (\n            replacement_header_line)\n      except ValueError:  # header_line missing from connection._buffer\n        pass\n\n    # Send the HTTP headers.\n    for header_name, value in headers.iteritems():\n      connection.putheader(header_name, value)\n    connection.endheaders()\n\n    # If there is data, send it in the request.\n    if body_parts:\n      for part in body_parts:\n        _send_data_part(part, connection)\n\n    # Return the HTTP Response from the server.\n    return connection.getresponse()", "response": "Makes an HTTP request using httplib. HTTPConnection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving the cursor of the Scanner to or by offset.", "response": "def seek(self, offset, mode='set', renew=False):\n    \"\"\"\n    Moves the cursor of the Scanner to or by *offset* depending on the *mode*.\n    Is is similar to a file's `seek()` function, however the *mode* parameter\n    also accepts the string-mode values `'set'`, `'cur'` and `'end'`.\n\n    Note that even for the `'end'` mode, the *offset* must be negative to\n    actually reach back up from the end of the file.\n\n    If *renew* is set to True, the line and column counting will always begin\n    from the start of the file. Keep in mind that this could can be very slow\n    because it has to go through each and every character until the desired\n    position is reached.\n\n    Otherwise, if *renew* is set to False, it will be decided if counting from\n    the start is shorter than counting from the current cursor position.\n    \"\"\"\n\n    mapping = {os.SEEK_SET: 'set', os.SEEK_CUR: 'cur', os.SEEK_END: 'end'}\n    mode = mapping.get(mode, mode)\n    if mode not in ('set', 'cur', 'end'):\n      raise ValueError('invalid mode: \"{}\"'.format(mode))\n\n    # Translate the other modes into the 'set' mode.\n    if mode == 'end':\n      offset = len(self.text) + offset\n      mode = 'set'\n    elif mode == 'cur':\n      offset = self.index + offset\n      mode = 'set'\n\n    assert mode == 'set'\n    if offset < 0:\n      offset = 0\n    elif offset > len(self.text):\n      offset = len(self.text) + 1\n\n    if self.index == offset:\n      return\n\n    # Figure which path is shorter:\n    # 1) Start counting from the beginning of the file,\n    if offset <= abs(self.index - offset):\n      text, index, lineno, colno = self.text, 0, 1, 0\n      while index != offset:\n        # Find the next newline in the string.\n        nli = text.find('\\n', index)\n        if nli >= offset or nli < 0:\n          colno = offset - index\n          index = offset\n          break\n        else:\n          colno = 0\n          lineno += 1\n          index = nli + 1\n\n    # 2) or step from the current position of the cursor.\n    else:\n      text, index, lineno, colno = self.text, self.index, self.lineno, self.colno\n\n      if offset < index:  # backwards\n        while index != offset:\n          nli = text.rfind('\\n', 0, index)\n          if nli < 0 or nli <= offset:\n            if text[offset] == '\\n':\n              assert (offset - nli) == 0, (offset, nli)\n              nli = text.rfind('\\n', 0, index-1)\n              lineno -= 1\n            colno = offset - nli - 1\n            index = offset\n            break\n          else:\n            lineno -= 1\n            index = nli - 1\n      else:  # forwards\n        while index != offset:\n          nli = text.find('\\n', index)\n          if nli < 0 or nli >= offset:\n            colno = offset - index\n            index = offset\n          else:\n            lineno += 1\n            index = nli + 1\n\n    assert lineno >= 1\n    assert colno >= 0\n    assert index == offset\n    self.index, self.lineno, self.colno = index, lineno, colno"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef next(self):\n    \" Move on to the next character in the text. \"\n\n    char = self.char\n    if char == '\\n':\n      self.lineno += 1\n      self.colno = 0\n    else:\n      self.colno += 1\n    self.index += 1\n    return self.char", "response": "Move on to the next character in the text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a full line from the scanner and returns it.", "response": "def readline(self):\n    \" Reads a full line from the scanner and returns it. \"\n\n    start = end = self.index\n    while end < len(self.text):\n      if self.text[end] == '\\n':\n        end += 1\n        break\n      end += 1\n    result = self.text[start:end]\n    self.index = end\n    if result.endswith('\\n'):\n      self.colno = 0\n      self.lineno += 1\n    else:\n      self.colno += end - start\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match(self, regex, flags=0):\n\n    if isinstance(regex, str):\n      regex = re.compile(regex, flags)\n    match = regex.match(self.text, self.index)\n    if not match:\n      return None\n    start, end = match.start(), match.end()\n    lines = self.text.count('\\n', start, end)\n    self.index = end\n    if lines:\n      self.colno = end - self.text.rfind('\\n', start, end) - 1\n      self.lineno += lines\n    else:\n      self.colno += end - start\n    return match", "response": "Matches the specified regex from the current character of the scanner and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the group that matches the given regex.", "response": "def getmatch(self, regex, group=0, flags=0):\n    \"\"\"\n    The same as #Scanner.match(), but returns the captured group rather than\n    the regex match object, or None if the pattern didn't match.\n    \"\"\"\n\n    match = self.match(regex, flags)\n    if match:\n      return match.group(group)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef restore(self, cursor):\n    \" Moves the scanner back (or forward) to the specified cursor location. \"\n\n    if not isinstance(cursor, Cursor):\n      raise TypeError('expected Cursor object', type(cursor))\n    self.index, self.lineno, self.colno = cursor", "response": "Moves the scanner back to the specified cursor location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n\n    self.rules_map = {}\n    self.skippable_rules = []\n    for rule in self.rules:\n      if not isinstance(rule, Rule):\n        raise TypeError('item must be Rule instance', type(rule))\n      self.rules_map.setdefault(rule.name, []).append(rule)\n      if rule.skip:\n        self.skippable_rules.append(rule)", "response": "Updates the internal internal"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the current token type name matches with any of the specified names*.", "response": "def expect(self, *names):\n    \"\"\"\n    Checks if the current #token#s type name matches with any of the specified\n    *names*. This is useful for asserting multiple valid token types at a\n    specific point in the parsing process.\n\n    # Arguments\n    names (str): One or more token type names. If zero are passed,\n      nothing happens.\n\n    # Raises\n    UnexpectedTokenError: If the current #token#s type name does not match\n      with any of the specified *names*.\n    \"\"\"\n\n    if not names:\n      return\n    if not self.token or self.token.type not in names:\n      raise UnexpectedTokenError(names, self.token)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept(self, *names, **kwargs):\n\n    return self.next(*names, as_accept=True, **kwargs)", "response": "Returns a new token that is accepted by the specified rules."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the next token from the input and returns it.", "response": "def next(self, *expectation, **kwargs):\n    \"\"\"\n    Parses the next token from the input and returns it. The new token can be\n    accessed from the #token attribute after the method was called.\n\n    If one or more arguments are specified, they must be rule names that are to\n    be expected at the current position. They will be attempted to be matched\n    first (in the specicied order). If the expectation could not be met, an\n    #UnexpectedTokenError is raised.\n\n    An expected Token will not be skipped, even if its rule defines it so.\n\n    # Arguments\n    expectation (str): The name of one or more rules that are expected from the\n      current position of the parser. If empty, the first matching token of ALL\n      rules will be returned. In this case, skippable tokens will be skipped.\n    as_accept (bool): If passed True, this method behaves the same as the\n      #accept() method. The default value is #False.\n    weighted (bool): If passed True, the tokens specified with *expectations*\n      are checked first, effectively giving them a higher priority than other\n      they would have from the order in the #rules list. The default value is\n      #False.\n\n    # Raises\n    ValueError: if an expectation doesn't match with a rule name.\n    UnexpectedTokenError: Ff an expectation is given and the expectation\n      wasn't fulfilled. Only when *as_accept* is set to #False.\n    TokenizationError: if a token could not be generated from the current\n      position of the Scanner.\n    \"\"\"\n\n    as_accept = kwargs.pop('as_accept', False)\n    weighted = kwargs.pop('weighted', False)\n    for key in kwargs:\n      raise TypeError('unexpected keyword argument {0!r}'.format(key))\n\n    if self.token and self.token.type == eof:\n      if not as_accept and expectation and eof not in expectation:\n        raise UnexpectedTokenError(expectation, self.token)\n      elif as_accept and eof in expectation:\n        return self.token\n      elif as_accept:\n        return None\n      return self.token\n\n    token = None\n    while token is None:\n      # Stop if we reached the end of the input.\n      cursor = self.scanner.cursor\n      if not self.scanner:\n        token = Token(eof, cursor, None, None)\n        break\n\n      value = None\n\n      # Try to match the expected tokens.\n      if weighted:\n        for rule_name in expectation:\n          if rule_name == eof:\n            continue\n          rules = self.rules_map.get(rule_name)\n          if rules is None:\n            raise ValueError('unknown rule', rule_name)\n          for rule in rules:\n            value = rule.tokenize(self.scanner)\n            if value:\n              break\n          if value:\n            break\n          self.scanner.restore(cursor)\n\n      # Match the rest of the rules, but only if we're not acting\n      # like the accept() method that doesn't need the next token\n      # for raising an UnexpectedTokenError.\n      if not value:\n        if as_accept and weighted:\n          # Check only skippable rules if we're only trying to accept\n          # a certain token type and may consume any skippable tokens\n          # until then.\n          check_rules = self.skippable_rules\n        else:\n          check_rules = self.rules\n        for rule in check_rules:\n          if weighted and expectation and rule.name in expectation:\n            # Skip rules that we already tried.\n            continue\n          value = rule.tokenize(self.scanner)\n          if value:\n            break\n          self.scanner.restore(cursor)\n\n      if not value:\n        if as_accept:\n          return None\n        token = Token(None, cursor, self.scanner.char, None)\n      else:\n        assert rule, \"we should have a rule by now\"\n        if type(value) is not Token:\n          if isinstance(value, tuple):\n            value, string_repr = value\n          else:\n            string_repr = None\n          value = Token(rule.name, cursor, value, string_repr)\n        token = value\n\n        expected = rule.name in expectation\n        if not expected and rule.skip:\n          # If we didn't expect this rule to match, and if its skippable,\n          # just skip it. :-)\n          token = None\n        elif not expected and as_accept:\n          # If we didn't expect this rule to match but are just accepting\n          # instead of expecting, restore to the original location and stop.\n          self.scanner.restore(cursor)\n          return None\n\n    self.token = token\n    if as_accept and token and token.type == eof:\n      if eof in expectation:\n        return token\n      return None\n\n    if token.type is None:\n      raise TokenizationError(token)\n    if not as_accept and expectation and token.type not in expectation:\n      raise UnexpectedTokenError(expectation, token)\n    assert not as_accept or (token and token.type in expectation)\n    return token"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new event to the list of available hooks.", "response": "def append(self, event, help=\"\"):\n        \"\"\"Creates a new event. `event` may be iterable or string\n\n        Args:\n            event (str): Name of event to declare\n\n        Kwrgs:\n            help (str): Help string for the event\n\n        Raises:\n            TypeError\n\n        **Please** describe the event and its calling arguments in the help\n        string.\n        \"\"\"\n        if isinstance(event, str):\n            self._events[event] = HookList(is_waterfall=self.is_waterfall)\n            self._help[event] = (help, getframeinfo(stack()[1][0]))\n\n            if not help:\n                logger.warning(\"Great, don't say anything about your hooks and \\\n                wait for plugin creators to figure it out.\")\n        elif isinstance(event, Iterable):\n            # Depricated. It does not give the ability to give help string\n            # TODO: Remove this\n            for name in event:\n                self.append(name)\n        else:\n            raise TypeError(\"Invalid event name!\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hook(self, function, event, dependencies):\n        # Hooks all events (recursively)\n        if event is None:\n            for e in self._events.keys():\n                self.hook(function, e, dependencies)\n            return\n\n        # Hook multiple, but specific events (recursively)\n        if not isinstance(event, str) and isinstance(event, Iterable):\n            for e in event:\n                self.hook(function, e, dependencies)\n            return\n\n        # Hook a simple event\n        event_list = self._events.get(event, None)\n        if event_list is None:\n            raise NameError(\n                \"Invalid key provided '%s'. Valid options: %s\"\n                % (event, \", \".join(self._events.keys()))\n            )\n            return\n\n        return event_list.hook(function, dependencies)", "response": "Tries to load the hook to the event\n            and returns the unique ID that the function should be called when the event is called."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call(path, *args, encoding=\"utf-8\", show_command=False):\n    returncode = 0\n    output = None\n    try:\n        # 2015-10-10 zrong\n        # \u5728 windows \u4e0a\u4f7f\u7528 universal_newlines=True\n        # \u4f1a\u5bfc\u81f4\u8f93\u51fa\u4fe1\u606f\u4e3a\u4e2d\u6587\u65f6\u51fa\u73b0\u7f16\u7801\u9519\u8bef\n        # \u539f\u56e0\u662f check_out \u4e2d\u8bfb\u5165 stdout \u5185\u5bb9\u7684 read \u65b9\u6cd5\u6ca1\u6709\u4f20\u9012\u7f16\u7801\u53c2\u6570\n        # \u56e0\u6b64\u4e0d\u518d\u4f7f\u7528 universal_newlines=True \u8fd9\u4e2a\u53c2\u6570\n        # \u800c\u6539\u7528\u76f4\u63a5\u8fd4\u56de bytes\uff0c\u7136\u540e\u5bf9\u5176\u89e3\u7801\n        arg_list = get_args(path, *args)\n        if show_command:\n            print('git call args:', arg_list)\n        output = subprocess.check_output(arg_list, \n                stderr=subprocess.STDOUT)\n        output = output.decode(encoding=encoding)\n    except subprocess.CalledProcessError as err:\n        returncode = err.returncode\n        output = err.output.decode(encoding=encoding)\n    return returncode, output", "response": "git call \u8c03\u7528 subprocess. check_output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget branches from git repository", "response": "def get_branches(path):\n    \"\"\"\u83b7\u53d6\u5f53\u524d\u6240\u6709\u5206\u652f\u540d\u79f0\u7684\u5217\u8868\u3002\n\n    :param str path: git \u4ed3\u5e93\u6587\u4ef6\u5939\u8def\u5f84\u3002\n    :return: \u5206\u652f\u540d\u79f0\u5217\u8868\u3002\u5f53\u524d\u5206\u652f\u4f4d\u4e8e\u5217\u8868\u7b2c\u4e00\u9879\u3002\n    :rtype: list\n\n    \"\"\"\n    code, output = call(path, 'branch', '--list')\n    if code > 0:\n        return None\n    branches = output.split('\\n')\n    newbr = [None]\n    for br in branches:\n        if br:\n            if br[0] == '*':\n                newbr[0] = br[2:]\n            else:\n                newbr.append(br[2:])\n    return newbr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncloning \u4e00\u4e2a git \u4ed3 \u7684 url", "response": "def clone(giturl, gitpath):\n    \"\"\"clone \u4e00\u4e2a git \u5e93\u3002\n\n    :param str giturl: git \u4ed3\u5e93\u7684 url \u5730\u5740\u3002\n    :param str gitpath: git \u4ed3\u5e93\u4fdd\u5b58\u8def\u5f84\u3002\n\n    \"\"\"\n    gitArgs = ['git', 'clone', giturl, gitpath]\n    slog.info(' '.join(gitArgs))\n    return subprocess.call(gitArgs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the sha1 of the current git revision.", "response": "def get_hash(path, cut=0):\n    \"\"\"\u83b7\u53d6\u53ef\u88ab git \u7684 HEAD \u7684 sha1 \u503c\u3002\n\n    :param str path: git \u4ed3\u5e93\u6587\u4ef6\u5939\u8def\u5f84\u3002\n    :param int cut: \u5305\u542b\u7684 sha1 \u503c\u7684\u957f\u5ea6\u30020\u4ee3\u8868\u4e0d\u526a\u5207\u3002\n    :returns: \u526a\u5207\u8fc7\u7684 sha1 \u7684\u503c\u3002\n    :rtype: str\n\n    \"\"\"\n    code, output = call(path, 'rev-parse', 'HEAD')\n    if code > 0:\n        return None\n    # maybe the string is with a linebreak.\n    sha1 = output.strip()\n    if cut > 0:\n        sha1 = sha1[:7]\n    return sha1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the submodules of a git archive", "response": "def update_submodules(path, init=True, update=True):\n    \"\"\"\u66f4\u65b0\u5b50\u6a21\u5757\u3002\n\n    :param str path: git \u4ed3\u5e93\u6587\u4ef6\u5939\u8def\u5f84\u3002\n    :param bool init: \u662f\u5426\u521d\u59cb\u5316\u5b50\u6a21\u5757\u3002\n    :param bool update: \u662f\u5426\u66f4\u65b0\u5b50\u6a21\u5757\u3002\n\n    \"\"\"\n    succ = None\n    if init:\n        arg = get_args(path, 'submodule', 'init', work_tree=False)\n        slog.info(' '.join(arg))\n        succ = subprocess.call(arg)\n        if succ>0:\n            slog.error('git execute error!')\n            return succ\n    if update:\n        arg = get_args(path, \"submodule\", \"update\", work_tree=False)\n        slog.info(' '.join(arg))\n        succ = subprocess.call(arg)\n        if succ>0:\n            slog.error('git execute error!')\n            return succ\n    return succ"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_message(self, message, verbosity_needed=1):\n        if self.args.verbosity >= verbosity_needed:\n            print(message)", "response": "Prints the message to stdout if verbosity is high enough."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the error message and exits with the given code.", "response": "def error(self, message, code=1):\n        \"\"\" Prints the error, and exits with the given code. \"\"\"\n        sys.stderr.write(message)\n        sys.exit(code)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_db_settings(self, settings):\n        if settings == 'DJANGO_SETTINGS_MODULE':\n            django_settings = os.environ.get('DJANGO_SETTINGS_MODULE')\n            self.print_message(\"Getting settings file from DJANGO_SETTINGS_MODULE=%s\"\n                               % django_settings)\n            path_pieces = django_settings.split('.')\n            path_pieces[-1] = '%s.py' % path_pieces[-1]\n            settings = os.path.join(*path_pieces)\n\n        self.print_message(\"Parsing settings from settings file '%s'\" % settings)\n        parser = DatabaseSettingsParser()\n        with open(settings) as settings_file:\n            settings_ast = ast.parse(settings_file.read())\n            parser.visit(settings_ast)\n\n        try:\n            return parser.database_settings['default']\n        except KeyError as e:\n            self.error(\"Missing key or value for: %s\\nSettings must be of the form: %s\"\n                       % (e, self.settings_format))", "response": "Parse out database settings from file or DJANGO_SETTINGS_MODULE."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialize_db_args(self, settings, db_key):\n        self.print_message(\"Initializing database settings for %s\" % db_key, verbosity_needed=2)\n\n        db_member = self.databases[db_key]\n\n        db_name = settings.get('NAME')\n        if db_name and not db_member['name']:\n            db_member['name'] = db_name\n\n        db_member['password'] = settings.get('PASSWORD')\n\n        args = []\n        for key in ['USER', 'HOST', 'PORT']:\n            value = settings.get(key)\n            if value:\n                self.print_message(\"Adding parameter %s\" % key.lower, verbosity_needed=2)\n                args.append('--%s=%s' % (key.lower(), value))\n\n        db_member['args'] = args", "response": "Initialize connection arguments for postgres commands."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download_file(self, url, filename):\n        self.print_message(\"Downloading to file '%s' from URL '%s'\" % (filename, url))\n        try:\n            db_file = urllib2.urlopen(url)\n            with open(filename, 'wb') as output:\n                output.write(db_file.read())\n            db_file.close()\n        except Exception as e:\n            self.error(str(e))\n        self.print_message(\"File downloaded\")", "response": "Download file from url to filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unzip_file_if_necessary(self, source_file):\n        if source_file.endswith(\".gz\"):\n            self.print_message(\"Decompressing '%s'\" % source_file)\n            subprocess.check_call([\"gunzip\", \"--force\", source_file])\n            source_file = source_file[:-len(\".gz\")]\n        return source_file", "response": "Unzip file if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_file_from_url(self, source_app, url):\n        if source_app:\n            source_name = source_app\n        else:\n            source_name = urlparse.urlparse(url).netloc.replace('.', '_')\n\n        filename = self.create_file_name(source_name)\n        self.download_file(url, filename)\n        return filename", "response": "Download file from source app or url and return local filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump_database(self):\n        db_file = self.create_file_name(self.databases['source']['name'])\n        self.print_message(\"Dumping postgres database '%s' to file '%s'\"\n                           % (self.databases['source']['name'], db_file))\n        self.export_pgpassword('source')\n        args = [\n            \"pg_dump\",\n            \"-Fc\",\n            \"--no-acl\",\n            \"--no-owner\",\n            \"--dbname=%s\" % self.databases['source']['name'],\n            \"--file=%s\" % db_file,\n        ]\n        args.extend(self.databases['source']['args'])\n        subprocess.check_call(args)\n        return db_file", "response": "Create dumpfile from postgres database and return filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplacing postgres database with database from specified source.", "response": "def replace_postgres_db(self, file_url):\n        \"\"\" Replace postgres database with database from specified source. \"\"\"\n        self.print_message(\"Replacing postgres database\")\n\n        if file_url:\n            self.print_message(\"Sourcing data from online backup file '%s'\" % file_url)\n            source_file = self.download_file_from_url(self.args.source_app, file_url)\n        elif self.databases['source']['name']:\n            self.print_message(\"Sourcing data from database '%s'\"\n                               % self.databases['source']['name'])\n            source_file = self.dump_database()\n        else:\n            self.print_message(\"Sourcing data from local backup file %s\" % self.args.file)\n            source_file = self.args.file\n\n        self.drop_database()\n        self.create_database()\n\n        source_file = self.unzip_file_if_necessary(source_file)\n\n        self.print_message(\"Importing '%s' into database '%s'\"\n                           % (source_file, self.databases['destination']['name']))\n        args = [\n            \"pg_restore\",\n            \"--no-acl\",\n            \"--no-owner\",\n            \"--dbname=%s\" % self.databases['destination']['name'],\n            source_file,\n        ]\n        args.extend(self.databases['destination']['args'])\n        subprocess.check_call(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_file_url_for_heroku_app(self, source_app):\n        self.print_message(\"Getting backup url for Heroku app '%s'\" % source_app)\n        args = [\n            \"heroku\",\n            \"pg:backups:url\",\n            \"--app=%s\" % source_app,\n        ]\n        if self.args.use_pgbackups:\n            args = [\n                \"heroku\",\n                \"pgbackups:url\",\n                \"--app=%s\" % source_app,\n            ]\n        return subprocess.check_output(args).strip().decode('ascii')", "response": "Get latest backup URL for a Heroku app."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncaptures Heroku database backup.", "response": "def capture_heroku_database(self):\n        \"\"\" Capture Heroku database backup. \"\"\"\n        self.print_message(\"Capturing database backup for app '%s'\" % self.args.source_app)\n        args = [\n            \"heroku\",\n            \"pg:backups:capture\",\n            \"--app=%s\" % self.args.source_app,\n        ]\n        if self.args.use_pgbackups:\n            args = [\n                \"heroku\",\n                \"pgbackups:capture\",\n                \"--app=%s\" % self.args.source_app,\n                \"--expire\",\n            ]\n        subprocess.check_call(args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces Heroku database with database from specified source.", "response": "def replace_heroku_db(self, file_url):\n        \"\"\" Replace Heroku database with database from specified source. \"\"\"\n        self.print_message(\"Replacing database for Heroku app '%s'\" % self.args.destination_app)\n\n        self.reset_heroku_database()\n\n        if file_url:\n            self.print_message(\"Restoring from URL '%s'\" % file_url)\n            args = [\n                \"heroku\",\n                \"pg:backups:restore\",\n                file_url,\n                \"--app=%s\" % self.args.destination_app,\n                \"DATABASE\",\n                \"--confirm\",\n                self.args.destination_app,\n            ]\n            if self.args.use_pgbackups:\n                args = [\n                    \"heroku\",\n                    \"pgbackups:restore\",\n                    \"--app=%s\" % self.args.destination_app,\n                    \"DATABASE_URL\",\n                    \"--confirm\",\n                    self.args.destination_app,\n                    file_url,\n                ]\n            subprocess.check_call(args)\n        else:\n            # TODO perhaps add support for file -> heroku by piping to pg:psql\n            self.print_message(\"Pushing data from database '%s'\" % self.databases['source']['name'])\n            self.print_message(\"NOTE: Any postgres authentication settings you passed to paragres \"\n                               \"will be ignored.\\nIf desired, you can export PG* variables.\\n\"\n                               \"You will be prompted for your psql password.\")\n            args = [\n                \"heroku\",\n                \"pg:push\",\n                self.databases['source']['name'],\n                \"DATABASE_URL\",\n                \"--app=%s\" % self.args.destination_app,\n            ]\n            subprocess.check_call(args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n        self.print_message(\"\\nBeginning database replacement process.\\n\")\n\n        if self.args.source_settings:\n            settings = self.parse_db_settings(self.args.source_settings)\n            self.initialize_db_args(settings, 'source')\n\n        if self.args.settings:\n            settings = self.parse_db_settings(self.args.settings)\n            self.initialize_db_args(settings, 'destination')\n\n        if self.args.capture:\n            self.capture_heroku_database()\n\n        file_url = self.args.url\n        if self.args.source_app:\n            self.print_message(\"Sourcing data from backup for Heroku app '%s'\"\n                               % self.args.source_app)\n            file_url = self.get_file_url_for_heroku_app(self.args.source_app)\n\n        if self.args.destination_app:\n            self.replace_heroku_db(file_url)\n        elif self.databases['destination']['name']:\n            self.replace_postgres_db(file_url)\n\n        self.print_message(\"\\nDone.\\n\\nDon't forget to update the Django Site entry if necessary!\")", "response": "Replace a database with the data from the specified source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_task_modules():\n    top_level_modules = settings.INSTALLED_APPS\n    module_names = []\n    for module in top_level_modules:\n        #Import package\n        mod = import_module(module)\n        #Find all modules in package path\n        for loader, module_name, is_pkg in  pkgutil.walk_packages(mod.__path__):\n            if not module_name.startswith(\"__\"):\n                #If the module is not __init__, add it to the registry\n                submod_name = \"{0}.{1}\".format(module,module_name)\n                module_names.append(submod_name)\n    #Once everything is imported, the metaclass will register them automatically\n    modules = map(import_module, module_names)\n    return modules", "response": "Import all installed apps and add modules to registry\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a given category namespace or name combination in the registry.", "response": "def find_in_registry(category = None, namespace = None, name = None):\n    \"\"\"\n    Find a given category/namespace/name combination in the registry\n    category - string, see utils.inputs.registrycategories\n    namespace - module namespace, see settings.NAMESPACE\n    name - lowercase name of module\n    \"\"\"\n    selected_registry = registry\n    if category is not None:\n        selected_registry = [re for re in selected_registry if re.category==category]\n    if namespace is not None:\n        selected_registry = [re for re in selected_registry if re.namespace==namespace]\n    if name is not None:\n        selected_registry = [re for re in selected_registry if re.name==name]\n    if len(selected_registry)>0:\n        return [sr.cls for sr in selected_registry]\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all services in a specific service or all services in a specific host.", "response": "def list(self, service_rec=None, host_rec=None, hostfilter=None):\n        \"\"\"\n        List a specific service or all services\n\n        :param service_rec: t_services.id\n        :param host_rec: t_hosts.id\n        :param hostfilter: Valid hostfilter or None\n        :return: [(svc.t_services.id, svc.t_services.f_hosts_id, svc.t_hosts.f_ipaddr,\n             svc.t_hosts.f_hostname, svc.t_services.f_proto,\n             svc.t_services.f_number, svc.t_services.f_status, svc.t_services.f_name,\n             svc.t_services.f_banner), ...]\n        \"\"\"\n        return self.send.service_list(service_rec, host_rec, hostfilter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns information about a service.", "response": "def info(self, svc_rec=None, ipaddr=None, proto=None, port=None):\n        \"\"\"\n        Information about a service.\n\n        :param svc_rec: t_services.id\n        :param ipaddr: IP Address\n        :param proto: Protocol (tcp, udp, info)\n        :param port: Port (0-65535)\n        :return: [ service_id, host_id, ipv4, ipv6, hostname, proto, number, status, name, banner ]\n        \"\"\"\n        return self.send.service_info(svc_rec, ipaddr, proto, port)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, ipaddr=None, proto=None, port=None, fields=None):\n        return self.send.service_add(ipaddr, proto, port, fields)", "response": "Add a service to the cache"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a service record in the t_services table", "response": "def delete(self, svc_rec=None, ipaddr=None, proto=None, port=None):\n        \"\"\"\n        Delete a t_services record\n\n        :param svc_rec: t_services.id\n        :param ipaddr: IP Address or t_hosts.id\n        :param proto: Protocol (tcp, udp, info)\n        :param port: Port (0-65535)\n        :return: [True, Response Message]\n        \"\"\"\n        return self.send.service_del(svc_rec, ipaddr, proto, port)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of ports with IPs banners and vulnerabilities", "response": "def report_list(self, service_id=None, service_port=None, hostfilter=None):\n        \"\"\"\n        Returns a list of ports with IPs, banners and vulnerabilities (warning, slow!)\n\n        :param service_id: t_services.id\n        :param service_port: Port (tcp/#, udp/#, info/#)\n        :param hostfilter: Valid hostfilter or None\n        :return: { 'port': [t_hosts.f_ipaddr, t_services.f_banner,\n            (t_vulndata.f_vulnid, t_vulndata.f_title, t_vulndata.f_severity, t_vulndata.f_cvss_score), ...}\n        \"\"\"\n        return self.send.service_report_list(service_id, service_port, hostfilter)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vulns_list(self, service_id=None, service_port=None, hostfilter=None):\n        return self.send.service_vulns_list(service_id, service_port, hostfilter)", "response": "List all vulnerabilities for a service"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef connect(nodes):\n    '''\n    Connect a list of nodes.\n\n    Connected nodes have an ``output`` member which is the following node in\n    the line. The last node's ``output`` is a :class:`Queue` for\n    easy plumbing.\n    '''\n    for a, b in zip(nodes[:-1], nodes[1:]):\n        a.output = b\n    b.output = queues.Queue()", "response": "Connect a list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse a jinja template to wrap the content inside a layout.", "response": "def render_layout(layout_name, content, **context):\n    \"\"\"Uses a jinja template to wrap the content inside a layout.\n    Wraps the content inside a block and adds the extend statement before rendering it\n    with jinja. The block name can be specified in the layout_name after the filename separated\n    by a colon. The default block name is \"content\".\n    \"\"\"\n    layout_block = \"content\"\n    if \":\" in layout_name:\n        layout_name, layout_block = layout_name.split(\":\")\n    tpl = '{%% extends \"%s\" %%}{%% block %s %%}%s{%% endblock %%}' % (layout_name, layout_block, content)\n    return render_template_string(tpl, **context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_template(app, filename):\n    if not hasattr(parse_template, \"cache\"):\n        parse_template.cache = {}\n    if filename not in parse_template.cache:\n        source = get_template_source(app, filename)\n        parse_template.cache[filename] = app.jinja_env.parse(source, filename=filename)\n    return parse_template.cache[filename]", "response": "Parses the given template using the jinja environment of the given app\n    and returns the AST."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a Jinja2 node to its python equivalent", "response": "def jinja_node_to_python(node):\n    \"\"\"Converts a Jinja2 node to its python equivalent\n    \"\"\"\n    if isinstance(node, nodes.Const):\n        return node.value\n    if isinstance(node, nodes.Neg):\n        return -jinja_node_to_python(node.node)\n    if isinstance(node, nodes.Name):\n        return node.name\n    if isinstance(node, (nodes.List, nodes.Tuple)):\n        value = []\n        for i in node.items:\n            value.append(jinja_node_to_python(i))\n        return value\n    if isinstance(node, nodes.Dict):\n        value = {}\n        for pair in node.items:\n            value[pair.key.value] = jinja_node_to_python(pair.value)\n        return value\n    if isinstance(node, nodes.Call):\n        if not isinstance(node.node, nodes.Name) or node.node.name not in (\"_\", \"translate\", \"gettext\"):\n            raise FormDefinitionError(\"Cannot convert function calls from jinja to python other than translation calls\")\n        return lazy_translate(jinja_node_to_python(node.args[0]))\n    raise Exception(\"Cannot convert jinja nodes to python\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of Groups that the LDAP member object is in.", "response": "def groups(self):\n        \"\"\"Get the list of Groups (by dn) that the bound CSH LDAP member object\n        is in.\n        \"\"\"\n        group_list = []\n        all_groups = self.get('memberof')\n        for group_dn in all_groups:\n            if self.__ldap_group_ou__ in group_dn:\n                group_list.append(group_dn)\n\n        return group_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning whether or not the LDAP member object is part of the specified group.", "response": "def in_group(self, group, dn=False):\n        \"\"\"Get whether or not the bound CSH LDAP member object is part of a\n        group.\n\n        Arguments:\n        group -- the CSHGroup object (or distinguished name) of the group to\n                 check membership for\n        \"\"\"\n        if dn:\n            return group in self.groups()\n        return group.check_member(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef savgol_filter(x, window_length, polyorder, deriv=0, delta=1.0, axis=-1, mode='interp', cval=0.0):\n    '''\n    Wrapper for the scipy.signal.savgol_filter function that handles Nan values.\n\n    See: https://github.com/wheeler-microfluidics/dmf-control-board-firmware/issues/3\n\n    Returns\n    -------\n    y : ndarray, same shape as `x`\n        The filtered data.\n    '''\n    # linearly interpolate missing values before filtering\n    x = np.ma.masked_invalid(pd.Series(x).interpolate())\n\n    try:\n        # start filtering from the first non-zero value since these won't be addressed by\n        # the interpolation above\n        ind = np.isfinite(x).nonzero()[0][0]\n        x[ind:] = signal.savgol_filter(x[ind:], window_length, polyorder, deriv,\n                                       delta, axis, mode, cval)\n    except IndexError:\n        pass\n    return np.ma.masked_invalid(x)", "response": "Wrapper for scipy. signal. savgol_filter function that handles Nan values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting measured data from FeedbackResults instance into pandas. DataFrame.", "response": "def feedback_results_to_measurements_frame(feedback_result):\n    '''\n    Extract measured data from `FeedbackResults` instance into\n    `pandas.DataFrame`.\n    '''\n    index = pd.Index(feedback_result.time * 1e-3, name='seconds')\n    df_feedback = pd.DataFrame(np.column_stack([feedback_result.V_fb,\n                                                feedback_result.V_hv,\n                                                feedback_result.fb_resistor,\n                                                feedback_result.hv_resistor]),\n                               columns=['V_fb', 'V_hv', 'fb_resistor',\n                                        'hv_resistor'],\n                               index=index)\n    df_feedback.insert(0, 'frequency', feedback_result.frequency)\n    return df_feedback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract computed impedance data from FeedbackResults instance into pandas. DataFrame.", "response": "def feedback_results_to_impedance_frame(feedback_result):\n    '''\n    Extract computed impedance data from `FeedbackResults` instance into\n    `pandas.DataFrame`.\n    '''\n    index = pd.Index(feedback_result.time * 1e-3, name='seconds')\n    df_feedback = pd.DataFrame(np.column_stack([feedback_result.V_actuation()\n                                                .filled(np.NaN),\n                                                feedback_result.capacitance()\n                                                .filled(np.NaN),\n                                                feedback_result.Z_device()\n                                                .filled(np.NaN)]),\n                               columns=['V_actuation', 'capacitance',\n                                        'impedance'],\n                               index=index)\n    df_feedback.insert(0, 'frequency', feedback_result.frequency)\n    df_feedback.insert(1, 'voltage', feedback_result.voltage)\n    return df_feedback"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary of compiled Arduino hex file paths.", "response": "def get_firmwares():\n    '''\n    Return `dmf_control_board` compiled Arduino hex file paths.\n\n    This function may be used to locate firmware binaries that are available\n    for flashing to [Arduino Mega2560][1] boards.\n\n    [1]: http://arduino.cc/en/Main/arduinoBoardMega2560\n    '''\n    return OrderedDict([(board_dir.name, [f.abspath() for f in\n                                          board_dir.walkfiles('*.hex')])\n                        for board_dir in\n                        package_path().joinpath('firmware').dirs()])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef safe_series_resistor_index_read(f, self, channel, resistor_index=None):\n    '''\n    This decorator checks the resistor-index from the current context _(i.e.,\n    the result of `self.series_resistor_index`)_.  If the resistor-index\n    specified by the `resistor_index` keyword argument is different than the\n    current context value, the series-resistor-index is temporarily set to the\n    value of `resistor_index` to execute the wrapped function before restoring\n    back to the original value.\n    '''\n    if resistor_index is not None:\n        original_resistor_index = self.series_resistor_index(channel)\n        # Save state of resistor-index\n        if resistor_index != original_resistor_index:\n            self.set_series_resistor_index(channel, resistor_index)\n\n    value = f(self, channel)\n\n    if (resistor_index is not None and\n            resistor_index != original_resistor_index):\n        # Restore state of resistor-index\n        self.set_series_resistor_index(channel, original_resistor_index)\n    return value", "response": "A function that checks the resistor - index of the current context and returns the value of the wrapped function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncatches and re - raise exceptions raised by remote control board firmware commands and re - raise as more specific FirmwareError exception type and FirmwareError exception type and original exception type.", "response": "def remote_command(function, self, *args, **kwargs):\n    '''\n    Catch `RuntimeError` exceptions raised by remote control board firmware\n    commands and re-raise as more specific `FirmwareError` exception type,\n    which includes command code and return code.\n    '''\n    try:\n        return function(self, *args, **kwargs)\n    except RuntimeError, exception:\n        error_message = str(exception)\n        match = CRE_REMOTE_ERROR.match(error_message)\n        if match:\n            # Exception message matches format of remote firmware error.\n            command_code = int(match.group('command_int'))\n            return_code = int(match.group('return_code_int'))\n            raise FirmwareError(command_code, return_code)\n\n        match = CRE_REMOTE_COMMAND_ERROR.match(error_message)\n        if match:\n            # Exception message matches format of remote firmware error.\n            command_code = int(match.group('command_int'))\n            command_name = NAMES_BY_COMMAND_CODE[command_code]\n            raise RuntimeError(CRE_REMOTE_COMMAND_ERROR.sub(command_name,\n                                                            error_message))\n\n        # Not a remote firmware error, so raise original exception.\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _upgrade(self):\n        logging.debug('[FeedbackResults]._upgrade()')\n        if hasattr(self, 'version'):\n            version = Version.fromstring(self.version)\n        else:\n            version = Version(0)\n        logging.debug('[FeedbackResults] version=%s, class_version=%s' %\n                      (str(version), self.class_version))\n        if version > Version.fromstring(self.class_version):\n            logging.debug('[FeedbackResults] version>class_version')\n            raise FutureVersionError(Version.fromstring(self.class_version),\n                                     version)\n        elif version < Version.fromstring(self.class_version):\n            if version < Version(0, 1):\n                self.calibration = FeedbackCalibration()\n            if version < Version(0, 2):\n                # flag invalid data points\n                self.version = str(Version(0, 2))\n                self.fb_resistor[self.V_fb > 5] = -1\n                self.hv_resistor[self.V_hv > 5] = -1\n            if version < Version(0, 3):\n                self.attempt = 0\n            if version < Version(0, 4):\n                del self.sampling_time_ms\n                del self.delay_between_samples_ms\n                self.voltage = self.options.voltage\n                del self.options\n                del self.attempt\n            if version < Version(0, 5):\n                self.area = 0\n                self.version = str(Version(0, 5))\n            if version < Version(0, 6):\n                self.amplifier_gain = None\n                self.vgnd_hv = None\n                self.vgnd_fb = None\n                self.version = str(Version(0, 6))\n                logging.info('[FeedbackResults] upgrade to version %s' %\n                             self.version)\n        else:\n            # Else the versions are equal and don't need to be upgraded.\n            pass", "response": "Upgrade the serialized object if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef V_total(self):\n        '''\n        Compute the input voltage (i.e., ``V1``) based on the measured\n        high-voltage feedback values for ``V2``, using the high-voltage\n        transfer function.\n\n        See also\n        --------\n        :meth:`V_actuation` for diagram with ``V1`` and ``V2`` labelled.\n        '''\n        ind = mlab.find(self.hv_resistor >= 0)\n        V1 = np.empty(self.hv_resistor.shape)\n        V1.fill(np.nan)\n        V1[ind] = compute_from_transfer_function(self.calibration.hw_version\n                                                 .major, 'V1',\n                                                 V2=self.V_hv[ind], R1=10e6,\n                                                 R2=self.calibration.R_hv\n                                                 [self.hv_resistor[ind]],\n                                                 C2=self.calibration.C_hv\n                                                 [self.hv_resistor[ind]],\n                                                 f=self.frequency)\n        # convert to masked array\n        V1 = np.ma.masked_invalid(pd.Series(V1, pd.to_datetime(self.time, unit='s')\n            ).interpolate(method='time').values)\n        V1.fill_value = np.nan\n        V1.data[V1.mask] = V1.fill_value\n        return V1", "response": "Compute the input voltage for the entry in the log file based on the measured\n        high - voltage feedback values for V2."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the voltage drop across the device.", "response": "def V_actuation(self):\n        '''\n        Return the voltage drop across the device (i.e., the ``Z1`` load) for\n        each feedback measurement.\n\n        Consider the feedback circuit diagrams below for the feedback\n        measurement circuits of the two the control board hardware versions.\n\n        .. code-block:: none\n\n                         # Hardware V1 #          # Hardware V2 #\n\n                         V_1 @ frequency          V_1 @ frequency\n                        \u252c    \u252f                        \u252f\n                        \u2502  \u250c\u2500\u2534\u2500\u2510                    \u250c\u2500\u2534\u2500\u2510    \u250c\u2500\u2500\u2500\u2510\n            V_actuation \u2502  \u2502Z_1\u2502                    \u2502Z_1\u2502  \u250c\u2500\u2524Z_2\u251c\u2500\u2510\n                        \u2502  \u2514\u2500\u252c\u2500\u2518                    \u2514\u2500\u252c\u2500\u2518  \u2502 \u2514\u2500\u2500\u2500\u2518 \u2502\n                        \u2534    \u251c\u2500\u2500\u2500O V_2                \u2502    \u2502  \u2502\\   \u251c\u2500\u2500\u2500O V_2\n                           \u250c\u2500\u2534\u2500\u2510                      \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2502-\\__\u2502\n                           \u2502Z_2\u2502                           \u250c\u2500\u2500\u2502+/\n                           \u2514\u2500\u252c\u2500\u2518                           \u2502  \u2502/\n                            \u2550\u2567\u2550                            \u2502\n                             \u00af                            \u2550\u2567\u2550\n                                                           \u00af\n\n        Note that in the case of **hardware version 1**, the input voltage\n        ``V1`` is divided across ``Z1`` *and* the feedback measurement load\n        ``Z2``. Therefore, the effective *actuation* voltage across the DMF\n        device is less than ``V1``.  Specifically, the effective *actuation*\n        voltage is ``V1 - V2``.\n\n        In **hardware version 2**, since the positive terminal of the op-amp is\n        attached to *(virtual)* ground, the negative op-amp terminal is also at\n        ground potential.  It follows that the actuation voltage is equal to\n        ``V1`` on **hardware version 2**.\n        '''\n        if self.calibration.hw_version.major == 1:\n            return self.V_total() - np.array(self.V_fb)\n        else:\n            return self.V_total()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the impedance of the DMF device.", "response": "def Z_device(self, filter_order=None, window_size=None, tol=0.05):\n        '''\n        Compute the impedance *(including resistive and capacitive load)* of\n        the DMF device *(i.e., dielectric and droplet)*.\n\n        See :func:`calibrate.compute_from_transfer_function`\n        for details.\n        '''\n        ind = mlab.find(self.fb_resistor >= 0)\n        Z1 = np.empty(self.fb_resistor.shape)\n        Z1.fill(np.nan)\n        # convert to masked array\n        Z1 = np.ma.masked_invalid(Z1)\n\n        R2 = self.calibration.R_fb[self.fb_resistor[ind]]\n        C2 = self.calibration.C_fb[self.fb_resistor[ind]]\n        Z1[ind] = compute_from_transfer_function(self.calibration.hw_version\n                                                 .major, 'Z1',\n                                                 V1=self.V_total()[ind],\n                                                 V2=self.V_fb[ind], R2=R2,\n                                                 C2=C2, f=self.frequency)\n\n        Z1 = np.ma.masked_invalid(pd.Series(Z1, pd.to_datetime(self.time, unit='s')\n            ).interpolate(method='time').values)\n        Z1.fill_value = np.nan\n        Z1.data[Z1.mask] = Z1.fill_value\n\n        # if we're filtering and we don't have a window size specified,\n        # automatically determine one\n        if filter_order and window_size is None:\n            window_size = self._get_window_size(tol)\n\n        # if the filter_order or window size is None or if the window size is\n        # smaller than filter_order + 2, don't filter\n        if (filter_order is None or window_size is None or window_size < filter_order + 2):\n            pass\n        else:\n            # if the window size is less than half the sample length\n            if window_size and window_size < len(Z1) / 2:\n                # suppress polyfit warnings\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    Z1 = savgol_filter(Z1, window_size, filter_order)\n            else: # fit a line\n                result = self.mean_velocity(tol=tol)\n                if result['dt'] and \\\n                    result['dt'] > 0.1 * self.time[-1] and result['p'][0] > 0:\n                    if self.calibration._c_drop:\n                        c_drop = self.calibration.c_drop(self.frequency)\n                    else:\n                        c_drop = self.capacitance()[-1] / self.area\n                    if self.calibration._c_filler:\n                        c_filler = self.calibration.c_filler(self.frequency)\n                    else:\n                        c_filler = 0\n                    x = result['p'][0]*self.time + result['p'][1]\n                    C = self.area * (x * (c_drop - c_filler) / \\\n                                     np.sqrt(self.area) + c_filler)\n                    Z1 = 1.0 / (2.0 * math.pi * self.frequency * C)\n                    Z1[mlab.find(self.time==result['t_end'])[0]+1:] = \\\n                        Z1[mlab.find(self.time==result['t_end'])[0]]\n                else:\n                    Z1 = np.mean(Z1)*np.ones(Z1.shape)\n        return Z1"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nestimates the applied force for a given set of active liquids.", "response": "def force(self, Ly=None):\n        '''\n        Estimate the applied force (in Newtons) on a drop according to the\n        electromechanical model [1].\n\n            Ly is the length of the actuated electrode along the y-axis\n                (perpendicular to the direction of motion) in milimeters. By\n                default, use the square root of the actuated electrode area,\n                i.e.,\n                    Ly=Lx=sqrt(Area)\n                To get the force normalized by electrode width (i.e., in units\n                of N/mm), set Ly=1.0.\n\n        1. Chatterjee et al., \"Electromechanical model for actuating liquids in\n           a two-plate droplet microfluidic device,\" Lab on a Chip, no. 9\n           (2009): 1219-1229.\n        '''\n        if self.calibration._c_drop:\n            c_drop = self.calibration.c_drop(self.frequency)\n        else:\n            c_drop = self.capacitance()[-1] / self.area\n        if self.calibration._c_filler:\n            c_filler = self.calibration.c_filler(self.frequency)\n        else:\n            c_filler = 0\n        if Ly is None:\n            Ly = np.sqrt(self.area)\n        return 1e3 * Ly * 0.5 * (c_drop - c_filler) * self.V_actuation()**2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef capacitance(self, filter_order=None, window_size=None, tol=0.05):\n        '''\n        Compute the capacitance of the DMF device _(i.e., dielectric and\n        droplet)_ based on the computed impedance value.\n\n        Note: this assumes impedance is purely capacitive load.\n        TODO: Is this assumption ok?\n        '''\n        C = np.ma.masked_invalid(1.0 / (2.0 * math.pi * self.frequency *\n                   self.Z_device(filter_order=filter_order,\n                                 window_size=window_size, tol=tol)))\n        C.fill_value = np.nan\n        C.data[C.mask] = C.fill_value\n        return C", "response": "Compute the capacitance of the DMF device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef x_position(self, filter_order=None, window_size=None, tol=0.05,\n                   Lx=None):\n        '''\n        Calculate $x$-position according to:\n\n               __ | C       |\n                           \u2572\u2571 a   \u22c5 | - - c_f |\n                  | a       |\n        x = \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n              c_d - c_f\n\n        where:\n\n         - $C$ is the measured capacitance.\n         - $c_f$ is the capacitance of the filler medium per unit area\n           _(e.g., air)_.\n         - $c_d$ is the capacitance of an electrode completely covered in\n           liquid per unit area.\n         - $a$ is the area of the actuated electrode(s).\n\n        Note that this equation for $x$ assumes a single drop moving across an\n        electrode with a length along the x-axis of Lx. If no value is provided\n        for Lx, the electrode is assumed to be square, i.e.,\n            Lx=Ly=sqrt(area)\n        '''\n        if self.calibration._c_drop:\n            c_drop = self.calibration.c_drop(self.frequency)\n        else:\n            c_drop = self.capacitance()[-1] / self.area\n        if self.calibration._c_filler:\n            c_filler = self.calibration.c_filler(self.frequency)\n        else:\n            c_filler = 0\n        if Lx is None:\n            Lx = np.sqrt(self.area)\n        return (self.capacitance(filter_order=filter_order,\n                                 window_size=window_size, tol=tol) / self.area \\\n                - c_filler) / (c_drop - c_filler) * Lx", "response": "Calculate the x - position of the current object in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the mean velocity for a step.", "response": "def mean_velocity(self, tol=0.05, Lx=None):\n        '''\n        Calculate the mean velocity for a step (mm/ms which is equivalent to\n        m/s). Fit a line to the capacitance data and get the slope.\n        '''\n        dx = None\n        dt = None\n        p = None\n        ind = None\n        t_end = None\n\n        if self.area == 0:\n            return dict(dx=dx, dt=dt, p=p, ind=ind, t_end=t_end)\n\n        x = self.x_position(Lx=Lx)\n\n        # find the first and last valid indices\n        ind_start = mlab.find(x.mask==False)[0]\n        ind_last = mlab.find(x.mask==False)[-1]\n\n        # if the original x value is within tol % of the final x value, include\n        # all samples\n        if x[ind_start] > (1 - tol) * x[ind_last] or x[ind_last] < 0:\n            ind_stop = ind_last\n        else: # otherwise, stop when x reaches (1 - tol) % of it's final value\n            ind_stop = mlab.find(x > (1 - tol) * x[ind_last])[0]\n\n        ind = [ind_start, ind_stop]\n\n        # if we have at least 2 valid samples\n        if len(ind) >=2:\n            dx = np.diff(x[ind])[0]\n            dt = np.diff(self.time[ind])[0] # ms\n\n            # suppress polyfit warnings\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                # fit a line to the data\n                p = np.polyfit(self.time[ind[0]:ind[1]], x[ind[0]:ind[1]], 1)\n\n            # find time when the the line intercepts x[ind_last]\n            ind_stop = mlab.find(self.time > \\\n                                 (x[ind_last] - p[1]) / p[0])\n            if len(ind_stop):\n                t_end = self.time[ind_stop[0]]\n            else:\n                t_end = self.time[-1]\n        return dict(dx=dx, dt=dt, p=p, ind=ind, t_end=t_end)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_frame(self, filter_order=3):\n\n        window_size = self._get_window_size()\n        L = np.sqrt(self.area)\n        velocity_results = self.mean_velocity(Lx=L)\n        mean_velocity = None\n        peak_velocity = None\n        dx = 0\n        dt = 0\n        dxdt = np.zeros(len(self.time))\n        dxdt_filtered = np.zeros(len(self.time))\n\n        # if the window size is too small for filtering, set filter_order to None\n        if filter_order and window_size and window_size < filter_order + 2:\n            filter_order = None\n\n        if velocity_results and velocity_results['dx']:\n            mean_velocity = velocity_results['p'][0] * 1e3\n\n            dx = velocity_results['dx']\n            dt = velocity_results['dt'] * 1e-3 # convert to seconds\n\n            t, dxdt = self.dxdt(Lx=L)\n            # interpolate dxdt to use the same time points as the impedance values.\n            dxdt = np.interp(self.time,\n                             t, dxdt) * 1e3 # multiply by 1000 to convert to mm/s\n            dxdt = np.ma.masked_invalid(dxdt)\n\n            t, dxdt_filtered = self.dxdt(filter_order=filter_order, Lx=L)\n            # interpolate dxdt_filtered to use the same time points as the impedance values.\n            dxdt_filtered = np.interp(self.time,\n                                      t, dxdt_filtered) * 1e3 # multiply by 1000 to convert to mm/s\n            dxdt_filtered = np.ma.masked_invalid(dxdt_filtered)\n\n            # calculate peak velocity from filtered data\n            peak_velocity = np.max(dxdt_filtered)\n\n        index = pd.Index(self.time * 1e-3, name='step_time')\n        df = pd.DataFrame({'target_voltage': self.voltage, # V\n                           'voltage': self.V_actuation(), # V\n                           'force': self.force(Ly=1.0) * 1e6, # uN/mm\n                           'Z_device_filtered': self.Z_device(filter_order=filter_order), # Ohms\n                           'capacitance_filtered': self.capacitance(filter_order=filter_order), # F\n                           'x_position_filtered': self.x_position(filter_order=filter_order), # mm\n                           'dxdt_filtered': dxdt_filtered, # mm/s\n                           'Z_device': self.Z_device(), # Ohms\n                           'capacitance': self.capacitance(), # F\n                           'x_position': self.x_position(), # mm\n                           'dxdt': dxdt, # mm/s\n                          }, index=index)\n\n        df['frequency'] = self.frequency\n        df['area'] = self.area # mm^2\n        df['dx'] = dx # mm\n        df['dt'] = dt # s\n        df['mean_velocity'] = mean_velocity # mm/s\n        df['peak_velocity'] = peak_velocity # mm/s\n        df['window_size'] = window_size\n        df['filter_order'] = filter_order\n\n        # re-order columns\n        return df[[u'frequency', u'target_voltage', u'voltage', u'force', u'area',\n                   u'Z_device_filtered', u'capacitance_filtered', u'x_position_filtered',\n                   u'dxdt_filtered', u'Z_device', u'capacitance', u'x_position', u'dxdt',\n                   u'dx', u'dt', u'mean_velocity', u'peak_velocity',\n                   u'window_size', u'filter_order']]", "response": "Convert the drop to a pandas. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _upgrade(self):\n        logging.debug(\"[FeedbackResultsSeries]._upgrade()\")\n        version = Version.fromstring(self.version)\n        logging.debug('[FeedbackResultsSeries] version=%s, class_version=%s',\n                      str(version), self.class_version)\n        if version > Version.fromstring(self.class_version):\n            logging.debug('[FeedbackResultsSeries] version>class_version')\n            raise FutureVersionError(Version.fromstring(self.class_version),\n                                     version)\n        elif version < Version.fromstring(self.class_version):\n            if version < Version(0, 1):\n                self.time = [None]*len(self.data)\n                self.version = str(Version(0, 1))", "response": "Upgrade the serialized object if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the c_drop of an object.", "response": "def c_drop(self, frequency):\n        '''\n        Capacitance of an electrode covered in liquid, normalized per unit\n        area (i.e., units are F/mm^2).\n        '''\n        try:\n            return np.interp(frequency,\n                             self._c_drop['frequency'],\n                             self._c_drop['capacitance']\n            )\n        except:\n            pass\n        return self._c_drop"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the electrode in the filler media with the given frequency.", "response": "def c_filler(self, frequency):\n        '''\n        Capacitance of an electrode covered in filler media (e.g., air or oil),\n        normalized per unit area (i.e., units are F/mm^2).\n        '''\n        try:\n            return np.interp(frequency,\n                             self._c_filler['frequency'],\n                             self._c_filler['capacitance']\n            )\n        except:\n            pass\n        return self._c_filler"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _upgrade(self):\n        logging.debug(\"[FeedbackCalibration]._upgrade()\")\n        version = Version.fromstring(self.version)\n        logging.debug('[FeedbackCalibration] version=%s, class_version=%s',\n                      str(version), self.class_version)\n        if version > Version.fromstring(self.class_version):\n            logging.debug('[FeedbackCalibration] version>class_version')\n            raise FutureVersionError(Version.fromstring(self.class_version),\n                                     version)\n        elif version < Version.fromstring(self.class_version):\n            if version < Version(0, 1):\n                self._c_filler = None\n                self._c_drop = None\n                self.version = str(Version(0, 1))\n            if version < Version(0, 2):\n                self.hw_version = Version(1)\n                self.version = str(Version(0, 2))\n                logging.info('[FeedbackCalibration] upgrade to version %s',\n                             self.version)\n            if version < Version(0, 2):\n                self.hw_version = Version(1)\n                self.version = str(Version(0, 2))\n                logging.info('[FeedbackCalibration] upgrade to version %s',\n                             self.version)\n            if version < Version(0, 3):\n                self.version = str(Version(0, 3))\n                logging.info('[FeedbackCalibration] upgrade to version %s',\n                             self.version)", "response": "Upgrade the serialized object if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a force in uN / mm to voltage.", "response": "def force_to_voltage(self, force, frequency):\n        '''\n        Convert a force in uN/mm to voltage.\n\n        Parameters\n        ----------\n        force : float\n            Force in **uN/mm**.\n        frequency : float\n            Actuation frequency.\n\n        Returns\n        -------\n        float\n            Actuation voltage to apply :data:`force` at an actuation frequency\n            of :data:`frequency`.\n        '''\n        c_drop = self.calibration.c_drop(frequency)\n\n        # if c_filler hasn't been set, assume c_filler = 0\n        if self.calibration._c_filler:\n            c_filler = self.calibration.c_filler(frequency)\n        else:\n            c_filler = 0\n\n        return np.sqrt(force * 1e-9/ (0.5 * (c_drop - c_filler)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the current series capacitance value for the specified channel.", "response": "def series_capacitance(self, channel, resistor_index=None):\n        '''\n        Parameters\n        ----------\n        channel : int\n            Analog channel index.\n        resistor_index : int, optional\n            Series resistor channel index.\n\n            If :data:`resistor_index` is not specified, the resistor-index from\n            the current context _(i.e., the result of\n            :attr:`series_resistor_index`)_ is used.\n\n            Otherwise, the series-resistor is temporarily set to the value of\n            :data:`resistor_index` to read the capacitance before restoring\n            back to the original value.\n\n            See definition of :meth:`safe_series_resistor_index_read`\n            decorator.\n\n        Returns\n        -------\n        float\n            Return the current series capacitance value for the specified\n            channel.\n        '''\n        if resistor_index is None:\n            resistor_index = self.series_resistor_index(channel)\n        value = self._series_capacitance(channel)\n        try:\n            if channel == 0:\n                self.calibration.C_hv[resistor_index] = value\n            else:\n                self.calibration.C_fb[resistor_index] = value\n        except:\n            pass\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the current series resistance value for the specified channel.", "response": "def series_resistance(self, channel, resistor_index=None):\n        '''\n        Parameters\n        ----------\n        channel : int\n            Analog channel index.\n        resistor_index : int, optional\n            Series resistor channel index.\n\n            If :data:`resistor_index` is not specified, the resistor-index from\n            the current context _(i.e., the result of\n            :attr:`series_resistor_index`)_ is used.\n\n            Otherwise, the series-resistor is temporarily set to the value of\n            :data:`resistor_index` to set the capacitance before restoring back\n            to the original value.\n\n            See definition of :meth:`safe_series_resistor_index_read`\n            decorator.\n\n        Returns\n        -------\n        float\n            Return the current series resistance value for the specified\n            channel.\n        '''\n        if resistor_index is None:\n            resistor_index = self.series_resistor_index(channel)\n        value = self._series_resistance(channel)\n        try:\n            if channel == 0:\n                self.calibration.R_hv[resistor_index] = value\n            else:\n                self.calibration.R_fb[resistor_index] = value\n        except:\n            pass\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the current series capacitance value for the specified channel.", "response": "def set_series_capacitance(self, channel, value, resistor_index=None):\n        '''\n        Set the current series capacitance value for the specified channel.\n\n        Parameters\n        ----------\n        channel : int\n            Analog channel index.\n        value : float\n            Series capacitance value.\n        resistor_index : int, optional\n            Series resistor channel index.\n\n            If :data:`resistor_index` is not specified, the resistor-index from\n            the current context _(i.e., the result of\n            :attr:`series_resistor_index`)_ is used.\n\n            Otherwise, the series-resistor is temporarily set to the value of\n            :data:`resistor_index` to read the resistance before restoring\n            back to the original value.\n\n        Returns\n        -------\n        int\n            Return code from embedded call.\n        '''\n        if resistor_index is None:\n            resistor_index = self.series_resistor_index(channel)\n        try:\n            if channel == 0:\n                self.calibration.C_hv[resistor_index] = value\n            else:\n                self.calibration.C_fb[resistor_index] = value\n        except:\n            pass\n        return self._set_series_capacitance(channel, value)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_series_resistance(self, channel, value, resistor_index=None):\n        '''\n        Set the current series resistance value for the specified channel.\n\n        Parameters\n        ----------\n        channel : int\n            Analog channel index.\n        value : float\n            Series resistance value.\n        resistor_index : int, optional\n            Series resistor channel index.\n\n            If :data:`resistor_index` is not specified, the resistor-index from\n            the current context _(i.e., the result of\n            :attr:`series_resistor_index`)_ is used.\n\n            Otherwise, the series-resistor is temporarily set to the value of\n            :data:`resistor_index` to set the resistance before restoring back\n            to the original value.\n\n            See definition of :meth:`safe_series_resistor_index_read`\n            decorator.\n\n        Returns\n        -------\n        int\n            Return code from embedded call.\n        '''\n        if resistor_index is None:\n            resistor_index = self.series_resistor_index(channel)\n        try:\n            if channel == 0:\n                self.calibration.R_hv[resistor_index] = value\n            else:\n                self.calibration.R_fb[resistor_index] = value\n        except:\n            pass\n        return self._set_series_resistance(channel, value)", "response": "Set the current series resistance value for the specified channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self, port=None, baud_rate=115200):\n        '''\n        Parameters\n        ----------\n        port : str or list-like, optional\n            Port (or list of ports) to try to connect to as a DMF Control\n            Board.\n        baud_rate : int, optional\n\n        Returns\n        -------\n        str\n            Port DMF control board was connected on.\n\n        Raises\n        ------\n        RuntimeError\n            If connection could not be established.\n        IOError\n            If no ports were specified and Arduino Mega2560 not found on any\n            port.\n        '''\n        if isinstance(port, types.StringTypes):\n            ports = [port]\n        else:\n            ports = port\n\n        if not ports:\n            # No port was specified.\n            #\n            # Try ports matching Mega2560 USB vendor/product ID.\n            ports = serial_ports().index.tolist()\n            if not ports:\n                raise IOError(\"Arduino Mega2560 not found on any port.\")\n\n        for comport_i in ports:\n            if self.connected():\n                self.disconnect()\n                self.port = None\n                self._i2c_devices = {}\n\n            # Try to connect to control board on available ports.\n            try:\n                logger.debug('Try to connect to: %s', comport_i)\n                # Explicitly cast `comport_i` to string since `Base.connect`\n                # Boost Python binding does not support unicode strings.\n                #\n                # Fixes [issue 8][issue-8].\n                #\n                # [issue-8]: https://github.com/wheeler-microfluidics/dmf-control-board-firmware/issues/8\n                Base.connect(self, str(comport_i), baud_rate)\n                self.port = comport_i\n                break\n            except BadVGND, exception:\n                logger.warning(exception)\n                break\n            except RuntimeError, exception:\n                continue\n        else:\n            raise RuntimeError('Could not connect to control board on any of '\n                               'the following ports: %s' % ports)\n\n        name = self.name()\n        version = self.hardware_version()\n        firmware = self.software_version()\n        serial_number_string = \"\"\n        try:\n            serial_number_string = \", S/N %03d\" % self.serial_number\n        except:\n            # Firmware does not support `serial_number` attribute.\n            pass\n        logger.info(\"Connected to %s v%s (Firmware: %s%s)\" %\n                    (name, version, firmware, serial_number_string))\n\n        logger.info(\"Poll control board for series resistors and \"\n                    \"capacitance values.\")\n\n        self._read_calibration_data()\n\n        try:\n            self.__aref__ = self._aref()\n            logger.info(\"Analog reference = %.2f V\" % self.__aref__)\n        except:\n            # Firmware does not support `__aref__` attribute.\n            pass\n\n        # Check VGND for both analog channels\n        expected = 2 ** 10/2\n        v = {}\n        channels = [0, 1]\n        damaged = []\n        for channel in channels:\n            try:\n                v[channel] = np.mean(self.analog_reads(channel, 10))\n                logger.info(\"A%d VGND = %.2f V (%.2f%% of Aref)\", channel,\n                            self.__aref__ * v[channel] / (2 ** 10), 100.0 *\n                            v[channel] / (2 ** 10))\n                # Make sure that the VGND is close to the expected value;\n                # otherwise, the op-amp may be damaged (expected error\n                # is <= 10%).\n                if np.abs(v[channel] - expected) / expected > .1:\n                    damaged.append(channel)\n            except:\n                # Firmware does not support `__aref__` attribute.\n                break\n\n        # Scan I2C bus to generate list of connected devices.\n        self._i2c_scan()\n\n        if damaged:\n            # At least one of the analog input channels appears to be damaged.\n            if len(damaged) == 1:\n                msg = \"Analog channel %d appears\" % damaged[0]\n            else:\n                msg = \"Analog channels %s appear\" % damaged\n            raise BadVGND(msg + \" to be damaged. You may need to replace the \"\n                          \"op-amp on the control board.\")\n\n        return self.RETURN_OK", "response": "Connect to a specific port or list of ports."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef persistent_write(self, address, byte, refresh_config=False):\n        '''\n        Write a single byte to an address in persistent memory.\n\n        Parameters\n        ----------\n        address : int\n            Address in persistent memory (e.g., EEPROM).\n        byte : int\n            Value to write to address.\n        refresh_config : bool, optional\n            Is ``True``, :meth:`load_config()` is called afterward to refresh\n            the configuration settings.\n        '''\n        self._persistent_write(address, byte)\n        if refresh_config:\n            self.load_config(False)", "response": "Writes a single byte to an address in persistent memory."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a chunk of data from persistent memory.", "response": "def persistent_read_multibyte(self, address, count=None, dtype=np.uint8):\n        '''\n        Read a chunk of data from persistent memory.\n\n        Parameters\n        ----------\n        address : int\n            Address in persistent memory (e.g., EEPROM).\n        count : int, optional\n            Number of values to read.\n\n            If not set, read a single value of the specified :data:`dtype`.\n        dtype : numpy.dtype, optional\n            The type of the value(s) to read.\n\n        Returns\n        -------\n        dtype or numpy.array(dtype=dtype)\n            If :data:`count` is ``None``, return single value.\n\n            Otherwise, return array of values.\n        '''\n        nbytes = np.dtype(dtype).itemsize\n        if count is not None:\n            nbytes *= count\n\n        # Read enough bytes starting at specified address to match the\n        # requested number of the specified data type.\n        data_bytes = np.array([self.persistent_read(address + i)\n                               for i in xrange(nbytes)], dtype=np.uint8)\n\n        # Cast byte array as array of specified data type.\n        result = data_bytes.view(dtype)\n\n        # If no count was specified, we return a scalar value rather than the\n        # resultant array.\n        if count is None:\n            return result[0]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef persistent_write_multibyte(self, address, data, refresh_config=False):\n        '''\n        Write multiple bytes to an address in persistent memory.\n\n        Parameters\n        ----------\n        address : int\n            Address in persistent memory (e.g., EEPROM).\n        data : numpy.array\n            Data to write.\n        refresh_config : bool, optional\n            Is ``True``, :meth:`load_config()` is called afterward to refresh\n            the configuration settings.\n        '''\n        for i, byte in enumerate(data.view(np.uint8)):\n            self.persistent_write(address + i, int(byte))\n        if refresh_config:\n            self.load_config(False)", "response": "Writes multiple bytes to an address in persistent memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmeasuring impedance of each control board.", "response": "def measure_impedance(self, sampling_window_ms, n_sampling_windows,\n                          delay_between_windows_ms, interleave_samples, rms,\n                          state):\n        '''\n        Measure voltage across load of each of the following control board\n        feedback circuits:\n\n         - Reference _(i.e., attenuated high-voltage amplifier output)_.\n         - Load _(i.e., voltage across DMF device)_.\n\n        The measured voltage _(i.e., ``V2``)_ can be used to compute the\n        impedance of the measured load, the input voltage _(i.e., ``V1``)_,\n        etc.\n\n        Parameters\n        ----------\n        sampling_window_ms : float\n            Length of sampling window (in milleseconds) for each\n            RMS/peak-to-peak voltage measurement.\n        n_sampling_windows : int\n            Number of RMS/peak-to-peak voltage measurements to take.\n        delay_between_windows_ms : float\n            Delay (in milleseconds) between RMS/peak-to-peak voltage\n            measurements.\n        interleave_samples : bool\n            If ``True``, interleave RMS/peak-to-peak measurements for analog\n            channels.\n\n            For example, ``[<i_0>, <j_0>, <i_1>, <j_1>, ..., <i_n>, <j_n>]``\n            where ``i`` and ``j`` correspond to two different analog channels.\n\n            If ``False``, all measurements for each analog channel are taken\n            together.  For example, ``[<i_0>, ..., <i_n>, <j_0>, ..., <j_n>]``\n            where ``i`` and ``j`` correspond to two different analog channels.\n        rms : bool\n            If ``True``, a RMS voltage measurement is collected for each\n            sampling window.\n\n            Otherwise, peak-to-peak measurements are collected.\n        state : list\n            State of device channels.  Length should be equal to the number of\n            device channels.\n\n        Returns\n        -------\n        :class:`FeedbackResults`\n        '''\n        state_ = uint8_tVector()\n        for i in range(0, len(state)):\n            state_.append(int(state[i]))\n\n        buffer = np.array(Base.measure_impedance(self,\n                                                 sampling_window_ms,\n                                                 n_sampling_windows,\n                                                 delay_between_windows_ms,\n                                                 interleave_samples,\n                                                 rms,\n                                                 state_))\n        return self.measure_impedance_buffer_to_feedback_result(buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sweep_channels(self,\n                       sampling_window_ms,\n                       n_sampling_windows_per_channel,\n                       delay_between_windows_ms,\n                       interleave_samples,\n                       rms,\n                       channel_mask):\n        '''\n        Measure voltage across load of each of the following control board\n        feedback circuits:\n\n         - Reference _(i.e., attenuated high-voltage amplifier output)_.\n         - Load _(i.e., voltage across DMF device)_.\n\n        For each channel in the channel mask. The measured voltage _(i.e.,\n        ``V2``)_ can be used to compute the impedance of the measured load, the\n        input voltage _(i.e., ``V1``)_, etc.\n\n        Parameters\n        ----------\n        sampling_window_ms : float\n            Length of sampling window (in milleseconds) for each\n            RMS/peak-to-peak voltage measurement.\n        n_sampling_windows_per_channel : int\n            Number of RMS/peak-to-peak voltage measurements to take.\n        delay_between_windows_ms : float\n            Delay (in milleseconds) between RMS/peak-to-peak voltage\n            measurements.\n        interleave_samples : bool\n            If ``True``, interleave RMS/peak-to-peak measurements for analog\n            channels.\n\n            For example, ``[<i_0>, <j_0>, <i_1>, <j_1>, ..., <i_n>, <j_n>]``\n            where ``i`` and ``j`` correspond to two different analog channels.\n\n            If ``False``, all measurements for each analog channel are taken\n            together.  For example, ``[<i_0>, ..., <i_n>, <j_0>, ..., <j_n>]``\n            where ``i`` and ``j`` correspond to two different analog channels.\n        rms : bool\n            If ``True``, a RMS voltage measurement is collected for each\n            sampling window.\n\n            Otherwise, peak-to-peak measurements are collected.\n        channel_mask : array-like\n            State of device channels.  Length should be equal to the number of\n            device channels.\n\n        Returns\n        -------\n        pandas.DataFrame\n            Table containing one actuation RMS measurement and one device load\n            impedance measurement per row and the columns ``frequency``,\n            ``voltage``, ``channel_i``, ``V_actuation``, ``capacitance``, and\n            ``impedance``.\n\n            Rows are indexed by time since first measurement in frame.\n        '''\n\n        channel_cumsum = np.cumsum(channel_mask)\n\n        # figure out how many channels are in the mask, and how many we can scan\n        # per request\n        n_channels_in_mask = channel_cumsum[-1]\n        max_channels_per_call = (self.MAX_PAYLOAD_LENGTH - 4*4) / \\\n                                 (3*2) / n_sampling_windows_per_channel\n\n        # cache the channel mask\n        self._channel_mask_cache = np.array(channel_mask)\n\n        buffer = np.zeros(4)\n        for i in range(int(math.ceil(n_channels_in_mask / max_channels_per_call))):\n            # figure out which channels to include in this call\n            ind = np.logical_and(channel_cumsum >= i * max_channels_per_call,\n                                 channel_cumsum < (i + 1) * max_channels_per_call)\n\n            # copy those channels from the cached mask\n            channel_mask_ = np.zeros(len(self._channel_mask_cache), dtype=int)\n            channel_mask_[ind] = self._channel_mask_cache[ind]\n\n            # convert it to a uint8_tVector\n            channel_mask_uint8 = uint8_tVector()\n            channel_mask_uint8.extend(channel_mask_)\n\n            buffer = buffer[:-4]\n            buffer = np.concatenate((buffer, np.array(Base.sweep_channels(self,\n                                                     sampling_window_ms,\n                                                     n_sampling_windows_per_channel,\n                                                     delay_between_windows_ms,\n                                                     interleave_samples,\n                                                     rms,\n                                                     channel_mask_uint8))))\n        return self.sweep_channels_buffer_to_feedback_result(buffer)", "response": "This method is used to sweep the analog channel states across the DMF device."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sweep_channels_slow(self, sampling_window_ms, n_sampling_windows,\n                            delay_between_windows_ms, interleave_samples,\n                            use_rms, channel_mask):\n        '''\n        Measure voltage across load of each of the following control board\n        feedback circuits:\n\n         - Reference _(i.e., attenuated high-voltage amplifier output)_.\n         - Load _(i.e., voltage across DMF device)_.\n\n        For each channel in the channel mask. The measured voltage _(i.e.,\n        ``V2``)_ can be used to compute the impedance of the measured load, the\n        input voltage _(i.e., ``V1``)_, etc.\n\n        **N.B.,** Use one firmware call per channel, as opposed to scanning all\n        channels with a single firmware call as in :meth:`sweep_channels`\n        method.\n\n        Returns\n        -------\n        pandas.DataFrame\n            Table containing one actuation RMS measurement and one device load\n            impedance measurement per row and the columns ``frequency``,\n            ``voltage``, ``channel_i``, ``V_actuation``, ``capacitance``, and\n            ``impedance``.\n\n            Rows are indexed by time since first measurement in frame.\n        '''\n        channel_count = len(channel_mask)\n        scan_count = sum(channel_mask)\n\n        frames = []\n\n        print ''\n        scan_count_i = 0\n        # Iterate through channel mask, measuring impedance for each selected\n        # channel in the mask.\n        for channel_i, state_i in enumerate(channel_mask):\n            if state_i:\n                scan_count_i += 1\n                print '\\rMeasure impedance: {} ({}/{})'.format(channel_i,\n                                                               scan_count_i,\n                                                               scan_count),\n                channel_states_i = [0] * channel_count\n                channel_states_i[channel_i] = 1\n                start_time_i = datetime.utcnow()\n                feedback_results_i = \\\n                    self.measure_impedance(sampling_window_ms,\n                                           n_sampling_windows,\n                                           delay_between_windows_ms,\n                                           interleave_samples, use_rms,\n                                           channel_states_i)\n                # Convert custom feedback results object into a\n                # `pandas.DataFrame`.\n                df_result_i =\\\n                    feedback_results_to_impedance_frame(feedback_results_i)\n                df_result_i.insert(2, 'channel_i', channel_i)\n                df_result_i.insert(0, 'utc_start', start_time_i)\n                frames.append(df_result_i)\n        print ''\n\n        if not frames:\n            df_result = pd.DataFrame(None, columns=['utc_start', 'seconds',\n                                                    'channel_i', 'frequency',\n                                                    'V_actuation',\n                                                    'capacitance',\n                                                    'impedance'])\n        else:\n            df_result = pd.concat(frames)\n        return df_result", "response": "Sweep the selected channel mask and measure impedance of each selected channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef i2c_write(self, address, data):\n        '''\n        Parameters\n        ----------\n        address : int\n            Address of I2C device.\n        data : array-like\n            Array of bytes to send to device.\n        '''\n        data_ = uint8_tVector()\n        for i in range(0, len(data)):\n            data_.append(int(data[i]))\n        Base.i2c_write(self, address, data_)", "response": "Write to the specified address and data to the specified I2C device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all values for the specified channel of the type corresponding to the function f.", "response": "def read_all_series_channel_values(self, f, channel):\n        '''\n        Return all values for the specified channel of the type corresponding\n        to the function `f`, where `f` is either `self.series_resistance` or\n        `self.series_capacitance`.\n        '''\n        values = []\n        channel_max_param_count = [3, 5]\n        for i in range(channel_max_param_count[channel]):\n            try:\n                values.append(f(channel, i))\n            except RuntimeError:\n                break\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites all values for the specified channel of the type corresponding to the function f.", "response": "def write_all_series_channel_values(self, read_f, write_f, channel,\n                                        values):\n        '''\n        Return all values for the specified channel of the type corresponding\n        to the function `f`, where `f` is either `self.series_resistance` or\n        `self.series_capacitance`.\n        '''\n\n        # Create a copy of the new values we intend to write. Otherwise, if\n        # `values` is a reference to the calibration object owned by the\n        # control board, it can be overwritten in the following step which will\n        # prevent the update.\n        #\n        # See http://microfluidics.utoronto.ca/trac/dropbot/ticket/81\n        values = copy.deepcopy(values)\n\n        # Read the current values, and only update the values that are\n        # different.\n        original_values = self.read_all_series_channel_values(read_f, channel)\n\n        # Make sure that the number of supplied values matches the number of\n        # corresponding values read from the channel.\n        assert(len(values) == len(original_values))\n\n        for i in range(len(original_values)):\n            if values[i] != original_values[i]:\n                write_f(channel, values[i], i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the list of modified files that are Python or Jinja2.", "response": "def _get_files_modified():\n    \"\"\"Get the list of modified files that are Python or Jinja2.\"\"\"\n    cmd = \"git diff-index --cached --name-only --diff-filter=ACMRTUXB HEAD\"\n    _, files_modified, _ = run(cmd)\n\n    extensions = [re.escape(ext) for ext in list(SUPPORTED_FILES) + [\".rst\"]]\n    test = \"(?:{0})$\".format(\"|\".join(extensions))\n    return list(filter(lambda f: re.search(test, f), files_modified))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_git_author():\n    _, stdout, _ = run(\"git var GIT_AUTHOR_IDENT\")\n\n    git_author = stdout[0]\n    return git_author[:git_author.find(\">\") + 1]", "response": "Return the git author from the git variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting component name from filename.", "response": "def _get_component(filename, default=\"global\"):\n    \"\"\"Get component name from filename.\"\"\"\n    if hasattr(filename, \"decode\"):\n        filename = filename.decode()\n    parts = filename.split(os.path.sep)\n\n    if len(parts) >= 3:\n        if parts[1] in \"modules legacy ext\".split():\n            return parts[2]\n    if len(parts) >= 2:\n        if parts[1] in \"base celery utils\".split():\n            return parts[1]\n    if len(parts) >= 1:\n        if parts[0] in \"grunt docs\".split():\n            return parts[0]\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _prepare_commit_msg(tmp_file, author, files_modified=None, template=None):\n    files_modified = files_modified or []\n    template = template or \"{component}:\\n\\nSigned-off-by: {author}\\n{extra}\"\n    if hasattr(template, \"decode\"):\n        template = template.decode()\n\n    with open(tmp_file, \"r\", \"utf-8\") as fh:\n        contents = fh.readlines()\n        msg = filter(lambda x: not (x.startswith(\"#\") or x.isspace()),\n                     contents)\n        if len(list(msg)):\n            return\n\n    component = \"unknown\"\n    components = _get_components(files_modified)\n\n    if len(components) == 1:\n        component = components[0]\n    elif len(components) > 1:\n        component = \"/\".join(components)\n        contents.append(\n            \"# WARNING: Multiple components detected - consider splitting \"\n            \"commit.\\r\\n\"\n        )\n\n    with open(tmp_file, \"w\", \"utf-8\") as fh:\n        fh.write(template.format(component=component,\n                                 author=author,\n                                 extra=\"\".join(contents)))", "response": "Prepare the commit message in tmp_file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_message(message, options):\n    options = options or dict()\n    options.update(get_options())\n    options.update(_read_local_kwalitee_configuration())\n\n    errors = check_message(message, **options)\n\n    if errors:\n        for error in errors:\n            print(error, file=sys.stderr)\n\n        return False\n    return True", "response": "Checks the message and printing the errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhooking to prepare a commit message.", "response": "def prepare_commit_msg_hook(argv):\n    \"\"\"Hook: prepare a commit message.\"\"\"\n    options = get_options()\n    # Check if the repo has a configuration repo\n    options.update(_read_local_kwalitee_configuration())\n\n    _prepare_commit_msg(argv[1],\n                        _get_git_author(),\n                        _get_files_modified(),\n                        options.get('template'))\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef commit_msg_hook(argv):\n    with open(argv[1], \"r\", \"utf-8\") as fh:\n        message = \"\\n\".join(filter(lambda x: not x.startswith(\"#\"),\n                                   fh.readlines()))\n    options = {\"allow_empty\": True}\n\n    if not _check_message(message, options):\n        click.echo(\n            \"Aborting commit due to commit message errors (override with \"\n            \"'git commit --no-verify').\", file=sys.stderr)\n        raise click.Abort\n    return 0", "response": "Hook for checking commit message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post_commit_hook(argv):\n    _, stdout, _ = run(\"git log -1 --format=%B HEAD\")\n    message = \"\\n\".join(stdout)\n    options = {\"allow_empty\": True}\n\n    if not _check_message(message, options):\n        click.echo(\n            \"Commit message errors (fix with 'git commit --amend').\",\n            file=sys.stderr)\n        return 1  # it should not fail with exit\n    return 0", "response": "Hook for checking commit message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_local_kwalitee_configuration(directory=\".\"):\n    filepath = os.path.abspath(os.path.join(directory, '.kwalitee.yml'))\n    data = {}\n    if os.path.exists(filepath):\n        with open(filepath, 'r') as file_read:\n            data = yaml.load(file_read.read())\n    return data", "response": "Check if the repo has a. kwalitee. yaml file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pre_commit(files, options):\n    errors = []\n    tmpdir = mkdtemp()\n    files_to_check = []\n    try:\n        for (file_, content) in files:\n            # write staged version of file to temporary directory\n            dirname, filename = os.path.split(os.path.abspath(file_))\n            prefix = os.path.commonprefix([dirname, tmpdir])\n            dirname = os.path.relpath(dirname, start=prefix)\n            dirname = os.path.join(tmpdir, dirname)\n            if not os.path.isdir(dirname):\n                os.makedirs(dirname)\n            filename = os.path.join(dirname, filename)\n            with open(filename, \"wb\") as fh:\n                fh.write(content)\n            files_to_check.append((file_, filename))\n\n        for (file_, filename) in files_to_check:\n            errors += list(map(lambda x: \"{0}: {1}\".format(file_, x),\n                               check_file(filename, **options) or []))\n    finally:\n        shutil.rmtree(tmpdir, ignore_errors=True)\n\n    return errors", "response": "Run the check on the files of the added version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pre_commit_hook(argv):\n    options = get_options()\n    # Check if the repo has a configuration repo\n    options.update(_read_local_kwalitee_configuration())\n\n    files = []\n    for filename in _get_files_modified():\n        # get the staged version of the file and\n        # write the staged version to temp dir with its full path to\n        # avoid overwriting files with the same name\n        _, stdout, _ = run(\"git show :{0}\".format(filename), raw_output=True)\n        files.append((filename, stdout))\n\n    errors = _pre_commit(files, options)\n\n    for error in errors:\n        if hasattr(error, \"decode\"):\n            error = error.decode()\n        click.echo(error, file=sys.stderr)\n\n    if errors:\n        click.echo(\n            \"Aborting commit due to kwalitee errors (override with \"\n            \"'git commit --no-verify').\",\n            file=sys.stderr)\n        raise click.Abort\n    return 0", "response": "Hook to check the staged files."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(command, raw_output=False):\n    p = Popen(command.split(), stdout=PIPE, stderr=PIPE)\n    (stdout, stderr) = p.communicate()\n    # On python 3, subprocess.Popen returns bytes objects.\n    if not raw_output:\n        return (\n            p.returncode,\n            [line.rstrip() for line in stdout.decode(\"utf-8\").splitlines()],\n            [line.rstrip() for line in stderr.decode(\"utf-8\").splitlines()]\n        )\n    else:\n        return (p.returncode, stdout, stderr)", "response": "Runs a command using subprocess. Popen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a weight from matplotlib definition to Qt weight", "response": "def mpl_weight2qt(weight):\n    \"\"\"Convert a weight from matplotlib definition to a Qt weight\n\n    Parameters\n    ----------\n    weight: int or string\n        Either an integer between 1 and 1000 or a string out of\n        :attr:`weights_mpl2qt`\n\n    Returns\n    -------\n    int\n        One type of the PyQt5.QtGui.QFont.Weight\"\"\"\n    try:\n        weight = weights_mpl2qt[weight]\n    except KeyError:\n        try:\n            weight = float(weight) / 10\n        except (ValueError, TypeError):\n            weight = QtGui.QFont.Normal\n        else:\n            try:\n                weight = min(filter(lambda w: w >= weight, weights_qt2mpl),\n                             key=lambda w: abs(w - weight))\n            except ValueError:\n                weight = QtGui.QFont.Normal\n    return weight"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef artist_to_qfont(artist):\n        size = int(artist.get_size())\n        weight = mpl_weight2qt(artist.get_weight())\n        italic = artist.get_style() == 'italic'\n        for family in artist.get_family():\n            if family in ['sans-serif', 'cursive', 'monospace', 'serif']:\n                for name in mpl.rcParams['font.' + family]:\n                    font = QtGui.QFont(name, size, weight, italic)\n                    if font.exactMatch():\n                        break\n            else:\n                font = QtGui.QFont(family, size, weight, italic)\n        return font", "response": "Convert a matplotlib. text. Text artist to a PyQt5 QFont object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef choose_font(self, font=None):\n        fmt_widget = self.parent()\n        if font is None:\n            if self.current_font:\n                font, ok = QFontDialog.getFont(\n                    self.current_font, fmt_widget,\n                    'Select %s font' % self.fmto_name,\n                    QFontDialog.DontUseNativeDialog)\n            else:\n                font, ok = QFontDialog.getFont(fmt_widget)\n            if not ok:\n                return\n        self.current_font = font\n        properties = self.load_properties()\n        properties.update(self.qfont_to_artist_props(font))\n        fmt_widget.set_obj(properties)\n        self.refresh()", "response": "Choose a font for the label through a dialog"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrefreshes the widgets from the current font", "response": "def refresh(self):\n        \"\"\"Refresh the widgets from the current font\"\"\"\n        font = self.current_font\n\n        # refresh btn_bold\n        self.btn_bold.blockSignals(True)\n        self.btn_bold.setChecked(font.weight() > 50)\n        self.btn_bold.blockSignals(False)\n\n        # refresh btn_italic\n        self.btn_italic.blockSignals(True)\n        self.btn_italic.setChecked(font.italic())\n        self.btn_italic.blockSignals(False)\n\n        # refresh font size\n        self.spin_box.blockSignals(True)\n        self.spin_box.setValue(font.pointSize())\n        self.spin_box.blockSignals(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbinds the HTML prettifying with the Flask object app. after_request.", "response": "def init_app(self, app):\n        \"\"\"\n        Initializes a Flask object `app`: binds the HTML prettifying with\n        app.after_request.\n\n        :param app: The Flask application object.\n        \"\"\"\n        app.config.setdefault('PRETTIFY', False)\n\n        if app.config['PRETTIFY']:\n            app.after_request(self._prettify_response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprettifies the HTML response.", "response": "def _prettify_response(self, response):\n        \"\"\"\n        Prettify the HTML response.\n\n        :param response: A Flask Response object.\n        \"\"\"\n        if response.content_type == 'text/html; charset=utf-8':\n            ugly = response.get_data(as_text=True)\n            soup = BeautifulSoup(ugly, 'html.parser')\n            pretty = soup.prettify(formatter='html')\n            response.direct_passthrough = False\n            response.set_data(pretty)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling the SABnzbd API", "response": "async def _call(self, params):\n        \"\"\"Call the SABnzbd API\"\"\"\n        if self._session.closed:\n            raise SabnzbdApiException('Session already closed')\n\n        p = {**self._default_params, **params}\n        try:\n            async with timeout(self._timeout, loop=self._session.loop):\n                async with self._session.get(self._api_url, params=p) as resp:\n                    data = await resp.json()\n                    if data.get('status', True) is False:\n                        self._handle_error(data, params)\n                    else:\n                        return data\n        except aiohttp.ClientError:\n            raise SabnzbdApiException('Unable to communicate with Sabnzbd API')\n        except asyncio.TimeoutError:\n            raise SabnzbdApiException('SABnzbd API request timed out')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def refresh_data(self):\n        queue = await self.get_queue()\n        history = await self.get_history()\n        totals = {}\n        for k in history:\n            if k[-4:] == 'size':\n                totals[k] = self._convert_size(history.get(k))\n        self.queue = {**totals, **queue}", "response": "Refresh the cached SABnzbd queue data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting units to GB", "response": "def _convert_size(self, size_str):\n        \"\"\"Convert units to GB\"\"\"\n        suffix = size_str[-1]\n        if suffix == 'K':\n            multiplier = 1.0 / (1024.0 * 1024.0)\n        elif suffix == 'M':\n            multiplier = 1.0 / 1024.0\n        elif suffix == 'T':\n            multiplier = 1024.0\n        else:\n            multiplier = 1\n\n        try:\n            val = float(size_str.split(' ')[0])\n            return val * multiplier\n        except ValueError:\n            return 0.0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling an error response from the SABnzbd API", "response": "def _handle_error(self, data, params):\n        \"\"\"Handle an error response from the SABnzbd API\"\"\"\n        error = data.get('error', 'API call failed')\n        mode = params.get('mode')\n        raise SabnzbdApiException(error, mode=mode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the ssh key and return the ssh config location", "response": "def __generate_key(self, config):\n        \"\"\"\n        Generate the ssh key, and return the ssh config location\n        \"\"\"\n        cwd = config.get('ssh_path', self._install_directory())\n        if config.is_affirmative('create', default=\"yes\"):\n            if not os.path.exists(cwd):\n                os.makedirs(cwd)\n            if not os.path.exists(os.path.join(cwd, config.get('keyname'))):\n                command = \"ssh-keygen -t %(type)s -f %(keyname)s -N  \" % config.to_dict()\n                lib.call(command, cwd=cwd, output_log_level=logging.DEBUG)\n        if not config.has('ssh_path'):\n            config.set('ssh_path', cwd)\n        config.set('ssh_key_path', os.path.join(config.get('ssh_path'), config.get('keyname')))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __install_ssh_config(self, config):\n        if not config.is_affirmative('use_global_ssh', default=\"no\"):\n            ssh_config_injection = self._build_ssh_config(config)\n\n            if not os.path.exists(ssh_config_path):\n\n                if self.injections.in_noninjected_file(ssh_config_path, \"Host %s\" % config.get('host')):\n                    if config.is_affirmative('override'):\n                        self.injections.inject(ssh_config_path, ssh_config_injection)\n                else:\n                    self.injections.inject(ssh_config_path, ssh_config_injection)\n\n            else:\n                self.injections.inject(ssh_config_path, ssh_config_injection)\n\n            self.injections.commit()", "response": "Install the ssh configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_ssh_config(self, config):\n        ssh_config_injection = ssh_config_template % {\n            'host': config.get('host'),\n            'hostname': config.get('hostname'),\n            'ssh_key_path': config.get('ssh_key_path'),\n            'user': config.get('user')\n        }\n        if config.has('port'):\n            ssh_config_injection += \"  Port {0}\\n\".format(config.get('port'))\n        return ssh_config_injection", "response": "build the ssh injection configuration"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_followups(task):\n    callbacks = task.request.callbacks\n    errbacks = task.request.errbacks\n    task.request.callbacks = None\n    return {'link': callbacks, 'link_error': errbacks}", "response": "Extract callbacks and errbacks from provided task instance"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate Procfiles which can be used with honcho or foreman.", "response": "def gen_procfile(ctx, wsgi, dev):\n    \"\"\"Generates Procfiles which can be used with honcho or foreman.\n    \"\"\"\n    if wsgi is None:\n        if os.path.exists(\"wsgi.py\"):\n            wsgi = \"wsgi.py\"\n        elif os.path.exists(\"app.py\"):\n            wsgi = \"app.py\"\n        else:\n            wsgi = \"app.py\"\n            ctx.invoke(gen_apppy)\n\n    def write_procfile(filename, server_process, debug):\n        processes = [server_process] + current_app.processes\n        procfile = []\n        for name, cmd in procfile_processes(processes, debug).iteritems():\n            procfile.append(\"%s: %s\" % (name, cmd))\n        with open(filename, \"w\") as f:\n            f.write(\"\\n\".join(procfile))\n\n    write_procfile(\"Procfile\", (\"web\", [\"gunicorn\", wsgi]), False)\n    if dev:\n        write_procfile(\"Procfile.dev\", (\"web\", [\"frasco\", \"serve\"]), True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, host, filename, data, f_type, f_other_type=None, f_text=''):\n        return self.send.evidence_add(host, filename, data, f_type, f_other_type, f_text)", "response": "Add an evidence to a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the number of seconds elapsed since epoch Returns the number of seconds elapsed since epoch", "response": "def utc_mktime(utc_tuple):\n  \"\"\"Returns number of seconds elapsed since epoch\n  Note that no timezone are taken into consideration.\n  utc tuple must be: (year, month, day, hour, minute, second)\n  \"\"\"\n  if len(utc_tuple) == 6:\n      utc_tuple += (0, 0, 0)\n  return time.mktime(utc_tuple) - time.mktime((1970, 1, 1, 0, 0, 0, 0, 0, 0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef f(self):\n        if self.data.minute == 0:\n            return self.g()\n        return u'%s:%s' % (self.g(), self.i())", "response": "Returns the ISO - 8601 formatted string for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef g(self):\n        \"Hour, 12-hour format without leading zeros; i.e. '1' to '12'\"\n        if self.data.hour == 0:\n            return 12\n        if self.data.hour > 12:\n            return self.data.hour - 12\n        return self.data.hour", "response": "Hour 12 - hour format without leading zeros ; i. e. 1 to 12"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the ISO 8601 time string for the current locale.", "response": "def P(self):\n        \"\"\"\n        Time, in 12-hour hours, minutes and 'a.m.'/'p.m.', with minutes left off\n        if they're zero and the strings 'midnight' and 'noon' if appropriate.\n        Examples: '1 a.m.', '1:30 p.m.', 'midnight', 'noon', '12:30 p.m.'\n        Proprietary extension.\n        \"\"\"\n        if self.data.minute == 0 and self.data.hour == 0:\n            return _('midnight')\n        if self.data.minute == 0 and self.data.hour == 12:\n            return _('noon')\n        return u'%s %s' % (self.f(), self.a())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef I(self):\n    \"'1' if Daylight Savings Time, '0' otherwise.\"\n    if self.timezone and self.timezone.dst(self.data):\n      return u'1'\n    else:\n      return u'0'", "response": "1 if Daylight Savings Time 0 otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef S(self):\n    \"English ordinal suffix for the day of the month, 2 characters; i.e. 'st', 'nd', 'rd' or 'th'\"\n    if self.data.day in (11, 12, 13): # Special case\n      return u'th'\n    last = self.data.day % 10\n    if last == 1:\n      return u'st'\n    if last == 2:\n      return u'nd'\n    if last == 3:\n      return u'rd'\n    return u'th'", "response": "English ordinal suffix for the day of the month 2 characters ; i. e. st nd rd or th"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef t(self):\n    \"Number of days in the given month; i.e. '28' to '31'\"\n    return u'%02d' % calendar.monthrange(self.data.year, self.data.month)[1]", "response": "Number of days in the given month ; i. e. 28 to 31"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntime zone of this machine ; e. g. EST or MDT", "response": "def T(self):\n    \"Time zone of this machine; e.g. 'EST' or 'MDT'\"\n    name = self.timezone and self.timezone.tzname(self.data) or None\n    if name is None:\n      name = self.format('O')\n    return unicode(name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef U(self):\n    \"Seconds since the Unix epoch (January 1 1970 00:00:00 GMT)\"\n    if getattr(self.data, 'tzinfo', None):\n      return int(calendar.timegm(self.data.utctimetuple()))\n    else:\n      return int(time.mktime(self.data.timetuple()))", "response": "Seconds since the Unix epoch ( January 1 1970 00 : 00 GMT"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Z offset in seconds.", "response": "def Z(self):\n    \"\"\"\n    Time zone offset in seconds (i.e. '-43200' to '43200'). The offset for\n    timezones west of UTC is always negative, and for those east of UTC is\n    always positive.\n    \"\"\"\n    if not self.timezone:\n      return 0\n    offset = self.timezone.utcoffset(self.data)\n    # Only days can be negative, so negative offsets have days=-1 and\n    # seconds positive. Positive offsets have days=0\n    return offset.days * 86400 + offset.seconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_metric(name, count, elapsed):\n    _do_print(name, count, elapsed, file=sys.stdout)", "response": "A metric function that prints to standard output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stderr_metric(name, count, elapsed):\n    _do_print(name, count, elapsed, file=sys.stderr)", "response": "A metric function that prints to standard error"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a metric function that calls multiple metrics.", "response": "def make_multi_metric(*metrics):\n    \"\"\"Make a new metric function that calls the supplied metrics\n\n    :arg functions metrics: metric functions\n    :rtype: function\n    \"\"\"\n    def multi_metric(name, count, elapsed):\n        \"\"\"Calls multiple metrics (closure)\"\"\"\n        for m in metrics:\n            m(name, count, elapsed)\n    return multi_metric"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True iff the given scc is reachable from elsewhere.", "response": "def _is_orphan(scc, graph):\n    \"\"\"\n    Return False iff the given scc is reachable from elsewhere.\n\n    \"\"\"\n    return all(p in scc for v in scc for p in graph.parents(v))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef key_cycles():\n    graph = garbage()\n    sccs = graph.strongly_connected_components()\n    return [scc for scc in sccs if _is_orphan(scc, graph)]", "response": "Collect cyclic garbage and return the strongly connected components that were keeping the garbage alive."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_command(self, command, **kwargs):\n        try:\n            return {'output': subprocess.check_output(command, **kwargs)}\n        except Exception as e:\n            return {'error': str(e)}", "response": "Wrapper to pass command to plowshare."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _hosts_by_success(self, hosts=[]):\n        hosts = hosts if hosts else self.hosts\n        return sorted(hosts, key=lambda h: self._host_errors[h])", "response": "Order hosts by successful connections."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove sources with errors and return ordered by host success.", "response": "def _filter_sources(self, sources):\n        \"\"\"Remove sources with errors and return ordered by host success.\n\n        :param sources: List of potential sources to connect to.\n        :type sources: list\n        :returns: Sorted list of potential sources without errors.\n        :rtype: list\n        \"\"\"\n        filtered, hosts = [], []\n        for source in sources:\n            if 'error' in source:\n                continue\n            filtered.append(source)\n            hosts.append(source['host_name'])\n\n        return sorted(filtered, key=lambda s:\n                      self._hosts_by_success(hosts).index(s['host_name']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload(self, filename, number_of_hosts):\n        return self.multiupload(filename, self.random_hosts(number_of_hosts))", "response": "Uploads the given file to the specified number of hosts."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload a file from one of the provided sources.", "response": "def download(self, sources, output_directory, filename):\n        \"\"\"Download a file from one of the provided sources\n\n        The sources will be ordered by least amount of errors, so most\n        successful hosts will be tried first. In case of failure, the next\n        source will be attempted, until the first successful download is\n        completed or all sources have been depleted.\n\n        :param sources: A list of dicts with 'host_name' and 'url' keys.\n        :type sources: list\n        :param output_directory: Directory to save the downloaded file in.\n        :type output_directory: str\n        :param filename: Filename assigned to the downloaded file.\n        :type filename: str\n        :returns: A dict with 'host_name' and 'filename' keys if the download\n                  is successful, or an empty dict otherwise.\n        :rtype: dict\n        \"\"\"\n        valid_sources = self._filter_sources(sources)\n        if not valid_sources:\n            return {'error': 'no valid sources'}\n\n        manager = Manager()\n        successful_downloads = manager.list([])\n\n        def f(source):\n            if not successful_downloads:\n                result = self.download_from_host(\n                    source, output_directory, filename)\n                if 'error' in result:\n                    self._host_errors[source['host_name']] += 1\n                else:\n                    successful_downloads.append(result)\n\n        multiprocessing.dummy.Pool(len(valid_sources)).map(f, valid_sources)\n\n        return successful_downloads[0] if successful_downloads else {}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndownloads a file from a given host.", "response": "def download_from_host(self, source, output_directory, filename):\n        \"\"\"Download a file from a given host.\n\n        This method renames the file to the given string.\n\n        :param source: Dictionary containing information about host.\n        :type source: dict\n        :param output_directory: Directory to place output in.\n        :type output_directory: str\n        :param filename: The filename to rename to.\n        :type filename: str\n        :returns: Dictionary with information about downloaded file.\n        :rtype: dict\n        \"\"\"\n        result = self._run_command(\n            [\"plowdown\", source[\"url\"], \"-o\",\n                output_directory, \"--temp-rename\"],\n            stderr=open(\"/dev/null\", \"w\")\n        )\n\n        result['host_name'] = source['host_name']\n\n        if 'error' in result:\n            return result\n\n        temporary_filename = self.parse_output(\n            result['host_name'], result['output'])\n        result['filename'] = os.path.join(output_directory, filename)\n        result.pop('output')\n\n        os.rename(temporary_filename, result['filename'])\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multiupload(self, filename, hosts):\n        manager = Manager()\n        successful_uploads = manager.list([])\n\n        def f(host):\n            if len(successful_uploads) / float(len(hosts)) < \\\n                    settings.MIN_FILE_REDUNDANCY:\n                # Optimal redundancy not achieved, keep going\n                result = self.upload_to_host(filename, host)\n                if 'error' in result:\n                    self._host_errors[host] += 1\n                else:\n                    successful_uploads.append(result)\n\n        multiprocessing.dummy.Pool(len(hosts)).map(\n            f, self._hosts_by_success(hosts))\n\n        return list(successful_uploads)", "response": "Uploads a file to multiple hosts simultaneously."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload a file to a given host.", "response": "def upload_to_host(self, filename, hostname):\n        \"\"\"Upload a file to the given host.\n\n        This method relies on 'plowup' being installed on the system.\n        If it succeeds, this method returns a dictionary with the host name,\n        and the final URL. Otherwise, it returns a dictionary with the\n        host name and an error flag.\n\n        :param filename: The filename of the file to upload.\n        :type filename: str\n        :param hostname: The host you are uploading the file to.\n        :type hostname: str\n        :returns: Dictionary containing information about upload to host.\n        :rtype: dict\n        \"\"\"\n        result = self._run_command(\n            [\"plowup\", hostname, filename],\n            stderr=open(\"/dev/null\", \"w\")\n        )\n\n        result['host_name'] = hostname\n        if 'error' not in result:\n            result['url'] = self.parse_output(hostname, result.pop('output'))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_output(self, hostname, output):\n        if isinstance(output, bytes):\n            output = output.decode('utf-8')\n        return output.split()[-1]", "response": "Parse the output of a plowup s get_n_jobs command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the set of queues known by this worker.", "response": "def _generate_queues(queues, exchange, platform_queue):\n        \"\"\" Queues known by this worker \"\"\"\n        return set([\n            Queue('celery', exchange, routing_key='celery'),\n            Queue(platform_queue, exchange, routing_key='#'),\n        ] + [\n            Queue(q_name, exchange, routing_key=q_name)\n            for q_name in queues\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _erf(x):\n    T = [\n        9.60497373987051638749E0,\n        9.00260197203842689217E1,\n        2.23200534594684319226E3,\n        7.00332514112805075473E3,\n        5.55923013010394962768E4,\n    ]\n\n    U = [\n        3.35617141647503099647E1,\n        5.21357949780152679795E2,\n        4.59432382970980127987E3,\n        2.26290000613890934246E4,\n        4.92673942608635921086E4,\n    ]\n\n    # Shorcut special cases\n    if x == 0:\n        return 0\n    if x >= MAXVAL:\n        return 1\n    if x <= -MAXVAL:\n        return -1\n\n    if abs(x) > 1:\n        return 1 - erfc(x)\n\n    z = x * x\n    return x * _polevl(z, T, 4) / _p1evl(z, U, 5)", "response": "Return the erf of a number x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _erfc(a):\n    # approximation for abs(a) < 8 and abs(a) >= 1\n    P = [\n        2.46196981473530512524E-10,\n        5.64189564831068821977E-1,\n        7.46321056442269912687E0,\n        4.86371970985681366614E1,\n        1.96520832956077098242E2,\n        5.26445194995477358631E2,\n        9.34528527171957607540E2,\n        1.02755188689515710272E3,\n        5.57535335369399327526E2,\n    ]\n\n    Q = [\n        1.32281951154744992508E1,\n        8.67072140885989742329E1,\n        3.54937778887819891062E2,\n        9.75708501743205489753E2,\n        1.82390916687909736289E3,\n        2.24633760818710981792E3,\n        1.65666309194161350182E3,\n        5.57535340817727675546E2,\n    ]\n\n    # approximation for abs(a) >= 8\n    R = [\n        5.64189583547755073984E-1,\n        1.27536670759978104416E0,\n        5.01905042251180477414E0,\n        6.16021097993053585195E0,\n        7.40974269950448939160E0,\n        2.97886665372100240670E0,\n    ]\n\n    S = [\n        2.26052863220117276590E0,\n        9.39603524938001434673E0,\n        1.20489539808096656605E1,\n        1.70814450747565897222E1,\n        9.60896809063285878198E0,\n        3.36907645100081516050E0,\n    ]\n\n    # Shortcut special cases\n    if a == 0:\n        return 1\n    if a >= MAXVAL:\n        return 0\n    if a <= -MAXVAL:\n        return 2\n\n    x = a\n    if a < 0:\n        x = -a\n\n    # computationally cheaper to calculate erf for small values, I guess.\n    if x < 1:\n        return 1 - erf(a)\n\n    z = -a * a\n\n    z = math.exp(z)\n\n    if x < 8:\n        p = _polevl(x, P, 8)\n        q = _p1evl(x, Q, 8)\n    else:\n        p = _polevl(x, R, 5)\n        q = _p1evl(x, S, 6)\n\n    y = (z * p) / q\n\n    if a < 0:\n        y = 2 - y\n\n    return y", "response": "Return the base erfc of the base erfc of the base cephes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _polevl(x, coefs, N):\n    ans = 0\n    power = len(coefs) - 1\n    for coef in coefs:\n        try:\n            ans += coef * x**power\n        except OverflowError:\n            pass\n        power -= 1\n    return ans", "response": "Evaluate polynomial x for a set of cephes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the ND - Triangle distribution function for the given logy value.", "response": "def _ndtri(y):\n    \"\"\"\n    Port of cephes ``ndtri.c``: inverse normal distribution function.\n\n    See https://github.com/jeremybarnes/cephes/blob/master/cprob/ndtri.c\n    \"\"\"\n    # approximation for 0 <= abs(z - 0.5) <= 3/8\n    P0 = [\n        -5.99633501014107895267E1,\n        9.80010754185999661536E1,\n        -5.66762857469070293439E1,\n        1.39312609387279679503E1,\n        -1.23916583867381258016E0,\n    ]\n\n    Q0 = [\n        1.95448858338141759834E0,\n        4.67627912898881538453E0,\n        8.63602421390890590575E1,\n        -2.25462687854119370527E2,\n        2.00260212380060660359E2,\n        -8.20372256168333339912E1,\n        1.59056225126211695515E1,\n        -1.18331621121330003142E0,\n    ]\n\n    # Approximation for interval z = sqrt(-2 log y ) between 2 and 8\n    # i.e., y between exp(-2) = .135 and exp(-32) = 1.27e-14.\n    P1 = [\n        4.05544892305962419923E0,\n        3.15251094599893866154E1,\n        5.71628192246421288162E1,\n        4.40805073893200834700E1,\n        1.46849561928858024014E1,\n        2.18663306850790267539E0,\n        -1.40256079171354495875E-1,\n        -3.50424626827848203418E-2,\n        -8.57456785154685413611E-4,\n    ]\n\n    Q1 = [\n        1.57799883256466749731E1,\n        4.53907635128879210584E1,\n        4.13172038254672030440E1,\n        1.50425385692907503408E1,\n        2.50464946208309415979E0,\n        -1.42182922854787788574E-1,\n        -3.80806407691578277194E-2,\n        -9.33259480895457427372E-4,\n    ]\n\n    # Approximation for interval z = sqrt(-2 log y ) between 8 and 64\n    # i.e., y between exp(-32) = 1.27e-14 and exp(-2048) = 3.67e-890.\n    P2 = [\n        3.23774891776946035970E0,\n        6.91522889068984211695E0,\n        3.93881025292474443415E0,\n        1.33303460815807542389E0,\n        2.01485389549179081538E-1,\n        1.23716634817820021358E-2,\n        3.01581553508235416007E-4,\n        2.65806974686737550832E-6,\n        6.23974539184983293730E-9,\n    ]\n\n    Q2 = [\n        6.02427039364742014255E0,\n        3.67983563856160859403E0,\n        1.37702099489081330271E0,\n        2.16236993594496635890E-1,\n        1.34204006088543189037E-2,\n        3.28014464682127739104E-4,\n        2.89247864745380683936E-6,\n        6.79019408009981274425E-9,\n    ]\n\n    sign_flag = 1\n\n    if y > (1 - EXP_NEG2):\n        y = 1 - y\n        sign_flag = 0\n\n    # Shortcut case where we don't need high precision\n    # between -0.135 and 0.135\n    if y > EXP_NEG2:\n        y -= 0.5\n        y2 = y ** 2\n        x = y + y * (y2 * _polevl(y2, P0, 4) / _p1evl(y2, Q0, 8))\n        x = x * ROOT_2PI\n        return x\n\n    x = math.sqrt(-2.0 * math.log(y))\n    x0 = x - math.log(x) / x\n\n    z = 1.0 / x\n    if x < 8.0:                 # y > exp(-32) = 1.2664165549e-14\n        x1 = z * _polevl(z, P1, 8) / _p1evl(z, Q1, 8)\n    else:\n        x1 = z * _polevl(z, P2, 8) / _p1evl(z, Q2, 8)\n\n    x = x0 - x1\n    if sign_flag != 0:\n        x = -x\n\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the inverse error function at point z.", "response": "def erfinv(z):\n    \"\"\"\n    Calculate the inverse error function at point ``z``.\n\n    This is a direct port of the SciPy ``erfinv`` function, originally\n    written in C.\n\n    Parameters\n    ----------\n    z : numeric\n\n    Returns\n    -------\n    float\n\n    References\n    ----------\n    + https://en.wikipedia.org/wiki/Error_function#Inverse_functions\n    + http://functions.wolfram.com/GammaBetaErf/InverseErf/\n\n    Examples\n    --------\n    >>> round(erfinv(0.1), 12)\n    0.088855990494\n    >>> round(erfinv(0.5), 12)\n    0.476936276204\n    >>> round(erfinv(-0.5), 12)\n    -0.476936276204\n    >>> round(erfinv(0.95), 12)\n    1.38590382435\n    >>> round(erf(erfinv(0.3)), 3)\n    0.3\n    >>> round(erfinv(erf(0.5)), 3)\n    0.5\n    >>> erfinv(0)\n    0\n    >>> erfinv(1)\n    inf\n    >>> erfinv(-1)\n    -inf\n    \"\"\"\n    if abs(z) > 1:\n        raise ValueError(\"`z` must be between -1 and 1 inclusive\")\n\n    # Shortcut special cases\n    if z == 0:\n        return 0\n    if z == 1:\n        return inf\n    if z == -1:\n        return -inf\n\n    # otherwise calculate things.\n    return _ndtri((z + 1) / 2.0) / math.sqrt(2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cmap(name, lut=None):\n    if name in rcParams['colors.cmaps']:\n        colors = rcParams['colors.cmaps'][name]\n        lut = lut or len(colors)\n        return FixedColorMap.from_list(name=name, colors=colors, N=lut)\n    elif name in _cmapnames:\n        colors = _cmapnames[name]\n        lut = lut or len(colors)\n        return FixedColorMap.from_list(name=name, colors=colors, N=lut)\n    else:\n        cmap = mpl_get_cmap(name)\n        # Note: we could include the `lut` in the call of mpl_get_cmap, but\n        # this raises a ValueError for colormaps like 'viridis' in mpl version\n        # 1.5. Besides the mpl_get_cmap function does not modify the lut if\n        # it does not match\n        if lut is not None and cmap.N != lut:\n            cmap = FixedColorMap.from_list(\n                name=cmap.name, colors=cmap(np.linspace(0, 1, lut)), N=lut)\n        return cmap", "response": "Returns the specified colormap or None if no colormap is specified."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_cmaps(names):\n    import matplotlib.pyplot as plt\n    available_cmaps = list(\n        chain(plt.cm.cmap_d, _cmapnames, rcParams['colors.cmaps']))\n    names = list(names)\n    wrongs = []\n    for arg in (arg for arg in names if (not isinstance(arg, Colormap) and\n                                         arg not in available_cmaps)):\n        if isinstance(arg, str):\n            similarkeys = get_close_matches(arg, available_cmaps)\n        if similarkeys != []:\n            warn(\"Colormap %s not found in standard colormaps.\\n\"\n                 \"Similar colormaps are %s.\" % (arg, ', '.join(similarkeys)))\n        else:\n            warn(\"Colormap %s not found in standard colormaps.\\n\"\n                 \"Run function without arguments to see all colormaps\" % arg)\n        names.remove(arg)\n        wrongs.append(arg)\n    if not names and not wrongs:\n        names = sorted(m for m in available_cmaps if not m.endswith(\"_r\"))\n    return names", "response": "Filter the given names for colormaps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to show standard colormaps from matplotlib.", "response": "def show_colormaps(names=[], N=10, show=True, use_qt=None):\n    \"\"\"Function to show standard colormaps from pyplot\n\n    Parameters\n    ----------\n    ``*args``: str or :class:`matplotlib.colors.Colormap`\n        If a colormap, it returned unchanged.\n        %(cmap_note)s\n    N: int, optional\n        Default: 11. The number of increments in the colormap.\n    show: bool, optional\n        Default: True. If True, show the created figure at the end with\n        pyplot.show(block=False)\n    use_qt: bool\n        If True, use the\n        :class:`psy_simple.widgets.color.ColormapDialog.show_colormaps`, if\n        False use a matplotlib implementation based on [1]_. If None, use\n        the Qt implementation if it is running in the psyplot GUI.\n\n    Returns\n    -------\n    psy_simple.widgets.color.ColormapDialog or matplitlib.figure.Figure\n        Depending on `use_qt`, either an instance of the\n        :class:`psy_simple.widgets.color.ColormapDialog` or the\n        :class:`matplotlib.figure.Figure`\n\n    References\n    ----------\n    .. [1] http://matplotlib.org/1.2.1/examples/pylab_examples/show_colormaps.html\n    \"\"\"\n    names = safe_list(names)\n    if use_qt or (use_qt is None and psyplot.with_gui):\n        from psy_simple.widgets.colors import ColormapDialog\n        from psyplot_gui.main import mainwindow\n        return ColormapDialog.show_colormap(names, N, show, parent=mainwindow)\n    import matplotlib.pyplot as plt\n    # This example comes from the Cookbook on www.scipy.org.  According to the\n    # history, Andrew Straw did the conversion from an old page, but it is\n    # unclear who the original author is.\n    a = np.vstack((np.linspace(0, 1, 256).reshape(1, -1)))\n    # Get a list of the colormaps in matplotlib.  Ignore the ones that end with\n    # '_r' because these are simply reversed versions of ones that don't end\n    # with '_r'\n    cmaps = _get_cmaps(names)\n    nargs = len(cmaps) + 1\n    fig = plt.figure(figsize=(5, 10))\n    fig.subplots_adjust(top=0.99, bottom=0.01, left=0.2, right=0.99)\n    for i, m in enumerate(cmaps):\n        ax = plt.subplot(nargs, 1, i+1)\n        plt.axis(\"off\")\n        plt.pcolormesh(a, cmap=get_cmap(m, N + 1))\n        pos = list(ax.get_position().bounds)\n        fig.text(pos[0] - 0.01, pos[1], m, fontsize=10,\n                 horizontalalignment='right')\n    fig.canvas.set_window_title(\"Figure %i: Predefined colormaps\" % fig.number)\n    if show:\n        plt.show(block=False)\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_stdout_logger(logging_level):\n    out_hdlr = logging.StreamHandler(sys.stdout)\n    out_hdlr.setFormatter(logging.Formatter(\n        '[%(asctime)s] %(message)s', \"%H:%M:%S\"\n    ))\n    out_hdlr.setLevel(logging_level)\n    for name in LOGGING_NAMES:\n        log = logging.getLogger(name)\n        log.addHandler(out_hdlr)\n        log.setLevel(logging_level)", "response": "Create a logger to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    docraptor = DocRaptor()\n\n    print(\"Create PDF\")\n    resp = docraptor.create(\n        {\n            \"document_content\": \"<h1>python-docraptor</h1><p>Async Test</p>\",\n            \"test\": True,\n            \"async\": True,\n        }\n    )\n    print(\"Status ID: {status_id}\".format(status_id=resp[\"status_id\"]))\n\n    status_id = resp[\"status_id\"]\n    resp = docraptor.status(status_id)\n\n    print(\"    {status}\".format(status=resp[\"status\"]))\n    while resp[\"status\"] != \"completed\":\n        time.sleep(3)\n        resp = docraptor.status(status_id)\n        print(\"    {status}\".format(status=resp[\"status\"]))\n\n    print(\"Download to test_async.pdf\")\n    with open(\"test_async.pdf\", \"wb\") as pdf_file:\n        pdf_file.write(docraptor.download(resp[\"download_key\"]).content)\n    print(\"[DONE]\")", "response": "Generate a PDF using the async method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple of all alternate types allowed by the typ type annotation.", "response": "def get_alternate_types_resolving_forwardref_union_and_typevar(typ, _memo: List[Any] = None) \\\n        -> Tuple[Any, ...]:\n    \"\"\"\n    Returns a tuple of all alternate types allowed by the `typ` type annotation.\n\n    If typ is a TypeVar,\n     * if the typevar is bound, return get_alternate_types_resolving_forwardref_union_and_typevar(bound)\n     * if the typevar has constraints, return a tuple containing all the types listed in the constraints (with\n     appropriate recursive call to get_alternate_types_resolving_forwardref_union_and_typevar for each of them)\n     * otherwise return (object, )\n\n    If typ is a Union, return a tuple containing all the types listed in the union (with\n     appropriate recursive call to get_alternate_types_resolving_forwardref_union_and_typevar for each of them)\n\n    If typ is a forward reference, it is evaluated and this method is applied to the results.\n\n    Otherwise (typ, ) is returned\n\n    Note that this function automatically prevent infinite recursion through forward references such as in\n    `A = Union[str, 'A']`, by keeping a _memo of already met symbols.\n\n    :param typ: \n    :return: \n    \"\"\"\n    # avoid infinite recursion by using a _memo\n    _memo = _memo or []\n    if typ in _memo:\n        return tuple()\n\n    # remember that this was already explored\n    _memo.append(typ)\n    if is_typevar(typ):\n        if hasattr(typ, '__bound__') and typ.__bound__ is not None:\n            # TypeVar is 'bound' to a class\n            if hasattr(typ, '__contravariant__') and typ.__contravariant__:\n                # Contravariant means that only super classes of this type are supported!\n                raise Exception('Contravariant TypeVars are not supported')\n            else:\n                # only subclasses of this are allowed (even if not covariant, because as of today we cant do otherwise)\n                return get_alternate_types_resolving_forwardref_union_and_typevar(typ.__bound__, _memo=_memo)\n\n        elif hasattr(typ, '__constraints__') and typ.__constraints__ is not None:\n            if hasattr(typ, '__contravariant__') and typ.__contravariant__:\n                # Contravariant means that only super classes of this type are supported!\n                raise Exception('Contravariant TypeVars are not supported')\n            else:\n                # TypeVar is 'constrained' to several alternate classes, meaning that subclasses of any of them are\n                # allowed (even if not covariant, because as of today we cant do otherwise)\n                return tuple(typpp for c in typ.__constraints__\n                             for typpp in get_alternate_types_resolving_forwardref_union_and_typevar(c, _memo=_memo))\n\n        else:\n            # A non-parametrized TypeVar means 'any'\n            return object,\n\n    elif is_union_type(typ):\n        # do not use typ.__args__, it may be wrong\n        # the solution below works even in typevar+config cases such as u = Union[T, str][Optional[int]]\n        return tuple(t for typpp in get_args(typ, evaluate=True)\n                     for t in get_alternate_types_resolving_forwardref_union_and_typevar(typpp, _memo=_memo))\n\n    elif is_forward_ref(typ):\n        return get_alternate_types_resolving_forwardref_union_and_typevar(resolve_forward_ref(typ), _memo=_memo)\n\n    else:\n        return typ,"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef robust_isinstance(inst, typ) -> bool:\n    if typ is Any:\n        return True\n    if is_typevar(typ):\n        if hasattr(typ, '__constraints__') and typ.__constraints__ is not None:\n            typs = get_args(typ, evaluate=True)\n            return any(robust_isinstance(inst, t) for t in typs)\n        elif hasattr(typ, '__bound__') and typ.__bound__ is not None:\n            return robust_isinstance(inst, typ.__bound__)\n        else:\n            # a raw TypeVar means 'anything'\n            return True\n    else:\n        if is_union_type(typ):\n            typs = get_args(typ, evaluate=True)\n            return any(robust_isinstance(inst, t) for t in typs)\n        else:\n            return isinstance(inst, get_base_generic_type(typ))", "response": "A robust version of isinstance that uses a base generic Type and a typeVar to determine if the instance is an instance of the type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_collection(object_type, strict: bool = False) -> bool:\n    if object_type is None or object_type is Any or is_union_type(object_type) or is_typevar(object_type):\n        return False\n    elif strict:\n        return object_type == dict \\\n               or object_type == list \\\n               or object_type == tuple \\\n               or object_type == set \\\n               or get_base_generic_type(object_type) == Dict \\\n               or get_base_generic_type(object_type) == List \\\n               or get_base_generic_type(object_type) == Set \\\n               or get_base_generic_type(object_type) == Tuple\n    else:\n        return issubclass(object_type, Dict) \\\n               or issubclass(object_type, List) \\\n               or issubclass(object_type, Set) \\\n               or issubclass(object_type, Tuple) \\\n               or issubclass(object_type, dict) \\\n               or issubclass(object_type, list) \\\n               or issubclass(object_type, tuple) \\\n               or issubclass(object_type, set)", "response": "Utility method to check if a type is a subclass of base collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all subclasses of the given type and supports generic types.", "response": "def get_all_subclasses(typ, recursive: bool = True, _memo = None) -> Sequence[Type[Any]]:\n    \"\"\"\n    Returns all subclasses, and supports generic types. It is recursive by default\n    See discussion at https://github.com/Stewori/pytypes/issues/31\n\n    :param typ:\n    :param recursive: a boolean indicating whether recursion is needed\n    :param _memo: internal variable used in recursion to avoid exploring subclasses that were already explored\n    :return:\n    \"\"\"\n    _memo = _memo or set()\n\n    # if we have collected the subclasses for this already, return\n    if typ in _memo:\n        return []\n\n    # else remember that we have collected them, and collect them\n    _memo.add(typ)\n    if is_generic_type(typ):\n        # We now use get_origin() to also find all the concrete subclasses in case the desired type is a generic\n        sub_list = get_origin(typ).__subclasses__()\n    else:\n        sub_list = typ.__subclasses__()\n\n    # recurse\n    result = []\n    for t in sub_list:\n        # only keep the origins in the list\n        to = get_origin(t) or t\n        try:\n            if to is not typ and to not in result and is_subtype(to, typ, bound_typevars={}):\n                result.append(to)\n        except:\n            # catching an error with is_subtype(Dict, Dict[str, int], bound_typevars={})\n            pass\n\n    # recurse\n    if recursive:\n        for typpp in sub_list:\n            for t in get_all_subclasses(typpp, recursive=True, _memo=_memo):\n                # unfortunately we have to check 't not in sub_list' because with generics strange things happen\n                # also is_subtype returns false when the parent is a generic\n                if t not in sub_list and is_subtype(t, typ, bound_typevars={}):\n                    result.append(t)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eval_forward_ref(typ: _ForwardRef):\n    for frame in stack():\n        m = getmodule(frame[0])\n        m_name = m.__name__ if m is not None else '<unknown>'\n        if m_name.startswith('parsyfiles.tests') or not m_name.startswith('parsyfiles'):\n            try:\n                # print(\"File {}:{}\".format(frame.filename, frame.lineno))\n                return typ._eval_type(frame[0].f_globals, frame[0].f_locals)\n            except NameError:\n                pass\n\n    raise InvalidForwardRefError(typ)", "response": "Evaluate the given Forward reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the provided type is a valid PEP484 type hint False otherwise.", "response": "def is_valid_pep484_type_hint(typ_hint, allow_forward_refs: bool = False):\n    \"\"\"\n    Returns True if the provided type is a valid PEP484 type hint, False otherwise.\n\n    Note: string type hints (forward references) are not supported by default, since callers of this function in\n    parsyfiles lib actually require them to be resolved already.\n\n    :param typ_hint:\n    :param allow_forward_refs:\n    :return:\n    \"\"\"\n    # most common case first, to be faster\n    try:\n        if isinstance(typ_hint, type):\n            return True\n    except:\n        pass\n\n    # optionally, check forward reference\n    try:\n        if allow_forward_refs and is_forward_ref(typ_hint):\n            return True\n    except:\n        pass\n\n    # finally check unions and typevars\n    try:\n        return is_union_type(typ_hint) or is_typevar(typ_hint)\n    except:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a given type is nonable.", "response": "def is_pep484_nonable(typ):\n    \"\"\"\n    Checks if a given type is nonable, meaning that it explicitly or implicitly declares a Union with NoneType.\n    Nested TypeVars and Unions are supported.\n\n    :param typ:\n    :return:\n    \"\"\"\n    # TODO rely on typing_inspect if there is an answer to https://github.com/ilevkivskyi/typing_inspect/issues/14\n    if typ is type(None):\n        return True\n    elif is_typevar(typ) or is_union_type(typ):\n        return any(is_pep484_nonable(tt) for tt in get_alternate_types_resolving_forwardref_union_and_typevar(typ))\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_collection_base_type(collection_object_type, exception_if_none: bool = True,\n                                  resolve_fwd_refs: bool = True) -> Tuple[Type, Optional[Type]]:\n    \"\"\"\n    Utility method to extract the base item type from a collection/iterable item type.\n    Throws\n    * a TypeError if the collection_object_type a Dict with non-string keys.\n    * an AttributeError if the collection_object_type is actually not a collection\n    * a TypeInformationRequiredError if somehow the inner type can't be found from the collection type (either if dict,\n    list, set, tuple were used instead of their typing module equivalents (Dict, List, Set, Tuple), or if the latter\n    were specified without inner content types (as in Dict instead of Dict[str, Foo])\n\n    :param collection_object_type:\n    :return: a tuple containing the collection's content type (which may itself be a Tuple in case of a Tuple) and the\n    collection's content key type for dicts (or None)\n    \"\"\"\n    contents_item_type = None\n    contents_key_type = None\n\n    check_var(collection_object_type, var_types=type, var_name='collection_object_type')\n\n    is_tuple = False\n    if is_tuple_type(collection_object_type):  # Tuple is a special construct, is_generic_type does not work\n        is_tuple = True\n        # --old: hack into typing module\n        # if hasattr(collection_object_type, '__args__') and collection_object_type.__args__ is not None:\n        # contents_item_type = collection_object_type.__args__\n\n        # --new : using typing_inspect\n        # __args = get_last_args(collection_object_type)\n        # this one works even in typevar+config cases such as t = Tuple[int, Tuple[T, T]][Optional[int]]\n        __args = get_args(collection_object_type, evaluate=True)\n        if len(__args) > 0:\n            contents_item_type = __args\n\n    elif issubclass(collection_object_type, Mapping):  # Dictionary-like\n        if is_generic_type(collection_object_type):\n            # --old: hack into typing module\n            # if hasattr(collection_object_type, '__args__') and collection_object_type.__args__ is not None:\n            # contents_key_type, contents_item_type = collection_object_type.__args__\n\n            # --new : using typing_inspect\n            # __args = get_last_args(collection_object_type)\n            # this one works even in typevar+config cases such as d = Dict[int, Tuple[T, T]][Optional[int]]\n            __args = get_args(collection_object_type, evaluate=True)\n            if len(__args) > 0:\n                contents_key_type, contents_item_type = __args\n                if not issubclass(contents_key_type, str):\n                    raise TypeError('Collection object has type Dict, but its PEP484 type hints declare '\n                                    'keys as being of type ' + str(contents_key_type) + ' which is not supported. Only '\n                                    'str keys are supported at the moment, since we use them as item names')\n\n    elif issubclass(collection_object_type, Iterable):  # List or Set. Should we rather use Container here ?\n        if is_generic_type(collection_object_type):\n            # --old: hack into typing module\n            # if hasattr(collection_object_type, '__args__') and collection_object_type.__args__ is not None:\n            # contents_item_type = collection_object_type.__args__[0]\n\n            # --new : using typing_inspect\n            # __args = get_last_args(collection_object_type)\n            # this one works even in typevar+config cases such as i = Iterable[Tuple[T, T]][Optional[int]]\n            __args = get_args(collection_object_type, evaluate=True)\n            if len(__args) > 0:\n                contents_item_type, = __args\n\n    elif issubclass(collection_object_type, dict) or issubclass(collection_object_type, list)\\\n            or issubclass(collection_object_type, tuple) or issubclass(collection_object_type, set):\n        # the error is now handled below with the other under-specified types situations\n        pass\n\n    else:\n        # Not a collection\n        raise AttributeError('Cannot extract collection base type, object type ' + str(collection_object_type)\n                             + ' is not a collection')\n\n    # Finally return if something was found, otherwise tell it\n    try:\n        if contents_item_type is None or contents_item_type is Parameter.empty:\n            # Empty type hints\n            raise TypeInformationRequiredError.create_for_collection_items(collection_object_type, contents_item_type)\n        \n        elif is_tuple:\n            # --- tuple: Iterate on all sub-types\n            resolved = []\n            for t in contents_item_type:\n                # Check for empty type hints\n                if contents_item_type is None or contents_item_type is Parameter.empty:\n                    raise TypeInformationRequiredError.create_for_collection_items(collection_object_type, t)\n\n                # Resolve any forward references if needed\n                if resolve_fwd_refs:\n                    t = resolve_forward_ref(t)\n                resolved.append(t)\n\n                # Final type hint compliance\n                if not is_valid_pep484_type_hint(t):\n                    raise InvalidPEP484TypeHint.create_for_collection_items(collection_object_type, t)\n\n            if resolve_fwd_refs:\n                contents_item_type = tuple(resolved)\n        \n        else:\n            # --- Not a tuple\n            # resolve any forward references first\n            if resolve_fwd_refs:\n                contents_item_type = resolve_forward_ref(contents_item_type)\n\n            # check validity then\n            if not is_valid_pep484_type_hint(contents_item_type):\n                # Invalid type hints\n                raise InvalidPEP484TypeHint.create_for_collection_items(collection_object_type, contents_item_type)\n\n    except TypeInformationRequiredError as e:\n        # only raise it if the flag says it\n        if exception_if_none:\n            raise e.with_traceback(e.__traceback__)\n\n    return contents_item_type, contents_key_type", "response": "Utility method to extract the base item type from a collection item type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_validated_attribute_type_info(typ, item_type, attr_name):\n    if (typ is None) or (typ is Parameter.empty):\n        raise TypeInformationRequiredError.create_for_object_attributes(item_type, attr_name, typ)\n\n    # resolve forward references\n    typ = resolve_forward_ref(typ)\n\n    if not is_valid_pep484_type_hint(typ):\n        raise InvalidPEP484TypeHint.create_for_object_attributes(item_type, attr_name, typ)\n\n    return typ", "response": "Function to validate that typ is a valid non - empty PEP484 type hint."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_for_object_attributes(item_type, faulty_attribute_name: str, hint):\n        # this leads to infinite loops\n        # try:\n        #     prt_type = get_pretty_type_str(item_type)\n        # except:\n        #     prt_type = str(item_type)\n        return TypeInformationRequiredError(\"Cannot create instances of type {t}: constructor attribute '{a}' has an\"\n                                            \" invalid PEP484 type hint: {h}.\".format(t=str(item_type),\n                                                                                     a=faulty_attribute_name, h=hint))", "response": "Helper method for creating object attributes of type item_type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bounding_box_from_annotation(source=None, padding=None, **kwargs):\n\n  if source is None:\n    # try to estimate the source\n    for s,k in available_sources.items():\n      # check if the according keyword arguments are given\n      if k[0] in kwargs and k[1] in kwargs:\n        # check if we already assigned a source before\n        if source is not None:\n          raise ValueError(\"The given list of keywords (%s) is ambiguous. Please specify a source\" % kwargs)\n        # assign source\n        source = s\n\n    # check if a source could be estimated from the keywords\n    if source is None:\n      raise ValueError(\"The given list of keywords (%s) could not be interpreted\" % kwargs)\n\n  assert source in available_sources\n\n  # use default padding if not specified\n  if padding is None:\n    padding = default_paddings[source]\n\n  keys = available_sources[source]\n  if source == 'ellipse':\n    # compute the tight bounding box for the ellipse\n    angle = kwargs['angle']\n    axis = kwargs['axis_radius']\n    center = kwargs['center']\n    dx = abs(math.cos(angle) * axis[0]) + abs(math.sin(angle) * axis[1])\n    dy = abs(math.sin(angle) * axis[0]) + abs(math.cos(angle) * axis[1])\n    top = center[0] - dy\n    bottom = center[0] + dy\n    left = center[1] - dx\n    right = center[1] + dx\n  elif padding is None:\n    # There is no padding to be applied -> take nodes as they are\n    top    = kwargs[keys[0]][0]\n    bottom = kwargs[keys[1]][0]\n    left   = kwargs[keys[0]][1]\n    right  = kwargs[keys[1]][1]\n  else:\n    # apply padding\n    pos_0 = kwargs[keys[0]]\n    pos_1 = kwargs[keys[1]]\n    tb_center = float(pos_0[0] + pos_1[0]) / 2.\n    lr_center = float(pos_0[1] + pos_1[1]) / 2.\n    distance = math.sqrt((pos_0[0] - pos_1[0])**2 + (pos_0[1] - pos_1[1])**2)\n\n    top    = tb_center + padding['top'] * distance\n    bottom = tb_center + padding['bottom'] * distance\n    left   = lr_center + padding['left'] * distance\n    right  = lr_center + padding['right'] * distance\n\n  return BoundingBox((top, left), (bottom - top, right - left))", "response": "Returns a bounding box from the given source and optional padding."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the expected eye positions based on the relative coordinates of the bounding box.", "response": "def expected_eye_positions(bounding_box, padding = None):\n  \"\"\"expected_eye_positions(bounding_box, padding) -> eyes\n\n  Computes the expected eye positions based on the relative coordinates of the bounding box.\n\n  This function can be used to translate between bounding-box-based image cropping and eye-location-based alignment.\n  The returned eye locations return the **average** eye locations, no landmark detection is performed.\n\n  **Parameters:**\n\n  ``bounding_box`` : :py:class:`BoundingBox`\n    The face bounding box as detected by one of the functions in ``bob.ip.facedetect``.\n\n  ``padding`` : {'top':float, 'bottom':float, 'left':float, 'right':float}\n    The padding that was used for the ``eyes`` source in :py:func:`bounding_box_from_annotation`, has a proper default.\n\n  **Returns:**\n\n  ``eyes`` : {'reye' : (rey, rex), 'leye' : (ley, lex)}\n    A dictionary containing the average left and right eye annotation.\n  \"\"\"\n  if padding is None:\n    padding = default_paddings['eyes']\n  top, left, right = padding['top'], padding['left'], padding['right']\n  inter_eye_distance = (bounding_box.size[1]) / (right - left)\n  return {\n    'reye':(bounding_box.top_f - top*inter_eye_distance, bounding_box.left_f - left/2.*inter_eye_distance),\n    'leye':(bounding_box.top_f - top*inter_eye_distance, bounding_box.right_f - right/2.*inter_eye_distance)\n  }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parallel_part(data, parallel):\n  if parallel is None or \"SGE_TASK_ID\" not in os.environ:\n    return data\n\n  data_per_job = int(math.ceil(float(len(data)) / float(parallel)))\n  task_id = int(os.environ['SGE_TASK_ID'])\n  first = (task_id-1) * data_per_job\n  last = min(len(data), task_id * data_per_job)\n  return data[first:last]", "response": "This function splits the given data into a list of parts of the given number of parallel jobs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quasi_random_indices(number_of_total_items, number_of_desired_items = None):\n  # check if we need to compute a sublist at all\n  if number_of_desired_items is None or number_of_desired_items >= number_of_total_items or number_of_desired_items < 0:\n    for i in range(number_of_total_items):\n      yield i\n  else:\n    increase = float(number_of_total_items)/float(number_of_desired_items)\n    # generate a regular quasi-random index list\n    for i in range(number_of_desired_items):\n      yield int((i +.5)*increase)", "response": "Quasi - random indices generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exception_class(self, exception):\n\n\t\tcls = type(exception)\n\t\tif cls.__module__ == 'exceptions':  # Built-in exception.\n\t\t\treturn cls.__name__\n\t\treturn \"%s.%s\" % (cls.__module__, cls.__name__)", "response": "Return a name representing the class of an exception."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of information for a given request.", "response": "def request_info(self, request):\n\n\t\t\"\"\"\n\t\tReturn a dictionary of information for a given request.\n\n\t\tThis will be run once for every request.\n\t\t\"\"\"\n\n\t\t# We have to re-resolve the request path here, because the information\n\t\t# is not stored on the request.\n\t\tview, args, kwargs = resolve(request.path)\n\t\tfor i, arg in enumerate(args):\n\t\t\tkwargs[i] = arg\n\n\t\tparameters = {}\n\t\tparameters.update(kwargs)\n\t\tparameters.update(request.POST.items())\n\n\t\tenviron = request.META\n\n\t\treturn {\n\t\t\t\t\"session\": dict(request.session),\n\t\t\t\t'cookies': dict(request.COOKIES),\n\t\t\t\t'headers': dict(get_headers(environ)),\n                'env': dict(get_environ(environ)),\n\t\t\t\t\"remote_ip\": request.META[\"REMOTE_ADDR\"],\n\t\t\t\t\"parameters\": parameters,\n\t\t\t\t\"action\": view.__name__,\n\t\t\t\t\"application\": view.__module__,\n\t\t\t\t\"method\": request.method,\n\t\t\t\t\"url\": request.build_absolute_uri()\n\t\t}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the bootstrapped training of a strong classifier using the given training data and a strong boosting trainer.", "response": "def run(self, training_set, trainer, filename = \"bootstrapped_model.hdf5\", force = False):\n    \"\"\"run(training_set, trainer, [filename], [force]) -> model\n\n    Runs the bootstrapped training of a strong classifier using the given training data and a strong classifier trainer.\n    The training set need to contain extracted features already, as this function will need the features several times.\n\n    **Parameters:**\n\n    ``training_set`` : :py:class:`TrainingSet`\n      The training set containing pre-extracted feature files\n\n    ``trainer`` : :py:class:`bob.learn.boosting.Boosting`\n      A strong boosting trainer to use for selecting the weak classifiers and their weights for each round.\n\n    ``filename`` : str\n      A filename, where to write the resulting strong classifier to.\n      This filename is also used as a base to compute filenames of intermediate files, which store results of each of the bootstrapping steps.\n\n    ``force`` : bool\n      If set to ``False`` (the default), the bootstrapping will continue the round, where it has been stopped during the last run (reading the current stage from respective files).\n      If set to ``True``, the training will start from the beginning.\n\n    **Returns:**\n\n    ``model`` : :py:class:`bob.learn.boosting.BoostedMachine`\n      The resulting strong classifier, a weighted combination of weak classifiers.\n    \"\"\"\n\n    feature_extractor = training_set.feature_extractor()\n\n    training_data = None\n    training_labels = None\n    model = None\n\n    positive_indices, negative_indices = set(), set()\n\n    for b in range(self.m_number_of_rounds):\n      # check if old results are present\n      temp_file = \"%s_round_%d.hdf5\" % (os.path.splitext(filename)[0], b+1)\n      if os.path.exists(temp_file) and not force:\n        logger.info(\"Loading already computed stage %d from %s.\", b+1, temp_file)\n        model, positives, negatives = self._load(bob.io.base.HDF5File(temp_file))\n        positive_indices |= positives\n        negative_indices |= negatives\n\n      else:\n        if positive_indices or negative_indices:\n          # load data from previous rounds\n          logger.info(\"Getting training data of previous rounds\")\n          training_data, training_labels = training_set.sample(positive_indices = positive_indices, negative_indices = negative_indices)\n          positive_indices, negative_indices = set(), set()\n\n        # get data for current round\n        logger.info(\"Getting new data for bootstrapping round %d\", b+1)\n        new_data, new_labels = training_set.sample(model, self.m_number_of_positive_examples_per_round, self.m_number_of_negative_examples_per_round)\n        if training_data is None:\n          training_data = new_data\n        else:\n          training_data = numpy.append(training_data, new_data, axis=0)\n        if training_labels is None:\n          training_labels = new_labels\n        else:\n          training_labels = numpy.append(training_labels, new_labels, axis=0)\n\n        logger.info(\"Starting training with %d examples\", training_data.shape[0])\n        model = trainer.train(training_data, training_labels, self.m_number_of_weak_learners_per_round[b], model)\n\n        # write model and extractor to temporary file to be able to catch up later\n        logger.info(\"Saving results for stage %d to file %s\", b+1, temp_file)\n        self._save(bob.io.base.HDF5File(temp_file, 'w'), model, training_set.positive_indices, training_set.negative_indices)\n\n      feature_extractor.model_indices = model.indices\n\n    # finally, return the trained model\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _save(self, hdf5, model, positives, negatives):\n    # write the model and the training set indices to the given HDF5 file\n    hdf5.set(\"PositiveIndices\", sorted(list(positives)))\n    hdf5.set(\"NegativeIndices\", sorted(list(negatives)))\n    hdf5.create_group(\"Model\")\n    hdf5.cd(\"Model\")\n    model.save(hdf5)\n    del hdf5", "response": "Saves the given intermediate state of the bootstrapping to file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _load(self, hdf5):\n    positives = set(hdf5.get(\"PositiveIndices\"))\n    negatives = set(hdf5.get(\"NegativeIndices\"))\n    hdf5.cd(\"Model\")\n    model = bob.learn.boosting.BoostedMachine(hdf5)\n    return model, positives, negatives", "response": "Loads the intermediate state of the bootstrapping from file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert sequences of compare/cjmp to switch instructions this must happen BEFORE linking minimumLength describes the minum amount of sequential compare/cjmp combinations needed to switch to a switch AUTOMATICALLY called by compile", "response": "def _optimize_switches(self, minimumLength=2):\r\n      '''\r\n      converts sequences of compare/cjmp to switch instructions\r\n      this must happen BEFORE linking\r\n      minimumLength describes the minum amount of sequential compare/cjmp combinations\r\n      needed to switch to a switch\r\n\r\n      AUTOMATICALLY called by compile\r\n      '''\r\n      # locate all the targets of switch statements\r\n      q = util.OneTimeQueue()\r\n      targets = {}\r\n      for i in range(len(self)):\r\n        if isinstance(self[i], Compare) and isinstance(self[i+1], CondJmp):\r\n          q.append(i)\r\n\r\n      while q:\r\n        front = q.popleft()\r\n        i = front\r\n        targets[i] = {}\r\n        targets[i][self[i].arg1] = self[i+1].arg1\r\n        while isinstance(self[i+2], Compare) and isinstance(self[i+3], CondJmp):\r\n          i += 2\r\n          targets[front][self[i].arg1] = self[i+1].arg1\r\n          q.remove(i)\r\n        if len(targets[front]) < minimumLength:\r\n          # don't convert single cjmps to switches\r\n          del targets[front]\r\n\r\n      # now replace sequences with their switch statements\r\n      # in order for our instruction numbers to be valid, do this\r\n      # in reverse order\r\n      _keys = targets.keys()\r\n      _keys.sort()\r\n      _keys.reverse()\r\n      for i in _keys:\r\n        del self[i:i+(len(targets[i])*2)]\r\n        self.insert(i, Switch(targets[i]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving all delayed arguments", "response": "def undelay(self):\r\n        '''resolves all delayed arguments'''\r\n        i = 0\r\n        while i < len(self):\r\n            op = self[i]\r\n            i += 1\r\n            if hasattr(op, 'arg1'):\r\n                if isinstance(op.arg1,DelayedArg):\r\n                    op.arg1 = op.arg1.resolve()\r\n                if isinstance(op.arg1,CodeBlock):\r\n                    op.arg1.undelay()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_logging(log_level=logging.INFO):\n    logging.basicConfig(level=log_level)\n    fmt = (\"%(asctime)s %(levelname)s (%(threadName)s) \"\n           \"[%(name)s] %(message)s\")\n    colorfmt = \"%(log_color)s{}%(reset)s\".format(fmt)\n    datefmt = '%Y-%m-%d %H:%M:%S'\n\n    # Suppress overly verbose logs from libraries that aren't helpful\n    logging.getLogger('requests').setLevel(logging.WARNING)\n\n    try:\n        from colorlog import ColoredFormatter\n        logging.getLogger().handlers[0].setFormatter(ColoredFormatter(\n            colorfmt,\n            datefmt=datefmt,\n            reset=True,\n            log_colors={\n                'DEBUG': 'cyan',\n                'INFO': 'green',\n                'WARNING': 'yellow',\n                'ERROR': 'red',\n                'CRITICAL': 'red',\n            }\n        ))\n    except ImportError:\n        pass\n\n    logger = logging.getLogger('')\n    logger.setLevel(log_level)", "response": "Set up the logging."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute command line helper.", "response": "def call():\n    \"\"\"Execute command line helper.\"\"\"\n    args = get_arguments()\n\n    if args.debug:\n        log_level = logging.DEBUG\n    elif args.quiet:\n        log_level = logging.WARN\n    else:\n        log_level = logging.INFO\n\n    setup_logging(log_level)\n\n    lupusec = None\n\n    if not args.username or not args.password or not args.ip_address:\n            raise Exception(\"Please supply a username, password and ip.\")\n\n    def _devicePrint(dev, append=''):\n        _LOGGER.info(\"%s%s\", dev.desc, append)\n\n    try:\n        if args.username and args.password and args.ip_address:\n            lupusec = lupupy.Lupusec(ip_address=args.ip_address,\n                                     username=args.username,\n                                     password=args.password)\n        \n        if args.arm:\n            if lupusec.get_alarm().set_away():\n                _LOGGER.info('Alarm mode changed to armed')\n            else:\n                _LOGGER.warning('Failed to change alarm mode to armed')\n        \n        if args.disarm:\n            if lupusec.get_alarm().set_standby():\n                _LOGGER.info('Alarm mode changed to disarmed')\n            else:\n                _LOGGER.warning('Failed to change alarm mode to disarmed')\n\n        if args.home:\n            if lupusec.get_alarm().set_home():\n                _LOGGER.info('Alarm mode changed to home')\n            else:\n                _LOGGER.warning('Failed to change alarm mode to home')\n            \n        if args.history:\n            _LOGGER.info(json.dumps(lupusec.get_history()['hisrows'], indent=4, sort_keys=True))\n\n        if args.status:\n            _LOGGER.info('Mode of panel: %s', lupusec.get_alarm().mode)\n        \n        if args.devices:\n            for device in lupusec.get_devices():\n                _devicePrint(device)\n                \n    except lupupy.LupusecException as exc:\n        _LOGGER.error(exc)\n    finally:\n        _LOGGER.info('--Finished running--')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_member_ibutton(self, val):\n        members = self.__con__.search_s(\n            CSHMember.__ldap_user_ou__,\n            ldap.SCOPE_SUBTREE,\n            \"(ibutton=%s)\" % val,\n            ['ipaUniqueID'])\n        if members:\n            return CSHMember(\n                    self,\n                    members[0][1]['ipaUniqueID'][0].decode('utf-8'),\n                    False)\n        return None", "response": "Returns a CSHMember object if the iButton ID of the member is the supplied value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_member_slackuid(self, slack):\n        members = self.__con__.search_s(\n            CSHMember.__ldap_user_ou__,\n            ldap.SCOPE_SUBTREE,\n            \"(slackuid=%s)\" % slack,\n            ['ipaUniqueID'])\n        if members:\n            return CSHMember(\n                    self,\n                    members[0][1]['ipaUniqueID'][0].decode('utf-8'),\n                    False)\n        return None", "response": "Returns a CSHMember object if the Slack UID of the member is equal to the Slack UID of the member."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the head of a directorship", "response": "def get_directorship_heads(self, val):\n        \"\"\"Get the head of a directorship\n\n        Arguments:\n        val -- the cn of the directorship\n        \"\"\"\n\n        __ldap_group_ou__ = \"cn=groups,cn=accounts,dc=csh,dc=rit,dc=edu\"\n\n        res = self.__con__.search_s(\n                __ldap_group_ou__,\n                ldap.SCOPE_SUBTREE,\n                \"(cn=eboard-%s)\" % val,\n                ['member'])\n\n        ret = []\n        for member in res[0][1]['member']:\n            try:\n                ret.append(member.decode('utf-8'))\n            except UnicodeDecodeError:\n                ret.append(member)\n            except KeyError:\n                continue\n\n        return [CSHMember(self,\n                dn.split('=')[1].split(',')[0],\n                True)\n                for dn in ret]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enqueue_mod(self, dn, mod):\n        # mark for update\n        if dn not in self.__pending_mod_dn__:\n            self.__pending_mod_dn__.append(dn)\n            self.__mod_queue__[dn] = []\n\n        self.__mod_queue__[dn].append(mod)", "response": "Enqueue a LDAP modification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflush all pending LDAP modifications.", "response": "def flush_mod(self):\n        \"\"\"Flush all pending LDAP modifications.\"\"\"\n        for dn in self.__pending_mod_dn__:\n            try:\n                if self.__ro__:\n                    for mod in self.__mod_queue__[dn]:\n                        if mod[0] == ldap.MOD_DELETE:\n                            mod_str = \"DELETE\"\n                        elif mod[0] == ldap.MOD_ADD:\n                            mod_str = \"ADD\"\n                        else:\n                            mod_str = \"REPLACE\"\n                        print(\"{} VALUE {} = {} FOR {}\".format(mod_str,\n                                                               mod[1],\n                                                               mod[2],\n                                                               dn))\n                else:\n                    self.__con__.modify_s(dn, self.__mod_queue__[dn])\n            except ldap.TYPE_OR_VALUE_EXISTS:\n                print(\"Error! Conflicting Batch Modification: %s\"\n                      % str(self.__mod_queue__[dn]))\n                continue\n            except ldap.NO_SUCH_ATTRIBUTE:\n                print(\"Error! Conflicting Batch Modification: %s\"\n                      % str(self.__mod_queue__[dn]))\n                continue\n            self.__mod_queue__[dn] = None\n        self.__pending_mod_dn__ = []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detect_encoding(value):\n    # https://tools.ietf.org/html/rfc4627#section-3\n    if six.PY2:\n        null_pattern = tuple(bool(ord(char)) for char in value[:4])\n    else:\n        null_pattern = tuple(bool(char) for char in value[:4])\n\n    encodings = {\n        # Zero is a null-byte, 1 is anything else.\n        (0, 0, 0, 1): 'utf-32-be',\n        (0, 1, 0, 1): 'utf-16-be',\n        (1, 0, 0, 0): 'utf-32-le',\n        (1, 0, 1, 0): 'utf-16-le',\n    }\n\n    return encodings.get(null_pattern, 'utf-8')", "response": "Returns the character encoding for a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge and encode query parameters with an URL.", "response": "def _merge_params(url, params):\n    \"\"\"Merge and encode query parameters with an URL.\"\"\"\n    if isinstance(params, dict):\n        params = list(params.items())\n\n    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)\n    url_params = urllib.parse.parse_qsl(query, keep_blank_values=True)\n    url_params.extend(params)\n\n    query = _encode_data(url_params)\n\n    return urllib.parse.urlunsplit((scheme, netloc, path, query, fragment))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef json(self, **kwargs):\n        encoding = detect_encoding(self.content[:4])\n        value = self.content.decode(encoding)\n\n        return simplejson.loads(value, **kwargs)", "response": "Decodes response as JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raise_for_status(self):\n        if 400 <= self.status_code < 600:\n            message = 'Error %s for %s' % (self.status_code, self.url)\n            raise HTTPError(message)", "response": "Raises HTTPError if the request got an error."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunpacks a text - io - wrapper object into a buffer and the encoding.", "response": "def unpack_text_io_wrapper(fp, encoding):\n  \"\"\"\n  If *fp* is a #io.TextIOWrapper object, this function returns the underlying\n  binary stream and the encoding of the IO-wrapper object. If *encoding* is not\n  None and does not match with the encoding specified in the IO-wrapper, a\n  #RuntimeError is raised.\n  \"\"\"\n\n  if isinstance(fp, io.TextIOWrapper):\n    if fp.writable() and encoding is not None and fp.encoding != encoding:\n      msg = 'TextIOWrapper.encoding({0!r}) != {1!r}'\n      raise RuntimeError(msg.format(fp.encoding, encoding))\n    if encoding is None:\n      encoding = fp.encoding\n    fp = fp.buffer\n\n  return fp, encoding"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noutputting all recorded metrics", "response": "def dump(cls):\n        \"\"\"Output all recorded metrics\"\"\"\n        with cls.lock:\n            if not cls.instances: return\n            atexit.unregister(cls.dump)\n\n            cls._pre_dump()\n\n            for self in cls.instances.values():\n                self._dump()\n\n            cls._post_dump()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dump(self):\n\n        try:\n            self.temp.seek(0) # seek to beginning\n            arr = np.fromfile(self.temp, self.dtype)\n            self.count_arr = arr['count']\n            self.elapsed_arr = arr['elapsed']\n\n            if self.calc_stats:\n                # calculate mean & standard deviation\n                self.count_mean = np.mean(self.count_arr)\n                self.count_std = np.std(self.count_arr)\n                self.elapsed_mean = np.mean(self.elapsed_arr)\n                self.elapsed_std = np.std(self.elapsed_arr)\n\n            self._output()\n        finally:\n            self.temp.close()\n            self._cleanup()", "response": "dump data for an individual metric. For internal use only."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, host_rec=None, service_rec=None, hostfilter=None):\n        return self.send.vuln_list(host_rec, service_rec, hostfilter)", "response": "Returns a list of vulnerabilities based on t_hosts. id or t_services. id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all IP Addresses with a vulnerability", "response": "def ip_info(self, vuln_name=None, vuln_id=None, ip_list_only=True, hostfilter=None):\n        \"\"\"\n        List of all IP Addresses with a vulnerability\n\n        :param vuln_name: t_vulndata.f_vulnid\n        :param vuln_id: t_vulndata.id\n        :param ip_list_only: IP List only (default) or rest of t_hosts fields\n        :param hostfilter: Valid hostfilter or none\n        :return: [(ip, hostname) ...] or [(ip, hostname, t_service_vulns.f_proof, t_service_vulns.f_status), ...]\n        \"\"\"\n        return self.send.vuln_ip_info(vuln_name, vuln_id, ip_list_only, hostfilter)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of vulns with services and IP Addresses", "response": "def service_list(self, vuln_name=None, vuln_id=None, hostfilter=None):\n        \"\"\"\n        Returns a dictionary of vulns with services and IP Addresses\n\n        :param vuln_name: t_vulndata.f_vulnid\n        :param vuln_id: t_vulndata.id\n        :param hostfilter: Valid hostfilter or none\n        :return: {'vuln-id': {'port': [ ip, hostname ]} ...} ...\n        \"\"\"\n        return self.send.vuln_service_list(vuln_name, vuln_id, hostfilter)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_code(mod_code, mod_name):\n    mod_obj = imp.new_module(mod_name)\n\n    mod_obj.__file__ = None\n\n    exec_(mod_code, mod_obj.__dict__, mod_obj.__dict__)\n\n    add_to_sys_modules(mod_name=mod_name, mod_obj=mod_obj)\n\n    return mod_obj", "response": "Create a module object by code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimporting a module by module name.", "response": "def import_name(mod_name):\n    \"\"\"Import a module by module name.\n\n    @param mod_name: module name.\n    \"\"\"\n    try:\n        mod_obj_old = sys.modules[mod_name]\n    except KeyError:\n        mod_obj_old = None\n\n    if mod_obj_old is not None:\n        return mod_obj_old\n\n    __import__(mod_name)\n\n    mod_obj = sys.modules[mod_name]\n\n    return mod_obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_path(mod_path, mod_name):\n    mod_code = open(mod_path).read()\n\n    mod_obj = import_code(\n        mod_code=mod_code,\n        mod_name=mod_name,\n    )\n\n    if not hasattr(mod_obj, '__file__'):\n        mod_obj.__file__ = mod_path\n\n    return mod_obj", "response": "Import a module by module file path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_obj(\n    uri,\n    mod_name=None,\n    mod_attr_sep='::',\n    attr_chain_sep='.',\n    retn_mod=False,\n):\n    \"\"\"Load an object from a module.\n\n    @param uri: an uri specifying which object to load.\n    An `uri` consists of two parts: module URI and attribute chain,\n     e.g. `a/b/c.py::x.y.z` or `a.b.c::x.y.z`\n\n    # Module URI\n    E.g.  `a/b/c.py` or `a.b.c`.\n    Can be either a module name or a file path.\n    Whether it is a file path is determined by whether it ends with `.py`.\n\n    # Attribute chain\n    E.g. `x.y.z`.\n\n    @param mod_name: module name.\n    Must be given when `uri` specifies a module file path, not a module name.\n\n    @param mod_attr_sep: the separator between module name and attribute name.\n\n    @param attr_chain_sep: the separator between parts of attribute name.\n\n    @retn_mod: whether return module object.\n    \"\"\"\n    if mod_attr_sep is None:\n        mod_attr_sep = '::'\n\n    uri_parts = split_uri(uri=uri, mod_attr_sep=mod_attr_sep)\n\n    protocol, mod_uri, attr_chain = uri_parts\n\n    if protocol == 'py':\n        mod_obj = import_name(mod_uri)\n\n    else:\n        if not mod_name:\n            msg = (\n                'Argument `mod_name` must be given when loading by file path.'\n            )\n\n            raise ValueError(msg)\n\n        mod_obj = import_path(mod_uri, mod_name=mod_name)\n\n    if not attr_chain:\n        if retn_mod:\n            return mod_obj, None\n        else:\n            return mod_obj\n\n    if attr_chain_sep is None:\n        attr_chain_sep = '.'\n\n    attr_obj = get_attr_chain(\n        obj=mod_obj,\n        attr_chain=attr_chain,\n        sep=attr_chain_sep,\n    )\n\n    if retn_mod:\n        return mod_obj, attr_obj\n    else:\n        return attr_obj", "response": "Load an object from a module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a module object to sys. modules.", "response": "def add_to_sys_modules(mod_name, mod_obj=None):\n    \"\"\"Add a module object to `sys.modules`.\n\n    @param mod_name: module name, used as key to `sys.modules`.\n    If `mod_name` is `a.b.c` while modules `a` and `a.b` are not existing,\n    empty modules will be created for `a` and `a.b` as well.\n\n    @param mod_obj: a module object.\n    If None, an empty module object will be created.\n    \"\"\"\n    mod_snames = mod_name.split('.')\n\n    parent_mod_name = ''\n\n    parent_mod_obj = None\n\n    for mod_sname in mod_snames:\n        if parent_mod_name == '':\n            current_mod_name = mod_sname\n        else:\n            current_mod_name = parent_mod_name + '.' + mod_sname\n\n        if current_mod_name == mod_name:\n            current_mod_obj = mod_obj\n        else:\n            current_mod_obj = sys.modules.get(current_mod_name, None)\n\n        if current_mod_obj is None:\n            current_mod_obj = imp.new_module(current_mod_name)\n\n        sys.modules[current_mod_name] = current_mod_obj\n\n        if parent_mod_obj is not None:\n            setattr(parent_mod_obj, mod_sname, current_mod_obj)\n\n        parent_mod_name = current_mod_name\n\n        parent_mod_obj = current_mod_obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_uri(uri, mod_attr_sep='::'):\n    uri_parts = uri.split(mod_attr_sep, 1)\n\n    if len(uri_parts) == 2:\n        mod_uri, attr_chain = uri_parts\n    else:\n        mod_uri = uri_parts[0]\n\n        attr_chain = None\n\n    if mod_uri.startswith('py://'):\n        protocol = 'py'\n\n        mod_uri = mod_uri[5:]\n\n    elif mod_uri.startswith('file://'):\n        protocol = 'file'\n\n        mod_uri = mod_uri[7:]\n\n    # If no protocol prefix is present, and the uri ends with `.py`, then\n    # consider the uri as module file path instead of module name.\n    elif mod_uri.endswith('.py'):\n        protocol = 'file'\n\n    else:\n        protocol = 'py'\n\n    info = (protocol, mod_uri, attr_chain)\n\n    return info", "response": "Split given URI into a tuple of protocol module URI and attribute chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_attr_chain(obj, attr_chain, sep='.'):\n    if sep is None:\n        sep = '.'\n\n    attr_names = attr_chain.split(sep)\n\n    new_obj = obj\n\n    for attr_name in attr_names:\n        new_obj = getattr(new_obj, attr_name)\n\n    return new_obj", "response": "Get the last attribute of given attribute chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fill_table_entry(self, row, col):\n        prefix = self._membership_query(row)\n        full_output = self._membership_query(row + col)\n        length = len(commonprefix([prefix, full_output]))\n        self.observation_table[row, col] = full_output[length:]", "response": "Fill an entry in the observation table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_in_hypothesis(self, mma, w_string, index):\n        state = mma[0]\n        for i in range(index):\n            for arc in state:\n                if mma.isyms.find(arc.ilabel) == w_string[i]:\n                    state = mma[arc.nextstate]\n                    s_index = arc.nextstate\n\n        # The id of the state is its index inside the Sm list\n        access_string = self.observation_table.sm_vector[s_index]\n        logging.debug(\n            'Access string for %d: %s - %d ',\n            index,\n            access_string,\n            s_index)\n\n        return access_string", "response": "Run the string in the hypothesis automaton for the given index steps and return the access string for the state reached concatanated with the\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_suffix(self, w_string, access_string, index):\n        prefix_as = self._membership_query(access_string)\n        full_as = self._membership_query(access_string + w_string[index:])\n\n        prefix_w = self._membership_query(w_string[:index])\n        full_w = self._membership_query(w_string)\n\n        length = len(commonprefix([prefix_as, full_as]))\n        as_suffix = full_as[length:]\n\n        length = len(commonprefix([prefix_w, full_w]))\n        w_suffix = full_w[length:]\n\n        if as_suffix != w_suffix:\n            logging.debug('Access string state incorrect')\n            return True\n        logging.debug('Access string state correct.')\n        return False", "response": "Checks if the suffix of the access string matches with the examined string suffix\n            Returns a boolean valuei indicating if matching was successful otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking for bad transitions using the examined string w_string.", "response": "def _find_bad_transition(self, mma, w_string):\n        \"\"\"\n        Checks for bad DFA transitions using the examined string\n        Args:\n            mma (DFA): The hypothesis automaton\n            w_string (str): The examined string to be consumed\n        Returns:\n            str: The prefix of the examined string that matches\n        \"\"\"\n        conj_out = mma.consume_input(w_string)\n        targ_out = self._membership_query(w_string)\n        # TODO: handle different length outputs from conjecture and target\n        # hypothesis.\n        length = min(len(conj_out), len(targ_out))\n        diff = [i for i in range(length)\n                if conj_out[i] != targ_out[i]]\n        if len(diff) == 0:\n            diff_index = len(targ_out)\n        else:\n            diff_index = diff[0]\n\n        low = 0\n        high = len(w_string)\n        while True:\n            i = (low + high) / 2\n            length = len(self._membership_query(w_string[:i]))\n            if length == diff_index + 1:\n                return w_string[:i]\n            elif length < diff_index + 1:\n                low = i + 1\n            else:\n                high = i - 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a counterexample in the Rivest - Schapire way.", "response": "def _process_counter_example(self, mma, w_string):\n        \"\"\"\"\n       Process a counterexample in the Rivest-Schapire way.\n       Args:\n           mma (DFA): The hypothesis automaton\n           w_string (str): The examined string to be consumed\n       Returns:\n           None\n        \"\"\"\n        w_string = self._find_bad_transition(mma, w_string)\n\n        diff = len(w_string)\n        same = 0\n        while True:\n            i = (same + diff) / 2\n            access_string = self._run_in_hypothesis(mma, w_string, i)\n            is_diff = self._check_suffix(w_string, access_string, i)\n            if is_diff:\n                diff = i\n            else:\n                same = i\n            if diff - same == 1:\n                break\n        exp = w_string[diff:]\n\n        self.observation_table.em_vector.append(exp)\n        for row in self.observation_table.sm_vector + self.observation_table.smi_vector:\n            self._fill_table_entry(row, exp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ot_make_closed(self, access_string):\n        self.observation_table.sm_vector.append(access_string)\n        for i in self.alphabet:\n            self.observation_table.smi_vector.append(access_string + i)\n            for e in self.observation_table.em_vector:\n                self._fill_table_entry(access_string + i, e)", "response": "This method is used to make a new state in the state machine closed by the other methods."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_mealy_conjecture(self):\n        mma = MealyMachine()\n        for s in self.observation_table.sm_vector:\n            for i in self.alphabet:\n                dst = self.observation_table.equiv_classes[s + i]\n                # If dst == None then the table is not closed.\n                if dst is None:\n                    logging.debug('Conjecture attempt on non closed table.')\n                    return None\n                o = self.observation_table[s, i]\n                src_id = self.observation_table.sm_vector.index(s)\n                dst_id = self.observation_table.sm_vector.index(dst)\n                mma.add_arc(src_id, dst_id, i, o)\n\n        # This works only for Mealy machines\n        for s in mma.states:\n            s.final = True\n        return mma", "response": "This method returns a Mealy Machine object for the given set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_table(self):\n        self.observation_table.sm_vector.append('')\n        self.observation_table.smi_vector = list(self.alphabet)\n        self.observation_table.em_vector = list(self.alphabet)\n\n        for i in self.observation_table.em_vector:\n            self._fill_table_entry('', i)\n\n        for s, e in product(self.observation_table.smi_vector, self.observation_table.em_vector):\n            self._fill_table_entry(s, e)", "response": "Initialize the observation table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimplementing the high level loop of the algorithm for learning a Mealy machine. Args: None Returns: MealyMachine: The learned mealy machine", "response": "def learn_mealy_machine(self):\n        \"\"\"\n        Implements the high level loop of the algorithm for learning a\n        Mealy machine.\n        Args:\n            None\n        Returns:\n            MealyMachine: The learned mealy machine\n        \"\"\"\n        logging.info('Initializing learning procedure.')\n        self._init_table()\n\n        logging.info('Generating a closed and consistent observation table.')\n        while True:\n\n            closed = False\n            # Make sure that the table is closed and consistent\n            while not closed:\n\n                logging.debug('Checking if table is closed.')\n                closed, string = self.observation_table.is_closed()\n                if not closed:\n                    logging.debug('Closing table.')\n                    self._ot_make_closed(string)\n                else:\n                    logging.debug('Table closed.')\n\n            # Create conjecture\n            mma = self.get_mealy_conjecture()\n\n            logging.info('Generated conjecture machine with %d states.',\n                         len(list(mma.states)))\n\n            # _check correctness\n            logging.debug('Running equivalence query.')\n            found, counter_example = self._equivalence_query(mma)\n\n            # Are we done?\n            if found:\n                logging.info('No counterexample found. Hypothesis is correct!')\n                break\n\n            # Add the new experiments into the table to reiterate the\n            # learning loop\n            logging.info(\n                'Processing counterexample %input_string with length %d.',\n                counter_example,\n                len(counter_example))\n            self._process_counter_example(mma, counter_example)\n\n        logging.info('Learning complete.')\n        return mma"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning only proper HTTP headers.", "response": "def get_headers(environ):\n    \"\"\"\n    Returns only proper HTTP headers.\n    \"\"\"\n    for key, value in environ.iteritems():\n        key = str(key)\n        if key.startswith('HTTP_') and key not in \\\n           ('HTTP_CONTENT_TYPE', 'HTTP_CONTENT_LENGTH'):\n            yield key[5:].replace('_', '-').title(), value\n        elif key in ('CONTENT_TYPE', 'CONTENT_LENGTH'):\n            yield key.replace('_', '-').title(), value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_host(environ):\n    scheme = environ.get('wsgi.url_scheme')\n    if 'HTTP_X_FORWARDED_HOST' in environ:\n        result = environ['HTTP_X_FORWARDED_HOST']\n    elif 'HTTP_HOST' in environ:\n        result = environ['HTTP_HOST']\n    else:\n        result = environ['SERVER_NAME']\n        if (scheme, str(environ['SERVER_PORT'])) not \\\n           in (('https', '443'), ('http', '80')):\n            result += ':' + environ['SERVER_PORT']\n    if result.endswith(':80') and scheme == 'http':\n        result = result[:-3]\n    elif result.endswith(':443') and scheme == 'https':\n        result = result[:-4]\n    return result", "response": "Returns the real host for the given WSGI environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_library(lib_files):\n    tracks, playlists = lib_files\n    lib = MusicLibrary()\n    lib_length = len(tracks)\n    i = 0\n\n    writer = lib.ix.writer()\n    previous_procent_done_str = \"\"\n    for f in tracks:\n        track_info = TrackInfo(f)\n        lib.add_track_internal(track_info, writer)\n        current_percent_done_str = \"%d%%\" % (i / lib_length * 100)\n        if current_percent_done_str != previous_procent_done_str:\n            logs.print_info(\"Analizowanie biblioteki muzycznej... \" + current_percent_done_str)\n            previous_procent_done_str = current_percent_done_str\n        i += 1.0\n    logs.print_info(\"Analizowanie playlist...\")\n    for f in playlists:\n        with open(f, 'r') as fo:\n            playlist_dict = loads(fo.read())\n            playlist = Playlist(lib, f, playlist_dict['title'], playlist_dict['tracks'])\n            lib.add_playlist(playlist)\n    writer.commit()\n    logs.print_info(\"Optymalizacja index-u...\")\n    lib.ix.optimize()\n    return lib", "response": "Zwraca pliki podane w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie w li\u015bcie"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the full subgraph of the object graph between the given set of objects.", "response": "def full_subgraph(self, objects):\n        \"\"\"\n        Return the subgraph of this graph whose vertices\n        are the given ones and whose edges are the edges\n        of the original graph between those vertices.\n\n        \"\"\"\n        vertices = ElementTransformSet(transform=id)\n        out_edges = KeyTransformDict(transform=id)\n        in_edges = KeyTransformDict(transform=id)\n        for obj in objects:\n            vertices.add(obj)\n            out_edges[obj] = []\n            in_edges[obj] = []\n\n        edges = set()\n        head = {}\n        tail = {}\n\n        for referrer in vertices:\n            for edge in self._out_edges[referrer]:\n                referent = self._head[edge]\n                if referent not in vertices:\n                    continue\n                edges.add(edge)\n                tail[edge] = referrer\n                head[edge] = referent\n                out_edges[referrer].append(edge)\n                in_edges[referent].append(edge)\n\n        return ObjectGraph._raw(\n            vertices=vertices,\n            edges=edges,\n            out_edges=out_edges,\n            in_edges=in_edges,\n            head=head,\n            tail=tail,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _raw(cls, vertices, edges, out_edges, in_edges, head, tail):\n        self = object.__new__(cls)\n        self._out_edges = out_edges\n        self._in_edges = in_edges\n        self._head = head\n        self._tail = tail\n        self._vertices = vertices\n        self._edges = edges\n        return self", "response": "Private constructor for direct construction\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _from_objects(cls, objects):\n        vertices = ElementTransformSet(transform=id)\n        out_edges = KeyTransformDict(transform=id)\n        in_edges = KeyTransformDict(transform=id)\n        for obj in objects:\n            vertices.add(obj)\n            out_edges[obj] = []\n            in_edges[obj] = []\n\n        # Edges are identified by simple integers, so\n        # we can use plain dictionaries for mapping\n        # edges to their heads and tails.\n        edge_label = itertools.count()\n        edges = set()\n        head = {}\n        tail = {}\n\n        for referrer in vertices:\n            for referent in gc.get_referents(referrer):\n                if referent not in vertices:\n                    continue\n                edge = next(edge_label)\n                edges.add(edge)\n                tail[edge] = referrer\n                head[edge] = referent\n                out_edges[referrer].append(edge)\n                in_edges[referent].append(edge)\n\n        return cls._raw(\n            vertices=vertices,\n            edges=edges,\n            out_edges=out_edges,\n            in_edges=in_edges,\n            head=head,\n            tail=tail,\n        )", "response": "Private constructor to create a graph from a list of Python objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef annotated(self):\n        # Build up dictionary of edge annotations.\n        edge_annotations = {}\n        for edge in self.edges:\n            if edge not in edge_annotations:\n                # We annotate all edges from a given object at once.\n                referrer = self._tail[edge]\n                known_refs = annotated_references(referrer)\n                for out_edge in self._out_edges[referrer]:\n                    referent = self._head[out_edge]\n                    if known_refs[referent]:\n                        annotation = known_refs[referent].pop()\n                    else:\n                        annotation = None\n                    edge_annotations[out_edge] = annotation\n\n        annotated_vertices = [\n            AnnotatedVertex(\n                id=id(vertex),\n                annotation=object_annotation(vertex),\n            )\n            for vertex in self.vertices\n        ]\n\n        annotated_edges = [\n            AnnotatedEdge(\n                id=edge,\n                annotation=edge_annotations[edge],\n                head=id(self._head[edge]),\n                tail=id(self._tail[edge]),\n            )\n            for edge in self.edges\n        ]\n\n        return AnnotatedGraph(\n            vertices=annotated_vertices,\n            edges=annotated_edges,\n        )", "response": "Annotate this graph returning an AnnotatedGraph object with the same structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_image(self, filename='refcycle.png', format=None,\n                     dot_executable='dot'):\n        \"\"\"\n        Export graph as an image.\n\n        This requires that Graphviz is installed and that the ``dot``\n        executable is in your path.\n\n        The *filename* argument specifies the output filename.\n\n        The *format* argument lets you specify the output format.  It may be\n        any format that ``dot`` understands, including extended format\n        specifications like ``png:cairo``.  If omitted, the filename extension\n        will be used; if no filename extension is present, ``png`` will be\n        used.\n\n        The *dot_executable* argument lets you provide a full path to the\n        ``dot`` executable if necessary.\n\n        \"\"\"\n        return self.annotated().export_image(\n            filename=filename,\n            format=format,\n            dot_executable=dot_executable,\n        )", "response": "Export the current state of the species as an image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef owned_objects(self):\n        return (\n            [\n                self,\n                self.__dict__,\n                self._head,\n                self._tail,\n                self._out_edges,\n                self._out_edges._keys,\n                self._out_edges._values,\n                self._in_edges,\n                self._in_edges._keys,\n                self._in_edges._values,\n                self._vertices,\n                self._vertices._elements,\n                self._edges,\n            ] +\n            list(six.itervalues(self._out_edges)) +\n            list(six.itervalues(self._in_edges))\n        )", "response": "Returns a list of gc - tracked objects owned by this objectGraph instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_by_typename(self, typename):\n        return self.find_by(lambda obj: type(obj).__name__ == typename)", "response": "Returns a list of all objects whose type has the given name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_input(self, key, value):\n        if key not in self._inputs:\n            raise InputException(\"Key {0} is not a valid input!\".format(key))\n        self._inputs[key].value = value", "response": "Sets the value of the input with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_input(self, key, force=False):\n        if key not in self._inputs:\n            raise InputException(\"Key {0} is not a valid input!\".format(key))\n\n        if self._inputs[key].prompt:\n            prompt = self._inputs[key].prompt\n        elif self._inputs[key].is_bool():\n            prompt = \"{0}?\".format(key)\n        else:\n            prompt = \"please enter your {0}\".format(key)\n        help_text = self._inputs[key].help if hasattr(self._inputs[key], 'help') else None\n\n        if self._inputs[key].value is EMPTY or force:\n\n            default_value = None\n            if self._inputs[key].default is not EMPTY:\n                default_value = self._inputs[key].default\n            if self._inputs[key].value is not EMPTY:\n                default_value = self._inputs[key].value\n\n            input_value = EMPTY\n            while input_value is EMPTY or input_value == '?':\n                if input_value == '?' and help_text:\n                    print(help_text)\n                input_value = lib.prompt(\n                    prompt,\n                    default=default_value,\n                    bool_type=self._inputs[key].in_type,\n                    secret=self._inputs[key].is_secret)\n            self._inputs[key].value = input_value\n\n        return self._inputs[key].value", "response": "Get the value of key from the user s input list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a set of unset inputs", "response": "def get_unset_inputs(self):\n        \"\"\" Return a set of unset inputs \"\"\"\n        return set([k for k, v in self._inputs.items() if v.is_empty(False)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prompt_unset_inputs(self, force=False):\n        for k, v in self._inputs.items():\n            if force or v.is_empty(False):\n                self.get_input(k, force=force)", "response": "Prompt for unset input values"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the values dictionary", "response": "def values(self, with_defaults=True):\n        \"\"\" Return the values dictionary, defaulting to default values \"\"\"\n        return dict(((k, str(v)) for k, v in self._inputs.items() if not v.is_empty(with_defaults)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_values(self):\n        return dict(((k, v.value) for k, v in self._inputs.items() if not v.is_secret and not v.is_empty(False)))", "response": "Return the dictionary with which to write values"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd inputs from the input string format", "response": "def add_inputs_from_inputstring(self, input_string):\n        \"\"\"\n        Add inputs using the input string format:\n\n        gitroot==~/workspace\n        username\n        password?\n        main_branch==comp_main\n        \"\"\"\n        raw_params = input_string.split('\\n')\n        param_attributes = (self._parse_param_line(rp) for rp in raw_params if len(rp.strip(' \\t')) > 0)\n        for param, attributes in param_attributes:\n            self.add_input(param, attributes)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_param_line(self, line):\n        value = line.strip('\\n \\t')\n        if len(value) > 0:\n            i = Input()\n            if value.find('#') != -1:\n                value, extra_attributes = value.split('#')\n                try:\n                    extra_attributes = eval(extra_attributes)\n                except SyntaxError:\n                    raise InputException(\"Incorrectly formatted input for {0}!\".format(value))\n                if not isinstance(extra_attributes, dict):\n                    raise InputException(\"Incorrectly formatted input for {0}!\".format(value))\n                if 'prompt' in extra_attributes:\n                    i.prompt = extra_attributes['prompt']\n                if 'help' in extra_attributes:\n                    i.help = extra_attributes['help']\n                if 'type' in extra_attributes:\n                    i.in_type = extra_attributes['type']\n                    if i.in_type.find('/') != -1:\n                        i.in_type, i.out_type = i.in_type.split('/')\n                if 'cast' in extra_attributes:\n                    i.out_type = extra_attributes['cast']\n            if value.find('==') != -1:\n                value, default = value.split('==')\n                i.default = default\n            if value.endswith('?'):\n                value = value[:-1]\n                i.is_secret = True\n            return (value, i)\n        return None", "response": "Parse a single param line."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the first CSV file found in the given ZIP file to the given destination file.", "response": "def extract_csv(zip_path, destination):\n    \"\"\"\n    Extract the first CSV file found in the given ``zip_path`` ZIP file to the\n    ``destination`` file. Raises :class:`LookupError` if no CSV file can be\n    found in the ZIP.\n    \"\"\"\n    with zipfile.ZipFile(zip_path) as zf:\n        member_to_unzip = None\n        for member in zf.namelist():\n            if member.endswith('.csv'):\n                member_to_unzip = member\n                break\n\n        if not member_to_unzip:\n            raise LookupError(\n                \"Couldn't find any CSV file in the archive\"\n            )\n\n        with zf.open(member_to_unzip) as zfp, \\\n                open(destination, 'wb') as dfp:\n            dfp.write(zfp.read())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download(self, overwrite=True):\n        if overwrite or not os.path.exists(self.file_path):\n            _, f = tempfile.mkstemp()\n            try:\n                urlretrieve(self.DOWNLOAD_URL, f)\n                extract_csv(f, self.file_path)\n            finally:\n                os.remove(f)", "response": "Download the zipcodes CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the zipcodes mapping as a list of Location dicts.", "response": "def get_locations(self):\n        \"\"\"\n        Return the zipcodes mapping as a list of ``{zipcode: location}`` dicts.\n        The zipcodes file will be downloaded if necessary.\n        \"\"\"\n        if not self.zipcode_mapping:\n            self.download(overwrite=False)\n\n            zipcode_mapping = {}\n            with UnicodeReader(self.file_path, delimiter=';', encoding='latin1') as csv_reader:\n                # Skip header\n                next(csv_reader)\n                for line in csv_reader:\n                    zipcode_mapping[int(line[1])] = Location(\n                        official_name=line[0],\n                        canton=line[5],\n                        municipality=line[3]\n                    )\n            self.zipcode_mapping = zipcode_mapping\n\n        return self.zipcode_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_zipcodes_for_canton(self, canton):\n        zipcodes = [\n            zipcode for zipcode, location in self.get_locations().items()\n            if location.canton == canton\n        ]\n\n        return zipcodes", "response": "Returns the list of zipcodes for the given canton code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of unique cantons sorted by name.", "response": "def get_cantons(self):\n        \"\"\"\n        Return the list of unique cantons, sorted by name.\n        \"\"\"\n        return sorted(list(set([\n            location.canton for location in self.get_locations().values()\n        ])))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of unique municipalities sorted by name.", "response": "def get_municipalities(self):\n        \"\"\"\n        Return the list of unique municipalities, sorted by name.\n        \"\"\"\n        return sorted(list(set([\n            location.municipality for location in self.get_locations().values()\n        ])))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the term_vector of the object", "response": "def term_vector(self, params):\n        '''\n        params are either True/False, 'with_offsets', 'with_positions', 'with_positions_offsets'\n        '''\n        if params == True:\n            self[self.field]['term_vector'] = 'yes'\n        elif params == False:\n            self[self.field]['term_vector'] = 'no'\n        else:\n            self[self.field]['term_vector'] = params\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a formula class object from the formula module if it exists create one if it doesn t exist", "response": "def _get_formula_class(self, formula):\n        \"\"\"\n        get a formula class object if it exists, else\n        create one, add it to the dict, and pass return it.\n        \"\"\"\n        # recursive import otherwise\n        from sprinter.formula.base import FormulaBase\n        if formula in LEGACY_MAPPINGS:\n            formula = LEGACY_MAPPINGS[formula]\n        formula_class, formula_url = formula, None\n        if ':' in formula:\n            formula_class, formula_url = formula.split(\":\", 1)\n        if formula_class not in self._formula_dict:\n            try:\n                self._formula_dict[formula_class] = lib.get_subclass_from_module(formula_class, FormulaBase)\n            except (SprinterException, ImportError):\n                logger.info(\"Downloading %s...\" % formula_class)\n                try:\n                    self._pip.install_egg(formula_url or formula_class)\n                    try:\n                        self._formula_dict[formula_class] = lib.get_subclass_from_module(formula_class, FormulaBase)\n                    except ImportError:\n                        logger.debug(\"FeatureDict import Error\", exc_info=sys.exc_info())\n                        raise SprinterException(\"Error: Unable to retrieve formula %s!\" % formula_class)\n                except PipException:\n                    logger.error(\"ERROR: Unable to download %s!\" % formula_class)\n        return self._formula_dict[formula_class]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if given class supports back up.", "response": "def is_backup_class(cls):\n    \"\"\"Return true if given class supports back up. Currently this means a\n    gludb.data.Storable-derived class that has a mapping as defined in\n    gludb.config\"\"\"\n    return True if (\n        isclass(cls) and\n        issubclass(cls, Storable) and\n        get_mapping(cls, no_mapping_ok=True)\n    ) else False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_package(\n        self,\n        pkg_name,\n        recurse=True,\n        include_bases=True,\n        parent_pkg=None\n    ):\n        \"\"\"Add all classes to the backup in the specified package (including\n        all modules and all sub-packages) for which is_backup_class returns\n        True. Note that self.add_class is used, so base classes will added as\n        well.\n\n        Parameters:\n        * pkg_name - a string representing the package name. It may be\n          relative _if_ parent_pkg is supplied as well\n        * recurse - (default value of True) if False, sub-packages will _not_\n          be examined\n        * include_bases - (default value of True) is passed directly to\n          add_class for every class added\n        * parent_pkg - a string representing the parent package of the relative\n          package specified in pkg_name. Note that you should specify\n          parent_pkg _only_ if pkg_name should be interpreted as relative\n\n        An an example of both relative and absolute package imports, these\n        are equivalent:\n\n        ````\n        backup.add_package('toppackage.subpackage')\n        backup.add_package('subpackage', parent_pkg='toppackage')\n        ````\n        \"\"\"\n        if parent_pkg:\n            pkg = import_module('.' + pkg_name, parent_pkg)\n        else:\n            pkg = import_module(pkg_name)\n\n        for module_loader, name, ispkg in pkgutil.walk_packages(pkg.__path__):\n            if not ispkg:\n                # Module\n                mod = import_module('.' + name, pkg_name)\n                for name, member in getmembers(mod):\n                    if is_backup_class(member):\n                        self.add_class(member, include_bases=include_bases)\n            elif recurse:\n                # Package and we're supposed to recurse\n                self.add_package(\n                    pkg_name + '.' + name,\n                    recurse=True,\n                    include_bases=include_bases,\n                    parent_pkg=parent_pkg\n                )", "response": "Add all classes from pkg_name to self."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_class(self, cls, include_bases=True):\n        if not is_backup_class(cls):\n            return 0\n\n        added = 0\n\n        cls_name = backup_name(cls)\n        if cls_name not in self.classes:\n            self.classes[cls_name] = cls\n            self.log(\"Added class for backup: %s\", cls_name)\n            added = 1\n\n        if include_bases:\n            for candidate_cls in getmro(cls):\n                if is_backup_class(cls):\n                    # Note that we don't keep recursing on base classes\n                    added += self.add_class(candidate_cls, include_bases=False)\n\n        return added", "response": "Add the specified class to the backup."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(self, entry, *args):\n        if args:\n            entry = entry % args\n        self.backup_log.append(entry)", "response": "Append the string supplied to the log"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_backup(self):\n        self.log(\"Starting backup at %s\", now_field())\n        self.log(\"Backup config object created at %s\", self.timestamp)\n\n        # Make sure we're good to go\n        for fld in ['aws_access_key', 'aws_secret_key', 'bucketname']:\n            val = getattr(self, fld, None)\n            if not val:\n                self.log(\"Backup cannot start: %s is a required field\", fld)\n                raise ValueError(self.backup_log[-1])\n\n        # Start the compressed tarball our data is stored in\n        backup_file = NamedTemporaryFile(suffix=\".tar.gz\")\n        backup_tarfile = tarfile.open(fileobj=backup_file, mode='w:gz')\n\n        for cls_name, cls in self.classes.items():\n            self.log(\"Backing up %s\", cls_name)\n\n            rec_count = 0\n\n            with NamedTemporaryFile() as record_file:\n                for rec in cls.find_all():\n                    write_line(record_file, rec.to_data())\n                    rec_count += 1\n\n                record_file.flush()\n                backup_tarfile.add(record_file.name, arcname=cls_name+'.json')\n\n            self.log(\"%s => %d records backed up\", cls_name, rec_count)\n\n        # Finalize archive\n        backup_tarfile.close()\n        backup_file.flush()\n        backup_size = os.stat(backup_file.name)[6]\n\n        # Figure out key name for archived file\n        key_name = ('Backup_' + now_field() + '.tar.gz').replace(':', '_')\n\n        # upload archive to s3\n        if os.environ.get('DEBUG', False) or os.environ.get('travis', False):\n            # Local or CI - connect to our mock s3 service\n            conn = S3Connection(\n                '', '',\n                is_secure=False, port=8888, host='localhost',\n                calling_format=OrdinaryCallingFormat()\n            )\n        else:\n            conn = S3Connection(self.aws_access_key, self.aws_secret_key)\n\n        bucket = conn.get_bucket(self.bucketname)\n        key = Key(bucket)\n        key.key = key_name\n\n        self.log(\n            \"Sending %s [size=%d bytes] with key name %s\",\n            backup_file.name,\n            backup_size,\n            key_name\n        )\n\n        # TODO: should probably look into a multi-part upload for larger backup\n        key.set_contents_from_filename(backup_file.name)\n        self.log(\"Sent %s\", backup_file.name)\n\n        # All done\n        backup_file.close()\n        self.log(\"Backup completed\")\n\n        # return the bucket name and key name for the completed backup\n        return self.bucketname, key_name", "response": "This function is called by backup_config to create a backup of the current instance of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy_dir(sou_dir, dst_dir, del_dst=False, del_subdst=False):\n    if del_dst and os.path.isdir(del_dst):\n        shutil.rmtree(dst_dir)\n    os.makedirs(dst_dir, exist_ok=True)\n    for cur_file in list_dir(sou_dir):\n        dst_file = os.path.join(dst_dir, cur_file)\n        cur_file = os.path.join(sou_dir, cur_file)\n        if os.path.isdir(cur_file):\n            if del_subdst and os.path.isdir(dst_file):\n                shutil.rmtree(dst_file)\n            os.makedirs(dst_file, exist_ok=True)\n            copy_dir(cur_file, dst_file)\n        else:\n            shutil.copyfile(cur_file, dst_file)", "response": "Copy the contents of the sou_dir to dst_dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all files in path.", "response": "def get_files(path, ext=[], include=True):\n    \"\"\"\u904d\u5386\u63d0\u4f9b\u7684\u6587\u4ef6\u5939\u7684\u6240\u6709\u5b50\u6587\u4ef6\u5939\uff0c\u996d\u540e\u751f\u6210\u5668\u5bf9\u8c61\u3002\n\n    :param str path: \u5f85\u5904\u7406\u7684\u6587\u4ef6\u5939\u3002\n    :param list ext: \u6269\u5c55\u540d\u5217\u8868\u3002\n    :param bool include:    \u82e5\u503c\u4e3a True\uff0c\u4ee3\u8868 ext \u63d0\u4f9b\u7684\u662f\u5305\u542b\u5217\u8868\uff1b\n                            \u5426\u5219\u662f\u6392\u9664\u5217\u8868\u3002\n    :returns: \u4e00\u4e2a\u751f\u6210\u5668\u5bf9\u8c61\u3002 \n\n    \"\"\"\n    has_ext = len(ext)>0\n    for p, d, fs in os.walk(path):\n        for f in fs:\n            if has_ext:\n                in_ext = False\n                for name in ext:\n                    if f.endswith(name):\n                        in_ext = True\n                        break\n                if (include and in_ext) or \\\n                (not include and not in_ext):\n                    yield os.path.join(p,f)\n            else:\n                yield os.path.join(p, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_file(file_path, **kws):\n    kw = {\"mode\":\"r\", \"encoding\":\"utf-8\"}\n    if kws:\n        for k,v in kws.items():\n            kw[k] = v\n    with open(file_path, **kw) as afile:\n        txt = afile.read()\n    return txt", "response": "read file and return it as string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_file(file_path, txt, **kws):\n    if not os.path.exists(file_path):\n        upDir = os.path.dirname(file_path)\n        if not os.path.isdir(upDir):\n            os.makedirs(upDir)\n\n    kw = {\"mode\":\"w\", \"encoding\":\"utf-8\"}\n    if kws:\n        for k,v in kws.items():\n            kw[k] = v\n    with open(file_path, **kw) as afile:\n        afile.write(txt)", "response": "Write a file to the file_path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite by templ to file", "response": "def write_by_templ(templ, target, sub_value, safe=False):\n    \"\"\"\u6839\u636e\u6a21\u7248\u5199\u5165\u6587\u4ef6\u3002\n\n    :param str templ: \u6a21\u7248\u6587\u4ef6\u6240\u5728\u8def\u5f84\u3002\n    :param str target: \u8981\u5199\u5165\u7684\u6587\u4ef6\u6240\u5728\u8def\u5f84\u3002\n    :param dict sub_value: \u88ab\u66ff\u6362\u7684\u5185\u5bb9\u3002\n\n    \"\"\"\n    templ_txt = read_file(templ)\n    txt = None\n    if safe:\n        txt = Template(templ_txt).safe_substitute(sub_value)\n    else:\n        txt = Template(templ_txt).substitute(sub_value)\n    write_file(target, txt)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget MD5 of file", "response": "def get_md5(path):\n    \"\"\"\u83b7\u53d6\u6587\u4ef6\u7684 MD5 \u503c\u3002\n\n    :param str path: \u6587\u4ef6\u8def\u5f84\u3002\n    :returns: MD5 \u503c\u3002\n    :rtype: str\n\n    \"\"\"\n    with open(path,'rb') as f:\n        md5obj = hashlib.md5()\n        md5obj.update(f.read())\n        return md5obj.hexdigest()\n    raise FileNotFoundError(\"Error when get md5 for %s!\"%path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_zip(files, trim_arcname=None, target_file=None, **zipfile_args):\n    zipname = None\n    azip = None\n    if not target_file:\n        azip = tempfile.NamedTemporaryFile(mode='wb', delete=False)\n        zipname = azip.name\n    else:\n        azip = target_file\n        zipname = target_file.name if hasattr(azip, 'read') else azip\n    slog.info('Package %d files to \"%s\"'%(len(files), azip.name))\n    fileNum = len(files)\n    curFile = 0\n    zipfile_args['mode'] = 'w'\n    if not zipfile_args.get('compression'):\n        zipfile_args['compression'] = zipfile.ZIP_DEFLATED\n    with zipfile.ZipFile(azip, **zipfile_args) as zipf:\n       for f in files:\n           percent = round(curFile/fileNum*100)\n           sys.stdout.write('\\r%d%%'%(percent))\n           sys.stdout.flush()\n           zipf.write(f, f[trim_arcname:] if trim_arcname else None )\n           curFile = curFile+1\n\n       sys.stdout.write('\\r100%\\n')\n       sys.stdout.flush()\n\n    if hasattr(azip, 'close'):\n        azip.close()\n    return zipname", "response": "Create a zip file from a list of files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget max ver of rookout", "response": "def get_max_ver(fmt, filelist):\n    \"\"\"\u6709\u4e00\u5806\u5b57\u7b26\u4e32\uff0c\u6587\u4ef6\u540d\u5747\u5305\u542b %d.%d.%d \u5f62\u5f0f\u7248\u672c\u53f7\uff0c\u8fd4\u56de\u5176\u4e2d\u7248\u672c\u53f7\u6700\u5927\u7684\u90a3\u4e2a\u3002\n    \u6211\u4e00\u822c\u7528\u5b83\u6765\u68c0\u6d4b\u4e00\u5806\u53d1\u884c\u7248\u4e2d\u7248\u672c\u53f7\u6700\u5927\u7684\u90a3\u4e2a\u6587\u4ef6\u3002\n\n    :param str fmt: \u8981\u68c0\u6d4b\u6d4b\u5b57\u7b26\u4e32\u5f62\u5f0f\uff0c\u4f8b\u5982 rookout-%s.tar.gz \uff0c\u5176\u4e2d %s \u4f1a\u88ab\u6b63\u5219\u66ff\u6362\u3002\n    :param list files: \u5b57\u7b26\u4e32\u5217\u8868\u3002\n    :returns: \u7248\u672c\u53f7\u6700\u5927\u7684\u5b57\u7b26\u4e32\u3002\n    :rtype: str\n\n    \"\"\"\n    x, y, z = 0,0,0\n    verpat = fmt%'(\\d+).(\\d+).(\\d+)'\n    verre = re.compile(r''+verpat+'', re.M) \n    for f in filelist:\n        match = verre.search(f)\n        if match:\n            x1 = int(match.group(1))\n            y1 = int(match.group(2))\n            z1 = int(match.group(3))\n            if x1 >= x and y1 >= y:\n                x = x1\n                y = y1\n                z = z1\n    verfmt = fmt%('%d.%d.%d')\n    name = verfmt%(x, y, z)\n    if x == 0 and y == 0 and z == 0:\n        slog.info('Can not find the string \"%s\" !'%name)\n        return None\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_dicts(d1, d2):\n    for k in set(d1.keys()).union(d2.keys()):\n        if k in d1 and k in d2:\n            if isinstance(d1[k], dict) and isinstance(d2[k], dict):\n                yield (k, dict(merge_dicts(d1[k], d2[k])))\n            elif isinstance(d1[k], list):\n                if isinstance(d2[k], list):\n                    d1[k].extend(d2[k])\n                else:\n                    d1[k].append(d2[k])\n                yield(k, d1)\n            else:\n                # If one of the values is not a dict, you can't continue merging it.\n                # Value from second dict overrides one in first and we move on.\n                yield (k, d2[k])\n                # Alternatively, replace this with exception raiser to alert you of value conflicts\n        elif k in d1:\n            yield (k, d1[k])\n        else:\n            yield (k, d2[k])", "response": "Merge two dict objects into a single list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(\n    hw_num: int,\n    problems_to_do: Optional[Iterable[int]] = None,\n    prefix: Optional[Path] = None,\n    by_hand: Optional[Iterable[int]] = None,\n) -> None:\n    \"\"\"Process the homework problems in ``prefix`` folder.\n\n    Arguments\n    ---------\n    hw_num\n        The number of this homework\n    problems_to_do, optional\n        A list of the problems to be processed\n    prefix, optional\n        A `~pathlib.Path` to this homework assignment folder\n    by_hand, optional\n        A list of the problems that should be labeled to be completed\n        by hand and have an image with the solution included.\n\n    \"\"\"\n    if prefix is None:\n        prefix = Path(\".\")\n\n    problems: Iterable[Path]\n\n    if problems_to_do is None:\n        # The glob syntax here means a the filename must start with\n        # homework-, be followed the homework number, followed by a\n        # dash, then a digit representing the problem number for this\n        # homework number, then any number of characters (in practice\n        # either nothing or, rarely, another digit), then the ipynb\n        # extension. Examples:\n        # homework-1-1.ipynb, homework-10-1.ipynb, homework-3-10.ipynb\n        problems = list(prefix.glob(f\"homework-{hw_num}-[0-9]*.ipynb\"))\n    else:\n        problems = [prefix / f\"homework-{hw_num}-{i}.ipynb\" for i in problems_to_do]\n\n    problems = sorted(problems, key=lambda k: k.stem[-1])\n\n    output_directory: Path = (prefix / \"output\").resolve()\n    fw = FilesWriter(build_directory=str(output_directory))\n\n    assignment_zip_name = output_directory / f\"homework-{hw_num}.zip\"\n    solution_zip_name = output_directory / f\"homework-{hw_num}-soln.zip\"\n\n    assignment_pdfs: List[BytesIO] = []\n    solution_pdfs: List[BytesIO] = []\n\n    assignment_pdf: bytes\n    solution_pdf: bytes\n    assignment_nb: str\n    solution_nb: str\n\n    res: Dict[str, Union[str, bool]] = {\n        \"delete_pymarkdown\": True,\n        \"global_content_filter\": {\"include_raw\": False},\n    }\n\n    for problem in problems:\n        print(\"Working on:\", problem)\n        res[\"unique_key\"] = problem.stem\n        problem_number = int(problem.stem.split(\"-\")[-1])\n        if by_hand is not None and problem_number in by_hand:\n            res[\"by_hand\"] = True\n        else:\n            res[\"by_hand\"] = False\n        problem_fname = str(problem.resolve())\n\n        # Process assignments\n        res[\"remove_solution\"] = True\n        assignment_pdf, _ = pdf_exp.from_filename(problem_fname, resources=res)\n        assignment_pdfs.append(BytesIO(assignment_pdf))\n\n        assignment_nb, _ = nb_exp.from_filename(problem_fname, resources=res)\n        with ZipFile(assignment_zip_name, mode=\"a\") as zip_file:\n            zip_file.writestr(problem.name, assignment_nb)\n\n        # Process solutions\n        res[\"remove_solution\"] = False\n        solution_pdf, _ = pdf_exp.from_filename(problem_fname, resources=res)\n        solution_pdfs.append(BytesIO(solution_pdf))\n\n        solution_nb, _ = nb_exp.from_filename(problem_fname, resources=res)\n        with ZipFile(solution_zip_name, mode=\"a\") as zip_file:\n            zip_file.writestr(problem.stem + \"-soln\" + problem.suffix, solution_nb)\n\n    resources: Dict[str, Any] = {\n        \"metadata\": {\n            \"name\": f\"homework-{hw_num}\",\n            \"path\": str(prefix),\n            \"modified_date\": date.today().strftime(\"%B %d, %Y\"),\n        },\n        \"output_extension\": \".pdf\",\n    }\n    fw.write(combine_pdf_as_bytes(assignment_pdfs), resources, f\"homework-{hw_num}\")\n\n    resources[\"metadata\"][\"name\"] = f\"homework-{hw_num}-soln\"\n    fw.write(combine_pdf_as_bytes(solution_pdfs), resources, f\"homework-{hw_num}-soln\")", "response": "Processes the homework problems in the prefix folder."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing arguments and process the homework assignment.", "response": "def main(argv: Optional[Sequence[str]] = None) -> None:\n    \"\"\"Parse arguments and process the homework assignment.\"\"\"\n    parser = ArgumentParser(description=\"Convert Jupyter Notebook assignments to PDFs\")\n    parser.add_argument(\n        \"--hw\",\n        type=int,\n        required=True,\n        help=\"Homework number to convert\",\n        dest=\"hw_num\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--problems\",\n        type=int,\n        help=\"Problem numbers to convert\",\n        dest=\"problems\",\n        nargs=\"*\",\n    )\n    parser.add_argument(\n        \"--by-hand\",\n        type=int,\n        help=\"Problem numbers to be completed by hand\",\n        dest=\"by_hand\",\n        nargs=\"*\",\n    )\n    args = parser.parse_args(argv)\n    prefix = Path(f\"homework/homework-{args.hw_num}\")\n    process(args.hw_num, args.problems, prefix=prefix, by_hand=args.by_hand)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the vsphere object associated with a given text name", "response": "def get_object_by_name(content, object_type, name, regex=False):\n    '''\n    Get the vsphere object associated with a given text name\n    Source: https://github.com/rreubenur/vmware-pyvmomi-examples/blob/master/create_template.py\n    '''\n    container = content.viewManager.CreateContainerView(\n        content.rootFolder, [object_type], True\n    )\n    for c in container.view:\n        if regex:\n            if re.match(name, c.name):\n                return c\n        elif c.name == name:\n            return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a VM by its name", "response": "def get_vm_by_name(content, name, regex=False):\n    '''\n    Get a VM by its name\n    '''\n    return get_object_by_name(content, vim.VirtualMachine, name, regex)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_all(content, container, object_type):\n    '''\n    Get all items of a certain type\n    Example: get_all(content, vim.Datastore) return all datastore objects\n    '''\n    obj_list = list()\n    view_manager = content.viewManager\n    object_view = view_manager.CreateContainerView(\n        container, [object_type], True\n    )\n    for obj in object_view.view:\n        if isinstance(obj, object_type):\n            obj_list.append(obj)\n    object_view.Destroy()\n    return obj_list", "response": "Get all items of a certain type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_datacenter(content, obj):\n    '''\n    Get the datacenter to whom an object belongs\n    '''\n    datacenters = content.rootFolder.childEntity\n    for d in datacenters:\n        dch = get_all(content, d, type(obj))\n        if dch is not None and obj in dch:\n            return d", "response": "Get the datacenter to whom an object belongs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all the virtual switches in the content.", "response": "def get_all_vswitches(content):\n    '''\n    Get all the virtual switches\n    '''\n    vswitches = []\n    hosts = get_all_hosts(content)\n    for h in hosts:\n        for s in h.config.network.vswitch:\n            vswitches.append(s)\n    return vswitches"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_vm_info(vm):\n    '''\n    Print information for a particular virtual machine\n    '''\n    summary = vm.summary\n    print('Name  : ', summary.config.name)\n    print('Path  : ', summary.config.vmPathName)\n    print('Guest : ', summary.config.guestFullName)\n    annotation = summary.config.annotation\n    if annotation is not None and annotation != '':\n        print('Annotation : ', annotation)\n    print('State : ', summary.runtime.powerState)\n    if summary.guest is not None:\n        ip = summary.guest.ipAddress\n        if ip is not None and ip != '':\n            print('IP    : ', ip)\n    if summary.runtime.question is not None:\n        print('Question : ', summary.runtime.question.text)\n    print('')", "response": "Prints information for a particular virtual machine"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef module_import(module_path):\n    try:\n        # Import whole module path.\n        module = __import__(module_path)\n\n        # Split into components: ['contour',\n        # 'extras','appengine','ndb_persistence'].\n        components = module_path.split('.')\n\n        # Starting at the second component, set module to a\n        # a reference to that component. at the end\n        # module with be the last component. In this case:\n        # ndb_persistence\n        for component in components[1:]:\n            module = getattr(module, component)\n\n        return module\n\n    except ImportError:\n        raise BadModulePathError(\n            'Unable to find module \"%s\".' % (module_path,))", "response": "Imports the module indicated in name\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_contour_yaml(config_file=__file__, names=None):\n    checked = set()\n    contour_yaml = _find_countour_yaml(os.path.dirname(config_file), checked,\n                                       names=names)\n\n    if not contour_yaml:\n        contour_yaml = _find_countour_yaml(os.getcwd(), checked, names=names)\n\n    return contour_yaml", "response": "Traverse the directory trees to find a contour. yaml file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_countour_yaml(start, checked, names=None):\n    extensions = []\n\n    if names:\n        for name in names:\n            if not os.path.splitext(name)[1]:\n                extensions.append(name + \".yaml\")\n                extensions.append(name + \".yml\")\n\n    yaml_names = (names or []) + CONTOUR_YAML_NAMES + extensions\n    directory = start\n\n    while directory not in checked:\n        checked.add(directory)\n\n        for fs_yaml_name in yaml_names:\n            yaml_path = os.path.join(directory, fs_yaml_name)\n\n            if os.path.exists(yaml_path):\n                return yaml_path\n\n        directory = os.path.dirname(directory)\n\n    return", "response": "Traverse the directory tree identified by start and check if countour. yaml is found."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _load_yaml_config(path=None):\n    countour_yaml_path = path or find_contour_yaml()\n\n    if countour_yaml_path is None:\n        logging.debug(\"countour.yaml not found.\")\n        return None\n\n    with open(countour_yaml_path) as yaml_file:\n        return yaml_file.read()", "response": "Open and return the yaml contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_parser():\n    parser = argparse.ArgumentParser(\n        description='dockerstache templating util'\n    )\n    parser.add_argument(\n        '--output', '-o',\n        help='Working directory to render dockerfile and templates',\n        dest='output',\n        default=None\n        )\n    parser.add_argument(\n        '--input', '-i',\n        help='Working directory containing dockerfile and script mustache templates',\n        dest='input',\n        default=os.getcwd()\n        )\n    parser.add_argument(\n        '--context', '-c',\n        help='JSON file containing context dictionary to render templates',\n        dest='context',\n        default=None\n    )\n    parser.add_argument(\n        '--defaults', '-d',\n        help='JSON file containing default context dictionary to render templates',\n        dest='defaults',\n        default=None\n    )\n    parser.add_argument(\n        '--inclusive',\n        help='include non .mustache files from template',\n        default=False,\n        action='store_true'\n    )\n    parser.add_argument(\n        '--exclude', '-e',\n        help='exclude files from template in this list',\n        default=[],\n        nargs='+'\n    )\n    opts = parser.parse_args()\n    return vars(opts)", "response": "Build the CLI parser options and return the parsed options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_optional(attr):\n    return isinstance(attr.validator, _OptionalValidator) or (attr.default is not None and attr.default is not NOTHING)", "response": "Returns True if an attribute is mandatory or False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary of tuples that each key is the attribute name and value is the type of the attribute. Each value is a tuple that is the type of the attribute. Each key is the attribute type and each value is a boolean indicating whether the attribute is optional.", "response": "def get_attrs_declarations(item_type):\n    \"\"\"\n    Helper method to return a dictionary of tuples. Each key is attr_name, and value is (attr_type, attr_is_optional)\n\n    :param item_type:\n    :return:\n    \"\"\"\n\n    # this will raise an error if the type is not an attr-created type\n    attribs = fields(item_type)\n\n    res = dict()\n    for attr in attribs:\n        attr_name = attr.name\n\n        # -- is the attribute mandatory ?\n        optional = is_optional(attr)\n\n        # -- get and check the attribute type\n        typ = guess_type_from_validators(attr)\n\n        # -- store both info in result dict\n        res[attr_name] = (typ, optional)\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef preprocess(\n        self, nb: \"NotebookNode\", resources: dict\n    ) -> Tuple[\"NotebookNode\", dict]:\n        \"\"\"Remove any raw cells from the Notebook.\n\n        By default, exclude raw cells from the output. Change this by including\n        global_content_filter->include_raw = True in the resources dictionary.\n        This preprocessor is necessary because the NotebookExporter doesn't\n        include the exclude_raw config.\"\"\"\n        if not resources.get(\"global_content_filter\", {}).get(\"include_raw\", False):\n            keep_cells = []\n            for cell in nb.cells:\n                if cell.cell_type != \"raw\":\n                    keep_cells.append(cell)\n\n            nb.cells = keep_cells\n\n        return nb, resources", "response": "Remove any raw cells from the Notebook."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef preprocess(\n        self, nb: \"NotebookNode\", resources: dict\n    ) -> Tuple[\"NotebookNode\", dict]:\n        \"\"\"Preprocess the entire Notebook.\"\"\"\n\n        exam_num = resources[\"exam_num\"]\n        time = resources[\"time\"]\n        date = resources[\"date\"]\n\n        nb.cells.insert(0, new_markdown_cell(source=\"---\"))\n        nb.cells.insert(0, new_markdown_cell(source=\"\"))\n        nb.cells.insert(0, exam_instructions_cell)\n        first_cell_source = (\n            \"# ME 2233: Thermodynamic Principles\\n\\n\"\n            f\"# Exam {exam_num} - {time}\\n\\n# {date}\"\n        )\n        nb.cells.insert(0, new_markdown_cell(source=first_cell_source))\n\n        return nb, resources", "response": "Preprocess the entire Notebook."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_from_dict(json_dict):\n    order_columns = json_dict['columns']\n\n    order_list = MarketOrderList(\n        upload_keys=json_dict['uploadKeys'],\n        order_generator=json_dict['generator'],\n    )\n\n    for rowset in json_dict['rowsets']:\n        generated_at = parse_datetime(rowset['generatedAt'])\n        region_id = rowset['regionID']\n        type_id = rowset['typeID']\n        order_list.set_empty_region(region_id, type_id, generated_at)\n\n        for row in rowset['rows']:\n            order_kwargs = _columns_to_kwargs(\n                SPEC_TO_KWARG_CONVERSION, order_columns, row)\n            order_kwargs.update({\n                'region_id': region_id,\n                'type_id': type_id,\n                'generated_at': generated_at,\n            })\n\n            order_kwargs['order_issue_date'] = parse_datetime(order_kwargs['order_issue_date'])\n\n            order_list.add_order(MarketOrder(**order_kwargs))\n\n    return order_list", "response": "Given a Unified Uploader message as a JSON dict parse the contents and return a MarketOrderList."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode this list of MarketOrder instances to a JSON string.", "response": "def encode_to_json(order_list):\n    \"\"\"\n    Encodes this list of MarketOrder instances to a JSON string.\n\n    :param MarketOrderList order_list: The order list to serialize.\n    :rtype: str\n    \"\"\"\n    rowsets = []\n    for items_in_region_list in order_list._orders.values():\n        region_id = items_in_region_list.region_id\n        type_id = items_in_region_list.type_id\n        generated_at = gen_iso_datetime_str(items_in_region_list.generated_at)\n\n        rows = []\n        for order in items_in_region_list.orders:\n            issue_date = gen_iso_datetime_str(order.order_issue_date)\n\n            # The order in which these values are added is crucial. It must\n            # match STANDARD_ENCODED_COLUMNS.\n            rows.append([\n                order.price,\n                order.volume_remaining,\n                order.order_range,\n                order.order_id,\n                order.volume_entered,\n                order.minimum_volume,\n                order.is_bid,\n                issue_date,\n                order.order_duration,\n                order.station_id,\n                order.solar_system_id,\n            ])\n\n        rowsets.append(dict(\n            generatedAt = generated_at,\n            regionID = region_id,\n            typeID = type_id,\n            rows = rows,\n        ))\n\n    json_dict = {\n        'resultType': 'orders',\n        'version': '0.1',\n        'uploadKeys': order_list.upload_keys,\n        'generator': order_list.order_generator,\n        'currentTime': gen_iso_datetime_str(now_dtime_in_utc()),\n        # This must match the order of the values in the row assembling portion\n        # above this.\n        'columns': STANDARD_ENCODED_COLUMNS,\n        'rowsets': rowsets,\n    }\n\n    return json.dumps(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef weather(query):\n\n\tprint 'Identifying the location . . .'\n\ttry:\n\t\tresponse = unirest.post(\"https://textanalysis.p.mashape.com/nltk-stanford-ner\",\n\t  \t\theaders={\n\t    \t\"X-Mashape-Key\": \"E7WffsNDbNmshj4aVC4NUwj9dT9ep1S2cc3jsnFp5wSCzNBiaP\",\n\t    \t\"Content-Type\": \"application/x-www-form-urlencoded\"\n\t  \t\t\t},\n\t  \t\tparams={\n\t    \t\"text\": query\n\t  \t\t}\n\t\t)\n\texcept:\n\t\tprint 'Unable to connect to internet'\n\t\treturn\n\tlocation = ''\n\tfor entity in response.body['result'].split():\n\t\tword,tag = entity.split('/')\n\t\tif(tag == 'LOCATION'):\n\t\t\tlocation += ' '+word\n\tif(location != ''):\n\t\tprint 'Gathering weather information for'+location\n\t\timport urllib2, urllib, json\n\t\tbaseurl = \"https://query.yahooapis.com/v1/public/yql?\"\n\t\tyql_query = \"select * from weather.forecast where woeid in \\\n\t\t(select woeid from geo.places(1) where text=\\\"\"+location+\"\\\")\"\n\t\tyql_url = baseurl + urllib.urlencode({'q':yql_query}) + \"&format=json\"\n\t\ttry:\n\t\t\tresult = urllib2.urlopen(yql_url).read()\n\t\t\tdata = json.loads(result)\n\t\t\tresult = data['query']['results']['channel']\n\t\t\tprint result['location']['city']+' '+result['location']['country']+' '+result['location']['region']\n\t\t\tprint result['item']['condition']['date']\n\t\t\tprint result['item']['condition']['text']\n\t\t\tprint result['item']['condition']['temp']+' '+result['units']['temperature'] \n\t\texcept:\n\t\t\tprint 'Unable to connect to internet'\n\telse:\n\t\tprint 'Unable to get the location.'", "response": "This function is used to fetch weather information for a location entity in query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning true if the provided types are valid for constructor_with_str_arg conversion", "response": "def _can_construct_from_str(strict_mode: bool, from_type: Type, to_type: Type) -> bool:\n    \"\"\"\n    Returns true if the provided types are valid for constructor_with_str_arg conversion\n    Explicitly declare that we are not able to convert primitive types (they already have their own converters)\n\n    :param strict_mode:\n    :param from_type:\n    :param to_type:\n    :return:\n    \"\"\"\n    return to_type not in {int, float, bool}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef are_flags_valid(packet_type, flags):\n    if packet_type == MqttControlPacketType.publish:\n        rv = 0 <= flags <= 15\n    elif packet_type in (MqttControlPacketType.pubrel,\n                         MqttControlPacketType.subscribe,\n                         MqttControlPacketType.unsubscribe):\n        rv = flags == 2\n    elif packet_type in (MqttControlPacketType.connect,\n                         MqttControlPacketType.connack,\n                         MqttControlPacketType.puback,\n                         MqttControlPacketType.pubrec,\n                         MqttControlPacketType.pubcomp,\n                         MqttControlPacketType.suback,\n                         MqttControlPacketType.unsuback,\n                         MqttControlPacketType.pingreq,\n                         MqttControlPacketType.pingresp,\n                         MqttControlPacketType.disconnect):\n        rv = flags == 0\n    else:\n        raise NotImplementedError(packet_type)\n\n    return rv", "response": "Returns True if flags comply with [ MQTT - 2. 2 - 1 ) requirements based on packet_type ; False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(f):\n        decoder = mqtt_io.FileDecoder(f)\n        (byte_0,) = decoder.unpack(mqtt_io.FIELD_U8)\n\n        packet_type_u4 = (byte_0 >> 4)\n        flags = byte_0 & 0x0f\n\n        try:\n            packet_type = MqttControlPacketType(packet_type_u4)\n        except ValueError:\n            raise DecodeError('Unknown packet type 0x{:02x}.'.format(packet_type_u4))\n\n        if not are_flags_valid(packet_type, flags):\n            raise DecodeError('Invalid flags for packet type.')\n\n        num_bytes, num_remaining_bytes = decoder.unpack_varint(4)\n\n        return decoder.num_bytes_consumed, MqttFixedHeader(packet_type, flags, num_remaining_bytes)", "response": "Extracts a MqttFixedHeader object from a file - like object f."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_body(self, f):\n        num_bytes_written = 0\n        num_bytes_written += self.__encode_name(f)\n        num_bytes_written += self.__encode_protocol_level(f)\n        num_bytes_written += self.__encode_connect_flags(f)\n        num_bytes_written += self.__encode_keep_alive(f)\n\n        num_bytes_written += mqtt_io.encode_utf8(self.client_id, f)\n\n        if self.will is not None:\n            num_bytes_written += mqtt_io.encode_utf8(self.will.topic, f)\n            num_bytes_written += mqtt_io.encode_bytes(self.will.message, f)\n\n        if self.username is not None:\n            num_bytes_written += mqtt_io.encode_utf8(self.username, f)\n\n        if self.password is not None:\n            num_bytes_written += mqtt_io.encode_utf8(self.password, f)\n\n        return num_bytes_written", "response": "Encodes the message body of the object into a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a MqttSubscribe packet given a MqttFixedHeader.", "response": "def decode_body(cls, header, f):\n        \"\"\"Generates a `MqttSubscribe` packet given a\n        `MqttFixedHeader`.  This method asserts that header.packet_type\n        is `subscribe`.\n\n        Parameters\n        ----------\n        header: MqttFixedHeader\n        f: file\n            Object with a read method.\n\n        Raises\n        ------\n        DecodeError\n            When there are extra bytes at the end of the packet.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed from ``f``.\n        MqttSubscribe\n            Object extracted from ``f``.\n        \"\"\"\n        assert header.packet_type == MqttControlPacketType.subscribe\n\n        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))\n        packet_id, = decoder.unpack(mqtt_io.FIELD_PACKET_ID)\n\n        topics = []\n        while header.remaining_len > decoder.num_bytes_consumed:\n            num_str_bytes, name = decoder.unpack_utf8()\n            max_qos, = decoder.unpack(mqtt_io.FIELD_U8)\n            try:\n                sub_topic = MqttTopic(name, max_qos)\n            except ValueError:\n                raise DecodeError('Invalid QOS {}'.format(max_qos))\n            topics.append(sub_topic)\n\n        assert header.remaining_len == decoder.num_bytes_consumed\n\n        return decoder.num_bytes_consumed, MqttSubscribe(packet_id, topics)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_body(cls, header, f):\n        assert header.packet_type == MqttControlPacketType.suback\n\n        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))\n        packet_id, = decoder.unpack(mqtt_io.FIELD_PACKET_ID)\n\n        results = []\n        while header.remaining_len > decoder.num_bytes_consumed:\n            result, = decoder.unpack(mqtt_io.FIELD_U8)\n            try:\n                results.append(SubscribeResult(result))\n            except ValueError:\n                raise DecodeError('Unsupported result {:02x}.'.format(result))\n\n        assert header.remaining_len == decoder.num_bytes_consumed\n\n        return decoder.num_bytes_consumed, MqttSuback(packet_id, results)", "response": "Generates a MqttSuback packet given a MqttFixedHeader."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_body(cls, header, f):\n        assert header.packet_type == MqttControlPacketType.publish\n\n        dupe = bool(header.flags & 0x08)\n        retain = bool(header.flags & 0x01)\n        qos = ((header.flags & 0x06) >> 1)\n\n        if qos == 0 and dupe:\n            # The DUP flag MUST be set to 0 for all QoS 0 messages\n            # [MQTT-3.3.1-2]\n            raise DecodeError(\"Unexpected dupe=True for qos==0 message [MQTT-3.3.1-2].\")\n\n        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))\n        num_bytes_consumed, topic_name = decoder.unpack_utf8()\n\n        if qos != 0:\n            # See MQTT 3.1.1 section 3.3.2.2\n            # See https://github.com/kcallin/mqtt-codec/issues/5\n            packet_id, = decoder.unpack(mqtt_io.FIELD_PACKET_ID)\n        else:\n            packet_id = 0\n\n        payload_len = header.remaining_len - decoder.num_bytes_consumed\n        payload = decoder.read(payload_len)\n\n        return decoder.num_bytes_consumed, MqttPublish(packet_id, topic_name, payload, dupe, qos, retain)", "response": "Generates a MqttPublish packet given a MqttFixedHeader."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_body(cls, header, f):\n        assert header.packet_type == MqttControlPacketType.pubrel\n\n        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))\n        packet_id, = decoder.unpack(mqtt_io.FIELD_U16)\n\n        if header.remaining_len != decoder.num_bytes_consumed:\n            raise DecodeError('Extra bytes at end of packet.')\n\n        return decoder.num_bytes_consumed, MqttPubrel(packet_id)", "response": "Generates a MqttPubrel packet given a MqttFixedHeader."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_body(cls, header, f):\n        assert header.packet_type == MqttControlPacketType.unsubscribe\n\n        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))\n        packet_id, = decoder.unpack(mqtt_io.FIELD_PACKET_ID)\n\n        topics = []\n        while header.remaining_len > decoder.num_bytes_consumed:\n            num_str_bytes, topic = decoder.unpack_utf8()\n            topics.append(topic)\n\n        assert header.remaining_len - decoder.num_bytes_consumed == 0\n\n        return decoder.num_bytes_consumed, MqttUnsubscribe(packet_id, topics)", "response": "Generates a MqttUnsubscribe packet given a MqttFixedHeader."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a MqttUnsuback packet given a MqttFixedHeader.", "response": "def decode_body(cls, header, f):\n        \"\"\"Generates a `MqttUnsuback` packet given a\n        `MqttFixedHeader`.  This method asserts that header.packet_type\n        is `unsuback`.\n\n        Parameters\n        ----------\n        header: MqttFixedHeader\n        f: file\n            Object with a read method.\n\n        Raises\n        ------\n        DecodeError\n            When there are extra bytes at the end of the packet.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed from ``f``.\n        MqttUnsuback\n            Object extracted from ``f``.\n        \"\"\"\n        assert header.packet_type == MqttControlPacketType.unsuback\n\n        decoder = mqtt_io.FileDecoder(mqtt_io.LimitReader(f, header.remaining_len))\n        packet_id, = decoder.unpack(mqtt_io.FIELD_PACKET_ID)\n\n        if header.remaining_len != decoder.num_bytes_consumed:\n            raise DecodeError('Extra bytes at end of packet.')\n\n        return decoder.num_bytes_consumed, MqttUnsuback(packet_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a MqttPingreq packet given a MqttFixedHeader.", "response": "def decode_body(cls, header, f):\n        \"\"\"Generates a `MqttPingreq` packet given a\n        `MqttFixedHeader`.  This method asserts that header.packet_type\n        is `pingreq`.\n\n        Parameters\n        ----------\n        header: MqttFixedHeader\n        f: file\n            Object with a read method.\n\n        Raises\n        ------\n        DecodeError\n            When there are extra bytes at the end of the packet.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed from ``f``.\n        MqttPingreq\n            Object extracted from ``f``.\n        \"\"\"\n        assert header.packet_type == MqttControlPacketType.pingreq\n\n        if header.remaining_len != 0:\n            raise DecodeError('Extra bytes at end of packet.')\n\n        return 0, MqttPingreq()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a MqttPingresp packet given a MqttFixedHeader.", "response": "def decode_body(cls, header, f):\n        \"\"\"Generates a `MqttPingresp` packet given a\n        `MqttFixedHeader`.  This method asserts that header.packet_type\n        is `pingresp`.\n\n        Parameters\n        ----------\n        header: MqttFixedHeader\n        f: file\n            Object with a read method.\n\n        Raises\n        ------\n        DecodeError\n            When there are extra bytes at the end of the packet.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed from ``f``.\n        MqttPingresp\n            Object extracted from ``f``.\n        \"\"\"\n        assert header.packet_type == MqttControlPacketType.pingresp\n\n        if header.remaining_len != 0:\n            raise DecodeError('Extra bytes at end of packet.')\n\n        return 0, MqttPingresp()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a MqttDisconnect packet given a MqttFixedHeader.", "response": "def decode_body(cls, header, f):\n        \"\"\"Generates a :class:`MqttDisconnect` packet given a\n        :class:`MqttFixedHeader`.  This method asserts that\n        header.packet_type is :const:`MqttControlPacketType.disconnect`.\n\n        Parameters\n        ----------\n        header: MqttFixedHeader\n        f: file\n            Object with a read method.\n\n        Raises\n        ------\n        DecodeError\n            When there are extra bytes at the end of the packet.\n\n        Returns\n        -------\n        int\n            Number of bytes consumed from ``f``.\n        MqttDisconnect\n            Object extracted from ``f``.\n        \"\"\"\n        assert header.packet_type == MqttControlPacketType.disconnect\n\n        if header.remaining_len != 0:\n            raise DecodeError('Extra bytes at end of packet.')\n\n        return 0, MqttDisconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getter(name, key=None):\n\n  if not key:\n    key = lambda x: x\n\n  def wrapper(self):\n    return key(getattr(self, name))\n  wrapper.__name__ = wrapper.__qualname__ = name\n  return property(wrapper)", "response": "Creates a read - only property for the attribute name name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect to the Phabricator.", "response": "def connect(self):\n        \"\"\"\n        Sets up your Phabricator session, it's not necessary to call\n        this directly\n        \"\"\"\n        if self.token:\n            self.phab_session = {'token': self.token}\n            return\n\n        req = self.req_session.post('%s/api/conduit.connect' % self.host, data={\n            'params': json.dumps(self.connect_params),\n            'output': 'json',\n            '__conduit__': True,\n        })\n\n        # Parse out the response (error handling ommitted)\n        result = req.json()['result']\n        self.phab_session = {\n            'sessionKey': result['sessionKey'],\n            'connectionID': result['connectionID'],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request(self, method, params=None):\n        if params is None:\n            params = {}\n        if not self.phab_session:\n            self.connect()\n        url = '%s/api/%s' % (self.host, method)\n        params['__conduit__'] = self.phab_session\n        req = self.req_session.post(url, data={\n            'params': json.dumps(params),\n            'output': 'json',\n        })\n        return json.loads(\n            req.content.decode(),\n            object_pairs_hook=collections.OrderedDict\n        )['result']", "response": "Make a request to the API and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_musiclibrary():\n    lib_files = music_library.get_file_list(config.library_path)\n    global lib\n    lib = music_library.parse_library(lib_files)\n    \"\"\":type :musiclibrary.MusicLibrary\"\"\"\n    return lib", "response": "Get the music library object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef historic_doslegs_parse(html, url_an=None, logfile=sys.stderr, nth_dos_in_page=0, parse_previous_works=True, parse_next_works=True):\n\n    data = {\n        'url_dossier_assemblee': clean_url(url_an),\n        'urgence': False,\n    }\n\n    def log_error(*error):\n        print('## ERROR ###', *error, file=logfile)\n\n    def log_warning(*error):\n        print('## WARNING ###', *error, file=logfile)\n\n    soup = BeautifulSoup(html, 'lxml')\n\n    legislature, slug = parse_national_assembly_url(data['url_dossier_assemblee'])\n    data['assemblee_slug'] = slug\n    if legislature:\n        data['assemblee_legislature'] = legislature\n    else:  # strange link (old dosleg)\n        log_error('NO LEGISLATURE IN AN LINK: ' + data['url_dossier_assemblee'])\n    data['assemblee_id'] = '%s-%s' % (data.get('assemblee_legislature', ''), data['assemblee_slug'])\n\n    data['steps'] = []\n    curr_institution = 'assemblee'\n    curr_stage = '1\u00e8re lecture'\n    last_section = None  # Travaux des commissions/Discussion en s\u00e9ance publique\n    last_step_index = 0\n    travaux_prep_already = False\n    another_dosleg_inside = None\n    predicted_next_step = None  # For unfinished projects, we try to catch the next step\n    previous_works = None\n    url_jo = None\n\n    html_lines = html.split('\\n')\n    for i, line in enumerate(html_lines):\n        def parse_line():\n            return BeautifulSoup(line, 'lxml')\n\n        def line_text():\n            return parse_line().text.strip()\n\n        def get_last_step():\n            if len(data['steps']) > 0:\n                return data['steps'][-1]\n            return {}\n\n        if '<COMMENTAIRE>' in line or '<table border=\"1\"' in line:\n            continue\n\n        if '<font face=\"ARIAL\" size=\"3\" color=\"#000080\">' in line:\n            data['long_title'] = line_text()\n        if '<br><b><font color=\"#000099\">Travaux des commissions</font></b><br>' in line:\n            last_section = line_text()\n        if '<p align=\"center\"><b><font color=\"#000080\">Travaux pr\u00e9paratoires</font></b><br>' in line:\n            if travaux_prep_already:\n                if parse_next_works and not nth_dos_in_page:\n                    log_warning('FOUND ANOTHER DOSLEG INSIDE THE DOSLEG')\n                    another_dosleg_inside = '\\n'.join(html.split('\\n')[last_step_index + 1:])\n                if not nth_dos_in_page:\n                    break\n                travaux_prep_already = False\n            else:\n                travaux_prep_already = True\n        if not parse_next_works and travaux_prep_already and nth_dos_in_page:\n            continue\n\n        # Senat 1\u00e8re lecture, CMP, ...\n        if '<font color=\"#000099\" size=\"2\" face=\"Arial\">' in line:\n            text = line_text()\n            last_section = None\n            if 'Dossier en ligne sur le site du S\u00e9nat' in text:\n                data['url_dossier_senat'] = clean_url(parse_line().select(\n                    'a')[-1].attrs['href'])\n                text = text.replace(\n                    '(Dossier en ligne sur le site du S\u00e9nat)', '')\n            if 'S\u00e9nat' in text:\n                curr_institution = 'senat'\n            elif 'Assembl\u00e9e nationale' in text:\n                curr_institution = 'assemblee'\n            elif 'Commission Mixte Paritaire' in text or 'Lecture texte CMP' in text:\n                curr_institution = 'CMP'\n                curr_stage = 'CMP'\n            elif 'Conseil Constitutionnel' in text:\n                curr_institution = 'conseil constitutionnel'\n                curr_stage = 'constitutionnalit\u00e9'\n            elif 'Congr\u00e8s du Parlement' in text:\n                curr_institution = 'congr\u00e8s'\n                curr_stage = 'congr\u00e8s'\n\n            if '1\u00e8re lecture' in text:\n                curr_stage = '1\u00e8re lecture'\n            elif '2e lecture' in text:\n                curr_stage = '2\u00e8me lecture'\n            elif 'Nouvelle lecture' in text:\n                curr_stage = 'nouv. lect.'\n            elif 'Lecture d\u00e9finitive' in text:\n                curr_stage = 'l. d\u00e9finitive'\n            if not curr_stage:\n                curr_stage = text.split('-')[-1].strip().lower()\n\n            if curr_stage == \"cr\u00e9ation de la commission d'enqu\u00eate\":\n                log_warning('COMMISSION D\\'ENQUETE')\n                return None\n\n        if '>Proposition de r\u00e9solution europ\u00e9enne<' in line:\n            log_warning('PROPOSITION DE RESOLUTION EUROPEENE')\n            return None\n\n        if '>Acc\u00e8s aux Travaux pr\u00e9paratoires' in line and not previous_works:\n            previous_works = clean_url(urljoin(url_an, parse_line().find('a').attrs['href']))\n\n        curr_step = None\n        # conseil. consti. has no step but we should get the link\n        no_step_but_good_link = False\n        if 'Rapport portant \u00e9galement sur les propositions' in line:\n            continue\n        elif re.search(r'<a[^>]* href=[^>]*>(projet de loi|proposition de loi|proposition de r\u00e9solution)', line, re.I):\n            curr_step = 'depot'\n            if curr_stage == 'CMP':\n                continue\n        elif \">Texte de la commission\" in line or '/ta-commission/' in line:\n            curr_step = 'commission'\n\n        elif '/ta/' in line or '/leg/tas' in line:\n            if get_last_step().get('stage') != curr_stage:\n                curr_step = 'depot'\n                if curr_stage == 'CMP':\n                    curr_step = 'commission'\n            else:\n                curr_step = 'hemicycle'\n        elif ('/rapports/' in line or '/rap/' in line) and last_section and 'commissions' in last_section:\n            if get_last_step().get('step') == 'commission':\n                # log_warning('DOUBLE COMMISSION LINE: %s' % line)\n                continue\n            curr_step = 'commission'\n\n        elif 'www.conseil-constitutionnel.fr/decision/' in line:\n            no_step_but_good_link = True\n\n        # no commissions for l. d\u00e9finitive\n        if curr_stage == 'l. d\u00e9finitive' and curr_step == 'commission':\n            continue\n\n        if curr_step or no_step_but_good_link:\n            # if same step previously, replace or not the url\n            if get_last_step().get('step') == curr_step:\n                # log_warning('DOUBLE STEP: %s' % line)\n                # remove last step since we prefer text links instead of reports links\n                # TODO: add report link as bonus_url\n                last_url = get_last_step().get('source_url')\n                if not last_url or ('/rapports/' in last_url or '/rap/' in last_url):\n                    data['steps'] = data['steps'][:-1]\n                # looks like the last url was already a text, let's assume it's a multi-depot\n                else:\n                    # multi-depot if not CMP\n                    # TODO: re-order multi depot\n                    if curr_institution == 'senat' and curr_stage != 'CMP':\n                        curr_step = 'depot'\n\n            links = [a.attrs.get('href') for a in parse_line().select('a')]\n            links = [\n                href for href in links if href and 'fiches_id' not in href and '/senateur/' not in href and 'javascript:' not in href]\n            if not links:\n                log_error('NO LINK IN LINE: %s' % (line,))\n                continue\n            urls_raps = []\n            urls_others = []\n            for href in links:\n                if '/rap/' in href or '/rapports/' in href:\n                    urls_raps.append(href)\n                else:\n                    urls_others.append(href)\n\n            cmp_commission_other_url = None\n            if len(urls_others) > 0:\n                url = urls_others[0]\n                # CMP commission should produce two texts, one for each institution\n                if curr_step == 'commission' and curr_stage == 'CMP' and len(urls_others) > 1:\n                    cmp_commission_other_url = clean_url(urljoin(url_an, urls_others[1]))\n            else:\n                url = urls_raps[0]\n\n            url = clean_url(urljoin(url_an, url))\n\n            real_institution = curr_institution\n            if curr_stage == 'CMP' and curr_step == 'hemicycle':\n                if 'assemblee-nationale.fr' in url:\n                    real_institution = 'assemblee'\n                elif 'senat.fr' in url:\n                    real_institution = 'senat'\n\n            step = {\n                'institution': real_institution,\n                'stage': curr_stage,\n                'source_url': url,\n            }\n\n            if curr_step:\n                step['step'] = curr_step\n\n            if cmp_commission_other_url:\n                step['cmp_commission_other_url'] = cmp_commission_other_url\n\n            # try to detect a date\n            for test_line in (line, html_lines[i-1]):\n                test_line = test_line.replace('1<sup>er</sup>', '1')\n                date_match = re.search(r'(d\u00e9pos\u00e9e? le|adopt\u00e9e? .*? le|modifi\u00e9e? .*?|rejet\u00e9e? .*?)\\s*(\\d\\d? \\w\\w\\w+ \\d\\d\\d\\d)', test_line, re.I)\n                if date_match:\n                    step['date'] = format_date(date_match.group(2))\n                else:\n                    date_match = re.search(r'(mis en ligne le)\\s*(\\d\\d? \\w\\w\\w+ \\d\\d\\d\\d)', test_line, re.I)\n                    if date_match:\n                        step['date'] = format_date(date_match.group(2))\n                if 'date' in step and 'beginning' not in data:\n                    data['beginning'] = step['date']\n            data['steps'].append(step)\n            predicted_next_step = None\n            last_step_index = i\n\n        if 'publi\u00e9e au Journal Officiel' in line and not url_jo:\n            links = [clean_url(a.attrs['href']) for a in parse_line().select('a') if 'legifrance' in a.attrs.get('href', '')]\n            if not links:\n                log_error('NO GOOD LINK IN LINE: %s' % (line,))\n                continue\n            url_jo = links[-1]\n\n        if 'Le Gouvernement a engag\u00e9 la proc\u00e9dure acc\u00e9l\u00e9r\u00e9e' in line or 'engagement de la proc\u00e9dure acc\u00e9l\u00e9r\u00e9e' in line:\n            data['urgence'] = True\n\n        # Next step prediction via small clues\n        # TODO: this could be done via last_section (we parse two times the same thing)\n        # TODO: this fails for CMP hemicycle senat\n        if curr_stage != 'CMP':\n            if '>Discussion en s\u00e9ance publique<' in line:\n                predicted_next_step = {\n                    'institution': curr_institution,\n                    'stage': curr_stage,\n                    'step': 'hemicycle',\n                }\n            elif '>Travaux des commissions<' in line:\n                predicted_next_step = {\n                    'institution': curr_institution,\n                    'stage': curr_stage,\n                    'step': 'commission',\n                }\n\n    metas = {}\n    for meta in soup.select('meta'):\n        if 'name' in meta.attrs:\n            metas[meta.attrs['name']] = meta.attrs['content']\n\n    if not url_jo:\n        url_jo = metas.get('LIEN_LOI_PROMULGUEE')\n\n    if url_jo:\n        data['url_jo'] = clean_url(url_jo)\n        promulgation_step = {\n            'institution': 'gouvernement',\n            'stage': 'promulgation',\n            'source_url': data['url_jo'],\n        }\n        if metas.get('LOI_PROMULGUEE'):\n            data['end'] = find_promulgation_date(metas.get('LOI_PROMULGUEE'))\n            promulgation_step['date'] = data['end']\n        data['steps'].append(promulgation_step)\n    # add predicted next step for unfinished projects\n    elif predicted_next_step:\n        data['steps'].append(predicted_next_step)\n\n    if 'url_dossier_senat' not in data or 'dossier-legislatif' not in data['url_dossier_senat']:\n        senat_url = find_senat_url(data)\n        if senat_url:\n            data['url_dossier_senat'] = senat_url\n\n    # append previous works if there are some\n    if previous_works and parse_previous_works:\n        log_warning('MERGING %s WITH PREVIOUS WORKS %s' % (url_an, previous_works))\n        resp = download_historic_dosleg(previous_works)\n        prev_data = historic_doslegs_parse(\n            resp.text, previous_works,\n            logfile=logfile,\n            nth_dos_in_page=nth_dos_in_page, parse_next_works=False)\n        if prev_data:\n            prev_data = prev_data[nth_dos_in_page] if len(prev_data) > 1 else prev_data[0]\n            data = merge_previous_works_an(prev_data, data)\n        else:\n            log_warning('INVALID PREVIOUS WORKS', previous_works)\n\n    # is this part of a dosleg previous works ?\n    next_legislature = data['assemblee_legislature'] + 1 if 'assemblee_legislature' in data else 9999\n    if parse_next_works and next_legislature < 15:\n        #  TODO: parse 15th legislature from open data if it exists\n        resp = download_historic_dosleg(url_an.replace('/%d/' % data['assemblee_legislature'], '/%d/' % (data['assemblee_legislature'] + 1)))\n        if resp.status_code == 200:\n            recent_data = historic_doslegs_parse(\n                resp.text, resp.url,\n                logfile=logfile,\n                nth_dos_in_page=nth_dos_in_page, parse_previous_works=False)\n            if recent_data:\n                log_warning('FOUND MORE RECENT WORKS', resp.url)\n                recent_data = recent_data[nth_dos_in_page] if len(recent_data) > 1 else recent_data[0]\n                data = merge_previous_works_an(data, recent_data)\n\n    if another_dosleg_inside:\n        others = historic_doslegs_parse(another_dosleg_inside, url_an, logfile=logfile, nth_dos_in_page=nth_dos_in_page+1)\n        if others:\n            return [data] + others\n    return [data]", "response": "Parse an AN dosleg and return a dict of data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auto_need(form):\n\n    requirements = form.get_widget_requirements()\n\n    for library, version in requirements:\n        resources = resource_mapping[library]\n        if not isinstance(resources, list):  # pragma: no cover (bw compat only)\n            resources = [resources]\n        for resource in resources:\n            resource.need()", "response": "Automatically adds relevant Fanstatic resources to a Fanstatic ArcGIS form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef named_any(name):\n    assert name, 'Empty module name'\n    names = name.split('.')\n\n    topLevelPackage = None\n    moduleNames = names[:]\n    while not topLevelPackage:\n        if moduleNames:\n            trialname = '.'.join(moduleNames)\n            try:\n                topLevelPackage = __import__(trialname)\n            except Exception, ex:\n                moduleNames.pop()\n        else:\n            if len(names) == 1:\n                raise Exception(\"No module named %r\" % (name,))\n            else:\n                raise Exception('%r does not name an object' % (name,))\n\n    obj = topLevelPackage\n    for n in names[1:]:\n        obj = getattr(obj, n)\n\n    return obj", "response": "Retrieve a Python object by its fully qualified name from the global Python\n    module namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a class of classname from module modpath.", "response": "def for_name(modpath, classname):\n    '''\n    Returns a class of \"classname\" from module \"modname\".\n    '''\n    module = __import__(modpath, fromlist=[classname])\n    classobj = getattr(module, classname)\n    return classobj()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _convert(self, val):\n        if isinstance(val, dict) and not isinstance(val, DotDict):\n            return DotDict(val), True\n        elif isinstance(val, list) and not isinstance(val, DotList):\n            return DotList(val), True\n\n        return val, False", "response": "Convert the type if necessary and return if a conversion happened."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the full subgraph of this graph with the given vertices.", "response": "def full_subgraph(self, vertices):\n        \"\"\"\n        Return the subgraph of this graph whose vertices\n        are the given ones and whose edges are the edges\n        of the original graph between those vertices.\n\n        \"\"\"\n        obj_map = {vertex.id: vertex for vertex in vertices}\n        edges = [\n            edge for vertex_id in obj_map\n            for edge in self._out_edges[vertex_id]\n            if edge.head in obj_map\n        ]\n\n        return AnnotatedGraph(\n            vertices=obj_map.values(),\n            edges=edges,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(self):\n        obj = {\n            \"vertices\": [\n                {\n                    \"id\": vertex.id,\n                    \"annotation\": vertex.annotation,\n                }\n                for vertex in self.vertices\n            ],\n            \"edges\": [\n                {\n                    \"id\": edge.id,\n                    \"annotation\": edge.annotation,\n                    \"head\": edge.head,\n                    \"tail\": edge.tail,\n                }\n                for edge in self._edges\n            ],\n        }\n        # Ensure that we always return unicode output on Python 2.\n        return six.text_type(json.dumps(obj, ensure_ascii=False))", "response": "Convert to a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_json(cls, json_graph):\n        obj = json.loads(json_graph)\n\n        vertices = [\n            AnnotatedVertex(\n                id=vertex[\"id\"],\n                annotation=vertex[\"annotation\"],\n            )\n            for vertex in obj[\"vertices\"]\n        ]\n\n        edges = [\n            AnnotatedEdge(\n                id=edge[\"id\"],\n                annotation=edge[\"annotation\"],\n                head=edge[\"head\"],\n                tail=edge[\"tail\"],\n            )\n            for edge in obj[\"edges\"]\n        ]\n\n        return cls(vertices=vertices, edges=edges)", "response": "Reconstructs a new graph from a JSON representation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting the current node in JSON form to the given file.", "response": "def export_json(self, filename):\n        \"\"\"\n        Export graph in JSON form to the given file.\n\n        \"\"\"\n        json_graph = self.to_json()\n        with open(filename, 'wb') as f:\n            f.write(json_graph.encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimports the given JSON file into the current object.", "response": "def import_json(cls, filename):\n        \"\"\"\n        Import graph from the given file.  The file is expected\n        to contain UTF-8 encoded JSON data.\n\n        \"\"\"\n        with open(filename, 'rb') as f:\n            json_graph = f.read().decode('utf-8')\n        return cls.from_json(json_graph)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproduce a DOT representation of the current object.", "response": "def to_dot(self):\n        \"\"\"\n        Produce a graph in DOT format.\n\n        \"\"\"\n        edge_labels = {\n            edge.id: edge.annotation\n            for edge in self._edges\n        }\n\n        edges = [self._format_edge(edge_labels, edge) for edge in self._edges]\n\n        vertices = [\n            DOT_VERTEX_TEMPLATE.format(\n                vertex=vertex.id,\n                label=dot_quote(vertex.annotation),\n            )\n            for vertex in self.vertices\n        ]\n\n        return DOT_DIGRAPH_TEMPLATE.format(\n            edges=\"\".join(edges),\n            vertices=\"\".join(vertices),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_image(self, filename='refcycle.png', format=None,\n                     dot_executable='dot'):\n        \"\"\"\n        Export graph as an image.\n\n        This requires that Graphviz is installed and that the ``dot``\n        executable is in your path.\n\n        The *filename* argument specifies the output filename.\n\n        The *format* argument lets you specify the output format.  It may be\n        any format that ``dot`` understands, including extended format\n        specifications like ``png:cairo``.  If omitted, the filename extension\n        will be used; if no filename extension is present, ``png`` will be\n        used.\n\n        The *dot_executable* argument lets you provide a full path to the\n        ``dot`` executable if necessary.\n\n        \"\"\"\n        # Figure out what output format to use.\n        if format is None:\n            _, extension = os.path.splitext(filename)\n            if extension.startswith('.') and len(extension) > 1:\n                format = extension[1:]\n            else:\n                format = 'png'\n\n        # Convert to 'dot' format.\n        dot_graph = self.to_dot()\n\n        # We'll send the graph directly to the process stdin.\n        cmd = [\n            dot_executable,\n            '-T{}'.format(format),\n            '-o{}'.format(filename),\n        ]\n        dot = subprocess.Popen(cmd, stdin=subprocess.PIPE)\n        dot.communicate(dot_graph.encode('utf-8'))", "response": "Export the refcycle image to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serve_command(info, host, port, reload, debugger, eager_loading,\n                with_threads):\n    \"\"\"Runs a local development server for the Flask application.\n    This local server is recommended for development purposes only but it\n    can also be used for simple intranet deployments.  By default it will\n    not support any sort of concurrency at all to simplify debugging.  This\n    can be changed with the --with-threads option which will enable basic\n    multithreading.\n    The reloader and debugger are by default enabled if the debug flag of\n    Flask is enabled and disabled otherwise.\n    \"\"\"\n    from werkzeug.serving import run_simple\n\n    debug = get_debug_flag()\n    if reload is None:\n        reload = bool(debug)\n    if debugger is None:\n        debugger = bool(debug)\n    if eager_loading is None:\n        eager_loading = not reload\n\n    app = DispatchingApp(info.load_app, use_eager_loading=eager_loading)\n\n    # Extra startup messages.  This depends a but on Werkzeug internals to\n    # not double execute when the reloader kicks in.\n    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true':\n        # If we have an import path we can print it out now which can help\n        # people understand what's being served.  If we do not have an\n        # import path because the app was loaded through a callback then\n        # we won't print anything.\n        if info.app_import_path is not None:\n            print(' * Serving Flask app \"%s\"' % info.app_import_path)\n        if debug is not None:\n            print(' * Forcing debug mode %s' % (debug and 'on' or 'off'))\n\n    reloader_path = '.'\n    if info.app_import_path:\n        if os.path.isdir(info.app_import_path):\n            reloader_path = info.app_import_path\n        elif os.path.isfile(info.app_import_path):\n            reloader_path = os.path.dirname(info.app_import_path)\n    extra_files = get_reloader_extra_files(reloader_path)\n\n    run_simple(host, port, app, use_reloader=reload, extra_files=extra_files,\n               use_debugger=debugger, threaded=with_threads)", "response": "This function runs a local development server for the Flask application."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads the KvasirAPI calls into the API. call dictionary.", "response": "def load_calls(self, call_type='jsonrpc'):\n        \"\"\"Loads the KvasirAPI calls into API.call based on the call_type variable. Utilizes the `Calls` class to\n        establish an attribute-based access method. For instance a configuration with an instance called 'internal'\n        will create an API.call that can be used like this:\n\n            API.call.internal.hosts.list()  # return all hosts from Kvasir instance 'internal'\n\n        :param call_type: string of 'jsonrpc' or 'restapi'\n        :return: self.call dictionary\n        \"\"\"\n        valid = False\n        if call_type == 'jsonrpc':\n            #from jsonrpc import Hosts, Services, Accounts, Vulns, OS, NetBIOS, Evidence\n            import jsonrpc as api_calls\n            self.api_calls = api_calls\n            valid = True\n        #if call_type == 'rest'\n            # TODO: Implement restful API functions\n            #from restapi import hosts, services, accounts, vulns, os, netbios, evidence\n\n        if valid:\n            # if kvasir configuration is valid, go through the instances and build the self.call dict\n            for instance, values in self.configuration.instances_dict.items():\n                self.call[instance] = Calls()\n                self.call[instance].call_type = call_type\n                self.call[instance].hosts = self.api_calls.Hosts(values, self.configuration.web2py_dir)\n                self.call[instance].services = self.api_calls.Services(values, self.configuration.web2py_dir)\n                self.call[instance].accounts = self.api_calls.Accounts(values, self.configuration.web2py_dir)\n                self.call[instance].vulns = self.api_calls.Vulns(values, self.configuration.web2py_dir)\n                self.call[instance].os = self.api_calls.OpSys(values, self.configuration.web2py_dir)\n                self.call[instance].snmp = self.api_calls.SNMP(values, self.configuration.web2py_dir)\n                self.call[instance].netbios = self.api_calls.NetBIOS(values, self.configuration.web2py_dir)\n                self.call[instance].evidence = self.api_calls.Evidence(values, self.configuration.web2py_dir)\n                self.call[instance].stats = self.api_calls.Stats(values, self.configuration.web2py_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install_brew(target_path):\n    if not os.path.exists(target_path):\n        try:\n            os.makedirs(target_path)\n        except OSError:\n            logger.warn(\"Unable to create directory %s for brew.\" % target_path)\n            logger.warn(\"Skipping...\")\n            return\n    extract_targz(HOMEBREW_URL, target_path, remove_common_prefix=True)", "response": "Install brew to the target path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scales(self, image):\n    # compute the minimum scale so that the patch size still fits into the given image\n    minimum_scale = max(self.m_patch_box.size_f[0] / image.shape[-2], self.m_patch_box.size_f[1] / image.shape[-1])\n    if self.m_lowest_scale:\n      maximum_scale = min(minimum_scale / self.m_lowest_scale, 1.)\n    else:\n      maximum_scale = 1.\n    current_scale_power = 0.\n\n    # iterate over all possible scales\n    while True:\n      # scale the image\n      scale = minimum_scale * math.pow(self.m_scale_factor, current_scale_power)\n      if scale > maximum_scale:\n        # image is smaller than the requested minimum size\n        break\n      current_scale_power -= 1.\n      scaled_image_shape = bob.ip.base.scaled_output_shape(image, scale)\n\n      # return both the scale and the scaled image size\n      yield scale, scaled_image_shape", "response": "scales yields a tuple of the scale and the shape of the scaled image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a randomized image from the given shape.", "response": "def sample_scaled(self, shape):\n    \"\"\"sample_scaled(shape) -> bounding_box\n\n    Yields an iterator that iterates over all sampled bounding boxes in the given (scaled) image shape.\n\n    **Parameters:**\n\n    ``shape`` : (int, int) or (int, int, int)\n      The (current) shape of the (scaled) image\n\n    **Yields:**\n\n    ``bounding_box`` : :py:class:`BoundingBox`\n      An iterator iterating over all bounding boxes that are valid for the given shape\n    \"\"\"\n    for y in range(0, shape[-2]-self.m_patch_box.bottomright[0], self.m_distance):\n      for x in range(0, shape[-1]-self.m_patch_box.bottomright[1], self.m_distance):\n        # create bounding box for the current shift\n        yield self.m_patch_box.shift((y,x))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling the bounding boxes for the given image.", "response": "def sample(self, image):\n    \"\"\"sample(image) -> bounding_box\n\n    Yields an iterator over all bounding boxes in different scales that are sampled for the given image.\n\n    **Parameters:**\n\n    ``image`` : array_like(2D or 3D)\n      The image, for which the bounding boxes should be generated\n\n    **Yields:**\n\n    ``bounding_box`` : :py:class:`BoundingBox`\n      An iterator iterating over all bounding boxes for the given ``image``\n    \"\"\"\n    for scale, scaled_image_shape in self.scales(image):\n      # prepare the feature extractor to extract features from the given image\n      for bb in self.sample_scaled(scaled_image_shape):\n        # extract features for\n        yield bb.scale(1./scale)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninject a service instance into the kwargs", "response": "def pass_service(*names):\n    \"\"\"Injects a service instance into the kwargs\n    \"\"\"\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            for name in names:\n                kwargs[name] = service_proxy(name)\n            return f(*args, **kwargs)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_conn():\n    if os.environ.get('DEBUG', False) or os.environ.get('travis', False):\n        # In DEBUG mode - use the local DynamoDB\n        # This also works for travis since we'll be running dynalite\n        conn = DynamoDBConnection(\n            host='localhost',\n            port=8000,\n            aws_access_key_id='TEST',\n            aws_secret_access_key='TEST',\n            is_secure=False\n        )\n    else:\n        # Regular old production\n        conn = DynamoDBConnection()\n\n    return conn", "response": "Return a connection to DynamoDB."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a table schema call.", "response": "def table_schema_call(self, target, cls):\n        \"\"\"Perform a table schema call.\n\n        We call the callable target with the args and keywords needed for the\n        table defined by cls. This is how we centralize the Table.create and\n        Table ctor calls.\n        \"\"\"\n        index_defs = []\n        for name in cls.index_names() or []:\n            index_defs.append(GlobalIncludeIndex(\n                gsi_name(name),\n                parts=[HashKey(name)],\n                includes=['value']\n            ))\n\n        return target(\n            cls.get_table_name(),\n            connection=get_conn(),\n            schema=[HashKey('id')],\n            global_indexes=index_defs or None\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_table(self, cls):\n        exists = True\n        conn = get_conn()\n\n        try:\n            descrip = conn.describe_table(cls.get_table_name())\n            assert descrip is not None\n        except ResourceNotFoundException:\n            # Expected - this is what we get if there is no table\n            exists = False\n        except JSONResponseError:\n            # Also assuming no table\n            exists = False\n\n        if not exists:\n            table = self.table_schema_call(Table.create, cls)\n            assert table is not None", "response": "Ensures that the table with the given class exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all the objects in the class table that match the given index value.", "response": "def find_by_index(self, cls, index_name, value):\n        \"\"\"Required functionality.\"\"\"\n        query_args = {\n            index_name + '__eq': DynamoMappings.map_index_val(value),\n            'index': gsi_name(index_name)\n        }\n\n        final_results = []\n        for db_result in self.get_class_table(cls).query_2(**query_args):\n            obj = cls.from_data(db_result['value'])\n            final_results.append(obj)\n\n        return final_results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_event(self, name, subject, data):\n        method_mapping = Registry.get_event(name)\n        if not method_mapping:\n            log.info('@{}.process_event no subscriber for event `{}`'\n                     .format(self.__class__.__name__, name))\n            return\n        for event, methods in method_mapping.items():\n            event_instance = event(subject, data)\n            log.info('@{}.process_event `{}` for subject `{}`'.format(\n                self.__class__.__name__,\n                event_instance.__class__.__name__,\n                subject\n            ))\n            for method in methods:\n                with self._context_manager:\n                    log.info('>> Calling subscriber `{}`'\n                             .format(method.__name__))\n                    method(event_instance)", "response": "Process a single event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef thread(self):\n        log.info('@{}.thread starting'.format(self.__class__.__name__))\n        thread = threading.Thread(target=thread_wrapper(self.consume), args=())\n        thread.daemon = True\n        thread.start()", "response": "Start a thread for this consumer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new instance of a nodata. nodata", "response": "def create(parser: Parser, obj: PersistedObject = None):\n        \"\"\"\n        Helper method provided because we actually can't put that in the constructor, it creates a bug in Nose tests\n        https://github.com/nose-devs/nose/issues/725\n\n        :param parser:\n        :param obj:\n        :return:\n        \"\"\"\n        if obj is not None:\n            return _InvalidParserException('Error ' + str(obj) + ' cannot be parsed using ' + str(parser) + ' since '\n                                           + ' this parser does not support ' + obj.get_pretty_file_mode())\n        else:\n            return _InvalidParserException('Error this parser is neither SingleFile nor MultiFile !')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a multifile object into a new object.", "response": "def _parse_multifile(self, desired_type: Type[T], obj: PersistedObject,\n                         parsing_plan_for_children: Dict[str, ParsingPlan], logger: Logger,\n                         options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        First parse all children from the parsing plan, then calls _build_object_from_parsed_children\n\n        :param desired_type:\n        :param obj:\n        :param parsing_plan_for_children:\n        :param logger:\n        :param options:\n        :return:\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute(self, logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        in_root_call = False\n        if logger is not None:\n            # log only for the root object, not for the children that will be created by the code below\n            if not hasattr(_BaseParsingPlan.thrd_locals, 'flag_exec') \\\n                    or _BaseParsingPlan.thrd_locals.flag_exec == 0:\n                # print('Executing Parsing Plan for ' + str(self))\n                logger.debug('Executing Parsing Plan for [{location}]'\n                             ''.format(location=self.obj_on_fs_to_parse.get_pretty_location(append_file_ext=False)))\n                _BaseParsingPlan.thrd_locals.flag_exec = 1\n                in_root_call = True\n\n        # Common log message\n        logger.debug('(P) ' + get_parsing_plan_log_str(self.obj_on_fs_to_parse, self.obj_type,\n                                                       log_only_last=not in_root_call, parser=self.parser))\n\n        try:\n            res = super(_BaseParsingPlan, self).execute(logger, options)\n            if logger.isEnabledFor(DEBUG):\n                logger.info('(P) {loc} -> {type} SUCCESS !'\n                            ''.format(loc=self.obj_on_fs_to_parse.get_pretty_location(\n                    blank_parent_part=not GLOBAL_CONFIG.full_paths_in_logs,\n                    compact_file_ext=True),\n                    type=get_pretty_type_str(self.obj_type)))\n            else:\n                logger.info('SUCCESS parsed [{loc}] as a [{type}] successfully. Parser used was [{parser}]'\n                            ''.format(loc=self.obj_on_fs_to_parse.get_pretty_location(compact_file_ext=True),\n                                      type=get_pretty_type_str(self.obj_type),\n                                      parser=str(self.parser)))\n            if in_root_call:\n                # print('Completed parsing successfully')\n                logger.debug('Completed parsing successfully')\n            return res\n\n        finally:\n            # remove threadlocal flag if needed\n            if in_root_call:\n                _BaseParsingPlan.thrd_locals.flag_exec = 0", "response": "Execute the parsing plan."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_parsing_plan(self, desired_type: Type[T], filesystem_object: PersistedObject, logger: Logger,\n                            _main_call: bool = True):\n        \"\"\"\n        Implements the abstract parent method by using the recursive parsing plan impl. Subclasses wishing to produce\n        their own parsing plans should rather override _create_parsing_plan in order to benefit from this same log msg.\n\n        :param desired_type:\n        :param filesystem_object:\n        :param logger:\n        :param _main_call: internal parameter for recursive calls. Should not be changed by the user.\n        :return:\n        \"\"\"\n        in_root_call = False\n\n        # -- log msg only for the root call, not for the children that will be created by the code below\n        if _main_call and (not hasattr(AnyParser.thrd_locals, 'flag_init') or AnyParser.thrd_locals.flag_init == 0):\n            # print('Building a parsing plan to parse ' + str(filesystem_object) + ' into a ' +\n            #      get_pretty_type_str(desired_type))\n            logger.debug('Building a parsing plan to parse [{location}] into a {type}'\n                         ''.format(location=filesystem_object.get_pretty_location(append_file_ext=False),\n                                   type=get_pretty_type_str(desired_type)))\n            AnyParser.thrd_locals.flag_init = 1\n            in_root_call = True\n\n        # -- create the parsing plan\n        try:\n            pp = self._create_parsing_plan(desired_type, filesystem_object, logger, log_only_last=(not _main_call))\n        finally:\n            # remove threadlocal flag if needed\n            if in_root_call:\n                AnyParser.thrd_locals.flag_init = 0\n\n        # -- log success only if in root call\n        if in_root_call:\n            # print('Parsing Plan created successfully')\n            logger.debug('Parsing Plan created successfully')\n\n        # -- finally return\n        return pp", "response": "Creates a parsing plan for the specified type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a recursive parsing plan.", "response": "def _create_parsing_plan(self, desired_type: Type[T], filesystem_object: PersistedObject, logger: Logger,\n                             log_only_last: bool = False):\n        \"\"\"\n        Adds a log message and creates a recursive parsing plan.\n\n        :param desired_type:\n        :param filesystem_object:\n        :param logger:\n        :param log_only_last: a flag to only log the last part of the file path (default False)\n        :return:\n        \"\"\"\n        logger.debug('(B) ' + get_parsing_plan_log_str(filesystem_object, desired_type,\n                                                       log_only_last=log_only_last, parser=self))\n        return AnyParser._RecursiveParsingPlan(desired_type, filesystem_object, self, logger)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_parsing_plan_for_multifile_children(self, obj_on_fs: PersistedObject, desired_type: Type[T],\n                                                 logger: Logger) -> Dict[str, ParsingPlan[T]]:\n        \"\"\"\n        This method is called by the _RecursiveParsingPlan when created.\n        Implementing classes should return a dictionary containing a ParsingPlan for each child they plan to parse\n        using this framework. Note that for the files that will be parsed using a parsing library it is not necessary to\n        return a ParsingPlan.\n\n        In other words, implementing classes should return here everything they need for their implementation of\n        _parse_multifile to succeed. Indeed during parsing execution, the framework will call their _parse_multifile\n        method with that same dictionary as an argument (argument name is 'parsing_plan_for_children', see _BaseParser).\n\n        :param obj_on_fs:\n        :param desired_type:\n        :param logger:\n        :return:\n        \"\"\"\n        pass", "response": "This method is called by the _RecursiveParsingPlan when created. It returns a dictionary containing a ParsingPlan for each child of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_singlefile(self, desired_type: Type[T], file_path: str, encoding: str, logger: Logger,\n                          options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Implementation of the parent method : since this is a multifile parser, this is not implemented.\n\n        :param desired_type:\n        :param file_path:\n        :param encoding:\n        :param logger:\n        :param options:\n        :return:\n        \"\"\"\n        raise Exception('Not implemented since this is a MultiFileParser')", "response": "Implementation of the parent method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(parser_func: Union[ParsingMethodForStream, ParsingMethodForFile], caught: Exception):\n        msg = 'Caught TypeError while calling parsing function \\'' + str(parser_func.__name__) + '\\'. ' \\\n              'Note that the parsing function signature should be ' + parsing_method_stream_example_signature_str \\\n              + ' (streaming=True) or ' + parsing_method_file_example_signature_str + ' (streaming=False).' \\\n              'Caught error message is : ' + caught.__class__.__name__ + ' : ' + str(caught)\n        return CaughtTypeError(msg).with_traceback(caught.__traceback__)", "response": "Create a new CaughtTypeError instance with the given parser function and caught error."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrely on the inner parsing function to parse the file. If _streaming_mode is True, the file will be opened and closed by this method. Otherwise the parsing function will be responsible to open and close. :param desired_type: :param file_path: :param encoding: :param options: :return:", "response": "def _parse_singlefile(self, desired_type: Type[T], file_path: str, encoding: str, logger: Logger,\n                          options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Relies on the inner parsing function to parse the file.\n        If _streaming_mode is True, the file will be opened and closed by this method. Otherwise the parsing function\n        will be responsible to open and close.\n\n        :param desired_type:\n        :param file_path:\n        :param encoding:\n        :param options:\n        :return:\n        \"\"\"\n        opts = get_options_for_id(options, self.get_id_for_options())\n\n        if self._streaming_mode:\n\n            # We open the stream, and let the function parse from it\n            file_stream = None\n            try:\n                # Open the file with the appropriate encoding\n                file_stream = open(file_path, 'r', encoding=encoding)\n\n                # Apply the parsing function\n                if self.function_args is None:\n                    return self._parser_func(desired_type, file_stream, logger, **opts)\n                else:\n                    return self._parser_func(desired_type, file_stream, logger, **self.function_args, **opts)\n\n            except TypeError as e:\n                raise CaughtTypeError.create(self._parser_func, e)\n\n            finally:\n                if file_stream is not None:\n                    # Close the File in any case\n                    file_stream.close()\n\n        else:\n            # the parsing function will open the file itself\n            if self.function_args is None:\n                return self._parser_func(desired_type, file_path, encoding, logger, **opts)\n            else:\n                return self._parser_func(desired_type, file_path, encoding, logger, **self.function_args, **opts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef queryByPortSensor(portiaConfig, edgeId, port, sensor, strategy=SummaryStrategies.PER_HOUR, interval=1, params={ 'from': None, 'to': None, 'order': None, 'precision': 'ms', 'fill':'none', 'min': True, 'max': True, 'sum': True, 'avg': True, 'median': False, 'mode': False, 'stddev': False, 'spread': False }):\n\n    header = {'Accept': 'text/csv'}\n    endpoint = '/summary/device/{0}/port/{1}/sensor/{2}/{3}/{4}{5}'.format( edgeId, port, sensor, resolveStrategy(strategy), interval, utils.buildGetParams(params) )\n    response = utils.httpGetRequest(portiaConfig, endpoint, header)\n\n    if response.status_code == 200:\n        try:\n\n            dimensionSeries = pandas.read_csv( StringIO(response.text), sep=';' )\n            if portiaConfig['debug']:\n                print( '[portia-debug]: {0} rows'.format( len(dimensionSeries.index) ) )\n\n            return dimensionSeries\n\n        except:\n            raise Exception('couldn\\'t create pandas data frame')\n    else:\n        raise Exception('couldn\\'t retrieve data')", "response": "Returns a pandas data frame with the portia select resultset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a counterexample in the Rivest - Schapire way.", "response": "def _process_counter_example(self, mma, w_string):\n        \"\"\"\"\n        Process a counterexample in the Rivest-Schapire way.\n        Args:\n            mma (DFA): The hypothesis automaton\n            w_string (str): The examined string to be consumed\n        Returns:\n            None\n        \"\"\"\n        diff = len(w_string)\n        same = 0\n        membership_answer = self._membership_query(w_string)\n        while True:\n            i = (same + diff) / 2\n            access_string = self._run_in_hypothesis(mma, w_string, i)\n            if membership_answer != self._membership_query(access_string + w_string[i:]):\n                diff = i\n            else:\n                same = i\n            if diff - same == 1:\n                break\n        exp = w_string[diff:]\n        self.observation_table.em_vector.append(exp)\n        for row in self.observation_table.sm_vector + self.observation_table.smi_vector:\n            self._fill_table_entry(row, exp)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a DFA that is a conjecture of the current state of the Alphabetic Algebra.", "response": "def get_dfa_conjecture(self):\n        \"\"\"\n        Utilize the observation table to construct a Mealy Machine.\n        The library used for representing the Mealy Machine is the python\n        bindings of the openFST library (pyFST).\n        Args:\n            None\n        Returns:\n            MealyMachine: A mealy machine build based on a closed and consistent\n        observation table.\n        \"\"\"\n        dfa = DFA(self.alphabet)\n        for s in self.observation_table.sm_vector:\n            for i in self.alphabet:\n                dst = self.observation_table.equiv_classes[s + i]\n                # If dst == None then the table is not closed.\n                if dst == None:\n                    logging.debug('Conjecture attempt on non closed table.')\n                    return None\n                obsrv = self.observation_table[s, i]\n                src_id = self.observation_table.sm_vector.index(s)\n                dst_id = self.observation_table.sm_vector.index(dst)\n                dfa.add_arc(src_id, dst_id, i, obsrv)\n\n        # Mark the final states in the hypothesis automaton.\n        i = 0\n        for s in self.observation_table.sm_vector:\n            dfa[i].final = self.observation_table[s, self.epsilon]\n            i += 1\n        return dfa"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize the observation table.", "response": "def _init_table(self):\n        \"\"\"\n        Initialize the observation table.\n        \"\"\"\n        self.observation_table.sm_vector.append(self.epsilon)\n        self.observation_table.smi_vector = list(self.alphabet)\n        self.observation_table.em_vector.append(self.epsilon)\n\n        self._fill_table_entry(self.epsilon, self.epsilon)\n        for s in self.observation_table.smi_vector:\n            self._fill_table_entry(s, self.epsilon)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimplements the high level loop of the algorithm for learning a Mealy machine. Args: mma (DFA): The input automaton Returns: MealyMachine: A string and a model for the Mealy machine to be learned.", "response": "def learn_dfa(self, mma=None):\n        \"\"\"\n        Implements the high level loop of the algorithm for learning a\n        Mealy machine.\n        Args:\n            mma (DFA): The input automaton\n        Returns:\n            MealyMachine: A string and a model for the Mealy machine to be learned.\n        \"\"\"\n        logging.info('Initializing learning procedure.')\n        if mma:\n            self._init_table_from_dfa(mma)\n        else:\n            self._init_table()\n\n        logging.info('Generating a closed and consistent observation table.')\n        while True:\n\n            closed = False\n            # Make sure that the table is closed\n            while not closed:\n                logging.debug('Checking if table is closed.')\n                closed, string = self.observation_table.is_closed()\n                if not closed:\n                    logging.debug('Closing table.')\n                    self._ot_make_closed(string)\n                else:\n                    logging.debug('Table closed.')\n\n            # Create conjecture\n\n            dfa = self.get_dfa_conjecture()\n\n            logging.info('Generated conjecture machine with %d states.',len(list(dfa.states)))\n\n            # _check correctness\n            logging.debug('Running equivalence query.')\n            found, counter_example = self._equivalence_query(dfa)\n\n            # Are we done?\n            if found:\n                logging.info('No counterexample found. Hypothesis is correct!')\n                break\n\n            # Add the new experiments into the table to reiterate the\n            # learning loop\n            logging.info('Processing counterexample %s with length %d.', counter_example, len(counter_example))\n            self._process_counter_example(dfa, counter_example)\n\n        logging.info('Learning complete.')\n        logging.info('Learned em_vector table is the following:')\n        logging.info(self.observation_table.em_vector)\n        return '', dfa"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_error_to_io_stream(err: Exception, io: TextIOBase, print_big_traceback : bool = True):\n    if print_big_traceback:\n        traceback.print_tb(err.__traceback__, file=io, limit=-GLOBAL_CONFIG.multiple_errors_tb_limit)\n    else:\n        traceback.print_tb(err.__traceback__, file=io, limit=-1)\n    io.writelines('  ' + str(err.__class__.__name__) + ' : ' + str(err))", "response": "Utility method to print an exception s content to a stream"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if we can hide the traceback in the warnings messages.", "response": "def should_hide_traceback(e):\n    \"\"\" Returns True if we can hide the error traceback in the warnings messages \"\"\"\n    if type(e) in {WrongTypeCreatedError, CascadeError, TypeInformationRequiredError}:\n        return True\n    elif type(e).__name__ in {'InvalidAttributeNameForConstructorError', 'MissingMandatoryAttributeFiles'}:\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a parsing plan for the multifile children of the object.", "response": "def _get_parsing_plan_for_multifile_children(self, obj_on_fs: PersistedObject, desired_type: Type[Any],\n                                                 logger: Logger) -> Dict[str, Any]:\n        \"\"\"\n        Implementation of AnyParser API\n        \"\"\"\n        raise Exception('This should never happen, since this parser relies on underlying parsers')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_parser_to_cascade(self, parser: AnyParser, typ: Type = None):\n        # the first parser added will configure the cascade\n        if not self.configured:\n            self.supported_exts = parser.supported_exts\n            self.supported_types = parser.supported_types\n\n        # check if new parser is compliant with previous ones\n        if self.supports_singlefile():\n            if not parser.supports_singlefile():\n                raise ValueError(\n                    'Cannot add this parser to this parsing cascade : it does not match the rest of the cascades '\n                    'configuration (singlefile support)')\n\n        if self.supports_multifile():\n            if not parser.supports_multifile():\n                raise ValueError(\n                    'Cannot add this parser to this parsing cascade : it does not match the rest of the cascades '\n                    'configuration (multifile support)')\n\n        if AnyObject not in parser.supported_types:\n            if typ is None:\n                # in that case the expected types for this parser will be self.supported_types\n                if AnyObject in self.supported_types:\n                    raise ValueError(\n                        'Cannot add this parser to this parsing cascade : it does not match the rest of the cascades '\n                        'configuration (the cascade supports any type while the parser only supports '\n                        + str(parser.supported_types) + ')')\n                else:\n                    missing_types = set(self.supported_types) - set(parser.supported_types)\n                    if len(missing_types) > 0:\n                        raise ValueError(\n                            'Cannot add this parser to this parsing cascade : it does not match the rest of the '\n                            'cascades configuration (supported types should at least contain the supported types '\n                            'already in place. The parser misses type(s) ' + str(missing_types) + ')')\n            else:\n                # a parser is added but with a specific type target (parallel cascade)\n                if typ == AnyObject:\n                    raise ValueError(\n                        'Cannot add this parser to this parsing cascade : it does not match the expected type \"Any\", '\n                        'it only supports ' + str(parser.supported_types))\n                # else:\n                #     if get_base_generic_type(typ) not in parser.supported_types:\n                #         raise ValueError(\n                #             'Cannot add this parser to this parsing cascade : it does not match the expected type ' +\n                #             str(typ) + ', it only supports ' + str(parser.supported_types))\n\n        missing_exts = set(self.supported_exts) - set(parser.supported_exts)\n        if len(missing_exts) > 0:\n            raise ValueError(\n                'Cannot add this parser to this parsing cascade : it does not match the rest of the cascades '\n                'configuration (supported extensions should at least contain the supported extensions already in '\n                'place. The parser misses extension(s) ' + str(missing_exts) + ')')\n\n        # finally add it\n        self._parsers_list.append((typ, parser))", "response": "Adds the provided parser to the cascade."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_parsing_plan(self, desired_type: Type[T], filesystem_object: PersistedObject, logger: Logger,\n                             log_only_last: bool = False) -> ParsingPlan[T]:\n        \"\"\"\n        Creates a parsing plan to parse the given filesystem object into the given desired_type.\n        This overrides the method in AnyParser, in order to provide a 'cascading' parsing plan\n\n        :param desired_type:\n        :param filesystem_object:\n        :param logger:\n        :param log_only_last: a flag to only log the last part of the file path (default False)\n        :return:\n        \"\"\"\n        # build the parsing plan\n        logger.debug('(B) ' + get_parsing_plan_log_str(filesystem_object, desired_type,\n                                                       log_only_last=log_only_last, parser=self))\n        return CascadingParser.CascadingParsingPlan(desired_type, filesystem_object, self, self._parsers_list,\n                                                    logger=logger)", "response": "Creates a parsing plan for the given filesystem object into the given desired_type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single file into the desired type.", "response": "def _parse_singlefile(self, desired_type: Type[T], file_path: str, encoding: str, logger: Logger,\n                          options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Implementation of AnyParser API\n        \"\"\"\n        # first use the base parser to parse something compliant with the conversion chain\n        first = self._base_parser._parse_singlefile(self._converter.from_type, file_path, encoding,\n                                                    logger, options)\n\n        # then apply the conversion chain\n        return self._converter.convert(desired_type, first, logger, options)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a parsing plan for the given object on the given filesystem.", "response": "def _get_parsing_plan_for_multifile_children(self, obj_on_fs: PersistedObject, desired_type: Type[Any],\n                                                 logger: Logger) -> Dict[str, Any]:\n        \"\"\"\n        Implementation of AnyParser API\n        \"\"\"\n        return self._base_parser._get_parsing_plan_for_multifile_children(obj_on_fs, self._converter.from_type, logger)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a multifile object.", "response": "def _parse_multifile(self, desired_type: Type[T], obj: PersistedObject,\n                         parsing_plan_for_children: Dict[str, ParsingPlan],\n                         logger: Logger, options: Dict[str, Dict[str, Any]]) -> T:\n        \"\"\"\n        Implementation of AnyParser API\n        \"\"\"\n        # first use the base parser\n        # first = self._base_parser._parse_multifile(desired_type, obj, parsing_plan_for_children, logger, options)\n        first = self._base_parser._parse_multifile(self._converter.from_type, obj, parsing_plan_for_children, logger,\n                                                   options)\n\n        # then apply the conversion chain\n        return self._converter.convert(desired_type, first, logger, options)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_mode(self, mode):\n        _LOGGER.debug('State change called from alarm device')\n        if not mode:\n            _LOGGER.info('No mode supplied')\n        elif mode not in CONST.ALL_MODES:\n            _LOGGER.warning('Invalid mode')\n        response_object = self._lupusec.set_mode(CONST.MODE_TRANSLATION[mode])\n        if response_object['result'] != 1:\n            _LOGGER.warning('Mode setting unsuccessful')\n\n        self._json_state['mode'] = mode\n        _LOGGER.info('Mode set to: %s', mode)\n        return True", "response": "Set Lupusec alarm mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the amendement summary from the json response.", "response": "def parse_amendements_summary(url, json_response):\n    \"\"\"\n    json schema :\n    {\n      infoGenerales: {\n        nb_resultats, debut, nb_docs\n      },\n      data_table: 'id|numInit|titreDossierLegislatif|urlDossierLegislatif|'\n                  'instance|numAmend|urlAmend|designationArticle|'\n                  'designationAlinea|dateDepot|signataires|sort'\n    }\n\n    NB : the json response does not contain the dispositif and expose, that's\n    why we call it \"amendement's summary\"\n    \"\"\"\n\n    amendements = []\n    fields = [convert_camelcase_to_underscore(field) for field in\n              json_response['infoGenerales']['description_schema'].split('|')]\n\n    for row in json_response['data_table']:\n        values = row.split('|')\n        amd = AmendementSummary(**dict(zip(fields, values)))\n        amd.legislature = re.search(r'www.assemblee-nationale.fr/(\\d+)/',\n                                    amd.url_amend).groups()[0]\n        amendements.append(amd)\n\n    return AmendementSearchResult(**{\n        'url': url,\n        'total_count': json_response['infoGenerales']['nb_resultats'],\n        'start': json_response['infoGenerales']['debut'],\n        'size': json_response['infoGenerales']['nb_docs'],\n        'results': amendements\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of amendements for a given user.", "response": "def get(self, **kwargs):\n        \"\"\"\n        :param texteRecherche:\n        :param numAmend:\n        :param idArticle:\n        :param idAuteur:\n        :param idDossierLegislatif:\n        :param idExamen:\n        :param idExamens:\n        :param periodeParlementaire:\n        :param dateDebut:\n        :param dateFin:\n        :param rows:\n        :param start:\n        :param sort:\n        \"\"\"\n        params = self.default_params.copy()\n        params.update(kwargs)\n\n        start = time.time()\n        response = requests.get(self.base_url, params=params)\n        end = time.time()\n\n        LOGGER.debug(\n            'fetched amendements with search params: %s in %0.2f s',\n            params,\n            end - start\n        )\n\n        return parse_amendements_summary(response.url, response.json())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string encoded using UTF - 8.", "response": "def to_utf8(value):\n  \"\"\"Returns a string encoded using UTF-8.\n\n  This function comes from `Tornado`_.\n\n  :param value:\n    A unicode or string to be encoded.\n  :returns:\n    The encoded string.\n  \"\"\"\n  if isinstance(value, unicode):\n    return value.encode('utf-8')\n\n  assert isinstance(value, str)\n  return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_unicode(value):\n  if isinstance(value, str):\n    return value.decode('utf-8')\n\n  assert isinstance(value, unicode)\n  return value", "response": "Returns a unicode string from a string using UTF - 8 to decode if needed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_all_commands(management_dir):\n    try:\n        #Find all commands in the directory that are not __init__.py and end in .py.  Then, remove the trailing .py\n        return [f[:-3] for f in os.listdir(management_dir) if f.endswith('.py') and not f.startswith(\"__\")]\n    except OSError:\n        #If nothing is found, return empty\n        return []", "response": "Find all valid commands in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_commands_module(app_name):\n    parts = app_name.split('.')\n    parts.append('commands')\n    parts.reverse()\n    part = parts.pop()\n    path = None\n\n    #Load the module if needed\n    try:\n        f, path, descr = imp.find_module(part, path)\n    except ImportError as e:\n        if os.path.basename(os.getcwd()) != part:\n            raise e\n        else:\n            try:\n                if f:\n                    f.close()\n            except UnboundLocalError:\n                log.error(\"Could not import module {0} at path {1}.  Sys.path is {2}\".format(part, path, sys.path))\n\n    #Go down level by and level and try to load the module at each level\n    while parts:\n        part = parts.pop()\n        f, path, descr = imp.find_module(part, [path] if path else None)\n        if f:\n            f.close()\n    return path", "response": "Find the commands module in each app and return the path to the commands module"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_commands():\n    commands = {}\n\n    #Try to load the settings file (settings can be specified on the command line) and get the INSTALLED_APPS\n    try:\n        from percept.conf.base import settings\n        apps = settings.INSTALLED_APPS\n    except KeyError:\n        apps = []\n\n    #For each app, try to find the command module (command folder in the app)\n    #Then, try to load all commands in the directory\n    for app_name in apps:\n        try:\n            path = find_commands_module(app_name)\n            commands.update(dict([(name, app_name) for name in find_all_commands(path)]))\n        except ImportError as e:\n            pass\n\n    return commands", "response": "Get all valid commands in dictionary form\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the command with the command line arguments AttributeNames", "response": "def execute(self):\n        \"\"\"\n        Run the command with the command line arguments\n        \"\"\"\n        #Initialize the option parser\n        parser = LaxOptionParser(\n            usage=\"%prog subcommand [options] [args]\",\n            option_list=BaseCommand.option_list #This will define what is allowed input to the parser (ie --settings=)\n        )\n\n        #Parse the options\n        options, args = parser.parse_args(self.argv)\n        #Handle --settings and --pythonpath properly\n        options = handle_default_options(options)\n\n        try:\n            #Get the name of the subcommand\n            subcommand = self.argv[1]\n        except IndexError:\n            #If the subcommand name cannot be found, set it to help\n            subcommand = 'help'\n\n        #If the subcommand is help, print the usage of the parser, and available command names\n        if subcommand == 'help':\n            if len(args) <= 2:\n                parser.print_help()\n                sys.stdout.write(self.help_text + '\\n')\n        else:\n            #Otherwise, run the given command\n            self.fetch_command(subcommand).run_from_argv(self.argv)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat and prints the help text from the command list", "response": "def help_text(self):\n        \"\"\"\n        Formats and prints the help text from the command list\n        \"\"\"\n        help_text = '\\n'.join(sorted(get_commands().keys()))\n        help_text = \"\\nCommands:\\n\" + help_text\n        return help_text"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef missing(self, field, last=True):\n        '''\n        Numeric fields support specific handling for missing fields in a doc.\n        The missing value can be _last, _first, or a custom value\n        (that will be used for missing docs as the sort value).\n\n        missing('price')\n        > {\"price\" : {\"missing\": \"_last\" } }\n        missing('price',False)\n        > {\"price\" : {\"missing\": \"_first\"} }\n        '''\n        if last:\n            self.append({field: {'missing': '_last'}})\n        else:\n            self.append({field: {'missing': '_first'}})\n\n        return self", "response": "Add missing value to the list of missing values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ensure_table(self, cls):\n        cur = self._conn().cursor()\n\n        table_name = cls.get_table_name()\n        index_names = cls.index_names() or []\n\n        cols = ['id text primary key', 'value text']\n        for name in index_names:\n            cols.append(name + ' text')\n\n        cur.execute('create table if not exists %s (%s)' % (\n            table_name,\n            ','.join(cols)\n        ))\n\n        for name in index_names:\n            cur.execute('create index if not exists %s on %s(%s)' % (\n                table_name + '_' + name + '_idx',\n                table_name,\n                name\n            ))\n\n        self._conn().commit()\n        cur.close()", "response": "Ensure that the table exists - as per the gludb spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_by_index(self, cls, index_name, value):\n        cur = self._conn().cursor()\n\n        query = 'select id,value from %s where %s = ?' % (\n            cls.get_table_name(),\n            index_name\n        )\n\n        found = []\n        for row in cur.execute(query, (value,)):\n            id, data = row[0], row[1]\n            obj = cls.from_data(data)\n            assert id == obj.id\n            found.append(obj)\n\n        cur.close()\n\n        return found", "response": "Find all rows matching index query - as per the gludb spec."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(self):\n        register_url = self.base_url + \"api/0.1.0/register\"\n        register_headers = {\n            \"apikey\": str(self.owner_api_key),\n            \"resourceID\": str(self.entity_id),\n            \"serviceType\": \"publish,subscribe,historicData\"\n        }\n        with self.no_ssl_verification():\n            r = requests.get(register_url, {}, headers=register_headers)\n        response = r.content.decode(\"utf-8\")\n        if \"APIKey\" in str(r.content.decode(\"utf-8\")):\n            response = json.loads(response[:-331] + \"}\")  # Temporary fix to a middleware bug, should be removed in future\n            response[\"Registration\"] = \"success\"\n        else:\n            response = json.loads(response)\n            response[\"Registration\"] = \"failure\"\n        return response", "response": "Registers a new device with the name entity_id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrequest module fails due to lets encrypt ssl encryption. Will be fixed in the future release.", "response": "def no_ssl_verification(self):\n        \"\"\" Requests module fails due to lets encrypt ssl encryption. Will be fixed in the future release.\"\"\"\n        try:\n            from functools import partialmethod\n        except ImportError:\n            # Python 2 fallback: https://gist.github.com/carymrobbins/8940382\n            from functools import partial\n\n            class partialmethod(partial):\n                def __get__(self, instance, owner):\n                    if instance is None:\n                        return self\n\n                    return partial(self.func, instance, *(self.args or ()), **(self.keywords or {}))\n\n        old_request = requests.Session.request\n        requests.Session.request = partialmethod(old_request, verify=False)\n        warnings.filterwarnings('ignore', 'Unverified HTTPS request')\n        yield\n        warnings.resetwarnings()\n        requests.Session.request = old_request"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef publish(self, data):\n        if self.entity_api_key == \"\":\n            return {'status': 'failure', 'response': 'No API key found in request'}\n        publish_url = self.base_url + \"api/0.1.0/publish\"\n        publish_headers = {\"apikey\": self.entity_api_key}\n        publish_data = {\n            \"exchange\": \"amq.topic\",\n            \"key\": str(self.entity_id),\n            \"body\": str(data)\n        }\n        with self.no_ssl_verification():\n            r = requests.post(publish_url, json.dumps(publish_data), headers=publish_headers)\n        response = dict()\n        if \"No API key\" in str(r.content.decode(\"utf-8\")):\n            response[\"status\"] = \"failure\"\n            r = json.loads(r.content.decode(\"utf-8\"))['message']\n        elif 'publish message ok' in str(r.content.decode(\"utf-8\")):\n            response[\"status\"] = \"success\"\n            r = r.content.decode(\"utf-8\")\n        else:\n            response[\"status\"] = \"failure\"\n            r = r.content.decode(\"utf-8\")\n        response[\"response\"] = str(r)\n        return response", "response": "This function allows an entity to publish data to the middleware."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subscribe(self, devices_to_bind=[]):\n        if self.entity_api_key == \"\":\n            return {'status': 'failure', 'response': 'No API key found in request'}\n        self.bind(devices_to_bind)\n        loop = asyncio.new_event_loop()\n        t1 = threading.Thread(target=self.start_subscribe_worker, args=(loop,))\n        t1.daemon = True\n        t1.start()", "response": "This function allows an entity to subscribe for data from the devices specified in the bind operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts the event loop as a thread and run until complete.", "response": "def start_subscribe_worker(self, loop):\n        \"\"\" Switch to new event loop as a thread and run until complete. \"\"\"\n        url = self.base_url + \"api/0.1.0/subscribe\"\n        task = loop.create_task(self.asynchronously_get_data(url + \"?name={0}\".format(self.entity_id)))\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(task)\n        self.event_loop = loop"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def asynchronously_get_data(self, url):\n        headers = {\"apikey\": self.entity_api_key}\n        try:\n            async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(verify_ssl=False)) as session:\n                async with session.get(url, headers=headers, timeout=3000) as response:\n                    while True:  # loop over for each chunk of data\n                        chunk = await response.content.readchunk()\n                        if not chunk:\n                            break\n                        if platform == \"linux\" or platform == \"linux2\": # In linux systems, readchunk() returns a tuple\n                            chunk = chunk[0]\n                        resp = dict()\n                        resp[\"data\"] = chunk.decode()\n                        current_milli_time = lambda: int(round(time() * 1000))\n                        resp[\"timestamp\"] = str(current_milli_time())\n                        self.subscribe_data = resp\n        except Exception as e:\n            print(\"\\n*********    Oops: \" + url + \" \" + str(type(e)) + str(e) + \"     *********\\n\")\n        print('\\n*********    Closing TCP: {}     *********\\n'.format(url))", "response": "Asynchronously get data from Chunked transfer encoding of https://smartcity. rbccps. org."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop_subscribe(self):\n        asyncio.gather(*asyncio.Task.all_tasks()).cancel()\n        self.event_loop.stop()\n        self.event_loop.close()", "response": "Stop the event loop created when subscribe is called."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a nicely formatted string of the time between d and now.", "response": "def timesince(d, now=None):\n    \"\"\"\n    Takes two datetime objects and returns the time between d and now\n    as a nicely formatted string, e.g. \"10 minutes\".  If d occurs after now,\n    then \"0 minutes\" is returned.\n\n    Units used are years, months, weeks, days, hours, and minutes.\n    Seconds and microseconds are ignored.  Up to two adjacent units will be\n    displayed.  For example, \"2 weeks, 3 days\" and \"1 year, 3 months\" are\n    possible outputs, but \"2 weeks, 3 hours\" and \"1 year, 5 days\" are not.\n\n    Adapted from http://blog.natbat.co.uk/archive/2003/Jun/14/time_since\n    \"\"\"\n    chunks = (\n      (60 * 60 * 24 * 365, lambda n: ungettext('year', 'years', n)),\n      (60 * 60 * 24 * 30, lambda n: ungettext('month', 'months', n)),\n      (60 * 60 * 24 * 7, lambda n : ungettext('week', 'weeks', n)),\n      (60 * 60 * 24, lambda n : ungettext('day', 'days', n)),\n      (60 * 60, lambda n: ungettext('hour', 'hours', n)),\n      (60, lambda n: ungettext('minute', 'minutes', n))\n    )\n    # Convert datetime.date to datetime.datetime for comparison.\n    if not isinstance(d, datetime.datetime):\n        d = datetime.datetime(d.year, d.month, d.day)\n    if now and not isinstance(now, datetime.datetime):\n        now = datetime.datetime(now.year, now.month, now.day)\n\n    if not now:\n        if d.tzinfo:\n            now = datetime.datetime.now(LocalTimezone(d))\n        else:\n            now = datetime.datetime.now()\n\n    # ignore microsecond part of 'd' since we removed it from 'now'\n    delta = now - (d - datetime.timedelta(0, 0, d.microsecond))\n    since = delta.days * 24 * 60 * 60 + delta.seconds\n    if since <= 0:\n        # d is in the future compared to now, stop processing.\n        return u'0 ' + ugettext('minutes')\n    for i, (seconds, name) in enumerate(chunks):\n        count = since // seconds\n        if count != 0:\n            break\n    s = ugettext('%(number)d %(type)s') % {'number': count, 'type': name(count)}\n    if i + 1 < len(chunks):\n        # Now get the second item\n        seconds2, name2 = chunks[i + 1]\n        count2 = (since - (seconds * count)) // seconds2\n        if count2 != 0:\n            s += ugettext(', %(number)d %(type)s') % {'number': count2, 'type': name2(count2)}\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timeuntil(d, now=None):\n    if not now:\n        if getattr(d, 'tzinfo', None):\n            now = datetime.datetime.now(LocalTimezone(d))\n        else:\n            now = datetime.datetime.now()\n    return timesince(now, d)", "response": "Like timesince but returns the time until the given time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef updateCache(self, service, url, new_data, new_data_dt):\n        key = self._get_key(service, url)\n\n        # clear existing data\n        try:\n            value = self.client.get(key)\n            if value:\n                data = pickle.loads(value, encoding=\"utf8\")\n                if \"time_stamp\" in data:\n                    cached_data_dt = parse(data[\"time_stamp\"])\n                    if new_data_dt > cached_data_dt:\n                        self.client.delete(key)\n                        # may raise MemcachedException\n                        logger.info(\n                            \"IN cache (key: {}), older DELETE\".format(key))\n                    else:\n                        logger.info(\n                            \"IN cache (key: {}), newer KEEP\".format(key))\n                        return\n            else:\n                logger.info(\"NOT IN cache (key: {})\".format(key))\n\n        except MemcachedException as ex:\n            logger.error(\n                \"Clear existing data (key: {}) ==> {}\".format(key, str(ex)))\n            return\n\n        # store new value in cache\n        cdata, time_to_store = self._make_cache_data(\n            service, url, new_data, {}, 200, new_data_dt)\n\n        self.client.set(key, cdata, time=time_to_store)\n        # may raise MemcachedException\n        logger.info(\n            \"MemCached SET (key {}) for {:d} seconds\".format(\n                key, time_to_store))", "response": "Update the cache with the new data and time stamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_all_eggs(self):\n        path_to_delete = os.path.join(self.egg_directory, \"lib\", \"python\")\n        if os.path.exists(path_to_delete):\n            shutil.rmtree(path_to_delete)", "response": "delete all eggs in the directory specified"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_egg(self, egg_name):\n        if not os.path.exists(self.egg_directory):\n            os.makedirs(self.egg_directory)\n        self.requirement_set.add_requirement(\n            InstallRequirement.from_line(egg_name, None))\n        try:\n            self.requirement_set.prepare_files(self.finder)\n            self.requirement_set.install(['--prefix=' + self.egg_directory], [])\n        except DistributionNotFound:\n            self.requirement_set.requirements._keys.remove(egg_name)\n            raise PipException()", "response": "Install an egg into the egg directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new sequence of items from the iterable with the specified arguments.", "response": "def call(cls, iterable, *a, **kw):\n    \"\"\"\n    Calls every item in *iterable* with the specified arguments.\n    \"\"\"\n\n    return cls(x(*a, **kw) for x in iterable)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map(cls, iterable, func, *a, **kw):\n\n    return cls(func(x, *a, **kw) for x in iterable)", "response": "Returns a new sequence by applying func to each element of iterable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new sequence with elements from iterable that satisfy cond.", "response": "def filter(cls, iterable, cond, *a, **kw):\n    \"\"\"\n    Iterable-first replacement of Python's built-in `filter()` function.\n    \"\"\"\n\n    return cls(x for x in iterable if cond(x, *a, **kw))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unique(cls, iterable, key=None):\n\n    if key is None:\n      key = lambda x: x\n    def generator():\n      seen = set()\n      seen_add = seen.add\n      for item in iterable:\n        key_val = key(item)\n        if key_val not in seen:\n          seen_add(key_val)\n          yield item\n    return cls(generator())", "response": "Returns a new list with unique items from iterable whilst preserving the original order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chunks(cls, iterable, n, fill=None):\n\n    return cls(itertools.zip_longest(*[iter(iterable)] * n, fillvalue=fill))", "response": "Construct a new iterable that contains at most n elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef concat(cls, iterables):\n\n    def generator():\n      for it in iterables:\n        for element in it:\n          yield element\n    return cls(generator())", "response": "Returns a new sequence from the given iterable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chain(cls, *iterables):\n\n    def generator():\n      for it in iterables:\n        for element in it:\n          yield element\n    return cls(generator())", "response": "Returns a new object that is a chain of iterables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef attr(cls, iterable, attr_name):\n\n    return cls(getattr(x, attr_name) for x in iterable)", "response": "Returns a new object with the given attribute value applied to all elements of iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef of_type(cls, iterable, types):\n\n    return cls(x for x in iterable if isinstance(x, types))", "response": "Returns an iterable of the given types."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef partition(cls, iterable, pred):\n\n    t1, t2 = itertools.tee(iterable)\n    return cls(itertools.filterfalse(pred, t1), filter(pred, t2))", "response": "Create a new sequence from an iterable of items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of items in an iterable.", "response": "def count(cls, iterable):\n    \"\"\"\n    Returns the number of items in an iterable.\n    \"\"\"\n\n    iterable = iter(iterable)\n    count = 0\n    while True:\n      try:\n        next(iterable)\n      except StopIteration:\n        break\n      count += 1\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes raw SQL on the database and receive relaxing output.", "response": "def execute(self, sql, *args, **kwargs):\n    '''\n    Run raw SQL on the database, and receive relaxing output.\n    This is sort of the foundational method that most of the\n    others build on.\n    '''\n    try:\n      self.cursor.execute(sql, *args)\n    except self.sqlite3.InterfaceError, msg:\n      raise self.sqlite3.InterfaceError(unicode(msg) + '\\nTry converting types or pickling.')\n    rows = self.cursor.fetchall()\n\n    self.__commit_if_necessary(kwargs)\n\n    if None == self.cursor.description:\n      return None\n    else:\n      colnames = [d[0].decode('utf-8') for d in self.cursor.description]\n      rawdata = [OrderedDict(zip(colnames,row)) for row in rows]\n      return rawdata"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a unique index on the column ( s ) passed.", "response": "def create_index(self, columns, table_name, if_not_exists = True, unique = False, **kwargs):\n    'Create a unique index on the column(s) passed.'\n    index_name = simplify(table_name) + u'_' + u'_'.join(map(simplify, columns))\n    if unique:\n      sql = u'CREATE UNIQUE INDEX %s ON %s (%s)'\n    else:\n      sql = u'CREATE INDEX %s ON %s (%s)'\n\n    first_param = u'IF NOT EXISTS ' + index_name if if_not_exists else index_name\n\n    params = (first_param, quote(table_name), ','.join(map(quote, columns)))\n    self.execute(sql % params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_table(self, data, table_name, error_if_exists = False, **kwargs):\n    'Create a table based on the data, but don\\'t insert anything.'\n\n    converted_data = convert(data)\n    if len(converted_data) == 0 or converted_data[0] == []:\n      raise ValueError(u'You passed no sample values, or all the values you passed were null.')\n    else:\n      startdata = OrderedDict(converted_data[0])\n\n    # Select a non-null item\n    for k, v in startdata.items():\n      if v != None:\n        break\n    else:\n      v = None\n\n    if_not_exists = u'' if error_if_exists else u'IF NOT EXISTS'\n\n    # Do nothing if all items are null.\n    if v != None:\n      try:\n        # This is vulnerable to injection.\n        sql = u'''\n          CREATE TABLE %s %s (\n            %s %s\n          );''' % (if_not_exists, quote(table_name), quote(k), get_column_type(startdata[k]))\n        self.execute(sql, commit = False)\n      except:\n        raise\n      else:\n        self.commit()\n\n      for row in converted_data:\n        self.__check_and_add_columns(table_name, row)", "response": "Create a table based on the data but don t insert anything."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_var(self, key):\n    'Retrieve one saved variable from the database.'\n    vt = quote(self.__vars_table)\n    data = self.execute(u'SELECT * FROM %s WHERE `key` = ?' % vt, [key], commit = False)\n    if data == []:\n      raise NameError(u'The DumpTruck variables table doesn\\'t have a value for %s.' % key)\n    else:\n      tmp = quote(self.__vars_table_tmp)\n      row = data[0]\n\n      self.execute(u'DROP TABLE IF EXISTS %s' % tmp, commit = False)\n\n      # This is vulnerable to injection\n      self.execute(u'CREATE TEMPORARY TABLE %s (`value` %s)' % (tmp, row['type']), commit = False)\n\n      # This is ugly\n      self.execute(u'INSERT INTO %s (`value`) VALUES (?)' % tmp, [row['value']], commit = False)\n      value = self.dump(tmp)[0]['value']\n      self.execute(u'DROP TABLE %s' % tmp, commit = False)\n\n      return value", "response": "Retrieve one saved variable from the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave one variable to the database.", "response": "def save_var(self, key, value, **kwargs):\n    'Save one variable to the database.'\n\n    # Check whether Highwall's variables table exists\n    self.__check_or_create_vars_table()\n\n    column_type = get_column_type(value)\n    tmp = quote(self.__vars_table_tmp)\n\n    self.execute(u'DROP TABLE IF EXISTS %s' % tmp, commit = False)\n\n    # This is vulnerable to injection\n    self.execute(u'CREATE TABLE %s (`value` %s)' % (tmp, column_type), commit = False)\n\n    # This is ugly\n    self.execute(u'INSERT INTO %s (`value`) VALUES (?)' % tmp, [value], commit = False)\n    table = (quote(self.__vars_table), tmp)\n    params = [key, column_type]\n    self.execute(u'''\nINSERT OR REPLACE INTO %s (`key`, `type`, `value`)\n  SELECT\n    ? AS key,\n    ? AS type,\n    value\n  FROM %s\n''' % table, params)\n    self.execute(u'DROP TABLE %s' % tmp, commit = False)\n\n    self.__commit_if_necessary(kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sequence of tuples where type is either table or view.", "response": "def tablesAndViews(self):\n      \"\"\"Return a sequence of (name,type) pairs where type is\n      either \"table\" or \"view\".\"\"\"\n      result = self.execute(\n        u'SELECT name,type FROM sqlite_master WHERE type in (\"table\", \"view\")',\n        commit=False)\n      return ((row['name'],row['type']) for row in result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _install_p4v_osx(self, url, overwrite=False):\n        package_exists = False\n        root_dir = os.path.expanduser(os.path.join(\"~\", \"Applications\"))\n        package_exists = len([x for x in P4V_APPLICATIONS if os.path.exists(os.path.join(root_dir, x))])\n        if not package_exists or overwrite:\n            lib.extract_dmg(url, root_dir)\n        else:\n            self.logger.warn(\"P4V exists already in %s! Not overwriting...\" % root_dir)\n        return True", "response": "Install perforce applications and binaries for mac"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _install_p4v_linux(self, url):\n        lib.extract_targz(url,\n                          self.directory.install_directory(self.feature_name),\n                          remove_common_prefix=True)\n        bin_path = os.path.join(self.directory.install_directory(self.feature_name), 'bin')\n        if os.path.exists(bin_path):\n            for f in os.listdir(bin_path):\n                self.directory.symlink_to_bin(f, os.path.join(bin_path, f))\n        return True", "response": "Install perforce applications and binaries for linux"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconfigures the perforce client", "response": "def __configure_client(self, config):\n        \"\"\" write the perforce client \"\"\"\n        self.logger.info(\"Configuring p4 client...\")\n        client_dict = config.to_dict()\n        client_dict['root_path'] = os.path.expanduser(config.get('root_path'))\n        os.chdir(client_dict['root_path'])\n        client_dict['hostname'] = system.NODE\n        client_dict['p4view'] = config['p4view'] % self.environment.target.get_context_dict()\n        client = re.sub('//depot', '    //depot', p4client_template % client_dict)\n        self.logger.info(lib.call(\"%s client -i\" % self.p4_command,\n                                  stdin=client,\n                                  env=self.p4environ,\n                                  cwd=client_dict['root_path']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls eggs for a particular configuration", "response": "def __install_eggs(self, config):\n        \"\"\" Install eggs for a particular configuration \"\"\"\n        egg_carton = (self.directory.install_directory(self.feature_name),\n                      'requirements.txt')\n        eggs = self.__gather_eggs(config)\n\n        self.logger.debug(\"Installing eggs %s...\" % eggs)\n        self.__load_carton(egg_carton, eggs)\n\n        self.__prepare_eggs(egg_carton, config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd the proper paths into the environment", "response": "def __add_paths(self, config):\n        \"\"\" add the proper resources into the environment \"\"\"\n        bin_path = os.path.join(self.directory.install_directory(self.feature_name), 'bin')\n        whitelist_executables = self._get_whitelisted_executables(config)\n        for f in os.listdir(bin_path):\n            for pattern in BLACKLISTED_EXECUTABLES:\n                if re.match(pattern, f):\n                    continue\n            if whitelist_executables and f not in whitelist_executables:\n                continue\n            self.directory.symlink_to_bin(f, os.path.join(bin_path, f))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef analyse_body_paragraph(body_paragraph, labels=None):\n    # try to find leading label first:\n    for label, dummy in labels:\n        if body_paragraph.startswith('* ' + label):\n            return (label, body_paragraph[len(label) + 3:].replace('\\n  ',\n                                                                   ' '))\n    # no conformed leading label found; do we have leading asterisk?\n    if body_paragraph.startswith('* '):\n        return (None, body_paragraph[2:].replace('\\n  ', ' '))\n    # no leading asterisk found; ignore this paragraph silently:\n    return (None, None)", "response": "Analyse the body paragraph and return a tuple of the label and message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_ticket_directives(message):\n    if message:\n        message = re.sub(r'closes #', '#', message)\n        message = re.sub(r'addresses #', '#', message)\n        message = re.sub(r'references #', '#', message)\n    return message", "response": "Remove ticket directives from a message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning those git commit sha1s that have been amended later?", "response": "def amended_commits(commits):\n    \"\"\"Return those git commit sha1s that have been amended later.\"\"\"\n    # which SHA1 are declared as amended later?\n    amended_sha1s = []\n    for message in commits.values():\n        amended_sha1s.extend(re.findall(r'AMENDS\\s([0-f]+)', message))\n    return amended_sha1s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenriching git log with related information on tickets.", "response": "def enrich_git_log_dict(messages, labels):\n    \"\"\"Enrich git log with related information on tickets.\"\"\"\n    for commit_sha1, message in messages.items():\n        # detect module and ticket numbers for each commit:\n        component = None\n        title = message.split('\\n')[0]\n        try:\n            component, title = title.split(\":\", 1)\n            component = component.strip()\n        except ValueError:\n            pass  # noqa\n\n        paragraphs = [analyse_body_paragraph(p, labels)\n                      for p in message.split('\\n\\n')]\n        yield {\n            'sha1': commit_sha1,\n            'component': component,\n            'title': title.strip(),\n            'tickets': re.findall(r'\\s(#\\d+)', message),\n            'paragraphs': [\n                (label, remove_ticket_directives(message))\n                for label, message in paragraphs\n            ],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the context variables using the key value provided in the options", "response": "def set_default_var(**vars):\n    \"\"\"Sets context variables using the key/value provided in the options\n    \"\"\"\n    for k, v in vars.iteritems():\n        current_context.vars.setdefault(k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nincrements the context variables of the object.", "response": "def incr_obj(obj, **attrs):\n    \"\"\"Increments context variables\n    \"\"\"\n    for name, value in attrs.iteritems():\n        v = getattr(obj, name, None)\n        if not hasattr(obj, name) or v is None:\n            v = 0\n        setattr(obj, name, v + value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nredirect to the specified view or url", "response": "def redirect(view=None, url=None, **kwargs):\n    \"\"\"Redirects to the specified view or url\n    \"\"\"\n    if view:\n        if url:\n            kwargs[\"url\"] = url\n        url = flask.url_for(view, **kwargs)\n    current_context.exit(flask.redirect(url))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlines ( query ) -- print the number of lines in a given file", "response": "def lines(query):\n\t\"\"\"lines(query) -- print the number of lines in a given file\n\t\"\"\"\n\tfilename = support.get_file_name(query)\n\tif(os.path.isfile(filename)):\n\t\twith open(filename) as openfile:\n\t\t\tprint len(openfile.readlines())\n\telse:\n\t\tprint 'File not found : ' + filename"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nline ( query ) -- print the number of lines in a given file print", "response": "def words(query):\n\t\"\"\"lines(query) -- print the number of words in a given file\n\t\"\"\"\n\tfilename = support.get_file_name(query)\n\tif(os.path.isfile(filename)):\n\t\twith open(filename) as openfile:\n\t\t\tprint len(openfile.read().split())\n\telse:\n\t\tprint 'File not found : ' + filename"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint some human readable information of a given file", "response": "def file_info(query):\n\t\"\"\"file_info(query) -- print some human readable information of a given file\n\t\"\"\"\n\tfilename = support.get_file_name(query)\n\tif(os.path.isfile(filename)):\n\t\tstat_info = os.stat(filename)\n\t\towner_name = pwd.getpwuid(stat_info.st_uid).pw_name\n\t\tprint 'owner : ' + owner_name\n\t\tfile_size = support.get_readable_filesize(stat_info.st_size)\n\t\tprint 'size : ' + file_size\n\t\tprint 'created : ' + time.ctime(stat_info.st_ctime)\n\t\tprint 'last modified : ' + time.ctime(stat_info.st_mtime)\n\telse:\n\t\tprint 'file not found'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_to_path(query):\n\tnew_entry = support.get_path(query)\n\tif(new_entry):\n\t\tprint 'Adding '+new_entry+' to PATH variable.'\n\t\tprint '''1 : confirm\n\t\t2 : cancel\n\t\t'''\n\t\tchoice = int(raw_input('>> '))\n\t\tif(choice == 1):\n\t\t\thome_dir = os.path.expanduser('~')\n\t\t\tbashrc = open(os.path.join(home_dir, \".bashrc\"), \"a\")\n\t\t\tbashrc.write('\\n\\nexport PATH=\\\"'+new_entry+':$PATH\\\"\\n')\n\t\t\tbashrc.close()\n\t\t\tos.system('source '+os.path.join(os.path.expanduser('~'),'.bashrc'))\n\t\t\tprint 'Success!!'\n\t\t\tprint os.system('echo $PATH')\n\telse:\n\t\tprint 'We were unable to extract the \\'path\\' from your query.'", "response": "add user given path to environment PATH variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef system_info(query):\n\tproc = subprocess.Popen([\"uname -o\"], stdout=subprocess.PIPE, shell=True)\n\t(out, err) = proc.communicate()\n\tprint \"operating system : \"+str(out),\n\n\tproc = subprocess.Popen([\"uname\"], stdout=subprocess.PIPE, shell=True)\n\t(out, err) = proc.communicate()\n\tprint \"kernel : \"+str(out),\n\n\tproc = subprocess.Popen([\"uname -r\"], stdout=subprocess.PIPE, shell=True)\n\t(out, err) = proc.communicate()\n\tprint \"kernel release : \"+str(out),\n\n\tproc = subprocess.Popen([\"uname -m\"], stdout=subprocess.PIPE, shell=True)\n\t(out, err) = proc.communicate()\n\tprint \"architecture : \"+str(out),\n\n\tproc = subprocess.Popen([\"uname -n\"], stdout=subprocess.PIPE, shell=True)\n\t(out, err) = proc.communicate()\n\tprint \"network node name : \"+str(out),", "response": "system_info ( query ) -- print system specific information like operating system kernel release network node name etc."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render(template_name, template_vars={}, template_set='site', template_theme=None, template_extension='html', template_content=None):\n  global _jinja_env\n  \n  if not _jinja_env:\n    raise 'Jinja env not setup'\n\n  try:\n    _jinja_env.filters['timesince'] = timesince\n    _jinja_env.filters['timeuntil'] = timeuntil\n    _jinja_env.filters['date'] = date_format\n    _jinja_env.filters['time'] = time_format\n    _jinja_env.filters['shortdate'] = short_date\n    _jinja_env.filters['isodate'] = iso_date\n    _jinja_env.filters['rfcdate'] = rfc2822_date\n    _jinja_env.filters['tformat'] = datetimeformat\n    _jinja_env.filters['timestamp'] = timestamp\n  except NameError as errstr:\n    logging.info('Helper import error: %s' % errstr)\n\n  _template_name = \"%s.%s\" % (template_name, template_extension)\n  template = _jinja_env.get_template(_template_name, parent=template_theme)\n  \n  return template.render(template_vars)", "response": "Given a template path a template name and template variables will return rendered content using jinja2 library\n  \n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompile a Jinja2 template file to python code.", "response": "def compile_file(env, src_path, dst_path, encoding='utf-8', base_dir=''):\n  \"\"\"Compiles a Jinja2 template to python code.\n\n  :param env: a Jinja2 Environment instance.\n  :param src_path: path to the source file.\n  :param dst_path: path to the destination file.\n  :param encoding: template encoding.\n  :param base_dir: the base path to be removed from the compiled template filename.\n  \"\"\"\n  src_file = file(src_path, 'r')\n  source = src_file.read().decode(encoding)\n  name = src_path.replace(base_dir, '')\n  raw = env.compile(source, name=name, filename=name, raw=True)\n  src_file.close()\n\n  dst_file = open(dst_path, 'w')\n  dst_file.write(raw)\n  dst_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compile_dir(env, src_path, dst_path, pattern=r'^.*\\.html$', encoding='utf-8', base_dir=None):\n  from os import path, listdir, mkdir\n  file_re = re.compile(pattern)\n\n  if base_dir is None:\n    base_dir = src_path\n\n  for filename in listdir(src_path):\n    src_name = path.join(src_path, filename)\n    dst_name = path.join(dst_path, filename)\n\n    if path.isdir(src_name):\n      mkdir(dst_name)\n      compile_dir(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)\n    elif path.isfile(src_name) and file_re.match(filename):\n      compile_file(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)", "response": "Compiles a directory of Jinja2 templates to python code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noutput all recorded stats", "response": "def _pre_dump(cls):\n        \"\"\"Output all recorded stats\"\"\"\n        shutil.rmtree(cls.outdir, ignore_errors=True)\n        os.makedirs(cls.outdir)\n        super(PlotMetric, cls)._pre_dump()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _histogram(self, which, mu, sigma, data):\n\n        weights = np.ones_like(data)/len(data) # make bar heights sum to 100%\n        n, bins, patches = plt.hist(data, bins=25, weights=weights, facecolor='blue', alpha=0.5)\n\n        plt.title(r'%s %s: $\\mu=%.2f$, $\\sigma=%.2f$' % (self.name, which.capitalize(), mu, sigma))\n        plt.xlabel('Items' if which == 'count' else 'Seconds')\n        plt.ylabel('Frequency')\n        plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, position: \"{:.1f}%\".format(y*100)))", "response": "plot a histogram. For internal use only"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a scatter plot of count vs. elapsed", "response": "def _scatter(self):\n        \"\"\"plot a scatter plot of count vs. elapsed. For internal use only\"\"\"\n\n        plt.scatter(self.count_arr, self.elapsed_arr)\n        plt.title('{}: Count vs. Elapsed'.format(self.name))\n        plt.xlabel('Items')\n        plt.ylabel('Seconds')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbinds a worker function to the future.", "response": "def bind(self, __fun, *args, **kwargs):\n    \"\"\"\n    Bind a worker function to the future. This worker function will be\n    executed when the future is executed.\n    \"\"\"\n\n    with self._lock:\n      if self._running or self._completed or self._cancelled:\n        raise RuntimeError('Future object can not be reused')\n      if self._worker:\n        raise RuntimeError('Future object is already bound')\n      self._worker = functools.partial(__fun, *args, **kwargs)\n    return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a callback function to the internal list of done callbacks.", "response": "def add_done_callback(self, fun):\n    \"\"\"\n    Adds the callback *fun* to the future so that it be invoked when the\n    future completed. The future completes either when it has been completed\n    after being started with the :meth:`start` method (independent of whether\n    an error occurs or not) or when either :meth:`set_result` or\n    :meth:`set_exception` is called.\n\n    If the future is already complete, *fun* will be invoked directly.\n\n    The function *fun* must accept the future as its sole argument.\n    \"\"\"\n\n    with self._lock:\n      if self._completed:\n        fun()\n      else:\n        self._done_callbacks.append(fun)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmark the future as being enqueued in some kind of executor for futures. Calling :meth:`start()` with the *as_thread* parameter as :const:`True` will raise a :class:`RuntimeError` after this method has been called. This method will also validate the state of the future.", "response": "def enqueue(self):\n    \"\"\"\n    Mark the future as being enqueued in some kind of executor for futures.\n    Calling :meth:`start()` with the *as_thread* parameter as :const:`True`\n    will raise a :class:`RuntimeError` after this method has been called.\n\n    This method will also validate the state of the future.\n    \"\"\"\n\n    with self._lock:\n      if self._enqueued:\n        raise RuntimeError('Future object is already enqueued')\n      if self._running:\n        raise RuntimeError('Future object is already running')\n      if self._completed:\n        raise RuntimeError('Future object can not be restarted')\n      if not self._worker:\n        raise RuntimeError('Future object is not bound')\n      self._enqueued = True"}
