{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_download_conditions(self, lpath, rfile):\n        # type: (Downloader, pathlib.Path,\n        #        blobxfer.models.azure.StorageEntity) -> DownloadAction\n        \"\"\"Check for download conditions\n        :param Downloader self: this\n        :param pathlib.Path lpath: local path\n        :param blobxfer.models.azure.StorageEntity rfile: remote file\n        :rtype: DownloadAction\n        :return: download action\n        \"\"\"\n        if not lpath.exists():\n            if rfile.vectored_io is not None:\n                fpath = blobxfer.models.download.Descriptor.\\\n                    convert_vectored_io_slice_to_final_path_name(lpath, rfile)\n                if not fpath.exists():\n                    return DownloadAction.Download\n            else:\n                return DownloadAction.Download\n        if not self._spec.options.overwrite:\n            logger.info(\n                'not overwriting local file: {} (remote: {})'.format(\n                    lpath, rfile.path))\n            return DownloadAction.Skip\n        # check skip on options, MD5 match takes priority\n        md5 = blobxfer.models.metadata.get_md5_from_metadata(rfile)\n        if self._spec.skip_on.md5_match and blobxfer.util.is_not_empty(md5):\n            return DownloadAction.CheckMd5\n        # if neither of the remaining skip on actions are activated, download\n        if (not self._spec.skip_on.filesize_match and\n                not self._spec.skip_on.lmt_ge):\n            return DownloadAction.Download\n        # check skip on file size match\n        dl_fs = None\n        if self._spec.skip_on.filesize_match:\n            lsize = lpath.stat().st_size\n            if rfile.mode == blobxfer.models.azure.StorageModes.Page:\n                lsize = blobxfer.util.page_align_content_length(lsize)\n            if rfile.size == lsize:\n                dl_fs = False\n                if self._general_options.verbose:\n                    logger.debug('filesize match: {} == {} size={}'.format(\n                        lpath, rfile.path, lsize))\n            else:\n                dl_fs = True\n        # check skip on lmt ge\n        dl_lmt = None\n        if self._spec.skip_on.lmt_ge:\n            mtime = blobxfer.util.datetime_from_timestamp(\n                lpath.stat().st_mtime, as_utc=True)\n            if mtime >= rfile.lmt:\n                dl_lmt = False\n                if self._general_options.verbose:\n                    logger.debug('lmt ge match: {} lmt={} >= {} lmt={}'.format(\n                        lpath, mtime, rfile.path, rfile.lmt))\n            else:\n                dl_lmt = True\n        # download if either skip on mismatch is True\n        if dl_fs or dl_lmt:\n            return DownloadAction.Download\n        else:\n            return DownloadAction.Skip", "response": "Checks the download conditions for the local file and remote file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pre_md5_skip_on_check(self, lpath, rfile):\n        # type: (Downloader, pathlib.Path,\n        #        blobxfer.models.azure.StorageEntity) -> None\n        \"\"\"Perform pre MD5 skip on check\n        :param Downloader self: this\n        :param pathlib.Path lpath: local path\n        :param blobxfer.models.azure.StorageEntity rfile: remote file\n        \"\"\"\n        md5 = blobxfer.models.metadata.get_md5_from_metadata(rfile)\n        key = blobxfer.operations.download.Downloader.\\\n            create_unique_transfer_operation_id(rfile)\n        with self._md5_meta_lock:\n            self._md5_map[key] = rfile\n        slpath = str(lpath)\n        # temporarily create a download descriptor view for vectored io\n        if rfile.vectored_io is not None:\n            view, _ = blobxfer.models.download.Descriptor.generate_view(rfile)\n            fpath = str(\n                blobxfer.models.download.Descriptor.\n                convert_vectored_io_slice_to_final_path_name(lpath, rfile)\n            )\n        else:\n            view = None\n            fpath = slpath\n        self._md5_offload.add_localfile_for_md5_check(\n            key, slpath, fpath, md5, rfile.mode, view)", "response": "Perform pre MD5 skip on check\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _post_md5_skip_on_check(self, key, filename, size, md5_match):\n        # type: (Downloader, str, str, int, bool) -> None\n        \"\"\"Perform post MD5 skip on check\n        :param Downloader self: this\n        :param str key: md5 map key\n        :param str filename: local filename\n        :param int size: size of checked data\n        :param bool md5_match: if MD5 matches\n        \"\"\"\n        with self._md5_meta_lock:\n            rfile = self._md5_map.pop(key)\n        lpath = pathlib.Path(filename)\n        if md5_match:\n            if size is None:\n                size = lpath.stat().st_size\n            with self._transfer_lock:\n                self._transfer_set.remove(\n                    blobxfer.operations.download.Downloader.\n                    create_unique_transfer_operation_id(rfile))\n                self._download_total -= 1\n                self._download_bytes_total -= size\n            if self._general_options.dry_run:\n                logger.info('[DRY RUN] MD5 match, skipping: {} -> {}'.format(\n                    rfile.path, lpath))\n        else:\n            if self._general_options.dry_run:\n                with self._transfer_lock:\n                    self._transfer_set.remove(\n                        blobxfer.operations.download.Downloader.\n                        create_unique_transfer_operation_id(rfile))\n                    self._download_total -= 1\n                    self._download_bytes_total -= size\n                logger.info(\n                    '[DRY RUN] MD5 mismatch, download: {} -> {}'.format(\n                        rfile.path, lpath))\n            else:\n                self._add_to_download_queue(lpath, rfile)", "response": "Perform post MD5 skip on check on a given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_for_crypto_done(self):\n        # type: (Downloader) -> None\n        \"\"\"Check queue for crypto done\n        :param Downloader self: this\n        \"\"\"\n        cv = self._crypto_offload.done_cv\n        while not self.termination_check:\n            result = None\n            cv.acquire()\n            while True:\n                result = self._crypto_offload.pop_done_queue()\n                if result is None:\n                    # use cv timeout due to possible non-wake while running\n                    cv.wait(0.1)\n                    # check for terminating conditions\n                    if self.termination_check:\n                        break\n                else:\n                    break\n            cv.release()\n            if result is not None:\n                try:\n                    final_path, offsets = result\n                    with self._transfer_lock:\n                        dd = self._dd_map[final_path]\n                    self._finalize_chunk(dd, offsets)\n                except KeyError:\n                    # this can happen if all of the last integrity\n                    # chunks are processed at once\n                    pass", "response": "Check queue for crypto done and finalize the final transfer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_to_download_queue(self, lpath, rfile):\n        # type: (Downloader, pathlib.Path,\n        #        blobxfer.models.azure.StorageEntity) -> None\n        \"\"\"Add remote file to download queue\n        :param Downloader self: this\n        :param pathlib.Path lpath: local path\n        :param blobxfer.models.azure.StorageEntity rfile: remote file\n        \"\"\"\n        # prepare remote file for download\n        dd = blobxfer.models.download.Descriptor(\n            lpath, rfile, self._spec.options, self._general_options,\n            self._resume)\n        with self._transfer_lock:\n            self._transfer_cc[dd.entity.path] = 0\n            if dd.entity.is_encrypted:\n                self._dd_map[str(dd.final_path)] = dd\n        # add download descriptor to queue\n        self._transfer_queue.put(dd)\n        if self._download_start_time is None:\n            with self._transfer_lock:\n                if self._download_start_time is None:\n                    self._download_start_time = blobxfer.util.datetime_now()", "response": "Adds a remote file to the download queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _initialize_transfer_threads(self):\n        # type: (Downloader) -> None\n        \"\"\"Initialize transfer threads\n        :param Downloader self: this\n        \"\"\"\n        logger.debug('spawning {} transfer threads'.format(\n            self._general_options.concurrency.transfer_threads))\n        for _ in range(self._general_options.concurrency.transfer_threads):\n            thr = threading.Thread(target=self._worker_thread_transfer)\n            self._transfer_threads.append(thr)\n            thr.start()", "response": "Initialize threads for download threads."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _wait_for_disk_threads(self, terminate):\n        # type: (Downloader, bool) -> None\n        \"\"\"Wait for disk threads\n        :param Downloader self: this\n        :param bool terminate: terminate threads\n        \"\"\"\n        if terminate:\n            self._download_terminate = terminate\n        for thr in self._disk_threads:\n            blobxfer.util.join_thread(thr)", "response": "Wait for disk threads to be available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _worker_thread_transfer(self):\n        # type: (Downloader) -> None\n        \"\"\"Worker thread download\n        :param Downloader self: this\n        \"\"\"\n        max_set_len = self._general_options.concurrency.disk_threads << 2\n        while not self.termination_check:\n            try:\n                if len(self._disk_set) > max_set_len:\n                    time.sleep(0.1)\n                    continue\n                else:\n                    dd = self._transfer_queue.get(block=False, timeout=0.1)\n            except queue.Empty:\n                continue\n            try:\n                self._process_download_descriptor(dd)\n            except Exception as e:\n                with self._transfer_lock:\n                    self._exceptions.append(e)", "response": "This method is called by the worker thread to process the download of the next available entry in the transfer queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _worker_thread_disk(self):\n        # type: (Downloader) -> None\n        \"\"\"Worker thread for disk\n        :param Downloader self: this\n        \"\"\"\n        while not self.termination_check:\n            try:\n                dd, offsets, data = self._disk_queue.get(\n                    block=False, timeout=0.1)\n            except queue.Empty:\n                continue\n            try:\n                self._process_data(dd, offsets, data)\n            except Exception as e:\n                with self._transfer_lock:\n                    self._exceptions.append(e)", "response": "This function is called by the worker thread to process the data from the disk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a download descriptor and return the related object.", "response": "def _process_download_descriptor(self, dd):\n        # type: (Downloader, blobxfer.models.download.Descriptor) -> None\n        \"\"\"Process download descriptor\n        :param Downloader self: this\n        :param blobxfer.models.download.Descriptor dd: download descriptor\n        \"\"\"\n        # update progress bar\n        self._update_progress_bar()\n        # get download offsets\n        offsets, resume_bytes = dd.next_offsets()\n        # add resume bytes to counter\n        if resume_bytes is not None:\n            with self._disk_operation_lock:\n                self._download_bytes_sofar += resume_bytes\n                logger.debug('adding {} sofar {} from {}'.format(\n                    resume_bytes, self._download_bytes_sofar, dd.entity.name))\n            del resume_bytes\n        # check if all operations completed\n        if offsets is None and dd.all_operations_completed:\n            finalize = True\n            sfpath = str(dd.final_path)\n            # finalize integrity\n            dd.finalize_integrity()\n            # vectored io checks\n            if dd.entity.vectored_io is not None:\n                with self._transfer_lock:\n                    if sfpath not in self._vio_map:\n                        self._vio_map[sfpath] = 1\n                    else:\n                        self._vio_map[sfpath] += 1\n                    if (self._vio_map[sfpath] ==\n                            dd.entity.vectored_io.total_slices):\n                        self._vio_map.pop(sfpath)\n                    else:\n                        finalize = False\n            # finalize file\n            if finalize:\n                dd.finalize_file()\n            # accounting\n            with self._transfer_lock:\n                self._download_sofar += 1\n                if dd.entity.is_encrypted:\n                    self._dd_map.pop(sfpath)\n                self._transfer_set.remove(\n                    blobxfer.operations.download.Downloader.\n                    create_unique_transfer_operation_id(dd.entity))\n                self._transfer_cc.pop(dd.entity.path, None)\n            return\n        # re-enqueue for other threads to download\n        if offsets is None:\n            self._transfer_queue.put(dd)\n            return\n        # ensure forthcoming disk operation is accounted for\n        with self._disk_operation_lock:\n            self._disk_set.add(\n                blobxfer.operations.download.Downloader.\n                create_unique_disk_operation_id(dd, offsets))\n        # check if there are too many concurrent connections\n        with self._transfer_lock:\n            self._transfer_cc[dd.entity.path] += 1\n            cc_xfer = self._transfer_cc[dd.entity.path]\n        if cc_xfer <= self._spec.options.max_single_object_concurrency:\n            self._transfer_queue.put(dd)\n        # issue get range\n        if dd.entity.mode == blobxfer.models.azure.StorageModes.File:\n            data = blobxfer.operations.azure.file.get_file_range(\n                dd.entity, offsets)\n        else:\n            data = blobxfer.operations.azure.blob.get_blob_range(\n                dd.entity, offsets)\n        with self._transfer_lock:\n            self._transfer_cc[dd.entity.path] -= 1\n        if cc_xfer > self._spec.options.max_single_object_concurrency:\n            self._transfer_queue.put(dd)\n        # enqueue data for processing\n        self._disk_queue.put((dd, offsets, data))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_data(self, dd, offsets, data):\n        # type: (Downloader, blobxfer.models.download.Descriptor,\n        #        blobxfer.models.download.Offsets, bytes) -> None\n        \"\"\"Process downloaded data for disk\n        :param Downloader self: this\n        :param blobxfer.models.download.Descriptor dd: download descriptor\n        :param blobxfer.models.download.Offsets offsets: offsets\n        :param bytes data: data to process\n        \"\"\"\n        # decrypt if necessary\n        if dd.entity.is_encrypted:\n            # slice data to proper bounds and get iv for chunk\n            if offsets.chunk_num == 0:\n                # set iv\n                iv = dd.entity.encryption_metadata.content_encryption_iv\n                # set data to decrypt\n                encdata = data\n                # send iv through hmac\n                dd.hmac_iv(iv)\n            else:\n                # set iv\n                iv = data[:blobxfer.models.crypto.AES256_BLOCKSIZE_BYTES]\n                # set data to decrypt\n                encdata = data[blobxfer.models.crypto.AES256_BLOCKSIZE_BYTES:]\n            # write encdata to disk for hmac later\n            _hmac_datafile = dd.write_unchecked_hmac_data(\n                offsets, encdata)\n            # decrypt data\n            if self._crypto_offload is not None:\n                self._crypto_offload.add_decrypt_chunk(\n                    str(dd.final_path), dd.view.fd_start, offsets,\n                    dd.entity.encryption_metadata.symmetric_key,\n                    iv, _hmac_datafile)\n                # data will be integrity checked and written once\n                # retrieved from crypto queue\n                return\n            else:\n                data = blobxfer.operations.crypto.aes_cbc_decrypt_data(\n                    dd.entity.encryption_metadata.symmetric_key,\n                    iv, encdata, offsets.unpad)\n                dd.write_data(offsets, data)\n        else:\n            # write data to disk\n            dd.write_unchecked_data(offsets, data)\n        # finalize chunk\n        self._finalize_chunk(dd, offsets)", "response": "Process the data for the current object and store it in the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinalizes written chunk :param Downloader self: this :param blobxfer.models.download.Descriptor dd: download descriptor :param blobxfer.models.download.Offsets offsets: offsets", "response": "def _finalize_chunk(self, dd, offsets):\n        # type: (Downloader, blobxfer.models.download.Descriptor,\n        #        blobxfer.models.download.Offsets) -> None\n        \"\"\"Finalize written chunk\n        :param Downloader self: this\n        :param blobxfer.models.download.Descriptor dd: download descriptor\n        :param blobxfer.models.download.Offsets offsets: offsets\n        \"\"\"\n        if dd.entity.is_encrypted:\n            dd.mark_unchecked_chunk_decrypted(offsets.chunk_num)\n        # integrity check data and write to disk (this is called\n        # regardless of md5/hmac enablement for resume purposes)\n        dd.perform_chunked_integrity_check()\n        # remove from disk set and add bytes to counter\n        with self._disk_operation_lock:\n            self._disk_set.remove(\n                blobxfer.operations.download.Downloader.\n                create_unique_disk_operation_id(dd, offsets))\n            self._download_bytes_sofar += offsets.num_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _catalog_local_files_for_deletion(self):\n        # type: (Downloader) -> None\n        \"\"\"Catalog all local files if delete extraneous enabled\n        :param Downloader self: this\n        \"\"\"\n        if not (self._spec.options.delete_extraneous_destination and\n                self._spec.destination.is_dir):\n            return\n        dst = str(self._spec.destination.path)\n        for file in blobxfer.util.scantree(dst):\n            self._delete_after.add(pathlib.Path(file.path))", "response": "Catalog all local files if delete extraneous enabled\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _delete_extraneous_files(self):\n        # type: (Downloader) -> None\n        \"\"\"Delete extraneous files cataloged\n        :param Downloader self: this\n        \"\"\"\n        logger.info('attempting to delete {} extraneous files'.format(\n            len(self._delete_after)))\n        for file in self._delete_after:\n            if self._general_options.dry_run:\n                logger.info('[DRY RUN] deleting local file: {}'.format(\n                    file))\n            else:\n                if self._general_options.verbose:\n                    logger.debug('deleting local file: {}'.format(file))\n                try:\n                    file.unlink()\n                except OSError as e:\n                    logger.error('error deleting local file: {}'.format(\n                        str(e)))", "response": "Delete extraneous files in the cataloged\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run(self):\n        # type: (Downloader) -> None\n        \"\"\"Execute Downloader\n        :param Downloader self: this\n        \"\"\"\n        # mark start\n        self._start_time = blobxfer.util.datetime_now()\n        logger.info('blobxfer start time: {0}'.format(self._start_time))\n        # ensure destination path\n        blobxfer.operations.download.Downloader.ensure_local_destination(\n            self._creds, self._spec, self._general_options.dry_run)\n        logger.info('downloading blobs/files to local path: {}'.format(\n            self._spec.destination.path))\n        self._catalog_local_files_for_deletion()\n        # initialize resume db if specified\n        if self._general_options.resume_file is not None:\n            self._resume = blobxfer.operations.resume.DownloadResumeManager(\n                self._general_options.resume_file)\n        # initialize MD5 processes\n        if (self._spec.options.check_file_md5 and\n                self._general_options.concurrency.md5_processes > 0):\n            self._md5_offload = blobxfer.operations.md5.LocalFileMd5Offload(\n                num_workers=self._general_options.concurrency.md5_processes)\n            self._md5_offload.initialize_check_thread(\n                self._check_for_downloads_from_md5)\n        # initialize crypto processes\n        if self._general_options.concurrency.crypto_processes > 0:\n            self._crypto_offload = blobxfer.operations.crypto.CryptoOffload(\n                num_workers=self._general_options.concurrency.crypto_processes)\n            self._crypto_offload.initialize_check_thread(\n                self._check_for_crypto_done)\n        # initialize download threads\n        self._initialize_transfer_threads()\n        self._initialize_disk_threads()\n        # initialize local counters\n        files_processed = 0\n        skipped_files = 0\n        skipped_size = 0\n        # iterate through source paths to download\n        for src in self._spec.sources:\n            for rfile in src.files(\n                    self._creds, self._spec.options,\n                    self._general_options.dry_run):\n                # form local path for remote file\n                if (not self._spec.destination.is_dir and\n                        self._spec.options.rename):\n                    lpath = pathlib.Path(self._spec.destination.path)\n                else:\n                    lpath = None\n                    if self._spec.options.strip_components > 0:\n                        _lparts = pathlib.Path(rfile.name).parts\n                        _strip = min(\n                            (len(_lparts) - 1,\n                             self._spec.options.strip_components)\n                        )\n                        if _strip > 0:\n                            lpath = pathlib.Path(*_lparts[_strip:])\n                    if lpath is None:\n                        lpath = pathlib.Path(rfile.name)\n                    lpath = pathlib.Path(self._spec.destination.path) / lpath\n                files_processed += 1\n                # check on download conditions\n                action = self._check_download_conditions(lpath, rfile)\n                # remove from delete after set\n                try:\n                    self._delete_after.remove(lpath)\n                except KeyError:\n                    pass\n                if action == DownloadAction.Skip:\n                    skipped_files += 1\n                    skipped_size += rfile.size\n                    if self._general_options.dry_run:\n                        logger.info('[DRY RUN] skipping: {} -> {}'.format(\n                            lpath, rfile.path))\n                    continue\n                # add potential download to set\n                dlid = (\n                    blobxfer.operations.download.Downloader.\n                    create_unique_transfer_operation_id(rfile)\n                )\n                with self._transfer_lock:\n                    self._transfer_set.add(dlid)\n                    self._download_total += 1\n                    self._download_bytes_total += rfile.size\n                # either MD5 check or download now\n                if action == DownloadAction.CheckMd5:\n                    self._pre_md5_skip_on_check(lpath, rfile)\n                elif action == DownloadAction.Download:\n                    if self._general_options.dry_run:\n                        logger.info(\n                            '[DRY RUN] download: {} -> {}'.format(\n                                rfile.path, lpath))\n                        with self._transfer_lock:\n                            self._transfer_set.remove(dlid)\n                            self._download_total -= 1\n                            self._download_bytes_total -= rfile.size\n                    else:\n                        self._add_to_download_queue(lpath, rfile)\n        # set remote files processed\n        with self._md5_meta_lock:\n            self._all_remote_files_processed = True\n        with self._transfer_lock:\n            download_size_mib = (\n                self._download_bytes_total / blobxfer.util.MEGABYTE\n            )\n            logger.debug(\n                ('{0} files {1:.4f} MiB filesize and/or lmt_ge '\n                 'skipped').format(\n                    skipped_files, skipped_size / blobxfer.util.MEGABYTE))\n            logger.debug(\n                ('{0} remote files processed, waiting for download '\n                 'completion of approx. {1:.4f} MiB').format(\n                     files_processed, download_size_mib))\n        del files_processed\n        del skipped_files\n        del skipped_size\n        # wait for downloads to complete\n        self._wait_for_transfer_threads(terminate=False)\n        self._wait_for_disk_threads(terminate=False)\n        end_time = blobxfer.util.datetime_now()\n        # update progress bar\n        self._update_progress_bar()\n        # check for exceptions\n        if len(self._exceptions) > 0:\n            logger.error('exceptions encountered while downloading')\n            # raise the first one\n            raise self._exceptions[0]\n        # check for mismatches\n        if (self._download_sofar != self._download_total or\n                self._download_bytes_sofar != self._download_bytes_total):\n            raise RuntimeError(\n                'download mismatch: [count={}/{} bytes={}/{}]'.format(\n                    self._download_sofar, self._download_total,\n                    self._download_bytes_sofar, self._download_bytes_total))\n        # delete all remaining local files not accounted for if\n        # delete extraneous enabled\n        self._delete_extraneous_files()\n        # delete resume file if we've gotten this far\n        if self._resume is not None:\n            self._resume.delete()\n        # output throughput\n        if self._download_start_time is not None:\n            dltime = (end_time - self._download_start_time).total_seconds()\n            if dltime == 0:  # noqa\n                dltime = 1e-9\n            download_size_mib = (\n                self._download_bytes_total / blobxfer.util.MEGABYTE\n            )\n            dlmibspeed = download_size_mib / dltime\n            logger.info(\n                ('elapsed download + verify time and throughput of {0:.4f} '\n                 'GiB: {1:.3f} sec, {2:.4f} Mbps ({3:.3f} MiB/sec)').format(\n                     download_size_mib / 1024, dltime, dlmibspeed * 8,\n                     dlmibspeed))\n        end_time = blobxfer.util.datetime_now()\n        logger.info('blobxfer end time: {0} (elapsed: {1:.3f} sec)'.format(\n            end_time, (end_time - self._start_time).total_seconds()))", "response": "Execute Downloader\n        :param Downloader self: this"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encryption_metadata_exists(md):\n        # type: (dict) -> bool\n        \"\"\"Check if encryption metadata exists in json metadata\n        :param dict md: metadata dictionary\n        :rtype: bool\n        :return: if encryption metadata exists\n        \"\"\"\n        try:\n            if blobxfer.util.is_not_empty(\n                    md[EncryptionMetadata._METADATA_KEY_NAME]):\n                return True\n        except (KeyError, TypeError):\n            pass\n        return False", "response": "Check if the encryption metadata exists in json metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating new metadata entries for encryption.", "response": "def create_new_metadata(self, rsa_public_key):\n        # type: (EncryptionMetadata,\n        #        cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey)\n        #        -> None\n        \"\"\"Create new metadata entries for encryption (upload)\n        :param EncryptionMetadata self: this\n        :param cryptography.hazmat.primitives.asymmetric.rsa.RSAPublicKey:\n            rsa public key\n        \"\"\"\n        self._rsa_public_key = rsa_public_key\n        self._symkey = os.urandom(\n            blobxfer.operations.crypto._AES256_KEYLENGTH_BYTES)\n        self._signkey = os.urandom(\n            blobxfer.operations.crypto._AES256_KEYLENGTH_BYTES)\n        self.content_encryption_iv = os.urandom(AES256_BLOCKSIZE_BYTES)\n        self.encryption_agent = EncryptionAgent(\n            encryption_algorithm=EncryptionMetadata._ENCRYPTION_ALGORITHM,\n            protocol=EncryptionMetadata._ENCRYPTION_PROTOCOL_VERSION,\n        )\n        self.encryption_mode = EncryptionMetadata._ENCRYPTION_MODE"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_from_json(self, md, entityname, rsaprivatekey):\n        # type: (EncryptionMetadata, dict, str,\n        #        cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey)\n        #        -> None\n        \"\"\"Read metadata json into objects\n        :param EncryptionMetadata self: this\n        :param dict md: metadata dictionary\n        :param str entityname: entity name\n        :param rsaprivatekey: RSA private key\n        :type rsaprivatekey:\n            cryptography.hazmat.primitives.asymmetric.rsa.RSAPrivateKey\n        \"\"\"\n        # populate from encryption data\n        ed = json.loads(md[EncryptionMetadata._METADATA_KEY_NAME])\n        try:\n            self.blobxfer_extensions = EncryptionBlobxferExtensions(\n                pre_encrypted_content_md5=ed[\n                    EncryptionMetadata._JSON_KEY_BLOBXFER_EXTENSIONS][\n                        EncryptionMetadata._JSON_KEY_PREENCRYPTED_MD5],\n            )\n        except KeyError:\n            pass\n        self.content_encryption_iv = base64.b64decode(\n            ed[EncryptionMetadata._JSON_KEY_CONTENT_IV])\n        self.encryption_agent = EncryptionAgent(\n            encryption_algorithm=ed[\n                EncryptionMetadata._JSON_KEY_ENCRYPTION_AGENT][\n                    EncryptionMetadata._JSON_KEY_ENCRYPTION_ALGORITHM],\n            protocol=ed[\n                EncryptionMetadata._JSON_KEY_ENCRYPTION_AGENT][\n                    EncryptionMetadata._JSON_KEY_PROTOCOL],\n        )\n        if (self.encryption_agent.encryption_algorithm !=\n                EncryptionMetadata._ENCRYPTION_ALGORITHM):\n            raise RuntimeError('{}: unknown block cipher: {}'.format(\n                entityname, self.encryption_agent.encryption_algorithm))\n        if (self.encryption_agent.protocol !=\n                EncryptionMetadata._ENCRYPTION_PROTOCOL_VERSION):\n            raise RuntimeError('{}: unknown encryption protocol: {}'.format(\n                entityname, self.encryption_agent.protocol))\n        self.encryption_authentication = EncryptionAuthentication(\n            algorithm=ed[\n                EncryptionMetadata._JSON_KEY_INTEGRITY_AUTH][\n                    EncryptionMetadata._JSON_KEY_ALGORITHM],\n            message_authentication_code=ed[\n                EncryptionMetadata._JSON_KEY_INTEGRITY_AUTH][\n                    EncryptionMetadata._JSON_KEY_MAC],\n        )\n        if (self.encryption_authentication.algorithm !=\n                EncryptionMetadata._AUTH_ALGORITHM):\n            raise RuntimeError(\n                '{}: unknown integrity/auth method: {}'.format(\n                    entityname, self.encryption_authentication.algorithm))\n        self.encryption_mode = ed[\n            EncryptionMetadata._JSON_KEY_ENCRYPTION_MODE]\n        if self.encryption_mode != EncryptionMetadata._ENCRYPTION_MODE:\n            raise RuntimeError(\n                '{}: unknown encryption mode: {}'.format(\n                    entityname, self.encryption_mode))\n        try:\n            _eak = ed[EncryptionMetadata._JSON_KEY_WRAPPEDCONTENTKEY][\n                EncryptionMetadata._JSON_KEY_ENCRYPTED_AUTHKEY]\n        except KeyError:\n            _eak = None\n        self.wrapped_content_key = EncryptionWrappedContentKey(\n            algorithm=ed[\n                EncryptionMetadata._JSON_KEY_WRAPPEDCONTENTKEY][\n                    EncryptionMetadata._JSON_KEY_ALGORITHM],\n            encrypted_authentication_key=_eak,\n            encrypted_key=ed[\n                EncryptionMetadata._JSON_KEY_WRAPPEDCONTENTKEY][\n                    EncryptionMetadata._JSON_KEY_ENCRYPTED_KEY],\n            key_id=ed[\n                EncryptionMetadata._JSON_KEY_WRAPPEDCONTENTKEY][\n                    EncryptionMetadata._JSON_KEY_KEYID],\n        )\n        if (self.wrapped_content_key.algorithm !=\n                EncryptionMetadata._ENCRYPTED_KEY_SCHEME):\n            raise RuntimeError('{}: unknown key encryption scheme: {}'.format(\n                entityname, self.wrapped_content_key.algorithm))\n        # if RSA key is a public key, stop here as keys cannot be decrypted\n        if rsaprivatekey is None:\n            return\n        # decrypt symmetric key\n        self._symkey = blobxfer.operations.crypto.\\\n            rsa_decrypt_base64_encoded_key(\n                rsaprivatekey, self.wrapped_content_key.encrypted_key)\n        # decrypt signing key, if it exists\n        if blobxfer.util.is_not_empty(\n                self.wrapped_content_key.encrypted_authentication_key):\n            self._signkey = blobxfer.operations.crypto.\\\n                rsa_decrypt_base64_encoded_key(\n                    rsaprivatekey,\n                    self.wrapped_content_key.encrypted_authentication_key)\n        else:\n            self._signkey = None\n        # populate from encryption data authentication\n        try:\n            eda = json.loads(md[EncryptionMetadata._METADATA_KEY_AUTH_NAME])\n        except KeyError:\n            pass\n        else:\n            self.encryption_metadata_authentication = \\\n                EncryptionMetadataAuthentication(\n                    algorithm=eda[\n                        EncryptionMetadata._JSON_KEY_AUTH_METAAUTH][\n                            EncryptionMetadata._JSON_KEY_ALGORITHM],\n                    encoding=eda[\n                        EncryptionMetadata._JSON_KEY_AUTH_METAAUTH][\n                            EncryptionMetadata._JSON_KEY_AUTH_ENCODING],\n                    message_authentication_code=eda[\n                        EncryptionMetadata._JSON_KEY_AUTH_METAAUTH][\n                            EncryptionMetadata._JSON_KEY_MAC],\n                )\n            if (self.encryption_metadata_authentication.algorithm !=\n                    EncryptionMetadata._AUTH_ALGORITHM):\n                raise RuntimeError(\n                    '{}: unknown integrity/auth method: {}'.format(\n                        entityname,\n                        self.encryption_metadata_authentication.algorithm))\n            # verify hmac\n            authhmac = base64.b64decode(\n                self.encryption_metadata_authentication.\n                message_authentication_code)\n            bmeta = md[EncryptionMetadata._METADATA_KEY_NAME].encode(\n                self.encryption_metadata_authentication.encoding)\n            hmacsha256 = hmac.new(self._signkey, digestmod=hashlib.sha256)\n            hmacsha256.update(bmeta)\n            if hmacsha256.digest() != authhmac:\n                raise RuntimeError(\n                    '{}: encryption metadata authentication failed'.format(\n                        entityname))", "response": "Read the metadata dictionary and populate the object with the information from the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_to_json_with_mac(self, md5digest, hmacdigest):\n        # type: (EncryptionMetadata, str, str) -> dict\n        \"\"\"Constructs metadata for encryption\n        :param EncryptionMetadata self: this\n        :param str md5digest: md5 digest\n        :param str hmacdigest: hmac-sha256 digest (data)\n        :rtype: dict\n        :return: encryption metadata\n        \"\"\"\n        # encrypt keys\n        enc_content_key = blobxfer.operations.crypto.\\\n            rsa_encrypt_key_base64_encoded(\n                None, self._rsa_public_key, self.symmetric_key)\n        enc_sign_key = blobxfer.operations.crypto.\\\n            rsa_encrypt_key_base64_encoded(\n                None, self._rsa_public_key, self.signing_key)\n        # generate json\n        encjson = {\n            EncryptionMetadata._JSON_KEY_ENCRYPTION_MODE:\n            EncryptionMetadata._ENCRYPTION_MODE,\n            EncryptionMetadata._JSON_KEY_CONTENT_IV:\n            blobxfer.util.base64_encode_as_string(self.content_encryption_iv),\n            EncryptionMetadata._JSON_KEY_WRAPPEDCONTENTKEY: {\n                EncryptionMetadata._JSON_KEY_KEYID: 'private:pem',\n                EncryptionMetadata._JSON_KEY_ENCRYPTED_KEY: enc_content_key,\n                EncryptionMetadata._JSON_KEY_ENCRYPTED_AUTHKEY: enc_sign_key,\n                EncryptionMetadata._JSON_KEY_ALGORITHM:\n                EncryptionMetadata._ENCRYPTED_KEY_SCHEME,\n            },\n            EncryptionMetadata._JSON_KEY_ENCRYPTION_AGENT: {\n                EncryptionMetadata._JSON_KEY_PROTOCOL:\n                EncryptionMetadata._ENCRYPTION_PROTOCOL_VERSION,\n                EncryptionMetadata._JSON_KEY_ENCRYPTION_ALGORITHM:\n                EncryptionMetadata._ENCRYPTION_ALGORITHM,\n            },\n            EncryptionMetadata._JSON_KEY_INTEGRITY_AUTH: {\n                EncryptionMetadata._JSON_KEY_ALGORITHM:\n                EncryptionMetadata._AUTH_ALGORITHM,\n            },\n            EncryptionMetadata._JSON_KEY_KEY_WRAPPING_METADATA: {},\n        }\n        if md5digest is not None:\n            encjson[EncryptionMetadata._JSON_KEY_BLOBXFER_EXTENSIONS] = {\n                EncryptionMetadata._JSON_KEY_PREENCRYPTED_MD5: md5digest\n            }\n        if hmacdigest is not None:\n            encjson[EncryptionMetadata._JSON_KEY_INTEGRITY_AUTH][\n                EncryptionMetadata._JSON_KEY_MAC] = hmacdigest\n        bencjson = json.dumps(\n            encjson, sort_keys=True, ensure_ascii=False).encode(\n                EncryptionMetadata._AUTH_ENCODING_TYPE)\n        encjson = {\n            EncryptionMetadata._METADATA_KEY_NAME:\n            json.dumps(encjson, sort_keys=True)\n        }\n        # compute MAC over encjson\n        hmacsha256 = hmac.new(self._signkey, digestmod=hashlib.sha256)\n        hmacsha256.update(bencjson)\n        authjson = {\n            EncryptionMetadata._JSON_KEY_AUTH_METAAUTH: {\n                EncryptionMetadata._JSON_KEY_ALGORITHM:\n                EncryptionMetadata._AUTH_ALGORITHM,\n                EncryptionMetadata._JSON_KEY_AUTH_ENCODING:\n                EncryptionMetadata._AUTH_ENCODING_TYPE,\n                EncryptionMetadata._JSON_KEY_MAC:\n                blobxfer.util.base64_encode_as_string(hmacsha256.digest()),\n            }\n        }\n        encjson[EncryptionMetadata._METADATA_KEY_AUTH_NAME] = json.dumps(\n            authjson, sort_keys=True)\n        return encjson", "response": "Converts the md5 digest to json with the hmac - sha256 digest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize_hmac(self):\n        # type: (EncryptionMetadata) -> hmac.HMAC\n        \"\"\"Initialize an hmac from a signing key if it exists\n        :param EncryptionMetadata self: this\n        :rtype: hmac.HMAC or None\n        :return: hmac\n        \"\"\"\n        if self._signkey is not None:\n            return hmac.new(self._signkey, digestmod=hashlib.sha256)\n        else:\n            return None", "response": "Initialize an hmac from a signing key if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remote_is_file(self):\n        # type: (Descriptor) -> bool\n        \"\"\"Remote destination is an Azure File\n        :param Descriptor self: this\n        :rtype: bool\n        :return: remote is an Azure File\n        \"\"\"\n        return self.dst_entity.mode == blobxfer.models.azure.StorageModes.File", "response": "Returns true if the remote is an Azure File\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remote_is_page_blob(self):\n        # type: (Descriptor) -> bool\n        \"\"\"Remote destination is an Azure Page Blob\n        :param Descriptor self: this\n        :rtype: bool\n        :return: remote is an Azure Page Blob\n        \"\"\"\n        return self.dst_entity.mode == blobxfer.models.azure.StorageModes.Page", "response": "Returns true if the remote is an Azure Page Blob."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remote_is_block_blob(self):\n        # type: (Descriptor) -> bool\n        \"\"\"Remote destination is an Azure Block Blob\n        :param Descriptor self: this\n        :rtype: bool\n        :return: remote is an Azure Block Blob\n        \"\"\"\n        return self.dst_entity.mode == blobxfer.models.azure.StorageModes.Block", "response": "Returns true if the remote is an Azure Block Blob"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncomplete the upload for the offset .", "response": "def complete_offset_upload(self, chunk_num):\n        # type: (Descriptor, int) -> None\n        \"\"\"Complete the upload for the offset\n        :param Descriptor self: this\n        :param int chunk_num: chunk num completed\n        \"\"\"\n        with self._meta_lock:\n            self._outstanding_ops -= 1\n            # save resume state\n            if self.is_resumable:\n                # only set resumable completed if all replicas for this\n                # chunk are complete\n                if blobxfer.util.is_not_empty(self._dst_ase.replica_targets):\n                    if chunk_num not in self._replica_counters:\n                        # start counter at -1 since we need 1 \"extra\" for the\n                        # primary in addition to the replica targets\n                        self._replica_counters[chunk_num] = -1\n                    self._replica_counters[chunk_num] += 1\n                    if (self._replica_counters[chunk_num] !=\n                            len(self._dst_ase.replica_targets)):\n                        return\n                    else:\n                        self._replica_counters.pop(chunk_num)\n                self._completed_chunks.set(True, chunk_num)\n                completed = self._outstanding_ops == 0\n                self._resume_mgr.add_or_update_record(\n                    self._dst_ase, self._src_block_list, self._offset,\n                    self._chunk_size, self._total_chunks,\n                    self._completed_chunks.int, completed,\n                )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute chunk size given block list", "response": "def _compute_chunk_size(self):\n        # type: (Descriptor) -> int\n        \"\"\"Compute chunk size given block list\n        :param Descriptor self: this\n        :rtype: int\n        :return: chunk size bytes\n        \"\"\"\n        if self._src_block_list is not None:\n            blen = len(self._src_block_list)\n            if blen == 0:\n                # this is a one-shot block blob\n                return self._src_ase.size\n            elif blen == 1:\n                return self._src_block_list[0].size\n            else:\n                return -1\n        else:\n            return _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_total_chunks(self, chunk_size):\n        # type: (Descriptor, int) -> int\n        \"\"\"Compute total number of chunks for entity\n        :param Descriptor self: this\n        :param int chunk_size: chunk size\n        :rtype: int\n        :return: num chunks\n        \"\"\"\n        try:\n            if self._src_block_list is not None:\n                blen = len(self._src_block_list)\n                if blen > 0:\n                    return blen\n                else:\n                    return 1\n            else:\n                return int(math.ceil(self._src_ase.size / chunk_size))\n        except ZeroDivisionError:\n            return 1", "response": "Compute total number of chunks for the entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresuming a download if possible is set.", "response": "def _resume(self):\n        # type: (Descriptor) -> int\n        \"\"\"Resume a download, if possible\n        :param Descriptor self: this\n        :rtype: int or None\n        :return: verified download offset\n        \"\"\"\n        if self._resume_mgr is None or self._offset > 0:\n            return None\n        # check if path exists in resume db\n        rr = self._resume_mgr.get_record(self._dst_ase)\n        if rr is None:\n            logger.debug('no resume record for {}'.format(self._dst_ase.path))\n            return None\n        # ensure lengths are the same\n        if rr.length != self._src_ase.size:\n            logger.warning('resume length mismatch {} -> {}'.format(\n                rr.length, self._src_ase.size))\n            return None\n        # compute replica factor\n        if blobxfer.util.is_not_empty(self._dst_ase.replica_targets):\n            replica_factor = 1 + len(self._dst_ase.replica_targets)\n        else:\n            replica_factor = 1\n        # set offsets if completed\n        if rr.completed:\n            with self._meta_lock:\n                logger.debug('{} upload already completed'.format(\n                    self._dst_ase.path))\n                self._offset = rr.offset\n                self._src_block_list = rr.src_block_list\n                self._chunk_num = rr.total_chunks\n                self._chunk_size = rr.chunk_size\n                self._total_chunks = rr.total_chunks\n                self._completed_chunks.int = rr.completed_chunks\n                self._outstanding_ops = 0\n                return self._src_ase.size * replica_factor\n        # re-hash from 0 to offset if needed\n        _cc = bitstring.BitArray(length=rr.total_chunks)\n        _cc.int = rr.completed_chunks\n        curr_chunk = _cc.find('0b0')[0]\n        del _cc\n        # set values from resume\n        with self._meta_lock:\n            self._offset = rr.offset\n            self._src_block_list = rr.src_block_list\n            self._chunk_num = curr_chunk\n            self._chunk_size = rr.chunk_size\n            self._total_chunks = rr.total_chunks\n            self._completed_chunks = bitstring.BitArray(length=rr.total_chunks)\n            self._completed_chunks.set(True, range(0, curr_chunk + 1))\n            self._outstanding_ops = (\n                (rr.total_chunks - curr_chunk) * replica_factor\n            )\n            logger.debug(\n                ('resuming file {} from byte={} chunk={} chunk_size={} '\n                 'total_chunks={} outstanding_ops={}').format(\n                     self._src_ase.path, self._offset, self._chunk_num,\n                     self._chunk_size, self._total_chunks,\n                     self._outstanding_ops))\n            return rr.offset * replica_factor"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve the next offsets .", "response": "def next_offsets(self):\n        # type: (Descriptor) -> Offsets\n        \"\"\"Retrieve the next offsets\n        :param Descriptor self: this\n        :rtype: Offsets\n        :return: download offsets\n        \"\"\"\n        resume_bytes = self._resume()\n        with self._meta_lock:\n            if self._chunk_num >= self._total_chunks:\n                return None, resume_bytes\n            if self._chunk_size == -1 and self._src_block_list is not None:\n                num_bytes = self._src_block_list[self._chunk_num].size\n            else:\n                if self._offset + self._chunk_size > self._src_ase.size:\n                    num_bytes = self._src_ase.size - self._offset\n                else:\n                    num_bytes = self._chunk_size\n            chunk_num = self._chunk_num\n            range_start = self._offset\n            range_end = self._offset + num_bytes - 1\n            self._offset += num_bytes\n            self._chunk_num += 1\n            return Offsets(\n                chunk_num=chunk_num,\n                num_bytes=num_bytes,\n                range_start=range_start,\n                range_end=range_end,\n            ), resume_bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef can_rename(self):\n        # type: (LocalSourcePaths) -> bool\n        \"\"\"Check if source can be renamed\n        :param LocalSourcePath self: this\n        :rtype: bool\n        :return: if rename possible\n        \"\"\"\n        return len(self._paths) == 1 and (\n            self._paths[0].is_file() or\n            blobxfer.models.upload.LocalSourcePath.is_stdin(\n                str(self._paths[0]))\n        )", "response": "Checks if source can be renamed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef files(self, dry_run):\n        # type: (LocalSourcePaths, bool) -> LocalPath\n        \"\"\"Generator for files in paths\n        :param LocalSourcePath self: this\n        :param bool dry_run: dry run\n        :rtype: LocalPath\n        :return: LocalPath\n        \"\"\"\n        for _path in self._paths:\n            _ppath = os.path.expandvars(os.path.expanduser(str(_path)))\n            # check of path is stdin\n            if blobxfer.models.upload.LocalSourcePath.is_stdin(_ppath):\n                yield LocalPath(\n                    parent_path=pathlib.Path(),\n                    relative_path=pathlib.Path('stdin'),\n                    use_stdin=True,\n                )\n                continue\n            # resolve path\n            _expath = pathlib.Path(_ppath).resolve()\n            # check if path is a single file\n            tmp = pathlib.Path(_ppath)\n            if tmp.is_file():\n                if self._inclusion_check(tmp.name):\n                    yield LocalPath(\n                        parent_path=tmp.parent,\n                        relative_path=pathlib.Path(tmp.name),\n                        use_stdin=False,\n                    )\n                elif dry_run:\n                    logger.info(\n                        '[DRY RUN] skipping due to filters: {}'.format(tmp))\n            else:\n                del tmp\n                for entry in blobxfer.util.scantree(_ppath):\n                    _rpath = pathlib.Path(entry.path).relative_to(_ppath)\n                    if not self._inclusion_check(_rpath):\n                        if dry_run:\n                            logger.info(\n                                '[DRY RUN] skipping due to filters: {}'.format(\n                                    _rpath))\n                        continue\n                    yield LocalPath(\n                        parent_path=_expath,\n                        relative_path=_rpath,\n                        use_stdin=False,\n                    )", "response": "Generator for files in paths\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true if the remote is an Azure Append Blob", "response": "def remote_is_append_blob(self):\n        # type: (Descriptor) -> bool\n        \"\"\"Remote destination is an Azure Append Blob\n        :param Descriptor self: this\n        :rtype: bool\n        :return: remote is an Azure Append Blob\n        \"\"\"\n        return self.entity.mode == blobxfer.models.azure.StorageModes.Append"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompletes the upload for the offset.", "response": "def complete_offset_upload(self, chunk_num):\n        # type: (Descriptor, int) -> None\n        \"\"\"Complete the upload for the offset\n        :param Descriptor self: this\n        :param int chunk_num: chunk num completed\n        \"\"\"\n        with self._meta_lock:\n            self._outstanding_ops -= 1\n            # save resume state\n            if self.is_resumable:\n                # only set resumable completed if all replicas for this\n                # chunk are complete\n                if blobxfer.util.is_not_empty(self._ase.replica_targets):\n                    if chunk_num not in self._replica_counters:\n                        # start counter at -1 since we need 1 \"extra\" for the\n                        # primary in addition to the replica targets\n                        self._replica_counters[chunk_num] = -1\n                    self._replica_counters[chunk_num] += 1\n                    if (self._replica_counters[chunk_num] !=\n                            len(self._ase.replica_targets)):\n                        return\n                    else:\n                        self._replica_counters.pop(chunk_num)\n                self._completed_chunks.set(True, chunk_num)\n                completed = self._outstanding_ops == 0\n                if not completed and self.must_compute_md5:\n                    last_consecutive = (\n                        self._completed_chunks.find('0b0')[0] - 1\n                    )\n                    md5digest = self._md5_cache[last_consecutive]\n                else:\n                    md5digest = None\n                self._resume_mgr.add_or_update_record(\n                    self.local_path.absolute_path, self._ase, self._chunk_size,\n                    self._total_chunks, self._completed_chunks.int, completed,\n                    md5digest,\n                )\n                # prune md5 cache\n                if self.must_compute_md5:\n                    if completed:\n                        self._md5_cache.clear()\n                    elif (len(self._md5_cache) >\n                          _MD5_CACHE_RESUME_ENTRIES_GC_THRESHOLD):\n                        mkeys = sorted(list(self._md5_cache.keys()))\n                        for key in mkeys:\n                            if key >= last_consecutive:\n                                break\n                            self._md5_cache.pop(key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the encryption metadata for the current resource.", "response": "def _initialize_encryption(self, options):\n        # type: (Descriptor, blobxfer.models.options.Upload) -> None\n        \"\"\"Download is resume capable\n        :param Descriptor self: this\n        :param blobxfer.models.options.Upload options: upload options\n        \"\"\"\n        if (options.rsa_public_key is not None and self.local_path.size > 0 and\n                (self._ase.mode == blobxfer.models.azure.StorageModes.Block or\n                 self._ase.mode == blobxfer.models.azure.StorageModes.File)):\n            em = blobxfer.models.crypto.EncryptionMetadata()\n            em.create_new_metadata(options.rsa_public_key)\n            self.current_iv = em.content_encryption_iv\n            self._ase.encryption_metadata = em"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the remote size of the local file.", "response": "def _compute_remote_size(self, options):\n        # type: (Descriptor, blobxfer.models.options.Upload) -> None\n        \"\"\"Compute total remote file size\n        :param Descriptor self: this\n        :param blobxfer.models.options.Upload options: upload options\n        :rtype: int\n        :return: remote file size\n        \"\"\"\n        size = self.local_path.size\n        if (self._ase.mode == blobxfer.models.azure.StorageModes.Page and\n                self.local_path.use_stdin):\n            if options.stdin_as_page_blob_size == 0:\n                allocatesize = _MAX_PAGE_BLOB_SIZE\n                self._needs_resize = True\n            else:\n                allocatesize = options.stdin_as_page_blob_size\n        elif size > 0:\n            if self._ase.is_encrypted:\n                # cipher_len_without_iv = (clear_len / aes_bs + 1) * aes_bs\n                allocatesize = (size // self._AES_BLOCKSIZE + 1) * \\\n                    self._AES_BLOCKSIZE\n            else:\n                allocatesize = size\n        else:\n            allocatesize = 0\n        self._ase.size = allocatesize\n        if blobxfer.util.is_not_empty(self._ase.replica_targets):\n            for rt in self._ase.replica_targets:\n                rt.size = allocatesize\n        if self._verbose:\n            logger.debug('remote size for {} is {} bytes'.format(\n                self._ase.path, self._ase.size))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _adjust_chunk_size(self, options):\n        # type: (Descriptor, blobxfer.models.options.Upload) -> None\n        \"\"\"Adjust chunk size for entity mode\n        :param Descriptor self: this\n        :param blobxfer.models.options.Upload options: upload options\n        \"\"\"\n        chunk_size = options.chunk_size_bytes\n        # auto-select chunk size\n        if chunk_size == 0:\n            if self._ase.mode != blobxfer.models.azure.StorageModes.Block:\n                chunk_size = _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES\n            else:\n                if self._ase.size == 0:\n                    chunk_size = _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES\n                else:\n                    chunk_size = _DEFAULT_AUTO_CHUNKSIZE_BYTES\n                    while chunk_size < _MAX_BLOCK_BLOB_CHUNKSIZE_BYTES:\n                        chunks = int(math.ceil(self._ase.size / chunk_size))\n                        if chunks <= _MAX_NUM_CHUNKS:\n                            break\n                        chunk_size = chunk_size << 1\n            if self._verbose:\n                logger.debug(\n                    'auto-selected chunk size of {} for {}'.format(\n                        chunk_size, self.local_path.absolute_path))\n        if self.local_path.use_stdin:\n            self._chunk_size = max(\n                (chunk_size, _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES)\n            )\n        else:\n            self._chunk_size = min((chunk_size, self._ase.size))\n        # ensure chunk sizes are compatible with mode\n        if self._ase.mode == blobxfer.models.azure.StorageModes.Append:\n            if self._chunk_size > _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES:\n                self._chunk_size = _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES\n                if self._verbose:\n                    logger.debug(\n                        ('adjusting chunk size to {} for append blob '\n                         'from {}').format(\n                             self._chunk_size, self.local_path.absolute_path))\n        elif self._ase.mode == blobxfer.models.azure.StorageModes.Block:\n            if (not self.local_path.use_stdin and\n                    self._ase.size <= options.one_shot_bytes):\n                self._chunk_size = min(\n                    (self._ase.size, options.one_shot_bytes)\n                )\n            else:\n                if self._chunk_size > _MAX_BLOCK_BLOB_CHUNKSIZE_BYTES:\n                    self._chunk_size = _MAX_BLOCK_BLOB_CHUNKSIZE_BYTES\n                    if self._verbose:\n                        logger.debug(\n                            ('adjusting chunk size to {} for block blob '\n                             'from {}').format(\n                                self._chunk_size,\n                                 self.local_path.absolute_path))\n        elif self._ase.mode == blobxfer.models.azure.StorageModes.File:\n            if self._chunk_size > _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES:\n                self._chunk_size = _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES\n                if self._verbose:\n                    logger.debug(\n                        'adjusting chunk size to {} for file from {}'.format(\n                            self._chunk_size, self.local_path.absolute_path))\n        elif self._ase.mode == blobxfer.models.azure.StorageModes.Page:\n            if self._ase.size > _MAX_PAGE_BLOB_SIZE:\n                raise RuntimeError(\n                    '{} size {} exceeds maximum page blob size of {}'.format(\n                        self.local_path.absolute_path, self._ase.size,\n                        _MAX_PAGE_BLOB_SIZE))\n            if self._chunk_size > _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES:\n                self._chunk_size = _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES\n                if self._verbose:\n                    logger.debug(\n                        ('adjusting chunk size to {} for page blob '\n                         'from {}').format(\n                             self._chunk_size, self.local_path.absolute_path))", "response": "Adjusts the chunk size for the entity mode and the given upload options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_total_chunks(self, chunk_size):\n        # type: (Descriptor, int) -> int\n        \"\"\"Compute total number of chunks for entity\n        :param Descriptor self: this\n        :param int chunk_size: chunk size\n        :rtype: int\n        :return: num chunks\n        \"\"\"\n        try:\n            chunks = int(math.ceil(self._ase.size / chunk_size))\n        except ZeroDivisionError:\n            chunks = 1\n        # for stdin, override and use 1 chunk to start, this will change\n        # dynamically as data as read\n        if self.local_path.use_stdin:\n            chunks = 1\n        if (self._ase.mode != blobxfer.models.azure.StorageModes.Page and\n                chunks > 50000):\n            max_vector = False\n            if self._ase.mode == blobxfer.models.azure.StorageModes.Block:\n                if self._chunk_size == _MAX_BLOCK_BLOB_CHUNKSIZE_BYTES:\n                    max_vector = True\n            elif self._chunk_size == _MAX_NONBLOCK_BLOB_CHUNKSIZE_BYTES:\n                max_vector = True\n            if max_vector:\n                raise RuntimeError(\n                    ('number of chunks {} exceeds maximum permissible '\n                     'limit and chunk size is set at the maximum value '\n                     'for {}. Please try using stripe mode '\n                     'vectorization to overcome this limitation').format(\n                        chunks, self.local_path.absolute_path))\n            else:\n                raise RuntimeError(\n                    ('number of chunks {} exceeds maximum permissible '\n                     'limit for {}, please adjust chunk size higher or '\n                     'set to -1 for automatic chunk size selection').format(\n                         chunks, self.local_path.absolute_path))\n        return chunks", "response": "Compute the total number of chunks for the entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes file integrity checkers.", "response": "def _initialize_integrity_checkers(self, options):\n        # type: (Descriptor, blobxfer.models.options.Upload) -> None\n        \"\"\"Initialize file integrity checkers\n        :param Descriptor self: this\n        :param blobxfer.models.options.Upload options: upload options\n        \"\"\"\n        if self._ase.is_encrypted:\n            # ensure symmetric key exists\n            if blobxfer.util.is_none_or_empty(\n                    self._ase.encryption_metadata.symmetric_key):\n                raise RuntimeError(\n                    ('symmetric key is invalid: provide RSA private key '\n                     'or metadata corrupt for {}').format(\n                         self.local_path.absolute_path))\n            self.hmac = self._ase.encryption_metadata.initialize_hmac()\n        # both hmac and md5 can be enabled\n        if (options.store_file_properties.md5 and\n                not self.remote_is_append_blob):\n            self.md5 = blobxfer.util.new_md5_hasher()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _resume(self):\n        # type: (Descriptor) -> int\n        \"\"\"Resume upload\n        :param Descriptor self: this\n        :rtype: int\n        :return: resume bytes\n        \"\"\"\n        if self._resume_mgr is None or self._offset > 0:\n            return None\n        # check if path exists in resume db\n        rr = self._resume_mgr.get_record(self._ase)\n        if rr is None:\n            logger.debug('no resume record for {}'.format(self._ase.path))\n            return None\n        # ensure lengths are the same\n        if rr.length != self._ase.size:\n            logger.warning('resume length mismatch {} -> {}'.format(\n                rr.length, self._ase.size))\n            return None\n        # compute replica factor\n        if blobxfer.util.is_not_empty(self._ase.replica_targets):\n            replica_factor = 1 + len(self._ase.replica_targets)\n        else:\n            replica_factor = 1\n        # set offsets if completed\n        if rr.completed:\n            with self._meta_lock:\n                logger.debug('{} upload already completed'.format(\n                    self._ase.path))\n                self._offset = rr.total_chunks * rr.chunk_size\n                self._chunk_num = rr.total_chunks\n                self._chunk_size = rr.chunk_size\n                self._total_chunks = rr.total_chunks\n                self._completed_chunks.int = rr.completed_chunks\n                self._outstanding_ops = 0\n                return self._ase.size * replica_factor\n        # encrypted files are not resumable due to hmac requirement\n        if self._ase.is_encrypted:\n            logger.debug('cannot resume encrypted entity {}'.format(\n                self._ase.path))\n            return None\n        # check if path exists\n        if not pathlib.Path(rr.local_path).exists():\n            logger.warning('resume from local path {} does not exist'.format(\n                rr.local_path))\n            return None\n        # re-hash from 0 to offset if needed\n        _cc = bitstring.BitArray(length=rr.total_chunks)\n        _cc.int = rr.completed_chunks\n        curr_chunk = _cc.find('0b0')[0]\n        del _cc\n        _fd_offset = 0\n        _end_offset = min((curr_chunk * rr.chunk_size, rr.length))\n        if self.md5 is not None and curr_chunk > 0:\n            _blocksize = blobxfer.util.MEGABYTE << 2\n            logger.debug(\n                'integrity checking existing file {} offset {} -> {}'.format(\n                    self._ase.path,\n                    self.local_path.view.fd_start,\n                    self.local_path.view.fd_start + _end_offset)\n            )\n            with self._hasher_lock:\n                with self.local_path.absolute_path.open('rb') as filedesc:\n                    filedesc.seek(self.local_path.view.fd_start, 0)\n                    while _fd_offset < _end_offset:\n                        if (_fd_offset + _blocksize) > _end_offset:\n                            _blocksize = _end_offset - _fd_offset\n                        _buf = filedesc.read(_blocksize)\n                        self.md5.update(_buf)\n                        _fd_offset += _blocksize\n            del _blocksize\n            # compare hashes\n            hexdigest = self.md5.hexdigest()\n            if rr.md5hexdigest != hexdigest:\n                logger.warning(\n                    'MD5 mismatch resume={} computed={} for {}'.format(\n                        rr.md5hexdigest, hexdigest, self._ase.path))\n                # reset hasher\n                self.md5 = blobxfer.util.new_md5_hasher()\n                return None\n        # set values from resume\n        with self._meta_lock:\n            self._offset = _end_offset\n            self._chunk_num = curr_chunk\n            self._chunk_size = rr.chunk_size\n            self._total_chunks = rr.total_chunks\n            self._completed_chunks = bitstring.BitArray(length=rr.total_chunks)\n            self._completed_chunks.set(True, range(0, curr_chunk + 1))\n            self._outstanding_ops = (\n                (rr.total_chunks - curr_chunk) * replica_factor\n            )\n            logger.debug(\n                ('resuming file {} from byte={} chunk={} chunk_size={} '\n                 'total_chunks={} outstanding_ops={}').format(\n                     self._ase.path, self._offset, self._chunk_num,\n                     self._chunk_size, self._total_chunks,\n                     self._outstanding_ops))\n            return _end_offset * replica_factor", "response": "Resume upload of a new record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef next_offsets(self):\n        # type: (Descriptor) -> Offsets\n        \"\"\"Retrieve the next offsets\n        :param Descriptor self: this\n        :rtype: Offsets\n        :return: upload offsets\n        \"\"\"\n        resume_bytes = self._resume()\n        with self._meta_lock:\n            if self._chunk_num >= self._total_chunks:\n                return None, resume_bytes\n            if self._offset + self._chunk_size > self._ase.size:\n                num_bytes = self._ase.size - self._offset\n            else:\n                num_bytes = self._chunk_size\n            chunk_num = self._chunk_num\n            range_start = self._offset\n            range_end = self._offset + num_bytes - 1\n            self._offset += num_bytes\n            self._chunk_num += 1\n            if self._ase.is_encrypted and self._offset >= self._ase.size:\n                pad = True\n            else:\n                pad = False\n            return Offsets(\n                chunk_num=chunk_num,\n                num_bytes=num_bytes,\n                range_start=range_start,\n                range_end=range_end,\n                pad=pad,\n            ), resume_bytes", "response": "Retrieve the next offsets\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_data(self, offsets):\n        # type: (Descriptor, Offsets) -> Tuple[bytes, Offsets]\n        \"\"\"Read data from file\n        :param Descriptor self: this\n        :param Offsets offsets: offsets\n        :rtype: tuple\n        :return: (file data bytes, new Offsets if stdin)\n        \"\"\"\n        newoffset = None\n        if not self.local_path.use_stdin:\n            if offsets.num_bytes == 0:\n                return None, None\n            # compute start from view\n            start = self.local_path.view.fd_start + offsets.range_start\n            # encrypted offsets will read past the end of the file due\n            # to padding, but will be accounted for after encryption+padding\n            with self.local_path.absolute_path.open('rb') as fd:\n                fd.seek(start, 0)\n                data = fd.read(offsets.num_bytes)\n        else:\n            data = blobxfer.STDIN.read(self._chunk_size)\n            if not data:\n                with self._meta_lock:\n                    self._offset -= offsets.num_bytes\n                    self._ase.size -= offsets.num_bytes\n                    self._total_chunks -= 1\n                    self._chunk_num -= 1\n                    self._outstanding_ops -= 1\n            else:\n                num_bytes = len(data)\n                with self._meta_lock:\n                    self._offset -= offsets.num_bytes\n                    self._ase.size -= offsets.num_bytes\n                    newoffset = Offsets(\n                        chunk_num=self._chunk_num - 1,\n                        num_bytes=num_bytes,\n                        range_start=self._offset,\n                        range_end=self._offset + num_bytes - 1,\n                        pad=False,\n                    )\n                    self._total_chunks += 1\n                    self._outstanding_ops += 1\n                    self._offset += num_bytes\n                    self._ase.size += num_bytes\n        if self.must_compute_md5 and data:\n            with self._hasher_lock:\n                self.md5.update(data)\n                if self.is_resumable:\n                    self._md5_cache[self._chunk_num - 1] = self.md5.hexdigest()\n        return data, newoffset", "response": "Reads the data from the file and returns the data and the new offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates metadata for the given descriptor.", "response": "def generate_metadata(self):\n        # type: (Descriptor) -> dict\n        \"\"\"Generate metadata for descriptor\n        :param Descriptor self: this\n        :rtype: dict or None\n        :return: kv metadata dict\n        \"\"\"\n        genmeta = {}\n        encmeta = {}\n        # page align md5\n        if (self.must_compute_md5 and\n                self._ase.mode == blobxfer.models.azure.StorageModes.Page):\n            aligned = blobxfer.util.page_align_content_length(self._offset)\n            diff = aligned - self._offset\n            if diff > 0:\n                with self._hasher_lock:\n                    self.md5.update(b'\\0' * diff)\n        # generate encryption metadata\n        if self._ase.is_encrypted:\n            if self.must_compute_md5:\n                md5digest = blobxfer.util.base64_encode_as_string(\n                    self.md5.digest())\n            else:\n                md5digest = None\n            if self.hmac is not None:\n                hmacdigest = blobxfer.util.base64_encode_as_string(\n                    self.hmac.digest())\n            else:\n                hmacdigest = None\n            encmeta = self._ase.encryption_metadata.convert_to_json_with_mac(\n                md5digest, hmacdigest)\n        # generate file attribute metadata\n        if self._store_file_attr and not self.local_path.use_stdin:\n            merged = blobxfer.models.metadata.generate_fileattr_metadata(\n                self.local_path, genmeta)\n            if merged is not None:\n                genmeta = merged\n        # generate vectored io metadata\n        if self.local_path.view.mode == VectoredIoDistributionMode.Stripe:\n            merged = blobxfer.models.metadata.\\\n                generate_vectored_io_stripe_metadata(self.local_path, genmeta)\n            if merged is not None:\n                genmeta = merged\n        if len(encmeta) > 0:\n            metadata = encmeta\n        else:\n            metadata = {}\n        if len(genmeta) > 0:\n            metadata[blobxfer.models.metadata.JSON_KEY_BLOBXFER_METADATA] = \\\n                json.dumps(genmeta, ensure_ascii=False, sort_keys=True)\n        if len(metadata) == 0:\n            return None\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd CLI options to the configuration object.", "response": "def add_cli_options(cli_options, action):\n    # type: (dict, str) -> None\n    \"\"\"Adds CLI options to the configuration object\n    :param dict cli_options: CLI options dict\n    :param TransferAction action: action\n    \"\"\"\n    cli_options['_action'] = action.name.lower()\n    # if url is present, convert to constituent options\n    if blobxfer.util.is_not_empty(cli_options.get('storage_url')):\n        if (blobxfer.util.is_not_empty(cli_options.get('storage_account')) or\n                blobxfer.util.is_not_empty(cli_options.get('mode')) or\n                blobxfer.util.is_not_empty(cli_options.get('endpoint')) or\n                blobxfer.util.is_not_empty(cli_options.get('remote_path'))):\n            raise ValueError(\n                'Specified both --storage-url and --storage-account, '\n                '--mode, --endpoint, or --remote-path')\n        cli_options['storage_account'], mode, \\\n            cli_options['endpoint'], \\\n            cli_options['remote_path'], \\\n            sas = blobxfer.util.explode_azure_storage_url(\n                cli_options['storage_url'])\n        if blobxfer.util.is_not_empty(sas):\n            if blobxfer.util.is_not_empty(cli_options['sas']):\n                raise ValueError(\n                    'Specified both --storage-url with a SAS token and --sas')\n            cli_options['sas'] = sas\n        if mode != 'blob' and mode != 'file':\n            raise ValueError(\n                'Invalid derived mode from --storage-url: {}'.format(mode))\n        if mode == 'file':\n            cli_options['mode'] = mode\n        del mode\n        del sas\n    storage_account = cli_options.get('storage_account')\n    azstorage = {\n        'endpoint': cli_options.get('endpoint')\n    }\n    if blobxfer.util.is_not_empty(storage_account):\n        azstorage['accounts'] = {\n            storage_account: (\n                cli_options.get('access_key') or cli_options.get('sas')\n            )\n        }\n    sa_rp = {\n        storage_account: cli_options.get('remote_path')\n    }\n    local_resource = cli_options.get('local_resource')\n    # construct \"argument\" from cli options\n    if action == TransferAction.Download:\n        arg = {\n            'source': [sa_rp] if sa_rp[storage_account] is not None else None,\n            'destination': local_resource if local_resource is not None else\n            None,\n            'include': cli_options.get('include'),\n            'exclude': cli_options.get('exclude'),\n            'options': {\n                'check_file_md5': cli_options.get('file_md5'),\n                'chunk_size_bytes': cli_options.get('chunk_size_bytes'),\n                'delete_extraneous_destination': cli_options.get('delete'),\n                'max_single_object_concurrency': cli_options.get(\n                    'max_single_object_concurrency'),\n                'mode': cli_options.get('mode'),\n                'overwrite': cli_options.get('overwrite'),\n                'recursive': cli_options.get('recursive'),\n                'rename': cli_options.get('rename'),\n                'rsa_private_key': cli_options.get('rsa_private_key'),\n                'rsa_private_key_passphrase': cli_options.get(\n                    'rsa_private_key_passphrase'),\n                'restore_file_properties': {\n                    'attributes': cli_options.get('file_attributes'),\n                    'lmt': cli_options.get('restore_file_lmt'),\n                    'md5': None,\n                },\n                'strip_components': cli_options.get('strip_components'),\n                'skip_on': {\n                    'filesize_match': cli_options.get(\n                        'skip_on_filesize_match'),\n                    'lmt_ge': cli_options.get('skip_on_lmt_ge'),\n                    'md5_match': cli_options.get('skip_on_md5_match'),\n                },\n            },\n        }\n    elif action == TransferAction.Synccopy:\n        # if url is present, convert to constituent options\n        if blobxfer.util.is_not_empty(\n                cli_options.get('sync_copy_dest_storage_url')):\n            if (blobxfer.util.is_not_empty(\n                    cli_options.get('sync_copy_dest_storage_account')) or\n                    blobxfer.util.is_not_empty(\n                        cli_options.get('sync_copy_dest_mode')) or\n                    blobxfer.util.is_not_empty(\n                        cli_options.get('sync_copy_dest_remote_path'))):\n                raise ValueError(\n                    'Specified both --sync-copy-dest-storage-url and '\n                    '--sync-copy-dest-storage-account, '\n                    '--sync-copy-dest-mode, or'\n                    '--sync-copy-dest-remote-path')\n            cli_options['sync_copy_dest_storage_account'], mode, _, \\\n                cli_options['sync_copy_dest_remote_path'], sas = \\\n                blobxfer.util.explode_azure_storage_url(\n                    cli_options['sync_copy_dest_storage_url'])\n            if blobxfer.util.is_not_empty(sas):\n                if blobxfer.util.is_not_empty(\n                        cli_options['sync_copy_dest_sas']):\n                    raise ValueError(\n                        'Specified both --sync-copy-dest-storage-url with '\n                        'a SAS token and --sync-copy-dest-sas')\n                cli_options['sync_copy_dest_sas'] = sas\n            if mode != 'blob' and mode != 'file':\n                raise ValueError(\n                    'Invalid derived destination mode from '\n                    '--sync-copy-dest-storage-url: {}'.format(mode))\n            if mode == 'file':\n                cli_options['dest_mode'] = mode\n            del mode\n            del sas\n        sync_copy_dest_storage_account = cli_options.get(\n            'sync_copy_dest_storage_account')\n        sync_copy_dest_remote_path = cli_options.get(\n            'sync_copy_dest_remote_path')\n        if (sync_copy_dest_storage_account is not None and\n                sync_copy_dest_remote_path is not None):\n            sync_copy_dest = [\n                {\n                    sync_copy_dest_storage_account:\n                    sync_copy_dest_remote_path\n                }\n            ]\n            azstorage['accounts'][sync_copy_dest_storage_account] = (\n                cli_options.get('sync_copy_dest_access_key') or\n                cli_options.get('sync_copy_dest_sas')\n            )\n        else:\n            sync_copy_dest = None\n        arg = {\n            'source': [sa_rp] if sa_rp[storage_account] is not None else None,\n            'destination': sync_copy_dest,\n            'include': cli_options.get('include'),\n            'exclude': cli_options.get('exclude'),\n            'options': {\n                'access_tier': cli_options.get('access_tier'),\n                'chunk_size_bytes': cli_options.get('chunk_size_bytes'),\n                'dest_mode': cli_options.get('sync_copy_dest_mode'),\n                'mode': cli_options.get('mode'),\n                'overwrite': cli_options.get('overwrite'),\n                'rename': cli_options.get('rename'),\n                'skip_on': {\n                    'filesize_match': cli_options.get(\n                        'skip_on_filesize_match'),\n                    'lmt_ge': cli_options.get('skip_on_lmt_ge'),\n                    'md5_match': cli_options.get('skip_on_md5_match'),\n                },\n            },\n        }\n    elif action == TransferAction.Upload:\n        arg = {\n            'source': [local_resource] if local_resource is not None else None,\n            'destination': [sa_rp] if sa_rp[storage_account] is not None else\n            None,\n            'include': cli_options.get('include'),\n            'exclude': cli_options.get('exclude'),\n            'options': {\n                'access_tier': cli_options.get('access_tier'),\n                'chunk_size_bytes': cli_options.get('chunk_size_bytes'),\n                'delete_extraneous_destination': cli_options.get('delete'),\n                'mode': cli_options.get('mode'),\n                'one_shot_bytes': cli_options.get('one_shot_bytes'),\n                'overwrite': cli_options.get('overwrite'),\n                'recursive': cli_options.get('recursive'),\n                'rename': cli_options.get('rename'),\n                'rsa_private_key': cli_options.get('rsa_private_key'),\n                'rsa_private_key_passphrase': cli_options.get(\n                    'rsa_private_key_passphrase'),\n                'rsa_public_key': cli_options.get('rsa_public_key'),\n                'skip_on': {\n                    'filesize_match': cli_options.get(\n                        'skip_on_filesize_match'),\n                    'lmt_ge': cli_options.get('skip_on_lmt_ge'),\n                    'md5_match': cli_options.get('skip_on_md5_match'),\n                },\n                'stdin_as_page_blob_size': cli_options.get(\n                    'stdin_as_page_blob_size'),\n                'store_file_properties': {\n                    'attributes': cli_options.get('file_attributes'),\n                    'cache_control': cli_options.get('file_cache_control'),\n                    'lmt': None,\n                    'md5': cli_options.get('file_md5'),\n                },\n                'strip_components': cli_options.get('strip_components'),\n                'vectored_io': {\n                    'stripe_chunk_size_bytes': cli_options.get(\n                        'stripe_chunk_size_bytes'),\n                    'distribution_mode': cli_options.get('distribution_mode'),\n                },\n            },\n        }\n    count = 0\n    if arg['source'] is None:\n        arg.pop('source')\n        count += 1\n    if arg['destination'] is None:\n        arg.pop('destination')\n        count += 1\n    if count == 1:\n        if action == TransferAction.Synccopy:\n            raise ValueError(\n                '--remote-path and --sync-copy-dest-remote-path must be '\n                'specified together through the commandline')\n        else:\n            raise ValueError(\n                '--local-path and --remote-path must be specified together '\n                'through the commandline')\n    if 'accounts' in azstorage:\n        cli_options['azure_storage'] = azstorage\n    cli_options[action.name.lower()] = arg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _merge_setting(cli_options, conf, name, name_cli=None, default=None):\n    # type: (dict, dict, str, str, Any) -> Any\n    \"\"\"Merge a setting, preferring the CLI option if set\n    :param dict cli_options: cli options\n    :param dict conf: configuration sub-block\n    :param str name: key name\n    :param str name_cli: override key name for cli_options\n    :param Any default: default value to set if missing\n    :rtype: Any\n    :return: merged setting value\n    \"\"\"\n    val = cli_options.get(name_cli or name)\n    if val is None:\n        val = conf.get(name, default)\n    return val", "response": "Merge a setting preferring the CLI option if set\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_global_settings(config, cli_options):\n    # type: (dict, dict) -> None\n    \"\"\"Merge \"global\" CLI options into main config\n    :param dict config: config dict\n    :param dict cli_options: cli options\n    \"\"\"\n    # check for valid version from YAML\n    if (not blobxfer.util.is_none_or_empty(config) and\n            ('version' not in config or\n             config['version'] not in _SUPPORTED_YAML_CONFIG_VERSIONS)):\n        raise ValueError('\"version\" not specified in YAML config or invalid')\n    # get action\n    action = cli_options['_action']\n    if (action != TransferAction.Upload.name.lower() and\n            action != TransferAction.Download.name.lower() and\n            action != TransferAction.Synccopy.name.lower()):\n        raise ValueError('invalid action: {}'.format(action))\n    # merge credentials\n    if 'azure_storage' in cli_options:\n        if 'azure_storage' not in config:\n            config['azure_storage'] = {}\n        config['azure_storage'] = blobxfer.util.merge_dict(\n            config['azure_storage'], cli_options['azure_storage'])\n    if ('azure_storage' not in config or\n            blobxfer.util.is_none_or_empty(config['azure_storage'])):\n        raise ValueError('azure storage settings not specified')\n    # create action options\n    if action not in config:\n        config[action] = []\n    # append full specs, if they exist\n    if action in cli_options:\n        if 'source' in cli_options[action]:\n            srcdst = {\n                'source': cli_options[action]['source'],\n                'destination': cli_options[action]['destination'],\n            }\n            cli_options[action].pop('source')\n            cli_options[action].pop('destination')\n            config[action].append(srcdst)\n    # merge general and concurrency options\n    if 'options' not in config:\n        config['options'] = {}\n    if 'concurrency' not in config['options']:\n        config['options']['concurrency'] = {}\n    if 'timeout' not in config['options']:\n        config['options']['timeout'] = {}\n    if 'proxy' not in config['options']:\n        config['options']['proxy'] = {}\n    options = {\n        'enable_azure_storage_logger': _merge_setting(\n            cli_options, config['options'], 'enable_azure_storage_logger'),\n        'log_file': _merge_setting(cli_options, config['options'], 'log_file'),\n        'progress_bar': _merge_setting(\n            cli_options, config['options'], 'progress_bar', default=True),\n        'resume_file': _merge_setting(\n            cli_options, config['options'], 'resume_file'),\n        'timeout': {\n            # TODO deprecated timeout setting\n            'timeout': _merge_setting(\n                cli_options, config['options']['timeout'], 'timeout',\n                name_cli='timeout'),\n            'connect': _merge_setting(\n                cli_options, config['options']['timeout'], 'connect',\n                name_cli='connect_timeout'),\n            'read': _merge_setting(\n                cli_options, config['options']['timeout'], 'read',\n                name_cli='read_timeout'),\n            'max_retries': _merge_setting(\n                cli_options, config['options']['timeout'], 'max_retries',\n                default=1000),\n        },\n        'verbose': _merge_setting(\n            cli_options, config['options'], 'verbose', default=False),\n        'quiet': _merge_setting(\n            cli_options, config['options'], 'quiet', default=False),\n        'dry_run': _merge_setting(\n            cli_options, config['options'], 'dry_run', default=False),\n        'concurrency': {\n            'crypto_processes': _merge_setting(\n                cli_options, config['options']['concurrency'],\n                'crypto_processes', default=0),\n            'disk_threads': _merge_setting(\n                cli_options, config['options']['concurrency'],\n                'disk_threads', default=0),\n            'md5_processes': _merge_setting(\n                cli_options, config['options']['concurrency'],\n                'md5_processes', default=0),\n            'transfer_threads': _merge_setting(\n                cli_options, config['options']['concurrency'],\n                'transfer_threads', default=0),\n        },\n        'proxy': {\n            'host': _merge_setting(\n                cli_options, config['options']['proxy'], 'host',\n                name_cli='proxy_host'),\n            'username': _merge_setting(\n                cli_options, config['options']['proxy'], 'username',\n                name_cli='proxy_username'),\n            'password': _merge_setting(\n                cli_options, config['options']['proxy'], 'password',\n                name_cli='proxy_password'),\n        }\n    }\n    config['options'] = options", "response": "Merge global CLI options into main config dict."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_azure_storage_credentials(config, general_options):\n    # type: (dict, blobxfer.models.options.General) ->\n    #        blobxfer.operations.azure.StorageCredentials\n    \"\"\"Create an Azure StorageCredentials object from configuration\n    :param dict config: config dict\n    :param blobxfer.models.options.General: general options\n    :rtype: blobxfer.operations.azure.StorageCredentials\n    :return: credentials object\n    \"\"\"\n    creds = blobxfer.operations.azure.StorageCredentials(general_options)\n    endpoint = config['azure_storage'].get('endpoint') or 'core.windows.net'\n    for name in config['azure_storage']['accounts']:\n        key = config['azure_storage']['accounts'][name]\n        creds.add_storage_account(name, key, endpoint)\n    return creds", "response": "Create an Azure StorageCredentials object from configuration dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_general_options(config, action):\n    # type: (dict, TransferAction) -> blobxfer.models.options.General\n    \"\"\"Create a General Options object from configuration\n    :param dict config: config dict\n    :param TransferAction action: transfer action\n    :rtype: blobxfer.models.options.General\n    :return: general options object\n    \"\"\"\n    conc = config['options']['concurrency']\n    # split http proxy host into host:port\n    proxy = None\n    if blobxfer.util.is_not_empty(config['options']['proxy']['host']):\n        tmp = config['options']['proxy']['host'].split(':')\n        if len(tmp) != 2:\n            raise ValueError('Proxy host is malformed: host should be ip:port')\n        username = config['options']['proxy']['username']\n        if blobxfer.util.is_none_or_empty(username):\n            username = None\n        password = config['options']['proxy']['password']\n        if blobxfer.util.is_none_or_empty(password):\n            password = None\n        proxy = blobxfer.models.options.HttpProxy(\n            host=tmp[0],\n            port=int(tmp[1]),\n            username=username,\n            password=password,\n        )\n    return blobxfer.models.options.General(\n        concurrency=blobxfer.models.options.Concurrency(\n            crypto_processes=conc['crypto_processes'],\n            disk_threads=conc['disk_threads'],\n            md5_processes=conc['md5_processes'],\n            transfer_threads=conc['transfer_threads'],\n            action=action.value[0],\n        ),\n        log_file=config['options']['log_file'],\n        progress_bar=config['options']['progress_bar'],\n        resume_file=config['options']['resume_file'],\n        # TODO deprecated timeout setting\n        timeout=blobxfer.models.options.Timeout(\n            connect=(\n                config['options']['timeout']['connect'] or\n                config['options']['timeout']['timeout']\n            ),\n            read=(\n                config['options']['timeout']['read'] or\n                config['options']['timeout']['timeout']\n            ),\n            max_retries=config['options']['timeout']['max_retries'],\n        ),\n        verbose=config['options']['verbose'],\n        quiet=config['options']['quiet'],\n        dry_run=config['options']['dry_run'],\n        proxy=proxy,\n    )", "response": "Creates a General Options object from a configuration dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_download_specifications(ctx_cli_options, config):\n    # type: (dict, dict) -> List[blobxfer.models.download.Specification]\n    \"\"\"Create a list of Download Specification objects from configuration\n    :param dict ctx_cli_options: cli options\n    :param dict config: config dict\n    :rtype: list\n    :return: list of Download Specification objects\n    \"\"\"\n    cli_conf = ctx_cli_options[ctx_cli_options['_action']]\n    cli_options = cli_conf['options']\n    specs = []\n    for conf in config['download']:\n        if 'options' in conf:\n            conf_options = conf['options']\n        else:\n            conf_options = {}\n        # create download options\n        mode = _merge_setting(\n            cli_options, conf_options, 'mode', default='auto').lower()\n        if mode == 'auto':\n            mode = blobxfer.models.azure.StorageModes.Auto\n        elif mode == 'append':\n            mode = blobxfer.models.azure.StorageModes.Append\n        elif mode == 'block':\n            mode = blobxfer.models.azure.StorageModes.Block\n        elif mode == 'file':\n            mode = blobxfer.models.azure.StorageModes.File\n        elif mode == 'page':\n            mode = blobxfer.models.azure.StorageModes.Page\n        else:\n            raise ValueError('unknown mode: {}'.format(mode))\n        # load RSA private key PEM file if specified\n        rpk = _merge_setting(\n            cli_options, conf_options, 'rsa_private_key', default=None)\n        if blobxfer.util.is_not_empty(rpk):\n            rpkp = _merge_setting(\n                cli_options, conf_options, 'rsa_private_key_passphrase',\n                default=None)\n            rpk = blobxfer.operations.crypto.load_rsa_private_key_file(\n                rpk, rpkp)\n        else:\n            rpk = None\n        # create specification\n        conf_sod = conf_options.get('skip_on', {})\n        cli_sod = cli_options['skip_on']\n        conf_rfp = conf_options.get('restore_file_properties', {})\n        cli_rfp = cli_options['restore_file_properties']\n        ds = blobxfer.models.download.Specification(\n            download_options=blobxfer.models.options.Download(\n                check_file_md5=_merge_setting(\n                    cli_options, conf_options, 'check_file_md5',\n                    default=False),\n                chunk_size_bytes=_merge_setting(\n                    cli_options, conf_options, 'chunk_size_bytes',\n                    default=0),\n                delete_extraneous_destination=_merge_setting(\n                    cli_options, conf_options,\n                    'delete_extraneous_destination', default=False),\n                max_single_object_concurrency=_merge_setting(\n                    cli_options, conf_options,\n                    'max_single_object_concurrency', default=8),\n                mode=mode,\n                overwrite=_merge_setting(\n                    cli_options, conf_options, 'overwrite', default=True),\n                recursive=_merge_setting(\n                    cli_options, conf_options, 'recursive', default=True),\n                rename=_merge_setting(\n                    cli_options, conf_options, 'rename', default=False),\n                restore_file_properties=blobxfer.models.options.FileProperties(\n                    attributes=_merge_setting(\n                        cli_rfp, conf_rfp, 'attributes',\n                        default=False),\n                    cache_control=None,\n                    lmt=_merge_setting(\n                        cli_rfp, conf_rfp, 'lmt', default=False),\n                    md5=None,\n                ),\n                rsa_private_key=rpk,\n                strip_components=_merge_setting(\n                    cli_options, conf_options, 'strip_components',\n                    default=0),\n            ),\n            skip_on_options=blobxfer.models.options.SkipOn(\n                filesize_match=_merge_setting(\n                    cli_sod, conf_sod, 'filesize_match', default=False),\n                lmt_ge=_merge_setting(\n                    cli_sod, conf_sod, 'lmt_ge', default=False),\n                md5_match=_merge_setting(\n                    cli_sod, conf_sod, 'md5_match', default=False),\n            ),\n            local_destination_path=blobxfer.models.download.\n            LocalDestinationPath(\n                conf['destination']\n            )\n        )\n        # create remote source paths\n        for src in conf['source']:\n            if len(src) != 1:\n                raise RuntimeError(\n                    'invalid number of source pairs specified per entry')\n            sa = next(iter(src))\n            asp = blobxfer.operations.azure.SourcePath()\n            asp.add_path_with_storage_account(src[sa], sa)\n            incl = _merge_setting(cli_conf, conf, 'include', default=None)\n            if blobxfer.util.is_not_empty(incl):\n                asp.add_includes(incl)\n            excl = _merge_setting(cli_conf, conf, 'exclude', default=None)\n            if blobxfer.util.is_not_empty(excl):\n                asp.add_excludes(excl)\n            ds.add_azure_source_path(asp)\n        # append spec to list\n        specs.append(ds)\n    return specs", "response": "Creates a list of Download Specification objects from the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a list of SyncCopy Specification objects from the given configuration.", "response": "def create_synccopy_specifications(ctx_cli_options, config):\n    # type: (dict, dict) -> List[blobxfer.models.synccopy.Specification]\n    \"\"\"Create a list of SyncCopy Specification objects from configuration\n    :param dict ctx_cli_options: cli options\n    :param dict config: config dict\n    :rtype: list\n    :return: list of SyncCopy Specification objects\n    \"\"\"\n    cli_conf = ctx_cli_options[ctx_cli_options['_action']]\n    cli_options = cli_conf['options']\n    specs = []\n    for conf in config['synccopy']:\n        if 'options' in conf:\n            conf_options = conf['options']\n        else:\n            conf_options = {}\n        # get source mode\n        mode = _merge_setting(\n            cli_options, conf_options, 'mode', default='auto').lower()\n        if mode == 'auto':\n            mode = blobxfer.models.azure.StorageModes.Auto\n        elif mode == 'append':\n            mode = blobxfer.models.azure.StorageModes.Append\n        elif mode == 'block':\n            mode = blobxfer.models.azure.StorageModes.Block\n        elif mode == 'file':\n            mode = blobxfer.models.azure.StorageModes.File\n        elif mode == 'page':\n            mode = blobxfer.models.azure.StorageModes.Page\n        else:\n            raise ValueError('unknown source mode: {}'.format(mode))\n        # get destination mode\n        destmode = _merge_setting(\n            cli_options, conf_options, 'dest_mode', name_cli='dest_mode')\n        if blobxfer.util.is_none_or_empty(destmode):\n            destmode = mode\n        else:\n            destmode = destmode.lower()\n            if destmode == 'auto':\n                destmode = blobxfer.models.azure.StorageModes.Auto\n            elif destmode == 'append':\n                destmode = blobxfer.models.azure.StorageModes.Append\n            elif destmode == 'block':\n                destmode = blobxfer.models.azure.StorageModes.Block\n            elif destmode == 'file':\n                destmode = blobxfer.models.azure.StorageModes.File\n            elif destmode == 'page':\n                destmode = blobxfer.models.azure.StorageModes.Page\n            else:\n                raise ValueError('unknown dest mode: {}'.format(destmode))\n        # create specification\n        conf_sod = conf_options.get('skip_on', {})\n        cli_sod = cli_options['skip_on']\n        scs = blobxfer.models.synccopy.Specification(\n            synccopy_options=blobxfer.models.options.SyncCopy(\n                access_tier=_merge_setting(\n                    cli_options, conf_options, 'access_tier', default=None),\n                delete_extraneous_destination=_merge_setting(\n                    cli_options, conf_options,\n                    'delete_extraneous_destination', default=False),\n                dest_mode=destmode,\n                mode=mode,\n                overwrite=_merge_setting(\n                    cli_options, conf_options, 'overwrite', default=True),\n                recursive=_merge_setting(\n                    cli_options, conf_options, 'recursive', default=True),\n                rename=_merge_setting(\n                    cli_options, conf_options, 'rename', default=False),\n            ),\n            skip_on_options=blobxfer.models.options.SkipOn(\n                filesize_match=_merge_setting(\n                    cli_sod, conf_sod, 'filesize_match', default=False),\n                lmt_ge=_merge_setting(\n                    cli_sod, conf_sod, 'lmt_ge', default=False),\n                md5_match=_merge_setting(\n                    cli_sod, conf_sod, 'md5_match', default=False),\n            ),\n        )\n        # create remote source paths\n        for src in conf['source']:\n            sa = next(iter(src))\n            asp = blobxfer.operations.azure.SourcePath()\n            asp.add_path_with_storage_account(src[sa], sa)\n            incl = _merge_setting(cli_conf, conf, 'include', default=None)\n            if blobxfer.util.is_not_empty(incl):\n                asp.add_includes(incl)\n            excl = _merge_setting(cli_conf, conf, 'exclude', default=None)\n            if blobxfer.util.is_not_empty(excl):\n                asp.add_excludes(excl)\n            scs.add_azure_source_path(asp)\n        # create remote destination paths\n        for dst in conf['destination']:\n            if len(dst) != 1:\n                raise RuntimeError(\n                    'invalid number of destination pairs specified per entry')\n            sa = next(iter(dst))\n            adp = blobxfer.operations.azure.DestinationPath()\n            adp.add_path_with_storage_account(dst[sa], sa)\n            scs.add_azure_destination_path(adp)\n        # append spec to list\n        specs.append(scs)\n    return specs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a list of Upload Specification objects from the given CLI options and configuration.", "response": "def create_upload_specifications(ctx_cli_options, config):\n    # type: (dict, dict) -> List[blobxfer.models.upload.Specification]\n    \"\"\"Create a list of Upload Specification objects from configuration\n    :param dict ctx_cli_options: cli options\n    :param dict config: config dict\n    :rtype: list\n    :return: list of Upload Specification objects\n    \"\"\"\n    cli_conf = ctx_cli_options[ctx_cli_options['_action']]\n    cli_options = cli_conf['options']\n    specs = []\n    for conf in config['upload']:\n        if 'options' in conf:\n            conf_options = conf['options']\n        else:\n            conf_options = {}\n        # create upload options\n        mode = _merge_setting(\n            cli_options, conf_options, 'mode', default='auto').lower()\n        if mode == 'auto':\n            mode = blobxfer.models.azure.StorageModes.Auto\n        elif mode == 'append':\n            mode = blobxfer.models.azure.StorageModes.Append\n        elif mode == 'block':\n            mode = blobxfer.models.azure.StorageModes.Block\n        elif mode == 'file':\n            mode = blobxfer.models.azure.StorageModes.File\n        elif mode == 'page':\n            mode = blobxfer.models.azure.StorageModes.Page\n        else:\n            raise ValueError('unknown mode: {}'.format(mode))\n        # load RSA public key PEM if specified\n        rpk = _merge_setting(cli_options, conf_options, 'rsa_public_key')\n        if blobxfer.util.is_not_empty(rpk):\n            rpk = blobxfer.operations.crypto.load_rsa_public_key_file(rpk)\n        if rpk is None:\n            # load RSA private key PEM file if specified\n            rpk = _merge_setting(\n                cli_options, conf_options, 'rsa_private_key')\n            if blobxfer.util.is_not_empty(rpk):\n                rpkp = _merge_setting(\n                    cli_options, conf_options, 'rsa_private_key_passphrase')\n                rpk = blobxfer.operations.crypto.load_rsa_private_key_file(\n                    rpk, rpkp)\n                rpk = rpk.public_key()\n            else:\n                rpk = None\n        # create local source paths\n        lsp = blobxfer.models.upload.LocalSourcePath()\n        lsp.add_paths(conf['source'])\n        incl = _merge_setting(cli_conf, conf, 'include', default=None)\n        if blobxfer.util.is_not_empty(incl):\n            lsp.add_includes(incl)\n        excl = _merge_setting(cli_conf, conf, 'exclude', default=None)\n        if blobxfer.util.is_not_empty(excl):\n            lsp.add_excludes(excl)\n        # create specification\n        conf_sfp = conf_options.get('store_file_properties', {})\n        cli_sfp = cli_options['store_file_properties']\n        conf_vio = conf_options.get('vectored_io', {})\n        cli_vio = cli_options['vectored_io']\n        conf_sod = conf_options.get('skip_on', {})\n        cli_sod = cli_options['skip_on']\n        us = blobxfer.models.upload.Specification(\n            upload_options=blobxfer.models.options.Upload(\n                access_tier=_merge_setting(\n                    cli_options, conf_options, 'access_tier', default=None),\n                chunk_size_bytes=_merge_setting(\n                    cli_options, conf_options, 'chunk_size_bytes',\n                    default=0),\n                delete_extraneous_destination=_merge_setting(\n                    cli_options, conf_options,\n                    'delete_extraneous_destination', default=False),\n                mode=mode,\n                one_shot_bytes=_merge_setting(\n                    cli_options, conf_options, 'one_shot_bytes', default=0),\n                overwrite=_merge_setting(\n                    cli_options, conf_options, 'overwrite', default=True),\n                recursive=_merge_setting(\n                    cli_options, conf_options, 'recursive', default=True),\n                rename=_merge_setting(\n                    cli_options, conf_options, 'rename', default=False),\n                rsa_public_key=rpk,\n                store_file_properties=blobxfer.models.options.FileProperties(\n                    attributes=_merge_setting(\n                        cli_sfp, conf_sfp, 'attributes', default=False),\n                    cache_control=_merge_setting(\n                        cli_sfp, conf_sfp, 'cache_control', default=None),\n                    lmt=None,\n                    md5=_merge_setting(\n                        cli_sfp, conf_sfp, 'md5', default=False),\n                ),\n                stdin_as_page_blob_size=_merge_setting(\n                    cli_options, conf_options, 'stdin_as_page_blob_size',\n                    default=0),\n                strip_components=_merge_setting(\n                    cli_options, conf_options, 'strip_components',\n                    default=0),\n                vectored_io=blobxfer.models.options.VectoredIo(\n                    stripe_chunk_size_bytes=_merge_setting(\n                        cli_vio, conf_vio, 'stripe_chunk_size_bytes',\n                        default=1073741824),\n                    distribution_mode=blobxfer.\n                    models.upload.VectoredIoDistributionMode(\n                        _merge_setting(\n                            cli_vio, conf_vio, 'distribution_mode',\n                            default='disabled').lower()),\n                ),\n            ),\n            skip_on_options=blobxfer.models.options.SkipOn(\n                filesize_match=_merge_setting(\n                    cli_sod, conf_sod, 'filesize_match', default=False),\n                lmt_ge=_merge_setting(\n                    cli_sod, conf_sod, 'lmt_ge', default=False),\n                md5_match=_merge_setting(\n                    cli_sod, conf_sod, 'md5_match', default=False),\n            ),\n            local_source_path=lsp,\n        )\n        # create remote destination paths\n        for dst in conf['destination']:\n            if len(dst) != 1:\n                raise RuntimeError(\n                    'invalid number of destination pairs specified per entry')\n            sa = next(iter(dst))\n            adp = blobxfer.operations.azure.DestinationPath()\n            adp.add_path_with_storage_account(dst[sa], sa)\n            us.add_azure_destination_path(adp)\n        # append spec to list\n        specs.append(us)\n    return specs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _initialize_processes(self, target, num_workers, description):\n        # type: (_MultiprocessOffload, function, int, str) -> None\n        \"\"\"Initialize processes\n        :param _MultiprocessOffload self: this\n        :param function target: target function for process\n        :param int num_workers: number of worker processes\n        :param str description: description\n        \"\"\"\n        if num_workers is None or num_workers < 1:\n            raise ValueError('invalid num_workers: {}'.format(num_workers))\n        logger.debug('initializing {}{} processes'.format(\n            num_workers, ' ' + description if not None else ''))\n        for _ in range(num_workers):\n            proc = multiprocessing.Process(target=target)\n            proc.start()\n            self._procs.append(proc)", "response": "Initialize processes in the process pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinalizes processes with the term signal set to 1 and join all processes.", "response": "def finalize_processes(self):\n        # type: (_MultiprocessOffload) -> None\n        \"\"\"Finalize processes\n        :param _MultiprocessOffload self: this\n        \"\"\"\n        self._term_signal.value = 1\n        if self._check_thread is not None:\n            self._check_thread.join()\n        for proc in self._procs:\n            proc.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the multiprocess done queue check thread", "response": "def initialize_check_thread(self, check_func):\n        # type: (_MultiprocessOffload, function) -> None\n        \"\"\"Initialize the multiprocess done queue check thread\n        :param Downloader self: this\n        :param function check_func: check function\n        \"\"\"\n        self._check_thread = threading.Thread(target=check_func)\n        self._check_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download(ctx):\n    settings.add_cli_options(ctx.cli_options, settings.TransferAction.Download)\n    ctx.initialize(settings.TransferAction.Download)\n    specs = settings.create_download_specifications(\n        ctx.cli_options, ctx.config)\n    del ctx.cli_options\n    for spec in specs:\n        blobxfer.api.Downloader(\n            ctx.general_options, ctx.credentials, spec\n        ).start()", "response": "Download blobs or files from Azure Storage"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef synccopy(ctx):\n    settings.add_cli_options(ctx.cli_options, settings.TransferAction.Synccopy)\n    ctx.initialize(settings.TransferAction.Synccopy)\n    specs = settings.create_synccopy_specifications(\n        ctx.cli_options, ctx.config)\n    del ctx.cli_options\n    for spec in specs:\n        blobxfer.api.SyncCopy(\n            ctx.general_options, ctx.credentials, spec\n        ).start()", "response": "Synchronously copy blobs or files between Azure Storage accounts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload files to Azure Storage", "response": "def upload(ctx):\n    \"\"\"Upload files to Azure Storage\"\"\"\n    settings.add_cli_options(ctx.cli_options, settings.TransferAction.Upload)\n    ctx.initialize(settings.TransferAction.Upload)\n    specs = settings.create_upload_specifications(\n        ctx.cli_options, ctx.config)\n    del ctx.cli_options\n    for spec in specs:\n        blobxfer.api.Uploader(\n            ctx.general_options, ctx.credentials, spec\n        ).start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initialize(self, action):\n        # type: (CliContext, settings.TransferAction) -> None\n        \"\"\"Initialize context\n        :param CliContext self: this\n        :param settings.TransferAction action: transfer action\n        \"\"\"\n        self._init_config()\n        self.general_options = settings.create_general_options(\n            self.config, action)\n        self.credentials = settings.create_azure_storage_credentials(\n            self.config, self.general_options)", "response": "Initialize context with the given settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_yaml_file(self, yaml_file):\n        # type: (CliContext, pathlib.Path) -> None\n        \"\"\"Read a yaml file into self.config\n        :param CliContext self: this\n        :param pathlib.Path yaml_file: yaml file to load\n        \"\"\"\n        with yaml_file.open('r') as f:\n            if self.config is None:\n                self.config = ruamel.yaml.load(\n                    f, Loader=ruamel.yaml.RoundTripLoader)\n            else:\n                self.config = blobxfer.util.merge_dict(\n                    ruamel.yaml.load(f, Loader=ruamel.yaml.RoundTripLoader),\n                    self.config)", "response": "Read a yaml file into self. config"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_config(self):\n        # type: (CliContext) -> None\n        \"\"\"Initializes configuration of the context\n        :param CliContext self: this\n        \"\"\"\n        # load yaml config file into memory\n        if blobxfer.util.is_not_empty(self.cli_options['yaml_config']):\n            yaml_config = pathlib.Path(self.cli_options['yaml_config'])\n            self._read_yaml_file(yaml_config)\n        if self.config is None:\n            self.config = {}\n        # merge \"global\" cli options with config\n        settings.merge_global_settings(self.config, self.cli_options)\n        # set log file if specified\n        logfile = self.config['options'].get('log_file', None)\n        blobxfer.util.setup_logger(logger, logfile)\n        # set azure storage logging level\n        azstorage_logger = logging.getLogger('azure.storage')\n        if self.config['options'].get('enable_azure_storage_logger', False):\n            blobxfer.util.setup_logger(azstorage_logger, logfile)\n            azstorage_logger.setLevel(logging.INFO)\n        else:\n            # disable azure storage logging: setting logger level to CRITICAL\n            # effectively disables logging from azure storage\n            azstorage_logger.setLevel(logging.CRITICAL)\n        # set verbose logging\n        if self.config['options'].get('verbose', False):\n            blobxfer.util.set_verbose_logger_handlers()\n        # output mixed config\n        if self.show_config:\n            logger.debug('config: \\n{}'.format(\n                json.dumps(self.config, indent=4)))\n            logger.debug('cli config: \\n{}'.format(\n                json.dumps(\n                    self.cli_options[self.cli_options['_action']],\n                    indent=4, sort_keys=True)))\n        del self.show_config", "response": "Initializes the configuration of the context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends the given emails to the SG.", "response": "def send_messages(self, emails):\n        '''\n        Comments\n        '''\n        if not emails:\n            return\n\n        count = 0\n        for email in emails:\n            mail = self._build_sg_mail(email)\n            try:\n                self.sg.client.mail.send.post(request_body=mail)\n                count += 1\n            except HTTPError as e:\n                if not self.fail_silently:\n                    raise\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the list of bindings supported by an IDP", "response": "def get_idp_sso_supported_bindings(idp_entity_id=None, config=None):\n    \"\"\"Returns the list of bindings supported by an IDP\n    This is not clear in the pysaml2 code, so wrapping it in a util\"\"\"\n    if config is None:\n        # avoid circular import\n        from djangosaml2.conf import get_config\n        config = get_config()\n    # load metadata store from config\n    meta = getattr(config, 'metadata', {})\n    # if idp is None, assume only one exists so just use that\n    if idp_entity_id is None:\n        # .keys() returns dict_keys in python3.5+\n        try:\n            idp_entity_id = list(available_idps(config).keys())[0]\n        except IndexError:\n            raise ImproperlyConfigured(\"No IdP configured!\")\n    try:\n        return meta.service(idp_entity_id, 'idpsso_descriptor', 'single_sign_on_service').keys()\n    except UnknownSystemEntity:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fail_acs_response(request, *args, **kwargs):\n    failure_function = import_string(get_custom_setting('SAML_ACS_FAILURE_RESPONSE_FUNCTION',\n                                                        'djangosaml2.acs_failures.template_failure'))\n    return failure_function(request, *args, **kwargs)", "response": "Serves as a common mechanism for ending ACS in case of any SAML related failure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef login(request,\n          config_loader_path=None,\n          wayf_template='djangosaml2/wayf.html',\n          authorization_error_template='djangosaml2/auth_error.html',\n          post_binding_form_template='djangosaml2/post_binding_form.html'):\n    \"\"\"SAML Authorization Request initiator\n\n    This view initiates the SAML2 Authorization handshake\n    using the pysaml2 library to create the AuthnRequest.\n    It uses the SAML 2.0 Http Redirect protocol binding.\n\n    * post_binding_form_template - path to a template containing HTML form with\n    hidden input elements, used to send the SAML message data when HTTP POST\n    binding is being used. You can customize this template to include custom\n    branding and/or text explaining the automatic redirection process. Please\n    see the example template in\n    templates/djangosaml2/example_post_binding_form.html\n    If set to None or nonexistent template, default form from the saml2 library\n    will be rendered.\n    \"\"\"\n    logger.debug('Login process started')\n\n    came_from = request.GET.get('next', settings.LOGIN_REDIRECT_URL)\n    if not came_from:\n        logger.warning('The next parameter exists but is empty')\n        came_from = settings.LOGIN_REDIRECT_URL\n\n    # Ensure the user-originating redirection url is safe.\n    if not is_safe_url_compat(url=came_from, allowed_hosts={request.get_host()}):\n        came_from = settings.LOGIN_REDIRECT_URL\n\n    # if the user is already authenticated that maybe because of two reasons:\n    # A) He has this URL in two browser windows and in the other one he\n    #    has already initiated the authenticated session.\n    # B) He comes from a view that (incorrectly) send him here because\n    #    he does not have enough permissions. That view should have shown\n    #    an authorization error in the first place.\n    # We can only make one thing here and that is configurable with the\n    # SAML_IGNORE_AUTHENTICATED_USERS_ON_LOGIN setting. If that setting\n    # is True (default value) we will redirect him to the came_from view.\n    # Otherwise, we will show an (configurable) authorization error.\n    if callable_bool(request.user.is_authenticated):\n        redirect_authenticated_user = getattr(settings, 'SAML_IGNORE_AUTHENTICATED_USERS_ON_LOGIN', True)\n        if redirect_authenticated_user:\n            return HttpResponseRedirect(came_from)\n        else:\n            logger.debug('User is already logged in')\n            return render(request, authorization_error_template, {\n                    'came_from': came_from,\n                    })\n\n    selected_idp = request.GET.get('idp', None)\n    conf = get_config(config_loader_path, request)\n\n    # is a embedded wayf needed?\n    idps = available_idps(conf)\n    if selected_idp is None and len(idps) > 1:\n        logger.debug('A discovery process is needed')\n        return render(request, wayf_template, {\n                'available_idps': idps.items(),\n                'came_from': came_from,\n                })\n\n    # choose a binding to try first\n    sign_requests = getattr(conf, '_sp_authn_requests_signed', False)\n    binding = BINDING_HTTP_POST if sign_requests else BINDING_HTTP_REDIRECT\n    logger.debug('Trying binding %s for IDP %s', binding, selected_idp)\n\n    # ensure our selected binding is supported by the IDP\n    supported_bindings = get_idp_sso_supported_bindings(selected_idp, config=conf)\n    if binding not in supported_bindings:\n        logger.debug('Binding %s not in IDP %s supported bindings: %s',\n                     binding, selected_idp, supported_bindings)\n        if binding == BINDING_HTTP_POST:\n            logger.warning('IDP %s does not support %s,  trying %s',\n                           selected_idp, binding, BINDING_HTTP_REDIRECT)\n            binding = BINDING_HTTP_REDIRECT\n        else:\n            logger.warning('IDP %s does not support %s,  trying %s',\n                           selected_idp, binding, BINDING_HTTP_POST)\n            binding = BINDING_HTTP_POST\n        # if switched binding still not supported, give up\n        if binding not in supported_bindings:\n            raise UnsupportedBinding('IDP %s does not support %s or %s',\n                                     selected_idp, BINDING_HTTP_POST, BINDING_HTTP_REDIRECT)\n\n    client = Saml2Client(conf)\n    http_response = None\n\n    logger.debug('Redirecting user to the IdP via %s binding.', binding)\n    if binding == BINDING_HTTP_REDIRECT:\n        try:\n            # do not sign the xml itself, instead use the sigalg to\n            # generate the signature as a URL param\n            sig_alg_option_map = {'sha1': SIG_RSA_SHA1,\n                                  'sha256': SIG_RSA_SHA256}\n            sig_alg_option = getattr(conf, '_sp_authn_requests_signed_alg', 'sha1')\n            sigalg = sig_alg_option_map[sig_alg_option] if sign_requests else None\n            nsprefix = get_namespace_prefixes()\n            session_id, result = client.prepare_for_authenticate(\n                entityid=selected_idp, relay_state=came_from,\n                binding=binding, sign=False, sigalg=sigalg,\n                nsprefix=nsprefix)\n        except TypeError as e:\n            logger.error('Unable to know which IdP to use')\n            return HttpResponse(text_type(e))\n        else:\n            http_response = HttpResponseRedirect(get_location(result))\n    elif binding == BINDING_HTTP_POST:\n        if post_binding_form_template:\n            # get request XML to build our own html based on the template\n            try:\n                location = client.sso_location(selected_idp, binding)\n            except TypeError as e:\n                logger.error('Unable to know which IdP to use')\n                return HttpResponse(text_type(e))\n            session_id, request_xml = client.create_authn_request(\n                location,\n                binding=binding)\n            try:\n                if PY3:\n                    saml_request = base64.b64encode(binary_type(request_xml, 'UTF-8'))\n                else:\n                    saml_request = base64.b64encode(binary_type(request_xml))\n\n                http_response = render(request, post_binding_form_template, {\n                    'target_url': location,\n                    'params': {\n                        'SAMLRequest': saml_request,\n                        'RelayState': came_from,\n                        },\n                    })\n            except TemplateDoesNotExist:\n                pass\n\n        if not http_response:\n            # use the html provided by pysaml2 if no template was specified or it didn't exist\n            try:\n                session_id, result = client.prepare_for_authenticate(\n                    entityid=selected_idp, relay_state=came_from,\n                    binding=binding)\n            except TypeError as e:\n                logger.error('Unable to know which IdP to use')\n                return HttpResponse(text_type(e))\n            else:\n                http_response = HttpResponse(result['data'])\n    else:\n        raise UnsupportedBinding('Unsupported binding: %s', binding)\n\n    # success, so save the session ID and return our response\n    logger.debug('Saving the session_id in the OutstandingQueries cache')\n    oq_cache = OutstandingQueriesCache(request.session)\n    oq_cache.set(session_id, came_from)\n    return http_response", "response": "This function is called by the SAML2 library to initiate the SAML2 authentication process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assertion_consumer_service(request,\n                               config_loader_path=None,\n                               attribute_mapping=None,\n                               create_unknown_user=None):\n    \"\"\"SAML Authorization Response endpoint\n\n    The IdP will send its response to this view, which\n    will process it with pysaml2 help and log the user\n    in using the custom Authorization backend\n    djangosaml2.backends.Saml2Backend that should be\n    enabled in the settings.py\n    \"\"\"\n    attribute_mapping = attribute_mapping or get_custom_setting('SAML_ATTRIBUTE_MAPPING', {'uid': ('username', )})\n    create_unknown_user = create_unknown_user if create_unknown_user is not None else \\\n                          get_custom_setting('SAML_CREATE_UNKNOWN_USER', True)\n    conf = get_config(config_loader_path, request)\n    try:\n        xmlstr = request.POST['SAMLResponse']\n    except KeyError:\n        logger.warning('Missing \"SAMLResponse\" parameter in POST data.')\n        raise SuspiciousOperation\n\n    client = Saml2Client(conf, identity_cache=IdentityCache(request.session))\n\n    oq_cache = OutstandingQueriesCache(request.session)\n    outstanding_queries = oq_cache.outstanding_queries()\n\n    try:\n        response = client.parse_authn_request_response(xmlstr, BINDING_HTTP_POST, outstanding_queries)\n    except (StatusError, ToEarly):\n        logger.exception(\"Error processing SAML Assertion.\")\n        return fail_acs_response(request)\n    except ResponseLifetimeExceed:\n        logger.info(\"SAML Assertion is no longer valid. Possibly caused by network delay or replay attack.\", exc_info=True)\n        return fail_acs_response(request)\n    except SignatureError:\n        logger.info(\"Invalid or malformed SAML Assertion.\", exc_info=True)\n        return fail_acs_response(request)\n    except StatusAuthnFailed:\n        logger.info(\"Authentication denied for user by IdP.\", exc_info=True)\n        return fail_acs_response(request)\n    except StatusRequestDenied:\n        logger.warning(\"Authentication interrupted at IdP.\", exc_info=True)\n        return fail_acs_response(request)\n    except StatusNoAuthnContext:\n        logger.warning(\"Missing Authentication Context from IdP.\", exc_info=True)\n        return fail_acs_response(request)\n    except MissingKey:\n        logger.exception(\"SAML Identity Provider is not configured correctly: certificate key is missing!\")\n        return fail_acs_response(request)\n    except UnsolicitedResponse:\n        logger.exception(\"Received SAMLResponse when no request has been made.\")\n        return fail_acs_response(request)\n\n    if response is None:\n        logger.warning(\"Invalid SAML Assertion received (unknown error).\")\n        return fail_acs_response(request, status=400, exc_class=SuspiciousOperation)\n\n    session_id = response.session_id()\n    oq_cache.delete(session_id)\n\n    # authenticate the remote user\n    session_info = response.session_info()\n\n    if callable(attribute_mapping):\n        attribute_mapping = attribute_mapping()\n    if callable(create_unknown_user):\n        create_unknown_user = create_unknown_user()\n\n    logger.debug('Trying to authenticate the user. Session info: %s', session_info)\n    user = auth.authenticate(request=request,\n                             session_info=session_info,\n                             attribute_mapping=attribute_mapping,\n                             create_unknown_user=create_unknown_user)\n    if user is None:\n        logger.warning(\"Could not authenticate user received in SAML Assertion. Session info: %s\", session_info)\n        raise PermissionDenied\n\n    auth.login(request, user)\n    _set_subject_id(request.session, session_info['name_id'])\n    logger.debug(\"User %s authenticated via SSO.\", user)\n\n    logger.debug('Sending the post_authenticated signal')\n    post_authenticated.send_robust(sender=user, session_info=session_info)\n\n    # redirect the user to the view where he came from\n    default_relay_state = get_custom_setting('ACS_DEFAULT_REDIRECT_URL',\n                                             settings.LOGIN_REDIRECT_URL)\n    relay_state = request.POST.get('RelayState', default_relay_state)\n    if not relay_state:\n        logger.warning('The RelayState parameter exists but is empty')\n        relay_state = default_relay_state\n    if not is_safe_url_compat(url=relay_state, allowed_hosts={request.get_host()}):\n        relay_state = settings.LOGIN_REDIRECT_URL\n    logger.debug('Redirecting to the RelayState: %s', relay_state)\n    return HttpResponseRedirect(relay_state)", "response": "SAML Authorization Response endpoint\n\n    The IdP will send its response to this view, which\n    will process it with pysaml2 help and log the user\n    in using the custom Authorization backend\n    djangosaml2.backends.Saml2Backend that should be\n    enabled in the settings.py"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef echo_attributes(request,\n                    config_loader_path=None,\n                    template='djangosaml2/echo_attributes.html'):\n    \"\"\"Example view that echo the SAML attributes of an user\"\"\"\n    state = StateCache(request.session)\n    conf = get_config(config_loader_path, request)\n\n    client = Saml2Client(conf, state_cache=state,\n                         identity_cache=IdentityCache(request.session))\n    subject_id = _get_subject_id(request.session)\n    try:\n        identity = client.users.get_identity(subject_id,\n                                             check_not_on_or_after=False)\n    except AttributeError:\n        return HttpResponse(\"No active SAML identity found. Are you sure you have logged in via SAML?\")\n\n    return render(request, template, {'attributes': identity[0]})", "response": "Example view that echo the SAML attributes of an user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_logout_service(request, data, binding, config_loader_path=None, next_page=None,\n                   logout_error_template='djangosaml2/logout_error.html'):\n    \"\"\"SAML Logout Response endpoint\n\n    The IdP will send the logout response to this view,\n    which will process it with pysaml2 help and log the user\n    out.\n    Note that the IdP can request a logout even when\n    we didn't initiate the process as a single logout\n    request started by another SP.\n    \"\"\"\n    logger.debug('Logout service started')\n    conf = get_config(config_loader_path, request)\n\n    state = StateCache(request.session)\n    client = Saml2Client(conf, state_cache=state,\n                         identity_cache=IdentityCache(request.session))\n\n    if 'SAMLResponse' in data:  # we started the logout\n        logger.debug('Receiving a logout response from the IdP')\n        response = client.parse_logout_request_response(data['SAMLResponse'], binding)\n        state.sync()\n        return finish_logout(request, response, next_page=next_page)\n\n    elif 'SAMLRequest' in data:  # logout started by the IdP\n        logger.debug('Receiving a logout request from the IdP')\n        subject_id = _get_subject_id(request.session)\n        if subject_id is None:\n            logger.warning(\n                'The session does not contain the subject id for user %s. Performing local logout',\n                request.user)\n            auth.logout(request)\n            return render(request, logout_error_template, status=403)\n        else:\n            http_info = client.handle_logout_request(\n                data['SAMLRequest'],\n                subject_id,\n                binding,\n                relay_state=data.get('RelayState', ''))\n            state.sync()\n            auth.logout(request)\n            return HttpResponseRedirect(get_location(http_info))\n    else:\n        logger.error('No SAMLResponse or SAMLRequest parameter found')\n        raise Http404('No SAMLResponse or SAMLRequest parameter found')", "response": "SAML Logout Response endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an XML with the SAML 2. 0 metadata for this SP as configured in the settings. py file.", "response": "def metadata(request, config_loader_path=None, valid_for=None):\n    \"\"\"Returns an XML with the SAML 2.0 metadata for this\n    SP as configured in the settings.py file.\n    \"\"\"\n    conf = get_config(config_loader_path, request)\n    metadata = entity_descriptor(conf)\n    return HttpResponse(content=text_type(metadata).encode('utf-8'),\n                        content_type=\"text/xml; charset=utf8\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfigure a user after creation and returns the updated user.", "response": "def configure_user(self, user, attributes, attribute_mapping):\n        \"\"\"Configures a user after creation and returns the updated user.\n\n        By default, returns the user with his attributes updated.\n        \"\"\"\n        user.set_unusable_password()\n        return self.update_user(user, attributes, attribute_mapping,\n                                force_save=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate a user with a set of attributes and returns the updated user.", "response": "def update_user(self, user, attributes, attribute_mapping,\n                    force_save=False):\n        \"\"\"Update a user with a set of attributes and returns the updated user.\n\n        By default it uses a mapping defined in the settings constant\n        SAML_ATTRIBUTE_MAPPING. For each attribute, if the user object has\n        that field defined it will be set.\n        \"\"\"\n        if not attribute_mapping:\n            return user\n\n        user_modified = False\n        for saml_attr, django_attrs in attribute_mapping.items():\n            attr_value_list = attributes.get(saml_attr)\n            if not attr_value_list:\n                logger.debug(\n                    'Could not find value for \"%s\", not updating fields \"%s\"',\n                    saml_attr, django_attrs)\n                continue\n\n            for attr in django_attrs:\n                if hasattr(user, attr):\n                    user_attr = getattr(user, attr)\n                    if callable(user_attr):\n                        modified = user_attr(attr_value_list)\n                    else:\n                        modified = self._set_attribute(user, attr, attr_value_list[0])\n\n                    user_modified = user_modified or modified\n                else:\n                    logger.debug(\n                        'Could not find attribute \"%s\" on user \"%s\"', attr, user)\n\n        logger.debug('Sending the pre_save signal')\n        signal_modified = any(\n            [response for receiver, response\n             in pre_user_save.send_robust(sender=user.__class__,\n                                          instance=user,\n                                          attributes=attributes,\n                                          user_modified=user_modified)]\n            )\n\n        if user_modified or signal_modified or force_save:\n            user.save()\n\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset an attribute of an object to a specific value.", "response": "def _set_attribute(self, obj, attr, value):\n        \"\"\"Set an attribute of an object to a specific value.\n\n        Return True if the attribute was changed and False otherwise.\n        \"\"\"\n        field = obj._meta.get_field(attr)\n        if field.max_length is not None and len(value) > field.max_length:\n            cleaned_value = value[:field.max_length]\n            logger.warn('The attribute \"%s\" was trimmed from \"%s\" to \"%s\"',\n                        attr, value, cleaned_value)\n        else:\n            cleaned_value = value\n\n        old_value = getattr(obj, attr)\n        if cleaned_value != old_value:\n            setattr(obj, attr, cleaned_value)\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mkpath(*segments, **query):\n    # Remove empty segments (e.g. no key specified)\n    segments = [bytes_to_str(s) for s in segments if s is not None]\n    # Join the segments into a path\n    pathstring = '/'.join(segments)\n    # Remove extra slashes\n    pathstring = re.sub('/+', '/', pathstring)\n\n    # Add the query string if it exists\n    _query = {}\n    for key in query:\n        if query[key] in [False, True]:\n            _query[key] = str(query[key]).lower()\n        elif query[key] is not None:\n            if PY2 and isinstance(query[key], unicode):  # noqa\n                _query[key] = query[key].encode('utf-8')\n            else:\n                _query[key] = query[key]\n\n    if len(_query) > 0:\n        pathstring += \"?\" + urlencode(_query)\n\n    if not pathstring.startswith('/'):\n        pathstring = '/' + pathstring\n\n    return pathstring", "response": "Constructs the path portion of a URI from a path segments\n    and a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_index_path(self, index=None, **options):\n        if not self.yz_wm_index:\n            raise RiakError(\"Yokozuna search is unsupported by this Riak node\")\n        if index:\n            quote_plus(index)\n        return mkpath(self.yz_wm_index, \"index\", index, **options)", "response": "Builds a Yokozuna search index URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_schema_path(self, index, **options):\n        if not self.yz_wm_schema:\n            raise RiakError(\"Yokozuna search is unsupported by this Riak node\")\n        return mkpath(self.yz_wm_schema, \"schema\", quote_plus(index),\n                      **options)", "response": "Builds a Yokozuna search Solr schema URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the URL for the bucket / key preflist information.", "response": "def preflist_path(self, bucket, key, bucket_type=None, **options):\n        \"\"\"\n        Generate the URL for bucket/key preflist information\n\n        :param bucket: Name of a Riak bucket\n        :type bucket: string\n        :param key: Name of a Key\n        :type key: string\n        :param bucket_type: Optional Riak Bucket Type\n        :type bucket_type: None or string\n        :rtype URL string\n        \"\"\"\n        if not self.riak_kv_wm_preflist:\n            raise RiakError(\"Preflists are unsupported by this Riak node\")\n        if self.riak_kv_wm_bucket_type and bucket_type:\n            return mkpath(\"/types\", quote_plus(bucket_type),\n                          \"buckets\", quote_plus(bucket),\n                          \"keys\", quote_plus(key),\n                          \"preflist\", **options)\n        else:\n            return mkpath(\"/buckets\", quote_plus(bucket),\n                          \"keys\", quote_plus(key),\n                          \"preflist\", **options)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge two deep dicts non - destructively", "response": "def deep_merge(a, b):\n    \"\"\"Merge two deep dicts non-destructively\n\n    Uses a stack to avoid maximum recursion depth exceptions\n\n    >>> a = {'a': 1, 'b': {1: 1, 2: 2}, 'd': 6}\n    >>> b = {'c': 3, 'b': {2: 7}, 'd': {'z': [1, 2, 3]}}\n    >>> c = deep_merge(a, b)\n    >>> from pprint import pprint; pprint(c)\n    {'a': 1, 'b': {1: 1, 2: 7}, 'c': 3, 'd': {'z': [1, 2, 3]}}\n    \"\"\"\n    assert quacks_like_dict(a), quacks_like_dict(b)\n    dst = a.copy()\n\n    stack = [(dst, b)]\n    while stack:\n        current_dst, current_src = stack.pop()\n        for key in current_src:\n            if key not in current_dst:\n                current_dst[key] = current_src[key]\n            else:\n                if (quacks_like_dict(current_src[key]) and\n                        quacks_like_dict(current_dst[key])):\n                    stack.append((current_dst[key], current_src[key]))\n                else:\n                    current_dst[key] = current_src[key]\n    return dst"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the modification operation of the Hll.", "response": "def to_op(self):\n        \"\"\"\n        Extracts the modification operation from the Hll.\n\n        :rtype: dict, None\n        \"\"\"\n        if not self._adds:\n            return None\n        changes = {}\n        if self._adds:\n            changes['adds'] = list(self._adds)\n        return changes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an element to the HyperLogLog. Datatype cardinality will be updated when the object is saved.", "response": "def add(self, element):\n        \"\"\"\n        Adds an element to the HyperLogLog. Datatype cardinality will\n        be updated when the object is saved.\n\n        :param element: the element to add\n        :type element: str\n        \"\"\"\n        if not isinstance(element, six.string_types):\n            raise TypeError(\"Hll elements can only be strings\")\n        self._adds.add(element)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ping(self):\n        status, _, body = self._request('GET', self.ping_path())\n        return(status is not None) and (bytes_to_str(body) == 'OK')", "response": "Check server is alive over HTTP\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stats(self):\n        status, _, body = self._request('GET', self.stats_path(),\n                                        {'Accept': 'application/json'})\n        if status == 200:\n            return json.loads(bytes_to_str(body))\n        else:\n            return None", "response": "Gets performance statistics and server information for a resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_resources(self):\n        status, _, body = self._request('GET', '/',\n                                        {'Accept': 'application/json'})\n        if status == 200:\n            tmp, resources = json.loads(bytes_to_str(body)), {}\n            for k in tmp:\n                # The keys and values returned by json.loads() are unicode,\n                # which will cause problems when passed into httplib later\n                # (expecting bytes both in Python 2.x and 3.x).\n                # We just encode the resource paths into bytes, with an\n                # encoding consistent with what the resources module expects.\n                resources[k] = tmp[k].encode('utf-8')\n            return resources\n        else:\n            return {}", "response": "Gets a JSON mapping of server - side resource names to paths\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, robj, r=None, pr=None, timeout=None, basic_quorum=None,\n            notfound_ok=None, head_only=False):\n        \"\"\"\n        Get a bucket/key from the server\n        \"\"\"\n        # We could detect quorum_controls here but HTTP ignores\n        # unknown flags/params.\n        params = {'r': r, 'pr': pr, 'timeout': timeout,\n                  'basic_quorum': basic_quorum,\n                  'notfound_ok': notfound_ok}\n\n        bucket_type = self._get_bucket_type(robj.bucket.bucket_type)\n\n        url = self.object_path(robj.bucket.name, robj.key,\n                               bucket_type=bucket_type, **params)\n        response = self._request('GET', url)\n        return self._parse_body(robj, response, [200, 300, 404])", "response": "Get a specific object from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nput an object in the object store.", "response": "def put(self, robj, w=None, dw=None, pw=None, return_body=True,\n            if_none_match=False, timeout=None):\n        \"\"\"\n        Puts a (possibly new) object.\n        \"\"\"\n        # We could detect quorum_controls here but HTTP ignores\n        # unknown flags/params.\n        params = {'returnbody': return_body, 'w': w, 'dw': dw, 'pw': pw,\n                  'timeout': timeout}\n\n        bucket_type = self._get_bucket_type(robj.bucket.bucket_type)\n\n        url = self.object_path(robj.bucket.name, robj.key,\n                               bucket_type=bucket_type,\n                               **params)\n        headers = self._build_put_headers(robj, if_none_match=if_none_match)\n        if PY2:\n            content = bytearray(robj.encoded_data)\n        else:\n            content = robj.encoded_data\n\n        if robj.key is None:\n            expect = [201]\n            method = 'POST'\n        else:\n            expect = [204]\n            method = 'PUT'\n\n        response = self._request(method, url, headers, content)\n        if return_body:\n            return self._parse_body(robj, response, [200, 201, 204, 300])\n        else:\n            self.check_http_code(response[0], expect)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches a list of keys for the bucket.", "response": "def get_keys(self, bucket, timeout=None):\n        \"\"\"\n        Fetch a list of keys for the bucket\n        \"\"\"\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.key_list_path(bucket.name, bucket_type=bucket_type,\n                                 timeout=timeout)\n        status, _, body = self._request('GET', url)\n\n        if status == 200:\n            props = json.loads(bytes_to_str(body))\n            return props['keys']\n        else:\n            raise RiakError('Error listing keys.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_buckets(self, bucket_type=None, timeout=None):\n        bucket_type = self._get_bucket_type(bucket_type)\n        url = self.bucket_list_path(bucket_type=bucket_type,\n                                    timeout=timeout)\n        status, headers, body = self._request('GET', url)\n\n        if status == 200:\n            props = json.loads(bytes_to_str(body))\n            return props['buckets']\n        else:\n            raise RiakError('Error getting buckets.')", "response": "Fetch a list of all buckets in the current user s account."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stream_buckets(self, bucket_type=None, timeout=None):\n        if not self.bucket_stream():\n            raise NotImplementedError('Streaming list-buckets is not '\n                                      \"supported on %s\" %\n                                      self.server_version.vstring)\n        bucket_type = self._get_bucket_type(bucket_type)\n        url = self.bucket_list_path(bucket_type=bucket_type,\n                                    buckets=\"stream\", timeout=timeout)\n        status, headers, response = self._request('GET', url, stream=True)\n\n        if status == 200:\n            return HttpBucketStream(response)\n        else:\n            raise RiakError('Error listing buckets.')", "response": "Stream list of buckets through an iterator"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bucket_props(self, bucket):\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.bucket_properties_path(bucket.name,\n                                          bucket_type=bucket_type)\n        status, headers, body = self._request('GET', url)\n\n        if status == 200:\n            props = json.loads(bytes_to_str(body))\n            return props['props']\n        else:\n            raise RiakError('Error getting bucket properties.')", "response": "Get properties for a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the properties on the object given", "response": "def set_bucket_props(self, bucket, props):\n        \"\"\"\n        Set the properties on the bucket object given\n        \"\"\"\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.bucket_properties_path(bucket.name,\n                                          bucket_type=bucket_type)\n        headers = {'Content-Type': 'application/json'}\n        content = json.dumps({'props': props})\n\n        # Run the request...\n        status, _, body = self._request('PUT', url, headers, content)\n\n        if status == 401:\n            raise SecurityError('Not authorized to set bucket properties.')\n        elif status != 204:\n            raise RiakError('Error setting bucket properties.')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the properties on the object given", "response": "def clear_bucket_props(self, bucket):\n        \"\"\"\n        reset the properties on the bucket object given\n        \"\"\"\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.bucket_properties_path(bucket.name,\n                                          bucket_type=bucket_type)\n        url = self.bucket_properties_path(bucket.name)\n        headers = {'Content-Type': 'application/json'}\n\n        # Run the request...\n        status, _, _ = self._request('DELETE', url, headers, None)\n\n        if status == 204:\n            return True\n        elif status == 405:\n            return False\n        else:\n            raise RiakError('Error %s clearing bucket properties.'\n                            % status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bucket_type_props(self, bucket_type):\n        self._check_bucket_types(bucket_type)\n        url = self.bucket_type_properties_path(bucket_type.name)\n        status, headers, body = self._request('GET', url)\n\n        if status == 200:\n            props = json.loads(bytes_to_str(body))\n            return props['props']\n        else:\n            raise RiakError('Error getting bucket-type properties.')", "response": "Get properties for a bucket - type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the properties on the bucket - type.", "response": "def set_bucket_type_props(self, bucket_type, props):\n        \"\"\"\n        Set the properties on the bucket-type\n        \"\"\"\n        self._check_bucket_types(bucket_type)\n        url = self.bucket_type_properties_path(bucket_type.name)\n        headers = {'Content-Type': 'application/json'}\n        content = json.dumps({'props': props})\n\n        # Run the request...\n        status, _, _ = self._request('PUT', url, headers, content)\n\n        if status != 204:\n            raise RiakError('Error setting bucket-type properties.')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a MapReduce query.", "response": "def mapred(self, inputs, query, timeout=None):\n        \"\"\"\n        Run a MapReduce query.\n        \"\"\"\n        # Construct the job, optionally set the timeout...\n        content = self._construct_mapred_json(inputs, query, timeout)\n\n        # Do the request...\n        url = self.mapred_path()\n        headers = {'Content-Type': 'application/json'}\n        status, headers, body = self._request('POST', url, headers, content)\n\n        # Make sure the expected status code came back...\n        if status != 200:\n            raise RiakError(\n                'Error running MapReduce operation. Headers: %s Body: %s' %\n                (repr(headers), repr(body)))\n\n        result = json.loads(bytes_to_str(body))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves a secondary index entry.", "response": "def get_index(self, bucket, index, startkey, endkey=None,\n                  return_terms=None, max_results=None, continuation=None,\n                  timeout=None, term_regex=None):\n        \"\"\"\n        Performs a secondary index query.\n        \"\"\"\n        if term_regex and not self.index_term_regex():\n            raise NotImplementedError(\"Secondary index term_regex is not \"\n                                      \"supported on %s\" %\n                                      self.server_version.vstring)\n\n        if timeout == 'infinity':\n            timeout = 0\n\n        params = {'return_terms': return_terms, 'max_results': max_results,\n                  'continuation': continuation, 'timeout': timeout,\n                  'term_regex': term_regex}\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.index_path(bucket.name, index, startkey, endkey,\n                              bucket_type=bucket_type, **params)\n        status, headers, body = self._request('GET', url)\n        self.check_http_code(status, [200])\n        json_data = json.loads(bytes_to_str(body))\n        if return_terms and u'results' in json_data:\n            results = []\n            for result in json_data[u'results'][:]:\n                term, key = list(result.items())[0]\n                results.append((decode_index_value(index, term), key),)\n        else:\n            results = json_data[u'keys'][:]\n\n        if max_results and u'continuation' in json_data:\n            return (results, json_data[u'continuation'])\n        else:\n            return (results, None)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stream_index(self, bucket, index, startkey, endkey=None,\n                     return_terms=None, max_results=None, continuation=None,\n                     timeout=None, term_regex=None):\n        \"\"\"\n        Streams a secondary index query.\n        \"\"\"\n        if not self.stream_indexes():\n            raise NotImplementedError(\"Secondary index streaming is not \"\n                                      \"supported on %s\" %\n                                      self.server_version.vstring)\n\n        if term_regex and not self.index_term_regex():\n            raise NotImplementedError(\"Secondary index term_regex is not \"\n                                      \"supported on %s\" %\n                                      self.server_version.vstring)\n\n        if timeout == 'infinity':\n            timeout = 0\n\n        params = {'return_terms': return_terms, 'stream': True,\n                  'max_results': max_results, 'continuation': continuation,\n                  'timeout': timeout, 'term_regex': term_regex}\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.index_path(bucket.name, index, startkey, endkey,\n                              bucket_type=bucket_type, **params)\n        status, headers, response = self._request('GET', url, stream=True)\n\n        if status == 200:\n            return HttpIndexStream(response, index, return_terms)\n        else:\n            raise RiakError('Error streaming secondary index.')", "response": "Streams a secondary index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_search_index(self, index, schema=None, n_val=None,\n                            timeout=None):\n        \"\"\"\n        Create a Solr search index for Yokozuna.\n\n        :param index: a name of a yz index\n        :type index: string\n        :param schema: XML of Solr schema\n        :type schema: string\n        :param n_val: N value of the write\n        :type n_val: int\n        :param timeout: optional timeout (in ms)\n        :type timeout: integer, None\n\n        :rtype boolean\n        \"\"\"\n        if not self.yz_wm_index:\n            raise NotImplementedError(\"Search 2.0 administration is not \"\n                                      \"supported for this version\")\n\n        url = self.search_index_path(index)\n        headers = {'Content-Type': 'application/json'}\n        content_dict = dict()\n        if schema:\n            content_dict['schema'] = schema\n        if n_val:\n            content_dict['n_val'] = n_val\n        if timeout:\n            content_dict['timeout'] = timeout\n        content = json.dumps(content_dict)\n\n        # Run the request...\n        status, _, _ = self._request('PUT', url, headers, content)\n\n        if status != 204:\n            raise RiakError('Error setting Search 2.0 index.')\n        return True", "response": "Create a Solr search index for Yokozuna."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the specified Solr search index for Yokozuna.", "response": "def get_search_index(self, index):\n        \"\"\"\n        Fetch the specified Solr search index for Yokozuna.\n\n        :param index: a name of a yz index\n        :type index: string\n\n        :rtype string\n        \"\"\"\n        if not self.yz_wm_index:\n            raise NotImplementedError(\"Search 2.0 administration is not \"\n                                      \"supported for this version\")\n\n        url = self.search_index_path(index)\n\n        # Run the request...\n        status, headers, body = self._request('GET', url)\n\n        if status == 200:\n            return json.loads(bytes_to_str(body))\n        else:\n            raise RiakError('Error getting Search 2.0 index.')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_search_indexes(self):\n        if not self.yz_wm_index:\n            raise NotImplementedError(\"Search 2.0 administration is not \"\n                                      \"supported for this version\")\n\n        url = self.search_index_path()\n\n        # Run the request...\n        status, headers, body = self._request('GET', url)\n\n        if status == 200:\n            json_data = json.loads(bytes_to_str(body))\n            # Return a list of dictionaries\n            return json_data\n        else:\n            raise RiakError('Error getting Search 2.0 index.')", "response": "Returns a list of Solr search indexes from Yokozuna."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes the specified Solr search index for Yokozuna.", "response": "def delete_search_index(self, index):\n        \"\"\"\n        Fetch the specified Solr search index for Yokozuna.\n\n        :param index: a name of a yz index\n        :type index: string\n\n        :rtype boolean\n        \"\"\"\n        if not self.yz_wm_index:\n            raise NotImplementedError(\"Search 2.0 administration is not \"\n                                      \"supported for this version\")\n\n        url = self.search_index_path(index)\n\n        # Run the request...\n        status, _, _ = self._request('DELETE', url)\n\n        if status != 204:\n            raise RiakError('Error setting Search 2.0 index.')\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_search_schema(self, schema, content):\n        if not self.yz_wm_schema:\n            raise NotImplementedError(\"Search 2.0 administration is not \"\n                                      \"supported for this version\")\n\n        url = self.search_schema_path(schema)\n        headers = {'Content-Type': 'application/xml'}\n\n        # Run the request...\n        status, header, body = self._request('PUT', url, headers, content)\n\n        if status != 204:\n            raise RiakError('Error creating Search 2.0 schema.')\n        return True", "response": "Create a new Solr schema for Yokozuna."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch a Solr schema from Yokozuna.", "response": "def get_search_schema(self, schema):\n        \"\"\"\n        Fetch a Solr schema from Yokozuna.\n\n        :param schema: name of Solr schema\n        :type schema: string\n\n        :rtype dict\n        \"\"\"\n        if not self.yz_wm_schema:\n            raise NotImplementedError(\"Search 2.0 administration is not \"\n                                      \"supported for this version\")\n        url = self.search_schema_path(schema)\n\n        # Run the request...\n        status, _, body = self._request('GET', url)\n\n        if status == 200:\n            result = {}\n            result['name'] = schema\n            result['content'] = bytes_to_str(body)\n            return result\n        else:\n            raise RiakError('Error getting Search 2.0 schema.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, index, query, **params):\n        if index is None:\n            index = 'search'\n\n        options = {}\n        if 'op' in params:\n            op = params.pop('op')\n            options['q.op'] = op\n\n        options.update(params)\n        url = self.solr_select_path(index, query, **options)\n        status, headers, data = self._request('GET', url)\n        self.check_http_code(status, [200])\n        if 'json' in headers['content-type']:\n            results = json.loads(bytes_to_str(data))\n            return self._normalize_json_search_response(results)\n        elif 'xml' in headers['content-type']:\n            return self._normalize_xml_search_response(data)\n        else:\n            raise ValueError(\"Could not decode search response\")", "response": "Perform a search query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd documents to the search index.", "response": "def fulltext_add(self, index, docs):\n        \"\"\"\n        Adds documents to the search index.\n        \"\"\"\n        xml = Document()\n        root = xml.createElement('add')\n        for doc in docs:\n            doc_element = xml.createElement('doc')\n            for key in doc:\n                value = doc[key]\n                field = xml.createElement('field')\n                field.setAttribute(\"name\", key)\n                text = xml.createTextNode(value)\n                field.appendChild(text)\n                doc_element.appendChild(field)\n            root.appendChild(doc_element)\n        xml.appendChild(root)\n\n        self._request('POST', self.solr_update_path(index),\n                      {'Content-Type': 'text/xml'},\n                      xml.toxml().encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves documents from the full - text index.", "response": "def fulltext_delete(self, index, docs=None, queries=None):\n        \"\"\"\n        Removes documents from the full-text index.\n        \"\"\"\n        xml = Document()\n        root = xml.createElement('delete')\n        if docs:\n            for doc in docs:\n                doc_element = xml.createElement('id')\n                text = xml.createTextNode(doc)\n                doc_element.appendChild(text)\n                root.appendChild(doc_element)\n        if queries:\n            for query in queries:\n                query_element = xml.createElement('query')\n                text = xml.createTextNode(query)\n                query_element.appendChild(text)\n                root.appendChild(query_element)\n\n        xml.appendChild(root)\n\n        self._request('POST', self.solr_update_path(index),\n                      {'Content-Type': 'text/xml'},\n                      xml.toxml().encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_preflist(self, bucket, key):\n        if not self.preflists():\n            raise NotImplementedError(\"fetching preflists is not supported.\")\n        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.preflist_path(bucket.name, key, bucket_type=bucket_type)\n        status, headers, body = self._request('GET', url)\n\n        if status == 200:\n            preflist = json.loads(bytes_to_str(body))\n            return preflist['preflist']\n        else:\n            raise RiakError('Error getting bucket/key preflist.')", "response": "Get the preflist for a bucket and key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrelease this resource back to the pool it came from.", "response": "def release(self):\n        \"\"\"\n        Releases this resource back to the pool it came from.\n        \"\"\"\n        if self.errored:\n            self.pool.delete_resource(self)\n        else:\n            self.pool.release(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a resource that is claimed by the pool.", "response": "def acquire(self, _filter=None, default=None):\n        \"\"\"\n        acquire(_filter=None, default=None)\n\n        Claims a resource from the pool for manual use. Resources are\n        created as needed when all members of the pool are claimed or\n        the pool is empty. Most of the time you will want to use\n        :meth:`transaction`.\n\n        :param _filter: a filter that can be used to select a member\n            of the pool\n        :type _filter: callable\n        :param default: a value that will be used instead of calling\n            :meth:`create_resource` if a new resource needs to be created\n        :rtype: Resource\n        \"\"\"\n        if not _filter:\n            def _filter(obj):\n                return True\n        elif not callable(_filter):\n            raise TypeError(\"_filter is not a callable\")\n\n        resource = None\n        with self.lock:\n            for e in self.resources:\n                if not e.claimed and _filter(e.object):\n                    resource = e\n                    break\n            if resource is None:\n                if default is not None:\n                    resource = Resource(default, self)\n                else:\n                    resource = Resource(self.create_resource(), self)\n                self.resources.append(resource)\n            resource.claimed = True\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrelease a resource from the pool.", "response": "def release(self, resource):\n        \"\"\"release(resource)\n\n        Returns a resource to the pool. Most of the time you will want\n        to use :meth:`transaction`, but if you use :meth:`acquire`,\n        you must release the acquired resource back to the pool when\n        finished. Failure to do so could result in deadlock.\n\n        :param resource: Resource\n        \"\"\"\n        with self.releaser:\n            resource.claimed = False\n            self.releaser.notify_all()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transaction(self, _filter=None, default=None, yield_resource=False):\n        resource = self.acquire(_filter=_filter, default=default)\n        try:\n            if yield_resource:\n                yield resource\n            else:\n                yield resource.object\n            if resource.errored:\n                self.delete_resource(resource)\n        except BadResource:\n            self.delete_resource(resource)\n            raise\n        finally:\n            self.release(resource)", "response": "This method creates a new resource in the pool for use in a thread - safe manner."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the resource from the pool and destroys the associated resource.", "response": "def delete_resource(self, resource):\n        \"\"\"\n        Deletes the resource from the pool and destroys the associated\n        resource. Not usually needed by users of the pool, but called\n        internally when BadResource is raised.\n\n        :param resource: the resource to remove\n        :type resource: Resource\n        \"\"\"\n        with self.lock:\n            self.resources.remove(resource)\n        self.destroy_resource(resource.object)\n        del resource"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_timeseries_put(self, tsobj):\n        '''\n        Returns an Erlang-TTB encoded tuple with the appropriate data and\n        metadata from a TsObject.\n\n        :param tsobj: a TsObject\n        :type tsobj: TsObject\n        :rtype: term-to-binary encoded object\n        '''\n        if tsobj.columns:\n            raise NotImplementedError('columns are not used')\n\n        if tsobj.rows and isinstance(tsobj.rows, list):\n            req_rows = []\n            for row in tsobj.rows:\n                req_r = []\n                for cell in row:\n                    req_r.append(self.encode_to_ts_cell(cell))\n                req_rows.append(tuple(req_r))\n            req = tsputreq_a, tsobj.table.name, [], req_rows\n            mc = MSG_CODE_TS_TTB_MSG\n            rc = MSG_CODE_TS_TTB_MSG\n            return Msg(mc, encode(req), rc)\n        else:\n            raise RiakError(\"TsObject requires a list of rows\")", "response": "Encodes a TsObject into an Erlang - TTB encoded tuple with the appropriate data and\n            metadata from a TsObject."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_timeseries(self, resp_ttb, tsobj,\n                          convert_timestamp=False):\n        \"\"\"\n        Fills an TsObject with the appropriate data and\n        metadata from a TTB-encoded TsGetResp / TsQueryResp.\n\n        :param resp_ttb: the decoded TTB data\n        :type resp_ttb: TTB-encoded tsqueryrsp or tsgetresp\n        :param tsobj: a TsObject\n        :type tsobj: TsObject\n        :param convert_timestamp: Convert timestamps to datetime objects\n        :type tsobj: boolean\n        \"\"\"\n        if resp_ttb is None:\n            return tsobj\n\n        self.maybe_err_ttb(resp_ttb)\n\n        # NB: some queries return a BARE 'tsqueryresp' atom\n        # catch that here:\n        if resp_ttb == tsqueryresp_a:\n            return tsobj\n\n        # The response atom is the first element in the response tuple\n        resp_a = resp_ttb[0]\n        if resp_a == tsputresp_a:\n            return\n        elif resp_a == tsgetresp_a or resp_a == tsqueryresp_a:\n            resp_data = resp_ttb[1]\n            if len(resp_data) == 0:\n                return\n            elif len(resp_data) == 3:\n                resp_colnames = resp_data[0]\n                resp_coltypes = resp_data[1]\n                tsobj.columns = self.decode_timeseries_cols(\n                        resp_colnames, resp_coltypes)\n                resp_rows = resp_data[2]\n                tsobj.rows = []\n                for resp_row in resp_rows:\n                    tsobj.rows.append(\n                        self.decode_timeseries_row(resp_row, resp_coltypes,\n                                                   convert_timestamp))\n            else:\n                raise RiakError(\n                    \"Expected 3-tuple in response, got: {}\".format(resp_data))\n        else:\n            raise RiakError(\"Unknown TTB response type: {}\".format(resp_a))", "response": "Decodes a TTB - encoded TsGetResp or TsQueryResp response and returns a TsObject containing the appropriate data and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecoding a TTB - encoded TsRow into a list of lists of the appropriate type.", "response": "def decode_timeseries_row(self, tsrow, tsct, convert_timestamp=False):\n        \"\"\"\n        Decodes a TTB-encoded TsRow into a list\n\n        :param tsrow: the TTB decoded TsRow to decode.\n        :type tsrow: TTB dncoded row\n        :param tsct: the TTB decoded column types (atoms).\n        :type tsct: list\n        :param convert_timestamp: Convert timestamps to datetime objects\n        :type tsobj: boolean\n        :rtype list\n        \"\"\"\n        row = []\n        for i, cell in enumerate(tsrow):\n            if cell is None:\n                row.append(None)\n            elif isinstance(cell, list) and len(cell) == 0:\n                row.append(None)\n            else:\n                if convert_timestamp and tsct[i] == timestamp_a:\n                    row.append(datetime_from_unix_time_millis(cell))\n                else:\n                    row.append(cell)\n        return row"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary containing the modification operations that are set in the current set.", "response": "def to_op(self):\n        \"\"\"\n        Extracts the modification operation from the set.\n\n        :rtype: dict, None\n        \"\"\"\n        if not self._adds and not self._removes:\n            return None\n        changes = {}\n        if self._adds:\n            changes['adds'] = list(self._adds)\n        if self._removes:\n            changes['removes'] = list(self._removes)\n        return changes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discard(self, element):\n        _check_element(element)\n        self._require_context()\n        self._removes.add(element)", "response": "Removes an element from the set."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all values matching the key", "response": "def getall(self, key):\n        \"\"\"\n        Return a list of all values matching the key (may be an empty list)\n        \"\"\"\n        result = []\n        for k, v in self._items:\n            if key == k:\n                result.append(v)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getone(self, key):\n        v = self.getall(key)\n        if not v:\n            raise KeyError('Key not found: %r' % key)\n        if len(v) > 1:\n            raise KeyError('Multiple values match %r: %r' % (key, v))\n        return v[0]", "response": "Get one value matching the key raising a KeyError if multiple values were found."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary where the values are either single values or a list of values when a key or value appears more than once in this dictionary.", "response": "def mixed(self):\n        \"\"\"\n        Returns a dictionary where the values are either single\n        values, or a list of values when a key/value appears more than\n        once in this dictionary.  This is similar to the kind of\n        dictionary often used to represent the variables in a web\n        request.\n        \"\"\"\n        result = {}\n        multi = {}\n        for key, value in self._items:\n            if key in result:\n                # We do this to not clobber any lists that are\n                # *actual* values in this dictionary:\n                if key in multi:\n                    result[key].append(value)\n                else:\n                    result[key] = [result[key], value]\n                    multi[key] = None\n            else:\n                result[key] = value\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary where each key is associated with a list of values.", "response": "def dict_of_lists(self):\n        \"\"\"\n        Returns a dictionary where each key is associated with a\n        list of values.\n        \"\"\"\n        result = {}\n        for key, value in self._items:\n            if key in result:\n                result[key].append(value)\n            else:\n                result[key] = [value]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef multiget(client, keys, **options):\n    transient_pool = False\n    outq = Queue()\n\n    if 'pool' in options:\n        pool = options['pool']\n        del options['pool']\n    else:\n        pool = MultiGetPool()\n        transient_pool = True\n\n    try:\n        pool.start()\n        for bucket_type, bucket, key in keys:\n            task = Task(client, outq, bucket_type, bucket, key, None, options)\n            pool.enq(task)\n\n        results = []\n        for _ in range(len(keys)):\n            if pool.stopped():\n                raise RuntimeError(\n                        'Multi-get operation interrupted by pool '\n                        'stopping!')\n            results.append(outq.get())\n            outq.task_done()\n    finally:\n        if transient_pool:\n            pool.stop()\n\n    return results", "response": "Executes a parallel - fetch across multiple threads."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute a parallel - store across multiple threads.", "response": "def multiput(client, objs, **options):\n    \"\"\"Executes a parallel-store across multiple threads. Returns a list\n    containing booleans or :class:`~riak.riak_object.RiakObject`\n\n    If a ``pool`` option is included, the request will use the given worker\n    pool and not a transient :class:`~riak.client.multi.MultiPutPool`. This\n    option will be passed by the client if the ``multiput_pool_size``\n    option was set on client initialization.\n\n    :param client: the client to use\n    :type client: :class:`RiakClient <riak.client.RiakClient>`\n    :param objs: the objects to store in parallel\n    :type objs: list of `RiakObject <riak.riak_object.RiakObject>` or\n                `TsObject <riak.ts_object.TsObject>`\n    :param options: request options to\n        :meth:`RiakClient.put <riak.client.RiakClient.put>`\n    :type options: dict\n    :rtype: list\n    \"\"\"\n    transient_pool = False\n    outq = Queue()\n\n    if 'pool' in options:\n        pool = options['pool']\n        del options['pool']\n    else:\n        pool = MultiPutPool()\n        transient_pool = True\n\n    try:\n        pool.start()\n        for obj in objs:\n            task = PutTask(client, outq, obj, options)\n            pool.enq(task)\n\n        results = []\n        for _ in range(len(objs)):\n            if pool.stopped():\n                raise RuntimeError(\n                        'Multi-put operation interrupted by pool '\n                        'stopping!')\n            results.append(outq.get())\n            outq.task_done()\n    finally:\n        if transient_pool:\n            pool.stop()\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enq(self, task):\n        if not self._stop.is_set():\n            self._inq.put(task)\n        else:\n            raise RuntimeError(\"Attempted to enqueue an operation while \"\n                               \"multi pool was shutdown!\")", "response": "Enqueues a fetch task to the pool of workers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the worker threads if they are not already started.", "response": "def start(self):\n        \"\"\"\n        Starts the worker threads if they are not already started.\n        This method is thread-safe and will be called automatically\n        when executing an operation.\n        \"\"\"\n        # Check whether we are already started, skip if we are.\n        if not self._started.is_set():\n            # If we are not started, try to capture the lock.\n            if self._lock.acquire(False):\n                # If we got the lock, go ahead and start the worker\n                # threads, set the started flag, and release the lock.\n                for i in range(self._size):\n                    name = \"riak.client.multi-worker-{0}-{1}\".format(\n                            self._name, i)\n                    worker = Thread(target=self._worker_method, name=name)\n                    worker.daemon = False\n                    worker.start()\n                    self._workers.append(worker)\n                self._started.set()\n                self._lock.release()\n            else:\n                # We didn't get the lock, so someone else is already\n                # starting the worker threads. Wait until they have\n                # signaled that the threads are started.\n                self._started.wait()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops all the worker threads and waits on them.", "response": "def stop(self):\n        \"\"\"\n        Signals the worker threads to exit and waits on them.\n        \"\"\"\n        if not self.stopped():\n            self._stop.set()\n            for worker in self._workers:\n                worker.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _worker_method(self):\n        while not self._should_quit():\n            try:\n                task = self._inq.get(block=True, timeout=0.25)\n            except TypeError:\n                if self._should_quit():\n                    break\n                else:\n                    raise\n            except Empty:\n                continue\n\n            try:\n                btype = task.client.bucket_type(task.bucket_type)\n                obj = btype.bucket(task.bucket).get(task.key, **task.options)\n                task.outq.put(obj)\n            except KeyboardInterrupt:\n                raise\n            except Exception as err:\n                errdata = (task.bucket_type, task.bucket, task.key, err)\n                task.outq.put(errdata)\n            finally:\n                self._inq.task_done()", "response": "The body of the multi - get worker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _worker_method(self):\n        while not self._should_quit():\n            try:\n                task = self._inq.get(block=True, timeout=0.25)\n            except TypeError:\n                if self._should_quit():\n                    break\n                else:\n                    raise\n            except Empty:\n                continue\n\n            try:\n                obj = task.object\n                if isinstance(obj, RiakObject):\n                    rv = task.client.put(obj, **task.options)\n                elif isinstance(obj, TsObject):\n                    rv = task.client.ts_put(obj, **task.options)\n                else:\n                    raise ValueError('unknown obj type: %s'.format(type(obj)))\n                task.outq.put(rv)\n            except KeyboardInterrupt:\n                raise\n            except Exception as err:\n                errdata = (task.object, err)\n                task.outq.put(errdata)\n            finally:\n                self._inq.task_done()", "response": "The body of the multi - put worker."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that a key is well - formed and raises a TypeError if not.", "response": "def _check_key(self, key):\n        \"\"\"\n        Ensures well-formedness of a key.\n        \"\"\"\n        if not len(key) == 2:\n            raise TypeError('invalid key: %r' % key)\n        elif key[1] not in TYPES:\n            raise TypeError('invalid datatype: %s' % key[1])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef value(self):\n        pvalue = {}\n        for key in self._value:\n            pvalue[key] = self._value[key].value\n        return pvalue", "response": "Returns a copy of the original map s value. Nested values are\n        pure Python values as returned by : attr : Datatype. value from\n        the nested types."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modified(self):\n        if self._removes:\n            return True\n        for v in self._value:\n            if self._value[v].modified:\n                return True\n        for v in self._updates:\n            if self._updates[v].modified:\n                return True\n        return False", "response": "Returns True if the map has staged local modifications."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_op(self):\n        removes = [('remove', r) for r in self._removes]\n        value_updates = list(self._extract_updates(self._value))\n        new_updates = list(self._extract_updates(self._updates))\n        all_updates = removes + value_updates + new_updates\n        if all_updates:\n            return all_updates\n        else:\n            return None", "response": "Extracts the modification operation from the map."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _format_python2_or_3(self):\n        pb_files = set()\n        with open(self.source, 'r', buffering=1) as csvfile:\n            reader = csv.reader(csvfile)\n            for row in reader:\n                _, _, proto = row\n                pb_files.add('riak/pb/{0}_pb2.py'.format(proto))\n\n        for im in sorted(pb_files):\n            with open(im, 'r', buffering=1) as pbfile:\n                contents = 'from six import *\\n' + pbfile.read()\n                contents = re.sub(r'riak_pb2',\n                                  r'riak.pb.riak_pb2',\n                                  contents)\n            # Look for this pattern in the protoc-generated file:\n            #\n            # class RpbCounterGetResp(_message.Message):\n            #    __metaclass__ = _reflection.GeneratedProtocolMessageType\n            #\n            # and convert it to:\n            #\n            # @add_metaclass(_reflection.GeneratedProtocolMessageType)\n            # class RpbCounterGetResp(_message.Message):\n            contents = re.sub(\n                r'class\\s+(\\S+)\\((\\S+)\\):\\s*\\n'\n                '\\s+__metaclass__\\s+=\\s+(\\S+)\\s*\\n',\n                r'@add_metaclass(\\3)\\nclass \\1(\\2):\\n', contents)\n\n            with open(im, 'w', buffering=1) as pbfile:\n                pbfile.write(contents)", "response": "Format the PB files to be Python 2 or Python 3. x\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreload the datatype from Riak.", "response": "def reload(self, **params):\n        \"\"\"\n        Reloads the datatype from Riak.\n\n        .. warning: This clears any local modifications you might have\n           made.\n\n        :param r: the read quorum\n        :type r: integer, string, None\n        :param pr: the primary read quorum\n        :type pr: integer, string, None\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param include_context: whether to return the opaque context\n          as well as the value, which is useful for removal operations\n          on sets and maps\n        :type include_context: bool\n        :rtype: :class:`Datatype`\n        \"\"\"\n        if not self.bucket:\n            raise ValueError('bucket property not assigned')\n\n        if not self.key:\n            raise ValueError('key property not assigned')\n\n        dtype, value, context = self.bucket._client._fetch_datatype(\n            self.bucket, self.key, **params)\n\n        if not dtype == self.type_name:\n            raise TypeError(\"Expected datatype {} but \"\n                            \"got datatype {}\".format(self.__class__,\n                                                     TYPES[dtype]))\n\n        self.clear()\n        self._context = context\n        self._set_value(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, **params):\n        self.clear()\n        self._context = None\n        self._set_value(self._default_value())\n        self.bucket._client.delete(self, **params)\n        return self", "response": "Deletes the datatype from Riak."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, **params):\n        if not self.modified:\n            raise ValueError(\"No operation to perform\")\n\n        params.setdefault('return_body', True)\n        self.bucket._client.update_datatype(self, **params)\n        self.clear()\n\n        return self", "response": "Send locally staged mutations to Riak."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_quorum(self, rw):\n        if rw in QUORUM_TO_PB:\n            return QUORUM_TO_PB[rw]\n        elif type(rw) is int and rw >= 0:\n            return rw\n        else:\n            return None", "response": "Converts a symbolic quorum value into its on - the - wire equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_contents(self, contents, obj):\n        obj.siblings = [self.decode_content(c, RiakContent(obj))\n                        for c in contents]\n        # Invoke sibling-resolution logic\n        if len(obj.siblings) > 1 and obj.resolver is not None:\n            obj.resolver(obj)\n        return obj", "response": "Decodes the list of siblings from the protobuf representation\n        into the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_content(self, rpb_content, sibling):\n\n        if rpb_content.HasField(\"deleted\") and rpb_content.deleted:\n            sibling.exists = False\n        else:\n            sibling.exists = True\n        if rpb_content.HasField(\"content_type\"):\n            sibling.content_type = bytes_to_str(rpb_content.content_type)\n        if rpb_content.HasField(\"charset\"):\n            sibling.charset = bytes_to_str(rpb_content.charset)\n        if rpb_content.HasField(\"content_encoding\"):\n            sibling.content_encoding = \\\n                bytes_to_str(rpb_content.content_encoding)\n        if rpb_content.HasField(\"vtag\"):\n            sibling.etag = bytes_to_str(rpb_content.vtag)\n\n        sibling.links = [self.decode_link(link)\n                         for link in rpb_content.links]\n        if rpb_content.HasField(\"last_mod\"):\n            sibling.last_modified = float(rpb_content.last_mod)\n            if rpb_content.HasField(\"last_mod_usecs\"):\n                sibling.last_modified += rpb_content.last_mod_usecs / 1000000.0\n\n        sibling.usermeta = dict([(bytes_to_str(usermd.key),\n                                  bytes_to_str(usermd.value))\n                                 for usermd in rpb_content.usermeta])\n        sibling.indexes = set([(bytes_to_str(index.key),\n                                decode_index_value(index.key, index.value))\n                               for index in rpb_content.indexes])\n        sibling.encoded_data = rpb_content.value\n\n        return sibling", "response": "Decodes a single RiakObject into a single RiakObject."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode_content(self, robj, rpb_content):\n        if robj.content_type:\n            rpb_content.content_type = str_to_bytes(robj.content_type)\n        if robj.charset:\n            rpb_content.charset = str_to_bytes(robj.charset)\n        if robj.content_encoding:\n            rpb_content.content_encoding = str_to_bytes(robj.content_encoding)\n        for uk in robj.usermeta:\n            pair = rpb_content.usermeta.add()\n            pair.key = str_to_bytes(uk)\n            pair.value = str_to_bytes(robj.usermeta[uk])\n        for link in robj.links:\n            pb_link = rpb_content.links.add()\n            try:\n                bucket, key, tag = link\n            except ValueError:\n                raise RiakError(\"Invalid link tuple %s\" % link)\n\n            pb_link.bucket = str_to_bytes(bucket)\n            pb_link.key = str_to_bytes(key)\n            if tag:\n                pb_link.tag = str_to_bytes(tag)\n            else:\n                pb_link.tag = str_to_bytes('')\n\n        for field, value in robj.indexes:\n            pair = rpb_content.indexes.add()\n            pair.key = str_to_bytes(field)\n            pair.value = str_to_bytes(str(value))\n\n        # Python 2.x data is stored in a string\n        if six.PY2:\n            rpb_content.value = str(robj.encoded_data)\n        else:\n            rpb_content.value = robj.encoded_data", "response": "Encodes the appropriate data and metadata from a RiakObject into a protobuf message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecode an RpbLink message into a tuple containing bucket key and tag.", "response": "def decode_link(self, link):\n        \"\"\"\n        Decodes an RpbLink message into a tuple\n\n        :param link: an RpbLink message\n        :type link: riak.pb.riak_pb2.RpbLink\n        :rtype tuple\n        \"\"\"\n\n        if link.HasField(\"bucket\"):\n            bucket = bytes_to_str(link.bucket)\n        else:\n            bucket = None\n        if link.HasField(\"key\"):\n            key = bytes_to_str(link.key)\n        else:\n            key = None\n        if link.HasField(\"tag\"):\n            tag = bytes_to_str(link.tag)\n        else:\n            tag = None\n\n        return (bucket, key, tag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes a secondary index value into the correct Python type.", "response": "def decode_index_value(self, index, value):\n        \"\"\"\n        Decodes a secondary index value into the correct Python type.\n        :param index: the name of the index\n        :type index: str\n        :param value: the value of the index entry\n        :type  value: str\n        :rtype str or int\n        \"\"\"\n        if index.endswith(\"_int\"):\n            return int(value)\n        else:\n            return bytes_to_str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_bucket_props(self, props, msg):\n        for prop in NORMAL_PROPS:\n            if prop in props and props[prop] is not None:\n                if isinstance(props[prop], six.string_types):\n                    setattr(msg.props, prop, str_to_bytes(props[prop]))\n                else:\n                    setattr(msg.props, prop, props[prop])\n        for prop in COMMIT_HOOK_PROPS:\n            if prop in props:\n                setattr(msg.props, 'has_' + prop, True)\n                self.encode_hooklist(props[prop], getattr(msg.props, prop))\n        for prop in MODFUN_PROPS:\n            if prop in props and props[prop] is not None:\n                self.encode_modfun(props[prop], getattr(msg.props, prop))\n        for prop in QUORUM_PROPS:\n            if prop in props and props[prop] not in (None, 'default'):\n                value = self.encode_quorum(props[prop])\n                if value is not None:\n                    if isinstance(value, six.string_types):\n                        setattr(msg.props, prop, str_to_bytes(value))\n                    else:\n                        setattr(msg.props, prop, value)\n        if 'repl' in props:\n            msg.props.repl = REPL_TO_PB[props['repl']]\n\n        return msg", "response": "Encodes a dictionary of bucket properties into the protobuf message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecode the protobuf bucket properties message into a dict.", "response": "def decode_bucket_props(self, msg):\n        \"\"\"\n        Decodes the protobuf bucket properties message into a dict.\n\n        :param msg: the protobuf message to decode\n        :type msg: riak.pb.riak_pb2.RpbBucketProps\n        :rtype dict\n        \"\"\"\n        props = {}\n        for prop in NORMAL_PROPS:\n            if msg.HasField(prop):\n                props[prop] = getattr(msg, prop)\n                if isinstance(props[prop], bytes):\n                    props[prop] = bytes_to_str(props[prop])\n        for prop in COMMIT_HOOK_PROPS:\n            if getattr(msg, 'has_' + prop):\n                props[prop] = self.decode_hooklist(getattr(msg, prop))\n        for prop in MODFUN_PROPS:\n            if msg.HasField(prop):\n                props[prop] = self.decode_modfun(getattr(msg, prop))\n        for prop in QUORUM_PROPS:\n            if msg.HasField(prop):\n                props[prop] = self.decode_quorum(getattr(msg, prop))\n        if msg.HasField('repl'):\n            props['repl'] = REPL_TO_PY[msg.repl]\n        return props"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nencoding a dict with mod and fun keys into a protobuf modfun message.", "response": "def encode_modfun(self, props, msg=None):\n        \"\"\"\n        Encodes a dict with 'mod' and 'fun' keys into a protobuf\n        modfun pair. Used in bucket properties.\n\n        :param props: the module/function pair\n        :type props: dict\n        :param msg: the protobuf message to fill\n        :type msg: riak.pb.riak_pb2.RpbModFun\n        :rtype riak.pb.riak_pb2.RpbModFun\n        \"\"\"\n        if msg is None:\n            msg = riak.pb.riak_pb2.RpbModFun()\n        msg.module = str_to_bytes(props['mod'])\n        msg.function = str_to_bytes(props['fun'])\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nencodes a list of commit hooks into their protobuf equivalent. Used in bucket properties.", "response": "def encode_hooklist(self, hooklist, msg):\n        \"\"\"\n        Encodes a list of commit hooks into their protobuf equivalent.\n        Used in bucket properties.\n\n        :param hooklist: a list of commit hooks\n        :type hooklist: list\n        :param msg: a protobuf field that is a list of commit hooks\n        \"\"\"\n        for hook in hooklist:\n            pbhook = msg.add()\n            self.encode_hook(hook, pbhook)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_hook(self, hook):\n        if hook.HasField('modfun'):\n            return self.decode_modfun(hook.modfun)\n        else:\n            return {'name': bytes_to_str(hook.name)}", "response": "Decodes a protobuf commit hook message into a dict. Used in\n        bucket properties."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding a commit hook dict into the protobuf message. Used in bucket properties.", "response": "def encode_hook(self, hook, msg):\n        \"\"\"\n        Encodes a commit hook dict into the protobuf message. Used in\n        bucket properties.\n\n        :param hook: the hook to encode\n        :type hook: dict\n        :param msg: the protobuf message to fill\n        :type msg: riak.pb.riak_pb2.RpbCommitHook\n        :rtype riak.pb.riak_pb2.RpbCommitHook\n        \"\"\"\n        if 'name' in hook:\n            msg.name = str_to_bytes(hook['name'])\n        else:\n            self.encode_modfun(hook, msg.modfun)\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nencode a secondary index request into a protobuf message.", "response": "def encode_index_req(self, bucket, index, startkey, endkey=None,\n                         return_terms=None, max_results=None,\n                         continuation=None, timeout=None, term_regex=None,\n                         streaming=False):\n        \"\"\"\n        Encodes a secondary index request into the protobuf message.\n\n        :param bucket: the bucket whose index to query\n        :type bucket: string\n        :param index: the index to query\n        :type index: string\n        :param startkey: the value or beginning of the range\n        :type startkey: integer, string\n        :param endkey: the end of the range\n        :type endkey: integer, string\n        :param return_terms: whether to return the index term with the key\n        :type return_terms: bool\n        :param max_results: the maximum number of results to return (page size)\n        :type max_results: integer\n        :param continuation: the opaque continuation returned from a\n            previous paginated request\n        :type continuation: string\n        :param timeout: a timeout value in milliseconds, or 'infinity'\n        :type timeout: int\n        :param term_regex: a regular expression used to filter index terms\n        :type term_regex: string\n        :param streaming: encode as streaming request\n        :type streaming: bool\n        :rtype riak.pb.riak_kv_pb2.RpbIndexReq\n        \"\"\"\n        req = riak.pb.riak_kv_pb2.RpbIndexReq(\n            bucket=str_to_bytes(bucket.name),\n            index=str_to_bytes(index))\n        self._add_bucket_type(req, bucket.bucket_type)\n        if endkey is not None:\n            req.qtype = riak.pb.riak_kv_pb2.RpbIndexReq.range\n            req.range_min = str_to_bytes(str(startkey))\n            req.range_max = str_to_bytes(str(endkey))\n        else:\n            req.qtype = riak.pb.riak_kv_pb2.RpbIndexReq.eq\n            req.key = str_to_bytes(str(startkey))\n        if return_terms is not None:\n            req.return_terms = return_terms\n        if max_results:\n            req.max_results = max_results\n        if continuation:\n            req.continuation = str_to_bytes(continuation)\n        if timeout:\n            if timeout == 'infinity':\n                req.timeout = 0\n            else:\n                req.timeout = timeout\n        if term_regex:\n            req.term_regex = str_to_bytes(term_regex)\n        req.stream = streaming\n        mc = riak.pb.messages.MSG_CODE_INDEX_REQ\n        rc = riak.pb.messages.MSG_CODE_INDEX_RESP\n        return Msg(mc, req.SerializeToString(), rc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_search_index(self, index):\n        result = {}\n        result['name'] = bytes_to_str(index.name)\n        if index.HasField('schema'):\n            result['schema'] = bytes_to_str(index.schema)\n        if index.HasField('n_val'):\n            result['n_val'] = index.n_val\n        return result", "response": "Decodes an RpbYokozunaIndex message into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef encode_timeseries_put(self, tsobj):\n        req = riak.pb.riak_ts_pb2.TsPutReq()\n        req.table = str_to_bytes(tsobj.table.name)\n\n        if tsobj.columns:\n            raise NotImplementedError(\"columns are not implemented yet\")\n\n        if tsobj.rows and isinstance(tsobj.rows, list):\n            for row in tsobj.rows:\n                tsr = req.rows.add()  # NB: type TsRow\n                if not isinstance(row, list):\n                    raise ValueError(\"TsObject row must be a list of values\")\n                for cell in row:\n                    tsc = tsr.cells.add()  # NB: type TsCell\n                    self.encode_to_ts_cell(cell, tsc)\n        else:\n            raise RiakError(\"TsObject requires a list of rows\")\n\n        mc = riak.pb.messages.MSG_CODE_TS_PUT_REQ\n        rc = riak.pb.messages.MSG_CODE_TS_PUT_RESP\n        return Msg(mc, req.SerializeToString(), rc)", "response": "Encodes a TsPutReq message with the appropriate data and\n        metadata from a TsObject."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode a TsGetResp message into a TsObject.", "response": "def decode_timeseries(self, resp, tsobj,\n                          convert_timestamp=False):\n        \"\"\"\n        Fills an TsObject with the appropriate data and\n        metadata from a TsGetResp / TsQueryResp.\n\n        :param resp: the protobuf message from which to process data\n        :type resp: riak.pb.riak_ts_pb2.TsQueryRsp or\n                    riak.pb.riak_ts_pb2.TsGetResp\n        :param tsobj: a TsObject\n        :type tsobj: TsObject\n        :param convert_timestamp: Convert timestamps to datetime objects\n        :type tsobj: boolean\n        \"\"\"\n        if resp.columns is not None:\n            col_names = []\n            col_types = []\n            for col in resp.columns:\n                col_names.append(bytes_to_str(col.name))\n                col_type = self.decode_timeseries_col_type(col.type)\n                col_types.append(col_type)\n            tsobj.columns = TsColumns(col_names, col_types)\n\n        tsobj.rows = []\n        if resp.rows is not None:\n            for row in resp.rows:\n                tsobj.rows.append(\n                    self.decode_timeseries_row(\n                        row, resp.columns, convert_timestamp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_timeseries_row(self, tsrow, tscols=None,\n                              convert_timestamp=False):\n        \"\"\"\n        Decodes a TsRow into a list\n\n        :param tsrow: the protobuf TsRow to decode.\n        :type tsrow: riak.pb.riak_ts_pb2.TsRow\n        :param tscols: the protobuf TsColumn data to help decode.\n        :type tscols: list\n        :rtype list\n        \"\"\"\n        row = []\n        for i, cell in enumerate(tsrow.cells):\n            col = None\n            if tscols is not None:\n                col = tscols[i]\n            if cell.HasField('varchar_value'):\n                if col and not (col.type == TsColumnType.Value('VARCHAR') or\n                                col.type == TsColumnType.Value('BLOB')):\n                    raise TypeError('expected VARCHAR or BLOB column')\n                else:\n                    row.append(cell.varchar_value)\n            elif cell.HasField('sint64_value'):\n                if col and col.type != TsColumnType.Value('SINT64'):\n                    raise TypeError('expected SINT64 column')\n                else:\n                    row.append(cell.sint64_value)\n            elif cell.HasField('double_value'):\n                if col and col.type != TsColumnType.Value('DOUBLE'):\n                    raise TypeError('expected DOUBLE column')\n                else:\n                    row.append(cell.double_value)\n            elif cell.HasField('timestamp_value'):\n                if col and col.type != TsColumnType.Value('TIMESTAMP'):\n                    raise TypeError('expected TIMESTAMP column')\n                else:\n                    dt = cell.timestamp_value\n                    if convert_timestamp:\n                        dt = datetime_from_unix_time_millis(\n                            cell.timestamp_value)\n                    row.append(dt)\n            elif cell.HasField('boolean_value'):\n                if col and col.type != TsColumnType.Value('BOOLEAN'):\n                    raise TypeError('expected BOOLEAN column')\n                else:\n                    row.append(cell.boolean_value)\n            else:\n                row.append(None)\n        return row", "response": "Decodes a riak. pb. riak_ts_pb2. TsRow into a list of riak. pb. riak_ts_pb2. TsColumn objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode_preflist(self, item):\n        result = {'partition': item.partition,\n                  'node': bytes_to_str(item.node),\n                  'primary': item. primary}\n        return result", "response": "Decodes a preflist response into a dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nping the remote server and return True if the server is alive False otherwise", "response": "def ping(self):\n        \"\"\"\n        Ping the remote server\n        \"\"\"\n        msg_code = riak.pb.messages.MSG_CODE_PING_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_ping()\n        resp_code, _ = self._request(msg, codec)\n        if resp_code == riak.pb.messages.MSG_CODE_PING_RESP:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_server_info(self):\n        # NB: can't do it this way due to recursion\n        # codec = self._get_codec(ttb_supported=False)\n        codec = PbufCodec()\n        msg = Msg(riak.pb.messages.MSG_CODE_GET_SERVER_INFO_REQ, None,\n                  riak.pb.messages.MSG_CODE_GET_SERVER_INFO_RESP)\n        resp_code, resp = self._request(msg, codec)\n        return codec.decode_get_server_info(resp)", "response": "Get information about the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, robj, r=None, pr=None, timeout=None, basic_quorum=None,\n            notfound_ok=None, head_only=False):\n        \"\"\"\n        Serialize get request and deserialize response\n        \"\"\"\n        msg_code = riak.pb.messages.MSG_CODE_GET_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get(robj, r, pr,\n                               timeout, basic_quorum,\n                               notfound_ok, head_only)\n        resp_code, resp = self._request(msg, codec)\n        return codec.decode_get(robj, resp)", "response": "Serialize get request and deserialize response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ts_stream_keys(self, table, timeout=None):\n        msg_code = riak.pb.messages.MSG_CODE_TS_LIST_KEYS_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_timeseries_listkeysreq(table, timeout)\n        self._send_msg(msg.msg_code, msg.data)\n        return PbufTsKeyStream(self, codec, self._ts_convert_timestamp)", "response": "Streams keys from a timeseries table returning an iterator that yields lists of keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all keys within a bucket.", "response": "def get_keys(self, bucket, timeout=None):\n        \"\"\"\n        Lists all keys within a bucket.\n        \"\"\"\n        msg_code = riak.pb.messages.MSG_CODE_LIST_KEYS_REQ\n        codec = self._get_codec(msg_code)\n        stream = self.stream_keys(bucket, timeout=timeout)\n        return codec.decode_get_keys(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstream keys from a bucket returning an iterator that yields the list of keys.", "response": "def stream_keys(self, bucket, timeout=None):\n        \"\"\"\n        Streams keys from a bucket, returning an iterator that yields\n        lists of keys.\n        \"\"\"\n        msg_code = riak.pb.messages.MSG_CODE_LIST_KEYS_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_stream_keys(bucket, timeout)\n        self._send_msg(msg.msg_code, msg.data)\n        return PbufKeyStream(self, codec)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_buckets(self, bucket_type=None, timeout=None):\n        msg_code = riak.pb.messages.MSG_CODE_LIST_BUCKETS_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_buckets(bucket_type,\n                                       timeout, streaming=False)\n        resp_code, resp = self._request(msg, codec)\n        return resp.buckets", "response": "Get a list of buckets from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stream_buckets(self, bucket_type=None, timeout=None):\n        if not self.bucket_stream():\n            raise NotImplementedError('Streaming list-buckets is not '\n                                      'supported')\n        msg_code = riak.pb.messages.MSG_CODE_LIST_BUCKETS_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_buckets(bucket_type,\n                                       timeout, streaming=True)\n        self._send_msg(msg.msg_code, msg.data)\n        return PbufBucketStream(self, codec)", "response": "Stream list of buckets through an iterator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nserialize bucket property request and deserialize response", "response": "def get_bucket_props(self, bucket):\n        \"\"\"\n        Serialize bucket property request and deserialize response\n        \"\"\"\n        msg_code = riak.pb.messages.MSG_CODE_GET_BUCKET_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_bucket_props(bucket)\n        resp_code, resp = self._request(msg, codec)\n        return codec.decode_bucket_props(resp.props)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing set bucket property request and deserialize response", "response": "def set_bucket_props(self, bucket, props):\n        \"\"\"\n        Serialize set bucket property request and deserialize response\n        \"\"\"\n        if not self.pb_all_bucket_props():\n            for key in props:\n                if key not in ('n_val', 'allow_mult'):\n                    raise NotImplementedError('Server only supports n_val and '\n                                              'allow_mult properties over PBC')\n        msg_code = riak.pb.messages.MSG_CODE_SET_BUCKET_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_set_bucket_props(bucket, props)\n        resp_code, resp = self._request(msg, codec)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears the bucket properties", "response": "def clear_bucket_props(self, bucket):\n        \"\"\"\n        Clear bucket properties, resetting them to their defaults\n        \"\"\"\n        if not self.pb_clear_bucket_props():\n            return False\n        msg_code = riak.pb.messages.MSG_CODE_RESET_BUCKET_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_clear_bucket_props(bucket)\n        self._request(msg, codec)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches bucket - type properties", "response": "def get_bucket_type_props(self, bucket_type):\n        \"\"\"\n        Fetch bucket-type properties\n        \"\"\"\n        self._check_bucket_types(bucket_type)\n        msg_code = riak.pb.messages.MSG_CODE_GET_BUCKET_TYPE_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_bucket_type_props(bucket_type)\n        resp_code, resp = self._request(msg, codec)\n        return codec.decode_bucket_props(resp.props)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset bucket - type properties", "response": "def set_bucket_type_props(self, bucket_type, props):\n        \"\"\"\n        Set bucket-type properties\n        \"\"\"\n        self._check_bucket_types(bucket_type)\n        msg_code = riak.pb.messages.MSG_CODE_SET_BUCKET_TYPE_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_set_bucket_type_props(bucket_type, props)\n        resp_code, resp = self._request(msg, codec)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the preflist for a bucket and key.", "response": "def get_preflist(self, bucket, key):\n        \"\"\"\n        Get the preflist for a bucket/key\n\n        :param bucket: Riak Bucket\n        :type bucket: :class:`~riak.bucket.RiakBucket`\n        :param key: Riak Key\n        :type key: string\n        :rtype: list of dicts\n        \"\"\"\n        if not self.preflists():\n            raise NotImplementedError(\"fetching preflists is not supported.\")\n        msg_code = riak.pb.messages.MSG_CODE_GET_BUCKET_KEY_PREFLIST_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_preflist(bucket, key)\n        resp_code, resp = self._request(msg, codec)\n        return [codec.decode_preflist(item) for item in resp.preflist]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_report(label, user, system, real):\n    print(\"{:<12s} {:12f} {:12f} ( {:12f} )\".format(label,\n                                                    user,\n                                                    system,\n                                                    real))", "response": "Print a report of a benchmark."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the next iteration of the benchmark.", "response": "def next(self):\n        \"\"\"\n        Runs the next iteration of the benchmark.\n        \"\"\"\n        if self.count == 0:\n            raise StopIteration\n        elif self.count > 1:\n            print_rehearsal_header()\n        else:\n            if self.rehearse:\n                gc.collect()\n                print(\"-\" * 59)\n                print()\n            print_header()\n\n        self.count -= 1\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd inputs to a map/reduce operation. This method takes three different forms, depending on the provided inputs. You can specify either a RiakObject, a string bucket name, or a bucket, key, and additional arg. :param arg1: the object or bucket to add :type arg1: RiakObject, string :param arg2: a key or list of keys to add (if a bucket is given in arg1) :type arg2: string, list, None :param arg3: key data for this input (must be convertible to JSON) :type arg3: string, list, dict, None :param bucket_type: Optional name of a bucket type :type bucket_type: string, None :rtype: :class:`RiakMapReduce`", "response": "def add(self, arg1, arg2=None, arg3=None, bucket_type=None):\n        \"\"\"\n        Add inputs to a map/reduce operation. This method takes three\n        different forms, depending on the provided inputs. You can\n        specify either a RiakObject, a string bucket name, or a bucket,\n        key, and additional arg.\n\n        :param arg1: the object or bucket to add\n        :type arg1: RiakObject, string\n        :param arg2: a key or list of keys to add (if a bucket is\n          given in arg1)\n        :type arg2: string, list, None\n        :param arg3: key data for this input (must be convertible to JSON)\n        :type arg3: string, list, dict, None\n        :param bucket_type: Optional name of a bucket type\n        :type bucket_type: string, None\n        :rtype: :class:`RiakMapReduce`\n        \"\"\"\n        from riak.riak_object import RiakObject\n        if (arg2 is None) and (arg3 is None):\n            if isinstance(arg1, RiakObject):\n                return self.add_object(arg1)\n            else:\n                return self.add_bucket(arg1, bucket_type)\n        else:\n            return self.add_bucket_key_data(arg1, arg2, arg3, bucket_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_object(self, obj):\n        return self.add_bucket_key_data(obj._bucket._name, obj._key, None)", "response": "Adds a RiakObject to the inputs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a bucket key - specific data triple to the inputs list.", "response": "def add_bucket_key_data(self, bucket, key, data, bucket_type=None):\n        \"\"\"\n        Adds a bucket/key/keydata triple to the inputs.\n\n        :param bucket: the bucket\n        :type bucket: string\n        :param key: the key or list of keys\n        :type key: string\n        :param data: the key-specific data\n        :type data: string, list, dict, None\n        :param bucket_type: Optional name of a bucket type\n        :type bucket_type: string, None\n        :rtype: :class:`RiakMapReduce`\n        \"\"\"\n        if self._input_mode == 'bucket':\n            raise ValueError('Already added a bucket, can\\'t add an object.')\n        elif self._input_mode == 'query':\n            raise ValueError('Already added a query, can\\'t add an object.')\n        else:\n            if isinstance(key, Iterable) and \\\n                    not isinstance(key, string_types):\n                if bucket_type is not None:\n                    for k in key:\n                        self._inputs.append([bucket, k, data, bucket_type])\n                else:\n                    for k in key:\n                        self._inputs.append([bucket, k, data])\n            else:\n                if bucket_type is not None:\n                    self._inputs.append([bucket, key, data, bucket_type])\n                else:\n                    self._inputs.append([bucket, key, data])\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding all keys in a bucket to the inputs.", "response": "def add_bucket(self, bucket, bucket_type=None):\n        \"\"\"\n        Adds all keys in a bucket to the inputs.\n\n        :param bucket: the bucket\n        :type bucket: string\n        :param bucket_type: Optional name of a bucket type\n        :type bucket_type: string, None\n        :rtype: :class:`RiakMapReduce`\n        \"\"\"\n        if not riak.disable_list_exceptions:\n            raise riak.ListError()\n        self._input_mode = 'bucket'\n        if isinstance(bucket, riak.RiakBucket):\n            if bucket.bucket_type.is_default():\n                self._inputs = {'bucket': bucket.name}\n            else:\n                self._inputs = {'bucket': [bucket.bucket_type.name,\n                                           bucket.name]}\n        elif bucket_type is not None and bucket_type != \"default\":\n            self._inputs = {'bucket': [bucket_type, bucket]}\n        else:\n            self._inputs = {'bucket': bucket}\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_key_filters(self, key_filters):\n        if self._input_mode == 'query':\n            raise ValueError('Key filters are not supported in a query.')\n\n        self._key_filters.extend(key_filters)\n        return self", "response": "Adds key filters to the inputs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_key_filter(self, *args):\n        if self._input_mode == 'query':\n            raise ValueError('Key filters are not supported in a query.')\n\n        self._key_filters.append(args)\n        return self", "response": "Adds a single key filter to the inputs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, index, query):\n        self._input_mode = 'query'\n        self._inputs = {'bucket': index,\n                        'index': index,\n                        'query': query}\n        return self", "response": "This method is used to start a map or reduce operation using a Search. This command will create a new entry in the Solr index and query and then execute the query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(self, bucket, index, startkey, endkey=None, bucket_type=None):\n        self._input_mode = 'query'\n\n        if endkey is None:\n            self._inputs = {'bucket': bucket,\n                            'index': index,\n                            'key': startkey}\n        else:\n            self._inputs = {'bucket': bucket,\n                            'index': index,\n                            'start': startkey,\n                            'end': endkey}\n        if bucket_type is not None:\n            self._inputs['bucket'] = [bucket_type, bucket]\n        return self", "response": "This method is used to perform a secondary index operation on the entry set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef link(self, bucket='_', tag='_', keep=False):\n        self._phases.append(RiakLinkPhase(bucket, tag, keep))\n        return self", "response": "Add a link phase to the map or reduce operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef map(self, function, options=None):\n        if options is None:\n            options = dict()\n        if isinstance(function, list):\n            language = 'erlang'\n        else:\n            language = 'javascript'\n\n        mr = RiakMapReducePhase('map',\n                                function,\n                                options.get('language', language),\n                                options.get('keep', False),\n                                options.get('arg', None))\n        self._phases.append(mr)\n        return self", "response": "Adds a map phase to the operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, timeout=None):\n        query, link_results_flag = self._normalize_query()\n\n        try:\n            result = self._client.mapred(self._inputs, query, timeout)\n        except riak.RiakError as e:\n            if 'worker_startup_failed' in e.value:\n                for phase in self._phases:\n                    if phase._language == 'erlang':\n                        if type(phase._function) is str:\n                            raise riak.RiakError(\n                                    'May have tried erlang strfun '\n                                    'when not allowed\\n'\n                                    'original error: ' + e.value)\n            raise e\n\n        # If the last phase is NOT a link phase, then return the result.\n        if not (link_results_flag or\n                isinstance(self._phases[-1], RiakLinkPhase)):\n            return result\n\n        # If there are no results, then return an empty list.\n        if result is None:\n            return []\n\n        # Otherwise, if the last phase IS a link phase, then convert the\n        # results to link tuples.\n        a = []\n        for r in result:\n            if (len(r) == 2):\n                link = (r[0], r[1], None)\n            elif (len(r) == 3):\n                link = (r[0], r[1], r[2])\n            a.append(link)\n\n        return a", "response": "Runs the map or reduce operation synchronously. Returns a list of links if the last phase is a link phase."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstream the MapReduce query.", "response": "def stream(self, timeout=None):\n        \"\"\"\n        Streams the MapReduce query (returns an iterator). Shortcut\n        for :meth:`riak.client.RiakClient.stream_mapred`.\n\n        :param timeout: Timeout in milliseconds\n        :type timeout: integer\n        :rtype: iterator that yields (phase_num, data) tuples\n        \"\"\"\n        query, lrf = self._normalize_query()\n        return self._client.stream_mapred(self._inputs, query, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce_sort(self, js_cmp=None, options=None):\n        if options is None:\n            options = dict()\n\n        if js_cmp:\n            options['arg'] = js_cmp\n\n        return self.reduce(\"Riak.reduceSort\", options=options)", "response": "Adds the Javascript built - in Riak. reduceSort to the query as a reduce phase."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce_limit(self, limit, options=None):\n        if options is None:\n            options = dict()\n\n        options['arg'] = limit\n        # reduceLimit is broken in riak_kv\n        code = \"\"\"function(value, arg) {\n            return value.slice(0, arg);\n        }\"\"\"\n        return self.reduce(code, options=options)", "response": "Adds the Javascript built - in reduceLimit to the query\nAttributeNames method of the riak client."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reduce_slice(self, start, end, options=None):\n        if options is None:\n            options = dict()\n\n        options['arg'] = [start, end]\n        return self.reduce(\"Riak.reduceSlice\", options=options)", "response": "Adds the Javascript built - in Riak. reduceSlice to the query as a reduce phase."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the RiakMapReducePhase to a format that can be output into JSON. Used internally.", "response": "def to_array(self):\n        \"\"\"\n        Convert the RiakMapReducePhase to a format that can be output\n        into JSON. Used internally.\n\n        :rtype: dict\n        \"\"\"\n        stepdef = {'keep': self._keep,\n                   'language': self._language,\n                   'arg': self._arg}\n\n        if self._language == 'javascript':\n            if isinstance(self._function, list):\n                stepdef['bucket'] = self._function[0]\n                stepdef['key'] = self._function[1]\n            elif isinstance(self._function, string_types):\n                if (\"{\" in self._function):\n                    stepdef['source'] = self._function\n                else:\n                    stepdef['name'] = self._function\n\n        elif (self._language == 'erlang' and isinstance(self._function, list)):\n            stepdef['module'] = self._function[0]\n            stepdef['function'] = self._function[1]\n\n        elif (self._language == 'erlang' and\n              isinstance(self._function, string_types)):\n            stepdef['source'] = self._function\n\n        return {self._type: stepdef}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_array(self):\n        stepdef = {'bucket': self._bucket,\n                   'tag': self._tag,\n                   'keep': self._keep}\n        return {'link': stepdef}", "response": "Convert the RiakLinkPhase to a format that can be output into\n        JSON. Used internally."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassemble a MapReduce operation.", "response": "def add(self, arg1, arg2=None, arg3=None, bucket_type=None):\n        \"\"\"\n        Start assembling a Map/Reduce operation. A shortcut for\n        :func:`RiakMapReduce.add`.\n\n        :param arg1: the object or bucket to add\n        :type arg1: RiakObject, string\n        :param arg2: a key or list of keys to add (if a bucket is\n          given in arg1)\n        :type arg2: string, list, None\n        :param arg3: key data for this input (must be convertible to JSON)\n        :type arg3: string, list, dict, None\n        :param bucket_type: Optional name of a bucket type\n        :type bucket_type: string, None\n        :rtype: :class:`RiakMapReduce`\n        \"\"\"\n        mr = RiakMapReduce(self)\n        return mr.add(arg1, arg2, arg3, bucket_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassembles a MapReduce operation based on secondary index query results.", "response": "def index(self, bucket, index, startkey, endkey=None, bucket_type=None):\n        \"\"\"\n        Start assembling a Map/Reduce operation based on secondary\n        index query results.\n\n        :param bucket: The bucket over which to perform the query\n        :type bucket: string\n        :param index: The index to use for query\n        :type index: string\n        :param startkey: The start key of index range, or the\n           value which all entries must equal\n        :type startkey: string, integer\n        :param endkey: The end key of index range (if doing a range query)\n        :type endkey: string, integer, None\n        :param bucket_type: Optional name of a bucket type\n        :type bucket_type: string, None\n        :rtype: :class:`RiakMapReduce`\n        \"\"\"\n\n        mr = RiakMapReduce(self)\n        return mr.index(bucket, index, startkey, endkey, bucket_type)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_index(self, field, value):\n        if field[-4:] not in (\"_bin\", \"_int\"):\n            raise RiakError(\"Riak 2i fields must end with either '_bin'\"\n                            \" or '_int'.\")\n\n        self.indexes.add((field, value))\n\n        return self._robject", "response": "Add an index to the object with the specified field and value pair for indexing."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves an index from the specified field and value pair on this object.", "response": "def remove_index(self, field=None, value=None):\n        \"\"\"\n        remove_index(field=None, value=None)\n\n        Remove the specified field/value pair as an index on this\n        object.\n\n        :param field: The index field.\n        :type field: string\n        :param value: The index value.\n        :type value: string or integer\n        :rtype: :class:`RiakObject <riak.riak_object.RiakObject>`\n        \"\"\"\n        if not field and not value:\n            self.indexes.clear()\n        elif field and not value:\n            for index in [x for x in self.indexes if x[0] == field]:\n                self.indexes.remove(index)\n        elif field and value:\n            self.indexes.remove((field, value))\n        else:\n            raise RiakError(\"Cannot pass value without a field\"\n                            \" name while removing index\")\n\n        return self._robject"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_index(self, field, value):\n        to_rem = set((x for x in self.indexes if x[0] == field))\n        self.indexes.difference_update(to_rem)\n        return self.add_index(field, value)", "response": "Set the value of a key in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a link to a RiakObject.", "response": "def add_link(self, obj, tag=None):\n        \"\"\"\n        add_link(obj, tag=None)\n\n        Add a link to a RiakObject.\n\n        :param obj: Either a RiakObject or 3 item link tuple consisting\n            of (bucket, key, tag).\n        :type obj: mixed\n        :param tag: Optional link tag. Defaults to bucket name. It is ignored\n            if ``obj`` is a 3 item link tuple.\n        :type tag: string\n        :rtype: :class:`RiakObject <riak.riak_object.RiakObject>`\n        \"\"\"\n        if isinstance(obj, tuple):\n            newlink = obj\n        else:\n            newlink = (obj.bucket.name, obj.key, tag)\n\n        self.links.append(newlink)\n        return self._robject"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefault OpenSSL certificate verification callback.", "response": "def verify_cb(conn, cert, errnum, depth, ok):\n    \"\"\"\n    The default OpenSSL certificate verification callback.\n    \"\"\"\n    if not ok:\n        raise SecurityError(\"Could not verify CA certificate {0}\"\n                            .format(cert.get_subject()))\n    return ok"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_page(self, timeout=None, stream=None):\n        if not self.continuation:\n            raise ValueError(\"Cannot get next index page, no continuation\")\n\n        if stream is not None:\n            self.stream = stream\n\n        args = {'bucket': self.bucket,\n                'index': self.index,\n                'startkey': self.startkey,\n                'endkey': self.endkey,\n                'return_terms': self.return_terms,\n                'max_results': self.max_results,\n                'continuation': self.continuation,\n                'timeout': timeout,\n                'term_regex': self.term_regex}\n\n        if self.stream:\n            return self.client.stream_index(**args)\n        else:\n            return self.client.get_index(**args)", "response": "Fetches the next page of items from the index."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _inject_term(self, result):\n        if self._should_inject_term(result):\n            if type(result) is list:\n                return [(self.startkey, r) for r in result]\n            else:\n                return (self.startkey, result)\n        else:\n            return result", "response": "Injects the term into the result if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if the given exception is something that is likely to cause the TCP connection to the next node to be retried.", "response": "def is_retryable(err):\n    \"\"\"\n    Determines if the given exception is something that is\n    network/socket-related and should thus cause the TCP connection to\n    close and the operation retried on another node.\n\n    :rtype: boolean\n    \"\"\"\n    if isinstance(err, ConnectionClosed):\n        # NB: only retryable if we're not mid-streaming\n        if err.mid_stream:\n            return False\n        else:\n            return True\n    elif isinstance(err, socket.error):\n        code = err.args[0]\n        return code in CONN_CLOSED_ERRORS\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that the given timeout is a valid value.", "response": "def _validate_timeout(timeout, infinity_ok=False):\n    \"\"\"\n    Raises an exception if the given timeout is an invalid value.\n    \"\"\"\n    if timeout is None:\n        return\n\n    if timeout == 'infinity':\n        if infinity_ok:\n            return\n        else:\n            raise ValueError(\n                'timeout must be a positive integer '\n                '(\"infinity\" is not valid)')\n\n    if isinstance(timeout, six.integer_types) and timeout > 0:\n        return\n\n    raise ValueError('timeout must be a positive integer')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_buckets(self, transport, bucket_type=None, timeout=None):\n        if not riak.disable_list_exceptions:\n            raise ListError()\n\n        _validate_timeout(timeout)\n\n        if bucket_type:\n            bucketfn = self._bucket_type_bucket_builder\n        else:\n            bucketfn = self._default_type_bucket_builder\n\n        return [bucketfn(bytes_to_str(name), bucket_type) for name in\n                transport.get_buckets(bucket_type=bucket_type,\n                                      timeout=timeout)]", "response": "Get the list of buckets from a RiakTransport."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstreaming the list of buckets in the current cluster.", "response": "def stream_buckets(self, bucket_type=None, timeout=None):\n        \"\"\"\n        Streams the list of buckets. This is a generator method that\n        should be iterated over.\n\n        .. warning:: Do not use this in production, as it requires\n           traversing through all keys stored in a cluster.\n\n        The caller should explicitly close the returned iterator,\n        either using :func:`contextlib.closing` or calling ``close()``\n        explicitly. Consuming the entire iterator will also close the\n        stream. If it does not, the associated connection might not be\n        returned to the pool. Example::\n\n            from contextlib import closing\n\n            # Using contextlib.closing\n            with closing(client.stream_buckets()) as buckets:\n                for bucket_list in buckets:\n                    do_something(bucket_list)\n\n            # Explicit close()\n            stream = client.stream_buckets()\n            for bucket_list in stream:\n                 do_something(bucket_list)\n            stream.close()\n\n        :param bucket_type: the optional containing bucket type\n        :type bucket_type: :class:`~riak.bucket.BucketType`\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :rtype: iterator that yields lists of :class:`RiakBucket\n             <riak.bucket.RiakBucket>` instances\n\n        \"\"\"\n        if not riak.disable_list_exceptions:\n            raise ListError()\n\n        _validate_timeout(timeout)\n\n        if bucket_type:\n            bucketfn = self._bucket_type_bucket_builder\n        else:\n            bucketfn = self._default_type_bucket_builder\n\n        def make_op(transport):\n            return transport.stream_buckets(\n                    bucket_type=bucket_type, timeout=timeout)\n\n        for bucket_list in self._stream_with_retry(make_op):\n            bucket_list = [bucketfn(bytes_to_str(name), bucket_type)\n                           for name in bucket_list]\n            if len(bucket_list) > 0:\n                yield bucket_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the index for a specific entry in a bucket.", "response": "def get_index(self, transport, bucket, index, startkey, endkey=None,\n                  return_terms=None, max_results=None, continuation=None,\n                  timeout=None, term_regex=None):\n        \"\"\"\n        get_index(bucket, index, startkey, endkey=None, return_terms=None,\\\n                  max_results=None, continuation=None, timeout=None,\\\n                  term_regex=None)\n\n        Queries a secondary index, returning matching keys.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param bucket: the bucket whose index will be queried\n        :type bucket: RiakBucket\n        :param index: the index to query\n        :type index: string\n        :param startkey: the sole key to query, or beginning of the query range\n        :type startkey: string, integer\n        :param endkey: the end of the query range (optional if equality)\n        :type endkey: string, integer\n        :param return_terms: whether to include the secondary index value\n        :type return_terms: boolean\n        :param max_results: the maximum number of results to return (page size)\n        :type max_results: integer\n        :param continuation: the opaque continuation returned from a\n            previous paginated request\n        :type continuation: string\n        :param timeout: a timeout value in milliseconds, or 'infinity'\n        :type timeout: int\n        :param term_regex: a regular expression used to filter index terms\n        :type term_regex: string\n        :rtype: :class:`~riak.client.index_page.IndexPage`\n        \"\"\"\n        _validate_timeout(timeout, infinity_ok=True)\n\n        page = IndexPage(self, bucket, index, startkey, endkey,\n                         return_terms, max_results, term_regex)\n\n        results, continuation = transport.get_index(\n            bucket, index, startkey, endkey, return_terms=return_terms,\n            max_results=max_results, continuation=continuation,\n            timeout=timeout, term_regex=term_regex)\n\n        page.results = results\n        page.continuation = continuation\n        return page"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates over a paginated index query. This is equivalent to calling :meth:`get_index` and then successively calling :meth:`~riak.client.index_page.IndexPage.next_page` until all results are exhausted. Because limiting the result set is necessary to invoke pagination, the ``max_results`` option has a default of ``1000``. :param bucket: the bucket whose index will be queried :type bucket: RiakBucket :param index: the index to query :type index: string :param startkey: the sole key to query, or beginning of the query range :type startkey: string, integer :param endkey: the end of the query range (optional if equality) :type endkey: string, integer :param return_terms: whether to include the secondary index value :type return_terms: boolean :param max_results: the maximum number of results to return (page size), defaults to 1000 :type max_results: integer :param continuation: the opaque continuation returned from a previous paginated request :type continuation: string :param timeout: a timeout value in milliseconds, or 'infinity' :type timeout: int :param term_regex: a regular expression used to filter index terms :type term_regex: string :rtype: generator over instances of :class:`~riak.client.index_page.IndexPage`", "response": "def paginate_index(self, bucket, index, startkey, endkey=None,\n                       max_results=1000, return_terms=None,\n                       continuation=None, timeout=None, term_regex=None):\n        \"\"\"\n        Iterates over a paginated index query. This is equivalent to calling\n        :meth:`get_index` and then successively calling\n        :meth:`~riak.client.index_page.IndexPage.next_page` until all\n        results are exhausted.\n\n        Because limiting the result set is necessary to invoke pagination,\n        the ``max_results`` option has a default of ``1000``.\n\n        :param bucket: the bucket whose index will be queried\n        :type bucket: RiakBucket\n        :param index: the index to query\n        :type index: string\n        :param startkey: the sole key to query, or beginning of the query range\n        :type startkey: string, integer\n        :param endkey: the end of the query range (optional if equality)\n        :type endkey: string, integer\n        :param return_terms: whether to include the secondary index value\n        :type return_terms: boolean\n        :param max_results: the maximum number of results to return (page\n            size), defaults to 1000\n        :type max_results: integer\n        :param continuation: the opaque continuation returned from a\n            previous paginated request\n        :type continuation: string\n        :param timeout: a timeout value in milliseconds, or 'infinity'\n        :type timeout: int\n        :param term_regex: a regular expression used to filter index terms\n        :type term_regex: string\n        :rtype: generator over instances of\n          :class:`~riak.client.index_page.IndexPage`\n\n        \"\"\"\n        page = self.get_index(bucket, index, startkey,\n                              endkey=endkey, max_results=max_results,\n                              return_terms=return_terms,\n                              continuation=continuation,\n                              timeout=timeout, term_regex=term_regex)\n        yield page\n        while page.has_next_page():\n            page = page.next_page()\n            yield page"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstream the secondary index for a specific key range and returns a list of the values.", "response": "def stream_index(self, bucket, index, startkey, endkey=None,\n                     return_terms=None, max_results=None, continuation=None,\n                     timeout=None, term_regex=None):\n        \"\"\"\n        Queries a secondary index, streaming matching keys through an\n        iterator.\n\n        The caller should explicitly close the returned iterator,\n        either using :func:`contextlib.closing` or calling ``close()``\n        explicitly. Consuming the entire iterator will also close the\n        stream. If it does not, the associated connection might not be\n        returned to the pool. Example::\n\n            from contextlib import closing\n\n            # Using contextlib.closing\n            with closing(client.stream_index(mybucket, 'name_bin',\n                                             'Smith')) as index:\n                for key in index:\n                    do_something(key)\n\n            # Explicit close()\n            stream = client.stream_index(mybucket, 'name_bin', 'Smith')\n            for key in stream:\n                 do_something(key)\n            stream.close()\n\n        :param bucket: the bucket whose index will be queried\n        :type bucket: RiakBucket\n        :param index: the index to query\n        :type index: string\n        :param startkey: the sole key to query, or beginning of the query range\n        :type startkey: string, integer\n        :param endkey: the end of the query range (optional if equality)\n        :type endkey: string, integer\n        :param return_terms: whether to include the secondary index value\n        :type return_terms: boolean\n        :param max_results: the maximum number of results to return (page size)\n        :type max_results: integer\n        :param continuation: the opaque continuation returned from a\n            previous paginated request\n        :type continuation: string\n        :param timeout: a timeout value in milliseconds, or 'infinity'\n        :type timeout: int\n        :param term_regex: a regular expression used to filter index terms\n        :type term_regex: string\n        :rtype: :class:`~riak.client.index_page.IndexPage`\n\n        \"\"\"\n        # TODO FUTURE: implement \"retry on connection closed\"\n        # as in stream_mapred\n        _validate_timeout(timeout, infinity_ok=True)\n\n        page = IndexPage(self, bucket, index, startkey, endkey,\n                         return_terms, max_results, term_regex)\n        page.stream = True\n        resource = self._acquire()\n        transport = resource.object\n        page.results = transport.stream_index(\n            bucket, index, startkey, endkey, return_terms=return_terms,\n            max_results=max_results, continuation=continuation,\n            timeout=timeout, term_regex=term_regex)\n        page.results.attach(resource)\n        return page"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paginate_stream_index(self, bucket, index, startkey, endkey=None,\n                              max_results=1000, return_terms=None,\n                              continuation=None, timeout=None,\n                              term_regex=None):\n        \"\"\"\n        Iterates over a streaming paginated index query. This is equivalent to\n        calling :meth:`stream_index` and then successively calling\n        :meth:`~riak.client.index_page.IndexPage.next_page` until all\n        results are exhausted.\n\n        Because limiting the result set is necessary to invoke\n        pagination, the ``max_results`` option has a default of ``1000``.\n\n        The caller should explicitly close each yielded page, either using\n        :func:`contextlib.closing` or calling ``close()`` explicitly. Consuming\n        the entire page will also close the stream. If it does not, the\n        associated connection might not be returned to the pool. Example::\n\n            from contextlib import closing\n\n            # Using contextlib.closing\n            for page in client.paginate_stream_index(mybucket, 'name_bin',\n                                                     'Smith'):\n                with closing(page):\n                    for key in page:\n                        do_something(key)\n\n            # Explicit close()\n            for page in client.paginate_stream_index(mybucket, 'name_bin',\n                                                     'Smith'):\n                for key in page:\n                    do_something(key)\n                page.close()\n\n        :param bucket: the bucket whose index will be queried\n        :type bucket: RiakBucket\n        :param index: the index to query\n        :type index: string\n        :param startkey: the sole key to query, or beginning of the query range\n        :type startkey: string, integer\n        :param endkey: the end of the query range (optional if equality)\n        :type endkey: string, integer\n        :param return_terms: whether to include the secondary index value\n        :type return_terms: boolean\n        :param max_results: the maximum number of results to return (page\n            size), defaults to 1000\n        :type max_results: integer\n        :param continuation: the opaque continuation returned from a\n            previous paginated request\n        :type continuation: string\n        :param timeout: a timeout value in milliseconds, or 'infinity'\n        :type timeout: int\n        :param term_regex: a regular expression used to filter index terms\n        :type term_regex: string\n        :rtype: generator over instances of\n          :class:`~riak.client.index_page.IndexPage`\n\n        \"\"\"\n        # TODO FUTURE: implement \"retry on connection closed\"\n        # as in stream_mapred\n        page = self.stream_index(bucket, index, startkey,\n                                 endkey=endkey,\n                                 max_results=max_results,\n                                 return_terms=return_terms,\n                                 continuation=continuation,\n                                 timeout=timeout,\n                                 term_regex=term_regex)\n        yield page\n        while page.has_next_page():\n            page = page.next_page()\n            yield page", "response": "This method returns a paginated version of the stream index query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the properties of the given bucket.", "response": "def set_bucket_props(self, transport, bucket, props):\n        \"\"\"\n        set_bucket_props(bucket, props)\n\n        Sets bucket properties for the given bucket.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param bucket: the bucket whose properties will be set\n        :type bucket: RiakBucket\n        :param props: the properties to set\n        :type props: dict\n        \"\"\"\n        _validate_bucket_props(props)\n        return transport.set_bucket_props(bucket, props)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_bucket_type_props(self, transport, bucket_type, props):\n        _validate_bucket_props(props)\n        return transport.set_bucket_type_props(bucket_type, props)", "response": "Sets properties for the given bucket - type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_keys(self, transport, bucket, timeout=None):\n        if not riak.disable_list_exceptions:\n            raise ListError()\n\n        _validate_timeout(timeout)\n\n        return transport.get_keys(bucket, timeout=timeout)", "response": "Get all keys in a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstreams all keys in a bucket via a stream.", "response": "def stream_keys(self, bucket, timeout=None):\n        \"\"\"\n        Lists all keys in a bucket via a stream. This is a generator\n        method which should be iterated over.\n\n        .. warning:: Do not use this in production, as it requires\n           traversing through all keys stored in a cluster.\n\n        The caller should explicitly close the returned iterator,\n        either using :func:`contextlib.closing` or calling ``close()``\n        explicitly. Consuming the entire iterator will also close the\n        stream. If it does not, the associated connection might\n        not be returned to the pool. Example::\n\n            from contextlib import closing\n\n            # Using contextlib.closing\n            with closing(client.stream_keys(mybucket)) as keys:\n                for key_list in keys:\n                    do_something(key_list)\n\n            # Explicit close()\n            stream = client.stream_keys(mybucket)\n            for key_list in stream:\n                 do_something(key_list)\n            stream.close()\n\n        :param bucket: the bucket whose properties will be set\n        :type bucket: RiakBucket\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :rtype: iterator\n        \"\"\"\n        if not riak.disable_list_exceptions:\n            raise ListError()\n\n        _validate_timeout(timeout)\n\n        def make_op(transport):\n            return transport.stream_keys(bucket, timeout=timeout)\n\n        for keylist in self._stream_with_retry(make_op):\n            if len(keylist) > 0:\n                if six.PY2:\n                    yield keylist\n                else:\n                    yield [bytes_to_str(item) for item in keylist]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore an object in the Riak cluster.", "response": "def put(self, transport, robj, w=None, dw=None, pw=None, return_body=None,\n            if_none_match=None, timeout=None):\n        \"\"\"\n        put(robj, w=None, dw=None, pw=None, return_body=None,\\\n            if_none_match=None, timeout=None)\n\n        Stores an object in the Riak cluster.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param robj: the object to store\n        :type robj: RiakObject\n        :param w: the write quorum\n        :type w: integer, string, None\n        :param dw: the durable write quorum\n        :type dw: integer, string, None\n        :param pw: the primary write quorum\n        :type pw: integer, string, None\n        :param return_body: whether to return the resulting object\n           after the write\n        :type return_body: boolean\n        :param if_none_match: whether to fail the write if the object\n          exists\n        :type if_none_match: boolean\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        \"\"\"\n        _validate_timeout(timeout)\n        return transport.put(robj, w=w, dw=dw, pw=pw,\n                             return_body=return_body,\n                             if_none_match=if_none_match,\n                             timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ts_describe(self, transport, table):\n        t = table\n        if isinstance(t, six.string_types):\n            t = Table(self, table)\n        return transport.ts_describe(t)", "response": "Retrieves a time series table description from the Riak cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ts_get(self, transport, table, key):\n        t = table\n        if isinstance(t, six.string_types):\n            t = Table(self, table)\n        return transport.ts_get(t, key)", "response": "Retrieve a timeseries value by its key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the time series data in the Riak cluster.", "response": "def ts_query(self, transport, table, query, interpolations=None):\n        \"\"\"\n        ts_query(table, query, interpolations=None)\n\n        Queries time series data in the Riak cluster.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param table: The timeseries table.\n        :type table: string or :class:`Table <riak.table.Table>`\n        :param query: The timeseries query.\n        :type query: string\n        :rtype: :class:`TsObject <riak.ts_object.TsObject>`\n        \"\"\"\n        t = table\n        if isinstance(t, six.string_types):\n            t = Table(self, table)\n        return transport.ts_query(t, query, interpolations)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all keys in a time series table via a stream. This is a generator method which should be iterated over. The caller should explicitly close the returned iterator, either using :func:`contextlib.closing` or calling ``close()`` explicitly. Consuming the entire iterator will also close the stream. If it does not, the associated connection might not be returned to the pool. Example:: from contextlib import closing # Using contextlib.closing with closing(client.ts_stream_keys(mytable)) as keys: for key_list in keys: do_something(key_list) # Explicit close() stream = client.ts_stream_keys(mytable) for key_list in stream: do_something(key_list) stream.close() :param table: the table from which to stream keys :type table: string or :class:`Table <riak.table.Table>` :param timeout: a timeout value in milliseconds :type timeout: int :rtype: iterator", "response": "def ts_stream_keys(self, table, timeout=None):\n        \"\"\"\n        Lists all keys in a time series table via a stream. This is a\n        generator method which should be iterated over.\n\n        The caller should explicitly close the returned iterator,\n        either using :func:`contextlib.closing` or calling ``close()``\n        explicitly. Consuming the entire iterator will also close the\n        stream. If it does not, the associated connection might\n        not be returned to the pool. Example::\n\n            from contextlib import closing\n\n            # Using contextlib.closing\n            with closing(client.ts_stream_keys(mytable)) as keys:\n                for key_list in keys:\n                    do_something(key_list)\n\n            # Explicit close()\n            stream = client.ts_stream_keys(mytable)\n            for key_list in stream:\n                 do_something(key_list)\n            stream.close()\n\n        :param table: the table from which to stream keys\n        :type table: string or :class:`Table <riak.table.Table>`\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :rtype: iterator\n        \"\"\"\n        if not riak.disable_list_exceptions:\n            raise ListError()\n\n        t = table\n        if isinstance(t, six.string_types):\n            t = Table(self, table)\n\n        _validate_timeout(timeout)\n\n        resource = self._acquire()\n        transport = resource.object\n        stream = transport.ts_stream_keys(t, timeout)\n        stream.attach(resource)\n        try:\n            for keylist in stream:\n                if len(keylist) > 0:\n                    yield keylist\n        finally:\n            stream.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch the contents of a Riak object.", "response": "def get(self, transport, robj, r=None, pr=None, timeout=None,\n            basic_quorum=None, notfound_ok=None, head_only=False):\n        \"\"\"\n        get(robj, r=None, pr=None, timeout=None)\n\n        Fetches the contents of a Riak object.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param robj: the object to fetch\n        :type robj: RiakObject\n        :param r: the read quorum\n        :type r: integer, string, None\n        :param pr: the primary read quorum\n        :type pr: integer, string, None\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :param head_only: whether to fetch without value, so only metadata\n           (only available on PB transport)\n        :type head_only: bool\n        \"\"\"\n        _validate_timeout(timeout)\n        if not isinstance(robj.key, six.string_types):\n            raise TypeError(\n                'key must be a string, instead got {0}'.format(repr(robj.key)))\n\n        return transport.get(robj, r=r, pr=pr, timeout=timeout,\n                             basic_quorum=basic_quorum,\n                             notfound_ok=notfound_ok,\n                             head_only=head_only)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self, transport, robj, rw=None, r=None, w=None, dw=None,\n               pr=None, pw=None, timeout=None):\n        \"\"\"\n        delete(robj, rw=None, r=None, w=None, dw=None, pr=None, pw=None,\\\n               timeout=None)\n\n        Deletes an object from Riak.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param robj: the object to delete\n        :type robj: RiakObject\n        :param rw: the read/write (delete) quorum\n        :type rw: integer, string, None\n        :param r: the read quorum\n        :type r: integer, string, None\n        :param pr: the primary read quorum\n        :type pr: integer, string, None\n        :param w: the write quorum\n        :type w: integer, string, None\n        :param dw: the durable write quorum\n        :type dw: integer, string, None\n        :param pw: the primary write quorum\n        :type pw: integer, string, None\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        \"\"\"\n        _validate_timeout(timeout)\n        return transport.delete(robj, rw=rw, r=r, w=w, dw=dw, pr=pr,\n                                pw=pw, timeout=timeout)", "response": "This method deletes an object from Riak."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute a MapReduce query.", "response": "def mapred(self, transport, inputs, query, timeout):\n        \"\"\"\n        mapred(inputs, query, timeout)\n\n        Executes a MapReduce query.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param inputs: the input list/structure\n        :type inputs: list, dict\n        :param query: the list of query phases\n        :type query: list\n        :param timeout: the query timeout\n        :type timeout: integer, None\n        :rtype: mixed\n        \"\"\"\n        _validate_timeout(timeout)\n        return transport.mapred(inputs, query, timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstreams a MapReduce query as pairs.", "response": "def stream_mapred(self, inputs, query, timeout):\n        \"\"\"\n        Streams a MapReduce query as (phase, data) pairs. This is a\n        generator method which should be iterated over.\n\n        The caller should explicitly close the returned iterator,\n        either using :func:`contextlib.closing` or calling ``close()``\n        explicitly. Consuming the entire iterator will also close the\n        stream. If it does not, the associated connection might\n        not be returned to the pool. Example::\n\n            from contextlib import closing\n\n            # Using contextlib.closing\n            with closing(mymapred.stream()) as results:\n                for phase, result in results:\n                    do_something(phase, result)\n\n            # Explicit close()\n            stream = mymapred.stream()\n            for phase, result in stream:\n                 do_something(phase, result)\n            stream.close()\n\n        :param inputs: the input list/structure\n        :type inputs: list, dict\n        :param query: the list of query phases\n        :type query: list\n        :param timeout: the query timeout\n        :type timeout: integer, None\n        :rtype: iterator\n        \"\"\"\n        _validate_timeout(timeout)\n\n        def make_op(transport):\n            return transport.stream_mapred(inputs, query, timeout)\n\n        for phase, data in self._stream_with_retry(make_op):\n            yield phase, data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_search_index(self, transport, index, schema=None, n_val=None,\n                            timeout=None):\n        \"\"\"\n        create_search_index(index, schema=None, n_val=None)\n\n        Create a search index of the given name, and optionally set\n        a schema. If no schema is set, the default will be used.\n\n        :param index: the name of the index to create\n        :type index: string\n        :param schema: the schema that this index will follow\n        :type schema: string, None\n        :param n_val: this indexes N value\n        :type n_val: integer, None\n        :param timeout: optional timeout (in ms)\n        :type timeout: integer, None\n        \"\"\"\n        return transport.create_search_index(index, schema, n_val, timeout)", "response": "Create a search index for the given name and schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fulltext_search(self, transport, index, query, **params):\n        return transport.search(index, query, **params)", "response": "Perform a full - text search over the index."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fulltext_delete(self, transport, index, docs=None, queries=None):\n        transport.fulltext_delete(index, docs, queries)", "response": "This method is deprecated and will be removed from the full - text index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching many keys in parallel via threads.", "response": "def multiget(self, pairs, **params):\n        \"\"\"Fetches many keys in parallel via threads.\n\n        :param pairs: list of bucket_type/bucket/key tuple triples\n        :type pairs: list\n        :param params: additional request flags, e.g. r, pr\n        :type params: dict\n        :rtype: list of :class:`RiakObjects <riak.riak_object.RiakObject>`,\n            :class:`Datatypes <riak.datatypes.Datatype>`, or tuples of\n            bucket_type, bucket, key, and the exception raised on fetch\n        \"\"\"\n        if self._multiget_pool:\n            params['pool'] = self._multiget_pool\n        return riak.client.multi.multiget(self, pairs, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring objects in parallel via threads.", "response": "def multiput(self, objs, **params):\n        \"\"\"\n        Stores objects in parallel via threads.\n\n        :param objs: the objects to store\n        :type objs: list of `RiakObject <riak.riak_object.RiakObject>`\n        :param params: additional request flags, e.g. w, dw, pw\n        :type params: dict\n        :rtype: list of boolean or\n            :class:`RiakObjects <riak.riak_object.RiakObject>`,\n        \"\"\"\n        if self._multiput_pool:\n            params['pool'] = self._multiput_pool\n        return riak.client.multi.multiput(self, objs, **params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the value of a counter.", "response": "def get_counter(self, transport, bucket, key, r=None, pr=None,\n                    basic_quorum=None, notfound_ok=None):\n        \"\"\"get_counter(bucket, key, r=None, pr=None, basic_quorum=None,\\\n                       notfound_ok=None)\n\n        Gets the value of a counter.\n\n        .. deprecated:: 2.1.0 (Riak 2.0) Riak 1.4-style counters are\n           deprecated in favor of the :class:`~riak.datatypes.Counter`\n           datatype.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param bucket: the bucket of the counter\n        :type bucket: RiakBucket\n        :param key: the key of the counter\n        :type key: string\n        :param r: the read quorum\n        :type r: integer, string, None\n        :param pr: the primary read quorum\n        :type pr: integer, string, None\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :rtype: integer\n\n        \"\"\"\n        return transport.get_counter(bucket, key, r=r, pr=pr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_counter(self, bucket, key, value, w=None, dw=None, pw=None,\n                       returnvalue=False):\n        \"\"\"\n        update_counter(bucket, key, value, w=None, dw=None, pw=None,\\\n                       returnvalue=False)\n\n        .. deprecated:: 2.1.0 (Riak 2.0) Riak 1.4-style counters are\n           deprecated in favor of the :class:`~riak.datatypes.Counter`\n           datatype.\n\n        Updates a counter by the given value. This operation is not\n        idempotent and so should not be retried automatically.\n\n        :param bucket: the bucket of the counter\n        :type bucket: RiakBucket\n        :param key: the key of the counter\n        :type key: string\n        :param value: the amount to increment or decrement\n        :type value: integer\n        :param w: the write quorum\n        :type w: integer, string, None\n        :param dw: the durable write quorum\n        :type dw: integer, string, None\n        :param pw: the primary write quorum\n        :type pw: integer, string, None\n        :param returnvalue: whether to return the updated value of the counter\n        :type returnvalue: bool\n        \"\"\"\n        if not isinstance(value, six.integer_types):\n            raise TypeError(\"Counter update amount must be an integer\")\n        if value == 0:\n            raise ValueError(\"Cannot increment counter by 0\")\n\n        with self._transport() as transport:\n            return transport.update_counter(bucket, key, value,\n                                            w=w, dw=dw, pw=pw,\n                                            returnvalue=returnvalue)", "response": "Update the value of a key in a bucket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fetch_datatype(self, bucket, key, r=None, pr=None,\n                       basic_quorum=None, notfound_ok=None,\n                       timeout=None, include_context=None):\n        \"\"\"\n        Fetches the value of a Riak Datatype.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param bucket: the bucket of the datatype, which must belong to a\n          :class:`~riak.bucket.BucketType`\n        :type bucket: :class:`~riak.bucket.RiakBucket`\n        :param key: the key of the datatype\n        :type key: string\n        :param r: the read quorum\n        :type r: integer, string, None\n        :param pr: the primary read quorum\n        :type pr: integer, string, None\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool, None\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool, None\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int, None\n        :param include_context: whether to return the opaque context\n          as well as the value, which is useful for removal operations\n          on sets and maps\n        :type include_context: bool, None\n        :rtype: :class:`~riak.datatypes.Datatype`\n        \"\"\"\n        dtype, value, context = self._fetch_datatype(\n            bucket, key, r=r, pr=pr, basic_quorum=basic_quorum,\n            notfound_ok=notfound_ok, timeout=timeout,\n            include_context=include_context)\n\n        return TYPES[dtype](bucket=bucket, key=key, value=value,\n                            context=context)", "response": "Fetch the value of a Riak Datatype."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend an update to a Riak Datatype to the server.", "response": "def update_datatype(self, datatype, w=None, dw=None, pw=None,\n                        return_body=None, timeout=None,\n                        include_context=None):\n        \"\"\"\n        Sends an update to a Riak Datatype to the server. This operation is not\n        idempotent and so will not be retried automatically.\n\n        :param datatype: the datatype with pending updates\n        :type datatype: :class:`~riak.datatypes.Datatype`\n        :param w: the write quorum\n        :type w: integer, string, None\n        :param dw: the durable write quorum\n        :type dw: integer, string, None\n        :param pw: the primary write quorum\n        :type pw: integer, string, None\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param include_context: whether to return the opaque context\n          as well as the value, which is useful for removal operations\n          on sets and maps\n        :type include_context: bool\n        :rtype: tuple of datatype, opaque value and opaque context\n\n        \"\"\"\n        _validate_timeout(timeout)\n\n        with self._transport() as transport:\n            return transport.update_datatype(datatype, w=w, dw=dw, pw=pw,\n                                             return_body=return_body,\n                                             timeout=timeout,\n                                             include_context=include_context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfetches a Riak Datatype from the given bucket and key.", "response": "def _fetch_datatype(self, transport, bucket, key, r=None, pr=None,\n                        basic_quorum=None, notfound_ok=None,\n                        timeout=None, include_context=None):\n        \"\"\"\n        _fetch_datatype(bucket, key, r=None, pr=None, basic_quorum=None,\n                       notfound_ok=None, timeout=None, include_context=None)\n\n\n        Fetches the value of a Riak Datatype as raw data. This is used\n        internally to update already reified Datatype objects. Use the\n        public version to fetch a reified type.\n\n        .. note:: This request is automatically retried :attr:`retries`\n           times if it fails due to network error.\n\n        :param bucket: the bucket of the datatype, which must belong to a\n          :class:`~riak.BucketType`\n        :type bucket: RiakBucket\n        :param key: the key of the datatype\n        :type key: string, None\n        :param r: the read quorum\n        :type r: integer, string, None\n        :param pr: the primary read quorum\n        :type pr: integer, string, None\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param include_context: whether to return the opaque context\n          as well as the value, which is useful for removal operations\n          on sets and maps\n        :type include_context: bool\n        :rtype: tuple of type, value and context\n        \"\"\"\n        _validate_timeout(timeout)\n\n        return transport.fetch_datatype(bucket, key, r=r, pr=pr,\n                                        basic_quorum=basic_quorum,\n                                        notfound_ok=notfound_ok,\n                                        timeout=timeout,\n                                        include_context=include_context)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _non_connect_send_recv(self, msg_code, data=None):\n        self._non_connect_send_msg(msg_code, data)\n        return self._recv_msg()", "response": "Send and receive a message from the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to the server.", "response": "def _non_connect_send_msg(self, msg_code, data):\n        \"\"\"\n        Similar to self._send, but doesn't try to initiate a connection,\n        thus preventing an infinite loop.\n        \"\"\"\n        try:\n            self._socket.sendall(self._encode_msg(msg_code, data))\n        except (IOError, socket.error) as e:\n            if e.errno == errno.EPIPE:\n                raise ConnectionClosed(e)\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize a secure connection to the server.", "response": "def _init_security(self):\n        \"\"\"\n        Initialize a secure connection to the server.\n        \"\"\"\n        if not self._starttls():\n            raise SecurityError(\"Could not start TLS connection\")\n        # _ssh_handshake() will throw an exception upon failure\n        self._ssl_handshake()\n        if not self._auth():\n            raise SecurityError(\"Could not authorize connection\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if Riak responds with a STARTTLS response False otherwise.", "response": "def _starttls(self):\n        \"\"\"\n        Exchange a STARTTLS message with Riak to initiate secure communications\n        return True is Riak responds with a STARTTLS response, False otherwise\n        \"\"\"\n        resp_code, _ = self._non_connect_send_recv(\n            riak.pb.messages.MSG_CODE_START_TLS)\n        if resp_code == riak.pb.messages.MSG_CODE_START_TLS:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms an authorization request against Riak returns True upon success False upon failure", "response": "def _auth(self):\n        \"\"\"\n        Perform an authorization request against Riak\n        returns True upon success, False otherwise\n        Note: Riak will sleep for a short period of time upon a failed\n              auth request/response to prevent denial of service attacks\n        \"\"\"\n        codec = PbufCodec()\n        username = self._client._credentials.username\n        password = self._client._credentials.password\n        if not password:\n            password = ''\n        msg = codec.encode_auth(username, password)\n        resp_code, _ = self._non_connect_send_recv_msg(msg)\n        if resp_code == riak.pb.messages.MSG_CODE_AUTH_RESP:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _recv_msg(self, mid_stream=False):\n        try:\n            msgbuf = self._recv_pkt()\n        except BadResource as e:\n            e.mid_stream = mid_stream\n            raise\n        except socket.timeout as e:\n            # A timeout can leave the socket in an inconsistent state because\n            # it might still receive the data later and mix up with a\n            # subsequent request.\n            # https://github.com/basho/riak-python-client/issues/425\n            raise BadResource(e, mid_stream)\n        mv = memoryview(msgbuf)\n        mcb = mv[0:1]\n        if self.bytes_required:\n            mcb = mcb.tobytes()\n        try:\n            msg_code, = struct.unpack(\"B\", mcb)\n        except struct.error:\n            # NB: Python 2.7.3 requires this\n            # http://bugs.python.org/issue10212\n            msg_code, = struct.unpack(\"B\", mv[0:1].tobytes())\n            self.bytes_required = True\n        data = mv[1:].tobytes()\n        return (msg_code, data)", "response": "Receives a message from the socket and returns the message code and data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclose the underlying socket of the PB connection.", "response": "def close(self):\n        \"\"\"\n        Closes the underlying socket of the PB connection.\n        \"\"\"\n        if self._socket:\n            if USE_STDLIB_SSL:\n                # NB: Python 2.7.8 and earlier does not have a compatible\n                # shutdown() method due to the SSL lib\n                try:\n                    self._socket.shutdown(socket.SHUT_RDWR)\n                except EnvironmentError:\n                    # NB: sometimes these exceptions are raised if the initial\n                    # connection didn't succeed correctly, or if shutdown() is\n                    # called after the connection dies\n                    logging.debug('Exception occurred while shutting '\n                                  'down socket.', exc_info=True)\n            self._socket.close()\n            del self._socket"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelegate a property to the first sibling in a RiakObject, raising an error when the object is in conflict.", "response": "def content_property(name, doc=None):\n    \"\"\"\n    Delegates a property to the first sibling in a RiakObject, raising\n    an error when the object is in conflict.\n    \"\"\"\n    def _setter(self, value):\n        if len(self.siblings) == 0:\n            # In this case, assume that what the user wants is to\n            # create a new sibling inside an empty object.\n            self.siblings = [RiakContent(self)]\n        if len(self.siblings) != 1:\n            raise ConflictError()\n        setattr(self.siblings[0], name, value)\n\n    def _getter(self):\n        if len(self.siblings) == 0:\n            return\n        if len(self.siblings) != 1:\n            raise ConflictError()\n        return getattr(self.siblings[0], name)\n\n    return property(_getter, _setter, doc=doc)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndelegating a method to the first sibling in a RiakObject, raising an error when the object is in conflict.", "response": "def content_method(name):\n    \"\"\"\n    Delegates a method to the first sibling in a RiakObject, raising\n    an error when the object is in conflict.\n    \"\"\"\n    def _delegate(self, *args, **kwargs):\n        if len(self.siblings) != 1:\n            raise ConflictError()\n        return getattr(self.siblings[0], name).__call__(*args, **kwargs)\n\n    _delegate.__doc__ = getattr(RiakContent, name).__doc__\n\n    return _delegate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef store(self, w=None, dw=None, pw=None, return_body=True,\n              if_none_match=False, timeout=None):\n        \"\"\"\n        Store the object in Riak. When this operation completes, the\n        object could contain new metadata and possibly new data if Riak\n        contains a newer version of the object according to the object's\n        vector clock.\n\n        :param w: W-value, wait for this many partitions to respond\n         before returning to client.\n        :type w: integer\n        :param dw: DW-value, wait for this many partitions to\n         confirm the write before returning to client.\n        :type dw: integer\n\n        :param pw: PW-value, require this many primary partitions to\n                   be available before performing the put\n        :type pw: integer\n        :param return_body: if the newly stored object should be\n                            retrieved\n        :type return_body: bool\n        :param if_none_match: Should the object be stored only if\n                              there is no key previously defined\n        :type if_none_match: bool\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :rtype: :class:`RiakObject` \"\"\"\n        if len(self.siblings) != 1:\n            raise ConflictError(\"Attempting to store an invalid object, \"\n                                \"resolve the siblings first\")\n\n        self.client.put(self, w=w, dw=dw, pw=pw,\n                        return_body=return_body,\n                        if_none_match=if_none_match,\n                        timeout=timeout)\n\n        return self", "response": "Store the object in Riak."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreloads the object from Riak.", "response": "def reload(self, r=None, pr=None, timeout=None, basic_quorum=None,\n               notfound_ok=None, head_only=False):\n        \"\"\"\n        Reload the object from Riak. When this operation completes, the\n        object could contain new metadata and a new value, if the object\n        was updated in Riak since it was last retrieved.\n\n        .. note:: Even if the key is not found in Riak, this will\n           return a :class:`RiakObject`. Check the :attr:`exists`\n           property to see if the key was found.\n\n        :param r: R-Value, wait for this many partitions to respond\n         before returning to client.\n        :type r: integer\n        :param pr: PR-value, require this many primary partitions to\n                   be available before performing the read that\n                   precedes the put\n        :type pr: integer\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :param head_only: whether to fetch without value, so only metadata\n           (only available on PB transport)\n        :type head_only: bool\n        :rtype: :class:`RiakObject`\n        \"\"\"\n\n        self.client.get(self, r=r, pr=pr, timeout=timeout, head_only=head_only)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes this object from Riak.", "response": "def delete(self, r=None, w=None, dw=None, pr=None, pw=None,\n               timeout=None):\n        \"\"\"\n        Delete this object from Riak.\n\n        :param r: R-value, wait for this many partitions to read object\n         before performing the put\n        :type r: integer\n        :param w: W-value, wait for this many partitions to respond\n         before returning to client.\n        :type w: integer\n        :param dw: DW-value, wait for this many partitions to\n         confirm the write before returning to client.\n        :type dw: integer\n        :param pr: PR-value, require this many primary partitions to\n                   be available before performing the read that\n                   precedes the put\n        :type pr: integer\n        :param pw: PW-value, require this many primary partitions to\n                   be available before performing the put\n        :type pw: integer\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :rtype: :class:`RiakObject`\n        \"\"\"\n\n        self.client.delete(self, r=r, w=w, dw=dw, pr=pr, pw=pw,\n                           timeout=timeout)\n        self.clear()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, arg1, arg2=None, arg3=None, bucket_type=None):\n        mr = RiakMapReduce(self.client)\n        mr.add(self.bucket.name, self.key, bucket_type=bucket_type)\n        return mr.add(arg1, arg2, arg3, bucket_type)", "response": "A method to add a new entry to the map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart assembling a MapReduce operation.", "response": "def link(self, *args):\n        \"\"\"\n        Start assembling a Map/Reduce operation.\n        A shortcut for :meth:`~riak.mapreduce.RiakMapReduce.link`.\n\n        :rtype: :class:`~riak.mapreduce.RiakMapReduce`\n        \"\"\"\n        mr = RiakMapReduce(self.client)\n        mr.add(self.bucket.name, self.key)\n        return mr.link(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the encoder function for the provided content type for this bucket.", "response": "def get_encoder(self, content_type):\n        \"\"\"\n        Get the encoding function for the provided content type for\n        this bucket.\n\n        :param content_type: the requested media type\n        :type content_type: str\n        :param content_type: Content type requested\n        \"\"\"\n        if content_type in self._encoders:\n            return self._encoders[content_type]\n        else:\n            return self._client.get_encoder(content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_decoder(self, content_type):\n        if content_type in self._decoders:\n            return self._decoders[content_type]\n        else:\n            return self._client.get_decoder(content_type)", "response": "Get the decoding function for the provided content type for this bucket."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, key, r=None, pr=None, timeout=None, include_context=None,\n            basic_quorum=None, notfound_ok=None, head_only=False):\n        \"\"\"\n        Retrieve a :class:`~riak.riak_object.RiakObject` or\n        :class:`~riak.datatypes.Datatype`, based on the presence and value\n        of the :attr:`datatype <BucketType.datatype>` bucket property.\n\n        :param key: Name of the key.\n        :type key: string\n        :param r: R-Value of the request (defaults to bucket's R)\n        :type r: integer\n        :param pr: PR-Value of the request (defaults to bucket's PR)\n        :type pr: integer\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param include_context: if the bucket contains datatypes, include\n           the opaque context in the result\n        :type include_context: bool\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :param head_only: whether to fetch without value, so only metadata\n           (only available on PB transport)\n        :type head_only: bool\n        :rtype: :class:`RiakObject <riak.riak_object.RiakObject>` or\n           :class:`~riak.datatypes.Datatype`\n\n        \"\"\"\n        from riak import RiakObject\n        if self.bucket_type.datatype:\n            return self._client.fetch_datatype(self, key, r=r, pr=pr,\n                                               timeout=timeout,\n                                               include_context=include_context,\n                                               basic_quorum=basic_quorum,\n                                               notfound_ok=notfound_ok)\n        else:\n            obj = RiakObject(self._client, self, key)\n            return obj.reload(r=r, pr=pr, timeout=timeout,\n                              basic_quorum=basic_quorum,\n                              notfound_ok=notfound_ok,\n                              head_only=head_only)", "response": "Retrieves a key from the bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multiget(self, keys, r=None, pr=None, timeout=None,\n                 basic_quorum=None, notfound_ok=None,\n                 head_only=False):\n        \"\"\"\n        Retrieves a list of keys belonging to this bucket in parallel.\n\n        :param keys: the keys to fetch\n        :type keys: list\n        :param r: R-Value for the requests (defaults to bucket's R)\n        :type r: integer\n        :param pr: PR-Value for the requests (defaults to bucket's PR)\n        :type pr: integer\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :param basic_quorum: whether to use the \"basic quorum\" policy\n           for not-founds\n        :type basic_quorum: bool\n        :param notfound_ok: whether to treat not-found responses as successful\n        :type notfound_ok: bool\n        :param head_only: whether to fetch without value, so only metadata\n           (only available on PB transport)\n        :type head_only: bool\n        :rtype: list of :class:`RiakObjects <riak.riak_object.RiakObject>`,\n            :class:`Datatypes <riak.datatypes.Datatype>`, or tuples of\n            bucket_type, bucket, key, and the exception raised on fetch\n        \"\"\"\n        bkeys = [(self.bucket_type.name, self.name, key) for key in keys]\n        return self._client.multiget(bkeys, r=r, pr=pr, timeout=timeout,\n                                     basic_quorum=basic_quorum,\n                                     notfound_ok=notfound_ok,\n                                     head_only=head_only)", "response": "Fetches a list of keys belonging to this bucket in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new RiakObject from the contents of the specified file.", "response": "def new_from_file(self, key, filename):\n        \"\"\"Create a new Riak object in the bucket, using the contents of\n        the specified file. This is a shortcut for :meth:`new`, where the\n        ``encoded_data`` and ``content_type`` are set for you.\n\n        .. warning:: This is not supported for buckets that contain\n           :class:`Datatypes <riak.datatypes.Datatype>`.\n\n        :param key: the key of the new object\n        :type key: string\n        :param filename: the file to read the contents from\n        :type filename: string\n        :rtype: :class:`RiakObject <riak.riak_object.RiakObject>`\n        \"\"\"\n        binary_data = None\n        with open(filename, 'rb') as f:\n            binary_data = f.read()\n        mimetype, encoding = mimetypes.guess_type(filename)\n        if encoding:\n            binary_data = bytearray(binary_data, encoding)\n        else:\n            binary_data = bytearray(binary_data)\n        if not mimetype:\n            mimetype = 'application/octet-stream'\n        if PY2:\n            return self.new(key, encoded_data=binary_data,\n                            content_type=mimetype)\n        else:\n            return self.new(key, encoded_data=bytes(binary_data),\n                            content_type=mimetype)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, query, index=None, **params):\n        search_index = index or self.name\n        return self._client.fulltext_search(search_index, query, **params)", "response": "Queries a search index over objects in this bucket."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_index(self, index, startkey, endkey=None, return_terms=None,\n                  max_results=None, continuation=None, timeout=None,\n                  term_regex=None):\n        \"\"\"\n        Queries a secondary index over objects in this bucket,\n        returning keys or index/key pairs. See\n        :meth:`RiakClient.get_index()\n        <riak.client.RiakClient.get_index>` for more details.\n        \"\"\"\n        return self._client.get_index(self, index, startkey, endkey,\n                                      return_terms=return_terms,\n                                      max_results=max_results,\n                                      continuation=continuation,\n                                      timeout=timeout, term_regex=term_regex)", "response": "Queries a secondary index over objects in this bucket and returns keys or index pairs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the value of a counter stored in this bucket.", "response": "def get_counter(self, key, **kwargs):\n        \"\"\"\n        Gets the value of a counter stored in this bucket. See\n        :meth:`RiakClient.get_counter()\n        <riak.client.RiakClient.get_counter>` for options.\n\n        .. deprecated:: 2.1.0 (Riak 2.0) Riak 1.4-style counters are\n           deprecated in favor of the :class:`~riak.datatypes.Counter`\n           datatype.\n\n        :param key: the key of the counter\n        :type key: string\n        :rtype: int\n        \"\"\"\n        return self._client.get_counter(self, key, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_counter(self, key, value, **kwargs):\n        return self._client.update_counter(self, key, value, **kwargs)", "response": "Updates the value of a counter stored in this bucket. Positive values increment the counter negative values decrement the counter. Positive values increment the counter negative values decrement the counter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_buckets(self, timeout=None):\n        return self._client.get_buckets(bucket_type=self, timeout=timeout)", "response": "Get the list of buckets under this bucket - type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstream the list of buckets under this bucket - type.", "response": "def stream_buckets(self, timeout=None):\n        \"\"\"\n        Streams the list of buckets under this bucket-type. This is a\n        generator method that should be iterated over.\n\n        The caller must close the stream when finished.  See\n        :meth:`RiakClient.stream_buckets()\n        <riak.client.RiakClient.stream_buckets>` for more details.\n\n        .. warning:: Do not use this in production, as it requires\n           traversing through all keys stored in a cluster.\n\n        :param timeout: a timeout value in milliseconds\n        :type timeout: int\n        :rtype: iterator that yields lists of :class:`RiakBucket\n             <riak.bucket.RiakBucket>` instances\n        \"\"\"\n        return self._client.stream_buckets(bucket_type=self, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nincrease the value by the argument.", "response": "def incr(self, d):\n        \"\"\"\n        Increases the value by the argument.\n\n        :param d: the value to increase by\n        :type d: float\n        \"\"\"\n        with self.lock:\n            self.p = self.value() + d"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the current value of the current attribute.", "response": "def value(self):\n        \"\"\"\n        Returns the current value (adjusted for the time decay)\n\n        :rtype: float\n        \"\"\"\n        with self.lock:\n            now = time.time()\n            dt = now - self.t0\n            self.t0 = now\n            self.p = self.p * (math.pow(self.e, self.r * dt))\n            return self.p"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_random_client_id(self):\n        if PY2:\n            return ('py_%s' %\n                    base64.b64encode(str(random.randint(1, 0x40000000))))\n        else:\n            return ('py_%s' %\n                    base64.b64encode(bytes(str(random.randint(1, 0x40000000)),\n                                     'ascii')))", "response": "Returns a random client identifier"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a unique identifier for the current machine process and thread.", "response": "def make_fixed_client_id(self):\n        \"\"\"\n        Returns a unique identifier for the current machine/process/thread.\n        \"\"\"\n        machine = platform.node()\n        process = os.getpid()\n        thread = threading.currentThread().getName()\n        return base64.b64encode('%s|%s|%s' % (machine, process, thread))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch an object from the cache.", "response": "def get(self, robj, r=None, pr=None, timeout=None, basic_quorum=None,\n            notfound_ok=None, head_only=False):\n        \"\"\"\n        Fetches an object.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore an object in the cache.", "response": "def put(self, robj, w=None, dw=None, pw=None, return_body=None,\n            if_none_match=None, timeout=None):\n        \"\"\"\n        Stores an object.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete an object from the cache.", "response": "def delete(self, robj, rw=None, r=None, w=None, dw=None, pr=None,\n               pw=None, timeout=None):\n        \"\"\"\n        Deletes an object.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_index(self, bucket, index, startkey, endkey=None,\n                  return_terms=None, max_results=None, continuation=None,\n                  timeout=None, term_regex=None):\n        \"\"\"\n        Performs a secondary index query.\n        \"\"\"\n        raise NotImplementedError", "response": "Performs a secondary index query."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream_index(self, bucket, index, startkey, endkey=None,\n                     return_terms=None, max_results=None, continuation=None,\n                     timeout=None):\n        \"\"\"\n        Streams a secondary index query.\n        \"\"\"\n        raise NotImplementedError", "response": "Streams a secondary index query."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates a counter by the given value.", "response": "def update_counter(self, bucket, key, value, w=None, dw=None, pw=None,\n                       returnvalue=False):\n        \"\"\"\n        Updates a counter by the given value.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfetches a Riak Datatype object from a bucket.", "response": "def fetch_datatype(self, bucket, key, r=None, pr=None, basic_quorum=None,\n                       notfound_ok=None, timeout=None, include_context=None):\n        \"\"\"\n        Fetches a Riak Datatype.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates a Riak Datatype by sending local operations to the server.", "response": "def update_datatype(self, datatype, w=None, dw=None, pw=None,\n                        return_body=None, timeout=None, include_context=None):\n        \"\"\"\n        Updates a Riak Datatype by sending local operations to the server.\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _search_mapred_emu(self, index, query):\n        phases = []\n        if not self.phaseless_mapred():\n            phases.append({'language': 'erlang',\n                           'module': 'riak_kv_mapreduce',\n                           'function': 'reduce_identity',\n                           'keep': True})\n        mr_result = self.mapred({'module': 'riak_search',\n                                 'function': 'mapred_search',\n                                 'arg': [index, query]},\n                                phases)\n        result = {'num_found': len(mr_result),\n                  'max_score': 0.0,\n                  'docs': []}\n        for bucket, key, data in mr_result:\n            if u'score' in data and data[u'score'][0] > result['max_score']:\n                result['max_score'] = data[u'score'][0]\n            result['docs'].append({u'id': key})\n        return result", "response": "Emulates a search request via MapReduce. Used in the case where the transport supports MapReduce but has no native\n        search capability."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nemulates a secondary index request via MapReduce. Used in the case where the transport supports MapReduce but has no native secondary index query capability.", "response": "def _get_index_mapred_emu(self, bucket, index, startkey, endkey=None):\n        \"\"\"\n        Emulates a secondary index request via MapReduce. Used in the\n        case where the transport supports MapReduce but has no native\n        secondary index query capability.\n        \"\"\"\n        phases = []\n        if not self.phaseless_mapred():\n            phases.append({'language': 'erlang',\n                           'module': 'riak_kv_mapreduce',\n                           'function': 'reduce_identity',\n                           'keep': True})\n        if endkey:\n            result = self.mapred({'bucket': bucket,\n                                  'index': index,\n                                  'start': startkey,\n                                  'end': endkey},\n                                 phases)\n        else:\n            result = self.mapred({'bucket': bucket,\n                                  'index': index,\n                                  'key': startkey},\n                                 phases)\n        return [key for resultbucket, key in result]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_body(self, robj, response, expected_statuses):\n        # If no response given, then return.\n        if response is None:\n            return None\n\n        status, headers, data = response\n\n        # Check if the server is down(status==0)\n        if not status:\n            m = 'Could not contact Riak Server: http://{0}:{1}!'.format(\n                self._node.host, self._node.http_port)\n            raise RiakError(m)\n\n        # Make sure expected code came back\n        self.check_http_code(status, expected_statuses)\n\n        if 'x-riak-vclock' in headers:\n            robj.vclock = VClock(headers['x-riak-vclock'], 'base64')\n\n        # If 404(Not Found), then clear the object.\n        if status == 404:\n            robj.siblings = []\n            return None\n        # If 201 Created, we need to extract the location and set the\n        # key on the object.\n        elif status == 201:\n            robj.key = headers['location'].strip().split('/')[-1]\n        # If 300(Siblings), apply the siblings to the object\n        elif status == 300:\n            ctype, params = parse_header(headers['content-type'])\n            if ctype == 'multipart/mixed':\n                if six.PY3:\n                    data = bytes_to_str(data)\n                boundary = re.compile('\\r?\\n--%s(?:--)?\\r?\\n' %\n                                      re.escape(params['boundary']))\n                parts = [message_from_string(p)\n                         for p in re.split(boundary, data)[1:-1]]\n                robj.siblings = [self._parse_sibling(RiakContent(robj),\n                                                     part.items(),\n                                                     part.get_payload())\n                                 for part in parts]\n\n                # Invoke sibling-resolution logic\n                if robj.resolver is not None:\n                    robj.resolver(robj)\n\n                return robj\n            else:\n                raise Exception('unexpected sibling response format: {0}'.\n                                format(ctype))\n\n        robj.siblings = [self._parse_sibling(RiakContent(robj),\n                                             headers.items(),\n                                             data)]\n\n        return robj", "response": "Parse the response body and populate the object with the information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_sibling(self, sibling, headers, data):\n\n        sibling.exists = True\n\n        # Parse the headers...\n        for header, value in headers:\n            header = header.lower()\n            if header == 'content-type':\n                sibling.content_type, sibling.charset = \\\n                    self._parse_content_type(value)\n            elif header == 'etag':\n                sibling.etag = value\n            elif header == 'link':\n                sibling.links = self._parse_links(value)\n            elif header == 'last-modified':\n                sibling.last_modified = mktime_tz(parsedate_tz(value))\n            elif header.startswith('x-riak-meta-'):\n                metakey = header.replace('x-riak-meta-', '')\n                sibling.usermeta[metakey] = value\n            elif header.startswith('x-riak-index-'):\n                field = header.replace('x-riak-index-', '')\n                reader = csv.reader([value], skipinitialspace=True)\n                for line in reader:\n                    for token in line:\n                        token = decode_index_value(field, token)\n                        sibling.add_index(field, token)\n            elif header == 'x-riak-deleted':\n                sibling.exists = False\n\n        sibling.encoded_data = data\n\n        return sibling", "response": "Parses a single sibling from a response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_link_header(self, link):\n        try:\n            bucket, key, tag = link\n        except ValueError:\n            raise RiakError(\"Invalid link tuple %s\" % link)\n        tag = tag if tag is not None else bucket\n        url = self.object_path(bucket, key)\n        header = '<%s>; riaktag=\"%s\"' % (url, tag)\n        return header", "response": "Convert the link tuple to a link header string. Used internally."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the headers for a PUT request.", "response": "def _build_put_headers(self, robj, if_none_match=False):\n        \"\"\"Build the headers for a POST/PUT request.\"\"\"\n\n        # Construct the headers...\n        if robj.charset is not None:\n            content_type = ('%s; charset=\"%s\"' %\n                            (robj.content_type, robj.charset))\n        else:\n            content_type = robj.content_type\n\n        headers = MultiDict({'Content-Type': content_type,\n                             'X-Riak-ClientId': self._client_id})\n\n        # Add the vclock if it exists...\n        if robj.vclock is not None:\n            headers['X-Riak-Vclock'] = robj.vclock.encode('base64')\n\n        # Create the header from metadata\n        self._add_links_for_riak_object(robj, headers)\n\n        for key in robj.usermeta.keys():\n            headers['X-Riak-Meta-%s' % key] = robj.usermeta[key]\n\n        for field, value in robj.indexes:\n            key = 'X-Riak-Index-%s' % field\n            if key in headers:\n                headers[key] += \", \" + str(value)\n            else:\n                headers[key] = str(value)\n\n        if if_none_match:\n            headers['If-None-Match'] = '*'\n\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize_json_search_response(self, json):\n        result = {}\n        if 'facet_counts' in json:\n            result['facet_counts'] = json[u'facet_counts']\n        if 'grouped' in json:\n            result['grouped'] = json[u'grouped']\n        if 'stats' in json:\n            result['stats'] = json[u'stats']\n        if u'response' in json:\n            result['num_found'] = json[u'response'][u'numFound']\n            result['max_score'] = float(json[u'response'][u'maxScore'])\n            docs = []\n            for doc in json[u'response'][u'docs']:\n                resdoc = {}\n                if u'_yz_rk' in doc:\n                    # Is this a Riak 2.0 result?\n                    resdoc = doc\n                else:\n                    # Riak Search 1.0 Legacy assumptions about format\n                    resdoc[u'id'] = doc[u'id']\n                    if u'fields' in doc:\n                        for k, v in six.iteritems(doc[u'fields']):\n                            resdoc[k] = v\n                docs.append(resdoc)\n            result['docs'] = docs\n        return result", "response": "Normalizes a JSON search response so that PB and HTTP have the same return value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _normalize_xml_search_response(self, xml):\n        target = XMLSearchResult()\n        parser = ElementTree.XMLParser(target=target)\n        parser.feed(xml)\n        return parser.close()", "response": "Normalizes an XML search response so that PB and HTTP have the\n        same return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the content - type header into two parts.", "response": "def _parse_content_type(self, value):\n        \"\"\"\n        Split the content-type header into two parts:\n        1) Actual main/sub encoding type\n        2) charset\n\n        :param value: Complete MIME content-type string\n        \"\"\"\n        content_type, params = parse_header(value)\n        if 'charset' in params:\n            charset = params['charset']\n        else:\n            charset = None\n        return content_type, charset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the socket.", "response": "def connect(self):\n        \"\"\"\n        Set TCP_NODELAY on socket\n        \"\"\"\n        HTTPConnection.connect(self)\n        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self):\n        sock = socket.create_connection((self.host, self.port), self.timeout)\n        if not USE_STDLIB_SSL:\n            ssl_ctx = configure_pyopenssl_context(self.credentials)\n\n            # attempt to upgrade the socket to TLS\n            cxn = OpenSSL.SSL.Connection(ssl_ctx, sock)\n            cxn.set_connect_state()\n            while True:\n                try:\n                    cxn.do_handshake()\n                except OpenSSL.SSL.WantReadError:\n                    select.select([sock], [], [])\n                    continue\n                except OpenSSL.SSL.Error as e:\n                    raise SecurityError('bad handshake - ' + str(e))\n                break\n\n            self.sock = RiakWrappedSocket(cxn, sock)\n            self.credentials._check_revoked_cert(self.sock)\n        else:\n            ssl_ctx = configure_ssl_context(self.credentials)\n            if self.timeout is not None:\n                sock.settimeout(self.timeout)\n            self.sock = ssl.SSLSocket(sock=sock,\n                                      keyfile=self.credentials.pkey_file,\n                                      certfile=self.credentials.cert_file,\n                                      cert_reqs=ssl.CERT_REQUIRED,\n                                      ca_certs=self.credentials.cacert_file,\n                                      ciphers=self.credentials.ciphers,\n                                      server_hostname=self.host)\n            self.sock.context = ssl_ctx", "response": "Connect to a given host on a given port using PyOpenSSL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retryable(fn, protocol=None):\n    def wrapper(self, *args, **kwargs):\n        pool = self._choose_pool(protocol)\n\n        def thunk(transport):\n            return fn(self, transport, *args, **kwargs)\n\n        return self._with_retries(pool, thunk)\n\n    wrapper.__doc__ = fn.__doc__\n    wrapper.__repr__ = fn.__repr__\n\n    return wrapper", "response": "Decorator that returns a function that can be retried according to the set\n    : attr : RiakClient. retries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _with_retries(self, pool, fn):\n        skip_nodes = []\n\n        def _skip_bad_nodes(transport):\n            return transport._node not in skip_nodes\n\n        retry_count = self.retries - 1\n        first_try = True\n        current_try = 0\n        while True:\n            try:\n                with pool.transaction(\n                        _filter=_skip_bad_nodes,\n                        yield_resource=True) as resource:\n                    transport = resource.object\n                    try:\n                        return fn(transport)\n                    except (IOError, HTTPException, ConnectionClosed) as e:\n                        resource.errored = True\n                        if _is_retryable(e):\n                            transport._node.error_rate.incr(1)\n                            skip_nodes.append(transport._node)\n                            if first_try:\n                                continue\n                            else:\n                                raise BadResource(e)\n                        else:\n                            raise\n            except BadResource as e:\n                if current_try < retry_count:\n                    resource.errored = True\n                    current_try += 1\n                    continue\n                else:\n                    # Re-raise the inner exception\n                    raise e.args[0]\n            finally:\n                first_try = False", "response": "Performs the passed function with retries against the given connection pool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _choose_pool(self, protocol=None):\n        if not protocol:\n            protocol = self.protocol\n        if protocol == 'http':\n            pool = self._http_pool\n        elif protocol == 'tcp' or protocol == 'pbc':\n            pool = self._tcp_pool\n        else:\n            raise ValueError(\"invalid protocol %s\" % protocol)\n        if pool is None or self._closed:\n            # NB: GH-500, this can happen if client is closed\n            raise RuntimeError(\"Client is closed.\")\n        return pool", "response": "Selects a connection pool according to the default protocol\n        and the passed one."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default_encoder(obj):\n    if isinstance(obj, bytes):\n        return json.dumps(bytes_to_str(obj),\n                          ensure_ascii=False).encode(\"utf-8\")\n    else:\n        return json.dumps(obj, ensure_ascii=False).encode(\"utf-8\")", "response": "Default encoder for JSON datatypes which returns UTF - 8 encoded\n    json instead of the default bloated backslash u XXXX escaped ASCII strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bucket(self, name, bucket_type='default'):\n        if not isinstance(name, string_types):\n            raise TypeError('Bucket name must be a string')\n\n        if isinstance(bucket_type, string_types):\n            bucket_type = self.bucket_type(bucket_type)\n        elif not isinstance(bucket_type, BucketType):\n            raise TypeError('bucket_type must be a string '\n                            'or riak.bucket.BucketType')\n\n        b = RiakBucket(self, name, bucket_type)\n        return self._setdefault_handle_none(\n                self._buckets, (bucket_type, name), b)", "response": "Get the bucket by the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bucket_type(self, name):\n        if not isinstance(name, string_types):\n            raise TypeError('BucketType name must be a string')\n\n        btype = BucketType(self, name)\n        return self._setdefault_handle_none(\n                self._bucket_types, name, btype)", "response": "Returns the object with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the table with the specified name.", "response": "def table(self, name):\n        \"\"\"\n        Gets the table by the specified name. Tables do\n        not always exist (unlike buckets), but this will always return\n        a :class:`Table <riak.table.Table>` object.\n\n        :param name: the table name\n        :type name: str\n        :rtype: :class:`Table <riak.table.Table>`\n        \"\"\"\n        if not isinstance(name, string_types):\n            raise TypeError('Table name must be a string')\n\n        if name in self._tables:\n            return self._tables[name]\n        else:\n            table = Table(self, name)\n            self._tables[name] = table\n            return table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        if not self._closed:\n            self._closed = True\n            self._stop_multi_pools()\n            if self._http_pool is not None:\n                self._http_pool.clear()\n                self._http_pool = None\n            if self._tcp_pool is not None:\n                self._tcp_pool.clear()\n                self._tcp_pool = None", "response": "Close all of the resources in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate security credentials if necessary.", "response": "def _create_credentials(self, n):\n        \"\"\"\n        Create security credentials, if necessary.\n        \"\"\"\n        if not n:\n            return n\n        elif isinstance(n, SecurityCreds):\n            return n\n        elif isinstance(n, dict):\n            return SecurityCreds(**n)\n        else:\n            raise TypeError(\"%s is not a valid security configuration\"\n                            % repr(n))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _choose_node(self, nodes=None):\n        if not nodes:\n            nodes = self.nodes\n\n        # Prefer nodes which have gone a reasonable time without\n        # errors\n        def _error_rate(node):\n            return node.error_rate.value()\n\n        good = [n for n in nodes if _error_rate(n) < 0.1]\n\n        if len(good) is 0:\n            # Fall back to a minimally broken node\n            return min(nodes, key=_error_rate)\n        else:\n            return random.choice(good)", "response": "Returns a random node from the list of nodes in the client."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a HTTP request and return a 3 - tuple containing the response status and response body.", "response": "def _request(self, method, uri, headers={}, body='', stream=False):\n        \"\"\"\n        Given a Method, URL, Headers, and Body, perform and HTTP\n        request, and return a 3-tuple containing the response status,\n        response headers (as httplib.HTTPMessage), and response body.\n        \"\"\"\n        response = None\n        headers.setdefault('Accept',\n                           'multipart/mixed, application/json, */*;q=0.5')\n\n        if self._client._credentials:\n            self._security_auth_headers(self._client._credentials.username,\n                                        self._client._credentials.password,\n                                        headers)\n\n        try:\n            self._connection.request(method, uri, body, headers)\n            try:\n                response = self._connection.getresponse(buffering=True)\n            except TypeError:\n                response = self._connection.getresponse()\n\n            if stream:\n                # The caller is responsible for fully reading the\n                # response and closing it when streaming.\n                response_body = response\n            else:\n                response_body = response.read()\n        finally:\n            if response and not stream:\n                response.close()\n\n        return response.status, response.msg, response_body"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _connect(self):\n        timeout = None\n        if self._options is not None and 'timeout' in self._options:\n            timeout = self._options['timeout']\n\n        if self._client._credentials:\n            self._connection = self._connection_class(\n                host=self._node.host,\n                port=self._node.http_port,\n                credentials=self._client._credentials,\n                timeout=timeout)\n        else:\n            self._connection = self._connection_class(\n                    host=self._node.host,\n                    port=self._node.http_port,\n                    timeout=timeout)\n        # Forces the population of stats and resources before any\n        # other requests are made.\n        self.server_version", "response": "Connect to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _security_auth_headers(self, username, password, headers):\n        userColonPassword = username + \":\" + password\n        b64UserColonPassword = base64. \\\n            b64encode(str_to_bytes(userColonPassword)).decode(\"ascii\")\n        headers['Authorization'] = 'Basic %s' % b64UserColonPassword", "response": "Add in the requisite HTTP Authentication Headers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying a timeseries table.", "response": "def query(self, query, interpolations=None):\n        \"\"\"\n        Queries a timeseries table.\n\n        :param query: The timeseries query.\n        :type query: string\n        :rtype: :class:`TsObject <riak.ts_object.TsObject>`\n        \"\"\"\n        return self._client.ts_query(self, query, interpolations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getConfigDirectory():\n\t\tif platform.system() == 'Windows':\n\t\t\treturn os.path.join(os.environ['APPDATA'], 'ue4cli')\n\t\telse:\n\t\t\treturn os.path.join(os.environ['HOME'], '.config', 'ue4cli')", "response": "Determines the platform - specific config directory location for ue4cli"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the config data value for the specified dictionary key.", "response": "def setConfigKey(key, value):\n\t\t\"\"\"\n\t\tSets the config data value for the specified dictionary key\n\t\t\"\"\"\n\t\tconfigFile = ConfigurationManager._configFile()\n\t\treturn JsonDataManager(configFile).setKey(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clearCache():\n\t\tif os.path.exists(CachedDataManager._cacheDir()) == True:\n\t\t\tshutil.rmtree(CachedDataManager._cacheDir())", "response": "Clears any cached data that is not yet stored."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getCachedDataKey(engineVersionHash, key):\n\t\tcacheFile = CachedDataManager._cacheFileForHash(engineVersionHash)\n\t\treturn JsonDataManager(cacheFile).getKey(key)", "response": "Retrieves the cached data value for the specified engine version hash and dictionary key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setCachedDataKey(engineVersionHash, key, value):\n\t\tcacheFile = CachedDataManager._cacheFileForHash(engineVersionHash)\n\t\treturn JsonDataManager(cacheFile).setKey(key, value)", "response": "Sets the value of the specified key in the cached data file for the specified engine version hash and dictionary key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites data to a file", "response": "def writeFile(filename, data):\n\t\t\"\"\"\n\t\tWrites data to a file\n\t\t\"\"\"\n\t\twith open(filename, 'wb') as f:\n\t\t\tf.write(data.encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patchFile(filename, replacements):\n\t\tpatched = Utility.readFile(filename)\n\t\t\n\t\t# Perform each of the replacements in the supplied dictionary\n\t\tfor key in replacements:\n\t\t\tpatched = patched.replace(key, replacements[key])\n\t\t\n\t\tUtility.writeFile(filename, patched)", "response": "Applies the supplied list of replacements to a file\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nescape a filesystem path for use as a command - line argument.", "response": "def escapePathForShell(path):\n\t\t\"\"\"\n\t\tEscapes a filesystem path for use as a command-line argument\n\t\t\"\"\"\n\t\tif platform.system() == 'Windows':\n\t\t\treturn '\"{}\"'.format(path.replace('\"', '\"\"'))\n\t\telse:\n\t\t\treturn shellescape.quote(path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stripArgs(args, blacklist):\n\t\tblacklist = [b.lower() for b in blacklist]\n\t\treturn list([arg for arg in args if arg.lower() not in blacklist])", "response": "Removes any arguments in the supplied list that are contained in the specified blacklist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting a child process and captures its output", "response": "def capture(command, input=None, cwd=None, shell=False, raiseOnError=False):\n\t\t\"\"\"\n\t\tExecutes a child process and captures its output\n\t\t\"\"\"\n\t\t\n\t\t# Attempt to execute the child process\n\t\tproc = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, shell=shell, universal_newlines=True)\n\t\t(stdout, stderr) = proc.communicate(input)\n\t\t\n\t\t# If the child process failed and we were asked to raise an exception, do so\n\t\tif raiseOnError == True and proc.returncode != 0:\n\t\t\traise Exception(\n\t\t\t\t'child process ' + str(command) +\n\t\t\t\t' failed with exit code ' + str(proc.returncode) +\n\t\t\t\t'\\nstdout: \"' + stdout + '\"' +\n\t\t\t\t'\\nstderr: \"' + stderr + '\"'\n\t\t\t)\n\t\t\n\t\treturn CommandOutput(proc.returncode, stdout, stderr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(command, cwd=None, shell=False, raiseOnError=False):\n\t\treturncode = subprocess.call(command, cwd=cwd, shell=shell)\n\t\tif raiseOnError == True and returncode != 0:\n\t\t\traise Exception('child process ' + str(command) + ' failed with exit code ' + str(returncode))\n\t\treturn returncode", "response": "Executes a child process and waits for it to complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a user - specified directory as the root engine directory overriding any auto - detections.", "response": "def setEngineRootOverride(self, rootDir):\n\t\t\"\"\"\n\t\tSets a user-specified directory as the root engine directory, overriding any auto-detection\n\t\t\"\"\"\n\t\t\t\t\n\t\t# Set the new root directory\n\t\tConfigurationManager.setConfigKey('rootDirOverride', os.path.abspath(rootDir))\n\t\t\n\t\t# Check that the specified directory is valid and warn the user if it is not\n\t\ttry:\n\t\t\tself.getEngineVersion()\n\t\texcept:\n\t\t\tprint('Warning: the specified directory does not appear to contain a valid version of the Unreal Engine.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the root directory location of the latest installed version of UE4", "response": "def getEngineRoot(self):\n\t\t\"\"\"\n\t\tReturns the root directory location of the latest installed version of UE4\n\t\t\"\"\"\n\t\tif not hasattr(self, '_engineRoot'):\n\t\t\tself._engineRoot = self._getEngineRoot()\n\t\treturn self._engineRoot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getEngineVersion(self, outputFormat = 'full'):\n\t\tversion = self._getEngineVersionDetails()\n\t\tformats = {\n\t\t\t'major': version['MajorVersion'],\n\t\t\t'minor': version['MinorVersion'],\n\t\t\t'patch': version['PatchVersion'],\n\t\t\t'full': '{}.{}.{}'.format(version['MajorVersion'], version['MinorVersion'], version['PatchVersion']),\n\t\t\t'short': '{}.{}'.format(version['MajorVersion'], version['MinorVersion'])\n\t\t}\n\t\t\n\t\t# Verify that the requested output format is valid\n\t\tif outputFormat not in formats:\n\t\t\traise Exception('unreconised version output format \"{}\"'.format(outputFormat))\n\t\t\n\t\treturn formats[outputFormat]", "response": "Returns the version number of the latest installed version of UE4"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the latest installed version of UE4 changelist identifier for the latest installed version of UE4 changelist", "response": "def getEngineChangelist(self):\n\t\t\"\"\"\n\t\tReturns the compatible Perforce changelist identifier for the latest installed version of UE4\n\t\t\"\"\"\n\t\t\n\t\t# Newer versions of the engine use the key \"CompatibleChangelist\", older ones use \"Changelist\"\n\t\tversion = self._getEngineVersionDetails()\n\t\tif 'CompatibleChangelist' in version:\n\t\t\treturn int(version['CompatibleChangelist'])\n\t\telse:\n\t\t\treturn int(version['Changelist'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isInstalledBuild(self):\n\t\tsentinelFile = os.path.join(self.getEngineRoot(), 'Engine', 'Build', 'InstalledBuild.txt')\n\t\treturn os.path.exists(sentinelFile)", "response": "Determines if the Engine is an Installed Build"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the location of the UE4Editor binary", "response": "def getEditorBinary(self, cmdVersion=False):\n\t\t\"\"\"\n\t\tDetermines the location of the UE4Editor binary\n\t\t\"\"\"\n\t\treturn os.path.join(self.getEngineRoot(), 'Engine', 'Binaries', self.getPlatformIdentifier(), 'UE4Editor' + self._editorPathSuffix(cmdVersion))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects the. uproject descriptor file for the Unreal project in the specified directory.", "response": "def getProjectDescriptor(self, dir):\n\t\t\"\"\"\n\t\tDetects the .uproject descriptor file for the Unreal project in the specified directory\n\t\t\"\"\"\n\t\tfor project in glob.glob(os.path.join(dir, '*.uproject')):\n\t\t\treturn os.path.realpath(project)\n\t\t\n\t\t# No project detected\n\t\traise UnrealManagerException('could not detect an Unreal project in the current directory')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetect the. uplugin descriptor file for the Unreal plugin in the specified directory.", "response": "def getPluginDescriptor(self, dir):\n\t\t\"\"\"\n\t\tDetects the .uplugin descriptor file for the Unreal plugin in the specified directory\n\t\t\"\"\"\n\t\tfor plugin in glob.glob(os.path.join(dir, '*.uplugin')):\n\t\t\treturn os.path.realpath(plugin)\n\t\t\n\t\t# No plugin detected\n\t\traise UnrealManagerException('could not detect an Unreal plugin in the current directory')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the descriptor file for the Unreal project or Unreal plugin in the specified directory.", "response": "def getDescriptor(self, dir):\n\t\t\"\"\"\n\t\tDetects the descriptor file for either an Unreal project or an Unreal plugin in the specified directory\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.getProjectDescriptor(dir)\n\t\texcept:\n\t\t\ttry:\n\t\t\t\treturn self.getPluginDescriptor(dir)\n\t\t\texcept:\n\t\t\t\traise UnrealManagerException('could not detect an Unreal project or plugin in the directory \"{}\"'.format(dir))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listThirdPartyLibs(self, configuration = 'Development'):\n\t\tinterrogator = self._getUE4BuildInterrogator()\n\t\treturn interrogator.list(self.getPlatformIdentifier(), configuration, self._getLibraryOverrides())", "response": "Lists the supported Unreal - bundled third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getThirdpartyLibs(self, libs, configuration = 'Development', includePlatformDefaults = True):\n\t\tif includePlatformDefaults == True:\n\t\t\tlibs = self._defaultThirdpartyLibs() + libs\n\t\tinterrogator = self._getUE4BuildInterrogator()\n\t\treturn interrogator.interrogate(self.getPlatformIdentifier(), configuration, libs, self._getLibraryOverrides())", "response": "Retrieves ThirdPartyLibraryDetails instance for Unreal - bundled versions of the specified third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getThirdPartyLibCompilerFlags(self, libs):\n\t\tfmt = PrintingFormat.singleLine()\n\t\tif libs[0] == '--multiline':\n\t\t\tfmt = PrintingFormat.multiLine()\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tplatformDefaults = True\n\t\tif libs[0] == '--nodefaults':\n\t\t\tplatformDefaults = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tdetails = self.getThirdpartyLibs(libs, includePlatformDefaults=platformDefaults)\n\t\treturn details.getCompilerFlags(self.getEngineRoot(), fmt)", "response": "Retrieves the compiler flags for building against the Unreal - bundled versions of the specified third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getThirdPartyLibLinkerFlags(self, libs):\n\t\tfmt = PrintingFormat.singleLine()\n\t\tif libs[0] == '--multiline':\n\t\t\tfmt = PrintingFormat.multiLine()\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tincludeLibs = True\n\t\tif (libs[0] == '--flagsonly'):\n\t\t\tincludeLibs = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tplatformDefaults = True\n\t\tif libs[0] == '--nodefaults':\n\t\t\tplatformDefaults = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tdetails = self.getThirdpartyLibs(libs, includePlatformDefaults=platformDefaults)\n\t\treturn details.getLinkerFlags(self.getEngineRoot(), fmt, includeLibs)", "response": "Retrieves the linker flags for building against the Unreal - bundled versions of the specified third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getThirdPartyLibCmakeFlags(self, libs):\n\t\tfmt = PrintingFormat.singleLine()\n\t\tif libs[0] == '--multiline':\n\t\t\tfmt = PrintingFormat.multiLine()\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tplatformDefaults = True\n\t\tif libs[0] == '--nodefaults':\n\t\t\tplatformDefaults = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tdetails = self.getThirdpartyLibs(libs, includePlatformDefaults=platformDefaults)\n\t\tCMakeCustomFlags.processLibraryDetails(details)\n\t\treturn details.getCMakeFlags(self.getEngineRoot(), fmt)", "response": "Retrieves the CMake invocation flags for building against the Unreal - bundled versions of the specified third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getThirdPartyLibIncludeDirs(self, libs):\n\t\tplatformDefaults = True\n\t\tif libs[0] == '--nodefaults':\n\t\t\tplatformDefaults = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tdetails = self.getThirdpartyLibs(libs, includePlatformDefaults=platformDefaults)\n\t\treturn details.getIncludeDirectories(self.getEngineRoot(), delimiter='\\n')", "response": "Retrieves the list of include directories for building against the Unreal - bundled versions of the specified third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the list of library files for building against the Unreal - bundled versions of the specified third - party libraries.", "response": "def getThirdPartyLibFiles(self, libs):\n\t\t\"\"\"\n\t\tRetrieves the list of library files for building against the Unreal-bundled versions of the specified third-party libraries\n\t\t\"\"\"\n\t\tplatformDefaults = True\n\t\tif libs[0] == '--nodefaults':\n\t\t\tplatformDefaults = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tdetails = self.getThirdpartyLibs(libs, includePlatformDefaults=platformDefaults)\n\t\treturn details.getLibraryFiles(self.getEngineRoot(), delimiter='\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getThirdPartyLibDefinitions(self, libs):\n\t\tplatformDefaults = True\n\t\tif libs[0] == '--nodefaults':\n\t\t\tplatformDefaults = False\n\t\t\tlibs = libs[1:]\n\t\t\n\t\tdetails = self.getThirdpartyLibs(libs, includePlatformDefaults=platformDefaults)\n\t\treturn details.getPreprocessorDefinitions(self.getEngineRoot(), delimiter='\\n')", "response": "Retrieves the list of preprocessor definitions for building against the Unreal - bundled versions of the specified third - party libraries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the project files for the Unreal project in the specified directory.", "response": "def generateProjectFiles(self, dir=os.getcwd(), args=[]):\n\t\t\"\"\"\n\t\tGenerates IDE project files for the Unreal project in the specified directory\n\t\t\"\"\"\n\t\t\n\t\t# If the project is a pure Blueprint project, then we cannot generate project files\n\t\tif os.path.exists(os.path.join(dir, 'Source')) == False:\n\t\t\tUtility.printStderr('Pure Blueprint project, nothing to generate project files for.')\n\t\t\treturn\n\t\t\n\t\t# Generate the project files\n\t\tgenScript = self.getGenerateScript()\n\t\tprojectFile = self.getProjectDescriptor(dir)\n\t\tUtility.run([genScript, '-project=' + projectFile, '-game', '-engine'] + args, cwd=os.path.dirname(genScript), raiseOnError=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cleanDescriptor(self, dir=os.getcwd()):\n\t\t\n\t\t# Verify that an Unreal project or plugin exists in the specified directory\n\t\tdescriptor = self.getDescriptor(dir)\n\t\t\n\t\t# Because performing a clean will also delete the engine build itself when using\n\t\t# a source build, we simply delete the `Binaries` and `Intermediate` directories\n\t\tshutil.rmtree(os.path.join(dir, 'Binaries'), ignore_errors=True)\n\t\tshutil.rmtree(os.path.join(dir, 'Intermediate'), ignore_errors=True)\n\t\t\n\t\t# If we are cleaning a project, also clean any plugins\n\t\tif self.isProject(descriptor):\n\t\t\tprojectPlugins = glob.glob(os.path.join(dir, 'Plugins', '*'))\n\t\t\tfor pluginDir in projectPlugins:\n\t\t\t\tself.cleanDescriptor(pluginDir)", "response": "Cleans the build artifacts for the Unreal project or plugin in the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef buildDescriptor(self, dir=os.getcwd(), configuration='Development', args=[], suppressOutput=False):\n\t\t\n\t\t# Verify that an Unreal project or plugin exists in the specified directory\n\t\tdescriptor = self.getDescriptor(dir)\n\t\tdescriptorType = 'project' if self.isProject(descriptor) else 'plugin'\n\t\t\n\t\t# If the project or plugin is Blueprint-only, there is no C++ code to build\n\t\tif os.path.exists(os.path.join(dir, 'Source')) == False:\n\t\t\tUtility.printStderr('Pure Blueprint {}, nothing to build.'.format(descriptorType))\n\t\t\treturn\n\t\t\n\t\t# Verify that the specified build configuration is valid\n\t\tif configuration not in self.validBuildConfigurations():\n\t\t\traise UnrealManagerException('invalid build configuration \"' + configuration + '\"')\n\t\t\n\t\t# Generate the arguments to pass to UBT\n\t\ttarget = self.getDescriptorName(descriptor) + 'Editor' if self.isProject(descriptor) else 'UE4Editor'\n\t\tbaseArgs = ['-{}='.format(descriptorType) + descriptor]\n\t\t\n\t\t# Perform the build\n\t\tself._runUnrealBuildTool(target, self.getPlatformIdentifier(), configuration, baseArgs + args, capture=suppressOutput)", "response": "Builds the editor modules for the Unreal project or plugin in the specified directory using the specified build configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the editor for the Unreal project in the specified directory.", "response": "def runEditor(self, dir=os.getcwd(), debug=False, args=[]):\n\t\t\"\"\"\n\t\tRuns the editor for the Unreal project in the specified directory (or without a project if dir is None)\n\t\t\"\"\"\n\t\tprojectFile = self.getProjectDescriptor(dir) if dir is not None else ''\n\t\textraFlags = ['-debug'] + args if debug == True else args\n\t\tUtility.run([self.getEditorBinary(True), projectFile, '-stdout', '-FullStdOutLogOutput'] + extraFlags, raiseOnError=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the Unreal Automation Tool with the supplied arguments.", "response": "def runUAT(self, args):\n\t\t\"\"\"\n\t\tRuns the Unreal Automation Tool with the supplied arguments\n\t\t\"\"\"\n\t\tUtility.run([self.getRunUATScript()] + args, cwd=self.getEngineRoot(), raiseOnError=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npackaging a build of the Unreal project in the specified directory using common packaging options", "response": "def packageProject(self, dir=os.getcwd(), configuration='Shipping', extraArgs=[]):\n\t\t\"\"\"\n\t\tPackages a build of the Unreal project in the specified directory, using common packaging options\n\t\t\"\"\"\n\t\t\n\t\t# Verify that the specified build configuration is valid\n\t\tif configuration not in self.validBuildConfigurations():\n\t\t\traise UnrealManagerException('invalid build configuration \"' + configuration + '\"')\n\t\t\n\t\t# Strip out the `-NoCompileEditor` flag if the user has specified it, since the Development version\n\t\t# of the Editor modules for the project are needed in order to run the commandlet that cooks content\n\t\textraArgs = Utility.stripArgs(extraArgs, ['-nocompileeditor'])\n\t\t\n\t\t# Prevent the user from specifying multiple `-platform=` or `-targetplatform=` arguments,\n\t\t# and use the current host platform if no platform argument was explicitly specified\n\t\tplatformArgs = Utility.findArgs(extraArgs, ['-platform=', '-targetplatform='])\n\t\tplatform = Utility.getArgValue(platformArgs[0]) if len(platformArgs) > 0 else self.getPlatformIdentifier()\n\t\textraArgs = Utility.stripArgs(extraArgs, platformArgs) + ['-platform={}'.format(platform)]\n\t\t\n\t\t# If we are packaging a Shipping build, do not include debug symbols\n\t\tif configuration == 'Shipping':\n\t\t\textraArgs.append('-nodebuginfo')\n\t\t\n\t\t# Do not create a .pak file when packaging for HTML5\n\t\tpakArg = '-package' if platform.upper() == 'HTML5' else '-pak'\n\t\t\n\t\t# Invoke UAT to package the build\n\t\tdistDir = os.path.join(os.path.abspath(dir), 'dist')\n\t\tself.runUAT([\n\t\t\t'BuildCookRun',\n\t\t\t'-utf8output',\n\t\t\t'-clientconfig=' + configuration,\n\t\t\t'-serverconfig=' + configuration,\n\t\t\t'-project=' + self.getProjectDescriptor(dir),\n\t\t\t'-noP4',\n\t\t\t'-cook',\n\t\t\t'-allmaps',\n\t\t\t'-build',\n\t\t\t'-stage',\n\t\t\t'-prereqs',\n\t\t\tpakArg,\n\t\t\t'-archive',\n\t\t\t'-archivedirectory=' + distDir\n\t\t] + extraArgs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef packagePlugin(self, dir=os.getcwd(), extraArgs=[]):\n\t\t\n\t\t# Invoke UAT to package the build\n\t\tdistDir = os.path.join(os.path.abspath(dir), 'dist')\n\t\tself.runUAT([\n\t\t\t'BuildPlugin',\n\t\t\t'-Plugin=' + self.getPluginDescriptor(dir),\n\t\t\t'-Package=' + distDir\n\t\t] + extraArgs)", "response": "Packages a build of the Unreal plugin in the specified directory suitable for use as a prebuilt Engine module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef packageDescriptor(self, dir=os.getcwd(), args=[]):\n\t\t\n\t\t# Verify that an Unreal project or plugin exists in the specified directory\n\t\tdescriptor = self.getDescriptor(dir)\n\t\t\n\t\t# Perform the packaging step\n\t\tif self.isProject(descriptor):\n\t\t\tself.packageProject(dir, args[0] if len(args) > 0 else 'Shipping', args[1:])\n\t\telse:\n\t\t\tself.packagePlugin(dir, args)", "response": "Packages a build of the Unreal project or plugin in the specified directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvoking the Automation Test commandlet for the specified project with the supplied list of automation test commands.", "response": "def runAutomationCommands(self, projectFile, commands, capture=False):\n\t\t'''\n\t\tInvokes the Automation Test commandlet for the specified project with the supplied automation test commands\n\t\t'''\n\t\t\n\t\t# IMPORTANT IMPLEMENTATION NOTE:\n\t\t# We need to format the command as a string and execute it using a shell in order to\n\t\t# ensure the \"-ExecCmds\" argument will be parsed correctly under Windows. This is because\n\t\t# the WinMain() function uses GetCommandLineW() to retrieve the raw command-line string,\n\t\t# rather than using an argv-style structure. The string is then passed to FParse::Value(),\n\t\t# which checks for the presence of a quote character after the equals sign to determine if\n\t\t# whitespace should be stripped or preserved. Without the quote character, the spaces in the\n\t\t# argument payload will be stripped out, corrupting our list of automation commands and\n\t\t# preventing them from executing correctly.\n\t\t\n\t\tcommand = '{} {}'.format(Utility.escapePathForShell(self.getEditorBinary(True)), Utility.escapePathForShell(projectFile))\n\t\tcommand += ' -game -buildmachine -stdout -fullstdoutlogoutput -forcelogflush -unattended -nopause -nullrhi -nosplash'\n\t\tcommand += ' -ExecCmds=\"automation {};quit\"'.format(';'.join(commands))\n\t\t\n\t\tif capture == True:\n\t\t\treturn Utility.capture(command, shell=True)\n\t\telse:\n\t\t\tUtility.run(command, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getEngineRoot(self):\n\t\toverride = ConfigurationManager.getConfigKey('rootDirOverride')\n\t\tif override != None:\n\t\t\tUtility.printStderr('Using user-specified engine root: ' + override)\n\t\t\treturn override\n\t\telse:\n\t\t\treturn self._detectEngineRoot()", "response": "Retrieves the user - specified engine root directory override or performs auto - detection\n\t returns None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the JSON version details for the latest installed version of UE4", "response": "def _getEngineVersionDetails(self):\n\t\t\"\"\"\n\t\tParses the JSON version details for the latest installed version of UE4\n\t\t\"\"\"\n\t\tversionFile = os.path.join(self.getEngineRoot(), 'Engine', 'Build', 'Build.version')\n\t\treturn json.loads(Utility.readFile(versionFile))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _getEngineVersionHash(self):\n\t\tversionDetails = self._getEngineVersionDetails()\n\t\thash = hashlib.sha256()\n\t\thash.update(json.dumps(versionDetails, sort_keys=True, indent=0).encode('utf-8'))\n\t\treturn hash.hexdigest()", "response": "Computes the SHA - 256 hash of the JSON version details for the latest installed version of UE4"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninvoke UnrealBuildTool with the specified parameters.", "response": "def _runUnrealBuildTool(self, target, platform, configuration, args, capture=False):\n\t\t\"\"\"\n\t\tInvokes UnrealBuildTool with the specified parameters\n\t\t\"\"\"\n\t\tplatform = self._transformBuildToolPlatform(platform)\n\t\targuments = [self.getBuildScript(), target, platform, configuration] + args\n\t\tif capture == True:\n\t\t\treturn Utility.capture(arguments, cwd=self.getEngineRoot(), raiseOnError=True)\n\t\telse:\n\t\t\tUtility.run(arguments, cwd=self.getEngineRoot(), raiseOnError=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a UE4BuildInterrogator object that can be used to interrogate UnrealBuildTool about third - party library details.", "response": "def _getUE4BuildInterrogator(self):\n\t\t\"\"\"\n\t\tUses UE4BuildInterrogator to interrogate UnrealBuildTool about third-party library details\n\t\t\"\"\"\n\t\tubtLambda = lambda target, platform, config, args: self._runUnrealBuildTool(target, platform, config, args, True)\n\t\tinterrogator = UE4BuildInterrogator(self.getEngineRoot(), self._getEngineVersionDetails(), self._getEngineVersionHash(), ubtLambda)\n\t\treturn interrogator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the value for the specified key in the dictionary", "response": "def getKey(self, key):\n\t\t\"\"\"\n\t\tRetrieves the value for the specified dictionary key\n\t\t\"\"\"\n\t\tdata = self.getDictionary()\n\t\tif key in data:\n\t\t\treturn data[key]\n\t\telse:\n\t\t\treturn None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the entire data dictionary", "response": "def getDictionary(self):\n\t\t\"\"\"\n\t\tRetrieves the entire data dictionary\n\t\t\"\"\"\n\t\tif os.path.exists(self.jsonFile):\n\t\t\treturn json.loads(Utility.readFile(self.jsonFile))\n\t\telse:\n\t\t\treturn {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the value for the specified key in the specified dictionary.", "response": "def setKey(self, key, value):\n\t\t\"\"\"\n\t\tSets the value for the specified dictionary key\n\t\t\"\"\"\n\t\tdata = self.getDictionary()\n\t\tdata[key] = value\n\t\tself.setDictionary(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setDictionary(self, data):\n\t\t\n\t\t# Create the directory containing the JSON file if it doesn't already exist\n\t\tjsonDir = os.path.dirname(self.jsonFile)\n\t\tif os.path.exists(jsonDir) == False:\n\t\t\tos.makedirs(jsonDir)\n\t\t\n\t\t# Store the dictionary\n\t\tUtility.writeFile(self.jsonFile, json.dumps(data))", "response": "Set the dictionary to the given data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list(self, platformIdentifier, configuration, libOverrides = {}):\n\t\tmodules = self._getThirdPartyLibs(platformIdentifier, configuration)\n\t\treturn sorted([m['Name'] for m in modules] + [key for key in libOverrides])", "response": "Returns the list of UE4 - bundled third - party libraries that are supported by the current platform."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninterrogates UnrealBuildTool about the build flags for the specified third - party libraries.", "response": "def interrogate(self, platformIdentifier, configuration, libraries, libOverrides = {}):\n\t\t\"\"\"\n\t\tInterrogates UnrealBuildTool about the build flags for the specified third-party libraries\n\t\t\"\"\"\n\t\t\n\t\t# Determine which libraries need their modules parsed by UBT, and which are override-only\n\t\tlibModules = list([lib for lib in libraries if lib not in libOverrides])\n\t\t\n\t\t# Check that we have at least one module to parse\n\t\tdetails = ThirdPartyLibraryDetails()\n\t\tif len(libModules) > 0:\n\t\t\t\n\t\t\t# Retrieve the list of third-party library modules from UnrealBuildTool\n\t\t\tmodules = self._getThirdPartyLibs(platformIdentifier, configuration)\n\t\t\t\n\t\t\t# Filter the list of modules to include only those that were requested\n\t\t\tmodules = [m for m in modules if m['Name'] in libModules]\n\t\t\t\n\t\t\t# Emit a warning if any of the requested modules are not supported\n\t\t\tnames = [m['Name'] for m in modules]\n\t\t\tunsupported = ['\"' + m + '\"' for m in libModules if m not in names]\n\t\t\tif len(unsupported) > 0:\n\t\t\t\tUtility.printStderr('Warning: unsupported libraries ' + ','.join(unsupported))\n\t\t\t\n\t\t\t# Some libraries are listed as just the filename without the leading directory (especially prevalent under Windows)\n\t\t\tfor module in modules:\n\t\t\t\tif len(module['PublicAdditionalLibraries']) > 0 and len(module['PublicLibraryPaths']) > 0:\n\t\t\t\t\tlibPath = (self._absolutePaths(module['PublicLibraryPaths']))[0]\n\t\t\t\t\tlibs = list([lib.replace('\\\\', '/') for lib in module['PublicAdditionalLibraries']])\n\t\t\t\t\tlibs = list([os.path.join(libPath, lib) if '/' not in lib else lib for lib in libs])\n\t\t\t\t\tmodule['PublicAdditionalLibraries'] = libs\n\t\t\t\n\t\t\t# Flatten the lists of paths\n\t\t\tfields = [\n\t\t\t\t'Directory',\n\t\t\t\t'PublicAdditionalLibraries',\n\t\t\t\t'PublicLibraryPaths',\n\t\t\t\t'PublicSystemIncludePaths',\n\t\t\t\t'PublicIncludePaths',\n\t\t\t\t'PrivateIncludePaths',\n\t\t\t\t'PublicDefinitions'\n\t\t\t]\n\t\t\tflattened = {}\n\t\t\tfor field in fields:\n\t\t\t\ttransform = (lambda l: self._absolutePaths(l)) if field != 'Definitions' else None\n\t\t\t\tflattened[field] = self._flatten(field, modules, transform)\n\t\t\t\n\t\t\t# Compose the prefix directories from the module root directories, the header and library paths, and their direct parent directories\n\t\t\tlibraryDirectories = flattened['PublicLibraryPaths']\n\t\t\theaderDirectories  = flattened['PublicSystemIncludePaths'] + flattened['PublicIncludePaths'] + flattened['PrivateIncludePaths']\n\t\t\tmodulePaths        = flattened['Directory']\n\t\t\tprefixDirectories  = list(set(flattened['Directory'] + headerDirectories + libraryDirectories + [os.path.dirname(p) for p in headerDirectories + libraryDirectories]))\n\t\t\t\n\t\t\t# Wrap the results in a ThirdPartyLibraryDetails instance, converting any relative directory paths into absolute ones\n\t\t\tdetails = ThirdPartyLibraryDetails(\n\t\t\t\tprefixDirs  = prefixDirectories,\n\t\t\t\tincludeDirs = headerDirectories,\n\t\t\t\tlinkDirs    = libraryDirectories,\n\t\t\t\tdefinitions = flattened['PublicDefinitions'],\n\t\t\t\tlibs        = flattened['PublicAdditionalLibraries']\n\t\t\t)\n\t\t\n\t\t# Apply any overrides\n\t\toverridesToApply = list([libOverrides[lib] for lib in libraries if lib in libOverrides])\n\t\tfor override in overridesToApply:\n\t\t\tdetails.merge(override)\n\t\t\n\t\treturn details"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the supplied list of paths to absolute pathnames except for pure filenames without leading relative directories", "response": "def _absolutePaths(self, paths):\n\t\t\"\"\"\n\t\tConverts the supplied list of paths to absolute pathnames (except for pure filenames without leading relative directories)\n\t\t\"\"\"\n\t\tslashes = [p.replace('\\\\', '/') for p in paths]\n\t\tstripped = [p.replace('../', '') if p.startswith('../') else p for p in slashes]\n\t\treturn list([p if (os.path.isabs(p) or '/' not in p) else os.path.join(self.engineRoot, self.engineSourceDir, p) for p in stripped])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _flatten(self, field, items, transform = None):\n\t\t\n\t\t# Retrieve the value for each item in the iterable\n\t\tvalues = [item[field] for item in items]\n\t\t\n\t\t# Flatten any nested lists\n\t\tflattened = []\n\t\tfor value in values:\n\t\t\tflattened.extend([value] if isinstance(value, str) else value)\n\t\t\n\t\t# Apply any supplied transformation function\n\t\treturn transform(flattened) if transform != None else flattened", "response": "Flatten the values of the supplied iterable"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns UnrealBuildTool in JSON export mode and extracts the list of third-party libraries", "response": "def _getThirdPartyLibs(self, platformIdentifier, configuration):\n\t\t\"\"\"\n\t\tRuns UnrealBuildTool in JSON export mode and extracts the list of third-party libraries\n\t\t\"\"\"\n\t\t\n\t\t# If we have previously cached the library list for the current engine version, use the cached data\n\t\tcachedList = CachedDataManager.getCachedDataKey(self.engineVersionHash, 'ThirdPartyLibraries')\n\t\tif cachedList != None:\n\t\t\treturn cachedList\n\t\t\n\t\t# Create a temp directory to hold the JSON file\n\t\ttempDir = tempfile.mkdtemp()\n\t\tjsonFile = os.path.join(tempDir, 'ubt_output.json')\n\t\t\n\t\t# Installed Builds of the Engine only contain a small handful of third-party libraries, rather than the full set\n\t\t# included in a source build of the Engine. However, if the ThirdParty directory from a source build is copied\n\t\t# into an Installed Build and the `InstalledBuild.txt` sentinel file is temporarily renamed, we can get the best\n\t\t# of both worlds and utilise the full set of third-party libraries. Enable this sentinel renaming behaviour only\n\t\t# if you have copied the ThirdParty directory from a source build into your Installed Build, or else the UBT\n\t\t# command will fail trying to rebuild UnrealHeaderTool.\n\t\tsentinelFile = os.path.join(self.engineRoot, 'Engine', 'Build', 'InstalledBuild.txt')\n\t\tsentinelBackup = sentinelFile + '.bak'\n\t\trenameSentinel = os.path.exists(sentinelFile) and os.environ.get('UE4CLI_SENTINEL_RENAME', '0') == '1'\n\t\tif renameSentinel == True:\n\t\t\tshutil.move(sentinelFile, sentinelBackup)\n\t\t\n\t\t# Invoke UnrealBuildTool in JSON export mode (make sure we specify gathering mode, since this is a prerequisite of JSON export)\n\t\t# (Ensure we always perform sentinel file cleanup even when errors occur)\n\t\ttry:\n\t\t\targs = ['-Mode=JsonExport', '-OutputFile=' +jsonFile ] if self.engineVersion['MinorVersion'] >= 22 else ['-gather', '-jsonexport=' + jsonFile, '-SkipBuild']\n\t\t\tself.runUBTFunc('UE4Editor', platformIdentifier, configuration, args)\n\t\tfinally:\n\t\t\tif renameSentinel == True:\n\t\t\t\tshutil.move(sentinelBackup, sentinelFile)\n\t\t\n\t\t# Parse the JSON output\n\t\tresult = json.loads(Utility.readFile(jsonFile))\n\t\t\n\t\t# Extract the list of third-party library modules\n\t\t# (Note that since UE4.21, modules no longer have a \"Type\" field, so we must\n\t\t# rely on the \"Directory\" field filter below to identify third-party libraries)\n\t\tmodules = [result['Modules'][key] for key in result['Modules']]\n\t\t\n\t\t# Filter out any modules from outside the Engine/Source/ThirdParty directory\n\t\tthirdPartyRoot = os.path.join(self.engineRoot, 'Engine', 'Source', 'ThirdParty')\n\t\tthirdparty = list([m for m in modules if thirdPartyRoot in m['Directory']])\n\t\t\n\t\t# Remove the temp directory\n\t\ttry:\n\t\t\tshutil.rmtree(tempDir)\n\t\texcept:\n\t\t\tpass\n\t\t\n\t\t# Cache the list of libraries for use by subsequent runs\n\t\tCachedDataManager.setCachedDataKey(self.engineVersionHash, 'ThirdPartyLibraries', thirdparty)\n\t\t\n\t\treturn thirdparty"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the supplied ThirdPartyLibraryDetails instance and sets any custom CMake flags for those that are needed for the CMake command line.", "response": "def processLibraryDetails(details):\n\t\t\"\"\"\n\t\tProcesses the supplied ThirdPartyLibraryDetails instance and sets any custom CMake flags\n\t\t\"\"\"\n\t\t\n\t\t# If the header include directories list contains any directories we have flags for, add them\n\t\tfor includeDir in details.includeDirs:\n\t\t\t\n\t\t\t# If the directory path matches any of the substrings in our list, generate the relevant flags\n\t\t\tfor pattern in CUSTOM_FLAGS_FOR_INCLUDE_DIRS:\n\t\t\t\tif pattern in includeDir:\n\t\t\t\t\tflag = '-D' + CUSTOM_FLAGS_FOR_INCLUDE_DIRS[pattern] + '=' + includeDir\n\t\t\t\t\tdetails.cmakeFlags.append(flag)\n\t\t\n\t\t# If the libraries list contains any libs we have flags for, add them\n\t\tfor lib in details.libs:\n\t\t\t\n\t\t\t# Extract the name of the library from the filename\n\t\t\t# (We remove any \"lib\" prefix or numerical suffix)\n\t\t\tfilename = os.path.basename(lib)\n\t\t\t(name, ext) = os.path.splitext(filename)\n\t\t\tlibName = name.replace('lib', '') if name.startswith('lib') else name\n\t\t\tlibName = libName.rstrip('_-1234567890')\n\t\t\t\n\t\t\t# If the library name matches one in our list, generate its flag\n\t\t\tif libName in CUSTOM_FLAGS_FOR_LIBS:\n\t\t\t\tflag = '-D' + CUSTOM_FLAGS_FOR_LIBS[libName] + '=' + lib\n\t\t\t\tdetails.cmakeFlags.append(flag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPlugins():\n\t\t\n\t\t# Retrieve the list of detected entry points in the ue4cli.plugins group\n\t\tplugins = {\n\t\t\tentry_point.name: entry_point.load()\n\t\t\tfor entry_point\n\t\t\tin pkg_resources.iter_entry_points('ue4cli.plugins')\n\t\t}\n\t\t\n\t\t# Filter out any invalid plugins\n\t\tplugins = {\n\t\t\tname: plugins[name]\n\t\t\tfor name in plugins\n\t\t\tif\n\t\t\t\t'action' in plugins[name] and\n\t\t\t\t'description' in plugins[name] and\n\t\t\t\t'args' in plugins[name] and\n\t\t\t\tcallable(plugins[name]['action']) == True and\n\t\t\t\tlen(signature(plugins[name]['action']).parameters) == 2\n\t\t}\n\t\t\n\t\treturn plugins", "response": "Returns the list of valid ue4cli plugins"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getCompilerFlags(self, engineRoot, fmt):\n\t\treturn Utility.join(\n\t\t\tfmt.delim, \n\t\t\t\tself.prefixedStrings(self.definitionPrefix, self.definitions, engineRoot) +\n\t\t\t\tself.prefixedStrings(self.includeDirPrefix, self.includeDirs, engineRoot) +\n\t\t\t\tself.resolveRoot(self.cxxFlags, engineRoot),\n\t\t\tfmt.quotes\n\t\t)", "response": "Returns the compiler flags string for building against this library"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getLinkerFlags(self, engineRoot, fmt, includeLibs=True):\n\t\tcomponents = self.resolveRoot(self.ldFlags, engineRoot)\n\t\tif includeLibs == True:\n\t\t\tcomponents.extend(self.prefixedStrings(self.linkerDirPrefix, self.linkDirs, engineRoot))\n\t\t\tcomponents.extend(self.resolveRoot(self.libs, engineRoot))\n\t\t\n\t\treturn Utility.join(fmt.delim, components, fmt.quotes)", "response": "Constructs the linker flags string for building against this library"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getPrefixDirectories(self, engineRoot, delimiter=' '):\n\t\treturn delimiter.join(self.resolveRoot(self.prefixDirs, engineRoot))", "response": "Returns the list of prefix directories for this library joined using the specified delimiter"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of include directories for this library joined using the specified delimiter", "response": "def getIncludeDirectories(self, engineRoot, delimiter=' '):\n\t\t\"\"\"\n\t\tReturns the list of include directories for this library, joined using the specified delimiter\n\t\t\"\"\"\n\t\treturn delimiter.join(self.resolveRoot(self.includeDirs, engineRoot))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the list of linker directories for this library joined using the specified delimiter", "response": "def getLinkerDirectories(self, engineRoot, delimiter=' '):\n\t\t\"\"\"\n\t\tReturns the list of linker directories for this library, joined using the specified delimiter\n\t\t\"\"\"\n\t\treturn delimiter.join(self.resolveRoot(self.linkDirs, engineRoot))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the list of library files for this library joined using the specified delimiter", "response": "def getLibraryFiles(self, engineRoot, delimiter=' '):\n\t\t\"\"\"\n\t\tReturns the list of library files for this library, joined using the specified delimiter\n\t\t\"\"\"\n\t\treturn delimiter.join(self.resolveRoot(self.libs, engineRoot))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getPreprocessorDefinitions(self, engineRoot, delimiter=' '):\n\t\treturn delimiter.join(self.resolveRoot(self.definitions, engineRoot))", "response": "Returns the list of preprocessor definitions for this library joined using the specified delimiter"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing the CMake invocation flags string for building against this library", "response": "def getCMakeFlags(self, engineRoot, fmt):\n\t\t\"\"\"\n\t\tConstructs the CMake invocation flags string for building against this library\n\t\t\"\"\"\n\t\treturn Utility.join(\n\t\t\tfmt.delim,\n\t\t\t[\n\t\t\t\t'-DCMAKE_PREFIX_PATH=' + self.getPrefixDirectories(engineRoot, ';'),\n\t\t\t\t'-DCMAKE_INCLUDE_PATH=' + self.getIncludeDirectories(engineRoot, ';'),\n\t\t\t\t'-DCMAKE_LIBRARY_PATH=' + self.getLinkerDirectories(engineRoot, ';'),\n\t\t\t] + self.resolveRoot(self.cmakeFlags, engineRoot),\n\t\t\tfmt.quotes\n\t\t)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if current project has any noncommited changes.", "response": "def is_changed():\n    \"\"\" Checks if current project has any noncommited changes. \"\"\"\n    executed, changed_lines = execute_git('status --porcelain', output=False)\n    merge_not_finished = mod_path.exists('.git/MERGE_HEAD')\n    return changed_lines.strip() or merge_not_finished"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a User - Agent that identifies this client.", "response": "def user_agent():\n    \"\"\"\n    Return a User-Agent that identifies this client.\n\n    Example:\n        python-requests/2.9.1 edx-rest-api-client/1.7.2 ecommerce\n\n    The last item in the list will be the application name, taken from the\n    OS environment variable EDX_REST_API_CLIENT_NAME. If that environment\n    variable is not set, it will default to the hostname.\n    \"\"\"\n    client_name = 'unknown_client_name'\n    try:\n        client_name = os.environ.get(\"EDX_REST_API_CLIENT_NAME\") or socket.gethostbyname(socket.gethostname())\n    except:  # pylint: disable=bare-except\n        pass  # using 'unknown_client_name' is good enough.  no need to log.\n    return \"{} edx-rest-api-client/{} {}\".format(\n        requests.utils.default_user_agent(),  # e.g. \"python-requests/2.9.1\"\n        __version__,  # version of this client\n        client_name\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_oauth_access_token(url, client_id, client_secret, token_type='jwt', grant_type='client_credentials',\n                           refresh_token=None):\n    \"\"\" Retrieves OAuth 2.0 access token using the given grant type.\n\n    Args:\n        url (str): Oauth2 access token endpoint\n        client_id (str): client ID\n        client_secret (str): client secret\n    Kwargs:\n        token_type (str): Type of token to return. Options include bearer and jwt.\n        grant_type (str): One of 'client_credentials' or 'refresh_token'\n        refresh_token (str): The previous access token (for grant_type=refresh_token)\n\n    Returns:\n        tuple: Tuple containing access token string and expiration datetime.\n    \"\"\"\n    now = datetime.datetime.utcnow()\n    data = {\n        'grant_type': grant_type,\n        'client_id': client_id,\n        'client_secret': client_secret,\n        'token_type': token_type,\n    }\n    if refresh_token:\n        data['refresh_token'] = refresh_token\n    else:\n        assert grant_type != 'refresh_token', \"refresh_token parameter required\"\n\n    response = requests.post(\n        url,\n        data=data,\n        headers={\n            'User-Agent': USER_AGENT,\n        },\n    )\n\n    data = response.json()\n    try:\n        access_token = data['access_token']\n        expires_in = data['expires_in']\n    except KeyError:\n        raise requests.RequestException(response=response)\n\n    expires_at = now + datetime.timedelta(seconds=expires_in)\n\n    return access_token, expires_at", "response": "Retrieves an OAuth 2. 0 access token using the given grant type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride Session. request to ensure that the session is authenticated.", "response": "def request(self, method, url, **kwargs):  # pylint: disable=arguments-differ\n        \"\"\"\n        Overrides Session.request to ensure that the session is authenticated\n        \"\"\"\n        self._check_auth()\n        return super(OAuthAPIClient, self).request(method, url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run_command(self, scan_id, host, cmd):\n\n        ssh = paramiko.SSHClient()\n        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        options = self.get_scan_options(scan_id)\n\n        port = int(options['port'])\n        timeout = int(options['ssh_timeout'])\n\n        # For backward compatibility, consider the legacy mode to get\n        # credentials as scan_option.\n        # First and second modes should be removed in future releases.\n        # On the third case it receives the credentials as a subelement of\n        # the <target>.\n        credentials = self.get_scan_credentials(scan_id, host)\n        if ('username_password' in options and\n                ':' in options['username_password']):\n            username, password = options['username_password'].split(':', 1)\n        elif 'username' in options and options['username']:\n            username = options['username']\n            password = options['password']\n        elif credentials:\n            cred_params = credentials.get('ssh')\n            username = cred_params.get('username', '')\n            password = cred_params.get('password', '')\n        else:\n            self.add_scan_error(scan_id, host=host,\n                                value='Erroneous username_password value')\n            raise ValueError('Erroneous username_password value')\n\n        try:\n            ssh.connect(hostname=host, username=username, password=password,\n                        timeout=timeout, port=port)\n        except (paramiko.ssh_exception.AuthenticationException,\n                socket.error) as err:\n            # Errors: No route to host, connection timeout, authentication\n            # failure etc,.\n            self.add_scan_error(scan_id, host=host, value=str(err))\n            return None\n\n        _, stdout, _ = ssh.exec_command(cmd)\n        result = stdout.readlines()\n        ssh.close()\n\n        return result", "response": "Run a single command via SSH and return the content of stdout or None in case of an Error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_result_xml(result):\n    result_xml = Element('result')\n    for name, value in [('name', result['name']),\n                        ('type', ResultType.get_str(result['type'])),\n                        ('severity', result['severity']),\n                        ('host', result['host']),\n                        ('test_id', result['test_id']),\n                        ('port', result['port']),\n                        ('qod', result['qod'])]:\n        result_xml.set(name, str(value))\n    result_xml.text = result['value']\n    return result_xml", "response": "Takes a dictionary with a scan result and returns a xml element object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simple_response_str(command, status, status_text, content=\"\"):\n    response = Element('%s_response' % command)\n    for name, value in [('status', str(status)), ('status_text', status_text)]:\n        response.set(name, str(value))\n    if isinstance(content, list):\n        for elem in content:\n            response.append(elem)\n    elif isinstance(content, Element):\n        response.append(content)\n    else:\n        response.text = content\n    return tostring(response)", "response": "Create an OSP response XML string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind_socket(address, port):\n\n    assert address\n    assert port\n    bindsocket = socket.socket()\n    try:\n        bindsocket.bind((address, port))\n    except socket.error:\n        logger.error(\"Couldn't bind socket on %s:%s\", address, port)\n        return None\n\n    logger.info('Listening on %s:%s', address, port)\n    bindsocket.listen(0)\n    return bindsocket", "response": "Returns a socket bound to the given address and port."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bind_unix_socket(path):\n\n    assert path\n    bindsocket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n    try:\n        os.unlink(path)\n    except OSError:\n        if os.path.exists(path):\n            raise\n    try:\n        bindsocket.bind(path)\n    except socket.error:\n        logger.error(\"Couldn't bind socket on %s\", path)\n        return None\n\n    logger.info('Listening on %s', path)\n    bindsocket.listen(0)\n    return bindsocket", "response": "Returns a unix file socket bound on the given path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close_client_stream(client_stream, unix_path):\n    try:\n        client_stream.shutdown(socket.SHUT_RDWR)\n        if unix_path:\n            logger.debug('%s: Connection closed', unix_path)\n        else:\n            peer = client_stream.getpeername()\n            logger.debug('%s:%s: Connection closed', peer[0], peer[1])\n    except (socket.error, OSError) as exception:\n        logger.debug('Connection closing error: %s', exception)\n    client_stream.close()", "response": "Closes provided client stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_command_attributes(self, name, attributes):\n        if self.command_exists(name):\n            command = self.commands.get(name)\n            command['attributes'] = attributes", "response": "Sets the xml attributes of a specified command."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a scanner parameter.", "response": "def add_scanner_param(self, name, scanner_param):\n        \"\"\" Add a scanner parameter. \"\"\"\n\n        assert name\n        assert scanner_param\n        self.scanner_params[name] = scanner_param\n        command = self.commands.get('start_scan')\n        command['elements'] = {\n            'scanner_params':\n                {k: v['name'] for k, v in self.scanner_params.items()}}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_vt(self, vt_id, name=None, vt_params=None, vt_refs=None,\n               custom=None, vt_creation_time=None, vt_modification_time=None,\n               vt_dependencies=None, summary=None, impact=None, affected=None,\n               insight=None, solution=None, solution_t=None, detection=None,\n               qod_t=None, qod_v=None, severities=None):\n        \"\"\" Add a vulnerability test information.\n\n        Returns: The new number of stored VTs.\n        -1 in case the VT ID was already present and thus the\n        new VT was not considered.\n        -2 in case the vt_id was invalid.\n        \"\"\"\n\n        if not vt_id:\n            return -2  # no valid vt_id\n\n        if self.vt_id_pattern.fullmatch(vt_id) is None:\n            return -2  # no valid vt_id\n\n        if vt_id in self.vts:\n            return -1  # The VT was already in the list.\n\n        if name is None:\n            name = ''\n\n        self.vts[vt_id] = {'name': name}\n        if custom is not None:\n            self.vts[vt_id][\"custom\"] = custom\n        if vt_params is not None:\n            self.vts[vt_id][\"vt_params\"] = vt_params\n        if vt_refs is not None:\n            self.vts[vt_id][\"vt_refs\"] = vt_refs\n        if vt_dependencies is not None:\n            self.vts[vt_id][\"vt_dependencies\"] = vt_dependencies\n        if vt_creation_time is not None:\n            self.vts[vt_id][\"creation_time\"] = vt_creation_time\n        if vt_modification_time is not None:\n            self.vts[vt_id][\"modification_time\"] = vt_modification_time\n        if summary is not None:\n            self.vts[vt_id][\"summary\"] = summary\n        if impact is not None:\n            self.vts[vt_id][\"impact\"] = impact\n        if affected is not None:\n            self.vts[vt_id][\"affected\"] = affected\n        if insight is not None:\n            self.vts[vt_id][\"insight\"] = insight\n        if solution is not None:\n            self.vts[vt_id][\"solution\"] = solution\n            if solution_t is not None:\n                self.vts[vt_id][\"solution_type\"] = solution_t\n        if detection is not None:\n            self.vts[vt_id][\"detection\"] = detection\n        if qod_t is not None:\n            self.vts[vt_id][\"qod_type\"] = qod_t\n        elif qod_v is not None:\n            self.vts[vt_id][\"qod\"] = qod_v\n        if severities is not None:\n            self.vts[vt_id][\"severities\"] = severities\n\n        return len(self.vts)", "response": "Adds a new VT to the list of vulnerability test information."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses the scan parameters.", "response": "def _preprocess_scan_params(self, xml_params):\n        \"\"\" Processes the scan parameters. \"\"\"\n        params = {}\n        for param in xml_params:\n            params[param.tag] = param.text or ''\n        # Set default values.\n        for key in self.scanner_params:\n            if key not in params:\n                params[key] = self.get_scanner_param_default(key)\n                if self.get_scanner_param_type(key) == 'selection':\n                    params[key] = params[key].split('|')[0]\n        # Validate values.\n        for key in params:\n            param_type = self.get_scanner_param_type(key)\n            if not param_type:\n                continue\n            if param_type in ['integer', 'boolean']:\n                try:\n                    params[key] = int(params[key])\n                except ValueError:\n                    raise OSPDError('Invalid %s value' % key, 'start_scan')\n            if param_type == 'boolean':\n                if params[key] not in [0, 1]:\n                    raise OSPDError('Invalid %s value' % key, 'start_scan')\n            elif param_type == 'selection':\n                selection = self.get_scanner_param_default(key).split('|')\n                if params[key] not in selection:\n                    raise OSPDError('Invalid %s value' % key, 'start_scan')\n            if self.get_scanner_param_mandatory(key) and params[key] == '':\n                    raise OSPDError('Mandatory %s value is missing' % key,\n                                    'start_scan')\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_vts_params(self, scanner_vts):\n        vt_selection = {}\n        filters = list()\n        for vt in scanner_vts:\n            if vt.tag == 'vt_single':\n                vt_id = vt.attrib.get('id')\n                vt_selection[vt_id] = {}\n                for vt_value in vt:\n                    if not vt_value.attrib.get('id'):\n                        raise OSPDError('Invalid VT preference. No attribute id',\n                                        'start_scan')\n                    vt_value_id = vt_value.attrib.get('id')\n                    vt_value_value = vt_value.text if vt_value.text else ''\n                    vt_selection[vt_id][vt_value_id] = vt_value_value\n            if vt.tag == 'vt_group':\n                vts_filter = vt.attrib.get('filter', None)\n                if vts_filter is None:\n                    raise OSPDError('Invalid VT group. No filter given.',\n                                    'start_scan')\n                filters.append(vts_filter)\n        vt_selection['vt_groups'] = filters\n        return vt_selection", "response": "Process the Vulnerability Tests an their\n        parameters and return a dictionary containing the attribute and subelements that are used in a scan."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the credentials to run a scan against a given target.", "response": "def process_credentials_elements(cred_tree):\n        \"\"\" Receive an XML object with the credentials to run\n        a scan against a given target.\n\n        @param:\n        <credentials>\n          <credential type=\"up\" service=\"ssh\" port=\"22\">\n            <username>scanuser</username>\n            <password>mypass</password>\n          </credential>\n          <credential type=\"up\" service=\"smb\">\n            <username>smbuser</username>\n            <password>mypass</password>\n          </credential>\n        </credentials>\n\n        @return: Dictionary containing the credentials for a given target.\n                 Example form:\n                 {'ssh': {'type': type,\n                          'port': port,\n                          'username': username,\n                          'password': pass,\n                        },\n                  'smb': {'type': type,\n                          'username': username,\n                          'password': pass,\n                         },\n                   }\n        \"\"\"\n        credentials = {}\n        for credential in cred_tree:\n            service = credential.attrib.get('service')\n            credentials[service] = {}\n            credentials[service]['type'] = credential.attrib.get('type')\n            if service == 'ssh':\n                credentials[service]['port'] = credential.attrib.get('port')\n            for param in credential:\n                credentials[service][param.tag] = param.text\n\n        return credentials"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_targets_element(cls, scanner_target):\n\n        target_list = []\n        for target in scanner_target:\n            ports = ''\n            credentials = {}\n            for child in target:\n                if child.tag == 'hosts':\n                    hosts = child.text\n                if child.tag == 'ports':\n                    ports = child.text\n                if child.tag == 'credentials':\n                    credentials = cls.process_credentials_elements(child)\n            if hosts:\n                target_list.append([hosts, ports, credentials])\n            else:\n                raise OSPDError('No target to scan', 'start_scan')\n\n        return target_list", "response": "Process the XML element with target subelements and return a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles start scan command.", "response": "def handle_start_scan_command(self, scan_et):\n        \"\"\" Handles <start_scan> command.\n\n        @return: Response string for <start_scan> command.\n        \"\"\"\n\n        target_str = scan_et.attrib.get('target')\n        ports_str = scan_et.attrib.get('ports')\n        # For backward compatibility, if target and ports attributes are set,\n        # <targets> element is ignored.\n        if target_str is None or ports_str is None:\n            target_list = scan_et.find('targets')\n            if target_list is None or not target_list:\n                raise OSPDError('No targets or ports', 'start_scan')\n            else:\n                scan_targets = self.process_targets_element(target_list)\n        else:\n            scan_targets = []\n            for single_target in target_str_to_list(target_str):\n                scan_targets.append([single_target, ports_str, ''])\n\n        scan_id = scan_et.attrib.get('scan_id')\n        if scan_id is not None and scan_id != '' and not valid_uuid(scan_id):\n            raise OSPDError('Invalid scan_id UUID', 'start_scan')\n\n        try:\n            parallel = int(scan_et.attrib.get('parallel', '1'))\n            if parallel < 1 or parallel > 20:\n                parallel = 1\n        except ValueError:\n            raise OSPDError('Invalid value for parallel scans. '\n                            'It must be a number', 'start_scan')\n\n        scanner_params = scan_et.find('scanner_params')\n        if scanner_params is None:\n            raise OSPDError('No scanner_params element', 'start_scan')\n\n        params = self._preprocess_scan_params(scanner_params)\n\n        # VTS is an optional element. If present should not be empty.\n        vt_selection = {}\n        scanner_vts = scan_et.find('vt_selection')\n        if scanner_vts is not None:\n            if not scanner_vts:\n                raise OSPDError('VTs list is empty', 'start_scan')\n            else:\n                vt_selection = self.process_vts_params(scanner_vts)\n\n        # Dry run case.\n        if 'dry_run' in params and int(params['dry_run']):\n            scan_func = self.dry_run_scan\n            scan_params = None\n        else:\n            scan_func = self.start_scan\n            scan_params = self.process_scan_params(params)\n\n        scan_id = self.create_scan(scan_id, scan_targets,\n                                   scan_params, vt_selection)\n        scan_process = multiprocessing.Process(target=scan_func,\n                                               args=(scan_id,\n                                                     scan_targets,\n                                                     parallel))\n        self.scan_processes[scan_id] = scan_process\n        scan_process.start()\n        id_ = Element('id')\n        id_.text = scan_id\n        return simple_response_str('start_scan', 200, 'OK', id_)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles stop scan command.", "response": "def handle_stop_scan_command(self, scan_et):\n        \"\"\" Handles <stop_scan> command.\n\n        @return: Response string for <stop_scan> command.\n        \"\"\"\n\n        scan_id = scan_et.attrib.get('scan_id')\n        if scan_id is None or scan_id == '':\n            raise OSPDError('No scan_id attribute', 'stop_scan')\n        self.stop_scan(scan_id)\n\n        return simple_response_str('stop_scan', 200, 'OK')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a scan as finished.", "response": "def finish_scan(self, scan_id):\n        \"\"\" Sets a scan as finished. \"\"\"\n        self.set_scan_progress(scan_id, 100)\n        self.set_scan_status(scan_id, ScanStatus.FINISHED)\n        logger.info(\"%s: Scan finished.\", scan_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the type of a scanner parameter.", "response": "def get_scanner_param_type(self, param):\n        \"\"\" Returns type of a scanner parameter. \"\"\"\n        assert isinstance(param, str)\n        entry = self.scanner_params.get(param)\n        if not entry:\n            return None\n        return entry.get('type')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_scanner_param_mandatory(self, param):\n        assert isinstance(param, str)\n        entry = self.scanner_params.get(param)\n        if not entry:\n            return False\n        return entry.get('mandatory')", "response": "Returns if a scanner parameter is mandatory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns default value of a scanner parameter.", "response": "def get_scanner_param_default(self, param):\n        \"\"\" Returns default value of a scanner parameter. \"\"\"\n        assert isinstance(param, str)\n        entry = self.scanner_params.get(param)\n        if not entry:\n            return None\n        return entry.get('default')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the OSP Daemon s scanner params in xml format.", "response": "def get_scanner_params_xml(self):\n        \"\"\" Returns the OSP Daemon's scanner params in xml format. \"\"\"\n        scanner_params = Element('scanner_params')\n        for param_id, param in self.scanner_params.items():\n            param_xml = SubElement(scanner_params, 'scanner_param')\n            for name, value in [('id', param_id),\n                                ('type', param['type'])]:\n                param_xml.set(name, value)\n            for name, value in [('name', param['name']),\n                                ('description', param['description']),\n                                ('default', param['default']),\n                                ('mandatory', param['mandatory'])]:\n                elem = SubElement(param_xml, name)\n                elem.text = str(value)\n        return scanner_params"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_client_stream(self, sock):\n\n        assert sock\n        newsocket, fromaddr = sock.accept()\n        logger.debug(\"New connection from\"\n                     \" %s:%s\", fromaddr[0], fromaddr[1])\n        # NB: Despite the name, ssl.PROTOCOL_SSLv23 selects the highest\n        # protocol version that both the client and server support. In modern\n        # Python versions (>= 3.4) it supports TLS >= 1.0 with SSLv2 and SSLv3\n        # being disabled. For Python >=3.5, PROTOCOL_SSLv23 is an alias for\n        # PROTOCOL_TLS which should be used once compatibility with Python 3.4\n        # is no longer desired.\n        try:\n            ssl_socket = ssl.wrap_socket(newsocket, cert_reqs=ssl.CERT_REQUIRED,\n                                         server_side=True,\n                                         certfile=self.certs['cert_file'],\n                                         keyfile=self.certs['key_file'],\n                                         ca_certs=self.certs['ca_file'],\n                                         ssl_version=ssl.PROTOCOL_SSLv23)\n        except (ssl.SSLError, socket.error) as message:\n            logger.error(message)\n            return None\n        return ssl_socket", "response": "Returns a new ssl client stream from bind_socket."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_to_stream(stream, response, block_len=1024):\n        try:\n            i_start = 0\n            i_end = block_len\n            while True:\n                if i_end > len(response):\n                    stream(response[i_start:])\n                    break\n                stream(response[i_start:i_end])\n                i_start = i_end\n                i_end += block_len\n        except (socket.timeout, socket.error) as exception:\n            logger.debug('Error sending response to the client: %s', exception)", "response": "Send the response to the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_client_stream(self, stream, is_unix=False):\n\n        assert stream\n        data = []\n        stream.settimeout(2)\n        while True:\n            try:\n                if is_unix:\n                    buf = stream.recv(1024)\n                else:\n                    buf = stream.read(1024)\n                if not buf:\n                    break\n                data.append(buf)\n            except (AttributeError, ValueError) as message:\n                logger.error(message)\n                return\n            except (ssl.SSLError) as exception:\n                logger.debug('Error: %s', exception[0])\n                break\n            except (socket.timeout) as exception:\n                logger.debug('Error: %s', exception)\n                break\n        data = b''.join(data)\n        if len(data) <= 0:\n            logger.debug(\"Empty client stream\")\n            return\n        try:\n            response = self.handle_command(data)\n        except OSPDError as exception:\n            response = exception.as_xml()\n            logger.debug('Command error: %s', exception.message)\n        except Exception:\n            logger.exception('While handling client command:')\n            exception = OSPDError('Fatal error', 'error')\n            response = exception.as_xml()\n        if is_unix:\n            send_method = stream.send\n        else:\n            send_method = stream.write\n        self.write_to_stream(send_method, response)", "response": "Handles the client stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the scan with scan_id.", "response": "def parallel_scan(self, scan_id, target):\n        \"\"\" Starts the scan with scan_id. \"\"\"\n        try:\n            ret = self.exec_scan(scan_id, target)\n            if ret == 0:\n                self.add_scan_host_detail(scan_id, name='host_status',\n                                          host=target, value='0')\n            elif ret == 1:\n                self.add_scan_host_detail(scan_id, name='host_status',\n                                          host=target, value='1')\n            elif ret == 2:\n                self.add_scan_host_detail(scan_id, name='host_status',\n                                          host=target, value='2')\n            else:\n                logger.debug('%s: No host status returned', target)\n        except Exception as e:\n            self.add_scan_error(scan_id, name='', host=target,\n                                value='Host process failure (%s).' % e)\n            logger.exception('While scanning %s:', target)\n        else:\n            logger.info(\"%s: Host scan finished.\", target)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a scan process is still alive and stops the other processes if it is still alive.", "response": "def check_pending_target(self, scan_id, multiscan_proc):\n        \"\"\" Check if a scan process is still alive. In case the process\n        finished or is stopped, removes the process from the multiscan\n        _process list.\n        Processes dead and with progress < 100% are considered stopped\n        or with failures. Then will try to stop the other runnings (target)\n        scans owned by the same task.\n\n        @input scan_id        Scan_id of the whole scan.\n        @input multiscan_proc A list with the scan process which\n                              may still be alive.\n\n        @return Actualized list with current runnging scan processes.\n        \"\"\"\n        for running_target_proc, running_target_id in multiscan_proc:\n            if not running_target_proc.is_alive():\n                target_prog = self.get_scan_target_progress(\n                    scan_id, running_target_id)\n                if target_prog < 100:\n                    self.stop_scan(scan_id)\n                running_target = (running_target_proc, running_target_id)\n                multiscan_proc.remove(running_target)\n        return multiscan_proc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_progress(self, scan_id):\n\n        t_prog = dict()\n        for target in self.get_scan_target(scan_id):\n            t_prog[target] = self.get_scan_target_progress(scan_id, target)\n        return sum(t_prog.values())/len(t_prog)", "response": "Calculate the total scan progress from the partial target progress."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_scan(self, scan_id, targets, parallel=1):\n\n        os.setsid()\n        multiscan_proc = []\n        logger.info(\"%s: Scan started.\", scan_id)\n        target_list = targets\n        if target_list is None or not target_list:\n            raise OSPDError('Erroneous targets list', 'start_scan')\n\n        for index, target in enumerate(target_list):\n            while len(multiscan_proc) >= parallel:\n                progress = self.calculate_progress(scan_id)\n                self.set_scan_progress(scan_id, progress)\n                multiscan_proc = self.check_pending_target(scan_id,\n                                                           multiscan_proc)\n                time.sleep(1)\n\n            #If the scan status is stopped, does not launch anymore target scans\n            if self.get_scan_status(scan_id) == ScanStatus.STOPPED:\n                return\n\n            logger.info(\"%s: Host scan started on ports %s.\", target[0], target[1])\n            scan_process = multiprocessing.Process(target=self.parallel_scan,\n                                                   args=(scan_id, target[0]))\n            multiscan_proc.append((scan_process, target[0]))\n            scan_process.start()\n            self.set_scan_status(scan_id, ScanStatus.RUNNING)\n\n        # Wait until all single target were scanned\n        while multiscan_proc:\n            multiscan_proc = self.check_pending_target(scan_id, multiscan_proc)\n            if multiscan_proc:\n                progress = self.calculate_progress(scan_id)\n                self.set_scan_progress(scan_id, progress)\n            time.sleep(1)\n\n        # Only set the scan as finished if the scan was not stopped.\n        if self.get_scan_status(scan_id) != ScanStatus.STOPPED:\n            self.finish_scan(scan_id)", "response": "Handle N parallel scans if 'parallel' is greater than 1."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndry runs a scan.", "response": "def dry_run_scan(self, scan_id, targets):\n        \"\"\" Dry runs a scan. \"\"\"\n\n        os.setsid()\n        for _, target in enumerate(targets):\n            host = resolve_hostname(target[0])\n            if host is None:\n                logger.info(\"Couldn't resolve %s.\", target[0])\n                continue\n            port = self.get_scan_ports(scan_id, target=target[0])\n            logger.info(\"%s:%s: Dry run mode.\", host, port)\n            self.add_scan_log(scan_id, name='', host=host,\n                              value='Dry run result')\n        self.finish_scan(scan_id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_timeout(self, scan_id, host):\n        self.add_scan_error(scan_id, host=host, name=\"Timeout\",\n                            value=\"{0} exec timeout.\"\n                            .format(self.get_scanner_name()))", "response": "Handles scanner reaching timeout error."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the host to the list of finished hosts", "response": "def set_scan_host_finished(self, scan_id, target, host):\n        \"\"\" Add the host in a list of finished hosts \"\"\"\n        self.scan_collection.set_host_finished(scan_id, target, host)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_scan_target_progress(\n            self, scan_id, target, host, progress):\n        \"\"\" Sets host's progress which is part of target. \"\"\"\n        self.scan_collection.set_target_progress(\n            scan_id, target, host, progress)", "response": "Sets the scan_id s progress which is part of target."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle the get_scans command.", "response": "def handle_get_scans_command(self, scan_et):\n        \"\"\" Handles <get_scans> command.\n\n        @return: Response string for <get_scans> command.\n        \"\"\"\n\n        scan_id = scan_et.attrib.get('scan_id')\n        details = scan_et.attrib.get('details')\n        pop_res = scan_et.attrib.get('pop_results')\n        if details and details == '0':\n            details = False\n        else:\n            details = True\n            if pop_res and pop_res == '1':\n                pop_res = True\n            else:\n                pop_res = False\n\n        responses = []\n        if scan_id and scan_id in self.scan_collection.ids_iterator():\n            self.check_scan_process(scan_id)\n            scan = self.get_scan_xml(scan_id, details, pop_res)\n            responses.append(scan)\n        elif scan_id:\n            text = \"Failed to find scan '{0}'\".format(scan_id)\n            return simple_response_str('get_scans', 404, text)\n        else:\n            for scan_id in self.scan_collection.ids_iterator():\n                self.check_scan_process(scan_id)\n                scan = self.get_scan_xml(scan_id, details, pop_res)\n                responses.append(scan)\n        return simple_response_str('get_scans', 200, 'OK', responses)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_get_vts_command(self, vt_et):\n\n        vt_id = vt_et.attrib.get('vt_id')\n        vt_filter = vt_et.attrib.get('filter')\n\n        if vt_id and vt_id not in self.vts:\n            text = \"Failed to find vulnerability test '{0}'\".format(vt_id)\n            return simple_response_str('get_vts', 404, text)\n\n        filtered_vts = None\n        if vt_filter:\n            filtered_vts = self.vts_filter.get_filtered_vts_list(\n                self.vts, vt_filter)\n\n        responses = []\n\n        vts_xml = self.get_vts_xml(vt_id, filtered_vts)\n\n        responses.append(vts_xml)\n\n        return simple_response_str('get_vts', 200, 'OK', responses)", "response": "Handles the get_vts command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling <help> command. @return: Response string for <help> command.", "response": "def handle_help_command(self, scan_et):\n        \"\"\" Handles <help> command.\n\n        @return: Response string for <help> command.\n        \"\"\"\n        help_format = scan_et.attrib.get('format')\n        if help_format is None or help_format == \"text\":\n            # Default help format is text.\n            return simple_response_str('help', 200, 'OK',\n                                       self.get_help_text())\n        elif help_format == \"xml\":\n            text = self.get_xml_str(self.commands)\n            return simple_response_str('help', 200, 'OK', text)\n        raise OSPDError('Bogus help format', 'help')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the help output in plain text format.", "response": "def get_help_text(self):\n        \"\"\" Returns the help output in plain text format.\"\"\"\n\n        txt = str('\\n')\n        for name, info in self.commands.items():\n            command_txt = \"\\t{0: <22} {1}\\n\".format(name, info['description'])\n            if info['attributes']:\n                command_txt = ''.join([command_txt, \"\\t Attributes:\\n\"])\n                for attrname, attrdesc in info['attributes'].items():\n                    attr_txt = \"\\t  {0: <22} {1}\\n\".format(attrname, attrdesc)\n                    command_txt = ''.join([command_txt, attr_txt])\n            if info['elements']:\n                command_txt = ''.join([command_txt, \"\\t Elements:\\n\",\n                                       self.elements_as_text(info['elements'])])\n            txt = ''.join([txt, command_txt])\n        return txt"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef elements_as_text(self, elems, indent=2):\n        assert elems\n        text = \"\"\n        for elename, eledesc in elems.items():\n            if isinstance(eledesc, dict):\n                desc_txt = self.elements_as_text(eledesc, indent + 2)\n                desc_txt = ''.join(['\\n', desc_txt])\n            elif isinstance(eledesc, str):\n                desc_txt = ''.join([eledesc, '\\n'])\n            else:\n                assert False, \"Only string or dictionary\"\n            ele_txt = \"\\t{0}{1: <22} {2}\".format(' ' * indent, elename,\n                                                 desc_txt)\n            text = ''.join([text, ele_txt])\n        return text", "response": "Returns the elems dictionary as plain text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_delete_scan_command(self, scan_et):\n        scan_id = scan_et.attrib.get('scan_id')\n        if scan_id is None:\n            return simple_response_str('delete_scan', 404,\n                                       'No scan_id attribute')\n\n        if not self.scan_exists(scan_id):\n            text = \"Failed to find scan '{0}'\".format(scan_id)\n            return simple_response_str('delete_scan', 404, text)\n        self.check_scan_process(scan_id)\n        if self.delete_scan(scan_id):\n            return simple_response_str('delete_scan', 200, 'OK')\n        raise OSPDError('Scan in progress', 'delete_scan')", "response": "Handles the delete_scan command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes scan_id scan from collection.", "response": "def delete_scan(self, scan_id):\n        \"\"\" Deletes scan_id scan from collection.\n\n        @return: 1 if scan deleted, 0 otherwise.\n        \"\"\"\n        if self.get_scan_status(scan_id) == ScanStatus.RUNNING:\n            return 0\n\n        try:\n            del self.scan_processes[scan_id]\n        except KeyError:\n            logger.debug('Scan process for %s not found', scan_id)\n        return self.scan_collection.delete_scan(scan_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget scan_id scan s results in XML format.", "response": "def get_scan_results_xml(self, scan_id, pop_res):\n        \"\"\" Gets scan_id scan's results in XML format.\n\n        @return: String of scan results in xml.\n        \"\"\"\n        results = Element('results')\n        for result in self.scan_collection.results_iterator(scan_id, pop_res):\n            results.append(get_result_xml(result))\n\n        logger.info('Returning %d results', len(results))\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_xml_str(self, data):\n\n        responses = []\n        for tag, value in data.items():\n            elem = Element(tag)\n            if isinstance(value, dict):\n                for value in self.get_xml_str(value):\n                    elem.append(value)\n            elif isinstance(value, list):\n                value = ', '.join([m for m in value])\n                elem.text = value\n            else:\n                elem.text = value\n            responses.append(elem)\n        return responses", "response": "Creates a string of data in XML Format using the provided data structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the scan in XML format.", "response": "def get_scan_xml(self, scan_id, detailed=True, pop_res=False):\n        \"\"\" Gets scan in XML format.\n\n        @return: String of scan in XML format.\n        \"\"\"\n        if not scan_id:\n            return Element('scan')\n\n        target = ','.join(self.get_scan_target(scan_id))\n        progress = self.get_scan_progress(scan_id)\n        status = self.get_scan_status(scan_id)\n        start_time = self.get_scan_start_time(scan_id)\n        end_time = self.get_scan_end_time(scan_id)\n        response = Element('scan')\n        for name, value in [('id', scan_id),\n                            ('target', target),\n                            ('progress', progress),\n                            ('status', status.name.lower()),\n                            ('start_time', start_time),\n                            ('end_time', end_time)]:\n            response.set(name, str(value))\n        if detailed:\n            response.append(self.get_scan_results_xml(scan_id, pop_res))\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a single vulnerability test information in XML format.", "response": "def get_vt_xml(self, vt_id):\n        \"\"\" Gets a single vulnerability test information in XML format.\n\n        @return: String of single vulnerability test information in XML format.\n        \"\"\"\n        if not vt_id:\n            return Element('vt')\n\n        vt = self.vts.get(vt_id)\n\n        name = vt.get('name')\n        vt_xml = Element('vt')\n        vt_xml.set('id', vt_id)\n\n        for name, value in [('name', name)]:\n            elem = SubElement(vt_xml, name)\n            elem.text = str(value)\n\n        if vt.get('vt_params'):\n            params_xml_str = self.get_params_vt_as_xml_str(\n                vt_id, vt.get('vt_params'))\n            vt_xml.append(secET.fromstring(params_xml_str))\n\n        if vt.get('vt_refs'):\n            refs_xml_str = self.get_refs_vt_as_xml_str(\n                vt_id, vt.get('vt_refs'))\n            vt_xml.append(secET.fromstring(refs_xml_str))\n\n        if vt.get('vt_dependencies'):\n            dependencies = self.get_dependencies_vt_as_xml_str(\n                vt_id, vt.get('vt_dependencies'))\n            vt_xml.append(secET.fromstring(dependencies))\n\n        if vt.get('creation_time'):\n            vt_ctime = self.get_creation_time_vt_as_xml_str(\n                vt_id, vt.get('creation_time'))\n            vt_xml.append(secET.fromstring(vt_ctime))\n\n        if vt.get('modification_time'):\n            vt_mtime = self.get_modification_time_vt_as_xml_str(\n                vt_id, vt.get('modification_time'))\n            vt_xml.append(secET.fromstring(vt_mtime))\n\n        if vt.get('summary'):\n            summary_xml_str = self.get_summary_vt_as_xml_str(\n                vt_id, vt.get('summary'))\n            vt_xml.append(secET.fromstring(summary_xml_str))\n\n        if vt.get('impact'):\n            impact_xml_str = self.get_impact_vt_as_xml_str(\n                vt_id, vt.get('impact'))\n            vt_xml.append(secET.fromstring(impact_xml_str))\n\n        if vt.get('affected'):\n            affected_xml_str = self.get_affected_vt_as_xml_str(\n                vt_id, vt.get('affected'))\n            vt_xml.append(secET.fromstring(affected_xml_str))\n\n        if vt.get('insight'):\n            insight_xml_str = self.get_insight_vt_as_xml_str(\n                vt_id, vt.get('insight'))\n            vt_xml.append(secET.fromstring(insight_xml_str))\n\n        if vt.get('solution'):\n            solution_xml_str = self.get_solution_vt_as_xml_str(\n                vt_id, vt.get('solution'), vt.get('solution_type'))\n            vt_xml.append(secET.fromstring(solution_xml_str))\n\n        if vt.get('detection') or vt.get('qod_type') or vt.get('qod'):\n            detection_xml_str = self.get_detection_vt_as_xml_str(\n                vt_id, vt.get('detection'), vt.get('qod_type'), vt.get('qod'))\n            vt_xml.append(secET.fromstring(detection_xml_str))\n\n        if vt.get('severities'):\n            severities_xml_str = self.get_severities_vt_as_xml_str(\n                    vt_id, vt.get('severities'))\n            vt_xml.append(secET.fromstring(severities_xml_str))\n\n        if vt.get('custom'):\n            custom_xml_str = self.get_custom_vt_as_xml_str(\n                vt_id, vt.get('custom'))\n            vt_xml.append(secET.fromstring(custom_xml_str))\n\n        return vt_xml"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_vts_xml(self, vt_id=None, filtered_vts=None):\n\n        vts_xml = Element('vts')\n\n        if vt_id:\n            vts_xml.append(self.get_vt_xml(vt_id))\n        elif filtered_vts:\n            for vt_id in filtered_vts:\n                vts_xml.append(self.get_vt_xml(vt_id))\n        else:\n            for vt_id in self.vts:\n                vts_xml.append(self.get_vt_xml(vt_id))\n\n        return vts_xml", "response": "Gets the vulnerability test information in XML format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_get_scanner_details(self):\n        desc_xml = Element('description')\n        desc_xml.text = self.get_scanner_description()\n        details = [\n            desc_xml,\n            self.get_scanner_params_xml()\n        ]\n        return simple_response_str('get_scanner_details', 200, 'OK', details)", "response": "Handles the get_scanner_details command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle the get_version command.", "response": "def handle_get_version_command(self):\n        \"\"\" Handles <get_version> command.\n\n        @return: Response string for <get_version> command.\n        \"\"\"\n        protocol = Element('protocol')\n        for name, value in [('name', 'OSP'), ('version', self.get_protocol_version())]:\n            elem = SubElement(protocol, name)\n            elem.text = value\n\n        daemon = Element('daemon')\n        for name, value in [('name', self.get_daemon_name()), ('version', self.get_daemon_version())]:\n            elem = SubElement(daemon, name)\n            elem.text = value\n\n        scanner = Element('scanner')\n        for name, value in [('name', self.get_scanner_name()), ('version', self.get_scanner_version())]:\n            elem = SubElement(scanner, name)\n            elem.text = value\n\n        content = [protocol, daemon, scanner]\n\n        if self.get_vts_version():\n            vts = Element('vts')\n            elem = SubElement(vts, 'version')\n            elem.text = self.get_vts_version()\n            content.append(vts)\n\n        return simple_response_str('get_version', 200, 'OK', content)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_command(self, command):\n        try:\n            tree = secET.fromstring(command)\n        except secET.ParseError:\n            logger.debug(\"Erroneous client input: %s\", command)\n            raise OSPDError('Invalid data')\n\n        if not self.command_exists(tree.tag) and tree.tag != \"authenticate\":\n            raise OSPDError('Bogus command name')\n\n        if tree.tag == \"get_version\":\n            return self.handle_get_version_command()\n        elif tree.tag == \"start_scan\":\n            return self.handle_start_scan_command(tree)\n        elif tree.tag == \"stop_scan\":\n            return self.handle_stop_scan_command(tree)\n        elif tree.tag == \"get_scans\":\n            return self.handle_get_scans_command(tree)\n        elif tree.tag == \"get_vts\":\n            return self.handle_get_vts_command(tree)\n        elif tree.tag == \"delete_scan\":\n            return self.handle_delete_scan_command(tree)\n        elif tree.tag == \"help\":\n            return self.handle_help_command(tree)\n        elif tree.tag == \"get_scanner_details\":\n            return self.handle_get_scanner_details()\n        else:\n            assert False, \"Unhandled command: {0}\".format(tree.tag)", "response": "Handles an OSP command in a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, address, port, unix_path):\n        assert address or unix_path\n        if unix_path:\n            sock = bind_unix_socket(unix_path)\n        else:\n            sock = bind_socket(address, port)\n        if sock is None:\n            return False\n\n        sock.setblocking(False)\n        inputs = [sock]\n        outputs = []\n        try:\n            while True:\n                readable, _, _ = select.select(\n                    inputs, outputs, inputs, SCHEDULER_CHECK_PERIOD)\n                for r_socket in readable:\n                    if unix_path and r_socket is sock:\n                        client_stream, _ = sock.accept()\n                        logger.debug(\"New connection from %s\", unix_path)\n                        self.handle_client_stream(client_stream, True)\n                    else:\n                        client_stream = self.new_client_stream(sock)\n                        if client_stream is None:\n                            continue\n                        self.handle_client_stream(client_stream, False)\n                    close_client_stream(client_stream, unix_path)\n                self.scheduler()\n        except KeyboardInterrupt:\n            logger.info(\"Received Ctrl-C shutting-down ...\")\n        finally:\n            sock.shutdown(socket.SHUT_RDWR)\n            sock.close()", "response": "Starts the Daemon, handling commands until interrupted.\n\n        @return False if error. Runs indefinitely otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_scan(self, scan_id, targets, options, vts):\n        if self.scan_exists(scan_id):\n            logger.info(\"Scan %s exists. Resuming scan.\", scan_id)\n\n        return self.scan_collection.create_scan(scan_id, targets, options, vts)", "response": "Creates a new scan."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a scan s option to a provided value.", "response": "def set_scan_option(self, scan_id, name, value):\n        \"\"\" Sets a scan's option to a provided value. \"\"\"\n        return self.scan_collection.set_option(scan_id, name, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the scan s process and terminate the scan if not alive.", "response": "def check_scan_process(self, scan_id):\n        \"\"\" Check the scan's process, and terminate the scan if not alive. \"\"\"\n        scan_process = self.scan_processes[scan_id]\n        progress = self.get_scan_progress(scan_id)\n        if progress < 100 and not scan_process.is_alive():\n            self.set_scan_status(scan_id, ScanStatus.STOPPED)\n            self.add_scan_error(scan_id, name=\"\", host=\"\",\n                                value=\"Scan process failure.\")\n            logger.info(\"%s: Scan stopped with errors.\", scan_id)\n        elif progress == 100:\n            scan_process.join()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_scan_log(self, scan_id, host='', name='', value='', port='',\n                     test_id='', qod=''):\n        \"\"\" Adds a log result to scan_id scan. \"\"\"\n        self.scan_collection.add_result(scan_id, ResultType.LOG, host, name,\n                                        value, port, test_id, 0.0, qod)", "response": "Adds a log result to scan_id scan."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd an error result to scan_id scan.", "response": "def add_scan_error(self, scan_id, host='', name='', value='', port=''):\n        \"\"\" Adds an error result to scan_id scan. \"\"\"\n        self.scan_collection.add_result(scan_id, ResultType.ERROR, host, name,\n                                        value, port)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_scan_host_detail(self, scan_id, host='', name='', value=''):\n        self.scan_collection.add_result(scan_id, ResultType.HOST_DETAIL, host,\n                                        name, value)", "response": "Adds a host detail result to scan_id scan."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_scan_alarm(self, scan_id, host='', name='', value='', port='',\n                       test_id='', severity='', qod=''):\n        \"\"\" Adds an alarm result to scan_id scan. \"\"\"\n        self.scan_collection.add_result(scan_id, ResultType.ALARM, host, name,\n                                        value, port, test_id, severity, qod)", "response": "Adds an alarm result to scan_id scan."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_filters(self, vt_filter):\n\n        filter_list = vt_filter.split(';')\n        filters = list()\n        for single_filter in filter_list:\n            filter_aux = re.split('(\\W)', single_filter, 1)\n            if len(filter_aux) < 3:\n                raise OSPDError(\"Invalid number of argument in the filter\", \"get_vts\")\n            _element, _oper, _val = filter_aux\n            if _element not in self.allowed_filter:\n                raise OSPDError(\"Invalid filter element\", \"get_vts\")\n            if _oper not in self.filter_operator:\n                raise OSPDError(\"Invalid filter operator\", \"get_vts\")\n\n            filters.append(filter_aux)\n\n        return filters", "response": "Parse a string containing one or more filters separated with\n                semicolon and return a list of filters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting the value according to the given element.", "response": "def format_filter_value(self, element, value):\n        \"\"\" Calls the specific function to format value,\n        depending on the given element.\n\n        Arguments:\n            element (string): The element of the VT to be formatted.\n            value (dictionary): The element value.\n\n        Returns:\n            Returns a formatted value.\n\n        \"\"\"\n        format_func = self.allowed_filter.get(element)\n        return format_func(value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of vulnerability tests from the vts dictionary which match the filter.", "response": "def get_filtered_vts_list(self, vts, vt_filter):\n        \"\"\" Gets a collection of vulnerability test from the vts dictionary,\n        which match the filter.\n\n        Arguments:\n            vt_filter (string): Filter to apply to the vts collection.\n            vts (dictionary): The complete vts collection.\n\n        Returns:\n            Dictionary with filtered vulnerability tests.\n        \"\"\"\n        if not vt_filter:\n            raise RequiredArgument('vt_filter: A valid filter is required.')\n\n        filters = self.parse_filters(vt_filter)\n        if not filters:\n            return None\n\n        _vts_aux = vts.copy()\n        for _element, _oper, _filter_val in filters:\n            for vt_id in _vts_aux.copy():\n                if not _vts_aux[vt_id].get(_element):\n                    _vts_aux.pop(vt_id)\n                    continue\n                _elem_val = _vts_aux[vt_id].get(_element)\n                _val = self.format_filter_value(_element, _elem_val)\n                if self.filter_operator[_oper](_val, _filter_val):\n                    continue\n                else:\n                    _vts_aux.pop(vt_id)\n\n        return _vts_aux"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cvss_base_v2_value(cls, cvss_base_vector):\n        if not cvss_base_vector:\n            return None\n\n        _av, _ac, _au, _c, _i, _a = cls._parse_cvss_base_vector(\n            cvss_base_vector)\n\n        _impact = 10.41 * (1 - (1 - cvss_v2_metrics['C'].get(_c)) *\n                           (1 - cvss_v2_metrics['I'].get(_i)) *\n                           (1 - cvss_v2_metrics['A'].get(_a)))\n\n        _exploitability = (20 * cvss_v2_metrics['AV'].get(_av) *\n                           cvss_v2_metrics['AC'].get(_ac) *\n                           cvss_v2_metrics['Au'].get(_au))\n\n        f_impact = 0 if _impact == 0 else 1.176\n\n        cvss_base = ((0.6 * _impact) + (0.4 * _exploitability) - 1.5) * f_impact\n\n        return round(cvss_base, 1)", "response": "Calculates the cvss base score from a cvss base vector v2."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the cvss base score from a cvss base vector v3.", "response": "def cvss_base_v3_value(cls, cvss_base_vector):\n        \"\"\" Calculate the cvss base score from a cvss base vector\n        for cvss version 3.\n        Arguments:\n            cvss_base_vector (str) Cvss base vector v3.\n\n        Return the calculated score, None on fail.\n        \"\"\"\n        if not cvss_base_vector:\n            return None\n        _ver, _av, _ac, _pr, _ui, _s, _c, _i, _a = cls._parse_cvss_base_vector(\n            cvss_base_vector)\n\n        scope_changed = cvss_v3_metrics['S'].get(_s)\n\n        isc_base = 1 - (\n            (1 - cvss_v3_metrics['C'].get(_c)) *\n            (1 - cvss_v3_metrics['I'].get(_i)) *\n            (1 - cvss_v3_metrics['A'].get(_a))\n        )\n\n        if scope_changed:\n            _priv_req = cvss_v3_metrics['PR_SC'].get(_pr)\n        else:\n            _priv_req = cvss_v3_metrics['PR_SU'].get(_pr)\n\n        _exploitability = (\n            8.22 *\n            cvss_v3_metrics['AV'].get(_av) *\n            cvss_v3_metrics['AC'].get(_ac) *\n            _priv_req *\n            cvss_v3_metrics['UI'].get(_ui)\n        )\n\n        if scope_changed:\n            _impact = (7.52 * (isc_base - 0.029) -\n                       3.25 * pow(isc_base - 0.02, 15))\n            _base_score = min (1.08 * (_impact + _exploitability), 10)\n        else:\n            _impact = 6.42 * isc_base\n            _base_score = min (_impact + _exploitability, 10)\n\n        if _impact > 0 :\n            return cls.roundup(_base_score)\n\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inet_ntop(address_family, packed_ip):\n    global __inet_ntop\n    if __inet_ntop is None:\n        if hasattr(socket, 'inet_ntop'):\n            __inet_ntop = socket.inet_ntop\n        else:\n            from ospd import win_socket\n            __inet_ntop = win_socket.inet_ntop\n\n    return __inet_ntop(address_family, packed_ip)", "response": "A platform independent version of inet_ntop."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ipv4_range_to_list(start_packed, end_packed):\n\n    new_list = list()\n    start = struct.unpack('!L', start_packed)[0]\n    end = struct.unpack('!L', end_packed)[0]\n    for value in range(start, end + 1):\n        new_ip = socket.inet_ntoa(struct.pack('!L', value))\n        new_list.append(new_ip)\n    return new_list", "response": "Return a list of IPv4 entries from start_packed to end_packed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattempt to return a IPv4 short range list from a target string.", "response": "def target_to_ipv4_short(target):\n    \"\"\" Attempt to return a IPv4 short range list from a target string. \"\"\"\n\n    splitted = target.split('-')\n    if len(splitted) != 2:\n        return None\n    try:\n        start_packed = inet_pton(socket.AF_INET, splitted[0])\n        end_value = int(splitted[1])\n    except (socket.error, ValueError):\n        return None\n    start_value = int(binascii.hexlify(bytes(start_packed[3])), 16)\n    if end_value < 0 or end_value > 255 or end_value < start_value:\n        return None\n    end_packed = start_packed[0:3] + struct.pack('B', end_value)\n    return ipv4_range_to_list(start_packed, end_packed)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nattempt to return a IPv4 CIDR list from a target string.", "response": "def target_to_ipv4_cidr(target):\n    \"\"\" Attempt to return a IPv4 CIDR list from a target string. \"\"\"\n\n    splitted = target.split('/')\n    if len(splitted) != 2:\n        return None\n    try:\n        start_packed = inet_pton(socket.AF_INET, splitted[0])\n        block = int(splitted[1])\n    except (socket.error, ValueError):\n        return None\n    if block <= 0 or block > 30:\n        return None\n    start_value = int(binascii.hexlify(start_packed), 16) >> (32 - block)\n    start_value = (start_value << (32 - block)) + 1\n    end_value = (start_value | (0xffffffff >> block)) - 1\n    start_packed = struct.pack('!I', start_value)\n    end_packed = struct.pack('!I', end_value)\n    return ipv4_range_to_list(start_packed, end_packed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef target_to_ipv6_cidr(target):\n\n    splitted = target.split('/')\n    if len(splitted) != 2:\n        return None\n    try:\n        start_packed = inet_pton(socket.AF_INET6, splitted[0])\n        block = int(splitted[1])\n    except (socket.error, ValueError):\n        return None\n    if block <= 0 or block > 126:\n        return None\n    start_value = int(binascii.hexlify(start_packed), 16) >> (128 - block)\n    start_value = (start_value << (128 - block)) + 1\n    end_value = (start_value | (int('ff' * 16, 16) >> block)) - 1\n    high = start_value >> 64\n    low = start_value & ((1 << 64) - 1)\n    start_packed = struct.pack('!QQ', high, low)\n    high = end_value >> 64\n    low = end_value & ((1 << 64) - 1)\n    end_packed = struct.pack('!QQ', high, low)\n    return ipv6_range_to_list(start_packed, end_packed)", "response": "Attempt to return a IPv6 CIDR list from a target string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef target_to_ipv4_long(target):\n\n    splitted = target.split('-')\n    if len(splitted) != 2:\n        return None\n    try:\n        start_packed = inet_pton(socket.AF_INET, splitted[0])\n        end_packed = inet_pton(socket.AF_INET, splitted[1])\n    except socket.error:\n        return None\n    if end_packed < start_packed:\n        return None\n    return ipv4_range_to_list(start_packed, end_packed)", "response": "Attempt to return a IPv4 long - range list from a target string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of IPv6 entries from start_packed to end_packed.", "response": "def ipv6_range_to_list(start_packed, end_packed):\n    \"\"\" Return a list of IPv6 entries from start_packed to end_packed. \"\"\"\n\n    new_list = list()\n    start = int(binascii.hexlify(start_packed), 16)\n    end = int(binascii.hexlify(end_packed), 16)\n    for value in range(start, end + 1):\n        high = value >> 64\n        low = value & ((1 << 64) - 1)\n        new_ip = inet_ntop(socket.AF_INET6,\n                           struct.pack('!2Q', high, low))\n        new_list.append(new_ip)\n    return new_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef target_to_ipv6_short(target):\n\n    splitted = target.split('-')\n    if len(splitted) != 2:\n        return None\n    try:\n        start_packed = inet_pton(socket.AF_INET6, splitted[0])\n        end_value = int(splitted[1], 16)\n    except (socket.error, ValueError):\n        return None\n    start_value = int(binascii.hexlify(start_packed[14:]), 16)\n    if end_value < 0 or end_value > 0xffff or end_value < start_value:\n        return None\n    end_packed = start_packed[:14] + struct.pack('!H', end_value)\n    return ipv6_range_to_list(start_packed, end_packed)", "response": "Attempt to return a IPv6 short - range list from a target string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to return a IPv6 long - range list from a target string.", "response": "def target_to_ipv6_long(target):\n    \"\"\" Attempt to return a IPv6 long-range list from a target string. \"\"\"\n\n    splitted = target.split('-')\n    if len(splitted) != 2:\n        return None\n    try:\n        start_packed = inet_pton(socket.AF_INET6, splitted[0])\n        end_packed = inet_pton(socket.AF_INET6, splitted[1])\n    except socket.error:\n        return None\n    if end_packed < start_packed:\n        return None\n    return ipv6_range_to_list(start_packed, end_packed)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nattempting to return a single hostname list from a target string.", "response": "def target_to_hostname(target):\n    \"\"\" Attempt to return a single hostname list from a target string. \"\"\"\n\n    if len(target) == 0 or len(target) > 255:\n        return None\n    if not re.match(r'^[\\w.-]+$', target):\n        return None\n    return [target]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef target_to_list(target):\n\n    # Is it an IPv4 address ?\n    new_list = target_to_ipv4(target)\n    # Is it an IPv6 address ?\n    if not new_list:\n        new_list = target_to_ipv6(target)\n    # Is it an IPv4 CIDR ?\n    if not new_list:\n        new_list = target_to_ipv4_cidr(target)\n    # Is it an IPv6 CIDR ?\n    if not new_list:\n        new_list = target_to_ipv6_cidr(target)\n    # Is it an IPv4 short-range ?\n    if not new_list:\n        new_list = target_to_ipv4_short(target)\n    # Is it an IPv4 long-range ?\n    if not new_list:\n        new_list = target_to_ipv4_long(target)\n    # Is it an IPv6 short-range ?\n    if not new_list:\n        new_list = target_to_ipv6_short(target)\n    # Is it an IPv6 long-range ?\n    if not new_list:\n        new_list = target_to_ipv6_long(target)\n    # Is it a hostname ?\n    if not new_list:\n        new_list = target_to_hostname(target)\n    return new_list", "response": "Attempt to return a list of single hosts from a target string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef target_str_to_list(target_str):\n    new_list = list()\n    for target in target_str.split(','):\n        target = target.strip()\n        target_list = target_to_list(target)\n        if target_list:\n            new_list.extend(target_list)\n        else:\n            LOGGER.info(\"{0}: Invalid target value\".format(target))\n            return None\n    return list(collections.OrderedDict.fromkeys(new_list))", "response": "Parses a target string into a list of individual targets."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreceive a port range and expands it in individual ports.", "response": "def port_range_expand(portrange):\n    \"\"\"\n    Receive a port range and expands it in individual ports.\n\n    @input Port range.\n    e.g. \"4-8\"\n\n    @return List of integers.\n    e.g. [4, 5, 6, 7, 8]\n    \"\"\"\n    if not portrange or '-' not in portrange:\n        LOGGER.info(\"Invalid port range format\")\n        return None\n    port_list = list()\n    for single_port in range(int(portrange[:portrange.index('-')]),\n                             int(portrange[portrange.index('-') + 1:]) + 1):\n        port_list.append(single_port)\n    return port_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a str in the format T:<tcp ports U:<udp ports", "response": "def port_str_arrange(ports):\n    \"\"\" Gives a str in the format (always tcp listed first).\n    T:<tcp ports/portrange comma separated>U:<udp ports comma separated>\n    \"\"\"\n    b_tcp = ports.find(\"T\")\n    b_udp = ports.find(\"U\")\n    if (b_udp != -1 and b_tcp != -1) and b_udp < b_tcp:\n        return ports[b_tcp:] + ports[b_udp:b_tcp]\n\n    return ports"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the port string is well formed.", "response": "def ports_str_check_failed(port_str):\n    \"\"\"\n    Check if the port string is well formed.\n    Return True if fail, False other case.\n    \"\"\"\n\n    pattern = r'[^TU:0-9, \\-]'\n    if (\n        re.search(pattern, port_str)\n        or port_str.count('T') > 1\n        or port_str.count('U') > 1\n        or port_str.count(':') < (port_str.count('T') + port_str.count('U'))\n    ):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ports_as_list(port_str):\n    if not port_str:\n        LOGGER.info(\"Invalid port value\")\n        return [None, None]\n\n    if ports_str_check_failed(port_str):\n        LOGGER.info(\"{0}: Port list malformed.\")\n        return [None, None]\n\n    tcp_list = list()\n    udp_list = list()\n    ports = port_str.replace(' ', '')\n    b_tcp = ports.find(\"T\")\n    b_udp = ports.find(\"U\")\n\n    if ports[b_tcp - 1] == ',':\n        ports = ports[:b_tcp - 1] + ports[b_tcp:]\n    if ports[b_udp - 1] == ',':\n        ports = ports[:b_udp - 1] + ports[b_udp:]\n    ports = port_str_arrange(ports)\n\n    tports = ''\n    uports = ''\n    # TCP ports listed first, then UDP ports\n    if b_udp != -1 and b_tcp != -1:\n        tports = ports[ports.index('T:') + 2:ports.index('U:')]\n        uports = ports[ports.index('U:') + 2:]\n    # Only UDP ports\n    elif b_tcp == -1 and b_udp != -1:\n        uports = ports[ports.index('U:') + 2:]\n    # Only TCP ports\n    elif b_udp == -1 and b_tcp != -1:\n        tports = ports[ports.index('T:') + 2:]\n    else:\n        tports = ports\n\n    if tports:\n        for port in tports.split(','):\n            if '-' in port:\n                tcp_list.extend(port_range_expand(port))\n            else:\n                tcp_list.append(int(port))\n        tcp_list.sort()\n    if uports:\n        for port in uports.split(','):\n            if '-' in port:\n                udp_list.extend(port_range_expand(port))\n            else:\n                udp_list.append(int(port))\n        udp_list.sort()\n\n    return (tcp_list, udp_list)", "response": "Parses a string containing a port list into two list of individual tcp and udp ports."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompress a port list and return a string.", "response": "def port_list_compress(port_list):\n    \"\"\" Compress a port list and return a string. \"\"\"\n\n    if not port_list or len(port_list) == 0:\n        LOGGER.info(\"Invalid or empty port list.\")\n        return ''\n\n    port_list = sorted(set(port_list))\n    compressed_list = []\n    for key, group in itertools.groupby(enumerate(port_list),\n                                        lambda t: t[1] - t[0]):\n        group = list(group)\n        if group[0][1] == group[-1][1]:\n            compressed_list.append(str(group[0][1]))\n        else:\n            compressed_list.append(str(group[0][1]) + '-' + str(group[-1][1]))\n\n    return ','.join(compressed_list)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef valid_uuid(value):\n\n    try:\n        uuid.UUID(value, version=4)\n        return True\n    except (TypeError, ValueError, AttributeError):\n        return False", "response": "Check if value is a valid UUID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_args_parser(description):\n\n    parser = argparse.ArgumentParser(description=description)\n\n    def network_port(string):\n        \"\"\" Check if provided string is a valid network port. \"\"\"\n\n        value = int(string)\n        if not 0 < value <= 65535:\n            raise argparse.ArgumentTypeError(\n                'port must be in ]0,65535] interval')\n        return value\n\n    def cacert_file(cacert):\n        \"\"\" Check if provided file is a valid CA Certificate \"\"\"\n        try:\n            context = ssl.create_default_context(cafile=cacert)\n        except AttributeError:\n            # Python version < 2.7.9\n            return cacert\n        except IOError:\n            raise argparse.ArgumentTypeError('CA Certificate not found')\n        try:\n            not_after = context.get_ca_certs()[0]['notAfter']\n            not_after = ssl.cert_time_to_seconds(not_after)\n            not_before = context.get_ca_certs()[0]['notBefore']\n            not_before = ssl.cert_time_to_seconds(not_before)\n        except (KeyError, IndexError):\n            raise argparse.ArgumentTypeError('CA Certificate is erroneous')\n        if not_after < int(time.time()):\n            raise argparse.ArgumentTypeError('CA Certificate expired')\n        if not_before > int(time.time()):\n            raise argparse.ArgumentTypeError('CA Certificate not active yet')\n        return cacert\n\n    def log_level(string):\n        \"\"\" Check if provided string is a valid log level. \"\"\"\n\n        value = getattr(logging, string.upper(), None)\n        if not isinstance(value, int):\n            raise argparse.ArgumentTypeError(\n                'log level must be one of {debug,info,warning,error,critical}')\n        return value\n\n    def filename(string):\n        \"\"\" Check if provided string is a valid file path. \"\"\"\n\n        if not os.path.isfile(string):\n            raise argparse.ArgumentTypeError(\n                '%s is not a valid file path' % string)\n        return string\n\n    parser.add_argument('-p', '--port', default=PORT, type=network_port,\n                        help='TCP Port to listen on. Default: {0}'.format(PORT))\n    parser.add_argument('-b', '--bind-address', default=ADDRESS,\n                        help='Address to listen on. Default: {0}'\n                        .format(ADDRESS))\n    parser.add_argument('-u', '--unix-socket',\n                        help='Unix file socket to listen on.')\n    parser.add_argument('-k', '--key-file', type=filename,\n                        help='Server key file. Default: {0}'.format(KEY_FILE))\n    parser.add_argument('-c', '--cert-file', type=filename,\n                        help='Server cert file. Default: {0}'.format(CERT_FILE))\n    parser.add_argument('--ca-file', type=cacert_file,\n                        help='CA cert file. Default: {0}'.format(CA_FILE))\n    parser.add_argument('-L', '--log-level', default='warning', type=log_level,\n                        help='Wished level of logging. Default: WARNING')\n    parser.add_argument('--foreground', action='store_true',\n                        help='Run in foreground and logs all messages to console.')\n    parser.add_argument('-l', '--log-file', type=filename,\n                        help='Path to the logging file.')\n    parser.add_argument('--version', action='store_true',\n                        help='Print version then exit.')\n    return parser", "response": "Create an argument parser for OSPD."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef go_to_background():\n    try:\n        if os.fork():\n            sys.exit()\n    except OSError as errmsg:\n        LOGGER.error('Fork failed: {0}'.format(errmsg))\n        sys.exit('Fork failed')", "response": "Daemonize the running process."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns list of OSPD common command - line arguments from parser after validating provided values or setting default ones.", "response": "def get_common_args(parser, args=None):\n    \"\"\" Return list of OSPD common command-line arguments from parser, after\n    validating provided values or setting default ones.\n\n    \"\"\"\n\n    options = parser.parse_args(args)\n    # TCP Port to listen on.\n    port = options.port\n\n    # Network address to bind listener to\n    address = options.bind_address\n\n    # Unix file socket to listen on\n    unix_socket = options.unix_socket\n\n    # Debug level.\n    log_level = options.log_level\n\n    # Server key path.\n    keyfile = options.key_file or KEY_FILE\n\n    # Server cert path.\n    certfile = options.cert_file or CERT_FILE\n\n    # CA cert path.\n    cafile = options.ca_file or CA_FILE\n\n    common_args = dict()\n    common_args['port'] = port\n    common_args['address'] = address\n    common_args['unix_socket'] = unix_socket\n    common_args['keyfile'] = keyfile\n    common_args['certfile'] = certfile\n    common_args['cafile'] = cafile\n    common_args['log_level'] = log_level\n    common_args['foreground'] = options.foreground\n    common_args['log_file'] = options.log_file\n    common_args['version'] = options.version\n\n    return common_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_version(wrapper):\n\n    scanner_name = wrapper.get_scanner_name()\n    server_version = wrapper.get_server_version()\n    print(\"OSP Server for {0} version {1}\".format(scanner_name, server_version))\n    protocol_version = wrapper.get_protocol_version()\n    print(\"OSP Version: {0}\".format(protocol_version))\n    daemon_name = wrapper.get_daemon_name()\n    daemon_version = wrapper.get_daemon_version()\n    print(\"Using: {0} {1}\".format(daemon_name, daemon_version))\n    print(\"Copyright (C) 2014, 2015 Greenbone Networks GmbH\\n\"\n          \"License GPLv2+: GNU GPL version 2 or later\\n\"\n          \"This is free software: you are free to change\"\n          \" and redistribute it.\\n\"\n          \"There is NO WARRANTY, to the extent permitted by law.\")", "response": "Prints the server version and license information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a result to a scan in the table.", "response": "def add_result(self, scan_id, result_type, host='', name='', value='',\n                   port='', test_id='', severity='', qod=''):\n        \"\"\" Add a result to a scan in the table. \"\"\"\n\n        assert scan_id\n        assert len(name) or len(value)\n        result = dict()\n        result['type'] = result_type\n        result['name'] = name\n        result['severity'] = severity\n        result['test_id'] = test_id\n        result['value'] = value\n        result['host'] = host\n        result['port'] = port\n        result['qod'] = qod\n        results = self.scans_table[scan_id]['results']\n        results.append(result)\n        # Set scan_info's results to propagate results to parent process.\n        self.scans_table[scan_id]['results'] = results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_progress(self, scan_id, progress):\n\n        if progress > 0 and progress <= 100:\n            self.scans_table[scan_id]['progress'] = progress\n        if progress == 100:\n            self.scans_table[scan_id]['end_time'] = int(time.time())", "response": "Sets the progress of a scan."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the target_progress of a scan.", "response": "def set_target_progress(self, scan_id, target, host, progress):\n        \"\"\" Sets scan_id scan's progress. \"\"\"\n        if progress > 0 and progress <= 100:\n            targets =  self.scans_table[scan_id]['target_progress']\n            targets[target][host] = progress\n            # Set scan_info's target_progress to propagate progresses\n            # to parent process.\n            self.scans_table[scan_id]['target_progress'] = targets"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the host to the list of finished hosts", "response": "def set_host_finished(self, scan_id, target, host):\n        \"\"\" Add the host in a list of finished hosts \"\"\"\n        finished_hosts = self.scans_table[scan_id]['finished_hosts']\n        finished_hosts[target].extend(host)\n        self.scans_table[scan_id]['finished_hosts'] = finished_hosts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of finished hosts.", "response": "def get_hosts_unfinished(self, scan_id):\n        \"\"\" Get a list of finished hosts.\"\"\"\n\n        unfinished_hosts = list()\n        for target in self.scans_table[scan_id]['finished_hosts']:\n            unfinished_hosts.extend(target_str_to_list(target))\n        for target in self.scans_table[scan_id]['finished_hosts']:\n           for host in self.scans_table[scan_id]['finished_hosts'][target]:\n               unfinished_hosts.remove(host)\n\n        return unfinished_hosts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef results_iterator(self, scan_id, pop_res):\n        if pop_res:\n            result_aux = self.scans_table[scan_id]['results']\n            self.scans_table[scan_id]['results'] = list()\n            return iter(result_aux)\n\n        return iter(self.scans_table[scan_id]['results'])", "response": "Returns an iterator over the results of a scan."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves results for hosts that are not finished.", "response": "def del_results_for_stopped_hosts(self, scan_id):\n        \"\"\" Remove results from the result table for those host\n        \"\"\"\n        unfinished_hosts = self.get_hosts_unfinished(scan_id)\n        for result in self.results_iterator(scan_id, False):\n            if result['host'] in unfinished_hosts:\n                self.remove_single_result(scan_id, result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resume_scan(self, scan_id, options):\n        self.scans_table[scan_id]['status'] = ScanStatus.INIT\n        if options:\n            self.scans_table[scan_id]['options'] = options\n\n        self.del_results_for_stopped_hosts(scan_id)\n\n        return scan_id", "response": "Reset the scan status in the scan_table to INIT.\n        Also, overwrite the options, because a resume task cmd\n        can add some new option. E.g. exclude hosts list.\n        Parameters:\n            scan_id (uuid): Scan ID to identify the scan process to be resumed.\n            options (dict): Options for the scan to be resumed. This options\n                            are not added to the already existent ones.\n                            The old ones are removed\n\n        Return:\n            Scan ID which identifies the current scan."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_scan(self, scan_id='', targets='', options=None, vts=''):\n\n        if self.data_manager is None:\n            self.data_manager = multiprocessing.Manager()\n\n        # Check if it is possible to resume task. To avoid to resume, the\n        # scan must be deleted from the scans_table.\n        if scan_id and self.id_exists(scan_id) and (\n                self.get_status(scan_id) == ScanStatus.STOPPED):\n            return self.resume_scan(scan_id, options)\n\n        if not options:\n            options = dict()\n        scan_info = self.data_manager.dict()\n        scan_info['results'] = list()\n        scan_info['finished_hosts'] = dict(\n            [[target, []] for target, _, _ in targets])\n        scan_info['progress'] = 0\n        scan_info['target_progress'] = dict(\n            [[target, {}] for target, _, _ in targets])\n        scan_info['targets'] = targets\n        scan_info['vts'] = vts\n        scan_info['options'] = options\n        scan_info['start_time'] = int(time.time())\n        scan_info['end_time'] = \"0\"\n        scan_info['status'] = ScanStatus.INIT\n        if scan_id is None or scan_id == '':\n            scan_id = str(uuid.uuid4())\n        scan_info['scan_id'] = scan_id\n        self.scans_table[scan_id] = scan_info\n        return scan_id", "response": "Creates a new scan with the provided scan information."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_option(self, scan_id, name, value):\n\n        self.scans_table[scan_id]['options'][name] = value", "response": "Set a scan s name option to value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a target s current progress value.", "response": "def get_target_progress(self, scan_id, target):\n        \"\"\" Get a target's current progress value.\n        The value is calculated with the progress of each single host\n        in the target.\"\"\"\n\n        total_hosts = len(target_str_to_list(target))\n        host_progresses = self.scans_table[scan_id]['target_progress'].get(target)\n        try:\n            t_prog = sum(host_progresses.values()) / total_hosts\n        except ZeroDivisionError:\n            LOGGER.error(\"Zero division error in \", get_target_progress.__name__)\n            raise\n        return t_prog"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_target_list(self, scan_id):\n\n        target_list = []\n        for target, _, _ in self.scans_table[scan_id]['targets']:\n            target_list.append(target)\n        return target_list", "response": "Get a scan s target list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a scan s ports list.", "response": "def get_ports(self, scan_id, target):\n        \"\"\" Get a scan's ports list. If a target is specified\n        it will return the corresponding port for it. If not,\n        it returns the port item of the first nested list in\n        the target's list.\n        \"\"\"\n        if target:\n            for item in self.scans_table[scan_id]['targets']:\n                if target == item[0]:\n                    return item[1]\n\n        return self.scans_table[scan_id]['targets'][0][1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a scan s credential list. It return dictionary with COOKIE name the corresponding credential for a given target.", "response": "def get_credentials(self, scan_id, target):\n        \"\"\" Get a scan's credential list. It return dictionary with\n        the corresponding credential for a given target.\n        \"\"\"\n        if target:\n            for item in self.scans_table[scan_id]['targets']:\n                if target == item[0]:\n                    return item[2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_scan(self, scan_id):\n\n        if self.get_status(scan_id) == ScanStatus.RUNNING:\n            return False\n        self.scans_table.pop(scan_id)\n        if len(self.scans_table) == 0:\n            del self.data_manager\n            self.data_manager = None\n        return True", "response": "Delete a scan from the list of scans."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning string name of a result type.", "response": "def get_str(cls, result_type):\n        \"\"\" Return string name of a result type. \"\"\"\n        if result_type == cls.ALARM:\n            return \"Alarm\"\n        elif result_type == cls.LOG:\n            return \"Log Message\"\n        elif result_type == cls.ERROR:\n            return \"Error Message\"\n        elif result_type == cls.HOST_DETAIL:\n            return \"Host Detail\"\n        else:\n            assert False, \"Erroneous result type {0}.\".format(result_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning string name of a result type.", "response": "def get_type(cls, result_name):\n        \"\"\" Return string name of a result type. \"\"\"\n        if result_name == \"Alarm\":\n            return cls.ALARM\n        elif result_name == \"Log Message\":\n            return cls.LOG\n        elif result_name == \"Error Message\":\n            return cls.ERROR\n        elif result_name == \"Host Detail\":\n            return cls.HOST_DETAIL\n        else:\n            assert False, \"Erroneous result name {0}.\".format(result_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_float(obj):\n    is_f = isinstance(obj, float)\n    if not is_f:\n        try:\n            float(obj)\n            is_f = True\n        except (ValueError, TypeError):\n            is_f = False\n    return is_f and not is_bool(obj)", "response": "Checks if the object is a float."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_timestamp(obj):\n    return isinstance(obj, datetime.datetime) or is_string(obj) or is_int(obj) or is_float(obj)", "response": "Check if the object is a timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize logging settings with default set to INFO", "response": "def init_logging(log_level):\n    \"\"\"\n    Init logging settings with default set to INFO\n    \"\"\"\n    log_level = log_level_to_string_map[min(log_level, 5)]\n\n    msg = \"%(levelname)s - %(name)s:%(lineno)s - %(message)s\" if log_level in os.environ else \"%(levelname)s - %(message)s\"\n\n    logging_conf = {\n        \"version\": 1,\n        \"root\": {\n            \"level\": log_level,\n            \"handlers\": [\"console\"]\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"level\": log_level,\n                \"formatter\": \"simple\",\n                \"stream\": \"ext://sys.stdout\"\n            }\n        },\n        \"formatters\": {\n            \"simple\": {\n                \"format\": \" {0}\".format(msg)\n            }\n        }\n    }\n\n    logging.config.dictConfig(logging_conf)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keywords(self):\n        defined_keywords = [\n            ('allowempty_map', 'allowempty_map'),\n            ('assertion', 'assertion'),\n            ('default', 'default'),\n            ('class', 'class'),\n            ('desc', 'desc'),\n            ('enum', 'enum'),\n            ('example', 'example'),\n            ('extensions', 'extensions'),\n            ('format', 'format'),\n            ('func', 'func'),\n            ('ident', 'ident'),\n            ('include_name', 'include'),\n            ('length', 'length'),\n            ('map_regex_rule', 'map_regex_rule'),\n            ('mapping', 'mapping'),\n            ('matching', 'matching'),\n            ('matching_rule', 'matching_rule'),\n            ('name', 'name'),\n            ('nullable', 'nullable')\n            ('parent', 'parent'),\n            ('pattern', 'pattern'),\n            ('pattern_regexp', 'pattern_regexp'),\n            ('range', 'range'),\n            ('regex_mappings', 'regex_mappings'),\n            ('required', 'required'),\n            ('schema', 'schema'),\n            ('schema_str', 'schema_str'),\n            ('sequence', 'sequence'),\n            ('type', 'type'),\n            ('type_class', 'type_class'),\n            ('unique', 'unique'),\n            ('version', 'version'),\n        ]\n        found_keywords = []\n\n        for var_name, keyword_name in defined_keywords:\n            if getattr(self, var_name, None):\n                found_keywords.append(keyword_name)\n\n        return found_keywords", "response": "Returns a list of all keywords that this rule object has defined."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_type_keywords(self, schema, rule, path):\n        if not self.strict_rule_validation:\n            return\n\n        global_keywords = ['type', 'desc', 'example', 'extensions', 'name', 'nullable', 'version', 'func', 'include']\n        all_allowed_keywords = {\n            'str': global_keywords + ['default', 'pattern', 'range', 'enum', 'required', 'unique', 'req'],\n            'int': global_keywords + ['default', 'range', 'enum', 'required', 'unique'],\n            'float': global_keywords + ['default', 'enum', 'range', 'required'],\n            'number': global_keywords + ['default', 'enum'],\n            'bool': global_keywords + ['default', 'enum'],\n            'map': global_keywords + ['allowempty_map', 'mapping', 'map', 'allowempty', 'required', 'matching-rule', 'range', 'class'],\n            'seq': global_keywords + ['sequence', 'seq', 'required', 'range', 'matching'],\n            'sequence': global_keywords + ['sequence', 'seq', 'required'],\n            'mapping': global_keywords + ['mapping', 'seq', 'required'],\n            'timestamp': global_keywords + ['default', 'enum'],\n            'date': global_keywords + ['default', 'enum'],\n            'symbol': global_keywords + ['default', 'enum'],\n            'scalar': global_keywords + ['default', 'enum'],\n            'text': global_keywords + ['default', 'enum', 'pattern'],\n            'any': global_keywords + ['default', 'enum'],\n            'enum': global_keywords + ['default', 'enum'],\n            'none': global_keywords + ['default', 'enum', 'required'],\n        }\n        rule_type = schema.get('type')\n        if not rule_type:\n            # Special cases for the \"shortcut methods\"\n            if 'sequence' in schema or 'seq' in schema:\n                rule_type = 'sequence'\n            elif 'mapping' in schema or 'map' in schema:\n                rule_type = 'mapping'\n\n        allowed_keywords = all_allowed_keywords.get(rule_type)\n        if not allowed_keywords and 'sequence' not in schema and 'mapping' not in schema and 'seq' not in schema and 'map' not in schema:\n            raise RuleError('No allowed keywords found for type: {0}'.format(rule_type))\n\n        for k, v in schema.items():\n            if k not in allowed_keywords:\n                raise RuleError('Keyword \"{0}\" is not supported for type: \"{1}\" '.format(k, rule_type))", "response": "Check the type keywords for the type of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_extensions(self):\n        log.debug(u\"loading all extensions : %s\", self.extensions)\n\n        self.loaded_extensions = []\n\n        for f in self.extensions:\n            if not os.path.isabs(f):\n                f = os.path.abspath(f)\n\n            if not os.path.exists(f):\n                raise CoreError(u\"Extension file: {0} not found on disk\".format(f))\n\n            self.loaded_extensions.append(imp.load_source(\"\", f))\n\n        log.debug(self.loaded_extensions)\n        log.debug([dir(m) for m in self.loaded_extensions])", "response": "Load all extension files into the namespace pykwalify. ext\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_range(self, max_, min_, max_ex, min_ex, value, path, prefix):\n        if not isinstance(value, int) and not isinstance(value, float):\n            raise CoreError(\"Value must be a integer type\")\n\n        log.debug(\n            u\"Validate range : %s : %s : %s : %s : %s : %s\",\n            max_,\n            min_,\n            max_ex,\n            min_ex,\n            value,\n            path,\n        )\n\n        if max_ is not None and max_ < value:\n                self.errors.append(SchemaError.SchemaErrorEntry(\n                    msg=u\"Type '{prefix}' has size of '{value}', greater than max limit '{max_}'. Path: '{path}'\",\n                    path=path,\n                    value=nativestr(value) if tt['str'](value) else value,\n                    prefix=prefix,\n                    max_=max_))\n\n        if min_ is not None and min_ > value:\n                self.errors.append(SchemaError.SchemaErrorEntry(\n                    msg=u\"Type '{prefix}' has size of '{value}', less than min limit '{min_}'. Path: '{path}'\",\n                    path=path,\n                    value=nativestr(value) if tt['str'](value) else value,\n                    prefix=prefix,\n                    min_=min_))\n\n        if max_ex is not None and max_ex <= value:\n                self.errors.append(SchemaError.SchemaErrorEntry(\n                    msg=u\"Type '{prefix}' has size of '{value}', greater than or equals to max limit(exclusive) '{max_ex}'. Path: '{path}'\",\n                    path=path,\n                    value=nativestr(value) if tt['str'](value) else value,\n                    prefix=prefix,\n                    max_ex=max_ex))\n\n        if min_ex is not None and min_ex >= value:\n                self.errors.append(SchemaError.SchemaErrorEntry(\n                    msg=u\"Type '{prefix}' has size of '{value}', less than or equals to min limit(exclusive) '{min_ex}'. Path: '{path}'\",\n                    path=path,\n                    value=nativestr(value) if tt['str'](value) else value,\n                    prefix=prefix,\n                    min_ex=min_ex))", "response": "Validate that value is within range values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_cli():\n\n    #\n    # 1. parse cli arguments\n    #\n\n    __docopt__ = \"\"\"\nusage: pykwalify -d FILE -s FILE ... [-e FILE ...]\n       [--strict-rule-validation] [--fix-ruby-style-regex] [--allow-assertions] [--encoding ENCODING]\n       [-v ...] [-q]\n\noptional arguments:\n  -d FILE, --data-file FILE            the file to be tested\n  -e FILE, --extension FILE            file containing python extension\n  -s FILE, --schema-file FILE          schema definition file\n  --fix-ruby-style-regex               This flag fixes some of the quirks of ruby style regex\n                                       that is not compatible with python style regex\n  --strict-rule-validation             enables strict validation of all keywords for all\n                                       Rule objects to find unsupported keyword usage\n  --allow-assertions                   By default assertions is disabled due to security risk.\n                                       Error will be raised if assertion is used in schema\n                                       but this flag is not used. This option enables assert keyword.\n  --encoding ENCODING                  Specify encoding to open data and schema files with.\n  -h, --help                           show this help message and exit\n  -q, --quiet                          suppress terminal output\n  -v, --verbose                        verbose terminal output (multiple -v increases verbosity)\n  --version                            display the version number and exit\n\"\"\"\n\n    # Import pykwalify package\n    import pykwalify\n\n    args = docopt(__docopt__, version=pykwalify.__version__)\n\n    pykwalify.init_logging(1 if args[\"--quiet\"] else args[\"--verbose\"])\n    log = logging.getLogger(__name__)\n\n    #\n    # 2. validate arguments only, dont go into other code/logic\n    #\n\n    log.debug(\"Setting verbose level: %s\", args[\"--verbose\"])\n    log.debug(\"Arguments from CLI: %s\", args)\n\n    return args", "response": "This function parses the CLI arguments and returns a tuple of the parsed rules."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits the functionality into 2 methods. One for parsing the cli and one that runs the application.", "response": "def run(cli_args):\n    \"\"\"\n    Split the functionality into 2 methods.\n\n    One for parsing the cli and one that runs the application.\n    \"\"\"\n    from .core import Core\n\n    c = Core(\n        source_file=cli_args[\"--data-file\"],\n        schema_files=cli_args[\"--schema-file\"],\n        extensions=cli_args['--extension'],\n        strict_rule_validation=cli_args['--strict-rule-validation'],\n        fix_ruby_style_regex=cli_args['--fix-ruby-style-regex'],\n        allow_assertions=cli_args['--allow-assertions'],\n        file_encoding=cli_args['--encoding'],\n    )\n    c.validate()\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pbdesign(n):\r\n    assert n>0, 'Number of factors must be a positive integer'\r\n    keep = int(n)\r\n    n = 4*(int(n/4) + 1)  # calculate the correct number of rows (multiple of 4)\r\n    f, e = np.frexp([n, n/12., n/20.])\r\n    k = [idx for idx, val in enumerate(np.logical_and(f==0.5, e>0)) if val]\r\n    \r\n    assert isinstance(n, int) and k!=[], 'Invalid inputs. n must be a multiple of 4.'\r\n    \r\n    k = k[0]\r\n    e = e[k] - 1\r\n    \r\n    if k==0:  # N = 1*2**e\r\n        H = np.ones((1, 1))\r\n    elif k==1:  # N = 12*2**e\r\n        H = np.vstack((np.ones((1, 12)), np.hstack((np.ones((11, 1)), \r\n            toeplitz([-1, -1, 1, -1, -1, -1, 1, 1, 1, -1, 1],\r\n                     [-1, 1, -1, 1, 1, 1, -1, -1, -1, 1, -1])))))\r\n    elif k==2:  # N = 20*2**e\r\n        H = np.vstack((np.ones((1, 20)), np.hstack((np.ones((19, 1)),\r\n            hankel(\r\n            [-1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1, 1],\r\n            [1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, 1, -1, 1, 1, 1, 1, -1, -1])\r\n            ))))\r\n    \r\n    # Kronecker product construction\r\n    for i in range(e):\r\n        H = np.vstack((np.hstack((H, H)), np.hstack((H, -H))))\r\n    \r\n    # Reduce the size of the matrix as needed\r\n    H = H[:, 1:(keep + 1)]\r\n    \r\n    return np.flipud(H)", "response": "Returns a new design matrix with n columns and the next multiple of 4 higher than n."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef star(n, alpha='faced', center=(1, 1)):\r\n    # Star points at the center of each face of the factorial\r\n    if alpha=='faced':\r\n        a = 1\r\n    elif alpha=='orthogonal':\r\n        nc = 2**n  # factorial points\r\n        nco = center[0]  # center points to factorial\r\n        na = 2*n  # axial points\r\n        nao = center[1]  # center points to axial design\r\n        # value of alpha in orthogonal design\r\n        a = (n*(1 + nao/float(na))/(1 + nco/float(nc)))**0.5\r\n    elif alpha=='rotatable':\r\n        nc = 2**n  # number of factorial points\r\n        a = nc**(0.25)  # value of alpha in rotatable design\r\n    else:\r\n        raise ValueError('Invalid value for \"alpha\": {:}'.format(alpha))\r\n    \r\n    # Create the actual matrix now.\r\n    H = np.zeros((2*n, n))\r\n    for i in range(n):\r\n        H[2*i:2*i+2, i] = [-1, 1]\r\n    \r\n    H *= a\r\n    \r\n    return H, a", "response": "Returns a new version of the next n design matrices with the same alpha value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fold(H, columns=None):\r\n    H = np.array(H)\r\n    assert len(H.shape)==2, 'Input design matrix must be 2d.'\r\n    \r\n    if columns is None:\r\n        columns = range(H.shape[1])\r\n    \r\n    Hf = H.copy()\r\n    \r\n    for col in columns:\r\n        vals = np.unique(H[:, col])\r\n        assert len(vals)==2, 'Input design matrix must be 2-level factors only.'\r\n        \r\n        for i in range(H.shape[0]):\r\n            Hf[i, col] = vals[0] if H[i, col]==vals[1] else vals[1]\r\n    \r\n    Hf = np.vstack((H, Hf))\r\n    \r\n    return Hf", "response": "Folds a design matrix into 2 - level factors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_regression_matrix(H, model, build=None):\r\n    ListOfTokens = model.split(' ')\r\n    if H.shape[1]==1:\r\n        size_index = len(str(H.shape[0]))\r\n    else:\r\n        size_index = len(str(H.shape[1]))\r\n    \r\n    if build is None:\r\n        build = [True]*len(ListOfTokens)\r\n    \r\n    # Test if the vector has the wrong direction (lines instead of columns)\r\n    if H.shape[0]==1:\r\n        H = H.T\r\n    \r\n    # Collect the list of monomials\r\n    Monom_Index = []\r\n    for i in range(len(ListOfTokens)):\r\n        if build[i]:\r\n            Monom_Index += [grep(ListOfTokens, 'x' + str(0)*(size_index - \\\r\n            len(str(i))) + str(i))]\r\n    \r\n    Monom_Index = -np.sort(-Monom_Index)\r\n    Monom_Index = np.unique(Monom_Index)\r\n    \r\n    if H.shape[1]==1:\r\n        nb_var = H.shape[0]  # vector \"mode\": the number of vars is equal to the number of lines of H\r\n        VectorMode = True\r\n        \r\n        for i in range(nb_var):\r\n            for j in range(ListOfTokens.shape[0]):\r\n                ListOfTokens[j] = ListOfTokens[j].replace(\r\n                    'x' + str(0)*(size_index - len(str(i))) + str(i),\r\n                    'H(' + str(i) + ')')\r\n    else:\r\n        nb_var = H.shape[0]  # matrix \"mode\": the number of vars is equal to the number of columns of H\r\n        VectorMode = False\r\n        \r\n        for i in range(nb_var):\r\n            for j in range(ListOfTokens.shape[0]):\r\n                ListOfTokens[j] = ListOfTokens[j].replace(\r\n                    'x' + str(0)*(size_index - len(str(i))) + str(i),\r\n                    'H[i,' + str(i) + ')')\r\n    \r\n    # Now build the regression matrix\r\n    if VectorMode:\r\n        R = np.zeros((len(ListOfTokens), 1))\r\n        for j in range(len(ListOfTokens)):\r\n            R[j, 0] = eval(ListOfTokens[j])\r\n    else:\r\n        R = np.zeros((H.shape[0], len(ListOfTokens)))\r\n        for i in range(H.shape[0]):\r\n            for j in range(len(ListOfTokens)):\r\n                R[i, j] = eval(ListOfTokens[j])\r\n    \r\n    return R", "response": "Builds a regression matrix using a DOE matrix and a list of monomials."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_bedtool(iterator):\n    def gen():\n        for i in iterator:\n            yield helpers.asinterval(i)\n    return pybedtools.BedTool(gen())", "response": "Convert any iterator into a pybedtools. BedTool object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tsses(db, merge_overlapping=False, attrs=None, attrs_sep=\":\",\n          merge_kwargs=None, as_bed6=False, bedtools_227_or_later=True):\n    \"\"\"\n    Create 1-bp transcription start sites for all transcripts in the database\n    and return as a sorted pybedtools.BedTool object pointing to a temporary\n    file.\n\n    To save the file to a known location, use the `.moveto()` method on the\n    resulting `pybedtools.BedTool` object.\n\n    To extend regions upstream/downstream, see the `.slop()` method on the\n    resulting `pybedtools.BedTool object`.\n\n    Requires pybedtools.\n\n    Parameters\n    ----------\n    db : gffutils.FeatureDB\n        The database to use\n\n    as_bed6 : bool\n        If True, output file is in BED6 format; otherwise it remains in the\n        GFF/GTF format and dialect of the file used to create the database.\n\n        Note that the merge options below necessarily force `as_bed6=True`.\n\n    merge_overlapping : bool\n        If True, output will be in BED format. Overlapping TSSes will be merged\n        into a single feature, and their names will be collapsed using\n        `merge_sep` and placed in the new name field.\n\n    merge_kwargs : dict\n        If `merge_overlapping=True`, these keyword arguments are passed to\n        pybedtools.BedTool.merge(), which are in turn sent to `bedtools merge`.\n        The merge operates on a BED6 file which will have had the name field\n        constructed as specified by other arguments here.  See the available\n        options for your installed version of BEDTools; the defaults used here\n        are `merge_kwargs=dict(o='distinct', c=4, s=True)`.\n\n        Any provided `merge_kwargs` are used to *update* the default. It is\n        recommended to not override `c=4` and `s=True`, otherwise the\n        post-merge fixing may not work correctly. Good candidates for tweaking\n        are `d` (merge distance), `o` (operation), `delim` (delimiter to use\n        for collapse operations).\n\n    attrs : str or list\n        Only has an effect when `as_bed6=True` or `merge_overlapping=True`.\n\n        Determines what goes in the name field of an output BED file. By\n        default, \"gene_id\" for GTF databases and \"ID\" for GFF. If a list of\n        attributes is supplied, e.g. [\"gene_id\", \"transcript_id\"], then these\n        will be joined by `attr_join_sep` and then placed in the name field.\n\n    attrs_sep: str\n        If `as_bed6=True` or `merge_overlapping=True`, then use this character\n        to separate attributes in the name field of the output BED. If also\n        using `merge_overlapping=True`, you'll probably want this to be\n        different than `merge_sep` in order to parse things out later.\n\n    bedtools_227_or_later : bool\n        In version 2.27, BEDTools changed the output for merge. By default,\n        this function expects BEDTools version 2.27 or later, but set this to\n        False to assume the older behavior.\n\n        For testing purposes, the environment variable\n        GFFUTILS_USES_BEDTOOLS_227_OR_LATER is set to either \"true\" or \"false\"\n        and is used to override this argument.\n\n    Examples\n    --------\n\n    >>> import gffutils\n    >>> db = gffutils.create_db(\n    ...    gffutils.example_filename('FBgn0031208.gtf'),\n    ...    \":memory:\",\n    ...    keep_order=True,\n    ...    verbose=False)\n\n    Default settings -- no merging, and report a separate TSS on each line even\n    if they overlap (as in the first two):\n\n\n    >>> print(tsses(db))                        # doctest: +NORMALIZE_WHITESPACE\n    chr2L\tgffutils_derived\ttranscript_TSS\t7529\t7529\t.\t+\t.\tgene_id \"FBgn0031208\"; transcript_id \"FBtr0300689\";\n    chr2L\tgffutils_derived\ttranscript_TSS\t7529\t7529\t.\t+\t.\tgene_id \"FBgn0031208\"; transcript_id \"FBtr0300690\";\n    chr2L\tgffutils_derived\ttranscript_TSS\t11000\t11000\t.\t-\t.\tgene_id \"Fk_gene_1\"; transcript_id \"transcript_Fk_gene_1\";\n    chr2L\tgffutils_derived\ttranscript_TSS\t12500\t12500\t.\t-\t.\tgene_id \"Fk_gene_2\"; transcript_id \"transcript_Fk_gene_2\";\n    <BLANKLINE>\n\n\n    Default merging, showing the first two TSSes merged and reported as\n    a single unique TSS for the gene. Note the conversion to BED:\n\n    >>> x = tsses(db, merge_overlapping=True)\n    >>> print(x)  # doctest: +NORMALIZE_WHITESPACE\n    chr2L\t7528\t7529\tFBgn0031208\t.\t+\n    chr2L\t10999\t11000\tFk_gene_1\t.\t-\n    chr2L\t12499\t12500\tFk_gene_2\t.\t-\n    <BLANKLINE>\n\n    Report both gene ID and transcript ID in the name. In some cases this can\n    be easier to parse than the original GTF or GFF file. With no merging\n    specified, we must add `as_bed6=True` to see the names in BED format.\n\n    >>> x = tsses(db, attrs=['gene_id', 'transcript_id'], as_bed6=True)\n    >>> print(x)  # doctest: +NORMALIZE_WHITESPACE\n    chr2L\t7528\t7529\tFBgn0031208:FBtr0300689\t.\t+\n    chr2L\t7528\t7529\tFBgn0031208:FBtr0300690\t.\t+\n    chr2L\t10999\t11000\tFk_gene_1:transcript_Fk_gene_1\t.\t-\n    chr2L\t12499\t12500\tFk_gene_2:transcript_Fk_gene_2\t.\t-\n    <BLANKLINE>\n\n    Use a 3kb merge distance so the last 2 features are merged together:\n\n    >>> x = tsses(db, merge_overlapping=True, merge_kwargs=dict(d=3000))\n    >>> print(x)  # doctest: +NORMALIZE_WHITESPACE\n    chr2L\t7528\t7529\tFBgn0031208\t.\t+\n    chr2L\t10999\t12500\tFk_gene_1,Fk_gene_2\t.\t-\n    <BLANKLINE>\n\n\n    The set of unique TSSes for each gene, +1kb upstream and 500bp downstream:\n\n    >>> x = tsses(db, merge_overlapping=True)\n    >>> x = x.slop(l=1000, r=500, s=True, genome='dm3')\n    >>> print(x)  # doctest: +NORMALIZE_WHITESPACE\n    chr2L\t6528\t8029\tFBgn0031208\t.\t+\n    chr2L\t10499\t12000\tFk_gene_1\t.\t-\n    chr2L\t11999\t13500\tFk_gene_2\t.\t-\n    <BLANKLINE>\n\n\n    \"\"\"\n    _override = os.environ.get('GFFUTILS_USES_BEDTOOLS_227_OR_LATER', None)\n    if _override is not None:\n        if _override == 'true':\n            bedtools_227_or_later = True\n        elif _override == 'false':\n            bedtools_227_or_later = False\n        else:\n            raise ValueError(\n                \"Unknown value for GFFUTILS_USES_BEDTOOLS_227_OR_LATER \"\n                \"environment variable: {0}\".format(_override))\n\n    if bedtools_227_or_later:\n        _merge_kwargs = dict(o='distinct', s=True, c='4,5,6')\n    else:\n        _merge_kwargs = dict(o='distinct', s=True, c='4')\n\n    if merge_kwargs is not None:\n        _merge_kwargs.update(merge_kwargs)\n\n    def gen():\n        \"\"\"\n        Generator of pybedtools.Intervals representing TSSes.\n        \"\"\"\n        for gene in db.features_of_type('gene'):\n            for transcript in db.children(gene, level=1):\n                if transcript.strand == '-':\n                    transcript.start = transcript.stop\n                else:\n                    transcript.stop = transcript.start\n                transcript.featuretype = transcript.featuretype + '_TSS'\n                yield helpers.asinterval(transcript)\n\n    # GFF/GTF format\n    x = pybedtools.BedTool(gen()).sort()\n\n    # Figure out default attrs to use, depending on the original format.\n    if attrs is None:\n        if db.dialect['fmt'] == 'gtf':\n            attrs = 'gene_id'\n        else:\n            attrs = 'ID'\n\n    if merge_overlapping or as_bed6:\n\n        if isinstance(attrs, six.string_types):\n            attrs = [attrs]\n\n        def to_bed(f):\n            \"\"\"\n            Given a pybedtools.Interval, return a new Interval with the name\n            set according to the kwargs provided above.\n            \"\"\"\n            name = attrs_sep.join([f.attrs[i] for i in attrs])\n            return pybedtools.Interval(\n                f.chrom,\n                f.start,\n                f.stop,\n                name,\n                str(f.score),\n                f.strand)\n\n        x = x.each(to_bed).saveas()\n\n    if merge_overlapping:\n        if bedtools_227_or_later:\n            x = x.merge(**_merge_kwargs)\n        else:\n            def fix_merge(f):\n                f = featurefuncs.extend_fields(f, 6)\n                return pybedtools.Interval(\n                    f.chrom,\n                    f.start,\n                    f.stop,\n                    f[4],\n                    '.',\n                    f[3])\n            x = x.merge(**_merge_kwargs).saveas().each(fix_merge).saveas()\n\n    return x", "response": "Create a set of transcription start sites for all transcripts in a database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_gene_recs(self, db, gene_id):\n        gene_rec = db[gene_id]\n        # Output gene record\n        self.write_rec(gene_rec)\n        # Get each mRNA's lengths\n        mRNA_lens = {}\n        c = list(db.children(gene_id, featuretype=\"mRNA\"))\n        for mRNA in db.children(gene_id, featuretype=\"mRNA\"):\n            mRNA_lens[mRNA.id] = \\\n                sum(len(exon) for exon in db.children(mRNA,\n                                                      featuretype=\"exon\"))\n        # Sort mRNAs by length\n        sorted_mRNAs = \\\n            sorted(mRNA_lens.items(), key=lambda x: x[1], reverse=True)\n        for curr_mRNA in sorted_mRNAs:\n            mRNA_id = curr_mRNA[0]\n            mRNA_rec = db[mRNA_id]\n            # Write mRNA record to file\n            self.write_rec(mRNA_rec)\n            # Write mRNA's children records to file\n            self.write_mRNA_children(db, mRNA_id)\n        # Write non-mRNA children of gene (only level1)\n        for gene_child in db.children(gene_id, level=1):\n            if gene_child.featuretype != \"mRNA\":\n                self.write_rec(gene_child)", "response": "Write all records of a gene to a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting out the children records of the mRNA record given by the ID .", "response": "def write_mRNA_children(self, db, mRNA_id):\n        \"\"\"\n        Write out the children records of the mRNA given by the ID\n        (not including the mRNA record itself) in a canonical\n        order, where exons are sorted by start position and given\n        first.\n        \"\"\"\n        mRNA_children = db.children(mRNA_id, order_by='start')\n        nonexonic_children = []\n        for child_rec in mRNA_children:\n            if child_rec.featuretype == \"exon\":\n                self.write_rec(child_rec)\n                self.write_exon_children(db, child_rec)\n            else:\n                nonexonic_children.append(child_rec)\n        self.write_recs(nonexonic_children)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_exon_children(self, db, exon_id):\n        exon_children = db.children(exon_id, order_by='start')\n        for exon_child in exon_children:\n            self.write_rec(exon_child)", "response": "Write out the children records of the given exon."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the stream. Assumes stream has close method.", "response": "def close(self):\n        \"\"\"\n        Close the stream. Assumes stream has 'close' method.\n        \"\"\"\n        self.out_stream.close()\n        # If we're asked to write in place, substitute the named\n        # temporary file for the current file\n        if self.in_place:\n            shutil.move(self.temp_file.name, self.out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the variance of the regression error at x.", "response": "def var_regression_matrix(H, x, model, sigma=1):\r\n    \"\"\"\r\n    Compute the variance of the 'regression error'.\r\n    \r\n    Parameters\r\n    ----------\r\n    H : 2d-array\r\n        The regression matrix\r\n    x : 2d-array\r\n        The coordinates to calculate the regression error variance at.\r\n    model : str\r\n        A string of tokens that define the regression model (e.g. \r\n        '1 x1 x2 x1*x2')\r\n    sigma : scalar\r\n        An estimate of the variance (default: 1).\r\n    \r\n    Returns\r\n    -------\r\n    var : scalar\r\n        The variance of the regression error, evaluated at ``x``.\r\n        \r\n    \"\"\"\r\n    x = np.atleast_2d(x)\r\n    H = np.atleast_2d(H)\r\n    \r\n    if x.shape[0]==1:\r\n        x = x.T\r\n    \r\n    if np.rank(H)<(np.dot(H.T, H)).shape[0]:\r\n        raise ValueError(\"model and DOE don't suit together\")\r\n    \r\n    x_mod = build_regression_matrix(x, model)\r\n    var = sigma**2*np.dot(np.dot(x_mod.T, np.linalg.inv(np.dot(H.T, H))), x_mod)\r\n    return var"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_seqfeature(feature):\n    if isinstance(feature, six.string_types):\n        feature = feature_from_line(feature)\n\n    qualifiers = {\n        'source': [feature.source],\n        'score': [feature.score],\n        'seqid': [feature.seqid],\n        'frame': [feature.frame],\n    }\n    qualifiers.update(feature.attributes)\n    return SeqFeature(\n        # Convert from GFF 1-based to standard Python 0-based indexing used by\n        # BioPython\n        FeatureLocation(feature.start - 1, feature.stop),\n        id=feature.id,\n        type=feature.featuretype,\n        strand=_biopython_strand[feature.strand],\n        qualifiers=qualifiers\n    )", "response": "Converts a gffutils. Feature object to a Bio. SeqFeature object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_seqfeature(s, **kwargs):\n    source = s.qualifiers.get('source', '.')[0]\n    score = s.qualifiers.get('score', '.')[0]\n    seqid = s.qualifiers.get('seqid', '.')[0]\n    frame = s.qualifiers.get('frame', '.')[0]\n    strand = _feature_strand[s.strand]\n\n    # BioPython parses 1-based GenBank positions into 0-based for use within\n    # Python.  We need to convert back to 1-based GFF format here.\n    start = s.location.start.position + 1\n    stop = s.location.end.position\n    featuretype = s.type\n    id = s.id\n    attributes = dict(s.qualifiers)\n    attributes.pop('source', '.')\n    attributes.pop('score', '.')\n    attributes.pop('seqid', '.')\n    attributes.pop('frame', '.')\n    return Feature(seqid, source, featuretype, start, stop, score, strand,\n                   frame, attributes, id=id, **kwargs)", "response": "Converts a Bio. SeqFeature object to a gffutils. Feature object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_pragmas(self, pragmas):\n        self.pragmas = pragmas\n        c = self.conn.cursor()\n        c.executescript(\n            ';\\n'.join(\n                ['PRAGMA %s=%s' % i for i in self.pragmas.items()]\n            )\n        )\n        self.conn.commit()", "response": "Set the pragmas for the current database connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a feature object with additional database - specific defaults", "response": "def _feature_returner(self, **kwargs):\n        \"\"\"\n        Returns a feature, adding additional database-specific defaults\n        \"\"\"\n        kwargs.setdefault('dialect', self.dialect)\n        kwargs.setdefault('keep_order', self.keep_order)\n        kwargs.setdefault('sort_attribute_values', self.sort_attribute_values)\n        return Feature(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef schema(self):\n        c = self.conn.cursor()\n        c.execute(\n            '''\n            SELECT sql FROM sqlite_master\n            ''')\n        results = []\n        for i, in c:\n            if i is not None:\n                results.append(i)\n        return '\\n'.join(results)", "response": "Returns the database schema as a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef features_of_type(self, featuretype, limit=None, strand=None,\n                         order_by=None, reverse=False,\n                         completely_within=False):\n        \"\"\"\n        Returns an iterator of :class:`gffutils.Feature` objects.\n\n        Parameters\n        ----------\n        {_method_doc}\n        \"\"\"\n        query, args = helpers.make_query(\n            args=[],\n            limit=limit,\n            featuretype=featuretype,\n            order_by=order_by,\n            reverse=reverse,\n            strand=strand,\n            completely_within=completely_within,\n        )\n\n        for i in self._execute(query, args):\n            yield self._feature_returner(**i)", "response": "Returns an iterator of features of a certain type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate over all the parent records of a given featuretype and all of its children.", "response": "def iter_by_parent_childs(self, featuretype=\"gene\", level=None,\n                              order_by=None, reverse=False,\n                              completely_within=False):\n        \"\"\"\n        For each parent of type `featuretype`, yield a list L of that parent\n        and all of its children (`[parent] + list(children)`). The parent will\n        always be L[0].\n\n        This is useful for \"sanitizing\" a GFF file for downstream tools.\n\n        Additional kwargs are passed to :meth:`FeatureDB.children`, and will\n        therefore only affect items L[1:] in each yielded list.\n        \"\"\"\n        # Get all the parent records of the requested feature type\n        parent_recs = self.all_features(featuretype=featuretype)\n        # Return a generator of these parent records and their\n        # children\n        for parent_rec in parent_recs:\n            unit_records = \\\n                [parent_rec] + list(self.children(parent_rec.id))\n            yield unit_records"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over featuretypes found in the database.", "response": "def featuretypes(self):\n        \"\"\"\n        Iterate over feature types found in the database.\n\n        Returns\n        -------\n        A generator object that yields featuretypes (as strings)\n        \"\"\"\n        c = self.conn.cursor()\n        c.execute(\n            '''\n            SELECT DISTINCT featuretype from features\n            ''')\n        for i, in c:\n            yield i"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns parents of a feature.", "response": "def parents(self, id, level=None, featuretype=None, order_by=None,\n                reverse=False, completely_within=False, limit=None):\n        \"\"\"\n        Return parents of feature `id`.\n        {_relation_docstring}\n        \"\"\"\n        return self._relation(\n            id, join_on='parent', join_to='child', level=level,\n            featuretype=featuretype, order_by=order_by, reverse=reverse,\n            limit=limit, completely_within=completely_within)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self, query):\n        c = self.conn.cursor()\n        return c.execute(query)", "response": "Execute arbitrary queries on the db."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns features within specified genomic coordinates.", "response": "def region(self, region=None, seqid=None, start=None, end=None,\n               strand=None, featuretype=None, completely_within=False):\n        \"\"\"\n        Return features within specified genomic coordinates.\n\n        Specifying genomic coordinates can be done in a flexible manner\n\n        Parameters\n        ----------\n        region : string, tuple, or Feature instance\n            If string, then of the form \"seqid:start-end\".  If tuple, then\n            (seqid, start, end).  If :class:`Feature`, then use the features\n            seqid, start, and end values.\n\n            This argument is mutually exclusive with start/end/seqid.\n\n            *Note*: By design, even if a feature is provided, its strand will\n            be ignored.  If you want to restrict the output by strand, use the\n            separate `strand` kwarg.\n\n        strand : + | - | . | None\n            If `strand` is provided, then only those features exactly matching\n            `strand` will be returned. So `strand='.'` will only return\n            unstranded features. Default is `strand=None` which does not\n            restrict by strand.\n\n        seqid, start, end, strand\n            Mutually exclusive with `region`.  These kwargs can be used to\n            approximate slice notation; see \"Details\" section below.\n\n        featuretype : None, string, or iterable\n            If not None, then restrict output.  If string, then only report\n            that feature type.  If iterable, then report all featuretypes in\n            the iterable.\n\n        completely_within : bool\n            By default (`completely_within=False`), returns features that\n            partially or completely overlap `region`.  If\n            `completely_within=True`, features that are completely within\n            `region` will be returned.\n\n        Notes\n        -------\n\n        The meaning of `seqid`, `start`, and `end` is interpreted as follows:\n\n        ====== ====== ===== ======================================\n        seqid  start  end   meaning\n        ====== ====== ===== ======================================\n        str    int    int   equivalent to `region` kwarg\n        None   int    int   features from all chroms within coords\n        str    None   int   equivalent to [:end] slice notation\n        str    int    None  equivalent to [start:] slice notation\n        None   None   None  equivalent to FeatureDB.all_features()\n        ====== ====== ===== ======================================\n\n        If performance is a concern, use `completely_within=True`. This allows\n        the query to be optimized by only looking for features that fall in the\n        precise genomic bin (same strategy as UCSC Genome Browser and\n        BEDTools). Otherwise all features' start/stop coords need to be\n        searched to see if they partially overlap the region of interest.\n\n        Examples\n        --------\n\n        - `region(seqid=\"chr1\", start=1000)` returns all features on chr1 that\n          start or extend past position 1000\n\n        - `region(seqid=\"chr1\", start=1000, completely_within=True)` returns\n          all features on chr1 that start past position 1000.\n\n        - `region(\"chr1:1-100\", strand=\"+\", completely_within=True)` returns\n          only plus-strand features that completely fall within positions 1 to\n          100 on chr1.\n\n        Returns\n        -------\n        A generator object that yields :class:`Feature` objects.\n        \"\"\"\n        # Argument handling.\n        if region is not None:\n            if (seqid is not None) or (start is not None) or (end is not None):\n                raise ValueError(\n                    \"If region is supplied, do not supply seqid, \"\n                    \"start, or end as separate kwargs\")\n            if isinstance(region, six.string_types):\n                toks = region.split(':')\n                if len(toks) == 1:\n                    seqid = toks[0]\n                    start, end = None, None\n                else:\n                    seqid, coords = toks[:2]\n                    if len(toks) == 3:\n                        strand = toks[2]\n                    start, end = coords.split('-')\n\n            elif isinstance(region, Feature):\n                seqid = region.seqid\n                start = region.start\n                end = region.end\n                strand = region.strand\n\n            # otherwise assume it's a tuple\n            else:\n                seqid, start, end = region[:3]\n\n        # e.g.,\n        #   completely_within=True..... start >= {start} AND end <= {end}\n        #   completely_within=False.... start <  {end}   AND end >  {start}\n        if completely_within:\n            start_op = '>='\n            end_op = '<='\n        else:\n            start_op = '<'\n            end_op = '>'\n            end, start = start, end\n\n        args = []\n        position_clause = []\n        if seqid is not None:\n            position_clause.append('seqid = ?')\n            args.append(seqid)\n        if start is not None:\n            start = int(start)\n            position_clause.append('start %s ?' % start_op)\n            args.append(start)\n        if end is not None:\n            end = int(end)\n            position_clause.append('end %s ?' % end_op)\n            args.append(end)\n\n        position_clause = ' AND '.join(position_clause)\n\n        # Only use bins if we have defined boundaries and completely_within is\n        # True. Otherwise you can't know how far away a feature stretches\n        # (which means bins are not computable ahead of time)\n        _bin_clause = ''\n        if (start is not None) and (end is not None) and completely_within:\n            if start <= bins.MAX_CHROM_SIZE and end <= bins.MAX_CHROM_SIZE:\n                _bins = list(bins.bins(start, end, one=False))\n                # See issue #45\n                if len(_bins) < 900:\n                    _bin_clause = ' or ' .join(['bin = ?' for _ in _bins])\n                    _bin_clause = 'AND ( %s )' % _bin_clause\n                    args += _bins\n\n        query = ' '.join([\n            constants._SELECT,\n            'WHERE ',\n            position_clause,\n            _bin_clause])\n\n        # Add the featuretype clause\n        if featuretype is not None:\n            if isinstance(featuretype, six.string_types):\n                featuretype = [featuretype]\n            feature_clause = ' or '.join(\n                ['featuretype = ?' for _ in featuretype])\n            query += ' AND (%s) ' % feature_clause\n            args.extend(featuretype)\n\n        if strand is not None:\n            strand_clause = ' and strand = ? '\n            query += strand_clause\n            args.append(strand)\n\n        c = self.conn.cursor()\n        self._last_query = query\n        self._last_args = args\n        self._context = {\n            'start': start,\n            'end': end,\n            'seqid': seqid,\n            'region': region,\n        }\n        c.execute(query, tuple(args))\n        for i in c:\n            yield self._feature_returner(**i)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct new features representing the space between features. For example, if `features` is a list of exons, then this method will return the introns. If `features` is a list of genes, then this method will return the intergenic regions. Providing N features will return N - 1 new features. This method purposefully does *not* do any merging or sorting of coordinates, so you may want to use :meth:`FeatureDB.merge` first, or when selecting features use the `order_by` kwarg, e.g., `db.features_of_type('gene', order_by=('seqid', 'start'))`. Parameters ---------- features : iterable of :class:`feature.Feature` instances Sorted, merged iterable new_featuretype : string or None The new features will all be of this type, or, if None (default) then the featuretypes will be constructed from the neighboring features, e.g., `inter_exon_exon`. merge_attributes : bool If True, new features' attributes will be a merge of the neighboring features' attributes. This is useful if you have provided a list of exons; the introns will then retain the transcript and/or gene parents as a single item. Otherwise, if False, the attribute will be a comma-separated list of values, potentially listing the same gene ID twice. attribute_func : callable or None If None, then nothing special is done to the attributes. If callable, then the callable accepts two attribute dictionaries and returns a single attribute dictionary. If `merge_attributes` is True, then `attribute_func` is called before `merge_attributes`. This could be useful for manually managing IDs for the new features. update_attributes : dict After attributes have been modified and merged, this dictionary can be used to replace parts of the attributes dictionary. Returns ------- A generator that yields :class:`Feature` objects", "response": "def interfeatures(self, features, new_featuretype=None,\n                      merge_attributes=True, dialect=None,\n                      attribute_func=None, update_attributes=None):\n        \"\"\"\n        Construct new features representing the space between features.\n\n        For example, if `features` is a list of exons, then this method will\n        return the introns.  If `features` is a list of genes, then this method\n        will return the intergenic regions.\n\n        Providing N features will return N - 1 new features.\n\n        This method purposefully does *not* do any merging or sorting of\n        coordinates, so you may want to use :meth:`FeatureDB.merge` first, or\n        when selecting features use the `order_by` kwarg, e.g.,\n        `db.features_of_type('gene', order_by=('seqid', 'start'))`.\n\n        Parameters\n        ----------\n        features : iterable of :class:`feature.Feature` instances\n            Sorted, merged iterable\n\n        new_featuretype : string or None\n            The new features will all be of this type, or, if None (default)\n            then the featuretypes will be constructed from the neighboring\n            features, e.g., `inter_exon_exon`.\n\n        merge_attributes : bool\n            If True, new features' attributes will be a merge of the neighboring\n            features' attributes.  This is useful if you have provided a list of\n            exons; the introns will then retain the transcript and/or gene\n            parents as a single item. Otherwise, if False, the attribute will\n            be a comma-separated list of values, potentially listing the same\n            gene ID twice.\n\n        attribute_func : callable or None\n            If None, then nothing special is done to the attributes.  If\n            callable, then the callable accepts two attribute dictionaries and\n            returns a single attribute dictionary.  If `merge_attributes` is\n            True, then `attribute_func` is called before `merge_attributes`.\n            This could be useful for manually managing IDs for the new\n            features.\n\n        update_attributes : dict\n            After attributes have been modified and merged, this dictionary can\n            be used to replace parts of the attributes dictionary.\n\n        Returns\n        -------\n        A generator that yields :class:`Feature` objects\n        \"\"\"\n        for i, f in enumerate(features):\n            # no inter-feature for the first one\n            if i == 0:\n                interfeature_start = f.stop\n                last_feature = f\n                continue\n\n            interfeature_stop = f.start\n            if new_featuretype is None:\n                new_featuretype = 'inter_%s_%s' % (\n                    last_feature.featuretype, f.featuretype)\n            if last_feature.strand != f.strand:\n                new_strand = '.'\n            else:\n                new_strand = f.strand\n\n            if last_feature.chrom != f.chrom:\n                # We've moved to a new chromosome.  For example, if we're\n                # getting intergenic regions from all genes, they will be on\n                # different chromosomes. We still assume sorted features, but\n                # don't complain if they're on different chromosomes -- just\n                # move on.\n                last_feature = f\n                continue\n\n            strand = new_strand\n            chrom = last_feature.chrom\n\n            # Shrink\n            interfeature_start += 1\n            interfeature_stop -= 1\n\n            if merge_attributes:\n                new_attributes = helpers.merge_attributes(\n                    last_feature.attributes, f.attributes)\n            else:\n                new_attributes = {}\n\n            if update_attributes:\n                new_attributes.update(update_attributes)\n\n            new_bin = bins.bins(\n                interfeature_start, interfeature_stop, one=True)\n            _id = None\n            fields = dict(\n                seqid=chrom,\n                source='gffutils_derived',\n                featuretype=new_featuretype,\n                start=interfeature_start,\n                end=interfeature_stop,\n                score='.',\n                strand=strand,\n                frame='.',\n                attributes=new_attributes,\n                bin=new_bin)\n\n            if dialect is None:\n                # Support for @classmethod -- if calling from the class, then\n                # self.dialect is not defined, so defer to Feature's default\n                # (which will be constants.dialect, or GFF3).\n                try:\n                    dialect = self.dialect\n                except AttributeError:\n                    dialect = None\n            yield self._feature_returner(**fields)\n            interfeature_start = f.stop"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes features from database.", "response": "def delete(self, features, make_backup=True, **kwargs):\n        \"\"\"\n        Delete features from database.\n\n        features : str, iterable, FeatureDB instance\n            If FeatureDB, all features will be used. If string, assume it's the\n            ID of the feature to remove. Otherwise, assume it's an iterable of\n            Feature objects. The classes in gffutils.iterators may be helpful\n            in this case.\n\n        make_backup : bool\n            If True, and the database you're about to update is a file on disk,\n            makes a copy of the existing database and saves it with a .bak\n            extension.\n\n        Returns\n        -------\n        FeatureDB object, with features deleted.\n        \"\"\"\n        if make_backup:\n            if isinstance(self.dbfn, six.string_types):\n                shutil.copy2(self.dbfn, self.dbfn + '.bak')\n\n        c = self.conn.cursor()\n        query1 = \"\"\"\n        DELETE FROM features WHERE id = ?\n        \"\"\"\n        query2 = \"\"\"\n        DELETE FROM relations WHERE parent = ? OR child = ?\n        \"\"\"\n        if isinstance(features, FeatureDB):\n            features = features.all_features()\n        if isinstance(features, six.string_types):\n            features = [features]\n        if isinstance(features, Feature):\n            features = [features]\n        for feature in features:\n            if isinstance(feature, six.string_types):\n                _id = feature\n            else:\n                _id = feature.id\n            c.execute(query1, (_id,))\n            c.execute(query2, (_id, _id))\n        self.conn.commit()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, data, make_backup=True, **kwargs):\n        from gffutils import create\n        from gffutils import iterators\n        if make_backup:\n            if isinstance(self.dbfn, six.string_types):\n                shutil.copy2(self.dbfn, self.dbfn + '.bak')\n\n        # get iterator-specific kwargs\n        _iterator_kwargs = {}\n        for k, v in kwargs.items():\n            if k in constants._iterator_kwargs:\n                _iterator_kwargs[k] = v\n\n        # Handle all sorts of input\n        data = iterators.DataIterator(data, **_iterator_kwargs)\n\n        if self.dialect['fmt'] == 'gtf':\n            if 'id_spec' not in kwargs:\n                kwargs['id_spec'] = {\n                    'gene': 'gene_id', 'transcript': 'transcript_id'}\n            db = create._GTFDBCreator(\n                data=data, dbfn=self.dbfn, dialect=self.dialect, **kwargs)\n        elif self.dialect['fmt'] == 'gff3':\n            if 'id_spec' not in kwargs:\n                kwargs['id_spec'] = 'ID'\n            db = create._GFFDBCreator(\n                data=data, dbfn=self.dbfn, dialect=self.dialect, **kwargs)\n\n        else:\n            raise ValueError\n\n        db._populate_from_lines(data)\n        db._update_relations()\n        db._finalize()\n        return db", "response": "Update database with features in data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a relation between two features.", "response": "def add_relation(self, parent, child, level, parent_func=None,\n                     child_func=None):\n        \"\"\"\n        Manually add relations to the database.\n\n        Parameters\n        ----------\n        parent : str or Feature instance\n             Parent feature to add.\n\n        child : str or Feature instance\n            Child feature to add\n\n        level : int\n            Level of the relation.  For example, if parent is a gene and child\n            is an mRNA, then you might want level to be 1.  But if child is an\n            exon, then level would be 2.\n\n        parent_func, child_func : callable\n            These optional functions control how attributes are updated in the\n            database.  They both have the signature `func(parent, child)` and\n            must return a [possibly modified] Feature instance.  For example,\n            we could add the child's database id as the \"child\" attribute in\n            the parent::\n\n                def parent_func(parent, child):\n                    parent.attributes['child'] = child.id\n\n            and add the parent's \"gene_id\" as the child's \"Parent\" attribute::\n\n                def child_func(parent, child):\n                    child.attributes['Parent'] = parent['gene_id']\n\n        Returns\n        -------\n        FeatureDB object with new relations added.\n        \"\"\"\n        if isinstance(parent, six.string_types):\n            parent = self[parent]\n        if isinstance(child, six.string_types):\n            child = self[child]\n\n        c = self.conn.cursor()\n        c.execute('''\n                  INSERT INTO relations (parent, child, level)\n                  VALUES (?, ?, ?)''', (parent.id, child.id, level))\n\n        if parent_func is not None:\n            parent = parent_func(parent, child)\n            self._update(parent, c)\n        if child_func is not None:\n            child = child_func(parent, child)\n            self._update(child, c)\n\n        self.conn.commit()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts a new entry into the database.", "response": "def _insert(self, feature, cursor):\n        \"\"\"\n        Insert a feature into the database.\n        \"\"\"\n        try:\n            cursor.execute(constants._INSERT, feature.astuple())\n        except sqlite3.ProgrammingError:\n            cursor.execute(\n                constants._INSERT, feature.astuple(self.default_encoding))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates introns from existing annotations.", "response": "def create_introns(self, exon_featuretype='exon',\n                       grandparent_featuretype='gene', parent_featuretype=None,\n                       new_featuretype='intron', merge_attributes=True):\n        \"\"\"\n        Create introns from existing annotations.\n\n\n        Parameters\n        ----------\n        exon_featuretype : string\n            Feature type to use in order to infer introns.  Typically `\"exon\"`.\n\n        grandparent_featuretype : string\n            If `grandparent_featuretype` is not None, then group exons by\n            children of this featuretype.  If `granparent_featuretype` is\n            \"gene\" (default), then introns will be created for all first-level\n            children of genes.  This may include mRNA, rRNA, ncRNA, etc.  If\n            you only want to infer introns from one of these featuretypes\n            (e.g., mRNA), then use the `parent_featuretype` kwarg which is\n            mutually exclusive with `grandparent_featuretype`.\n\n        parent_featuretype : string\n            If `parent_featuretype` is not None, then only use this featuretype\n            to infer introns.  Use this if you only want a subset of\n            featuretypes to have introns (e.g., \"mRNA\" only, and not ncRNA or\n            rRNA). Mutually exclusive with `grandparent_featuretype`.\n\n        new_featuretype : string\n            Feature type to use for the inferred introns; default is\n            `\"intron\"`.\n\n        merge_attributes : bool\n            Whether or not to merge attributes from all exons. If False then no\n            attributes will be created for the introns.\n\n        Returns\n        -------\n        A generator object that yields :class:`Feature` objects representing\n        new introns\n\n        Notes\n        -----\n        The returned generator can be passed directly to the\n        :meth:`FeatureDB.update` method to permanently add them to the\n        database, e.g., ::\n\n            db.update(db.create_introns())\n\n        \"\"\"\n        if (grandparent_featuretype and parent_featuretype) or (\n            grandparent_featuretype is None and parent_featuretype is None\n        ):\n            raise ValueError(\"exactly one of `grandparent_featuretype` or \"\n                             \"`parent_featuretype` should be provided\")\n\n        if grandparent_featuretype:\n            def child_gen():\n                for gene in self.features_of_type(grandparent_featuretype):\n                    for child in self.children(gene, level=1):\n                        yield child\n        elif parent_featuretype:\n            def child_gen():\n                for child in self.features_of_type(parent_featuretype):\n                    yield child\n\n        for child in child_gen():\n            exons = self.children(child, level=1, featuretype=exon_featuretype,\n                                  order_by='start')\n            for intron in self.interfeatures(\n                exons, new_featuretype=new_featuretype,\n                merge_attributes=merge_attributes, dialect=self.dialect\n            ):\n                yield intron"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmerging overlapping features together.", "response": "def merge(self, features, ignore_strand=False):\n        \"\"\"\n        Merge overlapping features together.\n\n        Parameters\n        ----------\n\n        features : iterator of Feature instances\n\n        ignore_strand : bool\n            If True, features on multiple strands will be merged, and the final\n            strand will be set to '.'.  Otherwise, ValueError will be raised if\n            trying to merge features on differnt strands.\n\n        Returns\n        -------\n        A generator object that yields :class:`Feature` objects representing\n        the newly merged features.\n        \"\"\"\n\n        # Consume iterator up front...\n        features = list(features)\n\n        if len(features) == 0:\n            raise StopIteration\n\n        # Either set all strands to '+' or check for strand-consistency.\n        if ignore_strand:\n            strand = '.'\n        else:\n            strands = [i.strand for i in features]\n            if len(set(strands)) > 1:\n                raise ValueError('Specify ignore_strand=True to force merging '\n                                 'of multiple strands')\n            strand = strands[0]\n\n        # Sanity check to make sure all features are from the same chromosome.\n        chroms = [i.chrom for i in features]\n        if len(set(chroms)) > 1:\n            raise NotImplementedError('Merging multiple chromosomes not '\n                                      'implemented')\n        chrom = chroms[0]\n\n        # To start, we create a merged feature of just the first feature.\n        current_merged_start = features[0].start\n        current_merged_stop = features[0].stop\n\n        # We don't need to check the first one, so start at feature #2.\n        for feature in features[1:]:\n            # Does this feature start within the currently merged feature?...\n            if feature.start <= current_merged_stop + 1:\n                # ...It starts within, so leave current_merged_start where it\n                # is.  Does it extend any farther?\n                if feature.stop >= current_merged_stop:\n                    # Extends further, so set a new stop position\n                    current_merged_stop = feature.stop\n                else:\n                    # If feature.stop < current_merged_stop, it's completely\n                    # within the previous feature.  Nothing more to do.\n                    continue\n            else:\n                # The start position is outside the merged feature, so we're\n                # done with the current merged feature.  Prepare for output...\n                merged_feature = dict(\n                    seqid=feature.chrom,\n                    source='.',\n                    featuretype=feature.featuretype,\n                    start=current_merged_start,\n                    end=current_merged_stop,\n                    score='.',\n                    strand=strand,\n                    frame='.',\n                    attributes='')\n                yield self._feature_returner(**merged_feature)\n\n                # and we start a new one, initializing with this feature's\n                # start and stop.\n                current_merged_start = feature.start\n                current_merged_stop = feature.stop\n\n        # need to yield the last one.\n        if len(features) == 1:\n            feature = features[0]\n        merged_feature = dict(\n            seqid=feature.chrom,\n            source='.',\n            featuretype=feature.featuretype,\n            start=current_merged_start,\n            end=current_merged_stop,\n            score='.',\n            strand=strand,\n            frame='.',\n            attributes='')\n        yield self._feature_returner(**merged_feature)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the total bp of all children of a featuretype.", "response": "def children_bp(self, feature, child_featuretype='exon', merge=False,\n                    ignore_strand=False):\n        \"\"\"\n        Total bp of all children of a featuretype.\n\n        Useful for getting the exonic bp of an mRNA.\n\n        Parameters\n        ----------\n\n        feature : str or Feature instance\n\n        child_featuretype : str\n            Which featuretype to consider.  For example, to get exonic bp of an\n            mRNA, use `child_featuretype='exon'`.\n\n        merge : bool\n            Whether or not to merge child features together before summing\n            them.\n\n        ignore_strand : bool\n            If True, then overlapping features on different strands will be\n            merged together; otherwise, merging features with different strands\n            will result in a ValueError.\n\n        Returns\n        -------\n        Integer representing the total number of bp.\n        \"\"\"\n\n        children = self.children(feature, featuretype=child_featuretype,\n                                 order_by='start')\n        if merge:\n            children = self.merge(children, ignore_strand=ignore_strand)\n\n        total = 0\n        for child in children:\n            total += len(child)\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bed12(self, feature, block_featuretype=['exon'],\n              thick_featuretype=['CDS'], thin_featuretype=None,\n              name_field='ID', color=None):\n        \"\"\"\n        Converts `feature` into a BED12 format.\n\n        GFF and GTF files do not necessarily define genes consistently, so this\n        method provides flexiblity in specifying what to call a \"transcript\".\n\n        Parameters\n        ----------\n        feature : str or Feature instance\n            In most cases, this feature should be a transcript rather than\n            a gene.\n\n        block_featuretype : str or list\n            Which featuretype to use as the exons. These are represented as\n            blocks in the BED12 format.  Typically 'exon'.\n\n            Use the `thick_featuretype` and `thin_featuretype` arguments to\n            control the display of CDS as thicker blocks and UTRs as thinner\n            blocks.\n\n            Note that the features for `thick` or `thin` are *not*\n            automatically included in the blocks; if you do want them included,\n            then those featuretypes should be added to this `block_features`\n            list.\n\n            If no child features of type `block_featuretype` are found, then\n            the full `feature` is returned in BED12 format as if it had\n            a single exon.\n\n        thick_featuretype : str or list\n            Child featuretype(s) to use in order to determine the boundaries of\n            the \"thick\" blocks. In BED12 format, these represent coding\n            sequences; typically this would be set to \"CDS\".  This argument is\n            mutually exclusive with `thin_featuretype`.\n\n            Specifically, the BED12 thickStart will be the start coord of the\n            first `thick` item and the thickEnd will be the stop coord of the\n            last `thick` item.\n\n        thin_featuretype : str or list\n            Child featuretype(s) to use in order to determine the boundaries of\n            the \"thin\" blocks.  In BED12 format, these represent untranslated\n            regions.  Typically \"utr\" or ['three_prime_UTR', 'five_prime_UTR'].\n            Mutually exclusive with `thick_featuretype`.\n\n            Specifically, the BED12 thickStart field will be the stop coord of\n            the first `thin` item and the thickEnd field will be the start\n            coord of the last `thin` item.\n\n        name_field : str\n            Which attribute of `feature` to use as the feature's name.  If this\n            field is not present, a \".\" placeholder will be used instead.\n\n        color : None or str\n            If None, then use black (0,0,0) as the RGB color; otherwise this\n            should be a comma-separated string of R,G,B values each of which\n            are integers in the range 0-255.\n        \"\"\"\n        if thick_featuretype and thin_featuretype:\n            raise ValueError(\"Can only specify one of `thick_featuertype` or \"\n                             \"`thin_featuretype`\")\n\n        exons = list(self.children(feature, featuretype=block_featuretype,\n                                   order_by='start'))\n        if len(exons) == 0:\n            exons = [feature]\n        feature = self[feature]\n        first = exons[0].start\n        last = exons[-1].stop\n\n        if first != feature.start:\n            raise ValueError(\n                \"Start of first exon (%s) does not match start of feature (%s)\"\n                % (first, feature.start))\n        if last != feature.stop:\n            raise ValueError(\n                \"End of last exon (%s) does not match end of feature (%s)\"\n                % (last, feature.stop))\n\n        if color is None:\n            color = '0,0,0'\n        color = color.replace(' ', '').strip()\n\n        # Use field names as defined at\n        # http://genome.ucsc.edu/FAQ/FAQformat.html#format1\n        chrom = feature.chrom\n        chromStart = feature.start - 1\n        chromEnd = feature.stop\n        orig = constants.always_return_list\n        constants.always_return_list = True\n        try:\n            name = feature[name_field][0]\n        except KeyError:\n            name = \".\"\n\n        constants.always_return_list = orig\n\n        score = feature.score\n        if score == '.':\n            score = '0'\n        strand = feature.strand\n        itemRgb = color\n        blockCount = len(exons)\n        blockSizes = [len(i) for i in exons]\n        blockStarts = [i.start - 1 - chromStart for i in exons]\n\n        if thick_featuretype:\n            thick = list(self.children(feature, featuretype=thick_featuretype,\n                                       order_by='start'))\n            if len(thick) == 0:\n                thickStart = feature.start\n                thickEnd = feature.stop\n            else:\n                thickStart = thick[0].start - 1  # BED 0-based coords\n                thickEnd = thick[-1].stop\n\n        if thin_featuretype:\n            thin = list(self.children(feature, featuretype=thin_featuretype,\n                                      order_by='start'))\n            if len(thin) == 0:\n                thickStart = feature.start\n                thickEnd = feature.stop\n            else:\n                thickStart = thin[0].stop\n                thickEnd = thin[-1].start - 1  # BED 0-based coords\n\n        tst = chromStart + blockStarts[-1] + blockSizes[-1]\n        assert tst == chromEnd, \"tst=%s; chromEnd=%s\" % (tst, chromEnd)\n\n        fields = [\n            chrom,\n            chromStart,\n            chromEnd,\n            name,\n            score,\n            strand,\n            thickStart,\n            thickEnd,\n            itemRgb,\n            blockCount,\n            ','.join(map(str, blockSizes)),\n            ','.join(map(str, blockStarts))]\n        return '\\t'.join(map(str, fields))", "response": "This method converts a feature into a BED12 format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an iterator over the data in a tree tree.", "response": "def DataIterator(data, checklines=10, transform=None,\n                 force_dialect_check=False, from_string=False, **kwargs):\n    \"\"\"\n    Iterate over features, no matter how they are provided.\n\n    Parameters\n    ----------\n    data : str, iterable of Feature objs, FeatureDB\n        `data` can be a string (filename, URL, or contents of a file, if\n        from_string=True), any arbitrary iterable of features, or a FeatureDB\n        (in which case its all_features() method will be called).\n\n    checklines : int\n        Number of lines to check in order to infer a dialect.\n\n    transform : None or callable\n        If not None, `transform` should accept a Feature object as its only\n        argument and return either a (possibly modified) Feature object or\n        a value that evaluates to False.  If the return value is False, the\n        feature will be skipped.\n\n    force_dialect_check : bool\n        If True, check the dialect of every feature.  Thorough, but can be\n        slow.\n\n    from_string : bool\n        If True, `data` should be interpreted as the contents of a file rather\n        than the filename itself.\n\n    dialect : None or dict\n        Provide the dialect, which will override auto-detected dialects.  If\n        provided, you should probably also use `force_dialect_check=False` and\n        `checklines=0` but this is not enforced.\n    \"\"\"\n\n    _kwargs = dict(data=data, checklines=checklines, transform=transform,\n                   force_dialect_check=force_dialect_check, **kwargs)\n    if isinstance(data, six.string_types):\n        if from_string:\n            return _StringIterator(**_kwargs)\n        else:\n            if os.path.exists(data):\n                return _FileIterator(**_kwargs)\n            elif is_url(data):\n                return _UrlIterator(**_kwargs)\n    elif isinstance(data, FeatureDB):\n        _kwargs['data'] = data.all_features()\n        return _FeatureIterator(**_kwargs)\n\n    else:\n        return _FeatureIterator(**_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new Box - Behnken design with n factors and center points.", "response": "def bbdesign(n, center=None):\r\n    \"\"\"\r\n    Create a Box-Behnken design\r\n    \r\n    Parameters\r\n    ----------\r\n    n : int\r\n        The number of factors in the design\r\n    \r\n    Optional\r\n    --------\r\n    center : int\r\n        The number of center points to include (default = 1).\r\n    \r\n    Returns\r\n    -------\r\n    mat : 2d-array\r\n        The design matrix\r\n    \r\n    Example\r\n    -------\r\n    ::\r\n    \r\n        >>> bbdesign(3)\r\n        array([[-1., -1.,  0.],\r\n               [ 1., -1.,  0.],\r\n               [-1.,  1.,  0.],\r\n               [ 1.,  1.,  0.],\r\n               [-1.,  0., -1.],\r\n               [ 1.,  0., -1.],\r\n               [-1.,  0.,  1.],\r\n               [ 1.,  0.,  1.],\r\n               [ 0., -1., -1.],\r\n               [ 0.,  1., -1.],\r\n               [ 0., -1.,  1.],\r\n               [ 0.,  1.,  1.],\r\n               [ 0.,  0.,  0.],\r\n               [ 0.,  0.,  0.],\r\n               [ 0.,  0.,  0.]])\r\n        \r\n    \"\"\"\r\n    assert n>=3, 'Number of variables must be at least 3'\r\n    \r\n    # First, compute a factorial DOE with 2 parameters\r\n    H_fact = ff2n(2)\r\n    # Now we populate the real DOE with this DOE\r\n    \r\n    # We made a factorial design on each pair of dimensions\r\n    # - So, we created a factorial design with two factors\r\n    # - Make two loops\r\n    Index = 0\r\n    nb_lines = (n*(n-1)/2)*H_fact.shape[0]\r\n    H = repeat_center(n, nb_lines)\r\n    \r\n    for i in range(n - 1):\r\n        for j in range(i + 1, n):\r\n            Index = Index + 1\r\n            H[max([0, (Index - 1)*H_fact.shape[0]]):Index*H_fact.shape[0], i] = H_fact[:, 0]\r\n            H[max([0, (Index - 1)*H_fact.shape[0]]):Index*H_fact.shape[0], j] = H_fact[:, 1]\r\n\r\n    if center is None:\r\n        if n<=16:\r\n            points= [0, 0, 0, 3, 3, 6, 6, 6, 8, 9, 10, 12, 12, 13, 14, 15, 16]\r\n            center = points[n]\r\n        else:\r\n            center = n\r\n        \r\n    H = np.c_[H.T, repeat_center(n, center).T].T\r\n    \r\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inspect(data, look_for=['featuretype', 'chrom', 'attribute_keys',\n                            'feature_count'], limit=None, verbose=True):\n    \"\"\"\n    Inspect a GFF or GTF data source.\n\n    This function is useful for figuring out the different featuretypes found\n    in a file (for potential removal before creating a FeatureDB).\n\n    Returns a dictionary with a key for each item in `look_for` and\n    a corresponding value that is a dictionary of how many of each unique item\n    were found.\n\n    There will always be a `feature_count` key, indicating how many features\n    were looked at (if `limit` is provided, then `feature_count` will be the\n    same as `limit`).\n\n    For example, if `look_for` is ['chrom', 'featuretype'], then the result\n    will be a dictionary like::\n\n        {\n            'chrom': {\n                'chr1': 500,\n                'chr2': 435,\n                'chr3': 200,\n                ...\n                ...\n            }.\n\n            'featuretype': {\n                'gene': 150,\n                'exon': 324,\n                ...\n            },\n\n            'feature_count': 5000\n\n        }\n\n\n    Parameters\n    ----------\n    data : str, FeatureDB instance, or iterator of Features\n        If `data` is a string, assume it's a GFF or GTF filename.  If it's\n        a FeatureDB instance, then its `all_features()` method will be\n        automatically called. Otherwise, assume it's an iterable of Feature\n        objects.\n\n    look_for : list\n        List of things to keep track of. Options are:\n\n            - any attribute of a Feature object, such as chrom, source, start,\n              stop, strand.\n\n            - \"attribute_keys\", which will look at all the individual\n              attribute keys of each feature\n\n    limit : int\n        Number of features to look at.  Default is no limit.\n\n    verbose : bool\n        Report how many features have been processed.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n\n    results = {}\n    obj_attrs = []\n    for i in look_for:\n        if i not in ['attribute_keys', 'feature_count']:\n            obj_attrs.append(i)\n        results[i] = Counter()\n\n    attr_keys = 'attribute_keys' in look_for\n\n    d = iterators.DataIterator(data)\n    feature_count = 0\n    for f in d:\n        if verbose:\n            sys.stderr.write('\\r%s features inspected' % feature_count)\n            sys.stderr.flush()\n\n        for obj_attr in obj_attrs:\n            results[obj_attr].update([getattr(f, obj_attr)])\n\n        if attr_keys:\n            results['attribute_keys'].update(f.attributes.keys())\n\n        feature_count += 1\n        if limit and feature_count == limit:\n            break\n\n    new_results = {}\n    for k, v in results.items():\n        new_results[k] = dict(v)\n\n    new_results['feature_count'] = feature_count\n    return new_results", "response": "Inspect a GFF or GTF file and return a dictionary of unique item features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fullfact(levels):\r\n    n = len(levels)  # number of factors\r\n    nb_lines = np.prod(levels)  # number of trial conditions\r\n    H = np.zeros((nb_lines, n))\r\n    \r\n    level_repeat = 1\r\n    range_repeat = np.prod(levels)\r\n    for i in range(n):\r\n        range_repeat //= levels[i]\r\n        lvl = []\r\n        for j in range(levels[i]):\r\n            lvl += [j]*level_repeat\r\n        rng = lvl*range_repeat\r\n        level_repeat *= levels[i]\r\n        H[:, i] = rng\r\n     \r\n    return H", "response": "Create a general full - factorial design matrix with coded levels 0 to k - 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a 2 - level fractional - factorial design with a generator string.", "response": "def fracfact(gen):\r\n    \"\"\"\r\n    Create a 2-level fractional-factorial design with a generator string.\r\n    \r\n    Parameters\r\n    ----------\r\n    gen : str\r\n        A string, consisting of lowercase, uppercase letters or operators \"-\"\r\n        and \"+\", indicating the factors of the experiment\r\n    \r\n    Returns\r\n    -------\r\n    H : 2d-array\r\n        A m-by-n matrix, the fractional factorial design. m is 2^k, where k\r\n        is the number of letters in ``gen``, and n is the total number of\r\n        entries in ``gen``.\r\n    \r\n    Notes\r\n    -----\r\n    In ``gen`` we define the main factors of the experiment and the factors\r\n    whose levels are the products of the main factors. For example, if\r\n    \r\n        gen = \"a b ab\"\r\n    \r\n    then \"a\" and \"b\" are the main factors, while the 3rd factor is the product\r\n    of the first two. If we input uppercase letters in ``gen``, we get the same\r\n    result. We can also use the operators \"+\" and \"-\" in ``gen``.\r\n    \r\n    For example, if\r\n    \r\n        gen = \"a b -ab\"\r\n    \r\n    then the 3rd factor is the opposite of the product of \"a\" and \"b\".\r\n    \r\n    The output matrix includes the two level full factorial design, built by\r\n    the main factors of ``gen``, and the products of the main factors. The\r\n    columns of ``H`` follow the sequence of ``gen``.\r\n    \r\n    For example, if\r\n    \r\n        gen = \"a b ab c\"\r\n    \r\n    then columns H[:, 0], H[:, 1], and H[:, 3] include the two level full\r\n    factorial design and H[:, 2] includes the products of the main factors.\r\n    \r\n    Examples\r\n    --------\r\n    ::\r\n    \r\n        >>> fracfact(\"a b ab\")\r\n        array([[-1., -1.,  1.],\r\n               [ 1., -1., -1.],\r\n               [-1.,  1., -1.],\r\n               [ 1.,  1.,  1.]])\r\n       \r\n        >>> fracfact(\"A B AB\")\r\n        array([[-1., -1.,  1.],\r\n               [ 1., -1., -1.],\r\n               [-1.,  1., -1.],\r\n               [ 1.,  1.,  1.]])\r\n        \r\n        >>> fracfact(\"a b -ab c +abc\")\r\n        array([[-1., -1., -1., -1., -1.],\r\n               [ 1., -1.,  1., -1.,  1.],\r\n               [-1.,  1.,  1., -1.,  1.],\r\n               [ 1.,  1., -1., -1., -1.],\r\n               [-1., -1., -1.,  1.,  1.],\r\n               [ 1., -1.,  1.,  1., -1.],\r\n               [-1.,  1.,  1.,  1., -1.],\r\n               [ 1.,  1., -1.,  1.,  1.]])\r\n       \r\n    \"\"\"\r\n    # Recognize letters and combinations\r\n    A = [item for item in re.split('\\-?\\s?\\+?', gen) if item]  # remove empty strings\r\n    C = [len(item) for item in A]\r\n    \r\n    # Indices of single letters (main factors)\r\n    I = [i for i, item in enumerate(C) if item==1]\r\n    \r\n    # Indices of letter combinations (we need them to fill out H2 properly).\r\n    J = [i for i, item in enumerate(C) if item!=1]\r\n    \r\n    # Check if there are \"-\" or \"+\" operators in gen\r\n    U = [item for item in gen.split(' ') if item]  # remove empty strings\r\n    \r\n    # If R1 is either None or not, the result is not changed, since it is a\r\n    # multiplication of 1.\r\n    R1 = _grep(U, '+')\r\n    R2 = _grep(U, '-')\r\n    \r\n    # Fill in design with two level factorial design\r\n    H1 = ff2n(len(I))\r\n    H = np.zeros((H1.shape[0], len(C)))\r\n    H[:, I] = H1\r\n    \r\n    # Recognize combinations and fill in the rest of matrix H2 with the proper\r\n    # products\r\n    for k in J:\r\n        # For lowercase letters\r\n        xx = np.array([ord(c) for c in A[k]]) - 97\r\n        \r\n        # For uppercase letters\r\n        if np.any(xx<0):\r\n            xx = np.array([ord(c) for c in A[k]]) - 65\r\n        \r\n        H[:, k] = np.prod(H1[:, xx], axis=1)\r\n    \r\n    # Update design if gen includes \"-\" operator\r\n    if R2:\r\n        H[:, R2] *= -1\r\n        \r\n    # Return the fractional factorial design\r\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new n - by - samples design matrix that is uniformly spaced between zero and one.", "response": "def lhs(n, samples=None, criterion=None, iterations=None):\r\n    \"\"\"\r\n    Generate a latin-hypercube design\r\n    \r\n    Parameters\r\n    ----------\r\n    n : int\r\n        The number of factors to generate samples for\r\n    \r\n    Optional\r\n    --------\r\n    samples : int\r\n        The number of samples to generate for each factor (Default: n)\r\n    criterion : str\r\n        Allowable values are \"center\" or \"c\", \"maximin\" or \"m\", \r\n        \"centermaximin\" or \"cm\", and \"correlation\" or \"corr\". If no value \r\n        given, the design is simply randomized.\r\n    iterations : int\r\n        The number of iterations in the maximin and correlations algorithms\r\n        (Default: 5).\r\n    \r\n    Returns\r\n    -------\r\n    H : 2d-array\r\n        An n-by-samples design matrix that has been normalized so factor values\r\n        are uniformly spaced between zero and one.\r\n    \r\n    Example\r\n    -------\r\n    A 3-factor design (defaults to 3 samples)::\r\n    \r\n        >>> lhs(3)\r\n        array([[ 0.40069325,  0.08118402,  0.69763298],\r\n               [ 0.19524568,  0.41383587,  0.29947106],\r\n               [ 0.85341601,  0.75460699,  0.360024  ]])\r\n       \r\n    A 4-factor design with 6 samples::\r\n    \r\n        >>> lhs(4, samples=6)\r\n        array([[ 0.27226812,  0.02811327,  0.62792445,  0.91988196],\r\n               [ 0.76945538,  0.43501682,  0.01107457,  0.09583358],\r\n               [ 0.45702981,  0.76073773,  0.90245401,  0.18773015],\r\n               [ 0.99342115,  0.85814198,  0.16996665,  0.65069309],\r\n               [ 0.63092013,  0.22148567,  0.33616859,  0.36332478],\r\n               [ 0.05276917,  0.5819198 ,  0.67194243,  0.78703262]])\r\n       \r\n    A 2-factor design with 5 centered samples::\r\n    \r\n        >>> lhs(2, samples=5, criterion='center')\r\n        array([[ 0.3,  0.5],\r\n               [ 0.7,  0.9],\r\n               [ 0.1,  0.3],\r\n               [ 0.9,  0.1],\r\n               [ 0.5,  0.7]])\r\n       \r\n    A 3-factor design with 4 samples where the minimum distance between\r\n    all samples has been maximized::\r\n    \r\n        >>> lhs(3, samples=4, criterion='maximin')\r\n        array([[ 0.02642564,  0.55576963,  0.50261649],\r\n               [ 0.51606589,  0.88933259,  0.34040838],\r\n               [ 0.98431735,  0.0380364 ,  0.01621717],\r\n               [ 0.40414671,  0.33339132,  0.84845707]])\r\n       \r\n    A 4-factor design with 5 samples where the samples are as uncorrelated\r\n    as possible (within 10 iterations)::\r\n    \r\n        >>> lhs(4, samples=5, criterion='correlate', iterations=10)\r\n    \r\n    \"\"\"\r\n    H = None\r\n    \r\n    if samples is None:\r\n        samples = n\r\n    \r\n    if criterion is not None:\r\n        assert criterion.lower() in ('center', 'c', 'maximin', 'm', \r\n            'centermaximin', 'cm', 'correlation', \r\n            'corr'), 'Invalid value for \"criterion\": {}'.format(criterion)\r\n    else:\r\n        H = _lhsclassic(n, samples)\r\n\r\n    if criterion is None:\r\n        criterion = 'center'\r\n    \r\n    if iterations is None:\r\n        iterations = 5\r\n        \r\n    if H is None:\r\n        if criterion.lower() in ('center', 'c'):\r\n            H = _lhscentered(n, samples)\r\n        elif criterion.lower() in ('maximin', 'm'):\r\n            H = _lhsmaximin(n, samples, iterations, 'maximin')\r\n        elif criterion.lower() in ('centermaximin', 'cm'):\r\n            H = _lhsmaximin(n, samples, iterations, 'centermaximin')\r\n        elif criterion.lower() in ('correlate', 'corr'):\r\n            H = _lhscorrelate(n, samples, iterations)\r\n    \r\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _pdist(x):\r\n    \r\n    x = np.atleast_2d(x)\r\n    assert len(x.shape)==2, 'Input array must be 2d-dimensional'\r\n    \r\n    m, n = x.shape\r\n    if m<2:\r\n        return []\r\n    \r\n    d = []\r\n    for i in range(m - 1):\r\n        for j in range(i + 1, m):\r\n            d.append((sum((x[j, :] - x[i, :])**2))**0.5)\r\n    \r\n    return np.array(d)", "response": "Calculates the pair - wise point distances of a 2 - dimensional array and returns a 2 - dimensional array with the distance between the two points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean a GFF file by removing features on unwanted chromosomes and unwanted featuretypes.", "response": "def clean_gff(gff, cleaned, add_chr=False, chroms_to_ignore=None,\n              featuretypes_to_ignore=None):\n    \"\"\"\n    Cleans a GFF file by removing features on unwanted chromosomes and of\n    unwanted featuretypes.  Optionally adds \"chr\" to chrom names.\n    \"\"\"\n    logger.info(\"Cleaning GFF\")\n    chroms_to_ignore = chroms_to_ignore or []\n    featuretypes_to_ignore = featuretypes_to_ignore or []\n    with open(cleaned, 'w') as fout:\n        for i in gffutils.iterators.DataIterator(gff):\n            if add_chr:\n                i.chrom = \"chr\" + i.chrom\n\n            if i.chrom in chroms_to_ignore:\n                continue\n\n            if i.featuretype in featuretypes_to_ignore:\n                continue\n            fout.write(str(i) + '\\n')\n    return cleaned"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new Central composite design with the given number of factors and center points.", "response": "def ccdesign(n, center=(4, 4), alpha='orthogonal', face='circumscribed'):\r\n    \"\"\"\r\n    Central composite design\r\n    \r\n    Parameters\r\n    ----------\r\n    n : int\r\n        The number of factors in the design.\r\n    \r\n    Optional\r\n    --------\r\n    center : int array\r\n        A 1-by-2 array of integers, the number of center points in each block\r\n        of the design. (Default: (4, 4)).\r\n    alpha : str\r\n        A string describing the effect of alpha has on the variance. ``alpha``\r\n        can take on the following values:\r\n        \r\n        1. 'orthogonal' or 'o' (Default)\r\n        \r\n        2. 'rotatable' or 'r'\r\n        \r\n    face : str\r\n        The relation between the start points and the corner (factorial) points.\r\n        There are three options for this input:\r\n        \r\n        1. 'circumscribed' or 'ccc': This is the original form of the central\r\n           composite design. The star points are at some distance ``alpha``\r\n           from the center, based on the properties desired for the design.\r\n           The start points establish new extremes for the low and high\r\n           settings for all factors. These designs have circular, spherical,\r\n           or hyperspherical symmetry and require 5 levels for each factor.\r\n           Augmenting an existing factorial or resolution V fractional \r\n           factorial design with star points can produce this design.\r\n        \r\n        2. 'inscribed' or 'cci': For those situations in which the limits\r\n           specified for factor settings are truly limits, the CCI design\r\n           uses the factors settings as the star points and creates a factorial\r\n           or fractional factorial design within those limits (in other words,\r\n           a CCI design is a scaled down CCC design with each factor level of\r\n           the CCC design divided by ``alpha`` to generate the CCI design).\r\n           This design also requires 5 levels of each factor.\r\n        \r\n        3. 'faced' or 'ccf': In this design, the star points are at the center\r\n           of each face of the factorial space, so ``alpha`` = 1. This \r\n           variety requires 3 levels of each factor. Augmenting an existing \r\n           factorial or resolution V design with appropriate star points can \r\n           also produce this design.\r\n    \r\n    Notes\r\n    -----\r\n    - Fractional factorial designs are not (yet) available here.\r\n    - 'ccc' and 'cci' can be rotatable design, but 'ccf' cannot.\r\n    - If ``face`` is specified, while ``alpha`` is not, then the default value\r\n      of ``alpha`` is 'orthogonal'.\r\n        \r\n    Returns\r\n    -------\r\n    mat : 2d-array\r\n        The design matrix with coded levels -1 and 1\r\n    \r\n    Example\r\n    -------\r\n    ::\r\n    \r\n        >>> ccdesign(3)\r\n        array([[-1.        , -1.        , -1.        ],\r\n               [ 1.        , -1.        , -1.        ],\r\n               [-1.        ,  1.        , -1.        ],\r\n               [ 1.        ,  1.        , -1.        ],\r\n               [-1.        , -1.        ,  1.        ],\r\n               [ 1.        , -1.        ,  1.        ],\r\n               [-1.        ,  1.        ,  1.        ],\r\n               [ 1.        ,  1.        ,  1.        ],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [-1.82574186,  0.        ,  0.        ],\r\n               [ 1.82574186,  0.        ,  0.        ],\r\n               [ 0.        , -1.82574186,  0.        ],\r\n               [ 0.        ,  1.82574186,  0.        ],\r\n               [ 0.        ,  0.        , -1.82574186],\r\n               [ 0.        ,  0.        ,  1.82574186],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [ 0.        ,  0.        ,  0.        ],\r\n               [ 0.        ,  0.        ,  0.        ]])\r\n        \r\n       \r\n    \"\"\"\r\n    # Check inputs\r\n    assert isinstance(n, int) and n>1, '\"n\" must be an integer greater than 1.'\r\n    assert alpha.lower() in ('orthogonal', 'o', 'rotatable', \r\n        'r'), 'Invalid value for \"alpha\": {:}'.format(alpha)\r\n    assert face.lower() in ('circumscribed', 'ccc', 'inscribed', 'cci',\r\n        'faced', 'ccf'), 'Invalid value for \"face\": {:}'.format(face)\r\n    \r\n    try:\r\n        nc = len(center)\r\n    except:\r\n        raise TypeError('Invalid value for \"center\": {:}. Expected a 1-by-2 array.'.format(center))\r\n    else:\r\n        if nc!=2:\r\n            raise ValueError('Invalid number of values for \"center\" (expected 2, but got {:})'.format(nc))\r\n\r\n    # Orthogonal Design\r\n    if alpha.lower() in ('orthogonal', 'o'):\r\n        H2, a = star(n, alpha='orthogonal', center=center)\r\n    \r\n    # Rotatable Design\r\n    if alpha.lower() in ('rotatable', 'r'):\r\n        H2, a = star(n, alpha='rotatable')\r\n    \r\n    # Inscribed CCD\r\n    if face.lower() in ('inscribed', 'cci'):\r\n        H1 = ff2n(n)\r\n        H1 = H1/a  # Scale down the factorial points\r\n        H2, a = star(n)\r\n    \r\n    # Faced CCD\r\n    if face.lower() in ('faced', 'ccf'):\r\n        H2, a = star(n)  # Value of alpha is always 1 in Faced CCD\r\n        H1 = ff2n(n)\r\n    \r\n    # Circumscribed CCD\r\n    if face.lower() in ('circumscribed', 'ccc'):\r\n        H1 = ff2n(n)\r\n    \r\n    C1 = repeat_center(n, center[0])\r\n    C2 = repeat_center(n, center[1])\r\n\r\n    H1 = union(H1, C1)\r\n    H2 = union(H2, C2)\r\n    H = union(H1, H2)\r\n\r\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a line from a GFF file return a Feature object.", "response": "def feature_from_line(line, dialect=None, strict=True, keep_order=False):\n    \"\"\"\n    Given a line from a GFF file, return a Feature object\n\n    Parameters\n    ----------\n    line : string\n\n    strict : bool\n        If True (default), assume `line` is a single, tab-delimited string that\n        has at least 9 fields.\n\n        If False, then the input can have a more flexible format, useful for\n        creating single ad hoc features or for writing tests.  In this case,\n        `line` can be a multi-line string (as long as it has a single non-empty\n        line), and, as long as there are only 9 fields (standard GFF/GTF), then\n        it's OK to use spaces instead of tabs to separate fields in `line`.\n        But if >9 fields are to be used, then tabs must be used.\n\n    keep_order, dialect\n        Passed directly to :class:`Feature`; see docstring for that class for\n        description\n\n    Returns\n    -------\n    A new :class:`Feature` object.\n    \"\"\"\n    if not strict:\n        lines = line.splitlines(False)\n        _lines = []\n        for i in lines:\n            i = i.strip()\n            if len(i) > 0:\n                _lines.append(i)\n\n        assert len(_lines) == 1, _lines\n        line = _lines[0]\n\n        if '\\t' in line:\n            fields = line.rstrip('\\n\\r').split('\\t')\n        else:\n            fields = line.rstrip('\\n\\r').split(None, 8)\n    else:\n        fields = line.rstrip('\\n\\r').split('\\t')\n    try:\n        attr_string = fields[8]\n    except IndexError:\n        attr_string = \"\"\n    attrs, _dialect = parser._split_keyvals(attr_string, dialect=dialect)\n    d = dict(list(zip(constants._gffkeys, fields)))\n    d['attributes'] = attrs\n    d['extra'] = fields[9:]\n    d['keep_order'] = keep_order\n    if dialect is None:\n        dialect = _dialect\n    return Feature(dialect=dialect, **d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_bin(self, _bin=None):\n        if _bin is None:\n            try:\n                _bin = bins.bins(self.start, self.end, one=True)\n            except TypeError:\n                _bin = None\n        return _bin", "response": "Calculate the smallest UCSC genomic bin that will contain this feature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple suitable for import into a database.", "response": "def astuple(self, encoding=None):\n        \"\"\"\n        Return a tuple suitable for import into a database.\n\n        Attributes field and extra field jsonified into strings. The order of\n        fields is such that they can be supplied as arguments for the query\n        defined in :attr:`gffutils.constants._INSERT`.\n\n        If `encoding` is not None, then convert string fields to unicode using\n        the provided encoding.\n\n        Returns\n        -------\n        Tuple\n        \"\"\"\n        if not encoding:\n            return (\n                self.id, self.seqid, self.source, self.featuretype, self.start,\n                self.end, self.score, self.strand, self.frame,\n                helpers._jsonify(self.attributes),\n                helpers._jsonify(self.extra), self.calc_bin()\n            )\n        return (\n            self.id.decode(encoding), self.seqid.decode(encoding),\n            self.source.decode(encoding), self.featuretype.decode(encoding),\n            self.start, self.end, self.score.decode(encoding),\n            self.strand.decode(encoding), self.frame.decode(encoding),\n            helpers._jsonify(self.attributes).decode(encoding),\n            helpers._jsonify(self.extra).decode(encoding), self.calc_bin()\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the sequence of this feature as a string.", "response": "def sequence(self, fasta, use_strand=True):\n        \"\"\"\n        Retrieves the sequence of this feature as a string.\n\n        Uses the pyfaidx package.\n\n        Parameters\n        ----------\n\n        fasta : str\n            If str, then it's a FASTA-format filename; otherwise assume it's\n            a pyfaidx.Fasta object.\n\n        use_strand : bool\n            If True (default), the sequence returned will be\n            reverse-complemented for minus-strand features.\n\n        Returns\n        -------\n        string\n        \"\"\"\n        if isinstance(fasta, six.string_types):\n            fasta = Fasta(fasta, as_raw=False)\n\n        # recall GTF/GFF is 1-based closed;  pyfaidx uses Python slice notation\n        # and is therefore 0-based half-open.\n        seq = fasta[self.chrom][self.start-1:self.stop]\n        if use_strand and self.strand == '-':\n            seq = seq.reverse.complement\n        return seq.seq"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_bed12(f, db, child_type='exon', name_field='ID'):\n    if isinstance(f, six.string_types):\n        f = db[f]\n    children = list(db.children(f, featuretype=child_type, order_by='start'))\n    sizes = [len(i) for i in children]\n    starts = [i.start - f.start for i in children]\n    fields = [\n        f.chrom,\n        f.start - 1,  # GTF -> BED coord system\n        f.stop,\n        f.attributes.get(name_field, ['.'])[0],\n        f.score,\n        f.strand,\n        f.start,\n        f.stop,\n        '0,0,0',\n        len(children),\n        ','.join(map(str, sizes)),\n        ','.join(map(str, starts))\n    ]\n    return '\\t'.join(map(str, fields)) + '\\n'", "response": "Given a top - level feature or string returns a BED12 entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef infer_dialect(attributes):\n    if isinstance(attributes, six.string_types):\n        attributes = [attributes]\n    dialects = [parser._split_keyvals(i)[1] for i in attributes]\n    return _choose_dialect(dialects)", "response": "Infer the dialect based on the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _choose_dialect(dialects):\n    # NOTE: can use helpers.dialect_compare if you need to make this more\n    # complex....\n\n    # For now, this function favors the first dialect, and then appends the\n    # order of additional fields seen in the attributes of other lines giving\n    # priority to dialects that come first in the iterable.\n    if len(dialects) == 0:\n        return constants.dialect\n    final_order = []\n    for dialect in dialects:\n        for o in dialect['order']:\n            if o not in final_order:\n                final_order.append(o)\n    dialect = dialects[0]\n    dialect['order'] = final_order\n    return dialect", "response": "Given a list of dialects choose the one to use as the canonical version."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_query(args, other=None, limit=None, strand=None, featuretype=None,\n               extra=None, order_by=None, reverse=False,\n               completely_within=False):\n    \"\"\"\n    Multi-purpose, bare-bones ORM function.\n\n    This function composes queries given some commonly-used kwargs that can be\n    passed to FeatureDB methods (like .parents(), .children(), .all_features(),\n    .features_of_type()).  It handles, in one place, things like restricting to\n    featuretype, limiting to a genomic range, limiting to one strand, or\n    returning results ordered by different criteria.\n\n    Additional filtering/subsetting/sorting behavior should be added here.\n\n    (Note: this ended up having better performance (and flexibility) than\n    sqlalchemy)\n\n    This function also provides support for additional JOINs etc (supplied via\n    the `other` kwarg) and extra conditional clauses (`extra` kwarg).  See the\n    `_QUERY` var below for the order in which they are used.\n\n    For example, FeatureDB._relation uses `other` to supply the JOIN\n    substatment, and that same method also uses `extra` to supply the\n    \"relations.level = ?\" substatment (see the source for FeatureDB._relation\n    for more details).\n\n    `args` contains the arguments that will ultimately be supplied to the\n    sqlite3.connection.execute function.  It may be further populated below --\n    for example, if strand=\"+\", then the query will include a strand clause,\n    and the strand will be appended to the args.\n\n    `args` can be pre-filled with args that are passed to `other` and `extra`.\n    \"\"\"\n\n    _QUERY = (\"{_SELECT} {OTHER} {EXTRA} {FEATURETYPE} \"\n              \"{LIMIT} {STRAND} {ORDER_BY}\")\n\n    # Construct a dictionary `d` that will be used later as _QUERY.format(**d).\n    # Default is just _SELECT, which returns all records in the features table.\n    # (Recall that constants._SELECT gets the fields in the order needed to\n    # reconstruct a Feature)\n    d = dict(_SELECT=constants._SELECT, OTHER=\"\", FEATURETYPE=\"\", LIMIT=\"\",\n             STRAND=\"\", ORDER_BY=\"\", EXTRA=\"\")\n\n    if other:\n        d['OTHER'] = other\n    if extra:\n        d['EXTRA'] = extra\n\n    # If `other` and `extra` take args (that is, they have \"?\" in them), then\n    # they should have been provided in `args`.\n    required_args = (d['EXTRA'] + d['OTHER']).count('?')\n    if len(args) != required_args:\n        raise ValueError('Not enough args (%s) for subquery' % args)\n\n    # Below, if a kwarg is specified, then we create sections of the query --\n    # appending to args as necessary.\n    #\n    # IMPORTANT: the order in which things are processed here is the same as\n    # the order of the placeholders in _QUERY.  That is, we need to build the\n    # args in parallel with the query to avoid putting the wrong args in the\n    # wrong place.\n\n    if featuretype:\n        # Handle single or iterables of featuretypes.\n        #\n        # e.g., \"featuretype = 'exon'\"\n        #\n        # or, \"featuretype IN ('exon', 'CDS')\"\n        if isinstance(featuretype, six.string_types):\n            d['FEATURETYPE'] = \"features.featuretype = ?\"\n            args.append(featuretype)\n        else:\n            d['FEATURETYPE'] = (\n                \"features.featuretype IN  (%s)\"\n                % (','.join([\"?\" for _ in featuretype]))\n            )\n            args.extend(featuretype)\n\n    if limit:\n        # Restrict to a genomic region.  Makes use of the UCSC binning strategy\n        # for performance.\n        #\n        # `limit` is a string or a tuple of (chrom, start, stop)\n        #\n        # e.g., \"seqid = 'chr2L' AND start > 1000 AND end < 5000\"\n        if isinstance(limit, six.string_types):\n            seqid, startstop = limit.split(':')\n            start, end = startstop.split('-')\n        else:\n            seqid, start, end = limit\n\n        # Identify possible bins\n        _bins = bins.bins(int(start), int(end), one=False)\n\n        # Use different overlap conditions\n        if completely_within:\n            d['LIMIT'] = (\n                \"features.seqid = ? AND features.start >= ? \"\n                \"AND features.end <= ?\"\n            )\n            args.extend([seqid, start, end])\n\n        else:\n            d['LIMIT'] = (\n                \"features.seqid = ? AND features.start <= ? \"\n                \"AND features.end >= ?\"\n            )\n            # Note order (end, start)\n            args.extend([seqid, end, start])\n\n        # Add bin clause. See issue #45.\n        if len(_bins) < 900:\n            d['LIMIT'] += \" AND features.bin IN (%s)\" % (','.join(map(str, _bins)))\n\n    if strand:\n        # e.g., \"strand = '+'\"\n        d['STRAND'] = \"features.strand = ?\"\n        args.append(strand)\n\n    # TODO: implement file_order!\n    valid_order_by = constants._gffkeys_extra + ['file_order', 'length']\n    _order_by = []\n    if order_by:\n        # Default is essentially random order.\n        #\n        # e.g. \"ORDER BY seqid, start DESC\"\n        if isinstance(order_by, six.string_types):\n            _order_by.append(order_by)\n\n        else:\n            for k in order_by:\n                if k not in valid_order_by:\n                    raise ValueError(\"%s not a valid order-by value in %s\"\n                                     % (k, valid_order_by))\n\n                # There's no length field, so order by end - start\n                if k == 'length':\n                    k = '(end - start)'\n\n                _order_by.append(k)\n\n        _order_by = ','.join(_order_by)\n        if reverse:\n            direction = 'DESC'\n        else:\n            direction = 'ASC'\n        d['ORDER_BY'] = 'ORDER BY %s %s' % (_order_by, direction)\n\n    # Ensure only one \"WHERE\" is included; the rest get \"AND \".  This is ugly.\n    where = False\n    if \"where\" in d['OTHER'].lower():\n        where = True\n    for i in ['EXTRA', 'FEATURETYPE', 'LIMIT', 'STRAND']:\n        if d[i]:\n            if not where:\n                d[i] = \"WHERE \" + d[i]\n                where = True\n            else:\n                d[i] = \"AND \" + d[i]\n\n    return _QUERY.format(**d), args", "response": "This function is a multi - purpose ORM function that returns a query that can be used to reconstruct a feature table."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a dictionary yielded by the parser return the genomic UCSC bin containing the genomic UCSC entry.", "response": "def _bin_from_dict(d):\n    \"\"\"\n    Given a dictionary yielded by the parser, return the genomic \"UCSC\" bin\n    \"\"\"\n    try:\n        start = int(d['start'])\n        end = int(d['end'])\n        return bins.bins(start, end, one=True)\n\n    # e.g., if \".\"\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing most compact form of JSON", "response": "def _jsonify(x):\n    \"\"\"Use most compact form of JSON\"\"\"\n    if isinstance(x, dict_class):\n        return json.dumps(x._d, separators=(',', ':'))\n    return json.dumps(x, separators=(',', ':'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unjsonify(x, isattributes=False):\n    if isattributes:\n        obj = json.loads(x)\n        return dict_class(obj)\n    return json.loads(x)", "response": "Convert JSON string to an ordered defaultdict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _feature_to_fields(f, jsonify=True):\n    x = []\n    for k in constants._keys:\n        v = getattr(f, k)\n        if jsonify and (k in ('attributes', 'extra')):\n            x.append(_jsonify(v))\n        else:\n            x.append(v)\n    return tuple(x)", "response": "Convert a feature to a tuple for faster sqlite3 import"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dict_to_fields(d, jsonify=True):\n    x = []\n    for k in constants._keys:\n        v = d[k]\n        if jsonify and (k in ('attributes', 'extra')):\n            x.append(_jsonify(v))\n        else:\n            x.append(v)\n    return tuple(x)", "response": "Convert dict to tuple for faster sqlite3 import"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging two dictionary of attributes into a single dictionary.", "response": "def merge_attributes(attr1, attr2):\n    \"\"\"\n    Merges two attribute dictionaries into a single dictionary.\n\n    Parameters\n    ----------\n    `attr1`, `attr2` : dict\n\n    Returns\n    -------\n    dict\n    \"\"\"\n\n    new_d = copy.deepcopy(attr1)\n    new_d.update(attr2)\n\n    #all of attr2 key : values just overwrote attr1, fix it\n    for k, v in new_d.items():\n        if not isinstance(v, list):\n            new_d[k] = [v]\n\n    for k, v in six.iteritems(attr1):\n        if k in attr2:\n            if not isinstance(v, list):\n                v = [v]\n            new_d[k].extend(v)\n    return dict((k, sorted(set(v))) for k, v in new_d.items())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dialect_compare(dialect1, dialect2):\n    orig = set(dialect1.items())\n    new = set(dialect2.items())\n    return dict(\n        added=dict(list(new.difference(orig))),\n        removed=dict(list(orig.difference(new)))\n    )", "response": "Returns a dictionary of the differences between two dialects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sanitize_gff_db(db, gid_field=\"gid\"):\n    def sanitized_iterator():\n        # Iterate through the database by each gene's records\n        for gene_recs in db.iter_by_parent_childs():\n            # The gene's ID\n            gene_id = gene_recs[0].id\n            for rec in gene_recs:\n                # Fixup coordinates if necessary\n                if rec.start > rec.stop:\n                    rec.start, rec.stop = rec.stop, rec.start\n                # Add a gene id field to each gene's records\n                rec.attributes[gid_field] = [gene_id]\n                yield rec\n    # Return sanitized GFF database\n    sanitized_db = \\\n        gffutils.create_db(sanitized_iterator(), \":memory:\",\n                           verbose=False)\n    return sanitized_db", "response": "Sanitize GFF database by removing unnecessary coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sanitize_gff_file(gff_fname,\n                      in_memory=True,\n                      in_place=False):\n    \"\"\"\n    Sanitize a GFF file.\n    \"\"\"\n    db = None\n    if is_gff_db(gff_fname):\n        # It's a database filename, so load it\n        db = gffutils.FeatureDB(gff_fname)\n    else:\n        # Need to create a database for file\n        if in_memory:\n            db = gffutils.create_db(gff_fname, \":memory:\",\n                                    verbose=False)\n        else:\n            db = get_gff_db(gff_fname)\n    if in_place:\n        gff_out = gffwriter.GFFWriter(gff_fname,\n                                      in_place=in_place)\n    else:\n        gff_out = gffwriter.GFFWriter(sys.stdout)\n    sanitized_db = sanitize_gff_db(db)\n    for gene_rec in sanitized_db.all_features(featuretype=\"gene\"):\n        gff_out.write_gene_recs(sanitized_db, gene_rec.id)\n    gff_out.close()", "response": "Sanitize a GFF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the given filename is a GFF database.", "response": "def is_gff_db(db_fname):\n    \"\"\"\n    Return True if the given filename is a GFF database.\n\n    For now, rely on .db extension.\n    \"\"\"\n    if not os.path.isfile(db_fname):\n        return False\n    if db_fname.endswith(\".db\"):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_gff_db(gff_fname,\n               ext=\".db\"):\n    \"\"\"\n    Get db for GFF file. If the database has a .db file,\n    load that. Otherwise, create a named temporary file,\n    serialize the db to that, and return the loaded database.\n    \"\"\"\n    if not os.path.isfile(gff_fname):\n        # Not sure how we should deal with errors normally in\n        # gffutils -- Ryan?\n        raise ValueError(\"GFF %s does not exist.\" % (gff_fname))\n    candidate_db_fname = \"%s.%s\" % (gff_fname, ext)\n    if os.path.isfile(candidate_db_fname):\n        # Standard .db file found, so return it\n        return candidate_db_fname\n    # Otherwise, we need to create a temporary but non-deleted\n    # file to store the db in. It'll be up to the user\n    # of the function the delete the file when done.\n    ## NOTE: Ryan must have a good scheme for dealing with this\n    ## since pybedtools does something similar under the hood, i.e.\n    ## creating temporary files as needed without over proliferation\n    db_fname = tempfile.NamedTemporaryFile(delete=False)\n    # Create the database for the gff file (suppress output\n    # when using function internally)\n    print(\"Creating db for %s\" % (gff_fname))\n    t1 = time.time()\n    db = gffutils.create_db(gff_fname, db_fname.name,\n                            merge_strategy=\"merge\",\n                            verbose=False)\n    t2 = time.time()\n    print(\"  - Took %.2f seconds\" % (t2 - t1))\n    return db", "response": "Get db for a GFF file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a set of bin numbers that cover the given start and stop coordinates.", "response": "def bins(start, stop, fmt='gff', one=True):\n    \"\"\"\n    Uses the definition of a \"genomic bin\" described in Fig 7 of\n    http://genome.cshlp.org/content/12/6/996.abstract.\n\n    Parameters\n    ----------\n    one : boolean\n        If `one=True` (default), then only return the smallest bin that\n        completely contains these coordinates (useful for assigning a single\n        bin).\n\n        If `one=False`, then return the set of *all* bins that overlap these\n        coordinates (useful for looking for features that could intersect)\n\n    fmt : 'gff' | 'bed'\n        This specifies 1-based start coords (gff) or 0-based start coords (bed)\n    \"\"\"\n\n    # For very large coordinates, return 1 which is \"somewhere on the\n    # chromosome\".\n    if start >= MAX_CHROM_SIZE or stop >= MAX_CHROM_SIZE:\n        if one:\n            return 1\n        else:\n            return set([1])\n\n    # Jump to highest resolution bin that will fit these coords (depending on\n    # whether we have a BED or GFF-style coordinate).\n    #\n    # Some GFF files include negative coords, which will throw off this\n    # calculation.  If negative coords, then set the bin to the largest\n    # possible.\n    if start < 0:\n        if one:\n            return 1\n        else:\n            return set([1])\n\n    if stop < 0:\n        if one:\n            return 1\n        else:\n            return set([1])\n\n    start = (start - COORD_OFFSETS[fmt]) >> FIRST_SHIFT\n    stop = (stop) >> FIRST_SHIFT\n\n    # We always at least fit within the chrom, which is bin 1.\n    bins = set([1])\n\n    for offset in OFFSETS:\n        # Since we're going from smallest to largest bins, the first one where\n        # the feature's start and stop positions are both within the same bin\n        # is the smallest one these coords fit within.\n        if one:\n            if start == stop:\n                # Note that at this point, because of the bit-shifting, `start`\n                # is the number of bins (at this current level).  So we need to\n                # add it to `offset` to get the actual bin ID.\n                return offset + start\n\n        # See the Fig 7 reproduction above to see why range().\n        bins.update(list(range(offset + start, offset + stop + 1)))\n\n        # Move to the next level (8x larger bin size; i.e., 2**NEXT_SHIFT\n        # larger bin size)\n        start >>= NEXT_SHIFT\n        stop >>= NEXT_SHIFT\n\n    return bins"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the bin sizes of the current and next bin.", "response": "def print_bin_sizes():\n    \"\"\"\n    Useful for debugging: how large is each bin, and what are the bin IDs?\n    \"\"\"\n    for i, offset in enumerate(OFFSETS):\n        binstart = offset\n        try:\n            binstop = OFFSETS[i + 1]\n        except IndexError:\n            binstop = binstart\n\n        bin_size = 2 ** (FIRST_SHIFT + (i * NEXT_SHIFT))\n        actual_size = bin_size\n\n        # nice formatting\n        bin_size, suffix = bin_size / 1024, 'Kb'\n        if bin_size >= 1024:\n            bin_size, suffix = bin_size / 1024, 'Mb'\n        if bin_size >= 1024:\n            bin_size, suffix = bin_size / 1024, 'Gb'\n        size = '(%s %s)' % (bin_size, suffix)\n        actual_size = '%s bp' % (actual_size)\n\n        print('level: {i:1};  bins {binstart:<4} to {binstop:<4}; '\n              'size: {actual_size:<12} {size:<6}'.format(**locals()))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreconstructing the original attributes string according to the given dialect.", "response": "def _reconstruct(keyvals, dialect, keep_order=False,\n                 sort_attribute_values=False):\n    \"\"\"\n    Reconstructs the original attributes string according to the dialect.\n\n    Parameters\n    ==========\n    keyvals : dict\n        Attributes from a GFF/GTF feature\n\n    dialect : dict\n        Dialect containing info on how to reconstruct a string version of the\n        attributes\n\n    keep_order : bool\n        If True, then perform sorting of attribute keys to ensure they are in\n        the same order as those provided in the original file.  Default is\n        False, which saves time especially on large data sets.\n\n    sort_attribute_values : bool\n        If True, then sort values to ensure they will always be in the same\n        order.  Mostly only useful for testing; default is False.\n    \"\"\"\n    if not dialect:\n        raise AttributeStringError()\n    if not keyvals:\n        return \"\"\n    parts = []\n\n    # Re-encode when reconstructing attributes\n    if constants.ignore_url_escape_characters or dialect['fmt'] != 'gff3':\n        attributes = keyvals\n    else:\n        attributes = {}\n        for k, v in keyvals.items():\n            attributes[k] = []\n            for i in v:\n                attributes[k].append(''.join([quoter[j] for j in i]))\n\n    # May need to split multiple values into multiple key/val pairs\n    if dialect['repeated keys']:\n        items = []\n        for key, val in attributes.items():\n            if len(val) > 1:\n                for v in val:\n                    items.append((key, [v]))\n            else:\n                items.append((key, val))\n    else:\n        items = list(attributes.items())\n\n    def sort_key(x):\n        # sort keys by their order in the dialect; anything not in there will\n        # be in arbitrary order at the end.\n        try:\n            return dialect['order'].index(x[0])\n        except ValueError:\n            return 1e6\n\n    if keep_order:\n        items.sort(key=sort_key)\n\n    for key, val in items:\n\n        # Multival sep is usually a comma:\n        if val:\n            if sort_attribute_values:\n                val = sorted(val)\n\n            val_str = dialect['multival separator'].join(val)\n\n            if val_str:\n\n                # Surround with quotes if needed\n                if dialect['quoted GFF2 values']:\n                    val_str = '\"%s\"' % val_str\n\n                # Typically \"=\" for GFF3 or \" \" otherwise\n                part = dialect['keyval separator'].join([key, val_str])\n        else:\n            if dialect['fmt'] == 'gtf':\n                part = dialect['keyval separator'].join([key, '\"\"'])\n            else:\n                part = key\n        parts.append(part)\n\n    # Typically \";\" or \"; \"\n    parts_str = dialect['field separator'].join(parts)\n\n    # Sometimes need to add this\n    if dialect['trailing semicolon']:\n        parts_str += ';'\n\n    return parts_str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a GFF - like line and a dialect dictionary return the original string and the dialect dictionary.", "response": "def _split_keyvals(keyval_str, dialect=None):\n    \"\"\"\n    Given the string attributes field of a GFF-like line, split it into an\n    attributes dictionary and a \"dialect\" dictionary which contains information\n    needed to reconstruct the original string.\n\n    Lots of logic here to handle all the corner cases.\n\n    If `dialect` is None, then do all the logic to infer a dialect from this\n    attribute string.\n\n    Otherwise, use the provided dialect (and return it at the end).\n    \"\"\"\n\n    def _unquote_quals(quals, dialect):\n        \"\"\"\n        Handles the unquoting (decoding) of percent-encoded characters.\n\n        See notes on encoding/decoding above.\n        \"\"\"\n        if not constants.ignore_url_escape_characters and dialect['fmt'] == 'gff3':\n            for key, vals in quals.items():\n                unquoted = [urllib.parse.unquote(v) for v in vals]\n                quals[key] = unquoted\n        return quals\n\n    infer_dialect = False\n    if dialect is None:\n        # Make a copy of default dialect so it can be modified as needed\n        dialect = copy.copy(constants.dialect)\n        infer_dialect = True\n    from gffutils import feature\n    quals = feature.dict_class()\n    if not keyval_str:\n        return quals, dialect\n\n    # If a dialect was provided, then use that directly.\n    if not infer_dialect:\n        if dialect['trailing semicolon']:\n            keyval_str = keyval_str.rstrip(';')\n\n        parts = keyval_str.split(dialect['field separator'])\n\n        kvsep = dialect['keyval separator']\n        if dialect['leading semicolon']:\n            pieces = []\n            for p in parts:\n                if p and p[0] == ';':\n                    p = p[1:]\n                pieces.append(p.strip().split(kvsep))\n                key_vals = [(p[0], \" \".join(p[1:])) for p in pieces]\n\n        if dialect['fmt'] == 'gff3':\n            key_vals = [p.split(kvsep) for p in parts]\n        else:\n            leadingsemicolon = dialect['leading semicolon']\n            pieces = []\n            for i, p in enumerate(parts):\n                if i == 0 and leadingsemicolon:\n                    p = p[1:]\n                pieces.append(p.strip().split(kvsep))\n                key_vals = [(p[0], \" \".join(p[1:])) for p in pieces]\n\n        quoted = dialect['quoted GFF2 values']\n        for item in key_vals:\n            # Easy if it follows spec\n            if len(item) == 2:\n                key, val = item\n\n            # Only key provided?\n            elif len(item) == 1:\n                key = item[0]\n                val = ''\n\n            else:\n                key = item[0]\n                val = dialect['keyval separator'].join(item[1:])\n\n            try:\n                quals[key]\n            except KeyError:\n                quals[key] = []\n\n            if quoted:\n                if (len(val) > 0 and val[0] == '\"' and val[-1] == '\"'):\n                    val = val[1:-1]\n\n            if val:\n                # TODO: if there are extra commas for a value, just use empty\n                # strings\n                # quals[key].extend([v for v in val.split(',') if v])\n                vals = val.split(',')\n                quals[key].extend(vals)\n\n        quals = _unquote_quals(quals, dialect)\n        return quals, dialect\n\n    # If we got here, then we need to infer the dialect....\n    #\n    # Reset the order to an empty list so that it will only be populated with\n    # keys that are found in the file.\n    dialect['order'] = []\n\n    # ensembl GTF has trailing semicolon\n    if keyval_str[-1] == ';':\n        keyval_str = keyval_str[:-1]\n        dialect['trailing semicolon'] = True\n\n    # GFF2/GTF has a semicolon with at least one space after it.\n    # Spaces can be on both sides (e.g. wormbase)\n    # GFF3 works with no spaces.\n    # So split on the first one we can recognize...\n    for sep in (' ; ', '; ', ';'):\n        parts = keyval_str.split(sep)\n        if len(parts) > 1:\n            dialect['field separator'] = sep\n            break\n\n    # Is it GFF3?  They have key-vals separated by \"=\"\n    if gff3_kw_pat.match(parts[0]):\n        key_vals = [p.split('=') for p in parts]\n        dialect['fmt'] = 'gff3'\n        dialect['keyval separator'] = '='\n\n    # Otherwise, key-vals separated by space.  Key is first item.\n    else:\n        dialect['keyval separator'] = \" \"\n        pieces = []\n        for p in parts:\n            # Fix misplaced semicolons in keys in some GFF2 files\n            if p and p[0] == ';':\n                p = p[1:]\n                dialect['leading semicolon'] = True\n            pieces.append(p.strip().split(' '))\n        key_vals = [(p[0], \" \".join(p[1:])) for p in pieces]\n\n    for item in key_vals:\n\n        # Easy if it follows spec\n        if len(item) == 2:\n            key, val = item\n\n        # Only key provided?\n        elif len(item) == 1:\n                key = item[0]\n                val = ''\n\n        # Pathological cases where values of a key have within them the key-val\n        # separator, e.g.,\n        #  Alias=SGN-M1347;ID=T0028;Note=marker name(s): T0028 SGN-M1347 |identity=99.58|escore=2e-126\n        else:\n            key = item[0]\n            val = dialect['keyval separator'].join(item[1:])\n\n        # Is the key already in there?\n        if key in quals:\n            dialect['repeated keys'] = True\n        else:\n            quals[key] = []\n\n        # Remove quotes in GFF2\n        if len(val) > 0 and val[0] == '\"' and val[-1] == '\"':\n            val = val[1:-1]\n            dialect['quoted GFF2 values'] = True\n        if val:\n            # TODO: if there are extra commas for a value, just use empty\n            # strings\n            # quals[key].extend([v for v in val.split(',') if v])\n            vals = val.split(',')\n            if (len(vals) > 1) and dialect['repeated keys']:\n                raise AttributeStringError(\n                    \"Internally inconsistent attributes formatting: \"\n                    \"some have repeated keys, some do not.\")\n            quals[key].extend(vals)\n\n        # keep track of the order of keys\n        dialect['order'].append(key)\n\n    if (\n        (dialect['keyval separator'] == ' ') and\n        (dialect['quoted GFF2 values'])\n    ):\n        dialect['fmt'] = 'gtf'\n\n    quals = _unquote_quals(quals, dialect)\n    return quals, dialect"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a database from a GFF or GTF file.", "response": "def create_db(data, dbfn, id_spec=None, force=False, verbose=False,\n              checklines=10, merge_strategy='error', transform=None,\n              gtf_transcript_key='transcript_id', gtf_gene_key='gene_id',\n              gtf_subfeature='exon', force_gff=False,\n              force_dialect_check=False, from_string=False, keep_order=False,\n              text_factory=sqlite3.OptimizedUnicode, force_merge_fields=None,\n              pragmas=constants.default_pragmas, sort_attribute_values=False,\n              dialect=None, _keep_tempfiles=False, infer_gene_extent=True,\n              disable_infer_genes=False, disable_infer_transcripts=False,\n              **kwargs):\n    \"\"\"\n    Create a database from a GFF or GTF file.\n\n    For more details on when and how to use the kwargs below, see the examples\n    in the online documentation (:ref:`examples`).\n\n    Parameters\n    ----------\n    data : string or iterable\n\n        If a string (and `from_string` is False), then `data` is the path to\n        the original GFF or GTF file.\n\n        If a string and `from_string` is True, then assume `data` is the actual\n        data to use.\n\n        Otherwise, it's an iterable of Feature objects.\n\n    dbfn : string\n\n        Path to the database that will be created.  Can be the special string\n        \":memory:\" to create an in-memory database.\n\n    id_spec : string, list, dict, callable, or None\n\n        This parameter guides what will be used as the primary key for the\n        database, which in turn determines how you will access individual\n        features by name from the database.\n\n        If `id_spec=None`, then auto-increment primary keys based on the\n        feature type (e.g., \"gene_1\", \"gene_2\").  This is also the fallback\n        behavior for the other values below.\n\n        If `id_spec` is a string, then look for this key in the attributes.  If\n        it exists, then use its value as the primary key, otherwise\n        autoincrement based on the feature type.  For many GFF3 files, \"ID\"\n        usually works well.\n\n        If `id_spec` is a list or tuple of keys, then check for each one in\n        order, using the first one found.  For GFF3, this might be [\"ID\",\n        \"Name\"], which would use the ID if it exists, otherwise the Name,\n        otherwise autoincrement based on the feature type.\n\n        If `id_spec` is a dictionary, then it is a mapping of feature types to\n        what should be used as the ID.  For example, for GTF files, `{'gene':\n        'gene_id', 'transcript': 'transcript_id'}` may be useful.  The values\n        of this dictionary can also be a list, e.g., `{'gene': ['gene_id',\n        'geneID']}`\n\n        If `id_spec` is a callable object, then it accepts a dictionary from\n        the iterator and returns one of the following:\n\n            * None (in which case the feature type will be auto-incremented)\n            * string (which will be used as the primary key)\n            * special string starting with \"autoincrement:X\", where \"X\" is\n              a string that will be used for auto-incrementing.  For example,\n              if \"autoincrement:chr10\", then the first feature will be\n              \"chr10_1\", the second \"chr10_2\", and so on.\n\n    force : bool\n\n        If `False` (default), then raise an exception if `dbfn` already exists.\n        Use `force=True` to overwrite any existing databases.\n\n    verbose : bool\n\n        Report percent complete and other feedback on how the db creation is\n        progressing.\n\n        In order to report percent complete, the entire file needs to be read\n        once to see how many items there are; for large files you may want to\n        use `verbose=False` to avoid this.\n\n    checklines : int\n\n        Number of lines to check the dialect.\n\n    merge_strategy : str\n        One of {merge, create_unique, error, warning, replace}.\n\n        This parameter specifies the behavior when two items have an identical\n        primary key.\n\n        Using `merge_strategy=\"merge\"`, then there will be a single entry in\n        the database, but the attributes of all features with the same primary\n        key will be merged.\n\n        Using `merge_strategy=\"create_unique\"`, then the first entry will use\n        the original primary key, but the second entry will have a unique,\n        autoincremented primary key assigned to it\n\n        Using `merge_strategy=\"error\"`, a :class:`gffutils.DuplicateID`\n        exception will be raised.  This means you will have to edit the file\n        yourself to fix the duplicated IDs.\n\n        Using `merge_strategy=\"warning\"`, a warning will be printed to the\n        logger, and the duplicate feature will be skipped.\n\n        Using `merge_strategy=\"replace\"` will replace the entire existing\n        feature with the new feature.\n\n    transform : callable\n\n        Function (or other callable object) that accepts a `Feature` object and\n        returns a (possibly modified) `Feature` object.\n\n    gtf_transcript_key, gtf_gene_key : string\n\n        Which attribute to use as the transcript ID and gene ID respectively\n        for GTF files.  Default is `transcript_id` and `gene_id` according to\n        the GTF spec.\n\n    gtf_subfeature : string\n\n        Feature type to use as a \"gene component\" when inferring gene and\n        transcript extents for GTF files.  Default is `exon` according to the\n        GTF spec.\n\n    force_gff : bool\n        If True, do not do automatic format detection -- only use GFF.\n\n    force_dialect_check : bool\n        If True, the dialect will be checkef for every feature (instead of just\n        `checklines` features).  This can be slow, but may be necessary for\n        inconsistently-formatted input files.\n\n    from_string : bool\n        If True, then treat `data` as actual data (rather than the path to\n        a file).\n\n    keep_order : bool\n\n        If True, all features returned from this instance will have the\n        order of their attributes maintained.  This can be turned on or off\n        database-wide by setting the `keep_order` attribute or with this\n        kwarg, or on a feature-by-feature basis by setting the `keep_order`\n        attribute of an individual feature.\n\n        Note that a single order of attributes will be used for all features.\n        Specifically, the order will be determined by the order of attribute\n        keys in the first `checklines` of the input data. See\n        helpers._choose_dialect for more information on this.\n\n        Default is False, since this includes a sorting step that can get\n        time-consuming for many features.\n\n    infer_gene_extent : bool\n        DEPRECATED in version 0.8.4. See `disable_infer_transcripts` and\n        `disable_infer_genes` for more granular control.\n\n    disable_infer_transcripts, disable_infer_genes : bool\n        Only used for GTF files. By default -- and according to the GTF spec --\n        we assume that there are no transcript or gene features in the file.\n        gffutils then infers the extent of each transcript based on its\n        constituent exons and infers the extent of each gene bases on its\n        constituent transcripts.\n\n        This default behavior is problematic if the input file already contains\n        transcript or gene features (like recent GENCODE GTF files for human),\n        since 1) the work to infer extents is unnecessary, and 2)\n        trying to insert an inferred feature back into the database triggers\n        gffutils' feature-merging routines, which can get time consuming.\n\n        The solution is to use `disable_infer_transcripts=True` if your GTF\n        already has transcripts in it, and/or `disable_infer_genes=True` if it\n        already has genes in it. This can result in dramatic (100x) speedup.\n\n        Prior to version 0.8.4, setting `infer_gene_extents=False` would\n        disable both transcript and gene inference simultaneously. As of\n        version 0.8.4, these argument allow more granular control.\n\n    force_merge_fields : list\n        If merge_strategy=\"merge\", then features will only be merged if their\n        non-attribute values are identical (same chrom, source, start, stop,\n        score, strand, phase).  Using `force_merge_fields`, you can override\n        this behavior to allow merges even when fields are different.  This\n        list can contain one or more of ['seqid', 'source', 'featuretype',\n        'score', 'strand', 'frame'].  The resulting merged fields will be\n        strings of comma-separated values.  Note that 'start' and 'end' are not\n        available, since these fields need to be integers.\n\n    text_factory : callable\n        Text factory to use for the sqlite3 database.  See\n        https://docs.python.org/2/library/\\\n                sqlite3.html#sqlite3.Connection.text_factory\n        for details. The default sqlite3.OptimizedUnicode will return Unicode\n        objects only for non-ASCII data, and bytestrings otherwise.\n\n    pragmas : dict\n        Dictionary of pragmas used when creating the sqlite3 database. See\n        http://www.sqlite.org/pragma.html for a list of available pragmas.  The\n        defaults are stored in constants.default_pragmas, which can be used as\n        a template for supplying a custom dictionary.\n\n    sort_attribute_values : bool\n        All features returned from the database will have their attribute\n        values sorted.  Typically this is only useful for testing, since this\n        can get time-consuming for large numbers of features.\n\n    _keep_tempfiles : bool or string\n        False by default to clean up intermediate tempfiles created during GTF\n        import.  If True, then keep these tempfile for testing or debugging.\n        If string, then keep the tempfile for testing, but also use the string\n        as the suffix fo the tempfile. This can be useful for testing in\n        parallel environments.\n\n    Returns\n    -------\n    New :class:`FeatureDB` object.\n    \"\"\"\n    _locals = locals()\n\n    # Check if any older kwargs made it in\n    deprecation_handler(kwargs)\n\n    kwargs = dict((i, _locals[i]) for i in constants._iterator_kwargs)\n\n    # First construct an iterator so that we can identify the file format.\n    # DataIterator figures out what kind of data was provided (string of lines,\n    # filename, or iterable of Features) and checks `checklines` lines to\n    # identify the dialect.\n    iterator = iterators.DataIterator(**kwargs)\n\n    kwargs.update(**_locals)\n\n    if dialect is None:\n        dialect = iterator.dialect\n\n    # However, a side-effect of this is that  if `data` was a generator, then\n    # we've just consumed `checklines` items (see\n    # iterators.BaseIterator.__init__, which calls iterators.peek).\n    #\n    # But it also chains those consumed items back onto the beginning, and the\n    # result is available as as iterator._iter.\n    #\n    # That's what we should be using now for `data:\n    kwargs['data'] = iterator._iter\n    kwargs['directives'] = iterator.directives\n\n    # Since we've already checked lines, we don't want to do it again\n    kwargs['checklines'] = 0\n\n    if force_gff or (dialect['fmt'] == 'gff3'):\n        cls = _GFFDBCreator\n        id_spec = id_spec or 'ID'\n        add_kwargs = dict(\n            id_spec=id_spec,\n        )\n\n    elif dialect['fmt'] == 'gtf':\n        cls = _GTFDBCreator\n        id_spec = id_spec or {'gene': 'gene_id', 'transcript': 'transcript_id'}\n        add_kwargs = dict(\n            transcript_key=gtf_transcript_key,\n            gene_key=gtf_gene_key,\n            subfeature=gtf_subfeature,\n            id_spec=id_spec,\n        )\n\n    kwargs.update(**add_kwargs)\n    kwargs['dialect'] = dialect\n    c = cls(**kwargs)\n\n    c.create()\n    if dbfn == ':memory:':\n        db = interface.FeatureDB(c.conn,\n                                 keep_order=keep_order,\n                                 pragmas=pragmas,\n                                 sort_attribute_values=sort_attribute_values,\n                                 text_factory=text_factory)\n    else:\n        db = interface.FeatureDB(c,\n                                 keep_order=keep_order,\n                                 pragmas=pragmas,\n                                 sort_attribute_values=sort_attribute_values,\n                                 text_factory=text_factory)\n\n    return db"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _id_handler(self, f):\n\n        # If id_spec is a string, convert to iterable for later\n        if isinstance(self.id_spec, six.string_types):\n            id_key = [self.id_spec]\n\n        elif hasattr(self.id_spec, '__call__'):\n            id_key = [self.id_spec]\n\n        # If dict, then assume it's a feature -> attribute mapping, e.g.,\n        # {'gene': 'gene_id'} for GTF\n        elif isinstance(self.id_spec, dict):\n            try:\n                id_key = self.id_spec[f.featuretype]\n                if isinstance(id_key, six.string_types):\n                    id_key = [id_key]\n\n            # Otherwise, use default auto-increment.\n            except KeyError:\n                return self._increment_featuretype_autoid(f.featuretype)\n\n        # Otherwise assume it's an iterable.\n        else:\n            id_key = self.id_spec\n\n        # Then try them in order, returning the first one that works:\n        for k in id_key:\n\n            if hasattr(k, '__call__'):\n                _id = k(f)\n                if _id:\n                    if _id.startswith('autoincrement:'):\n                        return self._increment_featuretype_autoid(_id[14:])\n                    return _id\n            else:\n                # use GFF fields rather than attributes for cases like :seqid:\n                # or :strand:\n                if (len(k) > 3) and (k[0] == ':') and (k[-1] == ':'):\n                    # No [0] here -- only attributes key/vals are forced into\n                    # lists, not standard GFF fields.\n                    return getattr(f, k[1:-1])\n                else:\n                    try:\n                        return f.attributes[k][0]\n                    except (KeyError, IndexError):\n                        pass\n        # If we get here, then default autoincrement\n        return self._increment_featuretype_autoid(f.featuretype)", "response": "This function handles the ID processing for a feature from self. iterator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_duplicate(self, idspecid, newid):\n        c = self.conn.cursor()\n        try:\n            c.execute(\n                '''\n                INSERT INTO duplicates\n                (idspecid, newid)\n                VALUES (?, ?)''',\n                (idspecid, newid))\n        except sqlite3.ProgrammingError:\n            c.execute(\n                '''\n                INSERT INTO duplicates\n                (idspecid, newid)\n                VALUES (?, ?)''',\n                (idspecid.decode(self.default_encoding),\n                 newid.decode(self.default_encoding))\n            )\n        if self.verbose == 'debug':\n            logger.debug('added id=%s; new=%s' % (idspecid, newid))\n        self.conn.commit()", "response": "Adds a duplicate ID to the duplicates table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of features that were merged with f.", "response": "def _candidate_merges(self, f):\n        \"\"\"\n        Identifies those features that originally had the same ID as `f`\n        (according to the id_spec),  but were modified because of duplicate\n        IDs.\n        \"\"\"\n        candidates = [self._get_feature(f.id)]\n        c = self.conn.cursor()\n        results = c.execute(\n            constants._SELECT + '''\n            JOIN duplicates ON\n            duplicates.newid = features.id WHERE duplicates.idspecid = ?''',\n            (f.id,)\n        )\n        for i in results:\n            candidates.append(\n                feature.Feature(dialect=self.iterator.dialect, **i))\n        return list(set(candidates))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the tables in the database.", "response": "def _init_tables(self):\n        \"\"\"\n        Table creation\n        \"\"\"\n        c = self.conn.cursor()\n        v = sqlite3.sqlite_version_info\n        self.set_pragmas(self.pragmas)\n        c.executescript(constants.SCHEMA)\n        self.conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfinalize the internal state of the database.", "response": "def _finalize(self):\n        \"\"\"\n        Various last-minute stuff to perform after file has been parsed and\n        imported.\n\n        In general, if you'll be adding stuff to the meta table, do it here.\n        \"\"\"\n        c = self.conn.cursor()\n        directives = self.directives + self.iterator.directives\n        c.executemany('''\n                      INSERT INTO directives VALUES (?)\n                      ''', ((i,) for i in directives))\n        c.execute(\n            '''\n            INSERT INTO meta (version, dialect)\n            VALUES (:version, :dialect)''',\n            dict(version=version.version,\n                 dialect=helpers._jsonify(self.iterator.dialect))\n        )\n\n        c.executemany(\n            '''\n            INSERT OR REPLACE INTO autoincrements VALUES (?, ?)\n            ''', list(self._autoincrements.items()))\n\n        # These indexes are *well* worth the effort and extra storage: over\n        # 500x speedup on code like this:\n        #\n        #   genes = []\n        #   for i in db.features_of_type('snoRNA'):\n        #       for k in db.parents(i, level=1, featuretype='gene'):\n        #           genes.append(k.id)\n        #\n        logger.info(\"Creating relations(parent) index\")\n        c.execute('DROP INDEX IF EXISTS relationsparent')\n        c.execute('CREATE INDEX relationsparent ON relations (parent)')\n        logger.info(\"Creating relations(child) index\")\n        c.execute('DROP INDEX IF EXISTS relationschild')\n        c.execute('CREATE INDEX relationschild ON relations (child)')\n        logger.info(\"Creating features(featuretype) index\")\n        c.execute('DROP INDEX IF EXISTS featuretype')\n        c.execute('CREATE INDEX featuretype ON features (featuretype)')\n        logger.info(\"Creating features (seqid, start, end) index\")\n        c.execute('DROP INDEX IF EXISTS seqidstartend')\n        c.execute('CREATE INDEX seqidstartend ON features (seqid, start, end)')\n        logger.info(\"Creating features (seqid, start, end, strand) index\")\n        c.execute('DROP INDEX IF EXISTS seqidstartendstrand')\n        c.execute('CREATE INDEX seqidstartendstrand ON features (seqid, start, end, strand)')\n\n        # speeds computation 1000x in some cases\n        logger.info(\"Running ANALYSE features\")\n        c.execute('ANALYZE features')\n\n        self.conn.commit()\n\n        self.warnings = self.iterator.warnings"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self):\n        # Calls each of these methods in order.  _populate_from_lines and\n        # _update_relations must be implemented in subclasses.\n        self._init_tables()\n        self._populate_from_lines(self.iterator)\n        self._update_relations()\n        self._finalize()", "response": "Calls various methods sequentially in order to fully build the\n        database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a query directly on the database.", "response": "def execute(self, query):\n        \"\"\"\n        Execute a query directly on the database.\n        \"\"\"\n        c = self.conn.cursor()\n        result = c.execute(query)\n        for i in result:\n            yield i"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace a feature in the database.", "response": "def _replace(self, feature, cursor):\n        \"\"\"\n        Insert a feature into the database.\n        \"\"\"\n        try:\n            cursor.execute(\n                constants._UPDATE,\n                list(feature.astuple()) + [feature.id])\n        except sqlite3.ProgrammingError:\n            cursor.execute(\n                constants._INSERT,\n                list(feature.astuple(self.default_encoding)) + [feature.id])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_for_js(function):\n\n    @functools.wraps(function)\n    def wrapper(*args, **kwargs):  # pylint: disable=missing-docstring\n\n        # If not a method, then just call the function\n        if len(args) < 1:\n            return function(*args, **kwargs)\n\n        # Otherwise, retrieve `self` as the first arg\n        else:\n            self = args[0]\n\n            # If the class has been decorated by one of the\n            # JavaScript dependency decorators, it should have\n            # a `wait_for_js` method\n            if hasattr(self, 'wait_for_js'):\n                self.wait_for_js()\n\n            # Call the function\n            return function(*args, **kwargs)\n\n    return wrapper", "response": "Decorator that waits for JavaScript dependencies before executing the function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _decorator(store_name, store_values):\n    def decorator(clz):  # pylint: disable=missing-docstring\n\n        # Add a `wait_for_js` method to the class\n        if not hasattr(clz, 'wait_for_js'):\n            setattr(clz, 'wait_for_js', _wait_for_js)  # pylint: disable= literal-used-as-attribute\n\n        # Store the RequireJS module names in the class\n        if not hasattr(clz, store_name):\n            setattr(clz, store_name, set())\n\n        getattr(clz, store_name).update(store_values)\n        return clz\n\n    return decorator", "response": "Returns a class decorator that creates a new class list variable and stores the RequireJS module names in the class list variable store_name and store_values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclassing method added by the decorators to allow decorated classes to manually re-check JavaScript dependencies. Expect that `self` is a class that: 1) Has been decorated with either `js_defined` or `requirejs` 2) Has a `browser` property If either (1) or (2) is not satisfied, then do nothing.", "response": "def _wait_for_js(self):\n    \"\"\"\n    Class method added by the decorators to allow\n    decorated classes to manually re-check JavaScript\n    dependencies.\n\n    Expect that `self` is a class that:\n    1) Has been decorated with either `js_defined` or `requirejs`\n    2) Has a `browser` property\n\n    If either (1) or (2) is not satisfied, then do nothing.\n    \"\"\"\n\n    # No Selenium browser available, so return without doing anything\n    if not hasattr(self, 'browser'):\n        return\n\n    # pylint: disable=protected-access\n    # Wait for JavaScript variables to be defined\n    if hasattr(self, '_js_vars') and self._js_vars:\n        EmptyPromise(\n            lambda: _are_js_vars_defined(self.browser, self._js_vars),\n            u\"JavaScript variables defined: {0}\".format(\", \".join(self._js_vars))\n        ).fulfill()\n\n    # Wait for RequireJS dependencies to load\n    if hasattr(self, '_requirejs_deps') and self._requirejs_deps:\n        EmptyPromise(\n            lambda: _are_requirejs_deps_loaded(self.browser, self._requirejs_deps),\n            u\"RequireJS dependencies loaded: {0}\".format(\", \".join(self._requirejs_deps)),\n            try_limit=5\n        ).fulfill()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _are_js_vars_defined(browser, js_vars):\n    # This script will evaluate to True iff all of\n    # the required vars are defined.\n    script = u\" && \".join([\n        u\"!(typeof {0} === 'undefined')\".format(var)\n        for var in js_vars\n    ])\n\n    try:\n        return browser.execute_script(u\"return {}\".format(script))\n    except WebDriverException as exc:\n        if \"is not defined\" in exc.msg or \"is undefined\" in exc.msg:\n            return False\n        else:\n            raise", "response": "Returns a boolean indicating whether all the JavaScript\n    variables js_vars are defined on the current page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _are_requirejs_deps_loaded(browser, deps):\n\n    # This is a little complicated\n    #\n    # We're going to use `execute_async_script` to give control to\n    # the browser.  The browser indicates that it wants to return\n    # control to us by calling `callback`, which is the last item\n    # in the global `arguments` array.\n    #\n    # We install a RequireJS module with the dependencies we want\n    # to ensure are loaded.  When our module loads, we return\n    # control to the test suite.\n    script = dedent(u\"\"\"\n        // Retrieve the callback function used to return control to the test suite\n        var callback = arguments[arguments.length - 1];\n\n        // If RequireJS isn't defined, then return immediately\n        if (!window.require) {{\n            callback(\"RequireJS not defined\");\n        }}\n\n        // Otherwise, install a RequireJS module that depends on the modules\n        // we're waiting for.\n        else {{\n\n            // Catch errors reported by RequireJS\n            requirejs.onError = callback;\n\n            // Install our module\n            require({deps}, function() {{\n                callback('Success');\n            }});\n        }}\n    \"\"\").format(deps=json.dumps(list(deps)))\n\n    # Set a timeout to ensure we get control back\n    browser.set_script_timeout(30)\n\n    # Give control to the browser\n    # `result` will be the argument passed to the callback function\n    try:\n        result = browser.execute_async_script(script)\n        return result == 'Success'\n\n    except TimeoutException:\n        return False", "response": "Returns a boolean indicating whether all the RequireJS dependencies deps have loaded on the current page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pre_verify(method):\n    @wraps(method)\n    def wrapper(self, *args, **kwargs):  # pylint: disable=missing-docstring\n        self._verify_page()  # pylint: disable=protected-access\n        return method(self, *args, **kwargs)\n    return wrapper", "response": "Decorator that calls self. _verify_page before executing the decorated method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the rules to be run or ignored for the audit.", "response": "def set_rules(self, rules):\n        \"\"\"\n        Sets the rules to be run or ignored for the audit.\n\n        Args:\n\n            rules: a dictionary of the format `{\"ignore\": [], \"apply\": []}`.\n\n        See https://github.com/GoogleChrome/accessibility-developer-tools/tree/master/src/audits\n\n        Passing `{\"apply\": []}` or `{}` means to check for all available rules.\n\n        Passing `{\"apply\": None}` means that no audit should be done for this page.\n\n        Passing `{\"ignore\": []}` means to run all otherwise enabled rules.\n        Any rules in the \"ignore\" list will be ignored even if they were also\n        specified in the \"apply\".\n\n        Examples:\n\n            To check only `badAriaAttributeValue`::\n\n                page.a11y_audit.config.set_rules({\n                    \"apply\": ['badAriaAttributeValue']\n                })\n\n            To check all rules except `badAriaAttributeValue`::\n\n                page.a11y_audit.config.set_rules({\n                    \"ignore\": ['badAriaAttributeValue'],\n                })\n        \"\"\"\n        self.rules_to_ignore = rules.get(\"ignore\", [])\n        self.rules_to_run = rules.get(\"apply\", [])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_scope(self, include=None, exclude=None):\n        if include:\n            self.scope = u\"document.querySelector(\\\"{}\\\")\".format(\n                u', '.join(include)\n            )\n        else:\n            self.scope = \"null\"\n\n        if exclude is not None:\n            raise NotImplementedError(\n                \"The argument `exclude` has not been implemented in \"\n                \"AxsAuditConfig.set_scope method.\"\n            )", "response": "Sets the scope of the audit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the page for violations of the configured rules.", "response": "def _check_rules(browser, rules_js, config):\n        \"\"\"\n        Check the page for violations of the configured rules. By default,\n        all rules in the ruleset will be checked.\n\n        Args:\n            browser: a browser instance.\n            rules_js: the ruleset JavaScript as a string.\n            config: an AxsAuditConfig instance.\n\n        Returns:\n            A namedtuple with 'errors' and 'warnings' fields whose values are\n            the errors and warnings returned from the audit.\n\n            None if config has rules_to_run set to None.\n\n        __Caution__: You probably don't really want to call this method\n        directly! It will be used by `A11yAudit.do_audit` if using this ruleset.\n        \"\"\"\n        if config.rules_to_run is None:\n            msg = 'No accessibility rules were specified to check.'\n            log.warning(msg)\n            return None\n\n        # This line will only be included in the script if rules to check on\n        # this page are specified, as the default behavior of the js is to\n        # run all rules.\n        rules = config.rules_to_run\n        if rules:\n            rules_config = u\"auditConfig.auditRulesToRun = {rules};\".format(\n                rules=rules)\n        else:\n            rules_config = \"\"\n\n        ignored_rules = config.rules_to_ignore\n        if ignored_rules:\n            rules_config += (\n                u\"\\nauditConfig.auditRulesToIgnore = {rules};\".format(\n                    rules=ignored_rules\n                )\n            )\n\n        script = dedent(u\"\"\"\n            {rules_js}\n            var auditConfig = new axs.AuditConfiguration();\n            {rules_config}\n            auditConfig.scope = {scope};\n            var run_results = axs.Audit.run(auditConfig);\n            var audit_results = axs.Audit.auditResults(run_results)\n            return audit_results;\n        \"\"\".format(rules_js=rules_js, rules_config=rules_config, scope=config.scope))\n\n        result = browser.execute_script(script)\n\n        # audit_results is report of accessibility errors for that session\n        audit_results = AuditResults(\n            errors=result.get('errors_'),\n            warnings=result.get('warnings_')\n        )\n        return audit_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of audit errors.", "response": "def get_errors(audit_results):\n        \"\"\"\n        Args:\n\n            audit_results: results of `AxsAudit.do_audit()`.\n\n        Returns: a list of errors.\n        \"\"\"\n        errors = []\n        if audit_results:\n            if audit_results.errors:\n                errors.extend(audit_results.errors)\n        return errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef report_errors(audit, url):\n        errors = AxsAudit.get_errors(audit)\n        if errors:\n            msg = u\"URL '{}' has {} errors:\\n{}\".format(\n                url,\n                len(errors),\n                ', '.join(errors)\n            )\n            raise AccessibilityError(msg)", "response": "Report the errors that occurred in the audit."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the promise and return the result.", "response": "def fulfill(self):\n        \"\"\"\n        Evaluate the promise and return the result.\n\n        Returns:\n             The result of the `Promise` (second return value from the `check_func`)\n\n        Raises:\n            BrokenPromise: the `Promise` was not satisfied within the time or attempt limits.\n        \"\"\"\n        is_fulfilled, result = self._check_fulfilled()\n\n        if is_fulfilled:\n            return result\n        else:\n            raise BrokenPromise(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the promise has been fulfilled and if so run the check_func until it is satisfied.", "response": "def _check_fulfilled(self):\n        \"\"\"\n        Return tuple `(is_fulfilled, result)` where\n        `is_fulfilled` is a boolean indicating whether the promise has been fulfilled\n        and `result` is the value to pass to the `with` block.\n        \"\"\"\n        is_fulfilled = False\n        result = None\n        start_time = time.time()\n\n        # Check whether the promise has been fulfilled until we run out of time or attempts\n        while self._has_time_left(start_time) and self._has_more_tries():\n\n            # Keep track of how many attempts we've made so far\n            self._num_tries += 1\n\n            is_fulfilled, result = self._check_func()\n\n            # If the promise is satisfied, then continue execution\n            if is_fulfilled:\n                break\n\n            # Delay between checks\n            time.sleep(self._try_interval)\n\n        return is_fulfilled, result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(self):\n        self.q(css='button.btn').click()\n        GitHubSearchResultsPage(self.browser).wait_for_page()", "response": "Click on the Search button and wait for the GitHub Search results page to be displayed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the rules for the current resource.", "response": "def set_rules(self, rules):\n        \"\"\"\n        Set rules to ignore XOR limit to when checking for accessibility\n        errors on the page.\n\n        Args:\n\n            rules: a dictionary one of the following formats.\n                If you want to run all of the rules except for some::\n\n                    {\"ignore\": []}\n\n                If you want to run only a specific set of rules::\n\n                    {\"apply\": []}\n\n                If you want to run only rules of a specific standard::\n\n                    {\"tags\": []}\n\n        Examples:\n\n            To run only \"bad-link\" and \"color-contrast\" rules::\n\n                page.a11y_audit.config.set_rules({\n                    \"apply\": [\"bad-link\", \"color-contrast\"],\n                })\n\n            To run all rules except for \"bad-link\" and \"color-contrast\"::\n\n                page.a11y_audit.config.set_rules({\n                    \"ignore\": [\"bad-link\", \"color-contrast\"],\n                })\n\n            To run only WCAG 2.0 Level A rules::\n\n                page.a11y_audit.config.set_rules({\n                    \"tags\": [\"wcag2a\"],\n                })\n\n            To run all rules:\n                page.a11y_audit.config.set_rules({})\n\n        Related documentation:\n\n            * https://github.com/dequelabs/axe-core/blob/master/doc/API.md#options-parameter-examples\n            * https://github.com/dequelabs/axe-core/doc/rule-descriptions.md\n        \"\"\"\n        options = {}\n        if rules:\n            if rules.get(\"ignore\"):\n                options[\"rules\"] = {}\n                for rule in rules.get(\"ignore\"):\n                    options[\"rules\"][rule] = {\"enabled\": False}\n            elif rules.get(\"apply\"):\n                options[\"runOnly\"] = {\n                    \"type\": \"rule\",\n                    \"values\": rules.get(\"apply\"),\n                }\n            elif rules.get(\"tags\"):\n                options[\"runOnly\"] = {\n                    \"type\": \"tag\",\n                    \"values\": rules.get(\"tags\"),\n                }\n        self.rules = json.dumps(options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_scope(self, include=None, exclude=None):\n        context = {}\n\n        if exclude:\n            context[\"exclude\"] = [[selector] for selector in exclude]\n\n        if include:\n            context[\"include\"] = [[selector] for selector in include]\n\n        self.context = json.dumps(context) if context else 'document'", "response": "Sets the scope of the audit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the ruleset to include a set of custom rules.", "response": "def customize_ruleset(self, custom_ruleset_file=None):\n        \"\"\"\n        Updates the ruleset to include a set of custom rules. These rules will\n        be _added_ to the existing ruleset or replace the existing rule with\n        the same ID.\n\n        Args:\n\n            custom_ruleset_file (optional): The filepath to the custom rules.\n                Defaults to `None`. If `custom_ruleset_file` isn't passed, the\n                environment variable `BOKCHOY_A11Y_CUSTOM_RULES_FILE` will be\n                checked. If a filepath isn't specified by either of these\n                methods, the ruleset will not be updated.\n\n        Raises:\n\n            `IOError` if the specified file does not exist.\n\n        Examples:\n\n            To include the rules defined in `axe-core-custom-rules.js`::\n\n                page.a11y_audit.config.customize_ruleset(\n                    \"axe-core-custom-rules.js\"\n                )\n\n            Alternatively, use the environment variable `BOKCHOY_A11Y_CUSTOM_RULES_FILE`\n            to specify the path to the file containing the custom rules.\n\n        Documentation for how to write rules:\n\n            https://github.com/dequelabs/axe-core/blob/master/doc/developer-guide.md\n\n        An example of a custom rules file can be found at\n        https://github.com/edx/bok-choy/tree/master/tests/a11y_custom_rules.js\n        \"\"\"\n        custom_file = custom_ruleset_file or os.environ.get(\n            \"BOKCHOY_A11Y_CUSTOM_RULES_FILE\"\n        )\n\n        if not custom_file:\n            return\n\n        with open(custom_file, \"r\") as additional_rules:\n            custom_rules = additional_rules.read()\n\n        if \"var customRules\" not in custom_rules:\n            raise A11yAuditConfigError(\n                \"Custom rules file must include \\\"var customRules\\\"\"\n            )\n\n        self.custom_rules = custom_rules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun an accessibility audit on the page using the axe-core ruleset. Args: browser: a browser instance. rules_js: the ruleset JavaScript as a string. config: an AxsAuditConfig instance. Returns: A list of violations. Related documentation: https://github.com/dequelabs/axe-core/blob/master/doc/API.md#results-object __Caution__: You probably don't really want to call this method directly! It will be used by `AxeCoreAudit.do_audit`.", "response": "def _check_rules(browser, rules_js, config):\n        \"\"\"\n        Run an accessibility audit on the page using the axe-core ruleset.\n\n        Args:\n            browser: a browser instance.\n            rules_js: the ruleset JavaScript as a string.\n            config: an AxsAuditConfig instance.\n\n        Returns:\n            A list of violations.\n\n        Related documentation:\n\n            https://github.com/dequelabs/axe-core/blob/master/doc/API.md#results-object\n\n        __Caution__: You probably don't really want to call this method\n        directly! It will be used by `AxeCoreAudit.do_audit`.\n        \"\"\"\n        audit_run_script = dedent(u\"\"\"\n            {rules_js}\n            {custom_rules}\n            axe.configure(customRules);\n            var callback = function(err, results) {{\n                if (err) throw err;\n                window.a11yAuditResults = JSON.stringify(results);\n                window.console.log(window.a11yAuditResults);\n            }}\n            axe.run({context}, {options}, callback);\n        \"\"\").format(\n            rules_js=rules_js,\n            custom_rules=config.custom_rules,\n            context=config.context,\n            options=config.rules\n        )\n\n        audit_results_script = dedent(u\"\"\"\n            window.console.log(window.a11yAuditResults);\n            return window.a11yAuditResults;\n        \"\"\")\n\n        browser.execute_script(audit_run_script)\n\n        def audit_results_check_func():\n            \"\"\"\n            A method to check that the audit has completed.\n\n            Returns:\n\n                (True, results) if the results are available.\n                (False, None) if the results aren't available.\n            \"\"\"\n\n            unicode_results = browser.execute_script(audit_results_script)\n\n            try:\n                results = json.loads(unicode_results)\n            except (TypeError, ValueError):\n                results = None\n\n            if results:\n                return True, results\n            return False, None\n\n        result = Promise(\n            audit_results_check_func,\n            \"Timed out waiting for a11y audit results.\",\n            timeout=5,\n        ).fulfill()\n\n        # audit_results is report of accessibility violations for that session\n        # Note that this ruleset doesn't have distinct error/warning levels.\n        audit_results = result.get('violations')\n        return audit_results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary with keys errors and total.", "response": "def get_errors(audit_results):\n        \"\"\"\n        Args:\n\n            audit_results: results of `AxeCoreAudit.do_audit()`.\n\n        Returns:\n\n            A dictionary with keys \"errors\" and \"total\".\n        \"\"\"\n        errors = {\"errors\": [], \"total\": 0}\n        if audit_results:\n            errors[\"errors\"].extend(audit_results)\n            for i in audit_results:\n                for _node in i[\"nodes\"]:\n                    errors[\"total\"] += 1\n        return errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the errors and returns a string that can be used to display in the error output.", "response": "def format_errors(errors):\n        \"\"\"\n        Args:\n\n            errors: results of `AxeCoreAudit.get_errors()`.\n\n        Returns: The errors as a formatted string.\n        \"\"\"\n\n        def _get_message(node):\n            \"\"\"\n            Get the message to display in the error output.\n            \"\"\"\n            messages = set()\n\n            try:\n                messages.update([node['message']])\n            except KeyError:\n                pass\n\n            for check_group in ['any', 'all', 'none']:\n                try:\n                    for check in node[check_group]:\n                        messages.update([check.get('message')])\n                except KeyError:\n                    pass\n\n            messages = messages.difference([''])\n            return '; '.join(messages)\n\n        lines = []\n        for error_type in errors:\n            lines.append(u\"Severity: {}\".format(error_type.get(\"impact\")))\n            lines.append(u\"Rule ID: {}\".format(error_type.get(\"id\")))\n            lines.append(u\"Help URL: {}\\n\".format(error_type.get('helpUrl')))\n\n            for node in error_type['nodes']:\n                msg = u\"Message: {}\".format(_get_message(node))\n                html = u\"Html: {}\".format(node.get('html').encode('utf-8'))\n                target = u\"Target: {}\".format(node.get('target'))\n                fill_opts = {\n                    'width': 100,\n                    'initial_indent': '\\t',\n                    'subsequent_indent': '\\t\\t',\n                }\n                lines.append(fill(msg, **fill_opts))\n                lines.append(fill(html, **fill_opts))\n                lines.append(fill(target, **fill_opts))\n                lines.append('\\n')\n\n        return '\\n'.join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreport the errors that occurred in the audit.", "response": "def report_errors(audit, url):\n        \"\"\"\n        Args:\n\n            audit: results of `AxeCoreAudit.do_audit()`.\n            url: the url of the page being audited.\n\n        Raises: `AccessibilityError`\n        \"\"\"\n        errors = AxeCoreAudit.get_errors(audit)\n        if errors[\"total\"] > 0:\n            msg = u\"URL '{}' has {} errors:\\n\\n{}\".format(\n                url,\n                errors[\"total\"],\n                AxeCoreAudit.format_errors(errors[\"errors\"])\n            )\n            raise AccessibilityError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_source(driver, name):\n    source = driver.page_source\n    file_name = os.path.join(os.environ.get('SAVED_SOURCE_DIR'),\n                             '{name}.html'.format(name=name))\n\n    try:\n        with open(file_name, 'wb') as output_file:\n            output_file.write(source.encode('utf-8'))\n    except Exception:  # pylint: disable=broad-except\n        msg = u\"Could not save the browser page source to {}.\".format(file_name)\n        LOGGER.warning(msg)", "response": "Saves the rendered HTML of the browser to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_screenshot(driver, name):\n    if hasattr(driver, 'save_screenshot'):\n        screenshot_dir = os.environ.get('SCREENSHOT_DIR')\n        if not screenshot_dir:\n            LOGGER.warning('The SCREENSHOT_DIR environment variable was not set; not saving a screenshot')\n            return\n        elif not os.path.exists(screenshot_dir):\n            os.makedirs(screenshot_dir)\n        image_name = os.path.join(screenshot_dir, name + '.png')\n        driver.save_screenshot(image_name)\n\n    else:\n        msg = (\n            u\"Browser does not support screenshots. \"\n            u\"Could not save screenshot '{name}'\"\n        ).format(name=name)\n\n        LOGGER.warning(msg)", "response": "Save a screenshot of the browser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the logs of the driver to the log directory.", "response": "def save_driver_logs(driver, prefix):\n    \"\"\"\n    Save the selenium driver logs.\n\n    The location of the driver log files can be configured\n    by the environment variable `SELENIUM_DRIVER_LOG_DIR`.  If not set,\n    this defaults to the current working directory.\n\n    Args:\n        driver (selenium.webdriver): The Selenium-controlled browser.\n        prefix (str): A prefix which will be used in the output file names for the logs.\n\n    Returns:\n        None\n    \"\"\"\n    browser_name = os.environ.get('SELENIUM_BROWSER', 'firefox')\n    log_dir = os.environ.get('SELENIUM_DRIVER_LOG_DIR')\n    if not log_dir:\n        LOGGER.warning('The SELENIUM_DRIVER_LOG_DIR environment variable was not set; not saving logs')\n        return\n    elif not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    if browser_name == 'firefox':\n        # Firefox doesn't yet provide logs to Selenium, but does log to a separate file\n        # https://github.com/mozilla/geckodriver/issues/284\n        # https://firefox-source-docs.mozilla.org/testing/geckodriver/geckodriver/TraceLogs.html\n        log_path = os.path.join(os.getcwd(), 'geckodriver.log')\n        if os.path.exists(log_path):\n            dest_path = os.path.join(log_dir, '{}_geckodriver.log'.format(prefix))\n            copyfile(log_path, dest_path)\n        return\n\n    log_types = driver.log_types\n    for log_type in log_types:\n        try:\n            log = driver.get_log(log_type)\n            file_name = os.path.join(\n                log_dir, '{}_{}.log'.format(prefix, log_type)\n            )\n            with open(file_name, 'w') as output_file:\n                for line in log:\n                    output_file.write(\"{}{}\".format(dumps(line), '\\n'))\n        except:  # pylint: disable=bare-except\n            msg = (\n                u\"Could not save browser log of type '{log_type}'. \"\n                u\"It may be that the browser does not support it.\"\n            ).format(log_type=log_type)\n\n            LOGGER.warning(msg, exc_info=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef browser(tags=None, proxy=None, other_caps=None):\n\n    browser_name = os.environ.get('SELENIUM_BROWSER', 'firefox')\n\n    def browser_check_func():\n        \"\"\" Instantiate the browser and return the browser instance \"\"\"\n        # See https://openedx.atlassian.net/browse/TE-701\n        try:\n            # Get the class and kwargs required to instantiate the browser based on\n            # whether we are using a local or remote one.\n            if _use_remote_browser(SAUCE_ENV_VARS):\n                browser_class, browser_args, browser_kwargs = _remote_browser_class(\n                    SAUCE_ENV_VARS, tags)\n            elif _use_remote_browser(REMOTE_ENV_VARS):\n                browser_class, browser_args, browser_kwargs = _remote_browser_class(\n                    REMOTE_ENV_VARS, tags)\n            else:\n                browser_class, browser_args, browser_kwargs = _local_browser_class(\n                    browser_name)\n\n            # If we are using a proxy, we need extra kwargs passed on intantiation.\n            if proxy:\n                browser_kwargs = _proxy_kwargs(browser_name, proxy, browser_kwargs)\n\n            # Load in user given desired caps but override with derived caps from above. This is to retain existing\n            # behavior. Only for remote drivers, where various testing services use this info for configuration.\n            if browser_class == webdriver.Remote:\n                desired_caps = other_caps or {}\n                desired_caps.update(browser_kwargs.get('desired_capabilities', {}))\n                browser_kwargs['desired_capabilities'] = desired_caps\n\n            return True, browser_class(*browser_args, **browser_kwargs)\n\n        except (socket.error, WebDriverException) as err:\n            msg = str(err)\n            LOGGER.debug('Failed to instantiate browser: ' + msg)\n            return False, None\n\n    browser_instance = Promise(\n        # There are cases where selenium takes 30s to return with a failure, so in order to try 3\n        # times, we set a long timeout. If there is a hang on the first try, the timeout will\n        # be enforced.\n        browser_check_func, \"Browser is instantiated successfully.\", try_limit=3, timeout=95).fulfill()\n\n    return browser_instance", "response": "Configure the Selenium browser."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _firefox_profile():\n    profile_dir = os.environ.get(FIREFOX_PROFILE_ENV_VAR)\n\n    if profile_dir:\n        LOGGER.info(u\"Using firefox profile: %s\", profile_dir)\n        try:\n            firefox_profile = webdriver.FirefoxProfile(profile_dir)\n        except OSError as err:\n            if err.errno == errno.ENOENT:\n                raise BrowserConfigError(\n                    u\"Firefox profile directory {env_var}={profile_dir} does not exist\".format(\n                        env_var=FIREFOX_PROFILE_ENV_VAR, profile_dir=profile_dir))\n            elif err.errno == errno.EACCES:\n                raise BrowserConfigError(\n                    u\"Firefox profile directory {env_var}={profile_dir} has incorrect permissions. It must be \\\n                    readable and executable.\".format(env_var=FIREFOX_PROFILE_ENV_VAR, profile_dir=profile_dir))\n            else:\n                # Some other OSError:\n                raise BrowserConfigError(\n                    u\"Problem with firefox profile directory {env_var}={profile_dir}: {msg}\"\n                    .format(env_var=FIREFOX_PROFILE_ENV_VAR, profile_dir=profile_dir, msg=str(err)))\n    else:\n        LOGGER.info(\"Using default firefox profile\")\n        firefox_profile = webdriver.FirefoxProfile()\n\n        # Bypasses the security prompt displayed by the browser when it attempts to\n        # access a media device (e.g., a webcam)\n        firefox_profile.set_preference('media.navigator.permission.disabled', True)\n\n        # Disable the initial url fetch to 'learn more' from mozilla (so you don't have to\n        # be online to run bok-choy on firefox)\n        firefox_profile.set_preference('browser.startup.homepage', 'about:blank')\n        firefox_profile.set_preference('startup.homepage_welcome_url', 'about:blank')\n        firefox_profile.set_preference('startup.homepage_welcome_url.additional', 'about:blank')\n\n        # Disable fetching an updated version of firefox\n        firefox_profile.set_preference('app.update.enabled', False)\n\n        # Disable plugin checking\n        firefox_profile.set_preference('plugins.hide_infobar_for_outdated_plugin', True)\n\n        # Disable health reporter\n        firefox_profile.set_preference('datareporting.healthreport.service.enabled', False)\n\n        # Disable all data upload (Telemetry and FHR)\n        firefox_profile.set_preference('datareporting.policy.dataSubmissionEnabled', False)\n\n        # Disable crash reporter\n        firefox_profile.set_preference('toolkit.crashreporter.enabled', False)\n\n        # Disable the JSON Viewer\n        firefox_profile.set_preference('devtools.jsonview.enabled', False)\n\n        # Grant OS focus to the launched browser so focus-related tests function correctly\n        firefox_profile.set_preference('focusmanager.testmode', True)\n    for function in FIREFOX_PROFILE_CUSTOMIZERS:\n        function(firefox_profile)\n    return firefox_profile", "response": "Configure the Firefox profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns class kwargs and args needed to instantiate the local browser.", "response": "def _local_browser_class(browser_name):\n    \"\"\"\n    Returns class, kwargs, and args needed to instantiate the local browser.\n    \"\"\"\n\n    # Log name of local browser\n    LOGGER.info(u\"Using local browser: %s [Default is firefox]\", browser_name)\n\n    # Get class of local browser based on name\n    browser_class = BROWSERS.get(browser_name)\n    headless = os.environ.get('BOKCHOY_HEADLESS', 'false').lower() == 'true'\n    if browser_class is None:\n        raise BrowserConfigError(\n            u\"Invalid browser name {name}.  Options are: {options}\".format(\n                name=browser_name, options=\", \".join(list(BROWSERS.keys()))))\n    else:\n        if browser_name == 'firefox':\n            # Remove geckodriver log data from previous test cases\n            log_path = os.path.join(os.getcwd(), 'geckodriver.log')\n            if os.path.exists(log_path):\n                os.remove(log_path)\n\n            firefox_options = FirefoxOptions()\n            firefox_options.log.level = 'trace'\n            if headless:\n                firefox_options.headless = True\n            browser_args = []\n            browser_kwargs = {\n                'firefox_profile': _firefox_profile(),\n                'options': firefox_options,\n            }\n\n            firefox_path = os.environ.get('SELENIUM_FIREFOX_PATH')\n            firefox_log = os.environ.get('SELENIUM_FIREFOX_LOG')\n            if firefox_path and firefox_log:\n                browser_kwargs.update({\n                    'firefox_binary': FirefoxBinary(\n                        firefox_path=firefox_path, log_file=firefox_log)\n                })\n            elif firefox_path:\n                browser_kwargs.update({\n                    'firefox_binary': FirefoxBinary(firefox_path=firefox_path)\n                })\n            elif firefox_log:\n                browser_kwargs.update({\n                    'firefox_binary': FirefoxBinary(log_file=firefox_log)\n                })\n\n        elif browser_name == 'chrome':\n            chrome_options = ChromeOptions()\n            if headless:\n                chrome_options.headless = True\n\n            # Emulate webcam and microphone for testing purposes\n            chrome_options.add_argument('--use-fake-device-for-media-stream')\n\n            # Bypasses the security prompt displayed by the browser when it attempts to\n            # access a media device (e.g., a webcam)\n            chrome_options.add_argument('--use-fake-ui-for-media-stream')\n\n            browser_args = []\n            browser_kwargs = {\n                'options': chrome_options,\n            }\n        else:\n            browser_args, browser_kwargs = [], {}\n\n        return browser_class, browser_args, browser_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remote_browser_class(env_vars, tags=None):\n    if tags is None:\n        tags = []\n\n    # Interpret the environment variables, raising an exception if they're\n    # invalid\n    envs = _required_envs(env_vars)\n    envs.update(_optional_envs())\n\n    # Turn the environment variables into a dictionary of desired capabilities\n    caps = _capabilities_dict(envs, tags)\n\n    if 'accessKey' in caps:\n        LOGGER.info(u\"Using SauceLabs: %s %s %s\", caps['platform'], caps['browserName'], caps['version'])\n    else:\n        LOGGER.info(u\"Using Remote Browser: %s\", caps['browserName'])\n\n    # Create and return a new Browser\n    # We assume that the WebDriver end-point is running locally (e.g. using\n    # SauceConnect)\n    url = u\"http://{0}:{1}/wd/hub\".format(\n        envs['SELENIUM_HOST'], envs['SELENIUM_PORT'])\n\n    browser_args = []\n    browser_kwargs = {\n        'command_executor': url,\n        'desired_capabilities': caps,\n    }\n    if caps['browserName'] == 'firefox':\n        browser_kwargs['browser_profile'] = _firefox_profile()\n\n    return webdriver.Remote, browser_args, browser_kwargs", "response": "Create a new RemoteBrowser class based on the environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the kwargs needed to set up a proxy based on the browser type and the proxy.", "response": "def _proxy_kwargs(browser_name, proxy, browser_kwargs={}):  # pylint: disable=dangerous-default-value\n    \"\"\"\n    Determines the kwargs needed to set up a proxy based on the\n    browser type.\n\n    Returns: a dictionary of arguments needed to pass when\n        instantiating the WebDriver instance.\n    \"\"\"\n\n    proxy_dict = {\n        \"httpProxy\": proxy.proxy,\n        \"proxyType\": 'manual',\n    }\n\n    if browser_name == 'firefox' and 'desired_capabilities' not in browser_kwargs:\n        # This one works for firefox locally\n        wd_proxy = webdriver.common.proxy.Proxy(proxy_dict)\n        browser_kwargs['proxy'] = wd_proxy\n    else:\n        # This one works with chrome, both locally and remote\n        # This one works with firefox remote, but not locally\n        if 'desired_capabilities' not in browser_kwargs:\n            browser_kwargs['desired_capabilities'] = {}\n\n        browser_kwargs['desired_capabilities']['proxy'] = proxy_dict\n\n    return browser_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _required_envs(env_vars):\n    envs = {\n        key: os.environ.get(key)\n        for key in env_vars\n    }\n\n    # Check for missing keys\n    missing = [key for key, val in list(envs.items()) if val is None]\n    if missing:\n        msg = (\n            u\"These environment variables must be set: \" + u\", \".join(missing)\n        )\n        raise BrowserConfigError(msg)\n\n    # Check that we support this browser\n    if envs['SELENIUM_BROWSER'] not in BROWSERS:\n        msg = u\"Unsuppported browser: {0}\".format(envs['SELENIUM_BROWSER'])\n        raise BrowserConfigError(msg)\n\n    return envs", "response": "Parse environment variables for required values raising a BrowserConfig error if they are not found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse environment variables for optional values raising a BrowserConfig error if they are insufficiently specified.", "response": "def _optional_envs():\n    \"\"\"\n    Parse environment variables for optional values,\n    raising a `BrowserConfig` error if they are insufficiently specified.\n\n    Returns a `dict` of environment variables.\n    \"\"\"\n    envs = {\n        key: os.environ.get(key)\n        for key in OPTIONAL_ENV_VARS\n        if key in os.environ\n    }\n\n    # If we're using Jenkins, check that we have all the required info\n    if 'JOB_NAME' in envs and 'BUILD_NUMBER' not in envs:\n        raise BrowserConfigError(\"Missing BUILD_NUMBER environment var\")\n\n    if 'BUILD_NUMBER' in envs and 'JOB_NAME' not in envs:\n        raise BrowserConfigError(\"Missing JOB_NAME environment var\")\n\n    return envs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the dictionary of environment variables to a dictionary of desired capabilities to send to the Remote WebDriver.", "response": "def _capabilities_dict(envs, tags):\n    \"\"\"\n    Convert the dictionary of environment variables to\n    a dictionary of desired capabilities to send to the\n    Remote WebDriver.\n\n    `tags` is a list of string tags to apply to the SauceLabs job.\n    \"\"\"\n    capabilities = {\n        'browserName': envs['SELENIUM_BROWSER'],\n        'acceptInsecureCerts': bool(envs.get('SELENIUM_INSECURE_CERTS', False)),\n        'video-upload-on-pass': False,\n        'sauce-advisor': False,\n        'capture-html': True,\n        'record-screenshots': True,\n        'max-duration': 600,\n        'public': 'public restricted',\n        'tags': tags,\n    }\n\n    # Add SauceLabs specific environment vars if they are set.\n    if _use_remote_browser(SAUCE_ENV_VARS):\n        sauce_capabilities = {\n            'platform': envs['SELENIUM_PLATFORM'],\n            'version': envs['SELENIUM_VERSION'],\n            'username': envs['SAUCE_USER_NAME'],\n            'accessKey': envs['SAUCE_API_KEY'],\n        }\n\n        capabilities.update(sauce_capabilities)\n\n    # Optional: Add in Jenkins-specific environment variables\n    # to link Sauce output with the Jenkins job\n    if 'JOB_NAME' in envs:\n        jenkins_vars = {\n            'build': envs['BUILD_NUMBER'],\n            'name': envs['JOB_NAME'],\n        }\n\n        capabilities.update(jenkins_vars)\n\n    return capabilities"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace(self, **kwargs):\n        clone = copy(self)\n\n        clone.transforms = list(clone.transforms)\n        for key, value in kwargs.items():\n            if not hasattr(clone, key):\n                raise TypeError(u'replace() got an unexpected keyword argument {!r}'.format(key))\n\n            setattr(clone, key, value)\n        return clone", "response": "Returns a copy of this Query but with attributes specified\n        as keyword arguments replaced by the keyword values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self, transform, desc=None):\n        if desc is None:\n            desc = u'transform({})'.format(getattr(transform, '__name__', ''))\n\n        return self.replace(\n            transforms=self.transforms + [transform],\n            desc_stack=self.desc_stack + [desc]\n        )", "response": "Create a copy of this query transformed by transform."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map(self, map_fn, desc=None):\n        if desc is None:\n            desc = getattr(map_fn, '__name__', '')\n        desc = u'map({})'.format(desc)\n\n        return self.transform(lambda xs: (map_fn(x) for x in xs), desc=desc)", "response": "Returns a copy of this query with the values mapped through map_fn."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter(self, filter_fn=None, desc=None, **kwargs):\n        if filter_fn is not None and kwargs:\n            raise TypeError('Must supply either a filter_fn or attribute filter parameters to filter(), but not both.')\n        if filter_fn is None and not kwargs:\n            raise TypeError('Must supply one of filter_fn or one or more attribute filter parameters to filter().')\n\n        if desc is None:\n            if filter_fn is not None:\n                desc = getattr(filter_fn, '__name__', '')\n            elif kwargs:\n                desc = u\", \".join([u\"{}={!r}\".format(key, value) for key, value in kwargs.items()])\n        desc = u\"filter({})\".format(desc)\n\n        if kwargs:\n            def filter_fn(elem):  # pylint: disable=function-redefined, missing-docstring\n                return all(\n                    getattr(elem, filter_key) == filter_value\n                    for filter_key, filter_value\n                    in kwargs.items()\n                )\n\n        return self.transform(lambda xs: (x for x in xs if filter_fn(x)), desc=desc)", "response": "Returns a copy of this Query with some elements removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _execute(self):\n        data = self.seed_fn()\n        for transform in self.transforms:\n            data = transform(data)\n        return list(data)", "response": "Execute the query and return the results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute(self, try_limit=5, try_interval=0.5, timeout=30):\n        return Promise(\n            no_error(self._execute),\n            u\"Executing {!r}\".format(self),\n            try_limit=try_limit,\n            try_interval=try_interval,\n            timeout=timeout,\n        ).fulfill()", "response": "Execute this query and return the transformed results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef first(self):\n        def _transform(xs):  # pylint: disable=missing-docstring, invalid-name\n            try:\n                return [six.next(iter(xs))]\n            except StopIteration:\n                return []\n\n        return self.transform(_transform, 'first')", "response": "Returns a Query that selects only the first element of this Query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Query that selects the element at index.", "response": "def nth(self, index):\n        \"\"\"\n        Return a query that selects the element at `index` (starts from 0).\n        If no elements are available, returns a query with no results.\n\n        Example usage:\n\n        .. code:: python\n\n            >> q = Query(lambda: list(range(5)))\n            >> q.nth(2).results\n            [2]\n\n        Args:\n            index (int): The index of the element to select (starts from 0)\n\n        Returns:\n            Query\n        \"\"\"\n        def _transform(xs):  # pylint: disable=missing-docstring, invalid-name\n            try:\n                return [next(islice(iter(xs), index, None))]\n\n            # Gracefully handle (a) running out of elements, and (b) negative indices\n            except (StopIteration, ValueError):\n                return []\n\n        return self.transform(_transform, 'nth')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving HTML attribute values from the elements matched by the query.", "response": "def attrs(self, attribute_name):\n        \"\"\"\n        Retrieve HTML attribute values from the elements matched by the query.\n\n        Example usage:\n\n        .. code:: python\n\n            # Assume that the query matches html elements:\n            # <div class=\"foo\"> and <div class=\"bar\">\n            >> q.attrs('class')\n            ['foo', 'bar']\n\n        Args:\n            attribute_name (str): The name of the attribute values to retrieve.\n\n        Returns:\n            A list of attribute values for `attribute_name`.\n        \"\"\"\n        desc = u'attrs({!r})'.format(attribute_name)\n        return self.map(lambda el: el.get_attribute(attribute_name), desc).results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef selected(self):\n        query_results = self.map(lambda el: el.is_selected(), 'selected').results\n        if query_results:\n            return all(query_results)\n        return False", "response": "Check whether all the matched elements are selected."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visible(self):\n        query_results = self.map(lambda el: el.is_displayed(), 'visible').results\n        if query_results:\n            return all(query_results)\n        return False", "response": "Check whether all matched elements are visible."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if at least one element is focused. More specifically it checks whether the element is document. activeElement. More specifically it checks whether the element is document. activeElement.", "response": "def is_focused(self):\n        \"\"\"\n        Checks that *at least one* matched element is focused. More\n        specifically, it checks whether the element is document.activeElement.\n        If no matching element is focused, this returns `False`.\n\n        Returns:\n            bool\n        \"\"\"\n        active_el = self.browser.execute_script(\"return document.activeElement\")\n        query_results = self.map(lambda el: el == active_el, 'focused').results\n\n        if query_results:\n            return any(query_results)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the text value of each matched element to text.", "response": "def fill(self, text):\n        \"\"\"\n        Set the text value of each matched element to `text`.\n\n        Example usage:\n\n        .. code:: python\n\n            # Set the text of the first element matched by the query to \"Foo\"\n            q.first.fill('Foo')\n\n        Args:\n            text (str): The text used to fill the element (usually a text field or text area).\n\n        Returns:\n            None\n        \"\"\"\n        def _fill(elem):  # pylint: disable=missing-docstring\n            elem.clear()\n            elem.send_keys(text)\n\n        self.map(_fill, u'fill({!r})'.format(text)).execute()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef url_converter(self, *args, **kwargs):\n        upstream_converter = super(PatchedManifestStaticFilesStorage, self).url_converter(*args, **kwargs)\n\n        def converter(matchobj):\n            try:\n                upstream_converter(matchobj)\n            except ValueError:\n                # e.g. a static file 'static/media/logo.6a30f15f.svg' could not be found\n                # because the upstream converter stripped 'static/' from the path\n                matched, url = matchobj.groups()\n                return matched\n\n        return converter", "response": "Returns a custom URL converter for the given file name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prepare_headers(table, bound_columns):\n    if table.request is None:\n        return\n\n    for column in bound_columns:\n        if column.sortable:\n            params = table.request.GET.copy()\n            param_path = _with_path_prefix(table, 'order')\n            order = table.request.GET.get(param_path, None)\n            start_sort_desc = column.sort_default_desc\n            params[param_path] = column.name if not start_sort_desc else '-' + column.name\n            column.is_sorting = False\n            if order is not None:\n                is_desc = order.startswith('-')\n                order_field = order if not is_desc else order[1:]\n                if order_field == column.name:\n                    new_order = order_field if is_desc else ('-' + order_field)\n                    params[param_path] = new_order\n                    column.sort_direction = DESCENDING if is_desc else ASCENDING\n                    column.is_sorting = True\n\n            column.url = \"?\" + params.urlencode()\n        else:\n            column.is_sorting = False", "response": "Prepares the headers for the current page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef order_by_on_list(objects, order_field, is_desc=False):\n    if callable(order_field):\n        objects.sort(key=order_field, reverse=is_desc)\n        return\n\n    def order_key(x):\n        v = getattr_path(x, order_field)\n        if v is None:\n            return MIN\n        return v\n\n    objects.sort(key=order_key, reverse=is_desc)", "response": "Utility function to sort objects django - style even for non - query set collections"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of django - pre - 2. 0 context items for the given table.", "response": "def django_pre_2_0_table_context(\n        request,\n        table,\n        links=None,\n        paginate_by=None,\n        page=None,\n        extra_context=None,\n        paginator=None,\n        show_hits=False,\n        hit_label='Items'):\n    \"\"\"\n    :type table: Table\n    \"\"\"\n    if extra_context is None:  # pragma: no cover\n        extra_context = {}\n\n    assert table.data is not None\n\n    links, grouped_links = evaluate_and_group_links(links, table=table)\n\n    base_context = {\n        'links': links,\n        'grouped_links': grouped_links,\n        'table': table,\n    }\n\n    if paginate_by:\n        try:\n            paginate_by = int(request.GET.get('page_size', paginate_by))\n        except ValueError:  # pragma: no cover\n            pass\n        if paginator is None:\n            paginator = Paginator(table.data, paginate_by)\n            object_list = None\n        else:  # pragma: no cover\n            object_list = table.data\n        if not page:\n            page = request.GET.get('page', 1)\n        try:\n            page = int(page)\n            if page < 1:  # pragma: no cover\n                page = 1\n            if page > paginator.num_pages:  # pragma: no cover\n                page = paginator.num_pages\n            if object_list is None:\n                table.data = paginator.page(page).object_list\n        except (InvalidPage, ValueError):  # pragma: no cover\n            if page == 1:\n                table.data = []\n            else:\n                raise Http404\n\n        base_context.update({\n            'request': request,\n            'is_paginated': paginator.num_pages > 1,\n            'results_per_page': paginate_by,\n            'has_next': paginator.num_pages > page,\n            'has_previous': page > 1,\n            'page_size': paginate_by,\n            'page': page,\n            'next': page + 1,\n            'previous': page - 1,\n            'pages': paginator.num_pages,\n            'hits': paginator.count,\n            'show_hits': show_hits,\n            'hit_label': hit_label})\n    else:  # pragma: no cover\n        base_context.update({\n            'is_paginated': False})\n\n    base_context.update(extra_context)\n    return base_context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef table_context(request,\n                  table,\n                  links=None,\n                  paginate_by=None,\n                  page=None,\n                  extra_context=None,\n                  paginator=None,\n                  show_hits=False,\n                  hit_label='Items'):\n    \"\"\"\n    :type table: Table\n    \"\"\"\n    from django import __version__ as django_version\n    django_version = tuple([int(x) for x in django_version.split('.')])\n\n    if django_version < (2, 0):\n        return django_pre_2_0_table_context(request, table, links=links, paginate_by=paginate_by, extra_context=extra_context, paginator=paginator, show_hits=show_hits, hit_label=hit_label)\n\n    if extra_context is None:  # pragma: no cover\n        extra_context = {}\n\n    assert table.data is not None\n\n    links, grouped_links = evaluate_and_group_links(links, table=table)\n\n    base_context = {\n        'links': links,\n        'grouped_links': grouped_links,\n        'table': table,\n    }\n\n    if paginate_by:\n        try:\n            paginate_by = int(request.GET.get('page_size', paginate_by))\n        except ValueError:  # pragma: no cover\n            pass\n        if paginator is None:\n            paginator = Paginator(table.data, paginate_by)\n        if not page:\n            page = request.GET.get('page')  # None is translated to the default page in paginator.get_page\n        try:\n            page_obj = paginator.get_page(page)\n            table.data = page_obj.object_list\n        except (InvalidPage, ValueError):  # pragma: no cover\n            raise Http404\n\n        base_context.update({\n            'request': request,\n            'is_paginated': paginator.num_pages > 1,\n            'results_per_page': paginate_by,\n            'has_next': page_obj.has_next(),\n            'has_previous': page_obj.has_previous(),\n            'next': page_obj.next_page_number() if page_obj.has_next() else None,\n            'previous': page_obj.previous_page_number() if page_obj.has_previous() else None,\n            'page': page_obj.number,\n            'pages': paginator.num_pages,\n            'hits': paginator.count,\n            'show_hits': show_hits,\n            'hit_label': hit_label,\n        })\n    else:  # pragma: no cover\n        base_context.update({\n            'is_paginated': False})\n\n    base_context.update(extra_context)\n    return base_context", "response": "Returns a dictionary of context variables for a specific table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_table(request,\n                 table,\n                 links=None,\n                 context=None,\n                 template='tri_table/list.html',\n                 blank_on_empty=False,\n                 paginate_by=40,  # pragma: no mutate\n                 page=None,\n                 paginator=None,\n                 show_hits=False,\n                 hit_label='Items',\n                 post_bulk_edit=lambda table, queryset, updates: None):\n    \"\"\"\n    Render a table. This automatically handles pagination, sorting, filtering and bulk operations.\n\n    :param request: the request object. This is set on the table object so that it is available for lambda expressions.\n    :param table: an instance of Table\n    :param links: a list of instances of Link\n    :param context: dict of extra context parameters\n    :param template: if you need to render the table differently you can override this parameter with either a name of a template to load or a `Template` instance.\n    :param blank_on_empty: turn off the displaying of `{{ empty_message }}` in the template when the list is empty\n    :param show_hits: Display how many items there are total in the paginator.\n    :param hit_label: Label for the show_hits display.\n    :return: a string with the rendered HTML table\n    \"\"\"\n    if not context:\n        context = {}\n\n    if isinstance(table, Namespace):\n        table = table()\n\n    assert isinstance(table, Table), table\n    table.request = request\n\n    should_return, dispatch_result = handle_dispatch(request=request, obj=table)\n    if should_return:\n        return dispatch_result\n\n    context['bulk_form'] = table.bulk_form\n    context['query_form'] = table.query_form\n    context['tri_query_error'] = table.query_error\n\n    if table.bulk_form and request.method == 'POST':\n        if table.bulk_form.is_valid():\n            queryset = table.bulk_queryset()\n\n            updates = {\n                field.name: field.value\n                for field in table.bulk_form.fields\n                if field.value is not None and field.value != '' and field.attr is not None\n            }\n            queryset.update(**updates)\n\n            post_bulk_edit(table=table, queryset=queryset, updates=updates)\n\n            return HttpResponseRedirect(request.META['HTTP_REFERER'])\n\n    table.context = table_context(\n        request,\n        table=table,\n        links=links,\n        paginate_by=paginate_by,\n        page=page,\n        extra_context=context,\n        paginator=paginator,\n        show_hits=show_hits,\n        hit_label=hit_label,\n    )\n\n    if not table.data and blank_on_empty:\n        return ''\n\n    if table.query_form and not table.query_form.is_valid():\n        table.data = None\n        table.context['invalid_form_message'] = mark_safe('<i class=\"fa fa-meh-o fa-5x\" aria-hidden=\"true\"></i>')\n\n    return render_template(request, template, table.context)", "response": "Render a table. This automatically handles pagination, sorting, filtering and bulk operations.\n\n    :param request: the request object. This is set on the table object so that it is available for lambda expressions.\n    :param table: an instance of Table\n    :param links: a list of instances of Link\n    :param context: dict of extra context parameters\n    :param template: if you need to render the table differently you can override this parameter with either a name of a template to load or a `Template` instance.\n    :param blank_on_empty: turn off the displaying of `{{ empty_message }}` in the template when the list is empty\n    :param show_hits: Display how many items there are total in the paginator.\n    :param hit_label: Label for the show_hits display.\n    :return: a string with the rendered HTML table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_duid(mac):\n    valid = mac and isinstance(mac, six.string_types)\n    if not valid:\n        raise ValueError(\"Invalid argument was passed\")\n    return \"00:\" + mac[9:] + \":\" + mac", "response": "Generate DUID from MAC address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef try_value_to_bool(value, strict_mode=True):\n    if strict_mode:\n        true_list = ('True',)\n        false_list = ('False',)\n        val = value\n    else:\n        true_list = ('true', 'on', 'yes')\n        false_list = ('false', 'off', 'no')\n        val = str(value).lower()\n\n    if val in true_list:\n        return True\n    elif val in false_list:\n        return False\n    return value", "response": "Tries to convert value into boolean."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates NIOS Network and prepare DHCP options.", "response": "def create_network(self, net_view_name, cidr, nameservers=None,\n                       members=None, gateway_ip=None, dhcp_trel_ip=None,\n                       network_extattrs=None):\n        \"\"\"Create NIOS Network and prepare DHCP options.\n\n        Some DHCP options are valid for IPv4 only, so just skip processing\n        them for IPv6 case.\n\n        :param net_view_name: network view name\n        :param cidr: network to allocate, example '172.23.23.0/24'\n        :param nameservers: list of name servers hosts/ip\n        :param members: list of objects.AnyMember objects that are expected\n            to serve dhcp for created network\n        :param gateway_ip: gateway ip for the network (valid for IPv4 only)\n        :param dhcp_trel_ip: ip address of dhcp relay (valid for IPv4 only)\n        :param network_extattrs: extensible attributes for network (instance of\n            objects.EA)\n        :returns: created network (instance of objects.Network)\n        \"\"\"\n        ipv4 = ib_utils.determine_ip_version(cidr) == 4\n\n        options = []\n        if nameservers:\n            options.append(obj.DhcpOption(name='domain-name-servers',\n                                          value=\",\".join(nameservers)))\n        if ipv4 and gateway_ip:\n            options.append(obj.DhcpOption(name='routers',\n                                          value=gateway_ip))\n        if ipv4 and dhcp_trel_ip:\n            options.append(obj.DhcpOption(name='dhcp-server-identifier',\n                                          num=54,\n                                          value=dhcp_trel_ip))\n        return obj.Network.create(self.connector,\n                                  network_view=net_view_name,\n                                  cidr=cidr,\n                                  members=members,\n                                  options=options,\n                                  extattrs=network_extattrs,\n                                  check_if_exists=False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_ip_range(self, network_view, start_ip, end_ip, network,\n                        disable, range_extattrs):\n        \"\"\"Creates IPRange or fails if already exists.\"\"\"\n        return obj.IPRange.create(self.connector,\n                                  network_view=network_view,\n                                  start_addr=start_ip,\n                                  end_addr=end_ip,\n                                  cidr=network,\n                                  disable=disable,\n                                  extattrs=range_extattrs,\n                                  check_if_exists=False)", "response": "Creates an IPRange object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef network_exists(self, network_view, cidr):\n        LOG.warning(\n            \"DEPRECATION WARNING! Using network_exists() is deprecated \"\n            \"and to be removed in next releases. \"\n            \"Use get_network() or objects.Network.search instead\")\n        network = obj.Network.search(self.connector,\n                                     network_view=network_view,\n                                     cidr=cidr)\n        return network is not None", "response": "Deprecated use get_network or objects. Network. search instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_objects_associated_with_a_record(self, name, view, delete_list):\n        search_objects = {}\n        if 'record:cname' in delete_list:\n            search_objects['record:cname'] = 'canonical'\n        if 'record:txt' in delete_list:\n            search_objects['record:txt'] = 'name'\n\n        if not search_objects:\n            return\n\n        for obj_type, search_type in search_objects.items():\n            payload = {'view': view,\n                       search_type: name}\n            ib_objs = self.connector.get_object(obj_type, payload)\n            if ib_objs:\n                for ib_obj in ib_objs:\n                    self.delete_object_by_ref(ib_obj['_ref'])", "response": "Deletes records associated with record a or record aaaa."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_options(self, options):\n        attributes = ('host', 'wapi_version', 'username', 'password',\n                      'ssl_verify', 'http_request_timeout', 'max_retries',\n                      'http_pool_connections', 'http_pool_maxsize',\n                      'silent_ssl_warnings', 'log_api_calls_as_info',\n                      'max_results', 'paging')\n        for attr in attributes:\n            if isinstance(options, dict) and attr in options:\n                setattr(self, attr, options[attr])\n            elif hasattr(options, attr):\n                value = getattr(options, attr)\n                setattr(self, attr, value)\n            elif attr in self.DEFAULT_OPTIONS:\n                setattr(self, attr, self.DEFAULT_OPTIONS[attr])\n            else:\n                msg = \"WAPI config error. Option %s is not defined\" % attr\n                raise ib_ex.InfobloxConfigException(msg=msg)\n\n        for attr in ('host', 'username', 'password'):\n            if not getattr(self, attr):\n                msg = \"WAPI config error. Option %s can not be blank\" % attr\n                raise ib_ex.InfobloxConfigException(msg=msg)\n\n        self.wapi_url = \"https://%s/wapi/v%s/\" % (self.host,\n                                                  self.wapi_version)\n        self.cloud_api_enabled = self.is_cloud_wapi(self.wapi_version)", "response": "Copy needed options to self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to parse reply from NIOS.", "response": "def _parse_reply(request):\n        \"\"\"Tries to parse reply from NIOS.\n\n        Raises exception with content if reply is not in json format\n        \"\"\"\n        try:\n            return jsonutils.loads(request.content)\n        except ValueError:\n            raise ib_ex.InfobloxConnectionError(reason=request.content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a list of Infoblox objects of type 'obj_type' Some get requests like 'ipv4address' should be always proxied to GM on Hellfire If request is cloud and proxy is not forced yet, then plan to do 2 request: - the first one is not proxied to GM - the second is proxied to GM Args: obj_type (str): Infoblox object type, e.g. 'network', 'range', etc. payload (dict): Payload with data to send return_fields (list): List of fields to be returned extattrs (dict): List of Extensible Attributes force_proxy (bool): Set _proxy_search flag to process requests on GM max_results (int): Maximum number of objects to be returned. If set to a negative number the appliance will return an error when the number of returned objects would exceed the setting. The default is -1000. If this is set to a positive number, the results will be truncated when necessary. paging (bool): Enables paging to wapi calls if paging = True, it uses _max_results to set paging size of the wapi calls. If _max_results is negative it will take paging size as 1000. Returns: A list of the Infoblox objects requested Raises: InfobloxObjectNotFound", "response": "def get_object(self, obj_type, payload=None, return_fields=None,\n                   extattrs=None, force_proxy=False, max_results=None,\n                   paging=False):\n        \"\"\"Retrieve a list of Infoblox objects of type 'obj_type'\n\n        Some get requests like 'ipv4address' should be always\n        proxied to GM on Hellfire\n        If request is cloud and proxy is not forced yet,\n        then plan to do 2 request:\n        - the first one is not proxied to GM\n        - the second is proxied to GM\n\n        Args:\n            obj_type  (str): Infoblox object type, e.g. 'network',\n                            'range', etc.\n            payload (dict): Payload with data to send\n            return_fields (list): List of fields to be returned\n            extattrs      (dict): List of Extensible Attributes\n            force_proxy   (bool): Set _proxy_search flag\n                                  to process requests on GM\n            max_results   (int): Maximum number of objects to be returned.\n                If set to a negative number the appliance will return an error\n                when the number of returned objects would exceed the setting.\n                The default is -1000. If this is set to a positive number,\n                the results will be truncated when necessary.\n            paging    (bool): Enables paging to wapi calls if paging = True,\n                it uses _max_results to set paging size of the wapi calls.\n                If _max_results is negative it will take paging size as 1000.\n\n        Returns:\n            A list of the Infoblox objects requested\n        Raises:\n            InfobloxObjectNotFound\n        \"\"\"\n        self._validate_obj_type_or_die(obj_type, obj_type_expected=False)\n\n        # max_results passed to get_object has priority over\n        # one defined as connector option\n        if max_results is None and self.max_results:\n            max_results = self.max_results\n\n        if paging is False and self.paging:\n            paging = self.paging\n\n        query_params = self._build_query_params(payload=payload,\n                                                return_fields=return_fields,\n                                                max_results=max_results,\n                                                paging=paging)\n        # Clear proxy flag if wapi version is too old (non-cloud)\n        proxy_flag = self.cloud_api_enabled and force_proxy\n        ib_object = self._handle_get_object(obj_type, query_params, extattrs,\n                                            proxy_flag)\n        if ib_object:\n            return ib_object\n\n        # Do second get call with force_proxy if not done yet\n        if self.cloud_api_enabled and not force_proxy:\n            ib_object = self._handle_get_object(obj_type, query_params,\n                                                extattrs, proxy_flag=True)\n            if ib_object:\n                return ib_object\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an Infoblox object of type obj_type with payload payload. Returns the Infoblox object reference.", "response": "def create_object(self, obj_type, payload, return_fields=None):\n        \"\"\"Create an Infoblox object of type 'obj_type'\n\n        Args:\n            obj_type        (str): Infoblox object type,\n                                  e.g. 'network', 'range', etc.\n            payload       (dict): Payload with data to send\n            return_fields (list): List of fields to be returned\n        Returns:\n            The object reference of the newly create object\n        Raises:\n            InfobloxException\n        \"\"\"\n        self._validate_obj_type_or_die(obj_type)\n\n        query_params = self._build_query_params(return_fields=return_fields)\n\n        url = self._construct_url(obj_type, query_params)\n        opts = self._get_request_options(data=payload)\n        self._log_request('post', url, opts)\n        if(self.session.cookies):\n            # the first 'get' or 'post' action will generate a cookie\n            # after that, we don't need to re-authenticate\n            self.session.auth = None\n        r = self.session.post(url, **opts)\n\n        self._validate_authorized(r)\n\n        if r.status_code != requests.codes.CREATED:\n            response = utils.safe_json_load(r.content)\n            already_assigned = 'is assigned to another network view'\n            if response and already_assigned in response.get('text'):\n                exception = ib_ex.InfobloxMemberAlreadyAssigned\n            else:\n                exception = ib_ex.InfobloxCannotCreateObject\n            raise exception(\n                response=response,\n                obj_type=obj_type,\n                content=r.content,\n                args=payload,\n                code=r.status_code)\n\n        return self._parse_reply(r)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating an Infoblox object with the given ref with the given payload.", "response": "def update_object(self, ref, payload, return_fields=None):\n        \"\"\"Update an Infoblox object\n\n        Args:\n            ref      (str): Infoblox object reference\n            payload (dict): Payload with data to send\n        Returns:\n            The object reference of the updated object\n        Raises:\n            InfobloxException\n        \"\"\"\n        query_params = self._build_query_params(return_fields=return_fields)\n\n        opts = self._get_request_options(data=payload)\n        url = self._construct_url(ref, query_params)\n        self._log_request('put', url, opts)\n        r = self.session.put(url, **opts)\n\n        self._validate_authorized(r)\n\n        if r.status_code != requests.codes.ok:\n            self._check_service_availability('update', r, ref)\n\n            raise ib_ex.InfobloxCannotUpdateObject(\n                response=jsonutils.loads(r.content),\n                ref=ref,\n                content=r.content,\n                code=r.status_code)\n\n        return self._parse_reply(r)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_object(self, ref, delete_arguments=None):\n        opts = self._get_request_options()\n        if not isinstance(delete_arguments, dict):\n            delete_arguments = {}\n        url = self._construct_url(ref, query_params=delete_arguments)\n        self._log_request('delete', url, opts)\n        r = self.session.delete(url, **opts)\n\n        self._validate_authorized(r)\n\n        if r.status_code != requests.codes.ok:\n            self._check_service_availability('delete', r, ref)\n\n            raise ib_ex.InfobloxCannotDeleteObject(\n                response=jsonutils.loads(r.content),\n                ref=ref,\n                content=r.content,\n                code=r.status_code)\n\n        return self._parse_reply(r)", "response": "Removes an Infoblox object with the specified ref."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _remap_fields(cls, kwargs):\n        mapped = {}\n        for key in kwargs:\n            if key in cls._remap:\n                mapped[cls._remap[key]] = kwargs[key]\n            else:\n                mapped[key] = kwargs[key]\n        return mapped", "response": "Map fields from kwargs into dict acceptable by NIOS"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting extensible attributes from the NIOS reply to the object.", "response": "def from_dict(cls, eas_from_nios):\n        \"\"\"Converts extensible attributes from the NIOS reply.\"\"\"\n        if not eas_from_nios:\n            return\n        return cls({name: cls._process_value(ib_utils.try_value_to_bool,\n                                             eas_from_nios[name]['value'])\n                    for name in eas_from_nios})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts extensible attributes into the format suitable for NIOS.", "response": "def to_dict(self):\n        \"\"\"Converts extensible attributes into the format suitable for NIOS.\"\"\"\n        return {name: {'value': self._process_value(str, value)}\n                for name, value in self._ea_dict.items()\n                if not (value is None or value == \"\" or value == [])}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying processing method for value or each element in it.", "response": "def _process_value(func, value):\n        \"\"\"Applies processing method for value or each element in it.\n\n        :param func: method to be called with value\n        :param value: value to process\n        :return: if 'value' is list/tupe, returns iterable with func results,\n                 else func result is returned\n        \"\"\"\n        if isinstance(value, (list, tuple)):\n            return [func(item) for item in value]\n        return func(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild dict fields as SubObjects if needed.", "response": "def from_dict(cls, connector, ip_dict):\n        \"\"\"Build dict fields as SubObjects if needed.\n\n        Checks if lambda for building object from dict exists.\n        _global_field_processing and _custom_field_processing rules\n        are checked.\n        \"\"\"\n        mapping = cls._global_field_processing.copy()\n        mapping.update(cls._custom_field_processing)\n        # Process fields that require building themselves as objects\n        for field in mapping:\n            if field in ip_dict:\n                ip_dict[field] = mapping[field](ip_dict[field])\n        return cls(connector, **ip_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread field value and converts to dict if possible", "response": "def field_to_dict(self, field):\n        \"\"\"Read field value and converts to dict if possible\"\"\"\n        value = getattr(self, field)\n        if isinstance(value, (list, tuple)):\n            return [self.value_to_dict(val) for val in value]\n        return self.value_to_dict(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild dict with None object fields", "response": "def to_dict(self, search_fields=None):\n        \"\"\"Builds dict without None object fields\"\"\"\n        fields = self._fields\n        if search_fields == 'update':\n            fields = self._search_for_update_fields\n        elif search_fields == 'all':\n            fields = self._all_searchable_fields\n        elif search_fields == 'exclude':\n            # exclude search fields for update actions,\n            # but include updateable_search_fields\n            fields = [field for field in self._fields\n                      if field in self._updateable_search_fields or\n                      field not in self._search_for_update_fields]\n\n        return {field: self.field_to_dict(field) for field in fields\n                if getattr(self, field, None) is not None}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets mac and duid fields To have common interface with FixedAddress accept mac address and set duid as a side effect.", "response": "def mac(self, mac):\n        \"\"\"Set mac and duid fields\n\n        To have common interface with FixedAddress accept mac address\n        and set duid as a side effect.\n        'mac' was added to _shadow_fields to prevent sending it out over wapi.\n        \"\"\"\n        self._mac = mac\n        if mac:\n            self.duid = ib_utils.generate_duid(mac)\n        elif not hasattr(self, 'duid'):\n            self.duid = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering a property for bosh manifest according to its type.", "response": "def render_property(property):\n\t\"\"\"Render a property for bosh manifest, according to its type.\"\"\"\n\t# This ain't the prettiest thing, but it should get the job done.\n\t# I don't think we have anything more elegant available at bosh-manifest-generation time.\n\t# See https://docs.pivotal.io/partners/product-template-reference.html for list.\n\tif 'type' in property and property['type'] in PROPERTY_FIELDS:\n\t\tfields = {}\n\t\tfor field in PROPERTY_FIELDS[property['type']]:\n\t\t\tif type(field) is tuple:\n\t\t\t\tfields[field[0]] = '(( .properties.{}.{} ))'.format(property['name'], field[1])\n\t\t\telse:\n\t\t\t\tfields[field] = '(( .properties.{}.{} ))'.format(property['name'], field)\n\t\tout = { property['name']: fields }\n\telse:\n\t\tif property.get('is_reference', False):\n\t\t\tout = { property['name']: property['default'] }\n\t\telse:\n\t\t\tout = { property['name']: '(( .properties.{}.value ))'.format(property['name']) }\n\treturn out"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match(obj, matchers=TYPES):\n    buf = get_bytes(obj)\n\n    for matcher in matchers:\n        if matcher.match(buf):\n            return matcher\n\n    return None", "response": "Returns a new object that matches the given input againts the available\n    file type matchers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the first 262 bytes of the given bytearray as part of the file header signature.", "response": "def signature(array):\n    \"\"\"\n    Returns the first 262 bytes of the given bytearray\n    as part of the file header signature.\n\n    Args:\n        array: bytearray to extract the header signature.\n\n    Returns:\n        First 262 bytes of the file content as bytearray type.\n    \"\"\"\n    length = len(array)\n    index = _NUM_SIGNATURE_BYTES if length > _NUM_SIGNATURE_BYTES else length\n\n    return array[:index]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bytes(obj):\n    try:\n        obj = obj.read(_NUM_SIGNATURE_BYTES)\n    except AttributeError:\n        # duck-typing as readable failed - we'll try the other options\n        pass\n\n    kind = type(obj)\n\n    if kind is bytearray:\n        return signature(obj)\n\n    if kind is str:\n        return get_signature_bytes(obj)\n\n    if kind is bytes:\n        return signature(obj)\n\n    if kind is memoryview:\n        return signature(obj).tolist()\n\n    raise TypeError('Unsupported type as file input: %s' % kind)", "response": "Infers the input type and reads the first 262 bytes returning a sliced bytearray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_type(mime=None, ext=None):\n    for kind in types:\n        if kind.extension is ext or kind.mime is mime:\n            return kind\n    return None", "response": "Returns the file type instance matching the given MIME type or file extension."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nopens the file with the appropriate call", "response": "def open(self, encoding=None):\n        \"\"\"Opens the file with the appropriate call\"\"\"\n        try:\n            if IS_GZIPPED_FILE.search(self._filename):\n                _file = gzip.open(self._filename, 'rb')\n            else:\n                if encoding:\n                    _file = io.open(self._filename, 'r', encoding=encoding, errors='replace')\n                elif self._encoding:\n                    _file = io.open(self._filename, 'r', encoding=self._encoding, errors='replace')\n                else:\n                    _file = io.open(self._filename, 'r', errors='replace')\n        except IOError, e:\n            self._log_warning(str(e))\n            _file = None\n            self.close()\n\n        return _file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing all currently open file pointers", "response": "def close(self):\n        \"\"\"Closes all currently open file pointers\"\"\"\n        if not self.active:\n            return\n\n        self.active = False\n        if self._file:\n            self._file.close()\n            self._sincedb_update_position(force_update=True)\n\n        if self._current_event:\n            event = '\\n'.join(self._current_event)\n            self._current_event.clear()\n            self._callback_wrapper([event])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _buffer_extract(self, data):\n        # Extract token-delimited entities from the input string with the split command.\n        # There's a bit of craftiness here with the -1 parameter.  Normally split would\n        # behave no differently regardless of if the token lies at the very end of the\n        # input buffer or not (i.e. a literal edge case)  Specifying -1 forces split to\n        # return \"\" in this case, meaning that the last entry in the list represents a\n        # new segment of data where the token has not been encountered\n        entities = collections.deque(data.split(self._delimiter, -1))\n\n        # Check to see if the buffer has exceeded capacity, if we're imposing a limit\n        if self._size_limit:\n            if self.input_size + len(entities[0]) > self._size_limit:\n                raise Exception('input buffer full')\n            self._input_size += len(entities[0])\n\n        # Move the first entry in the resulting array into the input buffer.  It represents\n        # the last segment of a token-delimited entity unless it's the only entry in the list.\n        first_entry = entities.popleft()\n        if len(first_entry) > 0:\n            self._input.append(first_entry)\n\n        # If the resulting array from the split is empty, the token was not encountered\n        # (not even at the end of the buffer).  Since we've encountered no token-delimited\n        # entities this go-around, return an empty array.\n        if len(entities) == 0:\n            return []\n\n        # At this point, we've hit a token, or potentially multiple tokens.  Now we can bring\n        # together all the data we've buffered from earlier calls without hitting a token,\n        # and add it to our list of discovered entities.\n        entities.appendleft(''.join(self._input))\n\n        # Now that we've hit a token, joined the input buffer and added it to the entities\n        # list, we can go ahead and clear the input buffer.  All of the segments that were\n        # stored before the join can now be garbage collected.\n        self._input.clear()\n\n        # The last entity in the list is not token delimited, however, thanks to the -1\n        # passed to split.  It represents the beginning of a new list of as-yet-untokenized\n        # data, so we add it to the start of the list.\n        self._input.append(entities.pop())\n\n        # Set the new input buffer size, provided we're keeping track\n        if self._size_limit:\n            self._input_size = len(self._input[0])\n\n        # Now we're left with the list of extracted token-delimited entities we wanted\n        # in the first place.  Hooray!\n        return entities", "response": "Extracts the entries from the input string and returns an array of tokenized entities."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the file is good.", "response": "def _ensure_file_is_good(self, current_time):\n        \"\"\"Every N seconds, ensures that the file we are tailing is the file we expect to be tailing\"\"\"\n        if self._last_file_mapping_update and current_time - self._last_file_mapping_update <= self._stat_interval:\n            return\n\n        self._last_file_mapping_update = time.time()\n\n        try:\n            st = os.stat(self._filename)\n        except EnvironmentError, err:\n            if err.errno == errno.ENOENT:\n                self._log_info('file removed')\n                self.close()\n                return\n            raise\n\n        fid = self.get_file_id(st)\n        if fid != self._fid:\n            self._log_info('file rotated')\n            self.close()\n        elif self._file.tell() > st.st_size:\n            if st.st_size == 0 and self._ignore_truncate:\n                self._logger.info(\"[{0}] - file size is 0 {1}. \".format(fid, self._filename) +\n                                  \"If you use another tool (i.e. logrotate) to truncate \" +\n                                  \"the file, your application may continue to write to \" +\n                                  \"the offset it last wrote later. In such a case, we'd \" +\n                                  \"better do nothing here\")\n                return\n            self._log_info('file truncated')\n            self._update_file(seek_to_end=False)\n        elif REOPEN_FILES:\n            self._log_debug('file reloaded (non-linux)')\n            position = self._file.tell()\n            self._update_file(seek_to_end=False)\n            if self.active:\n                self._file.seek(position, os.SEEK_SET)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads lines from a file and performs a callback against them", "response": "def _run_pass(self):\n        \"\"\"Read lines from a file and performs a callback against them\"\"\"\n        while True:\n            try:\n                data = self._file.read(4096)\n            except IOError, e:\n                if e.errno == errno.ESTALE:\n                    self.active = False\n                    return False\n\n            lines = self._buffer_extract(data)\n\n            if not lines:\n                # Before returning, check if an event (maybe partial) is waiting for too long.\n                if self._current_event and time.time() - self._last_activity > 1:\n                    event = '\\n'.join(self._current_event)\n                    self._current_event.clear()\n                    self._callback_wrapper([event])\n                break\n\n            self._last_activity = time.time()\n\n            if self._multiline_regex_after or self._multiline_regex_before:\n                # Multiline is enabled for this file.\n                events = multiline_merge(\n                        lines,\n                        self._current_event,\n                        self._multiline_regex_after,\n                        self._multiline_regex_before)\n            else:\n                events = lines\n\n            if events:\n                self._callback_wrapper(events)\n\n            if self._sincedb_path:\n                current_line_count = len(lines)\n                self._sincedb_update_position(lines=current_line_count)\n\n        self._sincedb_update_position()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sincedb_init(self):\n        if not self._sincedb_path:\n            return\n\n        if not os.path.exists(self._sincedb_path):\n            self._log_debug('initializing sincedb sqlite schema')\n            conn = sqlite3.connect(self._sincedb_path, isolation_level=None)\n            conn.execute(\"\"\"\n            create table sincedb (\n                fid      text primary key,\n                filename text,\n                position integer default 1\n            );\n            \"\"\")\n            conn.close()", "response": "Initializes the sincedb schema in an sqlite db"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sincedb_update_position(self, lines=0, force_update=False):\n        if not self._sincedb_path:\n            return False\n\n        self._line_count = self._line_count + lines\n        old_count = self._line_count_sincedb\n        lines = self._line_count\n\n        current_time = int(time.time())\n        if not force_update:\n            if self._last_sincedb_write and current_time - self._last_sincedb_write <= self._sincedb_write_interval:\n                return False\n\n            if old_count == lines:\n                return False\n\n        self._sincedb_init()\n\n        self._last_sincedb_write = current_time\n\n        self._log_debug('updating sincedb to {0}'.format(lines))\n\n        conn = sqlite3.connect(self._sincedb_path, isolation_level=None)\n        cursor = conn.cursor()\n        query = 'insert or replace into sincedb (fid, filename) values (:fid, :filename);'\n        cursor.execute(query, {\n            'fid': self._fid,\n            'filename': self._filename\n        })\n\n        query = 'update sincedb set position = :position where fid = :fid and filename = :filename'\n        cursor.execute(query, {\n            'fid': self._fid,\n            'filename': self._filename,\n            'position': lines,\n        })\n        conn.close()\n\n        self._line_count_sincedb = lines\n\n        return True", "response": "Updates the starting position of the record with the given number of lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _sincedb_start_position(self):\n        if not self._sincedb_path:\n            return None\n\n        self._sincedb_init()\n        self._log_debug('retrieving start_position from sincedb')\n        conn = sqlite3.connect(self._sincedb_path, isolation_level=None)\n        cursor = conn.cursor()\n        cursor.execute('select position from sincedb where fid = :fid and filename = :filename', {\n            'fid': self._fid,\n            'filename': self._filename\n        })\n\n        start_position = None\n        for row in cursor.fetchall():\n            start_position, = row\n\n        return start_position", "response": "Retrieves the starting position from the sincedb sql db\n        for a given file\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the file with the current state of the object.", "response": "def _update_file(self, seek_to_end=True):\n        \"\"\"Open the file for tailing\"\"\"\n        try:\n            self.close()\n            self._file = self.open()\n        except IOError:\n            pass\n        else:\n            if not self._file:\n                return\n\n            self.active = True\n            try:\n                st = os.stat(self._filename)\n            except EnvironmentError, err:\n                if err.errno == errno.ENOENT:\n                    self._log_info('file removed')\n                    self.close()\n\n            fid = self.get_file_id(st)\n            if not self._fid:\n                self._fid = fid\n\n            if fid != self._fid:\n                self._log_info('file rotated')\n                self.close()\n            elif seek_to_end:\n                self._seek_to_end()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tail(self, fname, encoding, window, position=None):\n        if window <= 0:\n            raise ValueError('invalid window %r' % window)\n\n        encodings = ENCODINGS\n        if encoding:\n            encodings = [encoding] + ENCODINGS\n\n        for enc in encodings:\n            try:\n                f = self.open(encoding=enc)\n                if f:\n                    return self.tail_read(f, window, position=position)\n\n                return False\n            except IOError, err:\n                if err.errno == errno.ENOENT:\n                    return []\n                raise\n            except UnicodeDecodeError:\n                pass", "response": "Read last N lines from file fname."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_transport(beaver_config, logger):\n    transport_str = beaver_config.get('transport')\n    if '.' not in transport_str:\n        # allow simple names like 'redis' to load a beaver built-in transport\n        module_path = 'beaver.transports.%s_transport' % transport_str.lower()\n        class_name = '%sTransport' % transport_str.title()\n    else:\n        # allow dotted path names to load a custom transport class\n        try:\n            module_path, class_name = transport_str.rsplit('.', 1)\n        except ValueError:\n            raise Exception('Invalid transport {0}'.format(beaver_config.get('transport')))\n\n    _module = __import__(module_path, globals(), locals(), class_name, -1)\n    transport_class = getattr(_module, class_name)\n    transport = transport_class(beaver_config=beaver_config, logger=logger)\n\n    return transport", "response": "Creates and returns a transport object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef listdir(self):\n        ls = os.listdir(self._folder)\n        return [x for x in ls if os.path.splitext(x)[1][1:] == \"log\"]", "response": "Return a list of log files in the log folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect new files and adds new ones if they are not already loaded.", "response": "def update_files(self):\n        \"\"\"Ensures all files are properly loaded.\n        Detects new files, file removals, file rotation, and truncation.\n        On non-linux platforms, it will also manually reload the file for tailing.\n        Note that this hack is necessary because EOF is cached on BSD systems.\n        \"\"\"\n        if self._update_time and int(time.time()) - self._update_time < self._discover_interval:\n            return\n\n        self._update_time = int(time.time())\n\n        possible_files = []\n        files = []\n        if len(self._beaver_config.get('globs')) > 0:\n            extend_files = files.extend\n            for name, exclude in self._beaver_config.get('globs').items():\n                globbed = [os.path.realpath(filename) for filename in eglob(name, exclude)]\n                extend_files(globbed)\n                self._beaver_config.addglob(name, globbed)\n                self._callback((\"addglob\", (name, globbed)))\n        else:\n            append_files = files.append\n            for name in self.listdir():\n                append_files(os.path.realpath(os.path.join(self._folder, name)))\n\n        for absname in files:\n            try:\n                st = os.stat(absname)\n            except EnvironmentError, err:\n                if err.errno != errno.ENOENT:\n                    raise\n            else:\n                if not stat.S_ISREG(st.st_mode):\n                    continue\n                append_possible_files = possible_files.append\n                fid = self.get_file_id(st)\n                append_possible_files((fid, absname))\n\n        # add new ones\n        new_files = [fname for fid, fname in possible_files if fid not in self._tails]\n        self.watch(new_files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef close(self, signalnum=None, frame=None):\n        self._running = False\n        \"\"\"Closes all currently open Tail objects\"\"\"\n        self._log_debug(\"Closing all tail objects\")\n        self._active = False\n        for fid in self._tails:\n            self._tails[fid].close()\n        for n in range(0,self._number_of_consumer_processes):\n            if self._proc[n] is not None and self._proc[n].is_alive():\n                self._logger.debug(\"Terminate Process: \" + str(n))\n                self._proc[n].terminate()\n                self._proc[n].join()", "response": "Closes all currently open Tail objects and terminate all processes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes glob. glob but supports \"/path/**/{a b c. txt\" lookup", "response": "def eglob(path, exclude=None):\n    \"\"\"Like glob.glob, but supports \"/path/**/{a,b,c}.txt\" lookup\"\"\"\n    fi = itertools.chain.from_iterable\n    paths = list(fi(glob2.iglob(d) for d in expand_paths(path)))\n    if exclude:\n        cached_regex = cached_regices.get(exclude, None)\n        if not cached_regex:\n            cached_regex = cached_regices[exclude] = re.compile(exclude)\n        paths = [x for x in paths if not cached_regex.search(x)]\n\n    return paths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a path expands it to return all permutations of the path with expanded brackets.", "response": "def expand_paths(path):\n    \"\"\"When given a path with brackets, expands it to return all permutations\n       of the path with expanded brackets, similar to ant.\n\n       >>> expand_paths('../{a,b}/{c,d}')\n       ['../a/c', '../a/d', '../b/c', '../b/d']\n       >>> expand_paths('../{a,b}/{a,b}.py')\n       ['../a/a.py', '../a/b.py', '../b/a.py', '../b/b.py']\n       >>> expand_paths('../{a,b,c}/{a,b,c}')\n       ['../a/a', '../a/b', '../a/c', '../b/a', '../b/b', '../b/c', '../c/a', '../c/b', '../c/c']\n       >>> expand_paths('test')\n       ['test']\n       >>> expand_paths('')\n    \"\"\"\n    pr = itertools.product\n    parts = MAGIC_BRACKETS.findall(path)\n\n    if not path:\n        return\n\n    if not parts:\n        return [path]\n\n    permutations = [[(p[0], i, 1) for i in p[1].split(',')] for p in parts]\n    return [_replace_all(path, i) for i in pr(*permutations)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multiline_merge(lines, current_event, re_after, re_before):\n    events = []\n    for line in lines:\n        if re_before and re_before.match(line):\n            current_event.append(line)\n        elif re_after and current_event and re_after.match(current_event[-1]):\n            current_event.append(line)\n        else:\n            if current_event:\n                events.append('\\n'.join(current_event))\n            current_event.clear()\n            current_event.append(line)\n\n    return events", "response": "This method merges multiple lines of event names based on a regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a BeaverSshTunnel object if the current config requires us to be proxied through SSH tunnel.", "response": "def create_ssh_tunnel(beaver_config, logger=None):\n    \"\"\"Returns a BeaverSshTunnel object if the current config requires us to\"\"\"\n    if not beaver_config.use_ssh_tunnel():\n        return None\n\n    logger.info(\"Proxying transport using through local ssh tunnel\")\n    return BeaverSshTunnel(beaver_config, logger=logger)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npoll attached subprocess until it is available", "response": "def poll(self):\n        \"\"\"Poll attached subprocess until it is available\"\"\"\n        if self._subprocess is not None:\n            self._subprocess.poll()\n\n        time.sleep(self._beaver_config.get('subprocess_poll_sleep'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npublishing lines one by one to the given topic", "response": "def callback(self, filename, lines, **kwargs):\n        \"\"\"publishes lines one by one to the given topic\"\"\"\n        timestamp = self.get_timestamp(**kwargs)\n        if kwargs.get('timestamp', False):\n            del kwargs['timestamp']\n\n        for line in lines:\n            try:\n                import warnings\n                with warnings.catch_warnings():\n                    warnings.simplefilter('error')\n                    self._client.publish(self._topic, self.format(filename, line, timestamp, **kwargs), 0)\n            except Exception, e:\n                try:\n                    raise TransportException(e.strerror)\n                except AttributeError:\n                    raise TransportException('Unspecified exception encountered')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _to_unicode(self, data, encoding, errors='strict'):\n    '''Given a string and its encoding, decodes the string into Unicode.\n    %encoding is a string recognized by encodings.aliases'''\n\n    # strip Byte Order Mark (if present)\n    if (len(data) >= 4) and (data[:2] == '\\xfe\\xff') and (data[2:4] != '\\x00\\x00'):\n        encoding = 'utf-16be'\n        data = data[2:]\n    elif (len(data) >= 4) and (data[:2] == '\\xff\\xfe') and (data[2:4] != '\\x00\\x00'):\n        encoding = 'utf-16le'\n        data = data[2:]\n    elif data[:3] == '\\xef\\xbb\\xbf':\n        encoding = 'utf-8'\n        data = data[3:]\n    elif data[:4] == '\\x00\\x00\\xfe\\xff':\n        encoding = 'utf-32be'\n        data = data[4:]\n    elif data[:4] == '\\xff\\xfe\\x00\\x00':\n        encoding = 'utf-32le'\n        data = data[4:]\n    newdata = unicode(data, encoding, errors)\n    return newdata", "response": "Given a string and its encoding decodes the string into Unicode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef callback(self, filename, lines, **kwargs):\n        timestamp = self.get_timestamp(**kwargs)\n        if kwargs.get('timestamp', False):\n            del kwargs['timestamp']\n        \n\n        for line in lines:\n            try:\n                import warnings\n                with warnings.catch_warnings():\n                    warnings.simplefilter('error')\n                    m = self.format(filename, line, timestamp, **kwargs)\n                    self.logger.debug(\"Sending message \" + m)\n                    self.conn.send(destination=self.queue, body=m)    \n\n            except Exception, e:\n                self.logger.error(e)\n                try:\n                    raise TransportException(e)\n                except AttributeError:\n                    raise TransportException('Unspecified exception encountered')", "response": "This method is called by the client when a message is received from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nallows reconnection from when a TransportException is thrown", "response": "def reconnect(self):\n        \"\"\"Allows reconnection from when a handled\n        TransportException is thrown\"\"\"\n        try:\n            self.conn.close()\n        except Exception,e:\n            self.logger.warn(e)\n\n        self.createConnection()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_connections(self):\n\n        for server in self._servers:\n            if self._is_reachable(server):\n                server['down_until'] = 0\n            else:\n                server['down_until'] = time.time() + 5", "response": "Checks if all redis servers are reachable"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_reachable(self, server):\n\n        try:\n            server['redis'].ping()\n            return True\n        except UserWarning:\n            self._logger.warn('Cannot reach redis server: ' + server['url'])\n        except Exception:\n            self._logger.warn('Cannot reach redis server: ' + server['url'])\n\n        return False", "response": "Checks if the given redis server is reachable"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef invalidate(self):\n\n        super(RedisTransport, self).invalidate()\n        for server in self._servers:\n            server['redis'].connection_pool.disconnect()\n        return False", "response": "Invalidates the current transport and disconnects all redis connections"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend log lines to redis servers", "response": "def callback(self, filename, lines, **kwargs):\n        \"\"\"Sends log lines to redis servers\"\"\"\n\n        self._logger.debug('Redis transport called')\n\n        timestamp = self.get_timestamp(**kwargs)\n        if kwargs.get('timestamp', False):\n            del kwargs['timestamp']\n\n        namespaces = self._beaver_config.get_field('redis_namespace', filename)\n        if not namespaces:\n            namespaces = self._namespace\n        namespaces = namespaces.split(\",\")\n\n        self._logger.debug('Got namespaces: '.join(namespaces))\n\n        data_type = self._data_type\n        self._logger.debug('Got data type: ' + data_type)\n\n        server = self._get_next_server()\n        self._logger.debug('Got redis server: ' + server['url'])\n\n        pipeline = server['redis'].pipeline(transaction=False)\n\n        callback_map = {\n            self.LIST_DATA_TYPE: pipeline.rpush,\n            self.CHANNEL_DATA_TYPE: pipeline.publish,\n        }\n        callback_method = callback_map[data_type]\n\n        for line in lines:\n            for namespace in namespaces:\n                callback_method(\n                    namespace.strip(),\n                    self.format(filename, line, timestamp, **kwargs)\n                )\n\n        try:\n            pipeline.execute()\n        except redis.exceptions.RedisError, exception:\n            self._logger.warn('Cannot push lines to redis server: ' + server['url'])\n            raise TransportException(exception)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a valid redis server or raises a TransportException", "response": "def _get_next_server(self):\n        \"\"\"Returns a valid redis server or raises a TransportException\"\"\"\n\n        current_try = 0\n        max_tries = len(self._servers)\n\n        while current_try < max_tries:\n\n            server_index = self._raise_server_index()\n            server = self._servers[server_index]\n            down_until = server['down_until']\n\n            self._logger.debug('Checking server ' + str(current_try + 1) + '/' + str(max_tries) + ': ' + server['url'])\n\n            if down_until == 0:\n                self._logger.debug('Elected server: ' + server['url'])\n                return server\n\n            if down_until < time.time():\n                if self._is_reachable(server):\n                    server['down_until'] = 0\n                    self._logger.debug('Elected server: ' + server['url'])\n\n                    return server\n                else:\n                    self._logger.debug('Server still unavailable: ' + server['url'])\n                    server['down_until'] = time.time() + 5\n\n            current_try += 1\n\n        raise TransportException('Cannot reach any redis server')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nround robin magic to Raises the current redis server index and returns it", "response": "def _raise_server_index(self):\n        \"\"\"Round robin magic: Raises the current redis server index and returns it\"\"\"\n\n        self._current_server_index = (self._current_server_index + 1) % len(self._servers)\n\n        return self._current_server_index"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef valid(self):\n\n        valid_servers = 0\n        for server in self._servers:\n            if server['down_until'] <= time.time():\n                valid_servers += 1\n\n        return valid_servers > 0", "response": "Returns whether or not the transport can send data to any redis server"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef callback(self, filename, lines, **kwargs):\n        timestamp = self.get_timestamp(**kwargs)\n        if kwargs.get('timestamp', False):\n            del kwargs['timestamp']\n\n        for line in lines:\n            try:\n                import warnings\n                with warnings.catch_warnings():\n                    warnings.simplefilter('error')\n                    #produce message\n                    if self._key is None:\n                        response = self._prod.send_messages(self._kafka_config['topic'], self.format(filename, line, timestamp, **kwargs))\n                    else:\n                        response = self._prod.send_messages(self._kafka_config['topic'], self._key, self.format(filename, line, timestamp, **kwargs))\n\n                    if response:\n                        if response[0].error:\n                            self._logger.info('message error: {0}'.format(response[0].error))\n                            self._logger.info('message offset: {0}'.format(response[0].offset))\n\n            except Exception as e:\n                try:\n                    self._logger.error('Exception caught sending message/s : ' + str(e))\n                    raise TransportException(e.strerror)\n                except AttributeError:\n                    raise TransportException('Unspecified exception encountered')", "response": "publishes lines one by one to the given topic"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format(self, filename, line, timestamp, **kwargs):\n        line = unicode(line.encode(\"utf-8\"), \"utf-8\", errors=\"ignore\")\n        formatter = self._beaver_config.get_field('format', filename)\n        if formatter not in self._formatters:\n            formatter = self._default_formatter\n\n        data = {\n            self._fields.get('type'): kwargs.get('type'),\n            self._fields.get('tags'): kwargs.get('tags'),\n            '@timestamp': timestamp,\n            self._fields.get('host'): self._current_host,\n            self._fields.get('file'): filename,\n            self._fields.get('message'): line\n        }\n\n        if self._logstash_version == 0:\n            data['@source'] = 'file://{0}'.format(filename)\n            data['@fields'] = kwargs.get('fields')\n        else:\n            data['@version'] = self._logstash_version\n            fields = kwargs.get('fields')\n            for key in fields:\n                data[key] = fields.get(key)\n\n        return self._formatters[formatter](data)", "response": "Formats a log line"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_timestamp(self, **kwargs):\n        timestamp = kwargs.get('timestamp')\n        if not timestamp:\n            now = datetime.datetime.utcnow()\n            timestamp = now.strftime(\"%Y-%m-%dT%H:%M:%S\") + \".%03d\" % (now.microsecond / 1000) + \"Z\"\n\n        return timestamp", "response": "Retrieves the timestamp for a given set of data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake the file at path executable.", "response": "def _make_executable(path):\n    \"\"\"Make the file at path executable.\"\"\"\n    os.chmod(path, os.stat(path).st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nseparate method from main to make testing easier and to enable command - line access.", "response": "def subset_main(args):\n    \"\"\" Separate method from main() in order to make testing easier and to\n    enable command-line access. \"\"\"\n\n    # Read in each of the command line arguments\n    rid = _read_arg(args.rid)\n    cid = _read_arg(args.cid)\n    exclude_rid = _read_arg(args.exclude_rid)\n    exclude_cid = _read_arg(args.exclude_cid)\n\n    # If GCT, use subset_gctoo\n    if args.in_path.endswith(\".gct\"):\n\n        in_gct = parse_gct.parse(args.in_path)\n        out_gct = sg.subset_gctoo(in_gct, rid=rid, cid=cid,\n                                 exclude_rid=exclude_rid,\n                                 exclude_cid=exclude_cid)\n\n    # If GCTx, use parse_gctx\n    else:\n\n        if (exclude_rid is not None) or (exclude_cid is not None):\n            msg = \"exclude_{rid,cid} args not currently supported for parse_gctx.\"\n            raise(Exception(msg))\n\n        logger.info(\"Using hyperslab selection functionality of parse_gctx...\")\n        out_gct = parse_gctx.parse(args.in_path, rid=rid, cid=cid)\n\n    # Write the output gct\n    if args.out_type == \"gctx\":\n        wgx.write(out_gct, args.out_name)\n    else:\n        wg.write(out_gct, args.out_name, data_null=\"NaN\", metadata_null=\"NA\", filler_null=\"NA\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the n - item from the argument list.", "response": "def _read_arg(arg):\n    \"\"\"\n    If arg is a list with 1 element that corresponds to a valid file path, use\n    set_io.grp to read the grp file. Otherwise, check that arg is a list of strings.\n\n    Args:\n        arg (list or None)\n\n    Returns:\n        arg_out (list or None)\n    \"\"\"\n\n    # If arg is None, just return it back\n    if arg is None:\n        arg_out = arg\n\n    else:\n        # If len(arg) == 1 and arg[0] is a valid filepath, read it as a grp file\n        if len(arg) == 1 and os.path.exists(arg[0]):\n            arg_out = grp.read(arg[0])\n        else:\n            arg_out = arg\n\n        # Make sure that arg_out is a list of strings\n        assert isinstance(arg_out, list), \"arg_out must be a list.\"\n        assert type(arg_out[0]) == str, \"arg_out must be a list of strings.\"\n\n    return arg_out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the covariance matrix between the columns of x and y and returns the array of the covariance values for the rows of x and y.", "response": "def fast_cov(x, y=None, destination=None):\n    \"\"\"calculate the covariance matrix for the columns of x (MxN), or optionally, the covariance matrix between the\n    columns of x and and the columns of y (MxP).  (In the language of statistics, the columns are variables, the rows\n    are observations).\n\n    Args:\n        x (numpy array-like) MxN in shape\n        y (numpy array-like) MxP in shape\n        destination (numpy array-like) optional location where to store the results as they are calculated (e.g. a numpy\n            memmap of a file)\n\n        returns (numpy array-like) array of the covariance values\n            for defaults (y=None), shape is NxN\n            if y is provided, shape is NxP\n    \"\"\"\n    validate_inputs(x, y, destination)\n\n    if y is None:\n        y = x\n\n    if destination is None:\n        destination = numpy.zeros((x.shape[1], y.shape[1]))\n\n    mean_x = numpy.mean(x, axis=0)\n    mean_y = numpy.mean(y, axis=0)\n\n    mean_centered_x = (x - mean_x).astype(destination.dtype)\n    mean_centered_y = (y - mean_y).astype(destination.dtype)\n    \n    numpy.dot(mean_centered_x.T, mean_centered_y, out=destination)\n    numpy.divide(destination, (x.shape[0] - 1), out=destination)\n\n    return destination"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(file_path):\n    # Read in file\n    actual_file_path = os.path.expanduser(file_path)\n    with open(actual_file_path, 'r') as f:\n        lines = f.readlines()\n    \n    # Create GMT object\n    gmt = []\n    \n    # Iterate over each line\n    for line_num, line in enumerate(lines):\n        # Separate along tabs\n        fields = line.split('\\t')\n\n        assert len(fields) > 2, (\n            \"Each line must have at least 3 tab-delimited items. \" +\n            \"line_num: {}, fields: {}\").format(line_num, fields)\n        \n        # Get rid of trailing whitespace\n        fields[-1] = fields[-1].rstrip()\n        \n        # Collect entries\n        entries = fields[2:]\n        \n        # Remove empty entries\n        entries = [x for x in entries if x]\n\n        assert len(set(entries)) == len(entries), (\n            \"There should not be duplicate entries for the same set. \" +\n            \"line_num: {}, entries: {}\").format(line_num, entries)\n\n        # Store this line as a dictionary\n        line_dict = {SET_IDENTIFIER_FIELD: fields[0],\n                     SET_DESC_FIELD: fields[1],\n                     SET_MEMBERS_FIELD: entries}\n        gmt.append(line_dict)\n\n    verify_gmt_integrity(gmt)\n\n    return gmt", "response": "Reads a GMT file and returns a list of dicts where each dict corresponds to one ArcGIS line of the GMT file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef verify_gmt_integrity(gmt):\n\n    # Verify that set ids are unique\n    set_ids = [d[SET_IDENTIFIER_FIELD] for d in gmt]\n    assert len(set(set_ids)) == len(set_ids), (\n        \"Set identifiers should be unique. set_ids: {}\".format(set_ids))", "response": "Verify that set identifiers are unique."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(gmt, out_path):\n    with open(out_path, 'w') as f:\n        for _, each_dict in enumerate(gmt):\n            f.write(each_dict[SET_IDENTIFIER_FIELD] + '\\t')\n            f.write(each_dict[SET_DESC_FIELD] + '\\t')\n            f.write('\\t'.join([str(entry) for entry in each_dict[SET_MEMBERS_FIELD]]))\n            f.write('\\n')", "response": "Writes a GMT to a text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff_gctoo(gctoo, plate_control=True, group_field='pert_type', group_val='ctl_vehicle',\n               diff_method=\"robust_z\", upper_diff_thresh=10, lower_diff_thresh=-10):\n    ''' Converts a matrix of values (e.g. gene expression, viability, etc.)\n    into a matrix of differential values.\n\n    Args:\n    df (pandas df): data to make diff_gctoo\n    plate_control (bool): True means calculate diff_gctoo using plate control.\n        False means vehicle control.\n    group_field (string): Metadata field in which to find group_val\n    group_val (string): Value in group_field that indicates use in vehicle control\n    diff_method (string): Method of computing differential data; currently only\n        support either \"robust_z\" or \"median_norm\"\n    upper_diff_thresh (float): Maximum value for diff data\n    lower_diff_thresh (float): Minimum value for diff data\n\n    Returns:\n    out_gctoo (GCToo object): GCToo with differential data values\n    '''\n    assert diff_method in possible_diff_methods, (\n        \"possible_diff_methods: {}, diff_method: {}\".format(\n            possible_diff_methods, diff_method))\n\n    # Compute median and MAD using all samples in the dataset\n    if plate_control:\n\n        # Compute differential data\n        if diff_method == \"robust_z\":\n            diff_data = robust_zscore.robust_zscore(gctoo.data_df)\n\n        elif diff_method == \"median_norm\":\n            medians = gctoo.data_df.median(axis=1)\n            diff_data = gctoo.data_df.subtract(medians, axis='index')\n\n    # Compute median and MAD from negative controls, rather than all samples\n    else:\n\n        assert group_field in gctoo.col_metadata_df.columns.values, (\n            \"group_field {} not present in column metadata. \" +\n            \"gctoo.col_metadata_df.columns.values: {}\").format(\n            group_field, gctoo.col_metadata_df.columns.values)\n\n        assert sum(gctoo.col_metadata_df[group_field] == group_val) > 0, (\n            \"group_val {} not present in the {} column.\").format(\n            group_val, group_field)\n\n        # Find negative control samples\n        neg_ctl_samples = gctoo.col_metadata_df.index[gctoo.col_metadata_df[group_field] == group_val]\n        neg_ctl_df = gctoo.data_df[neg_ctl_samples]\n\n        # Compute differential data\n        if diff_method == \"robust_z\":\n            diff_data = robust_zscore.robust_zscore(gctoo.data_df, neg_ctl_df)\n\n        elif diff_method == \"median_norm\":\n            medians = gctoo.data_df.median(axis=1)\n            diff_data = gctoo.data_df.subtract(medians, axis='index')\n\n    # Threshold differential data before returning\n    diff_data = diff_data.clip(lower=lower_diff_thresh, upper=upper_diff_thresh)\n\n    # Construct output GCToo object\n    out_gctoo = GCToo.GCToo(data_df=diff_data,\n                            row_metadata_df=gctoo.row_metadata_df,\n                            col_metadata_df=gctoo.col_metadata_df)\n\n    return out_gctoo", "response": "Takes a dataframe of values and returns a matrix of differential values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a gctx file into a new object containing the contents of the object.", "response": "def parse(gctx_file_path, convert_neg_666=True, rid=None, cid=None,\n          ridx=None, cidx=None, row_meta_only=False, col_meta_only=False, make_multiindex=False):\n    \"\"\"\n    Primary method of script. Reads in path to a gctx file and parses into GCToo object.\n\n    Input:\n        Mandatory:\n        - gctx_file_path (str): full path to gctx file you want to parse.\n\n        Optional:\n        - convert_neg_666 (bool): whether to convert -666 values to numpy.nan or not\n            (see Note below for more details on this). Default = False.\n        - rid (list of strings): list of row ids to specifically keep from gctx. Default=None.\n        - cid (list of strings): list of col ids to specifically keep from gctx. Default=None.\n        - ridx (list of integers): only read the rows corresponding to this\n            list of integer ids. Default=None.\n        - cidx (list of integers): only read the columns corresponding to this\n            list of integer ids. Default=None.\n        - row_meta_only (bool): Whether to load data + metadata (if False), or just row metadata (if True)\n            as pandas DataFrame\n        - col_meta_only (bool): Whether to load data + metadata (if False), or just col metadata (if True)\n            as pandas DataFrame\n        - make_multiindex (bool): whether to create a multi-index df combining\n            the 3 component dfs\n\n    Output:\n        - myGCToo (GCToo): A GCToo instance containing content of parsed gctx file. Note: if meta_only = True,\n            this will be a GCToo instance where the data_df is empty, i.e. data_df = pd.DataFrame(index=rids,\n            columns = cids)\n\n    Note: why does convert_neg_666 exist?\n        - In CMap--for somewhat obscure historical reasons--we use \"-666\" as our null value\n        for metadata. However (so that users can take full advantage of pandas' methods,\n        including those for filtering nan's etc) we provide the option of converting these\n        into numpy.NaN values, the pandas default.\n    \"\"\"\n    full_path = os.path.expanduser(gctx_file_path)\n\n    # Verify that the  path exists\n    if not os.path.exists(full_path):\n        err_msg = \"The given path to the gctx file cannot be found. full_path: {}\"\n        logger.error(err_msg.format(full_path))\n        raise Exception(err_msg.format(full_path))\n    logger.info(\"Reading GCTX: {}\".format(full_path))\n\n    # open file\n    gctx_file = h5py.File(full_path, \"r\")\n\n    if row_meta_only:\n        # read in row metadata\n        row_dset = gctx_file[row_meta_group_node]\n        row_meta = parse_metadata_df(\"row\", row_dset, convert_neg_666)\n\n        # validate optional input ids & get indexes to subset by\n        (sorted_ridx, sorted_cidx) = check_and_order_id_inputs(rid, ridx, cid, cidx, row_meta, None)\n\n        gctx_file.close()\n\n        # subset if specified, then return\n        row_meta = row_meta.iloc[sorted_ridx]\n        return row_meta\n    elif col_meta_only:\n        # read in col metadata\n        col_dset = gctx_file[col_meta_group_node]\n        col_meta = parse_metadata_df(\"col\", col_dset, convert_neg_666)\n\n        # validate optional input ids & get indexes to subset by\n        (sorted_ridx, sorted_cidx) = check_and_order_id_inputs(rid, ridx, cid, cidx, None, col_meta)\n\n        gctx_file.close()\n\n        # subset if specified, then return\n        col_meta = col_meta.iloc[sorted_cidx]\n        return col_meta\n    else:\n        # read in row metadata\n        row_dset = gctx_file[row_meta_group_node]\n        row_meta = parse_metadata_df(\"row\", row_dset, convert_neg_666)\n\n        # read in col metadata\n        col_dset = gctx_file[col_meta_group_node]\n        col_meta = parse_metadata_df(\"col\", col_dset, convert_neg_666)\n\n        # validate optional input ids & get indexes to subset by\n        (sorted_ridx, sorted_cidx) = check_and_order_id_inputs(rid, ridx, cid, cidx, row_meta, col_meta)\n\n        data_dset = gctx_file[data_node]\n        data_df = parse_data_df(data_dset, sorted_ridx, sorted_cidx, row_meta, col_meta)\n\n        # (if subsetting) subset metadata\n        row_meta = row_meta.iloc[sorted_ridx]\n        col_meta = col_meta.iloc[sorted_cidx]\n\n        # get version\n        my_version = gctx_file.attrs[version_node]\n        if type(my_version) == np.ndarray:\n            my_version = my_version[0]\n\n        gctx_file.close()\n\n        # make GCToo instance\n        my_gctoo = GCToo.GCToo(data_df=data_df, row_metadata_df=row_meta, col_metadata_df=col_meta,\n                               src=full_path, version=my_version, make_multiindex=make_multiindex)\n        return my_gctoo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the input id inputs are of one type and returns the ordered ridx and cidx.", "response": "def check_and_order_id_inputs(rid, ridx, cid, cidx, row_meta_df, col_meta_df):\n    \"\"\"\n    Makes sure that (if entered) id inputs entered are of one type (string id or index)\n    Input:\n        - rid (list or None): if not None, a list of rids\n        - ridx (list or None): if not None, a list of indexes\n        - cid (list or None): if not None, a list of cids\n        - cidx (list or None): if not None, a list of indexes\n    Output:\n        - a tuple of the ordered ridx and cidx\n    \"\"\"\n    (row_type, row_ids) = check_id_idx_exclusivity(rid, ridx)\n    (col_type, col_ids) = check_id_idx_exclusivity(cid, cidx)\n\n    row_ids = check_and_convert_ids(row_type, row_ids, row_meta_df)\n    ordered_ridx = get_ordered_idx(row_type, row_ids, row_meta_df)\n\n    col_ids = check_and_convert_ids(col_type, col_ids, col_meta_df)\n    ordered_cidx = get_ordered_idx(col_type, col_ids, col_meta_df)\n    return (ordered_ridx, ordered_cidx)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that user didn t provide both ids and idx values to subset by.", "response": "def check_id_idx_exclusivity(id, idx):\n    \"\"\"\n    Makes sure user didn't provide both ids and idx values to subset by.\n\n    Input:\n        - id (list or None): if not None, a list of string id names\n        - idx (list or None): if not None, a list of integer id indexes\n\n    Output:\n        - a tuple: first element is subset type, second is subset content\n    \"\"\"\n    if (id is not None and idx is not None):\n        msg = (\"'id' and 'idx' fields can't both not be None,\" +\n               \" please specify subset in only one of these fields\")\n        logger.error(msg)\n        raise Exception(\"parse_gctx.check_id_idx_exclusivity: \" + msg)\n    elif id is not None:\n        return (\"id\", id)\n    elif idx is not None:\n        return (\"idx\", idx)\n    else:\n        return (None, [])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets index values corresponding to ids to subset and orders them.", "response": "def get_ordered_idx(id_type, id_list, meta_df):\n    \"\"\"\n    Gets index values corresponding to ids to subset and orders them.\n    Input:\n        - id_type (str): either \"id\", \"idx\" or None\n        - id_list (list): either a list of indexes or id names\n    Output:\n        - a sorted list of indexes to subset a dimension by\n    \"\"\"\n    if meta_df is not None:\n        if id_type is None:\n            id_list = range(0, len(list(meta_df.index)))\n        elif id_type == \"id\":\n            lookup = {x: i for (i,x) in enumerate(meta_df.index)}\n            id_list = [lookup[str(i)] for i in id_list]\n        return sorted(id_list)\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading in all metadata from hdf5 file to pandas DataFrame and returns a DataFrame corresponding to the metadata fields of the specified dimension.", "response": "def parse_metadata_df(dim, meta_group, convert_neg_666):\n    \"\"\"\n    Reads in all metadata from .gctx file to pandas DataFrame\n    with proper GCToo specifications.\n    Input:\n        - dim (str): Dimension of metadata; either \"row\" or \"column\"\n        - meta_group (HDF5 group): Group from which to read metadata values\n        - convert_neg_666 (bool): whether to convert \"-666\" values to np.nan or not\n    Output:\n        - meta_df (pandas DataFrame): data frame corresponding to metadata fields\n            of dimension specified.\n    \"\"\"\n    # read values from hdf5 & make a DataFrame\n    header_values = {}\n    array_index = 0\n    for k in meta_group.keys():\n        curr_dset = meta_group[k]\n        temp_array = np.empty(curr_dset.shape, dtype=curr_dset.dtype)\n        curr_dset.read_direct(temp_array)\n        # convert all values to str in temp_array so that\n        # to_numeric works consistently with gct and gct_x parser\n        temp_array = temp_array.astype('str')\n        header_values[str(k)] = temp_array\n        array_index = array_index + 1\n\n\n    meta_df = pd.DataFrame.from_dict(header_values)\n\n    # save the ids for later use in the index; we do not want to convert them to\n    # numeric\n    ids = meta_df[\"id\"].copy()\n    del meta_df[\"id\"]\n\n    # Convert metadata to numeric if possible, after converting everything to string first\n    # Note: This conversion first to string is to ensure consistent behavior between\n    #    the gctx and gct parser (which by default reads the entire text file into a string)\n    meta_df = meta_df.apply(lambda x: pd.to_numeric(x, errors=\"ignore\"))\n\n    meta_df.set_index(pd.Index(ids, dtype=str), inplace=True)\n\n    # Replace -666 and -666.0 with NaN; also replace \"-666\" if convert_neg_666 is True\n    meta_df = replace_666(meta_df, convert_neg_666)\n\n    # set index and columns appropriately\n    set_metadata_index_and_column_names(dim, meta_df)\n    return meta_df"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace - 666. 0 and optionally - 666. 0 with - 666. 0 and optionally - 666. 0.", "response": "def replace_666(meta_df, convert_neg_666):\n    \"\"\" Replace -666, -666.0, and optionally \"-666\".\n    Args:\n        meta_df (pandas df):\n        convert_neg_666 (bool):\n    Returns:\n        out_df (pandas df): updated meta_df\n    \"\"\"\n    if convert_neg_666:\n        out_df = meta_df.replace([-666, \"-666\", -666.0], np.nan)\n    else:\n        out_df = meta_df.replace([-666, -666.0], \"-666\")\n    return out_df"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_metadata_index_and_column_names(dim, meta_df):\n    if dim == \"row\":\n        meta_df.index.name = \"rid\"\n        meta_df.columns.name = \"rhd\"\n    elif dim == \"col\":\n        meta_df.index.name = \"cid\"\n        meta_df.columns.name = \"chd\"", "response": "Sets index and column names of the metadata fields\n            of the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing in data_df from hdf5 dataset", "response": "def parse_data_df(data_dset, ridx, cidx, row_meta, col_meta):\n    \"\"\"\n    Parses in data_df from hdf5, subsetting if specified.\n\n    Input:\n        -data_dset (h5py dset): HDF5 dataset from which to read data_df\n        -ridx (list): list of indexes to subset from data_df\n            (may be all of them if no subsetting)\n        -cidx (list): list of indexes to subset from data_df\n            (may be all of them if no subsetting)\n        -row_meta (pandas DataFrame): the parsed in row metadata\n        -col_meta (pandas DataFrame): the parsed in col metadata\n    \"\"\"\n    if len(ridx) == len(row_meta.index) and len(cidx) == len(col_meta.index):  # no subset\n        data_array = np.empty(data_dset.shape, dtype=np.float32)\n        data_dset.read_direct(data_array)\n        data_array = data_array.transpose()\n    elif len(ridx) <= len(cidx):\n        first_subset = data_dset[:, ridx].astype(np.float32)\n        data_array = first_subset[cidx, :].transpose()\n    elif len(cidx) < len(ridx):\n        first_subset = data_dset[cidx, :].astype(np.float32)\n        data_array = first_subset[:, ridx].transpose()\n    # make DataFrame instance\n    data_df = pd.DataFrame(data_array, index=row_meta.index[ridx], columns=col_meta.index[cidx])\n    return data_df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nopening .gctx file and returns only column metadata Input: Mandatory: - gctx_file_path (str): full path to gctx file you want to parse. Optional: - convert_neg_666 (bool): whether to convert -666 values to num Output: - col_meta (pandas DataFrame): a DataFrame of all column metadata values.", "response": "def get_column_metadata(gctx_file_path, convert_neg_666=True):\n    \"\"\"\n    Opens .gctx file and returns only column metadata\n\n    Input:\n        Mandatory:\n        - gctx_file_path (str): full path to gctx file you want to parse.\n\n        Optional:\n        - convert_neg_666 (bool): whether to convert -666 values to num\n\n    Output:\n        - col_meta (pandas DataFrame): a DataFrame of all column metadata values.\n    \"\"\"\n    full_path = os.path.expanduser(gctx_file_path)\n    # open file\n    gctx_file = h5py.File(full_path, \"r\")\n    col_dset = gctx_file[col_meta_group_node]\n    col_meta = parse_metadata_df(\"col\", col_dset, convert_neg_666)\n    gctx_file.close()\n    return col_meta"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening .gctx file and returns only row metadata Input: Mandatory: - gctx_file_path (str): full path to gctx file you want to parse. Optional: - convert_neg_666 (bool): whether to convert -666 values to num Output: - row_meta (pandas DataFrame): a DataFrame of all row metadata values.", "response": "def get_row_metadata(gctx_file_path, convert_neg_666=True):\n    \"\"\"\n    Opens .gctx file and returns only row metadata\n\n    Input:\n        Mandatory:\n        - gctx_file_path (str): full path to gctx file you want to parse.\n\n        Optional:\n        - convert_neg_666 (bool): whether to convert -666 values to num\n\n    Output:\n        - row_meta (pandas DataFrame): a DataFrame of all row metadata values.\n    \"\"\"\n    full_path = os.path.expanduser(gctx_file_path)\n    # open file\n    gctx_file = h5py.File(full_path, \"r\")\n    row_dset = gctx_file[row_meta_group_node]\n    row_meta = parse_metadata_df(\"row\", row_dset, convert_neg_666)\n    gctx_file.close()\n    return row_meta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multi_index_df_to_component_dfs(multi_index_df, rid=\"rid\", cid=\"cid\"):\n\n    # Id level of the multiindex will become the index\n    rids = list(multi_index_df.index.get_level_values(rid))\n    cids = list(multi_index_df.columns.get_level_values(cid))\n\n    # It's possible that the index and/or columns of multi_index_df are not\n    # actually multi-index; need to check for this and there are more than one level in index(python3)\n    if isinstance(multi_index_df.index, pd.MultiIndex):\n\n        # check if there are more than one levels in index (python3)\n        if len(multi_index_df.index.names) > 1:\n\n            # If so, drop rid because it won't go into the body of the metadata\n            mi_df_index = multi_index_df.index.droplevel(rid)\n\n            # Names of the multiindex levels become the headers\n            rhds = list(mi_df_index.names)\n\n            # Assemble metadata values\n            row_metadata = np.array([mi_df_index.get_level_values(level).values for level in list(rhds)]).T\n\n        # if there is one level in index (python3), then rhds and row metadata should be empty\n        else:\n            rhds = []\n            row_metadata = []\n\n    # If the index is not multi-index, then rhds and row metadata should be empty\n    else:\n        rhds = []\n        row_metadata = []\n\n    # Check if columns of multi_index_df are in fact multi-index\n    if isinstance(multi_index_df.columns, pd.MultiIndex):\n\n        # Check if there are more than one levels in columns(python3)\n        if len(multi_index_df.columns.names) > 1:\n\n            # If so, drop cid because it won't go into the body of the metadata\n            mi_df_columns = multi_index_df.columns.droplevel(cid)\n\n            # Names of the multiindex levels become the headers\n            chds = list(mi_df_columns.names)\n\n            # Assemble metadata values\n            col_metadata = np.array([mi_df_columns.get_level_values(level).values for level in list(chds)]).T\n\n        # If there is one level in columns (python3), then rhds and row metadata should be empty\n        else:\n            chds = []\n            col_metadata = []\n    # If the columns are not multi-index, then rhds and row metadata should be empty\n    else:\n        chds = []\n        col_metadata = []\n\n    # Create component dfs\n    row_metadata_df = pd.DataFrame.from_records(row_metadata, index=pd.Index(rids, name=\"rid\"), columns=pd.Index(rhds, name=\"rhd\"))\n    col_metadata_df = pd.DataFrame.from_records(col_metadata, index=pd.Index(cids, name=\"cid\"), columns=pd.Index(chds, name=\"chd\"))\n    data_df = pd.DataFrame(multi_index_df.values, index=pd.Index(rids, name=\"rid\"), columns=pd.Index(cids, name=\"cid\"))\n\n    return data_df, row_metadata_df, col_metadata_df", "response": "Convert a multi - index df into 3 component dfs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies that a dataframe instance and that its index and column values are unique and that they are unique.", "response": "def check_df(self, df):\n        \"\"\"\n        Verifies that df is a pandas DataFrame instance and\n        that its index and column values are unique.\n        \"\"\"\n        if isinstance(df, pd.DataFrame):\n            if not df.index.is_unique:\n                repeats = df.index[df.index.duplicated()].values\n                msg = \"Index values must be unique but aren't. The following entries appear more than once: {}\".format(repeats)\n                self.logger.error(msg)\n                raise Exception(\"GCToo GCToo.check_df \" + msg)\n            if not df.columns.is_unique:\n                repeats = df.columns[df.columns.duplicated()].values\n                msg = \"Columns values must be unique but aren't. The following entries appear more than once: {}\".format(repeats)\n                raise Exception(\"GCToo GCToo.check_df \" + msg)\n            else:\n                return True\n        else:\n            msg = \"expected Pandas DataFrame, got something else:  {}  of type:  {}\".format(df, type(df))\n            self.logger.error(msg)\n            raise Exception(\"GCToo GCToo.check_df \" + msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that id values match between data_df and meta_df", "response": "def id_match_check(self, data_df, meta_df, dim):\n        \"\"\"\n        Verifies that id values match between:\n            - row case: index of data_df & index of row metadata\n            - col case: columns of data_df & index of column metadata\n        \"\"\"\n        if dim == \"row\":\n            if len(data_df.index) == len(meta_df.index) and set(data_df.index) == set(meta_df.index):\n                return True\n            else:\n                msg = (\"The rids are inconsistent between data_df and row_metadata_df.\\n\" +\n                 \"data_df.index.values:\\n{}\\nrow_metadata_df.index.values:\\n{}\").format(data_df.index.values, meta_df.index.values)\n                self.logger.error(msg)\n                raise Exception(\"GCToo GCToo.id_match_check \" + msg)\n        elif dim == \"col\":\n            if len(data_df.columns) == len(meta_df.index) and set(data_df.columns) == set(meta_df.index):\n                return True\n            else:\n                msg = (\"The cids are inconsistent between data_df and col_metadata_df.\\n\" +\n                 \"data_df.columns.values:\\n{}\\ncol_metadata_df.index.values:\\n{}\").format(data_df.columns.values, meta_df.index.values)\n                self.logger.error(msg)\n                raise Exception(\"GCToo GCToo.id_match_check \" + msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassembles three component dataframes into a multiindex dataframe.", "response": "def assemble_multi_index_df(self):\n        \"\"\"Assembles three component dataframes into a multiindex dataframe.\n        Sets the result to self.multi_index_df.\n        IMPORTANT: Cross-section (\"xs\") is the best command for selecting\n        data. Be sure to use the flag \"drop_level=False\" with this command,\n        or else the dataframe that is returned will not have the same\n        metadata as the input.\n        N.B. \"level\" means metadata header.\n        N.B. \"axis=1\" indicates column annotations.\n        Examples:\n            1) Select the probe with pr_lua_id=\"LUA-3404\":\n            lua3404_df = multi_index_df.xs(\"LUA-3404\", level=\"pr_lua_id\", drop_level=False)\n            2) Select all DMSO samples:\n            DMSO_df = multi_index_df.xs(\"DMSO\", level=\"pert_iname\", axis=1, drop_level=False)\n        \"\"\"\n        #prepare row index\n        self.logger.debug(\"Row metadata shape: {}\".format(self.row_metadata_df.shape))\n        self.logger.debug(\"Is empty? {}\".format(self.row_metadata_df.empty))\n        row_copy = pd.DataFrame(self.row_metadata_df.index) if self.row_metadata_df.empty else self.row_metadata_df.copy()\n        row_copy[\"rid\"] = row_copy.index\n        row_index = pd.MultiIndex.from_arrays(row_copy.T.values, names=row_copy.columns)\n\n        #prepare column index\n        self.logger.debug(\"Col metadata shape: {}\".format(self.col_metadata_df.shape))\n        col_copy = pd.DataFrame(self.col_metadata_df.index) if self.col_metadata_df.empty else self.col_metadata_df.copy()\n        col_copy[\"cid\"] = col_copy.index\n        transposed_col_metadata = col_copy.T\n        col_index = pd.MultiIndex.from_arrays(transposed_col_metadata.values, names=transposed_col_metadata.index)\n\n        # Create multi index dataframe using the values of data_df and the indexes created above\n        self.logger.debug(\"Data df shape: {}\".format(self.data_df.shape))\n        self.multi_index_df = pd.DataFrame(data=self.data_df.values, index=row_index, columns=col_index)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a GCT file into a CMap object.", "response": "def parse(file_path, convert_neg_666=True, rid=None, cid=None,\n          ridx=None, cidx=None, row_meta_only=False, col_meta_only=False, make_multiindex=False):\n    \"\"\"\n    The main method.\n\n    Args:\n        - file_path (string): full path to gct(x) file you want to parse\n        - convert_neg_666 (bool): whether to convert -666 values to numpy.nan\n            (see Note below for more details). Default = False.\n        - rid (list of strings): list of row ids to specifically keep from gct. Default=None.\n        - cid (list of strings): list of col ids to specifically keep from gct. Default=None.\n        - ridx (list of integers): only read the rows corresponding to this\n            list of integer ids. Default=None.\n        - cidx (list of integers): only read the columns corresponding to this\n            list of integer ids. Default=None.\n        - row_meta_only (bool): Whether to load data + metadata (if False), or\n            just row metadata (if True) as pandas DataFrame\n        - col_meta_only (bool): Whether to load data + metadata (if False), or\n            just col metadata (if True) as pandas DataFrame\n        - make_multiindex (bool): whether to create a multi-index df combining\n            the 3 component dfs\n\n    Returns:\n        - myGCToo (GCToo object): A GCToo instance containing content of\n            parsed gct file ** OR **\n        - row_metadata (pandas df) ** OR ** col_metadata (pandas df)\n\n    Note: why is convert_neg_666 even a thing?\n        In CMap--for somewhat obscure historical reasons--we use \"-666\" as our null value\n        for metadata. However (so that users can take full advantage of pandas' methods,\n        including those for filtering nan's etc) we provide the option of converting these\n        into numpy.nan values, the pandas default.\n\n    \"\"\"\n    assert sum([row_meta_only, col_meta_only]) <= 1, (\n        \"row_meta_only and col_meta_only cannot both be requested.\")\n\n    nan_values = [\n        \"#N/A\", \"N/A\", \"NA\", \"#NA\", \"NULL\", \"NaN\", \"-NaN\",\n        \"nan\", \"-nan\", \"#N/A!\", \"na\", \"NA\", \"None\", \"#VALUE!\"]\n\n    # Add \"-666\" to the list of NaN values\n    if convert_neg_666:\n        nan_values.append(\"-666\")\n\n    # Verify that the gct path exists\n    if not os.path.exists(file_path):\n        err_msg = \"The given path to the gct file cannot be found. gct_path: {}\"\n        logger.error(err_msg.format(file_path))\n        raise Exception(err_msg.format(file_path))\n    logger.info(\"Reading GCT: {}\".format(file_path))\n\n    # Read version and dimensions\n    (version, num_data_rows, num_data_cols,\n     num_row_metadata, num_col_metadata) = read_version_and_dims(file_path)\n\n    # Read in metadata and data\n    (row_metadata, col_metadata, data) = parse_into_3_df(\n        file_path, num_data_rows, num_data_cols,\n        num_row_metadata, num_col_metadata, nan_values)\n\n    # Create the gctoo object and assemble 3 component dataframes\n    # Not the most efficient if only metadata requested (i.e. creating the\n    # whole GCToo just to return the metadata df), but simplest\n    myGCToo = create_gctoo_obj(file_path, version, row_metadata, col_metadata,\n                               data, make_multiindex)\n    # Subset if requested\n    if (rid is not None) or (ridx is not None) or (cid is not None) or (cidx is not None):\n        logger.info(\"Subsetting GCT... (note that there are no speed gains when subsetting GCTs)\")\n        myGCToo = sg.subset_gctoo(myGCToo, rid=rid, cid=cid, ridx=ridx, cidx=cidx)\n\n    if row_meta_only:\n        return myGCToo.row_metadata_df\n\n    elif col_meta_only:\n        return myGCToo.col_metadata_df\n\n    else:\n        return myGCToo"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if genes are present in the API", "response": "def are_genes_in_api(my_clue_api_client, gene_symbols):\n    \"\"\"determine if genes are present in the API\n\n    Args:\n        my_clue_api_client:\n        gene_symbols: collection of gene symbols to query the API with\n\n    Returns: set of the found gene symbols\n\n    \"\"\"\n    if len(gene_symbols) > 0:\n        query_gene_symbols = gene_symbols if type(gene_symbols) is list else list(gene_symbols)\n\n        query_result = my_clue_api_client.run_filter_query(resource_name,\n            {\"where\":{\"gene_symbol\":{\"inq\":query_gene_symbols}}, \"fields\":{\"gene_symbol\":True}})\n        logger.debug(\"query_result:  {}\".format(query_result))\n\n        r = set([x[\"gene_symbol\"] for x in query_result])\n        return r\n    else:\n        logger.warning(\"provided gene_symbols was empty, cannot run query\")\n        return set()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(gctoo, out_fname, data_null=\"NaN\", metadata_null=\"-666\", filler_null=\"-666\", data_float_format=\"%.4f\"):\n    # Create handle for output file\n    if not out_fname.endswith(\".gct\"):\n        out_fname += \".gct\"\n    f = open(out_fname, \"w\")\n\n    # Write first two lines\n    dims = [str(gctoo.data_df.shape[0]), str(gctoo.data_df.shape[1]),\n            str(gctoo.row_metadata_df.shape[1]), str(gctoo.col_metadata_df.shape[1])]\n    write_version_and_dims(VERSION, dims, f)\n\n    # Write top half of the gct\n    write_top_half(f, gctoo.row_metadata_df, gctoo.col_metadata_df,\n                   metadata_null, filler_null)\n\n    # Write bottom half of the gct\n    write_bottom_half(f, gctoo.row_metadata_df, gctoo.data_df,\n                      data_null, data_float_format, metadata_null)\n\n    f.close()\n    logger.info(\"GCT has been written to {}\".format(out_fname))", "response": "Write a gctoo object to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_version_and_dims(version, dims, f):\n    f.write((\"#\" + version + \"\\n\"))\n    f.write((dims[0] + \"\\t\" + dims[1] + \"\\t\" + dims[2] + \"\\t\" + dims[3] + \"\\n\"))", "response": "Write version and dims to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the top - half of the gct file.", "response": "def write_top_half(f, row_metadata_df, col_metadata_df, metadata_null, filler_null):\n    \"\"\" Write the top half of the gct file: top-left filler values, row metadata\n    headers, and top-right column metadata.\n\n    Args:\n        f (file handle): handle for output file\n        row_metadata_df (pandas df)\n        col_metadata_df (pandas df)\n        metadata_null (string): how to represent missing values in the metadata\n        filler_null (string): what value to fill the top-left filler block with\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize the top half of the gct including the third line\n    size_of_top_half_df = (1 + col_metadata_df.shape[1],\n                           1 + row_metadata_df.shape[1] + col_metadata_df.shape[0])\n\n    top_half_df = pd.DataFrame(np.full(size_of_top_half_df, filler_null, dtype=object))\n\n    # Assemble the third line of the gct: \"id\", then rhds, then cids\n    top_half_df.iloc[0, :] = np.hstack((\"id\", row_metadata_df.columns.values, col_metadata_df.index.values))\n\n    # Insert the chds\n    top_half_df.iloc[range(1, top_half_df.shape[0]), 0] = col_metadata_df.columns.values\n\n    # Insert the column metadata, but first convert to strings and replace NaNs\n    col_metadata_indices = (range(1, top_half_df.shape[0]),\n                            range(1 + row_metadata_df.shape[1], top_half_df.shape[1]))\n    # pd.DataFrame.at to insert into dataframe(python3)\n    top_half_df.at[col_metadata_indices[0], col_metadata_indices[1]] = (\n        col_metadata_df.astype(str).replace(\"nan\", value=metadata_null).T.values)\n\n    # Write top_half_df to file\n    top_half_df.to_csv(f, header=False, index=False, sep=\"\\t\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the bottom half of the gct file.", "response": "def write_bottom_half(f, row_metadata_df, data_df, data_null, data_float_format, metadata_null):\n    \"\"\" Write the bottom half of the gct file: row metadata and data.\n\n    Args:\n        f (file handle): handle for output file\n        row_metadata_df (pandas df)\n        data_df (pandas df)\n        data_null (string): how to represent missing values in the data\n        metadata_null (string): how to represent missing values in the metadata\n        data_float_format (string): how many decimal points to keep in representing data\n\n    Returns:\n        None\n    \"\"\"\n    # create the left side of the bottom half of the gct (for the row metadata)\n    size_of_left_bottom_half_df = (row_metadata_df.shape[0],\n                              1 + row_metadata_df.shape[1])\n    left_bottom_half_df = pd.DataFrame(np.full(size_of_left_bottom_half_df, metadata_null, dtype=object))\n\n    #create the full bottom half by combining with the above with the matrix data\n    bottom_half_df = pd.concat([left_bottom_half_df, data_df.reset_index(drop=True)], axis=1)\n    bottom_half_df.columns = range(bottom_half_df.shape[1])\n\n    # Insert the rids\n    bottom_half_df.iloc[:, 0] = row_metadata_df.index.values\n\n    # Insert the row metadata, but first convert to strings and replace NaNs\n    row_metadata_col_indices = range(1, 1 + row_metadata_df.shape[1])\n    bottom_half_df.iloc[:, row_metadata_col_indices] = (\n        row_metadata_df.astype(str).replace(\"nan\", value=metadata_null).values)\n\n    # Write bottom_half_df to file\n    bottom_half_df.to_csv(f, header=False, index=False, sep=\"\\t\",\n                          na_rep=data_null,\n                          float_format=data_float_format)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append_dims_and_file_extension(fname, data_df):\n    # If there's no .gct at the end of output file name, add the dims and .gct\n    if not fname.endswith(\".gct\"):\n        out_fname = '{0}_n{1}x{2}.gct'.format(fname, data_df.shape[1], data_df.shape[0])\n        return out_fname\n\n    # Otherwise, only add the dims\n    else:\n        basename = os.path.splitext(fname)[0]\n        out_fname = '{0}_n{1}x{2}.gct'.format(basename, data_df.shape[1], data_df.shape[0])\n        return out_fname", "response": "Append dimensions and file extension to output filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nseparate from main() in order to make command-line tool.", "response": "def gct2gctx_main(args):\n    \"\"\" Separate from main() in order to make command-line tool. \"\"\"\n\n    in_gctoo = parse_gct.parse(args.filename, convert_neg_666=False)\n\n    if args.output_filepath is None:\n        basename = os.path.basename(args.filename)\n        out_name = os.path.splitext(basename)[0] + \".gctx\"\n    else:\n        out_name = args.output_filepath\n\n    \"\"\" If annotations are supplied, parse table and set metadata_df \"\"\"\n    if args.row_annot_path is None:\n        pass\n    else:\n        row_metadata = pd.read_csv(args.row_annot_path, sep='\\t', index_col=0, header=0, low_memory=False)\n        assert all(in_gctoo.data_df.index.isin(row_metadata.index)), \\\n            \"Row ids in matrix missing from annotations file\"\n        in_gctoo.row_metadata_df = row_metadata.loc[row_metadata.index.isin(in_gctoo.data_df.index)]\n\n    if args.col_annot_path is None:\n        pass\n    else:\n        col_metadata = pd.read_csv(args.col_annot_path, sep='\\t', index_col=0, header=0, low_memory=False)\n        assert all(in_gctoo.data_df.columns.isin(col_metadata.index)), \\\n            \"Column ids in matrix missing from annotations file\"\n        in_gctoo.col_metadata_df = col_metadata.loc[col_metadata.index.isin(in_gctoo.data_df.columns)]\n\n    write_gctx.write(in_gctoo, out_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a GCT file into a CMap object.", "response": "def parse(file_path, convert_neg_666=True, rid=None, cid=None, ridx=None, cidx=None,\n          row_meta_only=False, col_meta_only=False, make_multiindex=False):\n    \"\"\"\n    Identifies whether file_path corresponds to a .gct or .gctx file and calls the\n    correct corresponding parse method.\n\n    Input:\n        Mandatory:\n        - gct(x)_file_path (str): full path to gct(x) file you want to parse.\n\n        Optional:\n        - convert_neg_666 (bool): whether to convert -666 values to numpy.nan or not\n            (see Note below for more details on this). Default = False.\n        - rid (list of strings): list of row ids to specifically keep from gctx. Default=None.\n        - cid (list of strings): list of col ids to specifically keep from gctx. Default=None.\n        - ridx (list of integers): only read the rows corresponding to this\n            list of integer ids. Default=None.\n        - cidx (list of integers): only read the columns corresponding to this\n            list of integer ids. Default=None.\n        - row_meta_only (bool): Whether to load data + metadata (if False), or just row metadata (if True)\n            as pandas DataFrame\n        - col_meta_only (bool): Whether to load data + metadata (if False), or just col metadata (if True)\n            as pandas DataFrame\n        - make_multiindex (bool): whether to create a multi-index df combining\n            the 3 component dfs\n\n    Output:\n        - out (GCToo object or pandas df): if row_meta_only or col_meta_only, then\n            out is a metadata df; otherwise, it's a GCToo instance containing\n            content of parsed gct(x) file\n\n    Note: why does convert_neg_666 exist?\n        - In CMap--for somewhat obscure historical reasons--we use \"-666\" as our null value\n        for metadata. However (so that users can take full advantage of pandas' methods,\n        including those for filtering nan's etc) we provide the option of converting these\n        into numpy.NaN values, the pandas default.\n    \"\"\"\n    if file_path.endswith(\".gct\"):\n        out = parse_gct.parse(file_path, convert_neg_666=convert_neg_666,\n                              rid=rid, cid=cid, ridx=ridx, cidx=cidx,\n                              row_meta_only=row_meta_only, col_meta_only=col_meta_only,\n                              make_multiindex=make_multiindex)\n\n    elif file_path.endswith(\".gctx\"):\n        out = parse_gctx.parse(file_path, convert_neg_666=convert_neg_666,\n                              rid=rid, cid=cid, ridx=ridx, cidx=cidx,\n                              row_meta_only=row_meta_only, col_meta_only=col_meta_only,\n                              make_multiindex=make_multiindex)\n\n    else:\n        err_msg = \"File to parse must be .gct or .gctx!\"\n        logger.error(err_msg)\n        raise Exception(err_msg)\n\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts upper triangle from a square matrix. Negative values are set to 0.", "response": "def get_upper_triangle(correlation_matrix):\n    ''' Extract upper triangle from a square matrix. Negative values are\n    set to 0.\n\n    Args:\n    correlation_matrix (pandas df): Correlations between all replicates\n\n    Returns:\n    upper_tri_df (pandas df): Upper triangle extracted from\n        correlation_matrix; rid is the row index, cid is the column index,\n        corr is the extracted correlation value\n    '''\n    upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(np.bool))\n\n    # convert matrix into long form description\n    upper_tri_df = upper_triangle.stack().reset_index(level=1)\n    upper_tri_df.columns = ['rid', 'corr']\n\n    # Index at this point is cid, it now becomes a column\n    upper_tri_df.reset_index(level=0, inplace=True)\n\n    # Get rid of negative values\n    upper_tri_df['corr'] = upper_tri_df['corr'].clip(lower=0)\n\n    return upper_tri_df.round(rounding_precision)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates a weighted average of the correlations between all replicates and the minimum raw weight", "response": "def calculate_weights(correlation_matrix, min_wt):\n    ''' Calculate a weight for each profile based on its correlation to other\n    replicates. Negative correlations are clipped to 0, and weights are clipped\n    to be min_wt at the least.\n\n    Args:\n    correlation_matrix (pandas df): Correlations between all replicates\n    min_wt (float): Minimum raw weight when calculating weighted average\n\n    Returns:\n    raw weights (pandas series):  Mean correlation to other replicates\n    weights (pandas series): raw_weights normalized such that they add to 1\n    '''\n    # fill diagonal of correlation_matrix with np.nan\n    np.fill_diagonal(correlation_matrix.values, np.nan)\n\n    # remove negative values\n    correlation_matrix = correlation_matrix.clip(lower=0)\n\n    # get average correlation for each profile (will ignore NaN)\n    raw_weights = correlation_matrix.mean(axis=1)\n\n    # threshold weights\n    raw_weights = raw_weights.clip(lower=min_wt)\n\n    # normalize raw_weights so that they add to 1\n    weights = raw_weights / sum(raw_weights)\n\n    return raw_weights.round(rounding_precision), weights.round(rounding_precision)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef agg_wt_avg(mat, min_wt = 0.01, corr_metric='spearman'):\n    ''' Aggregate a set of replicate profiles into a single signature using\n    a weighted average.\n\n    Args:\n    mat (pandas df): a matrix of replicate profiles, where the columns are\n        samples and the rows are features; columns correspond to the\n        replicates of a single perturbagen\n    min_wt (float): Minimum raw weight when calculating weighted average\n    corr_metric (string): Spearman or Pearson; the correlation method\n\n    Returns:\n    out_sig (pandas series): weighted average values\n    upper_tri_df (pandas df): the correlations between each profile that went into the signature\n    raw weights (pandas series): weights before normalization\n    weights (pandas series): weights after normalization\n    '''\n    assert mat.shape[1] > 0, \"mat is empty! mat: {}\".format(mat)\n\n    if mat.shape[1] == 1:\n\n        out_sig = mat\n        upper_tri_df = None\n        raw_weights = None\n        weights = None\n\n    else:\n\n        assert corr_metric in [\"spearman\", \"pearson\"]\n\n        # Make correlation matrix column wise\n        corr_mat = mat.corr(method=corr_metric)\n\n        # Save the values in the upper triangle\n        upper_tri_df = get_upper_triangle(corr_mat)\n\n        # Calculate weight per replicate\n        raw_weights, weights = calculate_weights(corr_mat, min_wt)\n\n        # Apply weights to values\n        weighted_values = mat * weights\n        out_sig = weighted_values.sum(axis=1)\n\n    return out_sig, upper_tri_df, raw_weights, weights", "response": "Aggregate a set of replicate profiles into a single signature using a weighted average."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef concat_main(args):\n\n    # Get files directly\n    if args.input_filepaths is not None:\n        files = args.input_filepaths\n\n    # Or find them\n    else:\n        files = get_file_list(args.file_wildcard)\n        \n        # No files found\n        if len(files) == 0:\n            msg = \"No files were found. args.file_wildcard: {}\".format(args.file_wildcard)\n            logger.error(msg)\n            raise Exception(msg)\n\n    # Only 1 file found\n    if len(files) == 1:\n        logger.warning(\"Only 1 file found. No concatenation needs to be done, exiting\")\n        return\n\n    # More than 1 file found\n    else:\n        # Parse each file and append to a list\n        gctoos = []\n        for f in files:\n            gctoos.append(parse.parse(f))\n\n        # Create concatenated gctoo object\n        if args.concat_direction == \"horiz\":\n            out_gctoo = hstack(gctoos, args.remove_all_metadata_fields, args.error_report_output_file,\n                               args.fields_to_remove, args.reset_ids)\n\n        elif args.concat_direction == \"vert\":\n            out_gctoo = vstack(gctoos, args.remove_all_metadata_fields, args.error_report_output_file,\n                               args.fields_to_remove, args.reset_ids)\n\n    # Write out_gctoo to file\n    logger.info(\"Writing to output file args.out_name:  {}\".format(args.out_name))\n\n    if args.out_type == \"gctx\":\n        write_gctx.write(out_gctoo, args.out_name)\n\n    elif args.out_type == \"gct\":\n        write_gct.write(out_gctoo, args.out_name,\n                          filler_null=args.filler_null,\n                          metadata_null=args.metadata_null,\n                          data_null=args.data_null)", "response": "This method is used to combine the gctoos and gctx objects into one gctoo with the same name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching for files to be concatenated. Currently very basic but could be more sophisticated.", "response": "def get_file_list(wildcard):\n    \"\"\" Search for files to be concatenated. Currently very basic, but could\n    expand to be more sophisticated.\n\n    Args:\n        wildcard (regular expression string)\n\n    Returns:\n        files (list of full file paths)\n\n    \"\"\"\n    files = glob.glob(os.path.expanduser(wildcard))\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hstack(gctoos, remove_all_metadata_fields=False, error_report_file=None, fields_to_remove=[], reset_ids=False):\n    # Separate each gctoo into its component dfs\n    row_meta_dfs = []\n    col_meta_dfs = []\n    data_dfs = []\n    srcs = []\n    for g in gctoos:\n        row_meta_dfs.append(g.row_metadata_df)\n        col_meta_dfs.append(g.col_metadata_df)\n        data_dfs.append(g.data_df)\n        srcs.append(g.src)\n\n    logger.debug(\"shapes of row_meta_dfs:  {}\".format([x.shape for x in row_meta_dfs]))\n\n    # Concatenate row metadata\n    all_row_metadata_df = assemble_common_meta(row_meta_dfs, fields_to_remove, srcs, remove_all_metadata_fields, error_report_file)\n\n    # Concatenate col metadata\n    all_col_metadata_df = assemble_concatenated_meta(col_meta_dfs, remove_all_metadata_fields)\n\n    # Concatenate the data_dfs\n    all_data_df = assemble_data(data_dfs, \"horiz\")\n\n    # Make sure df shapes are correct\n    assert all_data_df.shape[0] == all_row_metadata_df.shape[0], \"Number of rows in metadata does not match number of rows in data - all_data_df.shape[0]:  {}  all_row_metadata_df.shape[0]:  {}\".format(all_data_df.shape[0], all_row_metadata_df.shape[0])\n    assert all_data_df.shape[1] == all_col_metadata_df.shape[0], \"Number of columns in data does not match number of columns metadata - all_data_df.shape[1]:  {}  all_col_metadata_df.shape[0]:  {}\".format(all_data_df.shape[1], all_col_metadata_df.shape[0])\n    \n    # If requested, reset sample ids to be unique integers and move old sample\n    # ids into column metadata\n    if reset_ids:\n        do_reset_ids(all_col_metadata_df, all_data_df, \"horiz\")\n\n    logger.info(\"Build GCToo of all...\")\n    concated = GCToo.GCToo(row_metadata_df=all_row_metadata_df,\n                           col_metadata_df=all_col_metadata_df,\n                           data_df=all_data_df)\n\n    return concated", "response": "Horizontally concatenate a list of gctoos into a single tree tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassembles the common metadata dfs together.", "response": "def assemble_common_meta(common_meta_dfs, fields_to_remove, sources, remove_all_metadata_fields, error_report_file):\n    \"\"\" Assemble the common metadata dfs together. Both indices are sorted.\n    Fields that are not in all the dfs are dropped.\n\n    Args:\n        common_meta_dfs (list of pandas dfs)\n        fields_to_remove (list of strings): fields to be removed from the\n            common metadata because they don't agree across files\n\n    Returns:\n        all_meta_df_sorted (pandas df)\n\n    \"\"\"\n    all_meta_df, all_meta_df_with_dups = build_common_all_meta_df(common_meta_dfs, fields_to_remove, remove_all_metadata_fields)\n\n    if not all_meta_df.index.is_unique:\n        all_report_df = build_mismatched_common_meta_report([x.shape for x in common_meta_dfs],\n            sources, all_meta_df, all_meta_df_with_dups)\n\n        unique_duplicate_ids = all_report_df.index.unique()\n\n        if error_report_file is not None:\n            all_report_df.to_csv(error_report_file, sep=\"\\t\")\n\n        msg = \"\"\"There are inconsistencies in common_metadata_df between different files.  Try excluding metadata fields\nusing the fields_to_remove argument.  unique_duplicate_ids: {}\nall_report_df:\n{}\"\"\".format(unique_duplicate_ids, all_report_df)\n        raise MismatchCommonMetadataConcatException(msg)\n\n    # Finally, sort the index\n    all_meta_df_sorted = all_meta_df.sort_index(axis=0)\n\n    return all_meta_df_sorted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a dataframe containing all common metadata fields from the common_meta_dfs.", "response": "def build_common_all_meta_df(common_meta_dfs, fields_to_remove, remove_all_metadata_fields):\n    \"\"\"\n    concatenate the entries in common_meta_dfs, removing columns selectively (fields_to_remove) or entirely (\n        remove_all_metadata_fields=True; in this case, effectively just merges all the indexes in common_meta_dfs).\n\n        Returns 2 dataframes (in a tuple):  the first has duplicates removed, the second does not.\n\n    Args:\n        common_meta_dfs: collection of pandas DataFrames containing the metadata in the \"common\" direction of the\n            concatenation operation\n        fields_to_remove: columns to be removed (if present) from the common_meta_dfs\n        remove_all_metadata_fields: boolean indicating that all metadata fields should be removed from the\n            common_meta_dfs; overrides fields_to_remove if present\n\n    Returns:\n        tuple containing\n            all_meta_df:  pandas dataframe that is the concatenation of the dataframes in common_meta_dfs,\n            all_meta_df_with_dups:\n    \"\"\"\n\n    if remove_all_metadata_fields:\n        trimmed_common_meta_dfs = [pd.DataFrame(index=df.index) for df in common_meta_dfs]\n    else:\n        shared_column_headers = sorted(set.intersection(*[set(df.columns) for df in common_meta_dfs]))\n        logger.debug(\"shared_column_headers:  {}\".format(shared_column_headers))\n\n        trimmed_common_meta_dfs = [df[shared_column_headers] for df in common_meta_dfs]\n\n        # Remove any column headers that will prevent dfs from being identical\n        for df in trimmed_common_meta_dfs:\n            df.drop(fields_to_remove, axis=1, errors=\"ignore\", inplace=True)\n\n    # Concatenate all dfs and then remove duplicate rows\n    all_meta_df_with_dups = pd.concat(trimmed_common_meta_dfs, axis=0)\n    logger.debug(\"all_meta_df_with_dups.shape:  {}\".format(all_meta_df_with_dups.shape))\n    logger.debug(\"all_meta_df_with_dups.columns:  {}\".format(all_meta_df_with_dups.columns))\n    logger.debug(\"all_meta_df_with_dups.index:  {}\".format(all_meta_df_with_dups.index))\n\n    # If all metadata dfs were empty, df will be empty\n    if all_meta_df_with_dups.empty:\n        # Simply return unique ids\n        all_meta_df = pd.DataFrame(index=all_meta_df_with_dups.index.unique())\n\n    else:\n        all_meta_df_with_dups[\"concat_column_for_index\"] = all_meta_df_with_dups.index\n        all_meta_df = all_meta_df_with_dups.copy(deep=True).drop_duplicates()\n        all_meta_df.drop(\"concat_column_for_index\", axis=1, inplace=True)\n        all_meta_df_with_dups.drop(\"concat_column_for_index\", axis=1, inplace=True)\n\n    logger.debug(\"all_meta_df_with_dups.shape: {}\".format(all_meta_df_with_dups.shape))\n    logger.debug(\"all_meta_df.shape: {}\".format(all_meta_df.shape))\n\n    return (all_meta_df, all_meta_df_with_dups)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassemble the concatenated metadata dfs together.", "response": "def assemble_concatenated_meta(concated_meta_dfs, remove_all_metadata_fields):\n    \"\"\" Assemble the concatenated metadata dfs together. For example,\n    if horizontally concatenating, the concatenated metadata dfs are the\n    column metadata dfs. Both indices are sorted.\n\n    Args:\n        concated_meta_dfs (list of pandas dfs)\n\n    Returns:\n        all_concated_meta_df_sorted (pandas df)\n\n    \"\"\"\n    # Concatenate the concated_meta_dfs\n    if remove_all_metadata_fields:\n        for df in concated_meta_dfs:\n            df.drop(df.columns, axis=1, inplace=True)\n\n    all_concated_meta_df = pd.concat(concated_meta_dfs, axis=0)\n\n    # Sanity check: the number of rows in all_concated_meta_df should correspond\n    # to the sum of the number of rows in the input dfs\n    n_rows = all_concated_meta_df.shape[0]\n    logger.debug(\"all_concated_meta_df.shape[0]: {}\".format(n_rows))\n    n_rows_cumulative = sum([df.shape[0] for df in concated_meta_dfs])\n    assert n_rows == n_rows_cumulative\n\n    # Sort the index and columns\n    all_concated_meta_df_sorted = all_concated_meta_df.sort_index(axis=0).sort_index(axis=1)\n\n    return all_concated_meta_df_sorted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassemble the data dfs together. Both indices are sorted.", "response": "def assemble_data(data_dfs, concat_direction):\n    \"\"\" Assemble the data dfs together. Both indices are sorted.\n\n    Args:\n        data_dfs (list of pandas dfs)\n        concat_direction (string): 'horiz' or 'vert'\n\n    Returns:\n        all_data_df_sorted (pandas df)\n\n    \"\"\"\n    if concat_direction == \"horiz\":\n        # Concatenate the data_dfs horizontally\n        all_data_df = pd.concat(data_dfs, axis=1)\n\n        # Sanity check: the number of columns in all_data_df should\n        # correspond to the sum of the number of columns in the input dfs\n        n_cols = all_data_df.shape[1]\n        logger.debug(\"all_data_df.shape[1]: {}\".format(n_cols))\n        n_cols_cumulative = sum([df.shape[1] for df in data_dfs])\n        assert n_cols == n_cols_cumulative\n\n    elif concat_direction == \"vert\":\n\n        # Concatenate the data_dfs vertically\n        all_data_df = pd.concat(data_dfs, axis=0)\n\n        # Sanity check: the number of rows in all_data_df should\n        # correspond to the sum of the number of rows in the input dfs\n        n_rows = all_data_df.shape[0]\n        logger.debug(\"all_data_df.shape[0]: {}\".format(n_rows))\n        n_rows_cumulative = sum([df.shape[0] for df in data_dfs])\n        assert n_rows == n_rows_cumulative\n\n    # Sort both indices\n    all_data_df_sorted = all_data_df.sort_index(axis=0).sort_index(axis=1)\n\n    return all_data_df_sorted"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_reset_ids(concatenated_meta_df, data_df, concat_direction):\n    if concat_direction == \"horiz\":\n\n        # Make sure cids agree between data_df and concatenated_meta_df\n        assert concatenated_meta_df.index.equals(data_df.columns), (\n            \"cids in concatenated_meta_df do not agree with cids in data_df.\")\n\n        # Reset cids in concatenated_meta_df\n        reset_ids_in_meta_df(concatenated_meta_df)\n\n        # Replace cids in data_df with the new ones from concatenated_meta_df\n        # (just an array of unique integers, zero-indexed)\n        data_df.columns = pd.Index(concatenated_meta_df.index.values)\n\n    elif concat_direction == \"vert\":\n\n        # Make sure rids agree between data_df and concatenated_meta_df\n        assert concatenated_meta_df.index.equals(data_df.index), (\n            \"rids in concatenated_meta_df do not agree with rids in data_df.\")\n\n        # Reset rids in concatenated_meta_df\n        reset_ids_in_meta_df(concatenated_meta_df)\n\n        # Replace rids in data_df with the new ones from concatenated_meta_df\n        # (just an array of unique integers, zero-indexed)\n        data_df.index = pd.Index(concatenated_meta_df.index.values)", "response": "Reset ids in concatenated metadata and data dfs to unique integers and save the old ids in a metadata column."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset_ids_in_meta_df(meta_df):\n\n    # Record original index name, and then change it so that the column that it\n    # becomes will be appropriately named\n    original_index_name = meta_df.index.name\n    meta_df.index.name = \"old_id\"\n\n    # Reset index\n    meta_df.reset_index(inplace=True)\n\n    # Change the index name back to what it was\n    meta_df.index.name = original_index_name", "response": "Reset the index name and index name of the object in the meta_df so that the column that it was named\n    becomes the original index name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts a subset of data from a GCToo object.", "response": "def subset_gctoo(gctoo, row_bool=None, col_bool=None, rid=None, cid=None,\n                ridx=None, cidx=None, exclude_rid=None, exclude_cid=None):\n    \"\"\" Extract a subset of data from a GCToo object in a variety of ways.\n    The order of rows and columns will be preserved.\n\n    Args:\n        gctoo (GCToo object)\n        row_bool (list of bools): length must equal gctoo.data_df.shape[0]\n        col_bool (list of bools): length must equal gctoo.data_df.shape[1]\n        rid (list of strings): rids to include\n        cid (list of strings): cids to include\n        ridx (list of integers): row integer ids to include\n        cidx (list of integers): col integer ids to include\n        exclude_rid (list of strings): rids to exclude\n        exclude_cid (list of strings): cids to exclude\n\n    Returns:\n        out_gctoo (GCToo object): gctoo after subsetting\n    \"\"\"\n    assert sum([(rid is not None), (row_bool is not None), (ridx is not None)]) <= 1, (\n        \"Only one of rid, row_bool, and ridx can be provided.\")\n    assert sum([(cid is not None), (col_bool is not None), (cidx is not None)]) <= 1, (\n        \"Only one of cid, col_bool, and cidx can be provided.\")\n\n    # Figure out what rows and columns to keep\n    rows_to_keep = get_rows_to_keep(gctoo, rid, row_bool, ridx, exclude_rid)\n    cols_to_keep = get_cols_to_keep(gctoo, cid, col_bool, cidx, exclude_cid)\n\n    # Convert labels to boolean array to preserve order\n    rows_to_keep_bools = gctoo.data_df.index.isin(rows_to_keep)\n    cols_to_keep_bools = gctoo.data_df.columns.isin(cols_to_keep)\n\n    # Make the output gct\n    out_gctoo = GCToo.GCToo(\n        src=gctoo.src, version=gctoo.version,\n        data_df=gctoo.data_df.loc[rows_to_keep_bools, cols_to_keep_bools],\n        row_metadata_df=gctoo.row_metadata_df.loc[rows_to_keep_bools, :],\n        col_metadata_df=gctoo.col_metadata_df.loc[cols_to_keep_bools, :])\n\n    assert out_gctoo.data_df.size > 0, \"Subsetting yielded an empty gct!\"\n\n    logger.info((\"Initial GCToo with {} rows and {} columns subsetted down to \" +\n                 \"{} rows and {} columns.\").format(\n                      gctoo.data_df.shape[0], gctoo.data_df.shape[1],\n                      out_gctoo.data_df.shape[0], out_gctoo.data_df.shape[1]))\n\n    return out_gctoo"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfigure out which rows to keep in the current tree tree.", "response": "def get_rows_to_keep(gctoo, rid=None, row_bool=None, ridx=None, exclude_rid=None):\n    \"\"\" Figure out based on the possible row inputs which rows to keep.\n\n    Args:\n        gctoo (GCToo object):\n        rid (list of strings):\n        row_bool (boolean array):\n        ridx (list of integers):\n        exclude_rid (list of strings):\n\n    Returns:\n        rows_to_keep (list of strings): row ids to be kept\n\n    \"\"\"\n    # Use rid if provided\n    if rid is not None:\n        assert type(rid) == list, \"rid must be a list. rid: {}\".format(rid)\n\n        rows_to_keep = [gctoo_row for gctoo_row in gctoo.data_df.index if gctoo_row in rid]\n\n        # Tell user if some rids not found\n        num_missing_rids = len(rid) - len(rows_to_keep)\n        if num_missing_rids != 0:\n            logger.info(\"{} rids were not found in the GCT.\".format(num_missing_rids))\n\n    # Use row_bool if provided\n    elif row_bool is not None:\n\n        assert len(row_bool) == gctoo.data_df.shape[0], (\n            \"row_bool must have length equal to gctoo.data_df.shape[0]. \" +\n            \"len(row_bool): {}, gctoo.data_df.shape[0]: {}\".format(\n                len(row_bool), gctoo.data_df.shape[0]))\n        rows_to_keep = gctoo.data_df.index[row_bool].values\n\n    # Use ridx if provided\n    elif ridx is not None:\n\n        assert type(ridx[0]) is int, (\n            \"ridx must be a list of integers. ridx[0]: {}, \" +\n            \"type(ridx[0]): {}\").format(ridx[0], type(ridx[0]))\n\n        assert max(ridx) <= gctoo.data_df.shape[0], (\n            \"ridx contains an integer larger than the number of rows in \" +\n            \"the GCToo. max(ridx): {}, gctoo.data_df.shape[0]: {}\").format(\n                max(ridx), gctoo.data_df.shape[0])\n\n        rows_to_keep = gctoo.data_df.index[ridx].values\n\n    # If rid, row_bool, and ridx are all None, return all rows\n    else:\n        rows_to_keep = gctoo.data_df.index.values\n\n    # Use exclude_rid if provided\n    if exclude_rid is not None:\n\n        # Keep only those rows that are not in exclude_rid\n        rows_to_keep = [row_to_keep for row_to_keep in rows_to_keep if row_to_keep not in exclude_rid]\n\n    return rows_to_keep"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfigure out which columns to keep in the current tree tree.", "response": "def get_cols_to_keep(gctoo, cid=None, col_bool=None, cidx=None, exclude_cid=None):\n    \"\"\" Figure out based on the possible columns inputs which columns to keep.\n\n    Args:\n        gctoo (GCToo object):\n        cid (list of strings):\n        col_bool (boolean array):\n        cidx (list of integers):\n        exclude_cid (list of strings):\n\n    Returns:\n        cols_to_keep (list of strings): col ids to be kept\n\n    \"\"\"\n\n    # Use cid if provided\n    if cid is not None:\n        assert type(cid) == list, \"cid must be a list. cid: {}\".format(cid)\n\n        cols_to_keep = [gctoo_col for gctoo_col in gctoo.data_df.columns if gctoo_col in cid]\n\n        # Tell user if some cids not found\n        num_missing_cids = len(cid) - len(cols_to_keep)\n        if num_missing_cids != 0:\n            logger.info(\"{} cids were not found in the GCT.\".format(num_missing_cids))\n\n    # Use col_bool if provided\n    elif col_bool is not None:\n\n        assert len(col_bool) == gctoo.data_df.shape[1], (\n            \"col_bool must have length equal to gctoo.data_df.shape[1]. \" +\n            \"len(col_bool): {}, gctoo.data_df.shape[1]: {}\".format(\n                len(col_bool), gctoo.data_df.shape[1]))\n        cols_to_keep = gctoo.data_df.columns[col_bool].values\n\n    # Use cidx if provided\n    elif cidx is not None:\n\n        assert type(cidx[0]) is int, (\n            \"cidx must be a list of integers. cidx[0]: {}, \" +\n            \"type(cidx[0]): {}\").format(cidx[0], type(cidx[0]))\n\n        assert max(cidx) <= gctoo.data_df.shape[1], (\n            \"cidx contains an integer larger than the number of columns in \" +\n            \"the GCToo. max(cidx): {}, gctoo.data_df.shape[1]: {}\").format(\n                max(cidx), gctoo.data_df.shape[1])\n\n        cols_to_keep = gctoo.data_df.columns[cidx].values\n\n    # If cid, col_bool, and cidx are all None, return all columns\n    else:\n        cols_to_keep = gctoo.data_df.columns.values\n\n    # Use exclude_cid if provided\n    if exclude_cid is not None:\n\n        # Keep only those columns that are not in exclude_cid\n        cols_to_keep = [col_to_keep for col_to_keep in cols_to_keep if col_to_keep not in exclude_cid]\n\n    return cols_to_keep"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a grp file at the path specified by in_path.", "response": "def read(in_path):\n    \"\"\" Read a grp file at the path specified by in_path.\n\n    Args:\n        in_path (string): path to GRP file\n\n    Returns:\n        grp (list)\n\n    \"\"\"\n    assert os.path.exists(in_path), \"The following GRP file can't be found. in_path: {}\".format(in_path)\n\n    with open(in_path, \"r\") as f:\n        lines = f.readlines()\n        # need the second conditional to ignore comment lines\n        grp = [line.strip() for line in lines if line and not re.match(\"^#\", line)]\n\n    return grp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(grp, out_path):\n    with open(out_path, \"w\") as f:\n        for x in grp:\n            f.write(str(x) + \"\\n\")", "response": "Writes a GRP object to a new - line delimited text file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the pearson correlation matrix between the columns of x and y or optionally the pearson correlation matrix between the columns of x and y. If destination is provided the pearson correlation matrix is stored in the language of statistics.", "response": "def fast_corr(x, y=None, destination=None):\n    \"\"\"calculate the pearson correlation matrix for the columns of x (with dimensions MxN), or optionally, the pearson correlaton matrix\n    between x and y (with dimensions OxP).  If destination is provided, put the results there.  \n    In the language of statistics the columns are the variables and the rows are the observations.\n\n    Args:\n        x (numpy array-like) MxN in shape\n        y (optional, numpy array-like) OxP in shape.  M (# rows in x) must equal O (# rows in y)\n        destination (numpy array-like) optional location where to store the results as they are calculated (e.g. a numpy\n            memmap of a file)\n\n        returns (numpy array-like) array of the covariance values\n            for defaults (y=None), shape is NxN\n            if y is provied, shape is NxP\n    \"\"\"\n    if y is None:\n        y = x\n\n    r = fast_cov.fast_cov(x, y, destination)\n\n    std_x = numpy.std(x, axis=0, ddof=1)\n    std_y = numpy.std(y, axis=0, ddof=1)\n\n    numpy.divide(r, std_x[:, numpy.newaxis], out=r)\n    numpy.divide(r, std_y[numpy.newaxis, :], out=r)\n\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_specified_size_gctoo(og_gctoo, num_entries, dim):\n    assert dim in [\"row\", \"col\"], \"dim specified must be either 'row' or 'col'\"\n\n    dim_index = 0 if \"row\" == dim else 1\n    assert num_entries <= og_gctoo.data_df.shape[dim_index], (\"number of entries must be smaller than dimension being \"\n            \"subsetted - num_entries:  {}  dim:  {}  dim_index:  {}  og_gctoo.data_df.shape[dim_index]:  {}\".format(\n        num_entries, dim, dim_index, og_gctoo.data_df.shape[dim_index]))\n\n    if dim == \"col\":\n        columns = [x for x in og_gctoo.data_df.columns.values]\n        numpy.random.shuffle(columns)\n        columns = columns[0:num_entries]\n        rows = og_gctoo.data_df.index.values\n    else:\n        rows = [x for x in og_gctoo.data_df.index.values]\n        numpy.random.shuffle(rows)\n        rows = rows[0:num_entries]\n        columns = og_gctoo.data_df.columns.values\n\n    new_data_df = og_gctoo.data_df.loc[rows, columns]\n    new_row_meta = og_gctoo.row_metadata_df.loc[rows]\n    new_col_meta = og_gctoo.col_metadata_df.loc[columns]\n\n    logger.debug(\n        \"after slice - new_col_meta.shape: {}  new_row_meta.shape:  {}\".format(new_col_meta.shape, new_row_meta.shape))\n\n    # make & return new gctoo instance\n    new_gctoo = GCToo.GCToo(data_df=new_data_df, row_metadata_df=new_row_meta, col_metadata_df=new_col_meta)\n\n    return new_gctoo", "response": "This function creates a new GCToo instance with the specified size."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_filter_query(self, resource_name, filter_clause):\n        url = self.base_url + \"/\" + resource_name\n        params = {\"filter\":json.dumps(filter_clause)}\n\n        r = requests.get(url, headers=self.headers, params=params)\n        logger.debug(\"requests.get result r.status_code:  {}\".format(r.status_code))\n\n        ClueApiClient._check_request_response(r)\n\n        return r.json()", "response": "run a query against the API and user key fields of self and the fitler_clause provided\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a GCToo instance to an HDF5 file.", "response": "def write(gctoo_object, out_file_name, convert_back_to_neg_666=True, gzip_compression_level=6,\n    max_chunk_kb=1024, matrix_dtype=numpy.float32):\n    \"\"\"\n\tWrites a GCToo instance to specified file.\n\n\tInput:\n\t\t- gctoo_object (GCToo): A GCToo instance.\n\t\t- out_file_name (str): file name to write gctoo_object to.\n        - convert_back_to_neg_666 (bool): whether to convert np.NAN in metadata back to \"-666\"\n        - gzip_compression_level (int, default=6): Compression level to use for metadata. \n        - max_chunk_kb (int, default=1024): The maximum number of KB a given chunk will occupy\n        - matrix_dtype (numpy dtype, default=numpy.float32): Storage data type for data matrix. \n\t\"\"\"\n    # make sure out file has a .gctx suffix\n    gctx_out_name = add_gctx_to_out_name(out_file_name)\n\n    # open an hdf5 file to write to\n    hdf5_out = h5py.File(gctx_out_name, \"w\")\n\n    # write version\n    write_version(hdf5_out)\n\n    # write src\n    write_src(hdf5_out, gctoo_object, gctx_out_name)\n\n    # set chunk size for data matrix\n    elem_per_kb = calculate_elem_per_kb(max_chunk_kb, matrix_dtype)\n    chunk_size = set_data_matrix_chunk_size(gctoo_object.data_df.shape, max_chunk_kb, elem_per_kb)\n\n    # write data matrix\n    hdf5_out.create_dataset(data_matrix_node, data=gctoo_object.data_df.transpose().values,\n        dtype=matrix_dtype)\n\n    # write col metadata\n    write_metadata(hdf5_out, \"col\", gctoo_object.col_metadata_df, convert_back_to_neg_666,\n        gzip_compression=gzip_compression_level)\n\n    # write row metadata\n    write_metadata(hdf5_out, \"row\", gctoo_object.row_metadata_df, convert_back_to_neg_666,\n        gzip_compression=gzip_compression_level)\n\n    # close gctx file\n    hdf5_out.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_src(hdf5_out, gctoo_object, out_file_name):\n    if gctoo_object.src == None:\n        hdf5_out.attrs[src_attr] = out_file_name\n    else:\n        hdf5_out.attrs[src_attr] = gctoo_object.src", "response": "Writes the source attribute of the current object to the hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_elem_per_kb(max_chunk_kb, matrix_dtype):\n    if matrix_dtype == numpy.float32:\n        return (max_chunk_kb * 8)/32\n    elif matrix_dtype == numpy.float64:\n        return (max_chunk_kb * 8)/64\n    else:\n        msg = \"Invalid matrix_dtype: {}; only numpy.float32 and numpy.float64 are currently supported\".format(matrix_dtype)\n        logger.error(msg)\n        raise Exception(\"write_gctx.calculate_elem_per_kb \" + msg)", "response": "Calculates the number of elements per kb for a given max_chunk_kb and matrix_dtype."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the chunk size for writing a data matrix.", "response": "def set_data_matrix_chunk_size(df_shape, max_chunk_kb, elem_per_kb):\n    \"\"\"\n    Sets chunk size to use for writing data matrix. \n    Note. Calculation used here is for compatibility with cmapM and cmapR. \n\n    Input:\n        - df_shape (tuple): shape of input data_df. \n        - max_chunk_kb (int, default=1024): The maximum number of KB a given chunk will occupy\n        - elem_per_kb (int): Number of elements per kb \n\n    Returns:\n        chunk size (tuple) to use for chunking the data matrix \n    \"\"\" \n    row_chunk_size = min(df_shape[0], 1000)\n    col_chunk_size = min(((max_chunk_kb*elem_per_kb)//row_chunk_size), df_shape[1])\n    return (row_chunk_size, col_chunk_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_metadata(hdf5_out, dim, metadata_df, convert_back_to_neg_666, gzip_compression):\n    if dim == \"col\":\n        hdf5_out.create_group(col_meta_group_node)\n        metadata_node_name = col_meta_group_node\n    elif dim == \"row\":\n        hdf5_out.create_group(row_meta_group_node)\n        metadata_node_name = row_meta_group_node\n    else:\n        logger.error(\"'dim' argument must be either 'row' or 'col'!\")\n\n    # write id field to expected node\n    hdf5_out.create_dataset(metadata_node_name + \"/id\", data=[numpy.string_(x) for x in metadata_df.index],\n        compression=gzip_compression)\n\n    metadata_fields = list(metadata_df.columns.copy())\n\n    # if specified, convert numpy.nans in metadata back to -666\n    if convert_back_to_neg_666:\n        for c in metadata_fields:\n            metadata_df[[c]] = metadata_df[[c]].replace([numpy.nan], [\"-666\"])\n\n    # write metadata columns to their own arrays\n    for field in [entry for entry in metadata_fields if entry != \"ind\"]:\n        if numpy.array(metadata_df.loc[:, field]).dtype.type in (numpy.str_, numpy.object_):\n            array_write = numpy.array(metadata_df.loc[:, field]).astype('S')\n        else:\n            array_write = numpy.array(metadata_df.loc[:, field])\n        hdf5_out.create_dataset(metadata_node_name + \"/\" + field,\n                                data=array_write,\n                                compression=gzip_compression)", "response": "Writes metadata to proper node of gctx out."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a lazy user. Returns a 2 - tuple of the underlying User objects and the username.", "response": "def create_lazy_user(self):\n        \"\"\" Create a lazy user. Returns a 2-tuple of the underlying User\n        object (which may be of a custom class), and the username.\n        \"\"\"\n        user_class = self.model.get_user_class()\n        username = self.generate_username(user_class)\n        user = user_class.objects.create_user(username, '')\n        self.create(user=user)\n        return user, username"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a lazy user to a non - lazy one.", "response": "def convert(self, form):\n        \"\"\" Convert a lazy user to a non-lazy one. The form passed\n        in is expected to be a ModelForm instance, bound to the user\n        to be converted.\n\n        The converted ``User`` object is returned.\n\n        Raises a TypeError if the user is not lazy.\n        \"\"\"\n        if not is_lazy_user(form.instance):\n            raise NotLazyError('You cannot convert a non-lazy user')\n\n        user = form.save()\n\n        # We need to remove the LazyUser instance assocated with the\n        # newly-converted user\n        self.filter(user=user).delete()\n        converted.send(self, user=user)\n        return user"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_username(self, user_class):\n        m = getattr(user_class, 'generate_username', None)\n        if m:\n            return m()\n        else:\n            max_length = user_class._meta.get_field(\n                self.username_field).max_length\n            return uuid.uuid4().hex[:max_length]", "response": "Generate a new username for a user"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a temporary user to a real one.", "response": "def convert(request, form_class=None,\n            redirect_field_name='redirect_to',\n            anonymous_redirect=settings.LOGIN_URL,\n            template_name='lazysignup/convert.html',\n            ajax_template_name='lazysignup/convert_ajax.html'):\n    \"\"\" Convert a temporary user to a real one. Reject users who don't\n    appear to be temporary users (ie. they have a usable password)\n    \"\"\"\n    redirect_to = 'lazysignup_convert_done'\n\n    if form_class is None:\n        if constants.LAZYSIGNUP_CUSTOM_USER_CREATION_FORM is not None:\n            form_class = import_string(constants.LAZYSIGNUP_CUSTOM_USER_CREATION_FORM)\n        else:\n            form_class = UserCreationForm\n\n    # If we've got an anonymous user, redirect to login\n    if request.user.is_anonymous:\n        return HttpResponseRedirect(anonymous_redirect)\n\n    if request.method == 'POST':\n        redirect_to = request.POST.get(redirect_field_name) or redirect_to\n        form = form_class(request.POST, instance=request.user)\n        if form.is_valid():\n            try:\n                LazyUser.objects.convert(form)\n            except NotLazyError:\n                # If the user already has a usable password, return a Bad\n                # Request to an Ajax client, or just redirect back for a\n                # regular client.\n                if request.is_ajax():\n                    return HttpResponseBadRequest(\n                        content=_(u\"Already converted.\"))\n                else:\n                    return redirect(redirect_to)\n\n            # Re-log the user in, as they'll now not be authenticatable with\n            # the Lazy backend\n            login(request, authenticate(**form.get_credentials()))\n\n            # If we're being called via AJAX, then we just return a 200\n            # directly to the client. If not, then we redirect to a\n            # confirmation page or to redirect_to, if it's set.\n            if request.is_ajax():\n                return HttpResponse()\n            else:\n                return redirect(redirect_to)\n\n        # Invalid form, now check to see if is an ajax call\n        if request.is_ajax():\n            return HttpResponseBadRequest(content=str(form.errors))\n    else:\n        form = form_class()\n\n    # If this is an ajax request, prepend the ajax template to the list of\n    # templates to be searched.\n    if request.is_ajax():\n        template_name = [ajax_template_name, template_name]\n    return render(\n        request,\n        template_name,\n        {\n            'form': form,\n            'redirect_to': redirect_to\n        },\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the passed user is a lazy user.", "response": "def is_lazy_user(user):\n    \"\"\" Return True if the passed user is a lazy user. \"\"\"\n    # Anonymous users are not lazy.\n\n    if user.is_anonymous:\n        return False\n\n    # Check the user backend. If the lazy signup backend\n    # authenticated them, then the user is lazy.\n    backend = getattr(user, 'backend', None)\n    if backend == 'lazysignup.backends.LazySignupBackend':\n        return True\n\n    # Otherwise, we have to fall back to checking the database.\n    from lazysignup.models import LazyUser\n    return bool(LazyUser.objects.filter(user=user).count() > 0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(queue_name, payload=None, content_type=None, source=None, task_id=None,\n        build_id=None, release_id=None, run_id=None):\n    \"\"\"Adds a work item to a queue.\n\n    Args:\n        queue_name: Name of the queue to add the work item to.\n        payload: Optional. Payload that describes the work to do as a string.\n            If not a string and content_type is not provided, then this\n            function assumes the payload is a JSON-able Python object.\n        content_type: Optional. Content type of the payload.\n        source: Optional. Who or what originally created the task.\n        task_id: Optional. When supplied, only enqueue this task if a task\n            with this ID does not already exist. If a task with this ID already\n            exists, then this function will do nothing.\n        build_id: Build ID to associate with this task. May be None.\n        release_id: Release ID to associate with this task. May be None.\n        run_id: Run ID to associate with this task. May be None.\n\n    Returns:\n        ID of the task that was added.\n    \"\"\"\n    if task_id:\n        task = WorkQueue.query.filter_by(task_id=task_id).first()\n        if task:\n            return task.task_id\n    else:\n        task_id = uuid.uuid4().hex\n\n    if payload and not content_type and not isinstance(payload, basestring):\n        payload = json.dumps(payload)\n        content_type = 'application/json'\n\n    now = datetime.datetime.utcnow()\n    task = WorkQueue(\n        task_id=task_id,\n        queue_name=queue_name,\n        eta=now,\n        source=source,\n        build_id=build_id,\n        release_id=release_id,\n        run_id=run_id,\n        payload=payload,\n        content_type=content_type)\n    db.session.add(task)\n\n    return task.task_id", "response": "Adds a work item to a queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _task_to_dict(task):\n    payload = task.payload\n    if payload and task.content_type == 'application/json':\n        payload = json.loads(payload)\n\n    return dict(\n        task_id=task.task_id,\n        queue_name=task.queue_name,\n        eta=_datetime_to_epoch_seconds(task.eta),\n        source=task.source,\n        created=_datetime_to_epoch_seconds(task.created),\n        lease_attempts=task.lease_attempts,\n        last_lease=_datetime_to_epoch_seconds(task.last_lease),\n        payload=payload,\n        content_type=task.content_type)", "response": "Converts a WorkQueue to a JSON - able dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nleases a work item from a queue.", "response": "def lease(queue_name, owner, count=1, timeout_seconds=60):\n    \"\"\"Leases a work item from a queue, usually the oldest task available.\n\n    Args:\n        queue_name: Name of the queue to lease work from.\n        owner: Who or what is leasing the task.\n        count: Lease up to this many tasks. Return value will never have more\n            than this many items present.\n        timeout_seconds: Number of seconds to lock the task for before\n            allowing another owner to lease it.\n\n    Returns:\n        List of dictionaries representing the task that was leased, or\n        an empty list if no tasks are available to be leased.\n    \"\"\"\n    now = datetime.datetime.utcnow()\n    query = (\n        WorkQueue.query\n        .filter_by(queue_name=queue_name, status=WorkQueue.LIVE)\n        .filter(WorkQueue.eta <= now)\n        .order_by(WorkQueue.eta)\n        .with_lockmode('update')\n        .limit(count))\n\n    task_list = query.all()\n    if not task_list:\n        return None\n\n    next_eta = now + datetime.timedelta(seconds=timeout_seconds)\n\n    for task in task_list:\n        task.eta = next_eta\n        task.lease_attempts += 1\n        task.last_owner = owner\n        task.last_lease = now\n        task.heartbeat = None\n        task.heartbeat_number = 0\n        db.session.add(task)\n\n    return [_task_to_dict(task) for task in task_list]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_task_with_policy(queue_name, task_id, owner):\n    now = datetime.datetime.utcnow()\n    task = (\n        WorkQueue.query\n        .filter_by(queue_name=queue_name, task_id=task_id)\n        .with_lockmode('update')\n        .first())\n    if not task:\n        raise TaskDoesNotExistError('task_id=%r' % task_id)\n\n    # Lease delta should be positive, meaning it has not yet expired!\n    lease_delta = now - task.eta\n    if lease_delta > datetime.timedelta(0):\n        db.session.rollback()\n        raise LeaseExpiredError('queue=%r, task_id=%r expired %s' % (\n                                task.queue_name, task_id, lease_delta))\n\n    if task.last_owner != owner:\n        db.session.rollback()\n        raise NotOwnerError('queue=%r, task_id=%r, owner=%r' % (\n                            task.queue_name, task_id, task.last_owner))\n\n    return task", "response": "Fetches the specified task and enforces ownership policy."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the heartbeat status of the task and extends its lease.", "response": "def heartbeat(queue_name, task_id, owner, message, index):\n    \"\"\"Sets the heartbeat status of the task and extends its lease.\n\n    The task's lease is extended by the same amount as its last lease to\n    ensure that any operations following the heartbeat will still hold the\n    lock for the original lock period.\n\n    Args:\n        queue_name: Name of the queue the work item is on.\n        task_id: ID of the task that is finished.\n        owner: Who or what has the current lease on the task.\n        message: Message to report as the task's current status.\n        index: Number of this message in the sequence of messages from the\n            current task owner, starting at zero. This lets the API receive\n            heartbeats out of order, yet ensure that the most recent message\n            is actually saved to the database. This requires the owner issuing\n            heartbeat messages to issue heartbeat indexes sequentially.\n\n    Returns:\n        True if the heartbeat message was set, False if it is lower than the\n        current heartbeat index.\n\n    Raises:\n        TaskDoesNotExistError if the task does not exist.\n        LeaseExpiredError if the lease is no longer active.\n        NotOwnerError if the specified owner no longer owns the task.\n    \"\"\"\n    task = _get_task_with_policy(queue_name, task_id, owner)\n    if task.heartbeat_number > index:\n        return False\n\n    task.heartbeat = message\n    task.heartbeat_number = index\n\n    # Extend the lease by the time of the last lease.\n    now = datetime.datetime.utcnow()\n    timeout_delta = task.eta - task.last_lease\n    task.eta = now + timeout_delta\n    task.last_lease = now\n\n    db.session.add(task)\n\n    signals.task_updated.send(app, task=task)\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef finish(queue_name, task_id, owner, error=False):\n    task = _get_task_with_policy(queue_name, task_id, owner)\n\n    if not task.status == WorkQueue.LIVE:\n        logging.warning('Finishing already dead task. queue=%r, task_id=%r, '\n                        'owner=%r, status=%r',\n                        task.queue_name, task_id, owner, task.status)\n        return False\n\n    if not error:\n        task.status = WorkQueue.DONE\n    else:\n        task.status = WorkQueue.ERROR\n\n    task.finished = datetime.datetime.utcnow()\n    db.session.add(task)\n\n    signals.task_updated.send(app, task=task)\n\n    return True", "response": "Marks a task on a queue as finished."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nquery for work items based on their criteria.", "response": "def _query(queue_name=None, build_id=None, release_id=None, run_id=None,\n           count=None):\n    \"\"\"Queries for work items based on their criteria.\n\n    Args:\n        queue_name: Optional queue name to restrict to.\n        build_id: Optional build ID to restrict to.\n        release_id: Optional release ID to restrict to.\n        run_id: Optional run ID to restrict to.\n        count: How many tasks to fetch. Defaults to None, which means all\n            tasks are fetch that match the query.\n\n    Returns:\n        List of WorkQueue items.\n    \"\"\"\n    assert queue_name or build_id or release_id or run_id\n\n    q = WorkQueue.query\n    if queue_name:\n        q = q.filter_by(queue_name=queue_name)\n    if build_id:\n        q = q.filter_by(build_id=build_id)\n    if release_id:\n        q = q.filter_by(release_id=release_id)\n    if run_id:\n        q = q.filter_by(run_id=run_id)\n\n    q = q.order_by(WorkQueue.created.desc())\n\n    if count is not None:\n        q = q.limit(count)\n\n    return q.all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries for work items based on their criteria.", "response": "def query(**kwargs):\n    \"\"\"Queries for work items based on their criteria.\n\n    Args:\n        queue_name: Optional queue name to restrict to.\n        build_id: Optional build ID to restrict to.\n        release_id: Optional release ID to restrict to.\n        run_id: Optional run ID to restrict to.\n        count: How many tasks to fetch. Defaults to None, which means all\n            tasks are fetch that match the query.\n\n    Returns:\n        Dictionaries of the most recent tasks that match the criteria, in\n        order of most recently created. When count is 1 the return value will\n        be the most recent task or None. When count is not 1 the return value\n        will be a  list of tasks.\n    \"\"\"\n    count = kwargs.get('count', None)\n    task_list = _query(**kwargs)\n    task_dict_list = [_task_to_dict(task) for task in task_list]\n\n    if count == 1:\n        if not task_dict_list:\n            return None\n        else:\n            return task_dict_list[0]\n\n    return task_dict_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncancel work items based on their criteria.", "response": "def cancel(**kwargs):\n    \"\"\"Cancels work items based on their criteria.\n\n    Args:\n        **kwargs: Same parameters as the query() method.\n\n    Returns:\n        The number of tasks that were canceled.\n    \"\"\"\n    task_list = _query(**kwargs)\n    for task in task_list:\n        task.status = WorkQueue.CANCELED\n        task.finished = datetime.datetime.utcnow()\n        db.session.add(task)\n    return len(task_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handle_add(queue_name):\n    source = request.form.get('source', request.remote_addr, type=str)\n    try:\n        task_id = work_queue.add(\n            queue_name,\n            payload=request.form.get('payload', type=str),\n            content_type=request.form.get('content_type', type=str),\n            source=source,\n            task_id=request.form.get('task_id', type=str))\n    except work_queue.Error, e:\n        return utils.jsonify_error(e)\n\n    db.session.commit()\n    logging.info('Task added: queue=%r, task_id=%r, source=%r',\n                 queue_name, task_id, source)\n    return flask.jsonify(task_id=task_id)", "response": "Adds a task to a queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlease a task from a queue.", "response": "def handle_lease(queue_name):\n    \"\"\"Leases a task from a queue.\"\"\"\n    owner = request.form.get('owner', request.remote_addr, type=str)\n    try:\n        task_list = work_queue.lease(\n            queue_name,\n            owner,\n            request.form.get('count', 1, type=int),\n            request.form.get('timeout', 60, type=int))\n    except work_queue.Error, e:\n        return utils.jsonify_error(e)\n\n    if not task_list:\n        return flask.jsonify(tasks=[])\n\n    db.session.commit()\n    task_ids = [t['task_id'] for t in task_list]\n    logging.debug('Task leased: queue=%r, task_ids=%r, owner=%r',\n                  queue_name, task_ids, owner)\n    return flask.jsonify(tasks=task_list)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_heartbeat(queue_name):\n    task_id = request.form.get('task_id', type=str)\n    message = request.form.get('message', type=str)\n    index = request.form.get('index', type=int)\n    try:\n        work_queue.heartbeat(\n            queue_name,\n            task_id,\n            request.form.get('owner', request.remote_addr, type=str),\n            message,\n            index)\n    except work_queue.Error, e:\n        return utils.jsonify_error(e)\n\n    db.session.commit()\n    logging.debug('Task heartbeat: queue=%r, task_id=%r, message=%r, index=%d',\n                  queue_name, task_id, message, index)\n    return flask.jsonify(success=True)", "response": "Updates the heartbeat message for a task."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_finish(queue_name):\n    task_id = request.form.get('task_id', type=str)\n    owner = request.form.get('owner', request.remote_addr, type=str)\n    error = request.form.get('error', type=str) is not None\n    try:\n        work_queue.finish(queue_name, task_id, owner, error=error)\n    except work_queue.Error, e:\n        return utils.jsonify_error(e)\n\n    db.session.commit()\n    logging.debug('Task finished: queue=%r, task_id=%r, owner=%r, error=%r',\n                  queue_name, task_id, owner, error)\n    return flask.jsonify(success=True)", "response": "Marks a task on a queue as finished."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npage for viewing the index of all active work queues.", "response": "def view_all_work_queues():\n    \"\"\"Page for viewing the index of all active work queues.\"\"\"\n    count_list = list(\n        db.session.query(\n            work_queue.WorkQueue.queue_name,\n            work_queue.WorkQueue.status,\n            func.count(work_queue.WorkQueue.task_id))\n        .group_by(work_queue.WorkQueue.queue_name,\n                  work_queue.WorkQueue.status))\n\n    queue_dict = {}\n    for name, status, count in count_list:\n        queue_dict[(name, status)] = dict(\n            name=name, status=status, count=count)\n\n    max_created_list = list(\n        db.session.query(\n            work_queue.WorkQueue.queue_name,\n            work_queue.WorkQueue.status,\n            func.max(work_queue.WorkQueue.created))\n        .group_by(work_queue.WorkQueue.queue_name,\n                  work_queue.WorkQueue.status))\n\n    for name, status, newest_created in max_created_list:\n        queue_dict[(name, status)]['newest_created'] = newest_created\n\n    min_eta_list = list(\n        db.session.query(\n            work_queue.WorkQueue.queue_name,\n            work_queue.WorkQueue.status,\n            func.min(work_queue.WorkQueue.eta))\n        .group_by(work_queue.WorkQueue.queue_name,\n                  work_queue.WorkQueue.status))\n\n    for name, status, oldest_eta in min_eta_list:\n        queue_dict[(name, status)]['oldest_eta'] = oldest_eta\n\n    queue_list = list(queue_dict.values())\n    queue_list.sort(key=lambda x: (x['name'], x['status']))\n\n    context = dict(\n        queue_list=queue_list,\n    )\n    return render_template('view_work_queue_index.html', **context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef manage_work_queue(queue_name):\n    modify_form = forms.ModifyWorkQueueTaskForm()\n    if modify_form.validate_on_submit():\n        primary_key = (modify_form.task_id.data, queue_name)\n        task = work_queue.WorkQueue.query.get(primary_key)\n        if task:\n            logging.info('Action: %s task_id=%r',\n                         modify_form.action.data, modify_form.task_id.data)\n            if modify_form.action.data == 'retry':\n                task.status = work_queue.WorkQueue.LIVE\n                task.lease_attempts = 0\n                task.heartbeat = 'Retrying ...'\n                db.session.add(task)\n            else:\n                db.session.delete(task)\n            db.session.commit()\n        else:\n            logging.warning('Could not find task_id=%r to delete',\n                            modify_form.task_id.data)\n        return redirect(url_for('manage_work_queue', queue_name=queue_name))\n\n    query = (\n        work_queue.WorkQueue.query\n        .filter_by(queue_name=queue_name)\n        .order_by(work_queue.WorkQueue.created.desc()))\n\n    status = request.args.get('status', '', type=str).lower()\n    if status in work_queue.WorkQueue.STATES:\n        query = query.filter_by(status=status)\n    else:\n        status = None\n\n    item_list = list(query.limit(100))\n    work_list = []\n    for item in item_list:\n        form = forms.ModifyWorkQueueTaskForm()\n        form.task_id.data = item.task_id\n        form.delete.data = True\n        work_list.append((item, form))\n\n    context = dict(\n        queue_name=queue_name,\n        status=status,\n        work_list=work_list,\n    )\n    return render_template('view_work_queue.html', **context)", "response": "Page for viewing the contents of a work queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retryable_transaction(attempts=3, exceptions=(OperationalError,)):\n    assert len(exceptions) > 0\n    assert attempts > 0\n\n    def wrapper(f):\n        @functools.wraps(f)\n        def wrapped(*args, **kwargs):\n            for i in xrange(attempts):\n                try:\n                    return f(*args, **kwargs)\n                except exceptions, e:\n                    if i == (attempts - 1):\n                        raise\n                    logging.warning(\n                        'Retryable error in transaction on attempt %d. %s: %s',\n                        i + 1, e.__class__.__name__, e)\n                    db.session.rollback()\n\n        return wrapped\n\n    return wrapper", "response": "Decorator retries a function when expected exceptions are raised."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef jsonify_assert(asserted, message, status_code=400):\n    if asserted:\n        return\n    try:\n        raise AssertionError(message)\n    except AssertionError, e:\n        stack = traceback.extract_stack()\n        stack.pop()\n        logging.error('Assertion failed: %s\\n%s',\n                      str(e), ''.join(traceback.format_list(stack)))\n        abort(jsonify_error(e, status_code=status_code))", "response": "Asserts something is true aborts the request if not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef jsonify_error(message_or_exception, status_code=400):\n    if isinstance(message_or_exception, Exception):\n        message = '%s: %s' % (\n            message_or_exception.__class__.__name__, message_or_exception)\n    else:\n        message = message_or_exception\n\n    logging.debug('Returning status=%s, error message: %s',\n                  status_code, message)\n    response = jsonify(error=message)\n    response.status_code = status_code\n    return response", "response": "Returns a JSON payload that indicates the request had an error."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns string representing time since or time until.", "response": "def timesince(when):\n    \"\"\"Returns string representing \"time since\" or \"time until\".\n\n    Examples:\n        3 days ago, 5 hours ago, 3 minutes from now, 5 hours from now, now.\n    \"\"\"\n    if not when:\n        return ''\n\n    now = datetime.datetime.utcnow()\n    if now > when:\n        diff = now - when\n        suffix = 'ago'\n    else:\n        diff = when - now\n        suffix = 'from now'\n\n    periods = (\n        (diff.days / 365, 'year', 'years'),\n        (diff.days / 30, 'month', 'months'),\n        (diff.days / 7, 'week', 'weeks'),\n        (diff.days, 'day', 'days'),\n        (diff.seconds / 3600, 'hour', 'hours'),\n        (diff.seconds / 60, 'minute', 'minutes'),\n        (diff.seconds, 'second', 'seconds'),\n    )\n\n    for period, singular, plural in periods:\n        if period:\n            return '%d %s %s' % (\n                period,\n                singular if period == 1 else plural,\n                suffix)\n\n    return 'now'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef human_uuid():\n    return base64.b32encode(\n        hashlib.sha1(uuid.uuid4().bytes).digest()).lower().strip('=')", "response": "Returns a good UUID for using as a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a unique string represeting the current deployment. Used for busting caches.", "response": "def get_deployment_timestamp():\n    \"\"\"Returns a unique string represeting the current deployment.\n\n    Used for busting caches.\n    \"\"\"\n    # TODO: Support other deployment situations.\n    if os.environ.get('SERVER_SOFTWARE', '').startswith('Google App Engine'):\n        version_id = os.environ.get('CURRENT_VERSION_ID')\n        major_version, timestamp = version_id.split('.', 1)\n        return timestamp\n    return 'test'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nregistering this module as a worker with the given coordinator.", "response": "def register(coordinator):\n    \"\"\"Registers this module as a worker with the given coordinator.\"\"\"\n\n    if FLAGS.phantomjs_script:\n        utils.verify_binary('phantomjs_binary', ['--version'])\n        assert os.path.exists(FLAGS.phantomjs_script)\n    else:\n        utils.verify_binary('capture_binary', ['--version'])\n        assert FLAGS.capture_script\n        assert os.path.exists(FLAGS.capture_script)\n\n    assert FLAGS.capture_threads > 0\n    assert FLAGS.queue_server_prefix\n\n    item = queue_worker.RemoteQueueWorkflow(\n        constants.CAPTURE_QUEUE_NAME,\n        DoCaptureQueueWorkflow,\n        max_tasks=FLAGS.capture_threads,\n        wait_seconds=FLAGS.capture_wait_seconds)\n    item.root = True\n    coordinator.input_queue.put(item)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch the given request by using the local Flask context.", "response": "def fetch_internal(item, request):\n    \"\"\"Fetches the given request by using the local Flask context.\"\"\"\n    # Break client dependence on Flask if internal fetches aren't being used.\n    from flask import make_response\n    from werkzeug.test import EnvironBuilder\n    # Break circular dependencies.\n    from dpxdt.server import app\n\n    # Attempt to create a Flask environment from a urllib2.Request object.\n    environ_base = {\n        'REMOTE_ADDR': '127.0.0.1',\n    }\n\n    # The data object may be a generator from poster.multipart_encode, so we\n    # need to convert that to raw bytes here. Unfortunately EnvironBuilder\n    # only works with the whole request buffered in memory.\n    data = request.get_data()\n    if data and not isinstance(data, str):\n        data = ''.join(list(data))\n\n    builder = EnvironBuilder(\n        path=request.get_selector(),\n        base_url='%s://%s' % (request.get_type(), request.get_host()),\n        method=request.get_method(),\n        data=data,\n        headers=request.header_items(),\n        environ_base=environ_base)\n\n    with app.request_context(builder.get_environ()):\n        response = make_response(app.dispatch_request())\n        LOGGER.info('\"%s\" %s via internal routing',\n                    request.get_selector(), response.status_code)\n        item.status_code = response.status_code\n        item.content_type = response.mimetype\n        if item.result_path:\n            # TODO: Is there a better way to access the response stream?\n            with open(item.result_path, 'wb') as result_file:\n                for piece in response.iter_encoded():\n                    result_file.write(piece)\n        else:\n            item.data = response.get_data()\n\n    return item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_normal(item, request):\n    try:\n        conn = urllib2.urlopen(request, timeout=item.timeout_seconds)\n    except urllib2.HTTPError, e:\n        conn = e\n    except (urllib2.URLError, ssl.SSLError), e:\n        # TODO: Make this status more clear\n        item.status_code = 400\n        return item\n\n    try:\n        item.status_code = conn.getcode()\n        item.content_type = conn.info().gettype()\n        if item.result_path:\n            with open(item.result_path, 'wb') as result_file:\n                shutil.copyfileobj(conn, result_file)\n        else:\n            item.data = conn.read()\n    except socket.timeout, e:\n        # TODO: Make this status more clear\n        item.status_code = 400\n        return item\n    finally:\n        conn.close()\n\n    return item", "response": "Fetches the given request over HTTP."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(coordinator):\n    fetch_queue = Queue.Queue()\n    coordinator.register(FetchItem, fetch_queue)\n    for i in xrange(FLAGS.fetch_threads):\n        coordinator.worker_threads.append(\n            FetchThread(fetch_queue, coordinator.input_queue))", "response": "Registers this module as a worker with the given coordinator."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns de - JSONed data or None if it s a different content type.", "response": "def json(self):\n        \"\"\"Returns de-JSONed data or None if it's a different content type.\"\"\"\n        if self._data_json:\n            return self._data_json\n\n        if not self.data or self.content_type != 'application/json':\n            return None\n\n        self._data_json = json.loads(self.data)\n        return self._data_json"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maybe_imgur(self, path):\n        '''Uploads a file to imgur if requested via command line flags.\n\n        Returns either \"path\" or \"path url\" depending on the course of action.\n        '''\n        if not FLAGS.imgur_client_id:\n            return path\n\n        im = pyimgur.Imgur(FLAGS.imgur_client_id)\n        uploaded_image = im.upload_image(path)\n        return '%s %s' % (path, uploaded_image.link)", "response": "Uploads a file to imgur if requested via command line flags. Returns either path or url depending on the course of action."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncleans the given URL.", "response": "def clean_url(url, force_scheme=None):\n    \"\"\"Cleans the given URL.\"\"\"\n    # URL should be ASCII according to RFC 3986\n    url = str(url)\n    # Collapse ../../ and related\n    url_parts = urlparse.urlparse(url)\n    path_parts = []\n    for part in url_parts.path.split('/'):\n        if part == '.':\n            continue\n        elif part == '..':\n            if path_parts:\n                path_parts.pop()\n        else:\n            path_parts.append(part)\n\n    url_parts = list(url_parts)\n    if force_scheme:\n        url_parts[0] = force_scheme\n    url_parts[2] = '/'.join(path_parts)\n\n    if FLAGS.keep_query_string == False:\n        url_parts[4] = ''    # No query string\n\n    url_parts[5] = ''    # No path\n\n    # Always have a trailing slash\n    if not url_parts[2]:\n        url_parts[2] = '/'\n\n    return urlparse.urlunparse(url_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_urls(url, data, unescape=HTMLParser.HTMLParser().unescape):\n    parts = urlparse.urlparse(url)\n    prefix = '%s://%s' % (parts.scheme, parts.netloc)\n\n    accessed_dir = os.path.dirname(parts.path)\n    if not accessed_dir.endswith('/'):\n        accessed_dir += '/'\n\n    for pattern, replacement in REPLACEMENT_REGEXES:\n        fixed = replacement % {\n            'base': prefix,\n            'accessed_dir': accessed_dir,\n        }\n        data = re.sub(pattern, fixed, data)\n\n    result = set()\n    for match in re.finditer(MAYBE_HTML_URL_REGEX, data):\n        found_url = unescape(match.groupdict()['absurl'])\n        found_url = clean_url(\n            found_url,\n            force_scheme=parts[0])  # Use the main page's scheme\n        result.add(found_url)\n\n    return result", "response": "Extracts the URLs from an HTML document."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prune_urls(url_set, start_url, allowed_list, ignored_list):\n    result = set()\n\n    for url in url_set:\n        allowed = False\n        for allow_url in allowed_list:\n            if url.startswith(allow_url):\n                allowed = True\n                break\n\n        if not allowed:\n            continue\n\n        ignored = False\n        for ignore_url in ignored_list:\n            if url.startswith(ignore_url):\n                ignored = True\n                break\n\n        if ignored:\n            continue\n\n        prefix, suffix = (url.rsplit('.', 1) + [''])[:2]\n        if suffix.lower() in IGNORE_SUFFIXES:\n            continue\n\n        result.add(url)\n\n    return result", "response": "Prunes URLs that should be ignored."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_or_send(func, message):\n    if request.endpoint != func.func_name:\n        mail.send(message)\n\n    if (current_user.is_authenticated() and current_user.superuser):\n        return render_template('debug_email.html', message=message)", "response": "Renders an email message for debugging or actually sends it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an email indicating that the release is ready for review.", "response": "def send_ready_for_review(build_id, release_name, release_number):\n    \"\"\"Sends an email indicating that the release is ready for review.\"\"\"\n    build = models.Build.query.get(build_id)\n\n    if not build.send_email:\n        logging.debug(\n            'Not sending ready for review email because build does not have '\n            'email enabled. build_id=%r', build.id)\n        return\n\n    ops = operations.BuildOps(build_id)\n    release, run_list, stats_dict, _ = ops.get_release(\n        release_name, release_number)\n\n    if not run_list:\n        logging.debug(\n            'Not sending ready for review email because there are '\n            ' no runs. build_id=%r, release_name=%r, release_number=%d',\n            build.id, release.name, release.number)\n        return\n\n    title = '%s: %s - Ready for review' % (build.name, release.name)\n\n    email_body = render_template(\n        'email_ready_for_review.html',\n        build=build,\n        release=release,\n        run_list=run_list,\n        stats_dict=stats_dict)\n\n    recipients = []\n    if build.email_alias:\n        recipients.append(build.email_alias)\n    else:\n        for user in build.owners:\n            recipients.append(user.email_address)\n\n    if not recipients:\n        logging.debug(\n            'Not sending ready for review email because there are no '\n            'recipients. build_id=%r, release_name=%r, release_number=%d',\n            build.id, release.name, release.number)\n        return\n\n    message = Message(title, recipients=recipients)\n    message.html = email_body\n\n    logging.info('Sending ready for review email for build_id=%r, '\n                 'release_name=%r, release_number=%d to %r',\n                 build.id, release.name, release.number, recipients)\n\n    return render_or_send(send_ready_for_review, message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npages for crediting or editing a build.", "response": "def new_build():\n    \"\"\"Page for crediting or editing a build.\"\"\"\n    form = forms.BuildForm()\n    if form.validate_on_submit():\n        build = models.Build()\n        form.populate_obj(build)\n        build.owners.append(current_user)\n\n        db.session.add(build)\n        db.session.flush()\n\n        auth.save_admin_log(build, created_build=True, message=build.name)\n\n        db.session.commit()\n\n        operations.UserOps(current_user.get_id()).evict()\n\n        logging.info('Created build via UI: build_id=%r, name=%r',\n                     build.id, build.name)\n        return redirect(url_for('view_build', id=build.id))\n\n    return render_template(\n        'new_build.html',\n        build_form=form)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npages for viewing all releases in a build.", "response": "def view_build():\n    \"\"\"Page for viewing all releases in a build.\"\"\"\n    build = g.build\n    page_size = min(request.args.get('page_size', 10, type=int), 50)\n    offset = request.args.get('offset', 0, type=int)\n\n    ops = operations.BuildOps(build.id)\n    has_next_page, candidate_list, stats_counts = ops.get_candidates(\n        page_size, offset)\n\n    # Collate by release name, order releases by latest creation. Init stats.\n    release_dict = {}\n    created_dict = {}\n    run_stats_dict = {}\n    for candidate in candidate_list:\n        release_list = release_dict.setdefault(candidate.name, [])\n        release_list.append(candidate)\n        max_created = created_dict.get(candidate.name, candidate.created)\n        created_dict[candidate.name] = max(candidate.created, max_created)\n        run_stats_dict[candidate.id] = dict(\n            runs_total=0,\n            runs_complete=0,\n            runs_successful=0,\n            runs_failed=0,\n            runs_baseline=0,\n            runs_pending=0)\n\n    # Sort each release by candidate number descending\n    for release_list in release_dict.itervalues():\n        release_list.sort(key=lambda x: x.number, reverse=True)\n\n    # Sort all releases by created time descending\n    release_age_list = [\n        (value, key) for key, value in created_dict.iteritems()]\n    release_age_list.sort(reverse=True)\n    release_name_list = [key for _, key in release_age_list]\n\n    # Count totals for each run state within that release.\n    for candidate_id, status, count in stats_counts:\n        stats_dict = run_stats_dict[candidate_id]\n        for key in ops.get_stats_keys(status):\n            stats_dict[key] += count\n\n    return render_template(\n        'view_build.html',\n        build=build,\n        release_name_list=release_name_list,\n        release_dict=release_dict,\n        run_stats_dict=run_stats_dict,\n        has_next_page=has_next_page,\n        current_offset=offset,\n        next_offset=offset + page_size,\n        last_offset=max(0, offset - page_size),\n        page_size=page_size)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npage for viewing all tests in a release.", "response": "def view_release():\n    \"\"\"Page for viewing all tests runs in a release.\"\"\"\n    build = g.build\n    if request.method == 'POST':\n        form = forms.ReleaseForm(request.form)\n    else:\n        form = forms.ReleaseForm(request.args)\n\n    form.validate()\n\n    ops = operations.BuildOps(build.id)\n    release, run_list, stats_dict, approval_log = ops.get_release(\n        form.name.data, form.number.data)\n\n    if not release:\n        abort(404)\n\n    if request.method == 'POST':\n        decision_states = (\n            models.Release.REVIEWING,\n            models.Release.RECEIVING,\n            models.Release.PROCESSING)\n\n        if form.good.data and release.status in decision_states:\n            release.status = models.Release.GOOD\n            auth.save_admin_log(build, release_good=True, release=release)\n        elif form.bad.data and release.status in decision_states:\n            release.status = models.Release.BAD\n            auth.save_admin_log(build, release_bad=True, release=release)\n        elif form.reviewing.data and release.status in (\n                models.Release.GOOD, models.Release.BAD):\n            release.status = models.Release.REVIEWING\n            auth.save_admin_log(build, release_reviewing=True, release=release)\n        else:\n            logging.warning(\n                'Bad state transition for name=%r, number=%r, form=%r',\n                release.name, release.number, form.data)\n            abort(400)\n\n        db.session.add(release)\n        db.session.commit()\n\n        ops.evict()\n\n        return redirect(url_for(\n            'view_release',\n            id=build.id,\n            name=release.name,\n            number=release.number))\n\n    # Update form values for rendering\n    form.good.data = True\n    form.bad.data = True\n    form.reviewing.data = True\n\n    return render_template(\n        'view_release.html',\n        build=build,\n        release=release,\n        run_list=run_list,\n        release_form=form,\n        approval_log=approval_log,\n        stats_dict=stats_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the artifact details for the given run and file_type.", "response": "def _get_artifact_context(run, file_type):\n    \"\"\"Gets the artifact details for the given run and file_type.\"\"\"\n    sha1sum = None\n    image_file = False\n    log_file = False\n    config_file = False\n\n    if request.path == '/image':\n        image_file = True\n        if file_type == 'before':\n            sha1sum = run.ref_image\n        elif file_type == 'diff':\n            sha1sum = run.diff_image\n        elif file_type == 'after':\n            sha1sum = run.image\n        else:\n            abort(400)\n    elif request.path == '/log':\n        log_file = True\n        if file_type == 'before':\n            sha1sum = run.ref_log\n        elif file_type == 'diff':\n            sha1sum = run.diff_log\n        elif file_type == 'after':\n            sha1sum = run.log\n        else:\n            abort(400)\n    elif request.path == '/config':\n        config_file = True\n        if file_type == 'before':\n            sha1sum = run.ref_config\n        elif file_type == 'after':\n            sha1sum = run.config\n        else:\n            abort(400)\n\n    return image_file, log_file, config_file, sha1sum"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npages for viewing before and after for a specific test run.", "response": "def view_run():\n    \"\"\"Page for viewing before/after for a specific test run.\"\"\"\n    build = g.build\n    if request.method == 'POST':\n        form = forms.RunForm(request.form)\n    else:\n        form = forms.RunForm(request.args)\n\n    form.validate()\n\n    ops = operations.BuildOps(build.id)\n    run, next_run, previous_run, approval_log = ops.get_run(\n        form.name.data, form.number.data, form.test.data)\n\n    if not run:\n        abort(404)\n\n    file_type = form.type.data\n    image_file, log_file, config_file, sha1sum = (\n        _get_artifact_context(run, file_type))\n\n    if request.method == 'POST':\n        if form.approve.data and run.status == models.Run.DIFF_FOUND:\n            run.status = models.Run.DIFF_APPROVED\n            auth.save_admin_log(build, run_approved=True, run=run)\n        elif form.disapprove.data and run.status == models.Run.DIFF_APPROVED:\n            run.status = models.Run.DIFF_FOUND\n            auth.save_admin_log(build, run_rejected=True, run=run)\n        else:\n            abort(400)\n\n        db.session.add(run)\n        db.session.commit()\n\n        ops.evict()\n\n        return redirect(url_for(\n            request.endpoint,\n            id=build.id,\n            name=run.release.name,\n            number=run.release.number,\n            test=run.name,\n            type=file_type))\n\n    # Update form values for rendering\n    form.approve.data = True\n    form.disapprove.data = True\n\n    context = dict(\n        build=build,\n        release=run.release,\n        run=run,\n        run_form=form,\n        previous_run=previous_run,\n        next_run=next_run,\n        file_type=file_type,\n        image_file=image_file,\n        log_file=log_file,\n        config_file=config_file,\n        sha1sum=sha1sum,\n        approval_log=approval_log)\n\n    if file_type:\n        template_name = 'view_artifact.html'\n    else:\n        template_name = 'view_run.html'\n\n    response = flask.Response(render_template(template_name, **context))\n\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register(coordinator):\n    timer_queue = Queue.Queue()\n    coordinator.register(TimerItem, timer_queue)\n    coordinator.worker_threads.append(\n        TimerThread(timer_queue, coordinator.input_queue))", "response": "Registers this module as a worker with the given coordinator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_coordinator():\n    workflow_queue = Queue.Queue()\n    complete_queue = Queue.Queue()\n    coordinator = WorkflowThread(workflow_queue, complete_queue)\n    coordinator.register(WorkflowItem, workflow_queue)\n    return coordinator", "response": "Creates a coordinator and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints this WorkItem to the given stack depth.", "response": "def _print_repr(self, depth):\n        \"\"\"Print this WorkItem to the given stack depth.\n\n        The depth parameter ensures that we can print WorkItems in\n        arbitrarily long chains without hitting the max stack depth.\n        This can happen with WaitForUrlWorkflowItems, which\n        create long chains of small waits.\n        \"\"\"\n        if depth <= 0:\n            return '%s.%s#%d' % (\n                self.__class__.__module__,\n                self.__class__.__name__,\n                id(self))\n\n        return '%s.%s(%s)#%d' % (\n            self.__class__.__module__,\n            self.__class__.__name__,\n            self._print_tree(self._get_dict_for_repr(), depth - 1),\n            id(self))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the error for this barrier and all work items.", "response": "def error(self):\n        \"\"\"Returns the error for this barrier and all work items, if any.\"\"\"\n        # Copy the error from any failed item to be the error for the whole\n        # barrier. The first error seen \"wins\". Also handles the case where\n        # the WorkItems passed into the barrier have already completed and\n        # been marked with errors.\n        for item in self:\n            if isinstance(item, WorkItem) and item.error:\n                return item.error\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef outstanding(self):\n        # Allow the same WorkItem to be yielded multiple times but not\n        # count towards blocking the barrier.\n        done_count = 0\n        for item in self:\n            if not self.wait_any and item.fire_and_forget:\n                # Only count fire_and_forget items as done if this is\n                # *not* a WaitAny barrier. We only want to return control\n                # to the caller when at least one of the blocking items\n                # has completed.\n                done_count += 1\n            elif item.done:\n                done_count += 1\n\n        if self.wait_any and done_count > 0:\n            return False\n\n        if done_count == len(self):\n            return False\n\n        return True", "response": "Returns whether or not this barrier has pending work."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_item(self):\n        if self.was_list:\n            result = ResultList()\n            for item in self:\n                if isinstance(item, WorkflowItem):\n                    if item.done and not item.error:\n                        result.append(item.result)\n                    else:\n                        # When there's an error or the workflow isn't done yet,\n                        # just return the original WorkflowItem so the caller\n                        # can inspect its entire state.\n                        result.append(item)\n                else:\n                    result.append(item)\n            return result\n        else:\n            return self[0]", "response": "Returns the item to send back into the workflow generator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the coordinator thread and all related worker threads.", "response": "def start(self):\n        \"\"\"Starts the coordinator thread and all related worker threads.\"\"\"\n        assert not self.interrupted\n        for thread in self.worker_threads:\n            thread.start()\n        WorkerThread.start(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(self):\n        if self.interrupted:\n            return\n        for thread in self.worker_threads:\n            thread.interrupted = True\n        self.interrupted = True", "response": "Stops the coordinator thread and all related threads."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\njoin the coordinator thread and all worker threads.", "response": "def join(self):\n        \"\"\"Joins the coordinator thread and all worker threads.\"\"\"\n        for thread in self.worker_threads:\n            thread.join()\n        WorkerThread.join(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_one(self):\n        while True:\n            try:\n                item = self.output_queue.get(True, self.polltime)\n            except Queue.Empty:\n                continue\n            except KeyboardInterrupt:\n                LOGGER.debug('Exiting')\n                return\n            else:\n                item.check_result()\n                return", "response": "Waits until this worker has finished one work item."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef superuser_required(f):\n    @functools.wraps(f)\n    @login_required\n    def wrapped(*args, **kwargs):\n        if not (current_user.is_authenticated() and current_user.superuser):\n            abort(403)\n        return f(*args, **kwargs)\n    return wrapped", "response": "Requires the requestor to be a super user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the current user can access the build ID in the request.", "response": "def can_user_access_build(param_name):\n    \"\"\"Determines if the current user can access the build ID in the request.\n\n    Args:\n        param_name: Parameter name to use for getting the build ID from the\n            request. Will fetch from GET or POST requests.\n\n    Returns:\n        The build the user has access to.\n    \"\"\"\n    build_id = (\n        request.args.get(param_name, type=int) or\n        request.form.get(param_name, type=int) or\n        request.json[param_name])\n    if not build_id:\n        logging.debug('Build ID in param_name=%r was missing', param_name)\n        abort(400)\n\n    ops = operations.UserOps(current_user.get_id())\n    build, user_is_owner = ops.owns_build(build_id)\n    if not build:\n        logging.debug('Could not find build_id=%r', build_id)\n        abort(404)\n\n    if current_user.is_authenticated() and not user_is_owner:\n        # Assume the user should be able to access the build but can't because\n        # the cache is out of date. This forces the cache to repopulate, any\n        # outstanding user invitations to be completed, hopefully resulting in\n        # the user having access to the build.\n        ops.evict()\n        claim_invitations(current_user)\n        build, user_is_owner = ops.owns_build(build_id)\n\n    if not user_is_owner:\n        if current_user.is_authenticated() and current_user.superuser:\n            pass\n        elif request.method != 'GET':\n            logging.debug('No way to log in user via modifying request')\n            abort(403)\n        elif build.public:\n            pass\n        elif current_user.is_authenticated():\n            logging.debug('User does not have access to this build')\n            abort(flask.Response('You cannot access this build', 403))\n        else:\n            logging.debug('Redirecting user to login to get build access')\n            abort(login.unauthorized())\n    elif not login_fresh():\n        logging.debug('User login is old; forcing refresh')\n        abort(login.needs_refresh())\n\n    return build"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_api_key_ops():\n    auth_header = request.authorization\n    if not auth_header:\n        logging.debug('API request lacks authorization header')\n        abort(flask.Response(\n            'API key required', 401,\n            {'WWW-Authenticate': 'Basic realm=\"API key required\"'}))\n\n    return operations.ApiKeyOps(auth_header.username, auth_header.password)", "response": "Returns the API key ops instance for the current request."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the API key for the current request.", "response": "def current_api_key():\n    \"\"\"Determines the API key for the current request.\n\n    Returns:\n        The ApiKey instance.\n    \"\"\"\n    if app.config.get('IGNORE_AUTH'):\n        return models.ApiKey(\n            id='anonymous_superuser',\n            secret='',\n            superuser=True)\n\n    ops = _get_api_key_ops()\n    api_key = ops.get()\n    logging.debug('Authenticated as API key=%r', api_key.id)\n\n    return api_key"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if the current API key can access the build in the request.", "response": "def can_api_key_access_build(param_name):\n    \"\"\"Determines if the current API key can access the build in the request.\n\n    Args:\n        param_name: Parameter name to use for getting the build ID from the\n            request. Will fetch from GET or POST requests.\n\n    Returns:\n        (api_key, build) The API Key and the Build it has access to.\n    \"\"\"\n    build_id = (\n        request.args.get(param_name, type=int) or\n        request.form.get(param_name, type=int) or\n        request.json[param_name])\n    utils.jsonify_assert(build_id, 'build_id required')\n\n    if app.config.get('IGNORE_AUTH'):\n        api_key = models.ApiKey(\n            id='anonymous_superuser',\n            secret='',\n            superuser=True)\n        build = models.Build.query.get(build_id)\n        utils.jsonify_assert(build is not None, 'build must exist', 404)\n    else:\n        ops = _get_api_key_ops()\n        api_key, build = ops.can_access_build(build_id)\n\n    return api_key, build"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_api_access_required(f):\n    @functools.wraps(f)\n    def wrapped(*args, **kwargs):\n        g.api_key, g.build = can_api_key_access_build('build_id')\n        return f(*args, **kwargs)\n    return wrapped", "response": "Decorator ensures API key has access to the build ID in the request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef manage_api_keys():\n    build = g.build\n    create_form = forms.CreateApiKeyForm()\n    if create_form.validate_on_submit():\n        api_key = models.ApiKey()\n        create_form.populate_obj(api_key)\n        api_key.id = utils.human_uuid()\n        api_key.secret = utils.password_uuid()\n\n        save_admin_log(build, created_api_key=True, message=api_key.id)\n\n        db.session.add(api_key)\n        db.session.commit()\n\n        logging.info('Created API key=%r for build_id=%r',\n                     api_key.id, build.id)\n        return redirect(url_for('manage_api_keys', build_id=build.id))\n\n    create_form.build_id.data = build.id\n\n    api_key_query = (\n        models.ApiKey.query\n        .filter_by(build_id=build.id)\n        .order_by(models.ApiKey.created.desc())\n        .limit(1000))\n\n    revoke_form_list = []\n    for api_key in api_key_query:\n        form = forms.RevokeApiKeyForm()\n        form.id.data = api_key.id\n        form.build_id.data = build.id\n        form.revoke.data = True\n        revoke_form_list.append((api_key, form))\n\n    return render_template(\n        'view_api_keys.html',\n        build=build,\n        create_form=create_form,\n        revoke_form_list=revoke_form_list)", "response": "Page for viewing and creating API keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforms submission handler for revoking API keys.", "response": "def revoke_api_key():\n    \"\"\"Form submission handler for revoking API keys.\"\"\"\n    build = g.build\n    form = forms.RevokeApiKeyForm()\n    if form.validate_on_submit():\n        api_key = models.ApiKey.query.get(form.id.data)\n        if api_key.build_id != build.id:\n            logging.debug('User does not have access to API key=%r',\n                          api_key.id)\n            abort(403)\n\n        api_key.active = False\n        save_admin_log(build, revoked_api_key=True, message=api_key.id)\n\n        db.session.add(api_key)\n        db.session.commit()\n\n        ops = operations.ApiKeyOps(api_key.id, api_key.secret)\n        ops.evict()\n\n    return redirect(url_for('manage_api_keys', build_id=build.id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclaims any pending invitations for the given user s email address.", "response": "def claim_invitations(user):\n    \"\"\"Claims any pending invitations for the given user's email address.\"\"\"\n    # See if there are any build invitations present for the user with this\n    # email address. If so, replace all those invitations with the real user.\n    invitation_user_id = '%s:%s' % (\n        models.User.EMAIL_INVITATION, user.email_address)\n    invitation_user = models.User.query.get(invitation_user_id)\n    if invitation_user:\n        invited_build_list = list(invitation_user.builds)\n        if not invited_build_list:\n            return\n\n        db.session.add(user)\n        logging.debug('Found %d build admin invitations for id=%r, user=%r',\n                      len(invited_build_list), invitation_user_id, user)\n\n        for build in invited_build_list:\n            build.owners.remove(invitation_user)\n            if not build.is_owned_by(user.id):\n                build.owners.append(user)\n                logging.debug('Claiming invitation for build_id=%r', build.id)\n                save_admin_log(build, invite_accepted=True)\n            else:\n                logging.debug('User already owner of build. '\n                              'id=%r, build_id=%r', user.id, build.id)\n            db.session.add(build)\n\n        db.session.delete(invitation_user)\n        db.session.commit()\n        # Re-add the user to the current session so we can query with it.\n        db.session.add(current_user)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef manage_admins():\n    build = g.build\n\n    # Do not show cached data\n    db.session.add(build)\n    db.session.refresh(build)\n\n    add_form = forms.AddAdminForm()\n    if add_form.validate_on_submit():\n\n        invitation_user_id = '%s:%s' % (\n            models.User.EMAIL_INVITATION, add_form.email_address.data)\n\n        invitation_user = models.User.query.get(invitation_user_id)\n        if not invitation_user:\n            invitation_user = models.User(\n                id=invitation_user_id,\n                email_address=add_form.email_address.data)\n            db.session.add(invitation_user)\n\n        db.session.add(build)\n        db.session.add(invitation_user)\n        db.session.refresh(build, lockmode='update')\n\n        build.owners.append(invitation_user)\n        save_admin_log(build, invited_new_admin=True,\n                       message=invitation_user.email_address)\n\n        db.session.commit()\n\n        logging.info('Added user=%r as owner to build_id=%r',\n                     invitation_user.id, build.id)\n        return redirect(url_for('manage_admins', build_id=build.id))\n\n    add_form.build_id.data = build.id\n\n    revoke_form_list = []\n    for user in build.owners:\n        form = forms.RemoveAdminForm()\n        form.user_id.data = user.id\n        form.build_id.data = build.id\n        form.revoke.data = True\n        revoke_form_list.append((user, form))\n\n    return render_template(\n        'view_admins.html',\n        build=build,\n        add_form=add_form,\n        revoke_form_list=revoke_form_list)", "response": "Page for viewing and managing build admins."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef revoke_admin():\n    build = g.build\n    form = forms.RemoveAdminForm()\n    if form.validate_on_submit():\n        user = models.User.query.get(form.user_id.data)\n        if not user:\n            logging.debug('User being revoked admin access does not exist.'\n                          'id=%r, build_id=%r', form.user_id.data, build.id)\n            abort(400)\n\n        if user == current_user:\n            logging.debug('User trying to remove themself as admin. '\n                          'id=%r, build_id=%r', user.id, build.id)\n            abort(400)\n\n        db.session.add(build)\n        db.session.add(user)\n        db.session.refresh(build, lockmode='update')\n        db.session.refresh(user, lockmode='update')\n\n        user_is_owner = build.owners.filter_by(id=user.id)\n        if not user_is_owner:\n            logging.debug('User being revoked admin access is not owner. '\n                          'id=%r, build_id=%r.', user.id, build.id)\n            abort(400)\n\n        build.owners.remove(user)\n        save_admin_log(build, revoked_admin=True, message=user.email_address)\n\n        db.session.commit()\n\n        operations.UserOps(user.get_id()).evict()\n\n    return redirect(url_for('manage_admins', build_id=build.id))", "response": "This function is used to revoke admin access to a build."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave an action to the admin log.", "response": "def save_admin_log(build, **kwargs):\n    \"\"\"Saves an action to the admin log.\"\"\"\n    message = kwargs.pop('message', None)\n    release = kwargs.pop('release', None)\n    run = kwargs.pop('run', None)\n\n    if not len(kwargs) == 1:\n        raise TypeError('Must specify a LOG_TYPE argument')\n\n    log_enum = kwargs.keys()[0]\n    log_type = getattr(models.AdminLog, log_enum.upper(), None)\n    if not log_type:\n        raise TypeError('Bad log_type argument: %s' % log_enum)\n\n    if current_user.is_anonymous():\n        user_id = None\n    else:\n        user_id = current_user.get_id()\n\n    log = models.AdminLog(\n        build_id=build.id,\n        log_type=log_type,\n        message=message,\n        user_id=user_id)\n\n    if release:\n        log.release_id = release.id\n\n    if run:\n        log.run_id = run.id\n        log.release_id = run.release_id\n\n    db.session.add(log)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view_admin_log():\n    build = g.build\n\n    # TODO: Add paging\n\n    log_list = (\n        models.AdminLog.query\n        .filter_by(build_id=build.id)\n        .order_by(models.AdminLog.created.desc())\n        .all())\n\n    return render_template(\n        'view_admin_log.html',\n        build=build,\n        log_list=log_list)", "response": "Page for viewing the log of admin activity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef verify_binary(flag_name, process_args=None):\n    if process_args is None:\n        process_args = []\n\n    path = getattr(FLAGS, flag_name)\n    if not path:\n        logging.error('Flag %r not set' % flag_name)\n        sys.exit(1)\n\n    with open(os.devnull, 'w') as dev_null:\n        try:\n            subprocess.check_call(\n                [path] + process_args,\n                stdout=dev_null,\n                stderr=subprocess.STDOUT)\n        except:\n            logging.exception('--%s binary at path %r does not work',\n                              flag_name, path)\n            sys.exit(1)", "response": "Exits the program if the binary from the given flag doesn t work."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new release candidate for a build.", "response": "def create_release():\n    \"\"\"Creates a new release candidate for a build.\"\"\"\n    build = g.build\n    release_name = request.form.get('release_name')\n    utils.jsonify_assert(release_name, 'release_name required')\n    url = request.form.get('url')\n    utils.jsonify_assert(release_name, 'url required')\n\n    release = models.Release(\n        name=release_name,\n        url=url,\n        number=1,\n        build_id=build.id)\n\n    last_candidate = (\n        models.Release.query\n        .filter_by(build_id=build.id, name=release_name)\n        .order_by(models.Release.number.desc())\n        .first())\n    if last_candidate:\n        release.number += last_candidate.number\n        if last_candidate.status == models.Release.PROCESSING:\n            canceled_task_count = work_queue.cancel(\n                release_id=last_candidate.id)\n            logging.info('Canceling %d tasks for previous attempt '\n                         'build_id=%r, release_name=%r, release_number=%d',\n                         canceled_task_count, build.id, last_candidate.name,\n                         last_candidate.number)\n            last_candidate.status = models.Release.BAD\n            db.session.add(last_candidate)\n\n    db.session.add(release)\n    db.session.commit()\n\n    signals.release_updated_via_api.send(app, build=build, release=release)\n\n    logging.info('Created release: build_id=%r, release_name=%r, url=%r, '\n                 'release_number=%d', build.id, release.name,\n                 url, release.number)\n\n    return flask.jsonify(\n        success=True,\n        build_id=build.id,\n        release_name=release.name,\n        release_number=release.number,\n        url=url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the release is done processing.", "response": "def _check_release_done_processing(release):\n    \"\"\"Moves a release candidate to reviewing if all runs are done.\"\"\"\n    if release.status != models.Release.PROCESSING:\n        # NOTE: This statement also guards for situations where the user has\n        # prematurely specified that the release is good or bad. Once the user\n        # has done that, the system will not automatically move the release\n        # back into the 'reviewing' state or send the email notification below.\n        logging.info('Release not in processing state yet: build_id=%r, '\n                     'name=%r, number=%d', release.build_id, release.name,\n                     release.number)\n        return False\n\n    query = models.Run.query.filter_by(release_id=release.id)\n    for run in query:\n        if run.status == models.Run.NEEDS_DIFF:\n            # Still waiting for the diff to finish.\n            return False\n        if run.ref_config and not run.ref_image:\n            # Still waiting for the ref capture to process.\n            return False\n        if run.config and not run.image:\n            # Still waiting for the run capture to process.\n            return False\n\n    logging.info('Release done processing, now reviewing: build_id=%r, '\n                 'name=%r, number=%d', release.build_id, release.name,\n                 release.number)\n\n    # Send the email at the end of this request so we know it's only\n    # sent a single time (guarded by the release.status check above).\n    build_id = release.build_id\n    release_name = release.name\n    release_number = release.number\n\n    @utils.after_this_request\n    def send_notification_email(response):\n        emails.send_ready_for_review(build_id, release_name, release_number)\n\n    release.status = models.Release.REVIEWING\n    db.session.add(release)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_release_params():\n    release_name = request.form.get('release_name')\n    utils.jsonify_assert(release_name, 'release_name required')\n    release_number = request.form.get('release_number', type=int)\n    utils.jsonify_assert(release_number is not None, 'release_number required')\n    return release_name, release_number", "response": "Gets the release params from the current request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_last_good_run(build):\n    run_name = request.form.get('run_name', type=str)\n    utils.jsonify_assert(run_name, 'run_name required')\n\n    last_good_release = (\n        models.Release.query\n        .filter_by(\n            build_id=build.id,\n            status=models.Release.GOOD)\n        .order_by(models.Release.created.desc())\n        .first())\n\n    last_good_run = None\n\n    if last_good_release:\n        logging.debug('Found last good release for: build_id=%r, '\n                      'release_name=%r, release_number=%d',\n                      build.id, last_good_release.name,\n                      last_good_release.number)\n        last_good_run = (\n            models.Run.query\n            .filter_by(release_id=last_good_release.id, name=run_name)\n            .first())\n        if last_good_run:\n            logging.debug('Found last good run for: build_id=%r, '\n                          'release_name=%r, release_number=%d, '\n                          'run_name=%r',\n                          build.id, last_good_release.name,\n                          last_good_release.number, last_good_run.name)\n\n    return last_good_release, last_good_run", "response": "Finds the last good release and run for a build."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the last good run of the given name for a release.", "response": "def find_run():\n    \"\"\"Finds the last good run of the given name for a release.\"\"\"\n    build = g.build\n    last_good_release, last_good_run = _find_last_good_run(build)\n\n    if last_good_run:\n        return flask.jsonify(\n            success=True,\n            build_id=build.id,\n            release_name=last_good_release.name,\n            release_number=last_good_release.number,\n            run_name=last_good_run.name,\n            url=last_good_run.url,\n            image=last_good_run.image,\n            log=last_good_run.log,\n            config=last_good_run.config)\n\n    return utils.jsonify_error('Run not found')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_or_create_run(build):\n    release_name, release_number = _get_release_params()\n    run_name = request.form.get('run_name', type=str)\n    utils.jsonify_assert(run_name, 'run_name required')\n\n    release = (\n        models.Release.query\n        .filter_by(build_id=build.id, name=release_name, number=release_number)\n        .first())\n    utils.jsonify_assert(release, 'release does not exist')\n\n    run = (\n        models.Run.query\n        .filter_by(release_id=release.id, name=run_name)\n        .first())\n    if not run:\n        # Ignore re-reports of the same run name for this release.\n        logging.info('Created run: build_id=%r, release_name=%r, '\n                     'release_number=%d, run_name=%r',\n                     build.id, release.name, release.number, run_name)\n        run = models.Run(\n            release_id=release.id,\n            name=run_name,\n            status=models.Run.DATA_PENDING)\n        db.session.add(run)\n        db.session.flush()\n\n    return release, run", "response": "Gets a run for a build or creates it if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request_run():\n    build = g.build\n    current_release, current_run = _get_or_create_run(build)\n\n    current_url = request.form.get('url', type=str)\n    config_data = request.form.get('config', default='{}', type=str)\n    utils.jsonify_assert(current_url, 'url to capture required')\n    utils.jsonify_assert(config_data, 'config document required')\n\n    config_artifact = _enqueue_capture(\n        build, current_release, current_run, current_url, config_data)\n\n    ref_url = request.form.get('ref_url', type=str)\n    ref_config_data = request.form.get('ref_config', type=str)\n    utils.jsonify_assert(\n        bool(ref_url) == bool(ref_config_data),\n        'ref_url and ref_config must both be specified or not specified')\n\n    if ref_url and ref_config_data:\n        ref_config_artifact = _enqueue_capture(\n            build, current_release, current_run, ref_url, ref_config_data,\n            baseline=True)\n    else:\n        _, last_good_run = _find_last_good_run(build)\n        if last_good_run:\n            current_run.ref_url = last_good_run.url\n            current_run.ref_image = last_good_run.image\n            current_run.ref_log = last_good_run.log\n            current_run.ref_config = last_good_run.config\n\n    db.session.add(current_run)\n    db.session.commit()\n\n    signals.run_updated_via_api.send(\n        app, build=build, release=current_release, run=current_run)\n\n    return flask.jsonify(\n        success=True,\n        build_id=build.id,\n        release_name=current_release.name,\n        release_number=current_release.number,\n        run_name=current_run.name,\n        url=current_run.url,\n        config=current_run.config,\n        ref_url=current_run.ref_url,\n        ref_config=current_run.ref_config)", "response": "Requests a new run for a release candidate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreporting data for a run for a release candidate.", "response": "def report_run():\n    \"\"\"Reports data for a run for a release candidate.\"\"\"\n    build = g.build\n    release, run = _get_or_create_run(build)\n\n    db.session.refresh(run, lockmode='update')\n\n    current_url = request.form.get('url', type=str)\n    current_image = request.form.get('image', type=str)\n    current_log = request.form.get('log', type=str)\n    current_config = request.form.get('config', type=str)\n\n    ref_url = request.form.get('ref_url', type=str)\n    ref_image = request.form.get('ref_image', type=str)\n    ref_log = request.form.get('ref_log', type=str)\n    ref_config = request.form.get('ref_config', type=str)\n\n    diff_failed = request.form.get('diff_failed', type=str)\n    diff_image = request.form.get('diff_image', type=str)\n    diff_log = request.form.get('diff_log', type=str)\n\n    distortion = request.form.get('distortion', default=None, type=float)\n    run_failed = request.form.get('run_failed', type=str)\n\n    if current_url:\n        run.url = current_url\n    if current_image:\n        run.image = current_image\n    if current_log:\n        run.log = current_log\n    if current_config:\n        run.config = current_config\n    if current_image or current_log or current_config:\n        logging.info('Saving run data: build_id=%r, release_name=%r, '\n                     'release_number=%d, run_name=%r, url=%r, '\n                     'image=%r, log=%r, config=%r, run_failed=%r',\n                     build.id, release.name, release.number, run.name,\n                     run.url, run.image, run.log, run.config, run_failed)\n\n    if ref_url:\n        run.ref_url = ref_url\n    if ref_image:\n        run.ref_image = ref_image\n    if ref_log:\n        run.ref_log = ref_log\n    if ref_config:\n        run.ref_config = ref_config\n    if ref_image or ref_log or ref_config:\n        logging.info('Saved reference data: build_id=%r, release_name=%r, '\n                     'release_number=%d, run_name=%r, ref_url=%r, '\n                     'ref_image=%r, ref_log=%r, ref_config=%r',\n                     build.id, release.name, release.number, run.name,\n                     run.ref_url, run.ref_image, run.ref_log, run.ref_config)\n\n    if diff_image:\n        run.diff_image = diff_image\n    if diff_log:\n        run.diff_log = diff_log\n    if distortion:\n        run.distortion = distortion\n\n    if diff_image or diff_log:\n        logging.info('Saved pdiff: build_id=%r, release_name=%r, '\n                     'release_number=%d, run_name=%r, diff_image=%r, '\n                     'diff_log=%r, diff_failed=%r, distortion=%r',\n                     build.id, release.name, release.number, run.name,\n                     run.diff_image, run.diff_log, diff_failed, distortion)\n\n    if run.image and run.diff_image:\n        run.status = models.Run.DIFF_FOUND\n    elif run.image and run.ref_image and not run.diff_log:\n        run.status = models.Run.NEEDS_DIFF\n    elif run.image and run.ref_image and not diff_failed:\n        run.status = models.Run.DIFF_NOT_FOUND\n    elif run.image and not run.ref_config:\n        run.status = models.Run.NO_DIFF_NEEDED\n    elif run_failed or diff_failed:\n        run.status = models.Run.FAILED\n    else:\n        # NOTE: Intentionally do not transition state here in the default case.\n        # We allow multiple background workers to be writing to the same Run in\n        # parallel updating its various properties.\n        pass\n\n    # TODO: Verify the build has access to both the current_image and\n    # the reference_sha1sum so they can't make a diff from a black image\n    # and still see private data in the diff image.\n\n    if run.status == models.Run.NEEDS_DIFF:\n        task_id = '%s:%s:%s' % (run.id, run.image, run.ref_image)\n        logging.info('Enqueuing pdiff task=%r', task_id)\n\n        work_queue.add(\n            constants.PDIFF_QUEUE_NAME,\n            payload=dict(\n                build_id=build.id,\n                release_name=release.name,\n                release_number=release.number,\n                run_name=run.name,\n                run_sha1sum=run.image,\n                reference_sha1sum=run.ref_image,\n            ),\n            build_id=build.id,\n            release_id=release.id,\n            run_id=run.id,\n            source='report_run',\n            task_id=task_id)\n\n    # Flush the run so querying for Runs in _check_release_done_processing\n    # will be find the new run too and we won't deadlock.\n    db.session.add(run)\n    db.session.flush()\n\n    _check_release_done_processing(release)\n    db.session.commit()\n\n    signals.run_updated_via_api.send(\n        app, build=build, release=release, run=run)\n\n    logging.info('Updated run: build_id=%r, release_name=%r, '\n                 'release_number=%d, run_name=%r, status=%r',\n                 build.id, release.name, release.number, run.name, run.status)\n\n    return flask.jsonify(success=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef runs_done():\n    build = g.build\n    release_name, release_number = _get_release_params()\n\n    release = (\n        models.Release.query\n        .filter_by(build_id=build.id, name=release_name, number=release_number)\n        .with_lockmode('update')\n        .first())\n    utils.jsonify_assert(release, 'Release does not exist')\n\n    release.status = models.Release.PROCESSING\n    db.session.add(release)\n    _check_release_done_processing(release)\n    db.session.commit()\n\n    signals.release_updated_via_api.send(app, build=build, release=release)\n\n    logging.info('Runs done for release: build_id=%r, release_name=%r, '\n                 'release_number=%d', build.id, release.name, release.number)\n\n    results_url = url_for(\n        'view_release',\n        id=build.id,\n        name=release.name,\n        number=release.number,\n        _external=True)\n\n    return flask.jsonify(\n        success=True,\n        results_url=results_url)", "response": "Marks a release candidate as having all runs reported."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves an artifact to the DB and returns it.", "response": "def _save_artifact(build, data, content_type):\n    \"\"\"Saves an artifact to the DB and returns it.\"\"\"\n    sha1sum = hashlib.sha1(data).hexdigest()\n    artifact = models.Artifact.query.filter_by(id=sha1sum).first()\n\n    if artifact:\n      logging.debug('Upload already exists: artifact_id=%r', sha1sum)\n    else:\n      logging.info('Upload received: artifact_id=%r, content_type=%r',\n                   sha1sum, content_type)\n      artifact = models.Artifact(\n          id=sha1sum,\n          content_type=content_type,\n          data=data)\n      _artifact_created(artifact)\n\n    artifact.owners.append(build)\n    return artifact"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupload an artifact referenced by a run.", "response": "def upload():\n    \"\"\"Uploads an artifact referenced by a run.\"\"\"\n    build = g.build\n    utils.jsonify_assert(len(request.files) == 1,\n                         'Need exactly one uploaded file')\n\n    file_storage = request.files.values()[0]\n    data = file_storage.read()\n    content_type, _ = mimetypes.guess_type(file_storage.filename)\n\n    artifact = _save_artifact(build, data, content_type)\n\n    db.session.add(artifact)\n    db.session.commit()\n\n    return flask.jsonify(\n        success=True,\n        build_id=build.id,\n        sha1sum=artifact.id,\n        content_type=content_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_artifact_response(artifact):\n    response = flask.Response(\n        artifact.data,\n        mimetype=artifact.content_type)\n    response.cache_control.public = True\n    response.cache_control.max_age = 8640000\n    response.set_etag(artifact.id)\n    return response", "response": "Returns the response object for the given artifact."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download():\n    # Allow users with access to the build to download the file. Falls back\n    # to API keys with access to the build. Prefer user first for speed.\n    try:\n        build = auth.can_user_access_build('build_id')\n    except HTTPException:\n        logging.debug('User access to artifact failed. Trying API key.')\n        _, build = auth.can_api_key_access_build('build_id')\n\n    sha1sum = request.args.get('sha1sum', type=str)\n    if not sha1sum:\n        logging.debug('Artifact sha1sum=%r not supplied', sha1sum)\n        abort(404)\n\n    artifact = models.Artifact.query.get(sha1sum)\n    if not artifact:\n        logging.debug('Artifact sha1sum=%r does not exist', sha1sum)\n        abort(404)\n\n    build_id = request.args.get('build_id', type=int)\n    if not build_id:\n        logging.debug('build_id missing for artifact sha1sum=%r', sha1sum)\n        abort(404)\n\n    is_owned = artifact.owners.filter_by(id=build_id).first()\n    if not is_owned:\n        logging.debug('build_id=%r not owner of artifact sha1sum=%r',\n                      build_id, sha1sum)\n        abort(403)\n\n    # Make sure there are no Set-Cookie headers on the response so this\n    # request is cachable by all HTTP frontends.\n    @utils.after_this_request\n    def no_session(response):\n        if 'Set-Cookie' in response.headers:\n            del response.headers['Set-Cookie']\n\n    if not utils.is_production():\n        # Insert a sleep to emulate how the page loading looks in production.\n        time.sleep(1.5)\n\n    if request.if_none_match and request.if_none_match.contains(sha1sum):\n        response = flask.Response(status=304)\n        return response\n\n    return _get_artifact_response(artifact)", "response": "Downloads an artifact by its content hash."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(coordinator):\n    utils.verify_binary('pdiff_compare_binary', ['-version'])\n    utils.verify_binary('pdiff_composite_binary', ['-version'])\n\n    assert FLAGS.pdiff_threads > 0\n    assert FLAGS.queue_server_prefix\n\n    item = queue_worker.RemoteQueueWorkflow(\n        constants.PDIFF_QUEUE_NAME,\n        DoPdiffQueueWorkflow,\n        max_tasks=FLAGS.pdiff_threads,\n        wait_seconds=FLAGS.pdiff_wait_seconds)\n    item.root = True\n    coordinator.input_queue.put(item)", "response": "Registers this module as a worker with the given coordinator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef evict(self):\n        logging.debug('Evicting cache for %r', self.cache_key)\n        _clear_version_cache(self.cache_key)\n        # Cause the cache key to be refreshed next time any operation is\n        # run to make sure we don't act on old cached data.\n        self.versioned_cache_key = None", "response": "Evict all caches related to this operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsorts function for runs within a release.", "response": "def sort_run(run):\n        \"\"\"Sort function for runs within a release.\"\"\"\n        # Sort errors first, then by name. Also show errors that were manually\n        # approved, so the paging sort order stays the same even after users\n        # approve a diff on the run page.\n        if run.status in models.Run.DIFF_NEEDED_STATES:\n            return (0, run.name)\n        return (1, run.name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the given object as a validator instance.", "response": "def parse(obj, required_properties=None, additional_properties=None,\n          ignore_optional_property_errors=None):\n    \"\"\"Try to parse the given ``obj`` as a validator instance.\n\n    :param obj: The object to be parsed. If it is a...:\n\n        - :py:class:`Validator` instance, return it.\n        - :py:class:`Validator` subclass, instantiate it without arguments and\n          return it.\n        - :py:attr:`~Validator.name` of a known :py:class:`Validator` subclass,\n          instantiate the subclass without arguments and return it.\n        - otherwise find the first registered :py:class:`Validator` factory that\n          can create it. The search order is the reverse of the factory registration\n          order. The caller is responsible for ensuring there are no ambiguous\n          values that can be parsed by more than one factory.\n\n    :param required_properties: Specifies for this parse call whether parsed\n        :py:class:`~valideer.validators.Object` properties are required or\n        optional by default. It can be:\n\n        - ``True`` for required.\n        - ``False`` for optional.\n        - ``None`` to use the value of the\n          :py:attr:`~valideer.validators.Object.REQUIRED_PROPERTIES` attribute.\n\n    :param additional_properties: Specifies for this parse call the schema of\n        all :py:class:`~valideer.validators.Object` properties that are not\n        explicitly defined as optional or required. It can also be:\n\n        - ``True`` to allow any value for additional properties.\n        - ``False`` to disallow any additional properties.\n        - :py:attr:`~valideer.validators.Object.REMOVE` to remove any additional\n          properties from the adapted object.\n        - ``None`` to use the value of the\n          :py:attr:`~valideer.validators.Object.ADDITIONAL_PROPERTIES` attribute.\n\n    :param ignore_optional_property_errors: Determines if invalid optional\n        properties are ignored:\n\n        - ``True`` to ignore invalid optional properties.\n        - ``False`` to raise ValidationError for invalid optional properties.\n        - ``None`` to use the value of the\n          :py:attr:`~valideer.validators.Object.IGNORE_OPTIONAL_PROPERTY_ERRORS`\n          attribute.\n\n    :raises SchemaError: If no appropriate validator could be found.\n\n    .. warning:: Passing ``required_properties`` and/or ``additional_properties``\n        with value other than ``None`` may be non intuitive for schemas that\n        involve nested validators. Take for example the following schema::\n\n            v = V.parse({\n                \"x\": \"integer\",\n                \"child\": V.Nullable({\n                    \"y\": \"integer\"\n                })\n            }, required_properties=True)\n\n        Here the top-level properties 'x' and 'child' are required but the nested\n        'y' property is not. This is because by the time :py:meth:`parse` is called,\n        :py:class:`~valideer.validators.Nullable` has already parsed its argument\n        with the default value of ``required_properties``. Several other builtin\n        validators work similarly to :py:class:`~valideer.validators.Nullable`,\n        accepting one or more schemas to parse. In order to parse an arbitrarily\n        complex nested validator with the same value for ``required_properties``\n        and/or ``additional_properties``, use the :py:func:`parsing` context\n        manager instead::\n\n            with V.parsing(required_properties=True):\n                v = V.parse({\n                    \"x\": \"integer\",\n                    \"child\": V.Nullable({\n                        \"y\": \"integer\"\n                    })\n                })\n    \"\"\"\n    if not (required_properties is\n            additional_properties is\n            ignore_optional_property_errors is None):\n        with parsing(required_properties=required_properties,\n                     additional_properties=additional_properties,\n                     ignore_optional_property_errors=ignore_optional_property_errors):\n            return parse(obj)\n\n    validator = None\n\n    if isinstance(obj, Validator):\n        validator = obj\n    elif inspect.isclass(obj) and issubclass(obj, Validator):\n        validator = obj()\n    else:\n        try:\n            validator = _NAMED_VALIDATORS[obj]\n        except (KeyError, TypeError):\n            for factory in _VALIDATOR_FACTORIES:\n                validator = factory(obj)\n                if validator is not None:\n                    break\n        else:\n            if inspect.isclass(validator) and issubclass(validator, Validator):\n                _NAMED_VALIDATORS[obj] = validator = validator()\n\n    if not isinstance(validator, Validator):\n        raise SchemaError(\"%r cannot be parsed as a Validator\" % obj)\n\n    return validator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parsing(**kwargs):\n\n    from .validators import Object\n    with _VALIDATOR_FACTORIES_LOCK:\n        old_values = {}\n        for key, value in iteritems(kwargs):\n            if value is not None:\n                attr = key.upper()\n                old_values[key] = getattr(Object, attr)\n                setattr(Object, attr, value)\n        try:\n            yield\n        finally:\n            for key, value in iteritems(kwargs):\n                if value is not None:\n                    setattr(Object, key.upper(), old_values[key])", "response": "Context manager for overriding the default validator parsing rules for the\n    following code block."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a validator instance under the given name.", "response": "def register(name, validator):\n    \"\"\"Register a validator instance under the given ``name``.\"\"\"\n    if not isinstance(validator, Validator):\n        raise TypeError(\"Validator instance expected, %s given\" % validator.__class__)\n    _NAMED_VALIDATORS[name] = validator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a decorator for validating a given parameter.", "response": "def accepts(**schemas):\n    \"\"\"Create a decorator for validating function parameters.\n\n    Example::\n\n        @accepts(a=\"number\", body={\"+field_ids\": [int], \"is_ok\": bool})\n        def f(a, body):\n            print (a, body[\"field_ids\"], body.get(\"is_ok\"))\n\n    :param schemas: The schema for validating a given parameter.\n    \"\"\"\n    validate = parse(schemas).validate\n\n    @decorator\n    def validating(func, *args, **kwargs):\n        validate(inspect.getcallargs(func, *args, **kwargs), adapt=False)\n        return func(*args, **kwargs)\n    return validating"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a decorator for validating function return value.", "response": "def returns(schema):\n    \"\"\"Create a decorator for validating function return value.\n\n    Example::\n        @accepts(a=int, b=int)\n        @returns(int)\n        def f(a, b):\n            return a + b\n\n    :param schema: The schema for adapting a given parameter.\n    \"\"\"\n    validate = parse(schema).validate\n\n    @decorator\n    def validating(func, *args, **kwargs):\n        ret = func(*args, **kwargs)\n        validate(ret, adapt=False)\n        return ret\n    return validating"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a decorator for validating and adapting a given parameter.", "response": "def adapts(**schemas):\n    \"\"\"Create a decorator for validating and adapting function parameters.\n\n    Example::\n\n        @adapts(a=\"number\", body={\"+field_ids\": [V.AdaptTo(int)], \"is_ok\": bool})\n        def f(a, body):\n            print (a, body.field_ids, body.is_ok)\n\n    :param schemas: The schema for adapting a given parameter.\n    \"\"\"\n    validate = parse(schemas).validate\n\n    @decorator\n    def adapting(func, *args, **kwargs):\n        adapted = validate(inspect.getcallargs(func, *args, **kwargs), adapt=True)\n        argspec = inspect.getargspec(func)\n\n        if argspec.varargs is argspec.keywords is None:\n            # optimization for the common no varargs, no keywords case\n            return func(**adapted)\n\n        adapted_varargs = adapted.pop(argspec.varargs, ())\n        adapted_keywords = adapted.pop(argspec.keywords, {})\n        if not adapted_varargs:  # keywords only\n            if adapted_keywords:\n                adapted.update(adapted_keywords)\n            return func(**adapted)\n\n        adapted_posargs = [adapted[arg] for arg in argspec.args]\n        adapted_posargs.extend(adapted_varargs)\n        return func(*adapted_posargs, **adapted_keywords)\n\n    return adapting"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a python dict as an object.", "response": "def _ObjectFactory(obj):\n    \"\"\"Parse a python ``{name: schema}`` dict as an :py:class:`Object` instance.\n\n    - A property name prepended by \"+\" is required\n    - A property name prepended by \"?\" is optional\n    - Any other property is required if :py:attr:`Object.REQUIRED_PROPERTIES`\n      is True else it's optional\n    \"\"\"\n    if isinstance(obj, dict):\n        optional, required = {}, {}\n        for key, value in iteritems(obj):\n            if key.startswith(\"+\"):\n                required[key[1:]] = value\n            elif key.startswith(\"?\"):\n                optional[key[1:]] = value\n            elif Object.REQUIRED_PROPERTIES:\n                required[key] = value\n            else:\n                optional[key] = value\n        return Object(optional, required)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_checksum_metadata_tag(self):\n        if not self._checksums:\n            print(\"Warning: No checksums have been computed for this file.\")\n        return {str(_hash_name): str(_hash_value) for _hash_name, _hash_value in self._checksums.items()}", "response": "Returns a map of checksum values by the name of the hashing function that produced it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the checksums for a given file.", "response": "def compute_checksum(self):\n        \"\"\" Calculates checksums for a given file. \"\"\"\n        if self._filename.startswith(\"s3://\"):\n            print(\"Warning: Did not perform client-side checksumming for file in S3. To be implemented.\")\n            pass\n        else:\n            checksumCalculator = self.ChecksumCalculator(self._filename)\n            self._checksums = checksumCalculator.compute()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_credentials(self):\n        creds_mgr = CredentialsManager(self)\n        creds = creds_mgr.get_credentials_from_upload_api()\n        return {\n            'aws_access_key_id': creds['access_key'],\n            'aws_secret_access_key': creds['secret_key'],\n            'aws_session_token': creds['token'],\n            'expiry_time': creds['expiry_time']\n        }", "response": "Returns a set of AWS credentials that may be used to access the Upload Area folder in the S3 bucket."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring a small file in an Upload Area and return the information about the file", "response": "def store_file(self, filename, file_content, content_type):\n        \"\"\"\n        Store a small file in an Upload Area\n\n        :param str area_uuid: A RFC4122-compliant ID for the upload area\n        :param str filename: The name the file will have in the Upload Area\n        :param str file_content: The contents of the file\n        :param str content_type: The MIME-type for the file\n        :return: information about the stored file (similar to that returned by files_info)\n        :rtype: dict\n        :raises UploadApiException: if file could not be stored\n        \"\"\"\n\n        return self.upload_service.api_client.store_file(area_uuid=self.uuid,\n                                                         filename=filename,\n                                                         file_content=file_content,\n                                                         content_type=content_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload_files(self, file_paths, file_size_sum=0, dcp_type=\"data\", target_filename=None,\n                     use_transfer_acceleration=True, report_progress=False, sync=True):\n        \"\"\"\n        A function that takes in a list of file paths and other optional args for parallel file upload\n        \"\"\"\n        self._setup_s3_agent_for_file_upload(file_count=len(file_paths),\n                                             file_size_sum=file_size_sum,\n                                             use_transfer_acceleration=use_transfer_acceleration)\n        pool = ThreadPool()\n        if report_progress:\n            print(\"\\nStarting upload of %s files to upload area %s\" % (len(file_paths), self.uuid))\n        for file_path in file_paths:\n            pool.add_task(self._upload_file, file_path,\n                          target_filename=target_filename,\n                          use_transfer_acceleration=use_transfer_acceleration,\n                          report_progress=report_progress,\n                          sync=sync)\n        pool.wait_for_completion()\n        if report_progress:\n            number_of_errors = len(self.s3agent.failed_uploads)\n            if number_of_errors == 0:\n                print(\n                    \"Completed upload of %d files to upload area %s\\n\" %\n                    (self.s3agent.file_upload_completed_count, self.uuid))\n            else:\n                error = \"\\nThe following files failed:\"\n                for k, v in self.s3agent.failed_uploads.items():\n                    error += \"\\n%s: [Exception] %s\" % (k, v)\n                error += \"\\nPlease retry or contact an hca administrator at data-help@humancellatlas.org for help.\\n\"\n                raise UploadException(error)", "response": "This function is used to upload files to the area."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating the files in the Upload Area.", "response": "def validate_files(self, file_list, validator_image, original_validation_id=\"\", environment={}):\n        \"\"\"\n        Invoke supplied validator Docker image and give it access to the file/s.\n        The validator must be based off the base validator Docker image.\n\n        :param list file_list: A list of files within the Upload Area to be validated\n        :param str validator_image: the location of a docker image to use for validation\n        :param str original_validation_id: [optional]\n        :param dict environment: [optional] list of environment variable to set for the validator\n        :return: ID of scheduled validation\n        :rtype: dict\n        :raises UploadApiException: if information could not be obtained\n        \"\"\"\n        return self.upload_service.api_client.validate_files(area_uuid=self.uuid,\n                                                             file_list=file_list,\n                                                             validator_image=validator_image,\n                                                             original_validation_id=original_validation_id,\n                                                             environment=environment)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef checksum_status(self, filename):\n        return self.upload_service.api_client.checksum_status(area_uuid=self.uuid, filename=filename)", "response": "Retrieve checksum status and values for a file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting status and results of latest validation job for a file.", "response": "def validation_status(self, filename):\n        \"\"\"\n        Get status and results of latest validation job for a file.\n\n        :param str filename: The name of the file within the Upload Area\n        :return: a dict with validation information\n        :rtype: dict\n        :raises UploadApiException: if information could not be obtained\n        \"\"\"\n        return self.upload_service.api_client.validation_status(area_uuid=self.uuid, filename=filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwarns the user if their release is behind the latest PyPi __version__.", "response": "def check_if_release_is_current(log):\n    \"\"\"Warns the user if their release is behind the latest PyPi __version__.\"\"\"\n    if __version__ == '0.0.0':\n        return\n    client = xmlrpclib.ServerProxy('https://pypi.python.org/pypi')\n    latest_pypi_version = client.package_releases('hca')\n\n    latest_version_nums = [int(i) for i in latest_pypi_version[0].split('.')]\n    this_version_nums = [int(i) for i in __version__.split('.')]\n    for i in range(max([len(latest_version_nums), len(this_version_nums)])):\n        try:\n            if this_version_nums[i] < latest_version_nums[i]:\n                log.warning('WARNING: Python (pip) package \"hca\" is not up-to-date!\\n'\n                            'You have hca version:              ' + str(__version__) + '\\n'\n                            'Please use the latest hca version: ' + str(latest_pypi_version[0]))\n            # handles the odd case where a user's current __version__ is higher than PyPi's\n            elif this_version_nums[i] > latest_version_nums[i]:\n                break\n        # if 4.2 compared to 4.3.1, this handles the missing element\n        except IndexError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_docstring(docstring):\n         this will be the summary\n\n         :param name: describe the parameter called name.\n\n         this will be the descriptions\n\n         * more description\n         * more description\n\n         This will also be in the description\n        \\\"\"\"\n\n    :param str docstring:\n    :return:\n    :rtype: dict\n    \"\"\"\n    settings = OptionParser(components=(RSTParser,)).get_default_values()\n    rstparser = RSTParser()\n    document = utils.new_document(' ', settings)\n    rstparser.parse(docstring, document)\n    if document.children[0].tagname != 'block_quote':\n        logger.warning(\"The first line of the docstring must be blank.\")\n    else:\n        document = document.children[0]\n\n    def get_params(field_list_node, params):\n        for field in field_list_node.children:\n            name = field.children[0].rawsource.split(' ')\n            if 'param' == name[0]:\n                params[name[-1]] = field.children[1].astext()\n\n    method_args = {'summary': '', 'params': dict(), 'description': ''}\n    for node in document.children:\n        if node.tagname is 'paragraph' and method_args['summary'] == '':\n            method_args['summary'] = node.astext()\n        elif node.tagname is 'field_list':\n            get_params(node, method_args['params'])\n        else:\n            method_args['description'] += '\\n' + node.astext()\n    return method_args", "response": "Parses the docstring and returns a dict of parameters and descriptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sizeof_fmt(num, suffix='B'):\n    precision = {'': 0, 'Ki': 0, 'Mi': 0, 'Gi': 3, 'Ti': 6, 'Pi': 9, 'Ei': 12, 'Zi': 15}\n\n    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) < 1024.0:\n            format_string = \"{number:.%df} {unit}{suffix}\" % precision[unit]\n            return format_string.format(number=num, unit=unit, suffix=suffix)\n        num /= 1024.0\n    return \"%.18f %s%s\" % (num, 'Yi', suffix)", "response": "Format size of a resource in a resource in a resource in a resource in a resource in a resource in a resource in a resource in a resource in a resource in a resource in a resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn true if the key already exists in the current bucket and the clientside checksum matches the file s checksums and false otherwise.", "response": "def _item_exists_in_bucket(self, bucket, key, checksums):\n        \"\"\" Returns true if the key already exists in the current bucket and the clientside checksum matches the\n        file's checksums, and false otherwise.\"\"\"\n        try:\n            obj = self.target_s3.meta.client.head_object(Bucket=bucket, Key=key)\n            if obj and obj.containsKey('Metadata'):\n                if obj['Metadata'] == checksums:\n                    return True\n        except ClientError:\n            # An exception from calling `head_object` indicates that no file with the specified name could be found\n            # in the specified bucket.\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading files to the cloud.", "response": "def upload_to_cloud(file_handles, staging_bucket, replica, from_cloud=False):\n    \"\"\"\n    Upload files to cloud.\n\n    :param file_handles: If from_cloud, file_handles is a aws s3 directory path to files with appropriate\n                         metadata uploaded. Else, a list of binary file_handles to upload.\n    :param staging_bucket: The aws bucket to upload the files to.\n    :param replica: The cloud replica to write to. One of 'aws', 'gc', or 'azure'. No functionality now.\n    :return: a list of file uuids, key-names, and absolute file paths (local) for uploaded files\n    \"\"\"\n    s3 = boto3.resource(\"s3\")\n    file_uuids = []\n    key_names = []\n    abs_file_paths = []\n\n    if from_cloud:\n        file_uuids, key_names = _copy_from_s3(file_handles[0], s3)\n    else:\n        destination_bucket = s3.Bucket(staging_bucket)\n        for raw_fh in file_handles:\n            file_size = os.path.getsize(raw_fh.name)\n            multipart_chunksize = s3_multipart.get_s3_multipart_chunk_size(file_size)\n            tx_cfg = TransferConfig(multipart_threshold=s3_multipart.MULTIPART_THRESHOLD,\n                                    multipart_chunksize=multipart_chunksize)\n            with ChecksummingBufferedReader(raw_fh, multipart_chunksize) as fh:\n                file_uuid = str(uuid.uuid4())\n                key_name = \"{}/{}\".format(file_uuid, os.path.basename(fh.raw.name))\n                destination_bucket.upload_fileobj(\n                    fh,\n                    key_name,\n                    Config=tx_cfg,\n                    ExtraArgs={\n                        'ContentType': _mime_type(fh.raw.name),\n                    }\n                )\n                sums = fh.get_checksums()\n                metadata = {\n                    \"hca-dss-s3_etag\": sums[\"s3_etag\"],\n                    \"hca-dss-sha1\": sums[\"sha1\"],\n                    \"hca-dss-sha256\": sums[\"sha256\"],\n                    \"hca-dss-crc32c\": sums[\"crc32c\"],\n                }\n                s3.meta.client.put_object_tagging(Bucket=destination_bucket.name,\n                                                  Key=key_name,\n                                                  Tagging=dict(TagSet=encode_tags(metadata)))\n                file_uuids.append(file_uuid)\n                key_names.append(key_name)\n                abs_file_paths.append(fh.raw.name)\n\n    return file_uuids, key_names, abs_file_paths"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(self, bundle_uuid, replica, version=\"\", download_dir=\"\",\n                 metadata_files=('*',), data_files=('*',),\n                 num_retries=10, min_delay_seconds=0.25):\n        \"\"\"\n        Download a bundle and save it to the local filesystem as a directory.\n\n        :param str bundle_uuid: The uuid of the bundle to download\n        :param str replica: the replica to download from. The supported replicas are: `aws` for Amazon Web Services, and\n            `gcp` for Google Cloud Platform. [aws, gcp]\n        :param str version: The version to download, else if not specified, download the latest. The version is a\n            timestamp of bundle creation in RFC3339\n        :param str dest_name: The destination file path for the download\n        :param iterable metadata_files: one or more shell patterns against which all metadata files in the bundle will be\n            matched case-sensitively. A file is considered a metadata file if the `indexed` property in the manifest is\n            set. If and only if a metadata file matches any of the patterns in `metadata_files` will it be downloaded.\n        :param iterable data_files: one or more shell patterns against which all data files in the bundle will be matched\n            case-sensitively. A file is considered a data file if the `indexed` property in the manifest is not set. The\n            file will be downloaded only if a data file matches any of the patterns in `data_files` will it be\n            downloaded.\n        :param int num_retries: The initial quota of download failures to accept before exiting due to\n            failures. The number of retries increase and decrease as file chucks succeed and fail.\n        :param float min_delay_seconds: The minimum number of seconds to wait in between retries.\n\n        Download a bundle and save it to the local filesystem as a directory.\n        By default, all data and metadata files are downloaded. To disable the downloading of data files,\n        use `--data-files ''` if using the CLI (or `data_files=()` if invoking `download` programmatically).\n        Likewise for metadata files.\n\n        If a retryable exception occurs, we wait a bit and retry again.  The delay increases each time we fail and\n        decreases each time we successfully read a block.  We set a quota for the number of failures that goes up with\n        every successful block read and down with each failure.\n        \"\"\"\n        errors = 0\n\n        with concurrent.futures.ThreadPoolExecutor(self.threads) as executor:\n            futures_to_dss_file = {executor.submit(task): dss_file\n                                   for dss_file, task in self._download_tasks(bundle_uuid,\n                                                                              replica,\n                                                                              version,\n                                                                              download_dir,\n                                                                              metadata_files,\n                                                                              data_files,\n                                                                              num_retries,\n                                                                              min_delay_seconds)}\n            for future in concurrent.futures.as_completed(futures_to_dss_file):\n                dss_file = futures_to_dss_file[future]\n                try:\n                    future.result()\n                except Exception as e:\n                    errors += 1\n                    logger.warning('Failed to download file %s version %s from replica %s',\n                                   dss_file.uuid, dss_file.version, dss_file.replica, exc_info=e)\n        if errors:\n            raise RuntimeError('{} file(s) failed to download'.format(errors))", "response": "Download a bundle and save it to the local filesystem as a directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _download_to_filestore(self, download_dir, dss_file, num_retries=10, min_delay_seconds=0.25):\n        dest_path = self._file_path(dss_file.sha256, download_dir)\n        if os.path.exists(dest_path):\n            logger.info(\"Skipping download of '%s' because it already exists at '%s'.\", dss_file.name, dest_path)\n        else:\n            logger.debug(\"Downloading '%s' to '%s'.\", dss_file.name, dest_path)\n            self._download_file(dss_file, dest_path, num_retries=num_retries, min_delay_seconds=min_delay_seconds)\n            logger.info(\"Download '%s' to '%s'.\", dss_file.name, dest_path)\n        return dest_path", "response": "Download the data and save it in the filestore."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _download_file(self, dss_file, dest_path, num_retries=10, min_delay_seconds=0.25):\n        directory, _ = os.path.split(dest_path)\n        if directory:\n            try:\n                os.makedirs(directory)\n            except OSError as e:\n                if e.errno != errno.EEXIST:\n                    raise\n        with atomic_write(dest_path, mode=\"wb\", overwrite=True) as fh:\n            if dss_file.size == 0:\n                return\n\n            download_hash = self._do_download_file(dss_file, fh, num_retries, min_delay_seconds)\n\n            if download_hash.lower() != dss_file.sha256.lower():\n                # No need to delete what's been written. atomic_write ensures we're cleaned up\n                logger.error(\"%s\", \"File {}: GET FAILED. Checksum mismatch.\".format(dss_file.uuid))\n                raise ValueError(\"Expected sha256 {} Received sha256 {}\".format(\n                    dss_file.sha256.lower(), download_hash.lower()))", "response": "Download the file from DSS and store it in dest_path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nabstract away complications for downloading a file, handles retries and delays, and computes its hash", "response": "def _do_download_file(self, dss_file, fh, num_retries, min_delay_seconds):\n        \"\"\"\n        Abstracts away complications for downloading a file, handles retries and delays, and computes its hash\n        \"\"\"\n        hasher = hashlib.sha256()\n        delay = min_delay_seconds\n        retries_left = num_retries\n        while True:\n            try:\n                response = self.get_file._request(\n                    dict(uuid=dss_file.uuid, version=dss_file.version, replica=dss_file.replica),\n                    stream=True,\n                    headers={\n                        'Range': \"bytes={}-\".format(fh.tell())\n                    },\n                )\n                try:\n                    if not response.ok:\n                        logger.error(\"%s\", \"File {}: GET FAILED.\".format(dss_file.uuid))\n                        logger.error(\"%s\", \"Response: {}\".format(response.text))\n                        break\n\n                    consume_bytes = int(fh.tell())\n                    server_start = 0\n                    content_range_header = response.headers.get('Content-Range', None)\n                    if content_range_header is not None:\n                        cre = re.compile(\"bytes (\\d+)-(\\d+)\")\n                        mo = cre.search(content_range_header)\n                        if mo is not None:\n                            server_start = int(mo.group(1))\n\n                    consume_bytes -= server_start\n                    assert consume_bytes >= 0\n                    if server_start > 0 and consume_bytes == 0:\n                        logger.info(\"%s\", \"File {}: Resuming at {}.\".format(\n                            dss_file.uuid, server_start))\n                    elif consume_bytes > 0:\n                        logger.info(\"%s\", \"File {}: Resuming at {}. Dropping {} bytes to match\".format(\n                            dss_file.uuid, server_start, consume_bytes))\n\n                        while consume_bytes > 0:\n                            bytes_to_read = min(consume_bytes, 1024*1024)\n                            content = response.iter_content(chunk_size=bytes_to_read)\n                            chunk = next(content)\n                            if chunk:\n                                consume_bytes -= len(chunk)\n\n                    for chunk in response.iter_content(chunk_size=1024*1024):\n                        if chunk:\n                            fh.write(chunk)\n                            hasher.update(chunk)\n                            retries_left = min(retries_left + 1, num_retries)\n                            delay = max(delay / 2, min_delay_seconds)\n                    break\n                finally:\n                    response.close()\n            except (ChunkedEncodingError, ConnectionError, ReadTimeout):\n                if retries_left > 0:\n                    logger.info(\"%s\", \"File {}: GET FAILED. Attempting to resume.\".format(dss_file.uuid))\n                    time.sleep(delay)\n                    delay *= 2\n                    retries_left -= 1\n                    continue\n                raise\n        return hasher.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a file s relative path based on the nesting parameters and the files hash", "response": "def _file_path(cls, checksum, download_dir):\n        \"\"\"\n        returns a file's relative local path based on the nesting parameters and the files hash\n        :param checksum: a string checksum\n        :param download_dir: root directory for filestore\n        :return: relative Path object\n        \"\"\"\n        checksum = checksum.lower()\n        file_prefix = '_'.join(['files'] + list(map(str, cls.DIRECTORY_NAME_LENGTHS)))\n        path_pieces = [download_dir, '.hca', 'v2', file_prefix]\n        checksum_index = 0\n        assert(sum(cls.DIRECTORY_NAME_LENGTHS) <= len(checksum))\n        for prefix_length in cls.DIRECTORY_NAME_LENGTHS:\n            path_pieces.append(checksum[checksum_index:(checksum_index + prefix_length)])\n            checksum_index += prefix_length\n        path_pieces.append(checksum)\n        return os.path.join(*path_pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _write_output_manifest(self, manifest, filestore_root):\n        output = os.path.basename(manifest)\n        fieldnames, source_manifest = self._parse_manifest(manifest)\n        if 'file_path' not in fieldnames:\n            fieldnames.append('file_path')\n        with atomic_write(output, overwrite=True) as f:\n            delimiter = b'\\t' if USING_PYTHON2 else '\\t'\n            writer = csv.DictWriter(f, fieldnames, delimiter=delimiter, quoting=csv.QUOTE_NONE)\n            writer.writeheader()\n            for row in source_manifest:\n                row['file_path'] = self._file_path(row['file_sha256'], filestore_root)\n                writer.writerow(row)\n            if os.path.isfile(output):\n                logger.warning('Overwriting manifest %s', output)\n        logger.info('Rewrote manifest %s with additional column containing path to downloaded files.', output)", "response": "Writes the manifest to the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_manifest_v2(self, manifest, replica,\n                             num_retries=10,\n                             min_delay_seconds=0.25,\n                             download_dir='.'):\n        \"\"\"\n        Process the given manifest file in TSV (tab-separated values) format and download the files referenced by it.\n        The files are downloaded in the version 2 format.\n\n        This download format will serve as the main storage format for downloaded files. If a user specifies a different\n        format for download (coming in the future) the files will first be downloaded in this format, then hard-linked\n        to the user's preferred format.\n\n        :param str manifest: path to a TSV (tab-separated values) file listing files to download\n        :param str replica: the replica to download from. The supported replicas are: `aws` for Amazon Web Services, and\n            `gcp` for Google Cloud Platform. [aws, gcp]\n        :param int num_retries: The initial quota of download failures to accept before exiting due to\n            failures. The number of retries increase and decrease as file chucks succeed and fail.\n        :param float min_delay_seconds: The minimum number of seconds to wait in between retries.\n\n        Process the given manifest file in TSV (tab-separated values) format and download the files\n        referenced by it.\n\n        Each row in the manifest represents one file in DSS. The manifest must have a header row. The header row\n        must declare the following columns:\n\n        * `file_uuid` - the UUID of the file in DSS.\n\n        * `file_version` - the version of the file in DSS.\n\n        The TSV may have additional columns. Those columns will be ignored. The ordering of the columns is\n        insignificant because the TSV is required to have a header row.\n        \"\"\"\n        fieldnames, rows = self._parse_manifest(manifest)\n        errors = 0\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor:\n            futures_to_dss_file = {}\n            for row in rows:\n                dss_file = DSSFile.from_manifest_row(row, replica)\n                future = executor.submit(self._download_to_filestore, download_dir, dss_file,\n                                         num_retries=num_retries, min_delay_seconds=min_delay_seconds)\n                futures_to_dss_file[future] = dss_file\n            for future in concurrent.futures.as_completed(futures_to_dss_file):\n                dss_file = futures_to_dss_file[future]\n                try:\n                    future.result()\n                except Exception as e:\n                    errors += 1\n                    logger.warning('Failed to download file %s version %s from replica %s',\n                                   dss_file.uuid, dss_file.version, dss_file.replica, exc_info=e)\n        if errors:\n            raise RuntimeError('{} file(s) failed to download'.format(errors))\n        else:\n            self._write_output_manifest(manifest, download_dir)", "response": "Download the given manifest file in version 2 format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_manifest(self, manifest, replica, num_retries=10, min_delay_seconds=0.25, download_dir=''):\n        file_errors = 0\n        file_task, bundle_errors = self._download_manifest_tasks(manifest,\n                                                                 replica,\n                                                                 num_retries,\n                                                                 min_delay_seconds,\n                                                                 download_dir)\n        with concurrent.futures.ThreadPoolExecutor(self.threads) as executor:\n            futures_to_dss_file = {executor.submit(task): dss_file\n                                   for dss_file, task in file_task}\n            for future in concurrent.futures.as_completed(futures_to_dss_file):\n                dss_file = futures_to_dss_file[future]\n                try:\n                    future.result()\n                except Exception as e:\n                    file_errors += 1\n                    logger.warning('Failed to download file %s version %s from replica %s',\n                                   dss_file.uuid, dss_file.version, dss_file.replica, exc_info=e)\n        if file_errors or bundle_errors:\n            bundle_error_str = '{} bundle(s) failed to download'.format(bundle_errors) if bundle_errors else ''\n            file_error_str = '{} file(s) failed to download'.format(file_errors) if file_errors else ''\n            raise RuntimeError(bundle_error_str + (' and ' if bundle_errors and file_errors else '') + file_error_str)\n        else:\n            self._write_output_manifest(manifest, download_dir)\n            logger.info('Primary copies of the files have been downloaded to `.hca` and linked '\n                        'into per-bundle subdirectories of the current directory.')", "response": "Download the given manifest file into DSS."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nuploading a directory of files to the given replica and creates a bundle containing the uploaded files.", "response": "def upload(self, src_dir, replica, staging_bucket, timeout_seconds=1200):\n        \"\"\"\n        Upload a directory of files from the local filesystem and create a bundle containing the uploaded files.\n\n        :param str src_dir: file path to a directory of files to upload to the replica.\n        :param str replica: the replica to upload to. The supported replicas are: `aws` for Amazon Web Services, and\n            `gcp` for Google Cloud Platform. [aws, gcp]\n        :param str staging_bucket: a client controlled AWS S3 storage bucket to upload from.\n        :param int timeout_seconds: the time to wait for a file to upload to replica.\n\n        Upload a directory of files from the local filesystem and create a bundle containing the uploaded files.\n        This method requires the use of a client-controlled object storage bucket to stage the data for upload.\n        \"\"\"\n        bundle_uuid = str(uuid.uuid4())\n        version = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%S.%fZ\")\n\n        files_to_upload, files_uploaded = [], []\n        for filename in iter_paths(src_dir):\n            full_file_name = filename.path\n            files_to_upload.append(open(full_file_name, \"rb\"))\n\n        logger.info(\"Uploading %i files from %s to %s\", len(files_to_upload), src_dir, staging_bucket)\n        file_uuids, uploaded_keys, abs_file_paths = upload_to_cloud(files_to_upload, staging_bucket=staging_bucket,\n                                                                    replica=replica, from_cloud=False)\n        for file_handle in files_to_upload:\n            file_handle.close()\n        filenames = [object_name_builder(p, src_dir) for p in abs_file_paths]\n        filename_key_list = list(zip(filenames, file_uuids, uploaded_keys))\n\n        for filename, file_uuid, key in filename_key_list:\n            filename = filename.replace('\\\\', '/')  # for windows paths\n            if filename.startswith('/'):\n                filename = filename.lstrip('/')\n            logger.info(\"File %s: registering...\", filename)\n\n            # Generating file data\n            creator_uid = self.config.get(\"creator_uid\", 0)\n            source_url = \"s3://{}/{}\".format(staging_bucket, key)\n            logger.info(\"File %s: registering from %s -> uuid %s\", filename, source_url, file_uuid)\n\n            response = self.put_file._request(dict(\n                uuid=file_uuid,\n                bundle_uuid=bundle_uuid,\n                version=version,\n                creator_uid=creator_uid,\n                source_url=source_url\n            ))\n            files_uploaded.append(dict(name=filename, version=version, uuid=file_uuid, creator_uid=creator_uid))\n\n            if response.status_code in (requests.codes.ok, requests.codes.created):\n                logger.info(\"File %s: Sync copy -> %s\", filename, version)\n            else:\n                assert response.status_code == requests.codes.accepted\n                logger.info(\"File %s: Async copy -> %s\", filename, version)\n\n                timeout = time.time() + timeout_seconds\n                wait = 1.0\n                while time.time() < timeout:\n                    try:\n                        self.head_file(uuid=file_uuid, replica=\"aws\", version=version)\n                        break\n                    except SwaggerAPIException as e:\n                        if e.code != requests.codes.not_found:\n                            msg = \"File {}: Unexpected server response during registration\"\n                            req_id = 'X-AWS-REQUEST-ID: {}'.format(response.headers.get(\"X-AWS-REQUEST-ID\"))\n                            raise RuntimeError(msg.format(filename), req_id)\n                        time.sleep(wait)\n                        wait = min(60.0, wait * self.UPLOAD_BACKOFF_FACTOR)\n                else:\n                    # timed out. :(\n                    req_id = 'X-AWS-REQUEST-ID: {}'.format(response.headers.get(\"X-AWS-REQUEST-ID\"))\n                    raise RuntimeError(\"File {}: registration FAILED\".format(filename), req_id)\n                logger.debug(\"Successfully uploaded file\")\n\n        file_args = [{'indexed': file_[\"name\"].endswith(\".json\"),\n                      'name': file_['name'],\n                      'version': file_['version'],\n                      'uuid': file_['uuid']} for file_ in files_uploaded]\n\n        logger.info(\"%s\", \"Bundle {}: Registering...\".format(bundle_uuid))\n\n        response = self.put_bundle(uuid=bundle_uuid,\n                                   version=version,\n                                   replica=replica,\n                                   creator_uid=creator_uid,\n                                   files=file_args)\n        logger.info(\"%s\", \"Bundle {}: Registered successfully\".format(bundle_uuid))\n\n        return {\n            \"bundle_uuid\": bundle_uuid,\n            \"creator_uid\": creator_uid,\n            \"replica\": replica,\n            \"version\": response[\"version\"],\n            \"files\": files_uploaded\n        }"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction that recursively locates files within a folder", "response": "def iter_paths(src_dir):\n    \"\"\"\n    Function that recursively locates files within folder\n    Note: scandir does not guarantee ordering\n    :param src_dir:  string for directory to be parsed through\n    :return an iterable of DirEntry objects all files within the src_dir\n    \"\"\"\n    for x in scandir(os.path.join(src_dir)):\n        if x.is_dir(follow_symlinks=False):\n            for x in iter_paths(x.path):\n                yield x\n        else:\n            yield x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hardlink(source, link_name):\n    if sys.version_info < (3,) and platform.system() == 'Windows':  # pragma: no cover\n        import ctypes\n        create_hard_link = ctypes.windll.kernel32.CreateHardLinkW\n        create_hard_link.argtypes = [ctypes.c_wchar_p, ctypes.c_wchar_p, ctypes.c_void_p]\n        create_hard_link.restype = ctypes.wintypes.BOOL\n        res = create_hard_link(link_name, source, None)\n        if res == 0:\n            raise ctypes.WinError()\n    else:\n        try:\n            os.link(source, link_name)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise\n            else:\n                # It's possible that the user created a different file with the same name as the\n                # one we're trying to download. Thus we need to check the if the inode is different\n                # and raise an error in this case.\n                source_stat = os.stat(source)\n                dest_stat = os.stat(link_name)\n                # Check device first because different drives can have the same inode number\n                if source_stat.st_dev != dest_stat.st_dev or source_stat.st_ino != dest_stat.st_ino:\n                    raise", "response": "Create a hardlink in a portable way."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request_with_retries_on_post_search(self, session, url, query, json_input, stream, headers):\n        # TODO: Revert this PR as soon as the appropriate swagger definitions have percolated up\n        # to prod and merged; see https://github.com/HumanCellAtlas/data-store/pull/1961\n        status_code = 500\n        if '/v1/search' in url:\n            retry_count = 10\n        else:\n            retry_count = 1\n        while status_code in (500, 502, 503, 504) and retry_count > 0:\n            try:\n                retry_count -= 1\n                res = session.request(self.http_method,\n                                      url,\n                                      params=query,\n                                      json=json_input,\n                                      stream=stream,\n                                      headers=headers,\n                                      timeout=self.client.timeout_policy)\n                status_code = res.status_code\n            except SwaggerAPIException:\n                if retry_count > 0:\n                    pass\n                else:\n                    raise\n        return res", "response": "Submit a request and retry on POST search requests specifically."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_swagger_json(swagger_json, ptr_str=\"$ref\"):\n        refs = []\n\n        def store_refs(d):\n            if len(d) == 1 and ptr_str in d:\n                refs.append(d)\n            return d\n\n        swagger_content = json.load(swagger_json, object_hook=store_refs)\n        for ref in refs:\n            _, target = ref.popitem()\n            assert target[0] == \"#\"\n            ref.update(resolve_pointer(swagger_content, target[1:]))\n        return swagger_content", "response": "Load the Swagger JSON and resolve internal JSON Pointer references."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef refresh_swagger(self):\n        try:\n            os.remove(self._get_swagger_filename(self.swagger_url))\n        except EnvironmentError as e:\n            logger.warn(os.strerror(e.errno))\n        else:\n            self.__init__()", "response": "Refresh the current version of the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures and save authentication credentials.", "response": "def login(self, access_token=\"\"):\n        \"\"\"\n        Configure and save {prog} authentication credentials.\n\n        This command may open a browser window to ask for your\n        consent to use web service authentication credentials.\n        \"\"\"\n        if access_token:\n            credentials = argparse.Namespace(token=access_token, refresh_token=None, id_token=None)\n        else:\n            scopes = [\"openid\", \"email\", \"offline_access\"]\n\n            from google_auth_oauthlib.flow import InstalledAppFlow\n            flow = InstalledAppFlow.from_client_config(self.application_secrets, scopes=scopes)\n            msg = \"Authentication successful. Please close this tab and run HCA CLI commands in the terminal.\"\n            credentials = flow.run_local_server(success_message=msg, audience=self._audience)\n\n        # TODO: (akislyuk) test token autorefresh on expiration\n        self.config.oauth2_token = dict(access_token=credentials.token,\n                                        refresh_token=credentials.refresh_token,\n                                        id_token=credentials.id_token,\n                                        expires_at=\"-1\",\n                                        token_type=\"Bearer\")\n        print(\"Storing access credentials\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the URI for an Upload Area", "response": "def area_uri(self, area_uuid):\n        \"\"\"\n        Return the URI for an Upload Area\n        :param area_uuid: UUID of area for which we want URI\n        :return: Upload Area URI object\n        :rtype: UploadAreaURI\n        :raises UploadException: if area does not exist\n        \"\"\"\n        if area_uuid not in self.areas:\n            raise UploadException(\"I don't know about area {uuid}\".format(uuid=area_uuid))\n        return UploadAreaURI(self._config.upload.areas[area_uuid]['uri'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an area to the upload area list.", "response": "def add_area(self, uri):\n        \"\"\"\n        Record information about a new Upload Area\n\n        :param UploadAreaURI uri: An Upload Area URI.\n        \"\"\"\n        if uri.area_uuid not in self._config.upload.areas:\n            self._config.upload.areas[uri.area_uuid] = {'uri': uri.uri}\n        self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_area(self, area_uuid):\n\n        self._config.upload.current_area = area_uuid\n        self.save()", "response": "Select the area with this UUID"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nforget a specific Upload Area from out cache of known areas.", "response": "def forget_area(self, area_uuid):\n        \"\"\"\n        Remove an Upload Area from out cache of known areas.\n        :param str area_uuid: The RFC4122-compliant UUID of the Upload Area.\n        \"\"\"\n        if self._config.upload.current_area == area_uuid:\n            self._config.upload.current_area = None\n        if area_uuid in self._config.upload.areas:\n            del self._config.upload.areas[area_uuid]\n            self.save()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a partial UUID returns the name of the Upload Area that matches it.", "response": "def area_uuid_from_partial_uuid(self, partial_uuid):\n        \"\"\"\n        Given a partial UUID (a prefix), see if we have know about an Upload Area matching it.\n        :param (str) partial_uuid: UUID prefix\n        :return: a matching UUID\n        :rtype: str\n        :raises UploadException: if no or more than one UUIDs match.\n        \"\"\"\n        matching_areas = [uuid for uuid in self.areas if re.match(partial_uuid, uuid)]\n        if len(matching_areas) == 0:\n            raise UploadException(\"Sorry I don't recognize area \\\"%s\\\"\" % (partial_uuid,))\n        elif len(matching_areas) == 1:\n            return matching_areas[0]\n        else:\n            raise UploadException(\n                \"\\\"%s\\\" matches more than one area, please provide more characters.\" % (partial_uuid,))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the minimum prefix required to address this Upload Area UUID uniquely.", "response": "def unique_prefix(self, area_uuid):\n        \"\"\"\n        Find the minimum prefix required to address this Upload Area UUID uniquely.\n        :param (str) area_uuid: UUID of Upload Area\n        :return: a string with the minimum prefix required to be unique\n        :rtype: str\n        \"\"\"\n        for prefix_len in range(1, len(area_uuid)):\n            prefix = area_uuid[0:prefix_len]\n            matching_areas = [uuid for uuid in self.areas if re.match(prefix, uuid)]\n            if len(matching_areas) == 1:\n                return prefix"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the number of days since the last modified time of a file", "response": "def get_days_since_last_modified(filename):\n        \"\"\"\n        :param filename: Absolute file path\n        :return: Number of days since filename's last modified time\n        \"\"\"\n        now = datetime.now()\n        last_modified = datetime.fromtimestamp(os.path.getmtime(filename))\n        return (now - last_modified).days"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an Upload Area object", "response": "def create_area(self, area_uuid):\n        \"\"\"\n        Create an Upload Area\n        :param area_uuid: UUID of Upload Area to be created\n        :return: an Upload Area object\n        :rtype: UploadArea\n        \"\"\"\n        result = self.api_client.create_area(area_uuid=area_uuid)\n        area_uri = UploadAreaURI(uri=result['uri'])\n        return UploadArea(uri=area_uri, upload_service=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_area(self, area_uuid):\n        response = self._make_request('post',\n                                      path=\"/area/{id}\".format(id=area_uuid),\n                                      headers={'Api-Key': self.auth_token})\n        return response.json()", "response": "Create an Upload Area"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if an Upload Area exists", "response": "def area_exists(self, area_uuid):\n        \"\"\"\n        Check if an Upload Area exists\n\n        :param str area_uuid: A RFC4122-compliant ID for the upload area\n        :return: True or False\n        :rtype: bool\n        \"\"\"\n        response = requests.head(self._url(path=\"/area/{id}\".format(id=area_uuid)))\n        return response.ok"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_area(self, area_uuid):\n        self._make_request('delete', path=\"/area/{id}\".format(id=area_uuid),\n                           headers={'Api-Key': self.auth_token})\n        return True", "response": "Delete an Upload Area"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef credentials(self, area_uuid):\n        response = self._make_request(\"post\", path=\"/area/{uuid}/credentials\".format(uuid=area_uuid))\n        return response.json()", "response": "Get AWS Access Key SecretKey and SessionToken for the given Upload Area."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef store_file(self, area_uuid, filename, file_content, content_type):\n        url_safe_filename = urlparse.quote(filename)\n        path = \"/area/{id}/{filename}\".format(id=area_uuid, filename=url_safe_filename)\n        response = self._make_request('put',\n                                      path=path,\n                                      data=file_content,\n                                      headers={\n                                          'Api-Key': self.auth_token,\n                                          'Content-Type': content_type\n                                      })\n        return response.json()", "response": "Store a small file in an Upload Area and return the JSON response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_upload_notification(self, area_uuid, filename):\n        url_safe_filename = urlparse.quote(filename)\n        path = (\"/area/{area_uuid}/{filename}\".format(area_uuid=area_uuid, filename=url_safe_filename))\n        response = self._make_request('post', path=path)\n        return response.ok", "response": "Notify Upload Service that a file has been placed in an Upload Area"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef files_info(self, area_uuid, file_list):\n        path = \"/area/{uuid}/files_info\".format(uuid=area_uuid)\n        file_list = [urlparse.quote(filename) for filename in file_list]\n        response = self._make_request('put', path=path, json=file_list)\n        return response.json()", "response": "Get information about files in an Upload Area"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef checksum_status(self, area_uuid, filename):\n        url_safe_filename = urlparse.quote(filename)\n        path = \"/area/{uuid}/{filename}/checksum\".format(uuid=area_uuid, filename=url_safe_filename)\n        response = self._make_request('get', path)\n        return response.json()", "response": "Retrieve checksum status and values for a file in the Upload Area"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate a list of files within an Upload Area and give it access to the file.", "response": "def validate_files(self, area_uuid, file_list, validator_image, original_validation_id=\"\", environment={}):\n        \"\"\"\n        Invoke supplied validator Docker image and give it access to the file/s.\n        The validator must be based off the base validator Docker image.\n\n        :param str area_uuid: A RFC4122-compliant ID for the upload area\n        :param list file_list: A list of files within the Upload Area to be validated\n        :param str validator_image: the location of a docker image to use for validation\n        :param str original_validation_id: [optional]\n        :param dict environment: [optional] list of environment variable to set for the validator\n        :return: ID of scheduled validation\n        :rtype: dict\n        :raises UploadApiException: if information could not be obtained\n        \"\"\"\n        path = \"/area/{uuid}/validate\".format(uuid=area_uuid)\n        file_list = [urlparse.quote(filename) for filename in file_list]\n        payload = {\n            \"environment\": environment,\n            \"files\": file_list,\n            \"original_validation_id\": original_validation_id,\n            \"validator_image\": validator_image\n        }\n        result = self._make_request('put', path=path, json=payload, headers={'Api-Key': self.auth_token})\n        return result.json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting count of validation statuses for all files in the upload area.", "response": "def validation_statuses(self, area_uuid):\n        \"\"\"\n        Get count of validation statuses for all files in upload_area\n\n        :param str area_uuid: A RFC4122-compliant ID for the upload area\n        :return: a dict with key for each state and value being the count of files in that state\n        :rtype: dict\n        :raises UploadApiException: if information could not be obtained\n        \"\"\"\n        path = \"/area/{uuid}/validations\".format(uuid=area_uuid)\n        result = self._make_request('get', path)\n        return result.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef language_name(self, text: str) -> str:\n        values = extract(text)\n        input_fn = _to_func(([values], []))\n        pos: int = next(self._classifier.predict_classes(input_fn=input_fn))\n\n        LOGGER.debug(\"Predicted language position %s\", pos)\n        return sorted(self.languages)[pos]", "response": "Predict the programming language name of the given source code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef probable_languages(\n            self,\n            text: str,\n            max_languages: int = 3) -> Tuple[str, ...]:\n        \"\"\"List of most probable programming languages,\n        the list is ordered from the most probable to the least probable one.\n\n        :param text: source code.\n        :param max_languages: maximum number of listed languages.\n        :return: languages list\n        \"\"\"\n        scores = self.scores(text)\n\n        # Sorted from the most probable language to the least probable\n        sorted_scores = sorted(scores.items(), key=itemgetter(1), reverse=True)\n        languages, probabilities = list(zip(*sorted_scores))\n\n        # Find the most distant consecutive languages.\n        # A logarithmic scale is used here because the probabilities\n        # are most of the time really close to zero\n        rescaled_probabilities = [log(proba) for proba in probabilities]\n        distances = [\n            rescaled_probabilities[pos] - rescaled_probabilities[pos+1]\n            for pos in range(len(rescaled_probabilities)-1)]\n\n        max_distance_pos = max(enumerate(distances, 1), key=itemgetter(1))[0]\n        limit = min(max_distance_pos, max_languages)\n        return languages[:limit]", "response": "Return a list of most probable programming languages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef learn(self, input_dir: str) -> float:\n        if self.is_default:\n            LOGGER.error(\"Cannot learn using default model\")\n            raise GuesslangError('Cannot learn using default \"readonly\" model')\n\n        languages = self.languages\n\n        LOGGER.info(\"Extract training data\")\n        extensions = [ext for exts in languages.values() for ext in exts]\n        files = search_files(input_dir, extensions)\n        nb_files = len(files)\n        chunk_size = min(int(CHUNK_PROPORTION * nb_files), CHUNK_SIZE)\n\n        LOGGER.debug(\"Evaluation files count: %d\", chunk_size)\n        LOGGER.debug(\"Training files count: %d\", nb_files - chunk_size)\n        batches = _pop_many(files, chunk_size)\n\n        LOGGER.debug(\"Prepare evaluation data\")\n        evaluation_data = extract_from_files(next(batches), languages)\n        LOGGER.debug(\"Evaluation data count: %d\", len(evaluation_data[0]))\n\n        accuracy = 0\n        total = ceil(nb_files / chunk_size) - 1\n        LOGGER.info(\"Start learning\")\n        for pos, training_files in enumerate(batches, 1):\n            LOGGER.info(\"Step %.2f%%\", 100 * pos / total)\n\n            LOGGER.debug(\"Training data extraction\")\n            training_data = extract_from_files(training_files, languages)\n            LOGGER.debug(\"Training data count: %d\", len(training_data[0]))\n\n            steps = int(FITTING_FACTOR * len(training_data[0]) / 100)\n            LOGGER.debug(\"Fitting, steps count: %d\", steps)\n            self._classifier.fit(input_fn=_to_func(training_data), steps=steps)\n\n            LOGGER.debug(\"Evaluation\")\n            accuracy = self._classifier.evaluate(\n                input_fn=_to_func(evaluation_data), steps=1)['accuracy']\n            _comment(accuracy)\n\n        return accuracy", "response": "Learn languages features from source files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        'reportfile', type=argparse.FileType('r'),\n        help=\"test report file generated by `guesslang --test TESTDIR`\")\n    parser.add_argument(\n        '-d', '--debug', default=False, action='store_true',\n        help=\"show debug messages\")\n\n    args = parser.parse_args()\n    config_logging(args.debug)\n\n    report = json.load(args.reportfile)\n    graph_data = _build_graph(report)\n    index_path = _prepare_resources(graph_data)\n    webbrowser.open(str(index_path))", "response": "Report graph creator command line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_files(source: str, extensions: List[str]) -> List[Path]:\n    files = [\n        path for path in Path(source).glob('**/*')\n        if path.is_file() and path.suffix.lstrip('.') in extensions]\n    nb_files = len(files)\n    LOGGER.debug(\"Total files found: %d\", nb_files)\n\n    if nb_files < NB_FILES_MIN:\n        LOGGER.error(\"Too few source files\")\n        raise GuesslangError(\n            '{} source files found in {}. {} files minimum is required'.format(\n                nb_files, source, NB_FILES_MIN))\n\n    random.shuffle(files)\n    return files", "response": "Search the directory and its subdirectories for files that match one of the specified extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts arrays of features from the given files.", "response": "def extract_from_files(\n        files: List[Path],\n        languages: Dict[str, List[str]]) -> DataSet:\n    \"\"\"Extract arrays of features from the given files.\n\n    :param files: list of paths\n    :param languages: language name =>\n        associated file extension list\n    :return: features\n    \"\"\"\n    enumerator = enumerate(sorted(languages.items()))\n    rank_map = {ext: rank for rank, (_, exts) in enumerator for ext in exts}\n\n    with multiprocessing.Pool(initializer=_process_init) as pool:\n        file_iterator = ((path, rank_map) for path in files)\n        arrays = _to_arrays(pool.starmap(_extract_features, file_iterator))\n\n    LOGGER.debug(\"Extracted arrays count: %d\", len(arrays[0]))\n    return arrays"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_read_file(file_path: Path) -> str:\n    for encoding in FILE_ENCODINGS:\n        try:\n            return file_path.read_text(encoding=encoding)\n        except UnicodeError:\n            pass  # Ignore encoding error\n\n    raise GuesslangError('Encoding not supported for {!s}'.format(file_path))", "response": "Read a text file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef config_logging(debug: bool = False) -> None:\n    if debug:\n        level = 'DEBUG'\n        tf_level = tf.logging.INFO\n    else:\n        level = 'INFO'\n        tf_level = tf.logging.ERROR\n\n    logging_config = config_dict('logging.json')\n    for logger in logging_config['loggers'].values():\n        logger['level'] = level\n\n    logging.config.dictConfig(logging_config)\n    tf.logging.set_verbosity(tf_level)", "response": "Set - up application and tensorflow logging."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef config_dict(name: str) -> Dict[str, Any]:\n    try:\n        content = resource_string(PACKAGE, DATADIR.format(name)).decode()\n    except DistributionNotFound as error:\n        LOGGER.warning(\"Cannot load %s from packages: %s\", name, error)\n        content = DATA_FALLBACK.joinpath(name).read_text()\n\n    return cast(Dict[str, Any], json.loads(content))", "response": "Load a JSON configuration dictionary from Guesslang config directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve Guesslang model directory name and tells if it is the default model.", "response": "def model_info(model_dir: Optional[str] = None) -> Tuple[str, bool]:\n    \"\"\"Retrieve Guesslang model directory name,\n    and tells if it is the default model.\n\n    :param model_dir: model location, if `None` default model is selected\n    :return: selected model directory with an indication\n        that the model is the default or not\n    \"\"\"\n    if model_dir is None:\n        try:\n            model_dir = resource_filename(PACKAGE, DATADIR.format('model'))\n        except DistributionNotFound as error:\n            LOGGER.warning(\"Cannot load model from packages: %s\", error)\n            model_dir = str(DATA_FALLBACK.joinpath('model').absolute())\n        is_default_model = True\n    else:\n        is_default_model = False\n\n    model_path = Path(model_dir)\n    model_path.mkdir(exist_ok=True)\n    LOGGER.debug(\"Using model: %s, default: %s\", model_path, is_default_model)\n\n    return (model_dir, is_default_model)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format(self, record: logging.LogRecord) -> str:\n        if platform.system() != 'Linux':  # Avoid funny logs on Windows & MacOS\n            return super().format(record)\n\n        record.msg = (\n            self.STYLE[record.levelname] + record.msg + self.STYLE['END'])\n        record.levelname = (\n            self.STYLE['LEVEL'] + record.levelname + self.STYLE['END'])\n        return super().format(record)", "response": "Format log records to produce colored messages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\n        'githubtoken',\n        help=\"Github OAuth token, see https://developer.github.com/v3/oauth/\")\n    parser.add_argument('destination', help=\"location of the downloaded repos\")\n    parser.add_argument(\n        '-n', '--nbrepo', help=\"number of repositories per language\",\n        type=int, default=1000)\n    parser.add_argument(\n        '-d', '--debug', default=False, action='store_true',\n        help=\"show debug messages\")\n\n    args = parser.parse_args()\n    config_logging(args.debug)\n\n    destination = Path(args.destination)\n    nb_repos = args.nbrepo\n    token = args.githubtoken\n\n    languages = config_dict('languages.json')\n    destination.mkdir(exist_ok=True)\n\n    for pos, language in enumerate(sorted(languages), 1):\n        LOGGER.info(\"Step %.2f%%, %s\", 100 * pos / len(languages), language)\n        LOGGER.info(\"Fetch %d repos infos for language %s\", nb_repos, language)\n        repos = _retrieve_repo_details(language, nb_repos, token)\n        LOGGER.info(\"%d repos details kept. Downloading\", len(repos))\n        _download_repos(language, repos, destination)\n        LOGGER.info(\"Language %s repos downloaded\", language)\n\n    LOGGER.debug(\"Exit OK\")", "response": "Github repositories downloaded command line"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retry(default=None):\n\n    def decorator(func):\n        \"\"\"Retry decorator\"\"\"\n\n        @functools.wraps(func)\n        def _wrapper(*args, **kw):\n            for pos in range(1, MAX_RETRIES):\n                try:\n                    return func(*args, **kw)\n                except (RuntimeError, requests.ConnectionError) as error:\n                    LOGGER.warning(\"Failed: %s, %s\", type(error), error)\n\n                # Wait a bit before retrying\n                for _ in range(pos):\n                    _rest()\n\n            LOGGER.warning(\"Request Aborted\")\n            return default\n\n        return _wrapper\n\n    return decorator", "response": "Decorator to retry functions after failures"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split(text: str) -> List[str]:\n    return [word for word in SEPARATOR.split(text) if word.strip(' \\t')]", "response": "Split a text into a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfile extractor command line", "response": "def main():\n    \"\"\"Files extractor command line\"\"\"\n\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.add_argument('source', help=\"location of the downloaded repos\")\n    parser.add_argument('destination', help=\"location of the extracted files\")\n    parser.add_argument(\n        '-t', '--nb-test-files', help=\"number of testing files per language\",\n        type=int, default=5000)\n    parser.add_argument(\n        '-l', '--nb-learn-files', help=\"number of learning files per language\",\n        type=int, default=10000)\n    parser.add_argument(\n        '-r', '--remove', help=\"remove repos that cannot be read\",\n        action='store_true', default=False)\n    parser.add_argument(\n        '-d', '--debug', default=False, action='store_true',\n        help=\"show debug messages\")\n\n    args = parser.parse_args()\n    config_logging(args.debug)\n\n    source = Path(args.source)\n    destination = Path(args.destination)\n    nb_test = args.nb_test_files\n    nb_learn = args.nb_learn_files\n    remove = args.remove\n\n    repos = _find_repos(source)\n    split_repos = _split_repos(repos, nb_test, nb_learn)\n    split_files = _find_files(*split_repos, nb_test, nb_learn, remove)\n    _unzip_all(*split_files, destination)\n    LOGGER.info(\"Files saved into %s\", destination)\n    LOGGER.debug(\"Exit OK\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a list of pydicom datasets for an image series stitch them together into a single three - dimensional numpy array.", "response": "def combine_slices(slice_datasets, rescale=None):\n    '''\n    Given a list of pydicom datasets for an image series, stitch them together into a\n    three-dimensional numpy array.  Also calculate a 4x4 affine transformation\n    matrix that converts the ijk-pixel-indices into the xyz-coordinates in the\n    DICOM patient's coordinate system.\n\n    Returns a two-tuple containing the 3D-ndarray and the affine matrix.\n\n    If `rescale` is set to `None` (the default), then the image array dtype\n    will be preserved, unless any of the DICOM images contain either the\n    `Rescale Slope\n    <https://dicom.innolitics.com/ciods/ct-image/ct-image/00281053>`_ or the\n    `Rescale Intercept <https://dicom.innolitics.com/ciods/ct-image/ct-image/00281052>`_\n    attributes.  If either of these attributes are present, they will be\n    applied to each slice individually.\n\n    If `rescale` is `True` the voxels will be cast to `float32`, if set to\n    `False`, the original dtype will be preserved even if DICOM rescaling information is present.\n\n    The returned array has the column-major byte-order.\n\n    This function requires that the datasets:\n\n    - Be in same series (have the same\n      `Series Instance UID <https://dicom.innolitics.com/ciods/ct-image/general-series/0020000e>`_,\n      `Modality <https://dicom.innolitics.com/ciods/ct-image/general-series/00080060>`_,\n      and `SOP Class UID <https://dicom.innolitics.com/ciods/ct-image/sop-common/00080016>`_).\n    - The binary storage of each slice must be the same (have the same\n      `Bits Allocated <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280100>`_,\n      `Bits Stored <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280101>`_,\n      `High Bit <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280102>`_, and\n      `Pixel Representation <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280103>`_).\n    - The image slice must approximately form a grid. This means there can not\n      be any missing internal slices (missing slices on the ends of the dataset\n      are not detected).\n    - It also means that  each slice must have the same\n      `Rows <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280010>`_,\n      `Columns <https://dicom.innolitics.com/ciods/ct-image/image-pixel/00280011>`_,\n      `Pixel Spacing <https://dicom.innolitics.com/ciods/ct-image/image-plane/00280030>`_, and\n      `Image Orientation (Patient) <https://dicom.innolitics.com/ciods/ct-image/image-plane/00200037>`_\n      attribute values.\n    - The direction cosines derived from the\n      `Image Orientation (Patient) <https://dicom.innolitics.com/ciods/ct-image/image-plane/00200037>`_\n      attribute must, within 1e-4, have a magnitude of 1.  The cosines must\n      also be approximately perpendicular (their dot-product must be within\n      1e-4 of 0).  Warnings are displayed if any of these approximations are\n      below 1e-8, however, since we have seen real datasets with values up to\n      1e-4, we let them pass.\n    - The `Image Position (Patient) <https://dicom.innolitics.com/ciods/ct-image/image-plane/00200032>`_\n      values must approximately form a line.\n\n    If any of these conditions are not met, a `dicom_numpy.DicomImportException` is raised.\n    '''\n    if len(slice_datasets) == 0:\n        raise DicomImportException(\"Must provide at least one DICOM dataset\")\n\n    _validate_slices_form_uniform_grid(slice_datasets)\n\n    voxels = _merge_slice_pixel_arrays(slice_datasets, rescale)\n    transform = _ijk_to_patient_xyz_transform_matrix(slice_datasets)\n\n    return voxels, transform"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_slices_form_uniform_grid(slice_datasets):\n    '''\n    Perform various data checks to ensure that the list of slices form a\n    evenly-spaced grid of data.\n    Some of these checks are probably not required if the data follows the\n    DICOM specification, however it seems pertinent to check anyway.\n    '''\n    invariant_properties = [\n        'Modality',\n        'SOPClassUID',\n        'SeriesInstanceUID',\n        'Rows',\n        'Columns',\n        'PixelSpacing',\n        'PixelRepresentation',\n        'BitsAllocated',\n        'BitsStored',\n        'HighBit',\n    ]\n\n    for property_name in invariant_properties:\n        _slice_attribute_equal(slice_datasets, property_name)\n\n    _validate_image_orientation(slice_datasets[0].ImageOrientationPatient)\n    _slice_ndarray_attribute_almost_equal(slice_datasets, 'ImageOrientationPatient', 1e-5)\n\n    slice_positions = _slice_positions(slice_datasets)\n    _check_for_missing_slices(slice_positions)", "response": "Validate that the list of slices form a evenly - spaced grid of data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_image_orientation(image_orientation):\n    '''\n    Ensure that the image orientation is supported\n    - The direction cosines have magnitudes of 1 (just in case)\n    - The direction cosines are perpendicular\n    '''\n    row_cosine, column_cosine, slice_cosine = _extract_cosines(image_orientation)\n\n    if not _almost_zero(np.dot(row_cosine, column_cosine), 1e-4):\n        raise DicomImportException(\"Non-orthogonal direction cosines: {}, {}\".format(row_cosine, column_cosine))\n    elif not _almost_zero(np.dot(row_cosine, column_cosine), 1e-8):\n        logger.warning(\"Direction cosines aren't quite orthogonal: {}, {}\".format(row_cosine, column_cosine))\n\n    if not _almost_one(np.linalg.norm(row_cosine), 1e-4):\n        raise DicomImportException(\"The row direction cosine's magnitude is not 1: {}\".format(row_cosine))\n    elif not _almost_one(np.linalg.norm(row_cosine), 1e-8):\n        logger.warning(\"The row direction cosine's magnitude is not quite 1: {}\".format(row_cosine))\n\n    if not _almost_one(np.linalg.norm(column_cosine), 1e-4):\n        raise DicomImportException(\"The column direction cosine's magnitude is not 1: {}\".format(column_cosine))\n    elif not _almost_one(np.linalg.norm(column_cosine), 1e-8):\n        logger.warning(\"The column direction cosine's magnitude is not quite 1: {}\".format(column_cosine))", "response": "Validate the image orientation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a URL string into a dict of key - value pairs.", "response": "def parse_url(cls, string):  # pylint: disable=redefined-outer-name\n        \"\"\"\n        If it can be parsed as a version_guid with no preceding org + offering, returns a dict\n        with key 'version_guid' and the value,\n\n        If it can be parsed as a org + offering, returns a dict\n        with key 'id' and optional keys 'branch' and 'version_guid'.\n\n        Raises:\n            InvalidKeyError: if string cannot be parsed -or- string ends with a newline.\n        \"\"\"\n        match = cls.URL_RE.match(string)\n        if not match:\n            raise InvalidKeyError(cls, string)\n        return match.groupdict()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the name of the offering for this instance.", "response": "def offering(self):\n        \"\"\"\n        Deprecated. Use course and run independently.\n        \"\"\"\n        warnings.warn(\n            \"Offering is no longer a supported property of Locator. Please use the course and run properties.\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        if not self.course and not self.run:\n            return None\n        elif not self.run and self.course:\n            return self.course\n        return \"/\".join([self.course, self.run])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a CourseLocator parsing the given string", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        Return a CourseLocator parsing the given serialized string\n        :param serialized: matches the string to a CourseLocator\n        \"\"\"\n        parse = cls.parse_url(serialized)\n\n        if parse['version_guid']:\n            parse['version_guid'] = cls.as_object_id(parse['version_guid'])\n\n        return cls(**{key: parse.get(key) for key in cls.KEY_FIELDS})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeprecating mechanism for creating a UsageKey given a CourseKey and a serialized Location.", "response": "def make_usage_key_from_deprecated_string(self, location_url):\n        \"\"\"\n        Deprecated mechanism for creating a UsageKey given a CourseKey and a serialized Location.\n\n        NOTE: this prejudicially takes the tag, org, and course from the url not self.\n\n        Raises:\n            InvalidKeyError: if the url does not parse\n        \"\"\"\n        warnings.warn(\n            \"make_usage_key_from_deprecated_string is deprecated! Please use make_usage_key\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        return BlockUsageLocator.from_string(location_url).replace(run=self.run)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_deprecated_string(self):\n        return u'/'.join([self.org, self.course, self.run])", "response": "Returns an old - style course id represented as org / course / run"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an instance of cls parsed from its deprecated serialized form.", "response": "def _from_deprecated_string(cls, serialized):\n        \"\"\"\n        Return an instance of `cls` parsed from its deprecated `serialized` form.\n\n        This will be called only if :meth:`OpaqueKey.from_string` is unable to\n        parse a key out of `serialized`, and only if `set_deprecated_fallback` has\n        been called to register a fallback class.\n\n        Args:\n            cls: The :class:`OpaqueKey` subclass.\n            serialized (unicode): A serialized :class:`OpaqueKey`, with namespace already removed.\n\n        Raises:\n            InvalidKeyError: Should be raised if `serialized` is not a valid serialized key\n                understood by `cls`.\n        \"\"\"\n        if serialized.count('/') != 2:\n            raise InvalidKeyError(cls, serialized)\n\n        return cls(*serialized.split('/'), deprecated=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef for_branch(self, branch):\n        if self.org is None and branch is not None:\n            raise InvalidKeyError(self.__class__, \"Branches must have full library ids not just versions\")\n        return self.replace(branch=branch, version_guid=None)", "response": "Returns a new CourseLocator for another branch."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a string representing this location.", "response": "def _to_string(self):\n        \"\"\"\n        Return a string representing this location.\n        \"\"\"\n        parts = []\n        if self.library:  # pylint: disable=no-member\n            parts.extend([self.org, self.library])  # pylint: disable=no-member\n            if self.branch:  # pylint: disable=no-member\n                parts.append(u\"{prefix}@{branch}\".format(prefix=self.BRANCH_PREFIX, branch=self.branch))  # pylint: disable=no-member\n        if self.version_guid:  # pylint: disable=no-member\n            parts.append(u\"{prefix}@{guid}\".format(prefix=self.VERSION_PREFIX, guid=self.version_guid))  # pylint: disable=no-member\n        return u\"+\".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _from_string(cls, serialized):\n        # Allow access to _from_string protected method\n        course_key = CourseLocator._from_string(serialized)  # pylint: disable=protected-access\n        parsed_parts = cls.parse_url(serialized)\n        block_id = parsed_parts.get('block_id', None)\n        if block_id is None:\n            raise InvalidKeyError(cls, serialized)\n        return cls(course_key, parsed_parts.get('block_type'), block_id)", "response": "Requests CourseLocator to deserialize its part and adds the local deserialization of block\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef for_branch(self, branch):\n        return self.replace(course_key=self.course_key.for_branch(branch))", "response": "Return a UsageLocator for the same block in a different branch of the course."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a UsageLocator for the same block in a different branch of the course.", "response": "def for_version(self, version_guid):\n        \"\"\"\n        Return a UsageLocator for the same block in a different branch of the course.\n        \"\"\"\n        return self.replace(course_key=self.course_key.for_version(version_guid))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a block reference into a valid block reference.", "response": "def _parse_block_ref(cls, block_ref, deprecated=False):\n        \"\"\"\n        Given `block_ref`, tries to parse it into a valid block reference.\n\n        Returns `block_ref` if it is valid.\n\n        Raises:\n            InvalidKeyError: if `block_ref` is invalid.\n        \"\"\"\n\n        if deprecated and block_ref is None:\n            return None\n\n        if isinstance(block_ref, LocalId):\n            return block_ref\n\n        is_valid_deprecated = deprecated and cls.DEPRECATED_ALLOWED_ID_RE.match(block_ref)\n        is_valid = cls.ALLOWED_ID_RE.match(block_ref)\n\n        if is_valid or is_valid_deprecated:\n            return block_ref\n        else:\n            raise InvalidKeyError(cls, block_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_relative(cls, course_locator, block_type, block_id):\n        if hasattr(course_locator, 'course_key'):\n            course_locator = course_locator.course_key\n        return course_locator.make_usage_key(\n            block_type=block_type,\n            block_id=block_id\n        )", "response": "Return a new instance which has the given block_id in the given course_key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_string(self):\n        # Allow access to _to_string protected method\n        return u\"{course_key}+{BLOCK_TYPE_PREFIX}@{block_type}+{BLOCK_PREFIX}@{block_id}\".format(\n            course_key=self.course_key._to_string(),  # pylint: disable=protected-access\n            BLOCK_TYPE_PREFIX=self.BLOCK_TYPE_PREFIX,\n            block_type=self.block_type,\n            BLOCK_PREFIX=self.BLOCK_PREFIX,\n            block_id=self.block_id\n        )", "response": "Return a string representing this location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an id which can be used on an html page as an id attr of an html element.", "response": "def html_id(self):\n        \"\"\"\n        Return an id which can be used on an html page as an id attr of an html element.  It is currently also\n        persisted by some clients to identify blocks.\n\n        To make compatible with old Location object functionality. I don't believe this behavior fits at this\n        place, but I have no way to override. We should clearly define the purpose and restrictions of this\n        (e.g., I'm assuming periods are fine).\n        \"\"\"\n        if self.deprecated:\n            id_fields = [self.DEPRECATED_TAG, self.org, self.course, self.block_type, self.block_id, self.version_guid]\n            id_string = u\"-\".join([v for v in id_fields if v is not None])\n            return self.clean_for_html(id_string)\n        else:\n            return self.block_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_deprecated_string(self):\n        # pylint: disable=missing-format-attribute\n        url = u\"{0.DEPRECATED_TAG}://{0.course_key.org}/{0.course_key.course}/{0.block_type}/{0.block_id}\".format(self)\n        if self.course_key.branch:\n            url += u\"@{rev}\".format(rev=self.course_key.branch)\n        return url", "response": "Returns an old - style location represented as :\n        i4x://org / course / category / name[@revision > optional"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an instance of cls parsed from its deprecated serialized form.", "response": "def _from_deprecated_string(cls, serialized):\n        \"\"\"\n        Return an instance of `cls` parsed from its deprecated `serialized` form.\n\n        This will be called only if :meth:`OpaqueKey.from_string` is unable to\n        parse a key out of `serialized`, and only if `set_deprecated_fallback` has\n        been called to register a fallback class.\n\n        Args:\n            cls: The :class:`OpaqueKey` subclass.\n            serialized (unicode): A serialized :class:`OpaqueKey`, with namespace already removed.\n\n        Raises:\n            InvalidKeyError: Should be raised if `serialized` is not a valid serialized key\n                understood by `cls`.\n        \"\"\"\n        match = cls.DEPRECATED_URL_RE.match(serialized)\n        if match is None:\n            raise InvalidKeyError(BlockUsageLocator, serialized)\n        groups = match.groupdict()\n        course_key = CourseLocator(\n            org=groups['org'],\n            course=groups['course'],\n            run=None,\n            branch=groups.get('revision'),\n            deprecated=True,\n        )\n        return cls(course_key, groups['category'], groups['name'], deprecated=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_deprecated_son(self, prefix='', tag='i4x'):\n        # This preserves the old SON keys ('tag', 'org', 'course', 'category', 'name', 'revision'),\n        # because that format was used to store data historically in mongo\n\n        # adding tag b/c deprecated form used it\n        son = SON({prefix + 'tag': tag})\n        for field_name in ('org', 'course'):\n            # Temporary filtering of run field because deprecated form left it out\n            son[prefix + field_name] = getattr(self.course_key, field_name)\n        for (dep_field_name, field_name) in [('category', 'block_type'), ('name', 'block_id')]:\n            son[prefix + dep_field_name] = getattr(self, field_name)\n\n        son[prefix + 'revision'] = self.course_key.branch\n        return son", "response": "Returns a SON object that represents this location"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _from_deprecated_son(cls, id_dict, run):\n        course_key = CourseLocator(\n            id_dict['org'],\n            id_dict['course'],\n            run,\n            id_dict['revision'],\n            deprecated=True,\n        )\n        return cls(course_key, id_dict['category'], id_dict['name'], deprecated=True)", "response": "Return the Location decoding this id_dict and run"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new instance of the class from a string representation of a libraryLocator.", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        Requests LibraryLocator to deserialize its part and then adds the local deserialization of block\n        \"\"\"\n        # Allow access to _from_string protected method\n        library_key = LibraryLocator._from_string(serialized)  # pylint: disable=protected-access\n        parsed_parts = LibraryLocator.parse_url(serialized)\n\n        block_id = parsed_parts.get('block_id', None)\n        if block_id is None:\n            raise InvalidKeyError(cls, serialized)\n\n        block_type = parsed_parts.get('block_type')\n        if block_type is None:\n            raise InvalidKeyError(cls, serialized)\n\n        return cls(library_key, parsed_parts.get('block_type'), block_id)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef for_branch(self, branch):\n        return self.replace(library_key=self.library_key.for_branch(branch))", "response": "Return a UsageLocator for the same block in a different branch of the library."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a UsageLocator for the same block in a different version of the library.", "response": "def for_version(self, version_guid):\n        \"\"\"\n        Return a UsageLocator for the same block in a different version of the library.\n        \"\"\"\n        return self.replace(library_key=self.library_key.for_version(version_guid))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_string(self):\n        return u\"{}+{}@{}\".format(text_type(self.definition_id), self.BLOCK_TYPE_PREFIX, self.block_type)", "response": "Return a string representing this location."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a DefinitionLocator parsing the given string", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        Return a DefinitionLocator parsing the given serialized string\n        :param serialized: matches the string to\n        \"\"\"\n        parse = cls.URL_RE.match(serialized)\n        if not parse:\n            raise InvalidKeyError(cls, serialized)\n\n        parse = parse.groupdict()\n        if parse['definition_id']:\n            parse['definition_id'] = cls.as_object_id(parse['definition_id'])\n\n        return cls(**{key: parse.get(key) for key in cls.KEY_FIELDS})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an old - style location represented as :", "response": "def _to_deprecated_string(self):\n        \"\"\"\n        Returns an old-style location, represented as:\n\n        /c4x/org/course/category/name\n        \"\"\"\n        # pylint: disable=missing-format-attribute\n        url = u\"/{0.DEPRECATED_TAG}/{0.course_key.org}/{0.course_key.course}/{0.block_type}/{0.block_id}\".format(self)\n        if self.course_key.branch:\n            url += u'@{}'.format(self.course_key.branch)\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list representation of the current object.", "response": "def to_deprecated_list_repr(self):\n        \"\"\"\n        Thumbnail locations are stored as lists [c4x, org, course, thumbnail, path, None] in contentstore.mongo\n        That should be the only use of this method, but the method is general enough to provide the pre-opaque\n        Location fields as an array in the old order with the tag.\n        \"\"\"\n        return ['c4x', self.org, self.course, self.block_type, self.block_id, None]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _strip_object(key):\n    if hasattr(key, 'version_agnostic') and hasattr(key, 'for_branch'):\n        return key.for_branch(None).version_agnostic()\n    else:\n        return key", "response": "Strips branch and version info if the given key supports those attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating Empty values otherwise defer to the parent", "response": "def validate(self, value, model_instance):\n        \"\"\"Validate Empty values, otherwise defer to the parent\"\"\"\n        # raise validation error if the use of this field says it can't be blank but it is\n        if not self.blank and value is self.Empty:\n            raise ValidationError(self.error_messages['blank'])\n        else:\n            return super(OpaqueKeyField, self).validate(value, model_instance)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_validators(self, value):\n        if value is self.Empty:\n            return\n\n        return super(OpaqueKeyField, self).run_validators(value)", "response": "Validate Empty values otherwise defer to the parent"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new SlashSeparatedCourseKey with specific kwargs replacing any existing values.", "response": "def replace(self, **kwargs):\n        \"\"\"\n        Return: a new :class:`SlashSeparatedCourseKey` with specific ``kwargs`` replacing\n            their corresponding values.\n\n        Using CourseLocator's replace function results in a mismatch of __init__ args and kwargs.\n            Replace tries to instantiate a SlashSeparatedCourseKey object with CourseLocator args and kwargs.\n        \"\"\"\n        # Deprecation value is hard coded as True in __init__ and therefore does not need to be passed through.\n        return SlashSeparatedCourseKey(\n            kwargs.pop('org', self.org),\n            kwargs.pop('course', self.course),\n            kwargs.pop('run', self.run),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _deprecation_warning(cls):\n        if issubclass(cls, Location):\n            warnings.warn(\n                \"Location is deprecated! Please use locator.BlockUsageLocator\",\n                DeprecationWarning,\n                stacklevel=3\n            )\n        elif issubclass(cls, AssetLocation):\n            warnings.warn(\n                \"AssetLocation is deprecated! Please use locator.AssetLocator\",\n                DeprecationWarning,\n                stacklevel=3\n            )\n        else:\n            warnings.warn(\n                \"{} is deprecated!\".format(cls),\n                DeprecationWarning,\n                stacklevel=3\n            )", "response": "Display a deprecation warning for the given cls."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _from_deprecated_son(cls, id_dict, run):\n        cls._deprecation_warning()\n        return BlockUsageLocator._from_deprecated_son(id_dict, run)", "response": "Deprecated. See BlockUsageLocator. _from_deprecated_son"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new Location object with specific kwargs replacing those values.", "response": "def replace(self, **kwargs):\n        \"\"\"\n        Return: a new :class:`Location` with specific ``kwargs`` replacing\n            their corresponding values.\n\n        Using BlockUsageLocator's replace function results in a mismatch of __init__ args and kwargs.\n            Replace tries to instantiate a Location object with BlockUsageLocator's args and kwargs.\n        \"\"\"\n        #  NOTE: Deprecation value is hard coded as True in __init__ and therefore does not need to be passed through.\n        return Location(\n            kwargs.pop('org', self.course_key.org),\n            kwargs.pop('course', self.course_key.course),\n            kwargs.pop('run', self.course_key.run),\n            kwargs.pop('category', self.block_type),\n            kwargs.pop('name', self.block_id),\n            revision=kwargs.pop('revision', self.branch),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new object from a string representation of a CourseEntry.", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        see super\n        \"\"\"\n        # Allow access to _from_string protected method\n        parsed_parts = cls.parse_url(serialized)\n        course_key = CourseLocator(\n            parsed_parts.get('org'), parsed_parts.get('course'), parsed_parts.get('run'),\n            # specifically not saying deprecated=True b/c that would lose the run on serialization\n        )\n        block_id = parsed_parts.get('block_id')\n        return cls(course_key, parsed_parts.get('block_type'), block_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string representing this location.", "response": "def _to_string(self):\n        \"\"\"\n        Return a string representing this location.\n        \"\"\"\n        parts = [self.org, self.course, self.run, self.block_type, self.block_id]\n        return u\"+\".join(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new AssetLocation object with specific kwargs replacing any existing values.", "response": "def replace(self, **kwargs):\n        \"\"\"\n        Return: a new :class:`AssetLocation` with specific ``kwargs`` replacing\n            their corresponding values.\n\n        Using AssetLocator's replace function results in a mismatch of __init__ args and kwargs.\n            Replace tries to instantiate an AssetLocation object with AssetLocators args and kwargs.\n        \"\"\"\n        # NOTE: Deprecation value is hard coded as True in __init__ and therefore does not need to be passed through.\n        return AssetLocation(\n            kwargs.pop('org', self.org),\n            kwargs.pop('course', self.course),\n            kwargs.pop('run', self.run),\n            kwargs.pop('category', self.block_type),\n            kwargs.pop('name', self.block_id),\n            revision=kwargs.pop('revision', self.branch),\n            **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an instance of cls parsed from its serialized form.", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        Return an instance of `cls` parsed from its `serialized` form.\n\n        Args:\n            cls: The :class:`OpaqueKey` subclass.\n            serialized (unicode): A serialized :class:`OpaqueKey`, with namespace already removed.\n\n        Raises:\n            InvalidKeyError: Should be raised if `serialized` is not a valid serialized key\n                understood by `cls`.\n        \"\"\"\n        if ':' not in serialized:\n            raise InvalidKeyError(\n                \"BlockTypeKeyV1 keys must contain ':' separating the block family from the block_type.\", serialized)\n        family, __, block_type = serialized.partition(':')\n        return cls(family, block_type)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _decode_v1(value):\n    decode_colons = value.replace('$::', '::')\n    decode_dollars = decode_colons.replace('$$', '$')\n\n    reencoded = _encode_v1(decode_dollars)\n    if reencoded != value:\n        raise ValueError('Ambiguous encoded value, {!r} could have been encoded as {!r}'.format(value, reencoded))\n\n    return decode_dollars", "response": "Decode a string in v1 format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _join_keys_v1(left, right):\n    if left.endswith(':') or '::' in left:\n        raise ValueError(\"Can't join a left string ending in ':' or containing '::'\")\n    return u\"{}::{}\".format(_encode_v1(left), _encode_v1(right))", "response": "Join two keys into a format separable by using _split_keys_v1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit two keys out a string created by _join_keys_v1.", "response": "def _split_keys_v1(joined):\n    \"\"\"\n    Split two keys out a string created by _join_keys_v1.\n    \"\"\"\n    left, _, right = joined.partition('::')\n    return _decode_v1(left), _decode_v1(right)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _decode_v2(value):\n    if re.search(r'(?<!\\$):', value):\n        raise ValueError(\"Unescaped ':' in the encoded string\")\n\n    decode_colons = value.replace('$:', ':')\n\n    if re.search(r'(?<!\\$)(\\$\\$)*\\$([^$]|\\Z)', decode_colons):\n        raise ValueError(\"Unescaped '$' in encoded string\")\n    return decode_colons.replace('$$', '$')", "response": "Decode a string encoded by _encode."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _split_keys_v2(joined):\n    left, _, right = joined.rpartition('::')\n    return _decode_v2(left), _decode_v2(right)", "response": "Split two keys out a string created by _join_keys_v2."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef replace(self, **kwargs):\n        if 'definition_key' in kwargs:\n            for attr in self.DEFINITION_KEY_FIELDS:\n                kwargs.pop(attr, None)\n        else:\n            kwargs['definition_key'] = self.definition_key.replace(**{\n                key: kwargs.pop(key)\n                for key\n                in self.DEFINITION_KEY_FIELDS\n                if key in kwargs\n            })\n        return super(AsideDefinitionKeyV2, self).replace(**kwargs)", "response": "Replaces the KEY_FIELDS in kwargs with their corresponding values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an instance of cls parsed from its serialized form.", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        Return an instance of `cls` parsed from its `serialized` form.\n\n        Args:\n            cls: The :class:`OpaqueKey` subclass.\n            serialized (unicode): A serialized :class:`OpaqueKey`, with namespace already removed.\n\n        Raises:\n            InvalidKeyError: Should be raised if `serialized` is not a valid serialized key\n                understood by `cls`.\n        \"\"\"\n        try:\n            def_key, aside_type = _split_keys_v2(serialized)\n            return cls(DefinitionKey.from_string(def_key), aside_type)\n        except ValueError as exc:\n            raise InvalidKeyError(cls, exc.args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new instance of the same type as self but with the same type as the passed in course_key.", "response": "def map_into_course(self, course_key):\n        \"\"\"\n        Return a new :class:`UsageKey` or :class:`AssetKey` representing this usage inside the\n        course identified by the supplied :class:`CourseKey`. It returns the same type as\n        `self`\n\n        Args:\n            course_key (:class:`CourseKey`): The course to map this object into.\n\n        Returns:\n            A new :class:`CourseObjectMixin` instance.\n        \"\"\"\n        return self.replace(usage_key=self.usage_key.map_into_course(course_key))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace(self, **kwargs):\n        if 'usage_key' in kwargs:\n            for attr in self.USAGE_KEY_ATTRS:\n                kwargs.pop(attr, None)\n        else:\n            kwargs['usage_key'] = self.usage_key.replace(**{\n                key: kwargs.pop(key)\n                for key\n                in self.USAGE_KEY_ATTRS\n                if key in kwargs\n            })\n        return super(AsideUsageKeyV2, self).replace(**kwargs)", "response": "Replaces the keys in the AsideUsageKeyV2 object with the ones specified in kwargs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an instance of cls parsed from its serialized form.", "response": "def _from_string(cls, serialized):\n        \"\"\"\n        Return an instance of `cls` parsed from its `serialized` form.\n\n        Args:\n            cls: The :class:`OpaqueKey` subclass.\n            serialized (unicode): A serialized :class:`OpaqueKey`, with namespace already removed.\n\n        Raises:\n            InvalidKeyError: Should be raised if `serialized` is not a valid serialized key\n                understood by `cls`.\n        \"\"\"\n        try:\n            usage_key, aside_type = _split_keys_v1(serialized)\n            return cls(UsageKey.from_string(usage_key), aside_type)\n        except ValueError as exc:\n            raise InvalidKeyError(cls, exc.args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a SQLCompleter object and populates it with the relevant completion s suggestions.", "response": "def refresh(self, executor, callbacks, completer_options=None):\n        \"\"\"Creates a SQLCompleter object and populates it with the relevant\n        completion suggestions in a background thread.\n\n        executor - SQLExecute object, used to extract the credentials to connect\n                   to the database.\n        callbacks - A function or a list of functions to call after the thread\n                    has completed the refresh. The newly created completion\n                    object will be passed in as an argument to each callback.\n        completer_options - dict of options to pass to SQLCompleter.\n        \"\"\"\n        if completer_options is None:\n            completer_options = {}\n\n        if self.is_refreshing():\n            self._restart_refresh.set()\n            return [(None, None, None, 'Auto-completion refresh restarted.')]\n        else:\n            self._completer_thread = threading.Thread(\n                target=self._bg_refresh,\n                args=(executor, callbacks, completer_options),\n                name='completion_refresh')\n            self._completer_thread.setDaemon(True)\n            self._completer_thread.start()\n            return [(None, None, None,\n                     'Auto-completion refresh started in the background.')]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_cd_command(arg):\n    CD_CMD = 'cd'\n    tokens = arg.split(CD_CMD + ' ')\n    directory = tokens[-1] if len(tokens) > 1 else None\n    if not directory:\n        return False, \"No folder name was provided.\"\n    try:\n        os.chdir(directory)\n        subprocess.call(['pwd'])\n        return True, None\n    except OSError as e:\n        return False, e.strerror", "response": "Handles a cd shell command by calling python s os. chdir."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_uptime(uptime_in_seconds):\n\n    m, s = divmod(int(uptime_in_seconds), 60)\n    h, m = divmod(m, 60)\n    d, h = divmod(h, 24)\n\n    uptime_values = []\n\n    for value, unit in ((d, 'days'), (h, 'hours'), (m, 'min'), (s, 'sec')):\n        if value == 0 and not uptime_values:\n            # Don't include a value/unit if the unit isn't applicable to\n            # the uptime. E.g. don't do 0 days 0 hours 1 min 30 sec.\n            continue\n        elif value == 1 and unit.endswith('s'):\n            # Remove the \"s\" if the unit is singular.\n            unit = unit[:-1]\n        uptime_values.append('{0} {1}'.format(value, unit))\n\n    uptime = ' '.join(uptime_values)\n    return uptime", "response": "Format the uptime in seconds into a human - readable string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_editor_query(sql):\n    sql = sql.strip()\n\n    # The reason we can't simply do .strip('\\e') is that it strips characters,\n    # not a substring. So it'll strip \"e\" in the end of the sql also!\n    # Ex: \"select * from style\\e\" -> \"select * from styl\".\n    pattern = re.compile('(^\\\\\\e|\\\\\\e$)')\n    while pattern.search(sql):\n        sql = pattern.sub('', sql)\n\n    return sql", "response": "Get the query part of an editor command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nopen external editor and return the first element of the list.", "response": "def open_external_editor(filename=None, sql=None):\n    \"\"\"Open external editor, wait for the user to type in their query, return\n    the query.\n    :return: list with one tuple, query as first element.\n    \"\"\"\n\n    message = None\n    filename = filename.strip().split(' ', 1)[0] if filename else None\n\n    sql = sql or ''\n    MARKER = '# Type your query above this line.\\n'\n\n    # Populate the editor buffer with the partial sql (if available) and a\n    # placeholder comment.\n    query = click.edit(u'{sql}\\n\\n{marker}'.format(sql=sql, marker=MARKER),\n                       filename=filename, extension='.sql')\n\n    if filename:\n        try:\n            with open(filename, encoding='utf-8') as f:\n                query = f.read()\n        except IOError:\n            message = 'Error reading file: %s.' % filename\n\n    if query is not None:\n        query = query.split(MARKER, 1)[0].rstrip('\\n')\n    else:\n        # Don't return None for the caller to deal with.\n        # Empty string is ok.\n        query = sql\n\n    return (query, message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a query and return a list of results.", "response": "def execute_favorite_query(cur, arg, **_):\n    \"\"\"Returns (title, rows, headers, status)\"\"\"\n    if arg == '':\n        for result in list_favorite_queries():\n            yield result\n\n    \"\"\"Parse out favorite name and optional substitution parameters\"\"\"\n    name, _, arg_str = arg.partition(' ')\n    args = shlex.split(arg_str)\n\n    query = favoritequeries.get(name)\n    if query is None:\n        message = \"No favorite query: %s\" % (name)\n        yield (None, None, None, message)\n    else:\n        query, arg_error = subst_favorite_query_args(query, args)\n        if arg_error:\n            yield (None, None, None, arg_error)\n        else:\n            for sql in sqlparse.split(query):\n                _logger.debug(\"query is [%s]\", sql)\n                sql = sql.rstrip(';')\n                title = '> %s' % (sql)\n                cur.execute(sql)\n                if cur.description:\n                    headers = [x[0] for x in cur.description]\n                    yield (title, cur.fetchall(), headers, None)\n                else:\n                    yield (title, None, None, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist of all favorite queries. Returns ( title rows headers status", "response": "def list_favorite_queries():\n    \"\"\"List of all favorite queries.\n    Returns (title, rows, headers, status)\"\"\"\n\n    headers = [\"Name\", \"Query\"]\n    rows = [(r, favoritequeries.get(r)) for r in favoritequeries.list()]\n\n    if not rows:\n        status = '\\nNo favorite queries found.' + favoritequeries.usage\n    else:\n        status = ''\n    return [('', rows, headers, status)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subst_favorite_query_args(query, args):\n    for idx, val in enumerate(args):\n        subst_var = '$' + str(idx + 1)\n        if subst_var not in query:\n            return [None, 'query does not have substitution parameter ' + subst_var + ':\\n  ' + query]\n\n        query = query.replace(subst_var, val)\n\n    match = re.search('\\\\$\\d+', query)\n    if match:\n        return[None, 'missing substitution for ' + match.group(0) + ' in query:\\n  ' + query]\n\n    return [query, None]", "response": "substitute positional parameters in query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a new favorite query.", "response": "def save_favorite_query(arg, **_):\n    \"\"\"Save a new favorite query.\n    Returns (title, rows, headers, status)\"\"\"\n\n    usage = 'Syntax: \\\\fs name query.\\n\\n' + favoritequeries.usage\n    if not arg:\n        return [(None, None, None, usage)]\n\n    name, _, query = arg.partition(' ')\n\n    # If either name or query is missing then print the usage and complain.\n    if (not name) or (not query):\n        return [(None, None, None,\n            usage + 'Err: Both name and query are required.')]\n\n    favoritequeries.save(name, query)\n    return [(None, None, None, \"Saved.\")]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete an existing favorite query.", "response": "def delete_favorite_query(arg, **_):\n    \"\"\"Delete an existing favorite query.\n    \"\"\"\n    usage = 'Syntax: \\\\fd name.\\n\\n' + favoritequeries.usage\n    if not arg:\n        return [(None, None, None, usage)]\n\n    status = favoritequeries.delete(arg)\n\n    return [(None, None, None, status)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_system_command(arg, **_):\n    usage = \"Syntax: system [command].\\n\"\n\n    if not arg:\n      return [(None, None, None, usage)]\n\n    try:\n        command = arg.strip()\n        if command.startswith('cd'):\n            ok, error_message = handle_cd_command(arg)\n            if not ok:\n                return [(None, None, None, error_message)]\n            return [(None, None, None, '')]\n\n        args = arg.split(' ')\n        process = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        output, error = process.communicate()\n        response = output if not error else error\n\n        # Python 3 returns bytes. This needs to be decoded to a string.\n        if isinstance(response, bytes):\n            encoding = locale.getpreferredencoding(False)\n            response = response.decode(encoding)\n\n        return [(None, None, None, response)]\n    except OSError as e:\n        return [(None, None, None, 'OSError: %s' % e.strerror)]", "response": "Execute a system shell command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef need_completion_refresh(queries):\n    tokens = {\n        'use', '\\\\u',\n        'create',\n        'drop'\n    }\n\n    for query in sqlparse.split(queries):\n        try:\n            first_token = query.split()[0]\n            if first_token.lower() in tokens:\n                return True\n        except Exception:\n            return False", "response": "Determines if the completion needs a refresh by checking if the sql\n    statement is an alter create drop or change db."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if the statement is mutating based on the status.", "response": "def is_mutating(status):\n    \"\"\"Determines if the statement is mutating based on the status.\"\"\"\n    if not status:\n        return False\n\n    mutating = set(['insert', 'update', 'delete', 'alter', 'create', 'drop',\n                    'replace', 'truncate', 'load'])\n    return status.split(None, 1)[0].lower() in mutating"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the prompt format.", "response": "def change_prompt_format(self, arg, **_):\n        \"\"\"\n        Change the prompt format.\n        \"\"\"\n        if not arg:\n            message = 'Missing required argument, format.'\n            return [(None, None, None, message)]\n\n        self.prompt = self.get_prompt(arg)\n        return [(None, None, None, \"Changed prompt format to %s\" % arg)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_editor_command(self, cli, document):\n        # FIXME: using application.pre_run_callables like this here is not the best solution.\n        # It's internal api of prompt_toolkit that may change. This was added to fix\n        # https://github.com/dbcli/pgcli/issues/668. We may find a better way to do it in the future.\n        saved_callables = cli.application.pre_run_callables\n        while special.editor_command(document.text):\n            filename = special.get_filename(document.text)\n            query = (special.get_editor_query(document.text) or\n                     self.get_last_query())\n            sql, message = special.open_external_editor(filename, sql=query)\n            if message:\n                # Something went wrong. Raise an exception and bail.\n                raise RuntimeError(message)\n            cli.current_buffer.document = Document(sql, cursor_position=len(sql))\n            cli.application.pre_run_callables = []\n            document = cli.run()\n            continue\n        cli.application.pre_run_callables = saved_callables\n        return document", "response": "Handles the command that is used to edit the user s entry - point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_query(self, query, new_line=True):\n        if (self.destructive_warning and\n                confirm_destructive_query(query) is False):\n            message = 'Wise choice. Command execution stopped.'\n            click.echo(message)\n            return\n\n        results = self.sqlexecute.run(query)\n        for result in results:\n            title, rows, headers, _ = result\n            self.formatter.query = query\n            output = self.format_output(title, rows, headers)\n            for line in output:\n                click.echo(line, nl=new_line)", "response": "Runs a query and prints the output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_output_margin(self, status=None):\n        margin = self.get_reserved_space() + self.get_prompt(self.prompt).count('\\n') + 1\n        if special.is_timing_enabled():\n            margin += 1\n        if status:\n            margin += 1 + status.count('\\n')\n\n        return margin", "response": "Get the output margin for the prompt footer and the status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noutput text to stdout or a pager command.", "response": "def output(self, output, status=None):\n        \"\"\"Output text to stdout or a pager command.\n        The status text is not outputted to pager or files.\n        The message will be logged in the audit log, if enabled. The\n        message will be written to the tee file, if enabled. The\n        message will be written to the output file, if enabled.\n        \"\"\"\n        if output:\n            size = self.cli.output.get_size()\n\n            margin = self.get_output_margin(status)\n\n            fits = True\n            buf = []\n            output_via_pager = self.explicit_pager and special.is_pager_enabled()\n            for i, line in enumerate(output, 1):\n                special.write_tee(line)\n                special.write_once(line)\n\n                if fits or output_via_pager:\n                    # buffering\n                    buf.append(line)\n                    if len(line) > size.columns or i > (size.rows - margin):\n                        fits = False\n                        if not self.explicit_pager and special.is_pager_enabled():\n                            # doesn't fit, use pager\n                            output_via_pager = True\n\n                        if not output_via_pager:\n                            # doesn't fit, flush buffer\n                            for line in buf:\n                                click.secho(line)\n                            buf = []\n                else:\n                    click.secho(line)\n\n            if buf:\n                if output_via_pager:\n                    # sadly click.echo_via_pager doesn't accept generators\n                    click.echo_via_pager(\"\\n\".join(buf))\n                else:\n                    for line in buf:\n                        click.secho(line)\n\n        if status:\n            click.secho(status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls when completions are refreshed.", "response": "def _on_completions_refreshed(self, new_completer):\n        \"\"\"Swap the completer object in cli with the newly created completer.\n        \"\"\"\n        with self._completer_lock:\n            self.completer = new_completer\n            # When cli is first launched we call refresh_completions before\n            # instantiating the cli object. So it is necessary to check if cli\n            # exists before trying the replace the completer object in cli.\n            if self.cli:\n                self.cli.current_buffer.completer = new_completer\n\n        if self.cli:\n            # After refreshing, redraw the CLI to clear the statusbar\n            # \"Refreshing completions...\" indicator\n            self.cli.request_redraw()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_reserved_space(self):\n        reserved_space_ratio = .45\n        max_reserved_space = 8\n        _, height = click.get_terminal_size()\n        return min(int(round(height * reserved_space_ratio)), max_reserved_space)", "response": "Get the number of lines to reserve for the completion menu."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_path(root_dir):\n    res = []\n    if os.path.isdir(root_dir):\n        for name in os.listdir(root_dir):\n            res.append(name)\n    return res", "response": "List the path of the neccesary resource tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the path to complete that matches the last entered component.", "response": "def complete_path(curr_dir, last_dir):\n    \"\"\"Return the path to complete that matches the last entered component.\n    If the last entered component is ~, expanded path would not\n    match, so return all of the available paths.\n    :param curr_dir: str\n    :param last_dir: str\n    :return: str\n    \"\"\"\n    if not last_dir or curr_dir.startswith(last_dir):\n        return curr_dir\n    elif last_dir == '~':\n        return os.path.join(last_dir, curr_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits path into base_dir last_dir and position where last component starts.", "response": "def parse_path(root_dir):\n    \"\"\"Split path into head and last component for the completer.\n    Also return position where last component starts.\n    :param root_dir: str path\n    :return: tuple of (string, string, int)\n    \"\"\"\n    base_dir, last_dir, position = '', '', 0\n    if root_dir:\n        base_dir, last_dir = os.path.split(root_dir)\n        position = -len(last_dir) if last_dir else 0\n    return base_dir, last_dir, position"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef suggest_path(root_dir):\n    if not root_dir:\n        return [os.path.abspath(os.sep), '~', os.curdir, os.pardir]\n\n    if '~' in root_dir:\n        root_dir = os.path.expanduser(root_dir)\n\n    if not os.path.exists(root_dir):\n        root_dir, _ = os.path.split(root_dir)\n\n    return list_path(root_dir)", "response": "List all files and subdirectories in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extend_relations(self, data, kind):\n        # 'data' is a generator object. It can throw an exception while being\n        # consumed. This could happen if the user has launched the app without\n        # specifying a database name. This exception must be handled to prevent\n        # crashing.\n        try:\n            data = [self.escaped_names(d) for d in data]\n        except Exception:\n            data = []\n\n        # dbmetadata['tables'][$schema_name][$table_name] should be a list of\n        # column names. Default to an asterisk\n        metadata = self.dbmetadata[kind]\n        for relname in data:\n            try:\n                metadata[self.dbname][relname[0]] = ['*']\n            except KeyError:\n                _logger.error('%r %r listed in unrecognized schema %r',\n                              kind, relname[0], self.dbname)\n            self.all_completions.add(relname[0])", "response": "Extend metadata for tables or views."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extend_columns(self, column_data, kind):\n        # 'column_data' is a generator object. It can throw an exception while\n        # being consumed. This could happen if the user has launched the app\n        # without specifying a database name. This exception must be handled to\n        # prevent crashing.\n        try:\n            column_data = [self.escaped_names(d, '\"') for d in column_data]\n        except Exception:\n            column_data = []\n\n        metadata = self.dbmetadata[kind]\n        for relname, column in column_data:\n            metadata[self.dbname][relname].append(column)\n            self.all_completions.add(column)", "response": "Extend the metadata of the tables and views of the tables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_matches(text, collection, start_only=False, fuzzy=True, casing=None):\n        last = last_word(text, include='most_punctuations')\n        text = last.lower()\n\n        completions = []\n\n        if fuzzy:\n            regex = '.*?'.join(map(escape, text))\n            pat = compile('(%s)' % regex)\n            for item in sorted(collection):\n                r = pat.search(item.lower())\n                if r:\n                    completions.append((len(r.group()), r.start(), item))\n        else:\n            match_end_limit = len(text) if start_only else None\n            for item in sorted(collection):\n                match_point = item.lower().find(text, 0, match_end_limit)\n                if match_point >= 0:\n                    completions.append((len(text), match_point, item))\n\n        if casing == 'auto':\n            casing = 'lower' if last and last[-1].islower() else 'upper'\n\n        def apply_case(kw):\n            if casing == 'upper':\n                return kw.upper()\n            return kw.lower()\n\n        return (Completion(z if casing is None else apply_case(z), -len(text))\n                for x, y, z in sorted(completions))", "response": "Given the user s input text and a collection of available completions find completion matches for the last word of the text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_files(self, word):\n        base_path, last_path, position = parse_path(word)\n        paths = suggest_path(word)\n        for name in sorted(paths):\n            suggestion = complete_path(name, last_path)\n            if suggestion:\n                yield Completion(suggestion, position)", "response": "Yields matching directory or file names."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind all columns in a set of scoped tables and return a list of column names.", "response": "def populate_scoped_cols(self, scoped_tbls):\n        \"\"\"Find all columns in a set of scoped_tables\n        :param scoped_tbls: list of (schema, table, alias) tuples\n        :return: list of column names\n        \"\"\"\n        columns = []\n        meta = self.dbmetadata\n\n        for tbl in scoped_tbls:\n            # A fully qualified schema.relname reference or default_schema\n            # DO NOT escape schema names.\n            schema = tbl[0] or self.dbname\n            relname = tbl[1]\n            escaped_relname = self.escape_name(tbl[1])\n\n            # We don't know if schema.relname is a table or view. Since\n            # tables and views cannot share the same name, we can check one\n            # at a time\n            try:\n                columns.extend(meta['tables'][schema][relname])\n\n                # Table exists, so don't bother checking for a view\n                continue\n            except KeyError:\n                try:\n                    columns.extend(meta['tables'][schema][escaped_relname])\n                    # Table exists, so don't bother checking for a view\n                    continue\n                except KeyError:\n                    pass\n\n            try:\n                columns.extend(meta['views'][schema][relname])\n            except KeyError:\n                pass\n\n        return columns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn list of tables or functions for a schema", "response": "def populate_schema_objects(self, schema, obj_type):\n        \"\"\"Returns list of tables or functions for a (optional) schema\"\"\"\n        metadata = self.dbmetadata[obj_type]\n        schema = schema or self.dbname\n\n        try:\n            objects = metadata[schema].keys()\n        except KeyError:\n            # schema doesn't exist\n            objects = []\n\n        return objects"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log(logger, level, message):\n\n    if logger.parent.name != 'root':\n        logger.log(level, message)\n    else:\n        print(message, file=sys.stderr)", "response": "Logs message to stderr if logging isn t initialized."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a config file.", "response": "def read_config_file(f):\n    \"\"\"Read a config file.\"\"\"\n\n    if isinstance(f, basestring):\n        f = os.path.expanduser(f)\n\n    try:\n        config = ConfigObj(f, interpolation=False, encoding='utf8')\n    except ConfigObjError as e:\n        log(LOGGER, logging.ERROR, \"Unable to parse line {0} of config file \"\n            \"'{1}'.\".format(e.line_number, f))\n        log(LOGGER, logging.ERROR, \"Using successfully parsed config values.\")\n        return e.config\n    except (IOError, OSError) as e:\n        log(LOGGER, logging.WARNING, \"You don't have permission to read \"\n            \"config file '{0}'.\".format(e.filename))\n        return None\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading and merge a list of config files.", "response": "def read_config_files(files):\n    \"\"\"Read and merge a list of config files.\"\"\"\n\n    config = ConfigObj()\n\n    for _file in files:\n        _config = read_config_file(_file)\n        if bool(_config) is True:\n            config.merge(_config)\n            config.filename = _config.filename\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cli_bindings():\n    key_binding_manager = KeyBindingManager(\n        enable_open_in_editor=True,\n        enable_system_bindings=True,\n        enable_auto_suggest_bindings=True,\n        enable_search=True,\n        enable_abort_and_exit_bindings=True)\n\n    @key_binding_manager.registry.add_binding(Keys.F2)\n    def _(event):\n        \"\"\"\n        Enable/Disable SmartCompletion Mode.\n        \"\"\"\n        _logger.debug('Detected F2 key.')\n        buf = event.cli.current_buffer\n        buf.completer.smart_completion = not buf.completer.smart_completion\n\n    @key_binding_manager.registry.add_binding(Keys.F3)\n    def _(event):\n        \"\"\"\n        Enable/Disable Multiline Mode.\n        \"\"\"\n        _logger.debug('Detected F3 key.')\n        buf = event.cli.current_buffer\n        buf.always_multiline = not buf.always_multiline\n\n    @key_binding_manager.registry.add_binding(Keys.F4)\n    def _(event):\n        \"\"\"\n        Toggle between Vi and Emacs mode.\n        \"\"\"\n        _logger.debug('Detected F4 key.')\n        if event.cli.editing_mode == EditingMode.VI:\n            event.cli.editing_mode = EditingMode.EMACS\n        else:\n            event.cli.editing_mode = EditingMode.VI\n\n    @key_binding_manager.registry.add_binding(Keys.Tab)\n    def _(event):\n        \"\"\"\n        Force autocompletion at cursor.\n        \"\"\"\n        _logger.debug('Detected <Tab> key.')\n        b = event.cli.current_buffer\n        if b.complete_state:\n            b.complete_next()\n        else:\n            event.cli.start_completion(select_first=True)\n\n    @key_binding_manager.registry.add_binding(Keys.ControlSpace)\n    def _(event):\n        \"\"\"\n        Initialize autocompletion at cursor.\n        If the autocompletion menu is not showing, display it with the\n        appropriate completions for the context.\n        If the menu is showing, select the next completion.\n        \"\"\"\n        _logger.debug('Detected <C-Space> key.')\n\n        b = event.cli.current_buffer\n        if b.complete_state:\n            b.complete_next()\n        else:\n            event.cli.start_completion(select_first=False)\n\n    @key_binding_manager.registry.add_binding(Keys.ControlJ, filter=HasSelectedCompletion())\n    def _(event):\n        \"\"\"\n        Makes the enter key work as the tab key only when showing the menu.\n        \"\"\"\n        _logger.debug('Detected <C-J> key.')\n\n        event.current_buffer.complete_state = None\n        b = event.cli.current_buffer\n        b.complete_state = None\n\n    return key_binding_manager", "response": "Custom key bindings for cli."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the query is destructive and prompts the user to proceed.", "response": "def confirm_destructive_query(queries):\n    \"\"\"Check if the query is destructive and prompts the user to confirm.\n    Returns:\n    * None if the query is non-destructive or we can't prompt the user.\n    * True if the query is destructive and the user wants to proceed.\n    * False if the query is destructive and the user doesn't want to proceed.\n    \"\"\"\n    prompt_text = (\"You're about to run a destructive command.\\n\"\n                   \"Do you want to proceed? (y/n)\")\n    if is_destructive(queries) and sys.stdin.isatty():\n        return prompt(prompt_text, type=bool)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef confirm(*args, **kwargs):\n    try:\n        return click.confirm(*args, **kwargs)\n    except click.Abort:\n        return False", "response": "Prompt for confirmation and handle any abort exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prompt(*args, **kwargs):\n    try:\n        return click.prompt(*args, **kwargs)\n    except click.Abort:\n        return False", "response": "Prompt the user for input and handle any abort exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef style_factory(name, cli_style):\n    try:\n        style = pygments.styles.get_style_by_name(name)\n    except ClassNotFound:\n        style = pygments.styles.get_style_by_name('native')\n\n    style_tokens = {}\n    style_tokens.update(style.styles)\n    custom_styles = {string_to_tokentype(x): y for x, y in cli_style.items()}\n    style_tokens.update(custom_styles)\n\n    class CliStyle(pygments.style.Style):\n        default_styles = ''\n        styles = style_tokens\n\n    return CliStyle", "response": "Create a Pygments style class based on the user s token - type style preferences."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, statement):\n        '''Execute the sql in the database and return the results.\n\n        The results are a list of tuples. Each tuple has 4 values\n        (title, rows, headers, status).\n        '''\n        # Remove spaces and EOL\n        statement = statement.strip()\n        if not statement:  # Empty string\n            yield (None, None, None, None)\n\n        # Split the sql into separate queries and run each one.\n        components = sqlparse.split(statement)\n\n        for sql in components:\n            # Remove spaces, eol and semi-colons.\n            sql = sql.rstrip(';')\n\n            # \\G is treated specially since we have to set the expanded output.\n            if sql.endswith('\\\\G'):\n                special.set_expanded_output(True)\n                sql = sql[:-2].strip()\n\n            cur = self.conn.cursor()\n\n            try:\n                for result in special.execute(cur, sql):\n                    yield result\n            except special.CommandNotFound:  # Regular SQL\n                cur.execute(sql)\n                yield self.get_result(cur)", "response": "Execute the sql in the database and return the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the current result s data from the cursor.", "response": "def get_result(self, cursor):\n        '''Get the current result's data from the cursor.'''\n        title = headers = None\n\n        # cursor.description is not None for queries that return result sets,\n        # e.g. SELECT or SHOW.\n        if cursor.description is not None:\n            headers = [x[0] for x in cursor.description]\n            rows = cursor.fetchall()\n            status = '%d row%s in set' % (len(rows), '' if len(rows) == 1 else 's')\n        else:\n            logger.debug('No rows in result.')\n            rows = None\n            status = 'Query OK'\n        return (title, rows, headers, status)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_toolbar_tokens_func(get_is_refreshing, show_fish_help):\n    token = Token.Toolbar\n\n    def get_toolbar_tokens(cli):\n        result = []\n        result.append((token, ' '))\n\n        if cli.buffers[DEFAULT_BUFFER].always_multiline:\n            result.append((token.On, '[F3] Multiline: ON  '))\n        else:\n            result.append((token.Off, '[F3] Multiline: OFF  '))\n\n        if cli.buffers[DEFAULT_BUFFER].always_multiline:\n            result.append((token,\n                ' (Semi-colon [;] will end the line)'))\n\n        if cli.editing_mode == EditingMode.VI:\n            result.append((\n                token.On,\n                'Vi-mode ({})'.format(_get_vi_mode(cli))\n            ))\n\n        if show_fish_help():\n            result.append((token, '  Right-arrow to complete suggestion'))\n\n        if get_is_refreshing():\n            result.append((token, '     Refreshing completions...'))\n\n        return result\n    return get_toolbar_tokens", "response": "Creates a function that returns the toolbar tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_vi_mode(cli):\n    return {\n        InputMode.INSERT: 'I',\n        InputMode.NAVIGATION: 'N',\n        InputMode.REPLACE: 'R',\n        InputMode.INSERT_MULTIPLE: 'M'\n    }[cli.vi_state.input_mode]", "response": "Get the current vi mode for display."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking the full_text that is typed so far and also the text before the cursor to suggest completion type and scope. Returns a tuple with a type of entity and a scope for a column category.", "response": "def suggest_type(full_text, text_before_cursor):\n    \"\"\"Takes the full_text that is typed so far and also the text before the\n    cursor to suggest completion type and scope.\n    Returns a tuple with a type of entity ('table', 'column' etc) and a scope.\n    A scope for a column category will be a list of tables.\n    \"\"\"\n\n    word_before_cursor = last_word(text_before_cursor,\n            include='many_punctuations')\n\n    identifier = None\n\n    # here should be removed once sqlparse has been fixed\n    try:\n        # If we've partially typed a word then word_before_cursor won't be an empty\n        # string. In that case we want to remove the partially typed string before\n        # sending it to the sqlparser. Otherwise the last token will always be the\n        # partially typed string which renders the smart completion useless because\n        # it will always return the list of keywords as completion.\n        if word_before_cursor:\n            if word_before_cursor.endswith(\n                    '(') or word_before_cursor.startswith('\\\\'):\n                parsed = sqlparse.parse(text_before_cursor)\n            else:\n                parsed = sqlparse.parse(\n                    text_before_cursor[:-len(word_before_cursor)])\n\n                # word_before_cursor may include a schema qualification, like\n                # \"schema_name.partial_name\" or \"schema_name.\", so parse it\n                # separately\n                p = sqlparse.parse(word_before_cursor)[0]\n\n                if p.tokens and isinstance(p.tokens[0], Identifier):\n                    identifier = p.tokens[0]\n        else:\n            parsed = sqlparse.parse(text_before_cursor)\n    except (TypeError, AttributeError):\n        return (Keyword(),)\n\n    if len(parsed) > 1:\n        # Multiple statements being edited -- isolate the current one by\n        # cumulatively summing statement lengths to find the one that bounds the\n        # current position\n        current_pos = len(text_before_cursor)\n        stmt_start, stmt_end = 0, 0\n\n        for statement in parsed:\n            stmt_len = len(text_type(statement))\n            stmt_start, stmt_end = stmt_end, stmt_end + stmt_len\n\n            if stmt_end >= current_pos:\n                text_before_cursor = full_text[stmt_start:current_pos]\n                full_text = full_text[stmt_start:]\n                break\n\n    elif parsed:\n        # A single statement\n        statement = parsed[0]\n    else:\n        # The empty string\n        statement = None\n\n    # Check for special commands and handle those separately\n    if statement:\n        # Be careful here because trivial whitespace is parsed as a statement,\n        # but the statement won't have a first token\n        tok1 = statement.token_first()\n        if tok1 and tok1.value in ['\\\\', 'source']:\n            return suggest_special(text_before_cursor)\n\n    last_token = statement and statement.token_prev(len(statement.tokens))[1] or ''\n\n    return suggest_based_on_last_token(last_token, text_before_cursor,\n                                       full_text, identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the next step of the sequence.", "response": "def run_step(*args, prompt=None):\n    \"\"\"\n    Prints out the command and asks if it should be run.\n    If yes (default), runs it.\n    :param args: list of strings (command and args)\n    \"\"\"\n    global DRY_RUN\n\n    cmd = args\n    print(' '.join(cmd))\n    if skip_step():\n        print('--- Skipping...')\n    elif DRY_RUN:\n        print('--- Pretending to run...')\n    else:\n        if prompt:\n            print(prompt)\n        subprocess.check_output(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a special command and return the results.", "response": "def execute(cur, sql):\n    \"\"\"Execute a special command and return the results. If the special command\n    is not supported a KeyError will be raised.\n    \"\"\"\n    command, verbose, arg = parse_special_command(sql)\n\n    if (command not in COMMANDS) and (command.lower() not in COMMANDS):\n        raise CommandNotFound\n\n    try:\n        special_cmd = COMMANDS[command]\n    except KeyError:\n        special_cmd = COMMANDS[command.lower()]\n        if special_cmd.case_sensitive:\n            raise CommandNotFound('Command not found: %s' % command)\n\n    # \"help <SQL KEYWORD> is a special case.\n    if command == 'help' and arg:\n        return show_keyword_help(cur=cur, arg=arg)\n\n    if special_cmd.arg_type == NO_QUERY:\n        return special_cmd.handler()\n    elif special_cmd.arg_type == PARSED_QUERY:\n        return special_cmd.handler(cur=cur, arg=arg, verbose=verbose)\n    elif special_cmd.arg_type == RAW_QUERY:\n        return special_cmd.handler(cur=cur, query=sql)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_keyword_help(cur, arg):\n    keyword = arg.strip('\"').strip(\"'\")\n    query = \"help '{0}'\".format(keyword)\n    log.debug(query)\n    cur.execute(query)\n    if cur.description and cur.rowcount > 0:\n        headers = [x[0] for x in cur.description]\n        return [(None, cur.fetchall(), headers, '')]\n    else:\n        return [(None, None, None, 'No help found for {0}.'.format(keyword))]", "response": "Display the help for a given SQL keyword."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the last word in a text string.", "response": "def last_word(text, include='alphanum_underscore'):\n    \"\"\"\n    Find the last word in a sentence.\n    >>> last_word('abc')\n    'abc'\n    >>> last_word(' abc')\n    'abc'\n    >>> last_word('')\n    ''\n    >>> last_word(' ')\n    ''\n    >>> last_word('abc ')\n    ''\n    >>> last_word('abc def')\n    'def'\n    >>> last_word('abc def ')\n    ''\n    >>> last_word('abc def;')\n    ''\n    >>> last_word('bac $def')\n    'def'\n    >>> last_word('bac $def', include='most_punctuations')\n    '$def'\n    >>> last_word('bac \\def', include='most_punctuations')\n    '\\\\\\\\def'\n    >>> last_word('bac \\def;', include='most_punctuations')\n    '\\\\\\\\def;'\n    >>> last_word('bac::def', include='most_punctuations')\n    'def'\n    \"\"\"\n\n    if not text:   # Empty string\n        return ''\n\n    if text[-1].isspace():\n        return ''\n    else:\n        regex = cleanup_regex[include]\n        matches = regex.search(text)\n        if matches:\n            return matches.group(0)\n        else:\n            return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_table_identifiers(token_stream):\n\n    for item in token_stream:\n        if isinstance(item, IdentifierList):\n            for identifier in item.get_identifiers():\n                # Sometimes Keywords (such as FROM ) are classified as\n                # identifiers which don't have the get_real_name() method.\n                try:\n                    schema_name = identifier.get_parent_name()\n                    real_name = identifier.get_real_name()\n                except AttributeError:\n                    continue\n                if real_name:\n                    yield (schema_name, real_name, identifier.get_alias())\n        elif isinstance(item, Identifier):\n            real_name = item.get_real_name()\n            schema_name = item.get_parent_name()\n\n            if real_name:\n                yield (schema_name, real_name, item.get_alias())\n            else:\n                name = item.get_name()\n                yield (None, name, item.get_alias() or name)\n        elif isinstance(item, Function):\n            yield (None, item.get_name(), item.get_name())", "response": "Yields tuples of schema_name table_name table_alias"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the table names from an SQL statment.", "response": "def extract_tables(sql):\n    \"\"\"Extract the table names from an SQL statment.\n    Returns a list of (schema, table, alias) tuples\n    \"\"\"\n    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return []\n\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == 'insert'\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n    return list(extract_table_identifiers(stream))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_prev_keyword(sql):\n    if not sql.strip():\n        return None, ''\n\n    parsed = sqlparse.parse(sql)[0]\n    flattened = list(parsed.flatten())\n\n    logical_operators = ('AND', 'OR', 'NOT', 'BETWEEN')\n\n    for t in reversed(flattened):\n        if t.value == '(' or (t.is_keyword and (\n                              t.value.upper() not in logical_operators)):\n            # Find the location of token t in the original parsed statement\n            # We can't use parsed.token_index(t) because t may be a child token\n            # inside a TokenList, in which case token_index thows an error\n            # Minimal example:\n            #   p = sqlparse.parse('select * from foo where bar')\n            #   t = list(p.flatten())[-3]  # The \"Where\" token\n            #   p.token_index(t)  # Throws ValueError: not in list\n            idx = flattened.index(t)\n\n            # Combine the string values of all tokens in the original list\n            # up to and including the target keyword token t, to produce a\n            # query string with everything after the keyword token removed\n            text = ''.join(tok.value for tok in flattened[:idx+1])\n            return t, text\n\n    return None, ''", "response": "Find the last sql keyword in an SQL statement Returns the value of the last keyword and the text of the query with everything after the last keyword stripped\n Returns None and empty string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_starts_with(query, prefixes):\n    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n    return bool(formatted_sql) and formatted_sql.split()[0] in prefixes", "response": "Check if the query starts with any item from prefixes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if any queries start with any item from prefixes.", "response": "def queries_start_with(queries, prefixes):\n    \"\"\"Check if any queries start with any item from *prefixes*.\"\"\"\n    for query in sqlparse.split(queries):\n        if query and query_starts_with(query, prefixes) is True:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_thumbnail_options(self, context, instance):\n        width, height = None, None\n        subject_location = False\n        placeholder_width = context.get('width', None)\n        placeholder_height = context.get('height', None)\n        if instance.use_autoscale and placeholder_width:\n            # use the placeholder width as a hint for sizing\n            width = int(placeholder_width)\n        if instance.use_autoscale and placeholder_height:\n            height = int(placeholder_height)\n        elif instance.width:\n            width = instance.width\n        if instance.height:\n            height = instance.height\n        if instance.image:\n            if instance.image.subject_location:\n                subject_location = instance.image.subject_location\n            if not height and width:\n                # height was not externally defined: use ratio to scale it by the width\n                height = int(float(width) * float(instance.image.height) / float(instance.image.width))\n            if not width and height:\n                # width was not externally defined: use ratio to scale it by the height\n                width = int(float(height) * float(instance.image.width) / float(instance.image.height))\n            if not width:\n                # width is still not defined. fallback the actual image width\n                width = instance.image.width\n            if not height:\n                # height is still not defined. fallback the actual image height\n                height = instance.image.height\n        return {'size': (width, height),\n                'subject_location': subject_location}", "response": "Return the size and options of the thumbnail that should be inserted\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_image_plugin(filename, image, parent_plugin, **kwargs):\n    from cmsplugin_filer_image.models import FilerImage\n    from filer.models import Image\n    image_plugin = FilerImage()\n    image_plugin.placeholder = parent_plugin.placeholder\n    image_plugin.parent = CMSPlugin.objects.get(pk=parent_plugin.id)\n    image_plugin.position = CMSPlugin.objects.filter(parent=parent_plugin).count()\n    image_plugin.language = parent_plugin.language\n    image_plugin.plugin_type = 'FilerImagePlugin'\n    image.seek(0)\n    image_model = Image.objects.create(file=SimpleUploadedFile(name=filename, content=image.read()))\n    image_plugin.image = image_model\n    image_plugin.save()\n    return image_plugin", "response": "Create an image plugin from a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming tables from source to destination name.", "response": "def rename_tables(db, table_mapping, reverse=False):\n    \"\"\"\n    renames tables from source to destination name, if the source exists and the destination does\n    not exist yet.\n    \"\"\"\n    from django.db import connection\n    if reverse:\n        table_mapping = [(dst, src) for src, dst in table_mapping]\n    table_names = connection.introspection.table_names()\n    for source, destination in table_mapping:\n        if source in table_names and destination in table_names:\n            print(u\"    WARNING: not renaming {0} to {1}, because both tables already exist.\".format(source, destination))\n        elif source in table_names and destination not in table_names:\n            print(u\"     - renaming {0} to {1}\".format(source, destination))\n            db.rename_table(source, destination)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_and_sort_statements(stmt_list, ev_totals=None):\n    def _count(stmt):\n        if ev_totals is None:\n            return len(stmt.evidence)\n        else:\n            return ev_totals[stmt.get_hash()]\n\n    stmt_rows = defaultdict(list)\n    stmt_counts = defaultdict(lambda: 0)\n    arg_counts = defaultdict(lambda: 0)\n    for key, s in _get_keyed_stmts(stmt_list):\n        # Update the counts, and add key if needed.\n        stmt_rows[key].append(s)\n\n        # Keep track of the total evidence counts for this statement and the\n        # arguments.\n        stmt_counts[key] += _count(s)\n\n        # Add up the counts for the arguments, pairwise for Complexes and\n        # Conversions. This allows, for example, a complex between MEK, ERK,\n        # and something else to lend weight to the interactions between MEK\n        # and ERK.\n        if key[0] == 'Conversion':\n            subj = key[1]\n            for obj in key[2] + key[3]:\n                arg_counts[(subj, obj)] += _count(s)\n        else:\n            arg_counts[key[1:]] += _count(s)\n\n    # Sort the rows by count and agent names.\n    def process_rows(stmt_rows):\n        for key, stmts in stmt_rows.items():\n            verb = key[0]\n            inps = key[1:]\n            sub_count = stmt_counts[key]\n            arg_count = arg_counts[inps]\n            if verb == 'Complex' and sub_count == arg_count and len(inps) <= 2:\n                if all([len(set(ag.name for ag in s.agent_list())) > 2\n                        for s in stmts]):\n                    continue\n            new_key = (arg_count, inps, sub_count, verb)\n            stmts = sorted(stmts,\n                           key=lambda s: _count(s) + 1/(1+len(s.agent_list())),\n                           reverse=True)\n            yield new_key, verb, stmts\n\n    sorted_groups = sorted(process_rows(stmt_rows),\n                           key=lambda tpl: tpl[0], reverse=True)\n\n    return sorted_groups", "response": "Group INDRA statements by type and arguments and sort by prevalence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_stmt_from_sort_key(key, verb):\n    def make_agent(name):\n        if name == 'None' or name is None:\n            return None\n        return Agent(name)\n\n    StmtClass = get_statement_by_name(verb)\n    inps = list(key[1])\n    if verb == 'Complex':\n        stmt = StmtClass([make_agent(name) for name in inps])\n    elif verb == 'Conversion':\n        stmt = StmtClass(make_agent(inps[0]),\n                         [make_agent(name) for name in inps[1]],\n                         [make_agent(name) for name in inps[2]])\n    elif verb == 'ActiveForm' or verb == 'HasActivity':\n        stmt = StmtClass(make_agent(inps[0]), inps[1], inps[2])\n    else:\n        stmt = StmtClass(*[make_agent(name) for name in inps])\n    return stmt", "response": "Make a Statement from the sort key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_complete(queue_name, job_list=None, job_name_prefix=None,\n                      poll_interval=10, idle_log_timeout=None,\n                      kill_on_log_timeout=False, stash_log_method=None,\n                      tag_instances=False, result_record=None):\n    \"\"\"Return when all jobs in the given list finished.\n\n    If not job list is given, return when all jobs in queue finished.\n\n    Parameters\n    ----------\n    queue_name : str\n        The name of the queue to wait for completion.\n    job_list : Optional[list(dict)]\n        A list of jobID-s in a dict, as returned by the submit function.\n        Example: [{'jobId': 'e6b00f24-a466-4a72-b735-d205e29117b4'}, ...]\n        If not given, this function will return if all jobs completed.\n    job_name_prefix : Optional[str]\n        A prefix for the name of the jobs to wait for. This is useful if the\n        explicit job list is not available but filtering is needed.\n    poll_interval : Optional[int]\n        The time delay between API calls to check the job statuses.\n    idle_log_timeout : Optional[int] or None\n        If not None, then track the logs of the active jobs, and if new output\n        is not produced after `idle_log_timeout` seconds, a warning is printed.\n        If `kill_on_log_timeout` is set to True, the job will also be\n        terminated.\n    kill_on_log_timeout : Optional[bool]\n        If True, and if `idle_log_timeout` is set, jobs will be terminated\n        after timeout. This has no effect if `idle_log_timeout` is None.\n        Default is False.\n    stash_log_method : Optional[str]\n        Select a method to store the job logs, either 's3' or 'local'. If no\n        method is specified, the logs will not be loaded off of AWS. If 's3' is\n        specified, then `job_name_prefix` must also be given, as this will\n        indicate where on s3 to store the logs.\n    tag_instances : bool\n        Default is False. If True, apply tags to the instances. This is toady\n        typically done by each job, so in most cases this should not be needed.\n    result_record : dict\n        A dict which will be modified in place to record the results of the job.\n    \"\"\"\n    if stash_log_method == 's3' and job_name_prefix is None:\n        raise Exception('A job_name_prefix is required to post logs on s3.')\n\n    start_time = datetime.now()\n    if job_list is None:\n        job_id_list = []\n    else:\n        job_id_list = [job['jobId'] for job in job_list]\n\n    if result_record is None:\n        result_record = {}\n\n    def get_jobs_by_status(status, job_id_filter=None, job_name_prefix=None):\n        res = batch_client.list_jobs(jobQueue=queue_name,\n                                     jobStatus=status, maxResults=10000)\n        jobs = res['jobSummaryList']\n        if job_name_prefix:\n            jobs = [job for job in jobs if\n                    job['jobName'].startswith(job_name_prefix)]\n        if job_id_filter:\n            jobs = [job_def for job_def in jobs\n                    if job_def['jobId'] in job_id_filter]\n        return jobs\n\n    job_log_dict = {}\n\n    def check_logs(job_defs):\n        \"\"\"Updates teh job_log_dict.\"\"\"\n        stalled_jobs = set()\n\n        # Check the status of all the jobs we're tracking.\n        for job_def in job_defs:\n            try:\n                # Get the logs for this job.\n                log_lines = get_job_log(job_def, write_file=False)\n\n                # Get the job id.\n                jid = job_def['jobId']\n                now = datetime.now()\n                if jid not in job_log_dict.keys():\n                    # If the job is new...\n                    logger.info(\"Adding job %s to the log tracker at %s.\"\n                                % (jid, now))\n                    job_log_dict[jid] = {'log': log_lines,\n                                         'last change time': now}\n                elif len(job_log_dict[jid]['log']) == len(log_lines):\n                    # If the job log hasn't changed, announce as such, and check\n                    # to see if it has been the same for longer than stall time.\n                    check_dt = now - job_log_dict[jid]['last change time']\n                    logger.warning(('Job \\'%s\\' has not produced output for '\n                                    '%d seconds.')\n                                   % (job_def['jobName'], check_dt.seconds))\n                    if check_dt.seconds > idle_log_timeout:\n                        logger.warning(\"Job \\'%s\\' has stalled.\"\n                                       % job_def['jobName'])\n                        stalled_jobs.add(jid)\n                else:\n                    # If the job is known, and the logs have changed, update the\n                    # \"last change time\".\n                    old_log = job_log_dict[jid]['log']\n                    old_log += log_lines[len(old_log):]\n                    job_log_dict[jid]['last change time'] = now\n            except Exception as e:\n                # Sometimes due to sync et al. issues, a part of this will fail.\n                # Such things are usually transitory issues so we keep trying.\n                logger.error(\"Failed to check log for: %s\" % str(job_def))\n                logger.exception(e)\n\n        # Pass up the set of job id's for stalled jobs.\n        return stalled_jobs\n\n    # Don't start watching jobs added after this command was initialized.\n    observed_job_def_dict = {}\n    def get_dict_of_job_tuples(job_defs):\n        return {jdef['jobId']: [(k, jdef[k]) for k in ['jobName', 'jobId']]\n                for jdef in job_defs}\n\n    batch_client = boto3.client('batch')\n    if tag_instances:\n        ecs_cluster_name = get_ecs_cluster_for_queue(queue_name, batch_client)\n\n    terminate_msg = 'Job log has stalled for at least %f minutes.'\n    terminated_jobs = set()\n    stashed_id_set = set()\n    while True:\n        pre_run = []\n        for status in ('SUBMITTED', 'PENDING', 'RUNNABLE', 'STARTING'):\n            pre_run += get_jobs_by_status(status, job_id_list, job_name_prefix)\n        running = get_jobs_by_status('RUNNING', job_id_list, job_name_prefix)\n        failed = get_jobs_by_status('FAILED', job_id_list, job_name_prefix)\n        done = get_jobs_by_status('SUCCEEDED', job_id_list, job_name_prefix)\n\n        observed_job_def_dict.update(get_dict_of_job_tuples(pre_run + running))\n\n        logger.info('(%d s)=(pre: %d, running: %d, failed: %d, done: %d)' %\n                    ((datetime.now() - start_time).seconds, len(pre_run),\n                     len(running), len(failed), len(done)))\n\n        # Check the logs for new output, and possibly terminate some jobs.\n        stalled_jobs = check_logs(running)\n        if idle_log_timeout is not None:\n            if kill_on_log_timeout:\n                # Keep track of terminated jobs so we don't send a terminate\n                # message twice.\n                for jid in stalled_jobs - terminated_jobs:\n                    batch_client.terminate_job(\n                        jobId=jid,\n                        reason=terminate_msg % (idle_log_timeout/60.0)\n                        )\n                    logger.info('Terminating %s.' % jid)\n                    terminated_jobs.add(jid)\n\n        if job_id_list:\n            if (len(failed) + len(done)) == len(job_id_list):\n                ret = 0\n                break\n        else:\n            if (len(failed) + len(done) > 0) and \\\n               (len(pre_run) + len(running) == 0):\n                ret = 0\n                break\n\n        if tag_instances:\n            tag_instances_on_cluster(ecs_cluster_name)\n\n        # Stash the logs of things that have finished so far. Note that jobs\n        # terminated in this round will not be picked up until the next round.\n        if stash_log_method:\n            stash_logs(observed_job_def_dict, done, failed, queue_name,\n                       stash_log_method, job_name_prefix,\n                       start_time.strftime('%Y%m%d_%H%M%S'),\n                       ids_stashed=stashed_id_set)\n        sleep(poll_interval)\n\n    # Pick up any stragglers\n    if stash_log_method:\n        stash_logs(observed_job_def_dict, done, failed, queue_name,\n                   stash_log_method, job_name_prefix,\n                   start_time.strftime('%Y%m%d_%H%M%S'),\n                   ids_stashed=stashed_id_set)\n\n    result_record['terminated'] = terminated_jobs\n    result_record['failed'] = failed\n    result_record['succeeded'] = done\n\n    return ret", "response": "Wait for all jobs in the given queue to complete."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ecs_cluster_for_queue(queue_name, batch_client=None):\n    if batch_client is None:\n        batch_client = boto3.client('batch')\n\n    queue_resp = batch_client.describe_job_queues(jobQueues=[queue_name])\n    if len(queue_resp['jobQueues']) == 1:\n        queue = queue_resp['jobQueues'][0]\n    else:\n        raise BatchReadingError('Error finding queue with name %s.'\n                                % queue_name)\n\n    compute_env_names = queue['computeEnvironmentOrder']\n    if len(compute_env_names) == 1:\n        compute_env_name = compute_env_names[0]['computeEnvironment']\n    else:\n        raise BatchReadingError('Error finding the compute environment name '\n                                'for %s.' % queue_name)\n\n    compute_envs = batch_client.describe_compute_environments(\n        computeEnvironments=[compute_env_name]\n        )['computeEnvironments']\n    if len(compute_envs) == 1:\n        compute_env = compute_envs[0]\n    else:\n        raise BatchReadingError(\"Error getting compute environment %s for %s. \"\n                                \"Got %d environments instead of 1.\"\n                                % (compute_env_name, queue_name,\n                                   len(compute_envs)))\n\n    ecs_cluster_name = os.path.basename(compute_env['ecsClusterArn'])\n    return ecs_cluster_name", "response": "Get the name of the ecs cluster for a given queue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tag_instances_on_cluster(cluster_name, project='cwc'):\n    # Get the relevant instance ids from the ecs cluster\n    ecs = boto3.client('ecs')\n    task_arns = ecs.list_tasks(cluster=cluster_name)['taskArns']\n    if not task_arns:\n        return\n    tasks = ecs.describe_tasks(cluster=cluster_name, tasks=task_arns)['tasks']\n    container_instances = ecs.describe_container_instances(\n        cluster=cluster_name,\n        containerInstances=[task['containerInstanceArn'] for task in tasks]\n        )['containerInstances']\n    ec2_instance_ids = [ci['ec2InstanceId'] for ci in container_instances]\n\n    # Instantiate each instance to tag as a resource and create project tag\n    for instance_id in ec2_instance_ids:\n        tag_instance(instance_id, project=project)\n    return", "response": "Creates a project tag for each instance in the given cluster."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submit_reading(basename, pmid_list_filename, readers, start_ix=None,\n                   end_ix=None, pmids_per_job=3000, num_tries=2,\n                   force_read=False, force_fulltext=False, project_name=None):\n    \"\"\"Submit an old-style pmid-centered no-database s3 only reading job.\n\n    This function is provided for the sake of backward compatibility. It is\n    preferred that you use the object-oriented PmidSubmitter and the\n    submit_reading job going forward.\n    \"\"\"\n    sub = PmidSubmitter(basename, readers, project_name)\n    sub.set_options(force_read, force_fulltext)\n    sub.submit_reading(pmid_list_filename, start_ix, end_ix, pmids_per_job,\n                       num_tries)\n    return sub.job_list", "response": "Submit a read job for a list of readers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef submit_combine(basename, readers, job_ids=None, project_name=None):\n    sub = PmidSubmitter(basename, readers, project_name)\n    sub.job_list = job_ids\n    sub.submit_combine()\n    return sub", "response": "Submit a batch job to combine the outputs of a reading job."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the parser for reading content from all PMIDs.", "response": "def create_read_parser():\n    import argparse\n    parent_read_parser = argparse.ArgumentParser(add_help=False)\n    parent_read_parser.add_argument(\n        'input_file',\n        help=('Path to file containing input ids of content to read. For the '\n              'no-db options, this is simply a file with each line being a '\n              'pmid. For the with-db options, this is a file where each line '\n              'is of the form \\'<id type>:<id>\\', for example \\'pmid:12345\\'')\n    )\n    parent_read_parser.add_argument(\n        '--start_ix',\n        type=int,\n        help='Start index of ids to read.'\n    )\n    parent_read_parser.add_argument(\n        '--end_ix',\n        type=int,\n        help='End index of ids to read. If `None`, read content from all ids.'\n    )\n    parent_read_parser.add_argument(\n        '--force_read',\n        action='store_true',\n        help='Read papers even if previously read by current REACH.'\n    )\n    parent_read_parser.add_argument(\n        '--force_fulltext',\n        action='store_true',\n        help='Get full text content even if content already on S3.'\n    )\n    parent_read_parser.add_argument(\n        '--ids_per_job',\n        default=3000,\n        type=int,\n        help='Number of PMIDs to read for each AWS Batch job.'\n    )\n    ''' Not currently supported.\n    parent_read_parser.add_argument(\n        '--num_tries',\n        default=2,\n        type=int,\n        help='Maximum number of times to try running job.'\n        )\n    '''\n    return parent_read_parser"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsubmitting a batch of reading jobs.", "response": "def submit_reading(self, input_fname, start_ix, end_ix, ids_per_job,\n                       num_tries=1, stagger=0):\n        \"\"\"Submit a batch of reading jobs\n\n        Parameters\n        ----------\n        input_fname : str\n            The name of the file containing the ids to be read.\n        start_ix : int\n            The line index of the first item in the list to read.\n        end_ix : int\n            The line index of the last item in the list to be read.\n        ids_per_job : int\n            The number of ids to be given to each job.\n        num_tries : int\n            The number of times a job may be attempted.\n        stagger : float\n            The number of seconds to wait between job submissions.\n\n        Returns\n        -------\n        job_list : list[str]\n            A list of job id strings.\n        \"\"\"\n        # stash this for later.\n        self.ids_per_job = ids_per_job\n\n        # Upload the pmid_list to Amazon S3\n        id_list_key = 'reading_results/%s/%s' % (self.basename,\n                                                 self._s3_input_name)\n        s3_client = boto3.client('s3')\n        s3_client.upload_file(input_fname, bucket_name, id_list_key)\n\n        # If no end index is specified, read all the PMIDs\n        if end_ix is None:\n            with open(input_fname, 'rt') as f:\n                lines = f.readlines()\n                end_ix = len(lines)\n\n        if start_ix is None:\n            start_ix = 0\n\n        # Get environment variables\n        environment_vars = get_environment()\n\n        # Iterate over the list of PMIDs and submit the job in chunks\n        batch_client = boto3.client('batch', region_name='us-east-1')\n        job_list = []\n        for job_start_ix in range(start_ix, end_ix, ids_per_job):\n            sleep(stagger)\n            job_end_ix = job_start_ix + ids_per_job\n            if job_end_ix > end_ix:\n                job_end_ix = end_ix\n            job_name, cmd = self._make_command(job_start_ix, job_end_ix)\n            command_list = get_batch_command(cmd, purpose=self._purpose,\n                                             project=self.project_name)\n            logger.info('Command list: %s' % str(command_list))\n            job_info = batch_client.submit_job(\n                jobName=job_name,\n                jobQueue=self._job_queue,\n                jobDefinition=self._job_def,\n                containerOverrides={\n                    'environment': environment_vars,\n                    'command': command_list},\n                retryStrategy={'attempts': num_tries}\n            )\n            logger.info(\"submitted...\")\n            job_list.append({'jobId': job_info['jobId']})\n        self.job_list = job_list\n        return job_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef watch_and_wait(self, poll_interval=10, idle_log_timeout=None,\n                       kill_on_timeout=False, stash_log_method=None,\n                       tag_instances=False, **kwargs):\n        \"\"\"This provides shortcut access to the wait_for_complete_function.\"\"\"\n        return wait_for_complete(self._job_queue, job_list=self.job_list,\n                                 job_name_prefix=self.basename,\n                                 poll_interval=poll_interval,\n                                 idle_log_timeout=idle_log_timeout,\n                                 kill_on_log_timeout=kill_on_timeout,\n                                 stash_log_method=stash_log_method,\n                                 tag_instances=tag_instances, **kwargs)", "response": "This method is used to watch the job queue and wait for the job to complete."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, input_fname, ids_per_job, stagger=0, **wait_params):\n        submit_thread = Thread(target=self.submit_reading,\n                               args=(input_fname, 0, None, ids_per_job),\n                               kwargs={'stagger': stagger},\n                               daemon=True)\n        submit_thread.start()\n        self.watch_and_wait(**wait_params)\n        submit_thread.join(0)\n        if submit_thread.is_alive():\n            logger.warning(\"Submit thread is still running even after job\"\n                           \"completion.\")\n        return", "response": "This method will run this submission all the way."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the options for this run.", "response": "def set_options(self, force_read=False, force_fulltext=False):\n        \"\"\"Set the options for this run.\"\"\"\n        self.options['force_read'] = force_read\n        self.options['force_fulltext'] = force_fulltext\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a ChEBI name corresponding to the given ChEBI ID.", "response": "def get_chebi_name_from_id(chebi_id, offline=False):\n    \"\"\"Return a ChEBI name corresponding to the given ChEBI ID.\n\n    Parameters\n    ----------\n    chebi_id : str\n        The ChEBI ID whose name is to be returned.\n    offline : Optional[bool]\n        Choose whether to allow an online lookup if the local lookup fails. If\n        True, the online lookup is not attempted. Default: False.\n\n    Returns\n    -------\n    chebi_name : str\n        The name corresponding to the given ChEBI ID. If the lookup\n        fails, None is returned.\n    \"\"\"\n    chebi_name = chebi_id_to_name.get(chebi_id)\n    if chebi_name is None and not offline:\n        chebi_name = get_chebi_name_from_id_web(chebi_id)\n    return chebi_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_chebi_name_from_id_web(chebi_id):\n    url_base = 'http://www.ebi.ac.uk/webservices/chebi/2.0/test/'\n    url_fmt = url_base + 'getCompleteEntity?chebiId=%s'\n    resp = requests.get(url_fmt % chebi_id)\n    if resp.status_code != 200:\n        logger.warning(\"Got bad code form CHEBI client: %s\" % resp.status_code)\n        return None\n    tree = etree.fromstring(resp.content)\n\n    # Get rid of the namespaces.\n    # Credit: https://stackoverflow.com/questions/18159221/remove-namespace-and-prefix-from-xml-in-python-using-lxml\n    for elem in tree.getiterator():\n        if not hasattr(elem.tag, 'find'):\n            continue  # (1)\n        i = elem.tag.find('}')\n        if i >= 0:\n            elem.tag = elem.tag[i+1:]\n    objectify.deannotate(tree, cleanup_namespaces=True)\n\n    elem = tree.find('Body/getCompleteEntityResponse/return/chebiAsciiName')\n    if elem is not None:\n        return elem.text\n    return None", "response": "Get a ChEBI name corresponding to a given ChEBI ID using a REST API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a PySB model based on a subset of INDRA Statements.", "response": "def get_subnetwork(statements, nodes, relevance_network=None,\n                   relevance_node_lim=10):\n    \"\"\"Return a PySB model based on a subset of given INDRA Statements.\n\n    Statements are first filtered for nodes in the given list and other nodes\n    are optionally added based on relevance in a given network. The filtered\n    statements are then assembled into an executable model using INDRA's\n    PySB Assembler.\n\n    Parameters\n    ----------\n    statements : list[indra.statements.Statement]\n        A list of INDRA Statements to extract a subnetwork from.\n    nodes : list[str]\n        The names of the nodes to extract the subnetwork for.\n    relevance_network : Optional[str]\n        The UUID of the NDEx network in which nodes relevant to the given\n        nodes are found.\n    relevance_node_lim : Optional[int]\n        The maximal number of additional nodes to add to the subnetwork\n        based on relevance.\n\n    Returns\n    -------\n    model : pysb.Model\n        A PySB model object assembled using INDRA's PySB Assembler from\n        the INDRA Statements corresponding to the subnetwork.\n    \"\"\"\n    if relevance_network is not None:\n        relevant_nodes = _find_relevant_nodes(nodes, relevance_network,\n                                              relevance_node_lim)\n        all_nodes = nodes + relevant_nodes\n    else:\n        all_nodes = nodes\n    filtered_statements = _filter_statements(statements, all_nodes)\n    pa = PysbAssembler()\n    pa.add_statements(filtered_statements)\n    model = pa.make_model()\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _filter_statements(statements, agents):\n    filtered_statements = []\n    for s in stmts:\n        if all([a is not None for a in s.agent_list()]) and \\\n            all([a.name in agents for a in s.agent_list()]):\n            filtered_statements.append(s)\n    return filtered_statements", "response": "Returns INDRA Statements which have Agents in the given list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of nodes that are relevant for the query.", "response": "def _find_relevant_nodes(query_nodes, relevance_network, relevance_node_lim):\n    \"\"\"Return a list of nodes that are relevant for the query.\n\n    Parameters\n    ----------\n    query_nodes : list[str]\n        A list of node names to query for.\n    relevance_network : str\n        The UUID of the NDEx network to query relevance in.\n    relevance_node_lim : int\n        The number of top relevant nodes to return.\n\n    Returns\n    -------\n    nodes : list[str]\n        A list of node names that are relevant for the query.\n    \"\"\"\n    all_nodes = relevance_client.get_relevant_nodes(relevance_network,\n                                                    query_nodes)\n    nodes = [n[0] for n in all_nodes[:relevance_node_lim]]\n    return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess a JSON - LD file in the new format to extract INDRA Statements.", "response": "def process_jsonld_file(fname):\n    \"\"\"Process a JSON-LD file in the new format to extract Statements.\n\n    Parameters\n    ----------\n    fname : str\n        The path to the JSON-LD file to be processed.\n\n    Returns\n    -------\n    indra.sources.hume.HumeProcessor\n        A HumeProcessor instance, which contains a list of INDRA Statements\n        as its statements attribute.\n    \"\"\"\n    with open(fname, 'r') as fh:\n        json_dict = json.load(fh)\n    return process_jsonld(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nterminate all RUNNING RUNNABLE and STARTING jobs.", "response": "def kill_all(job_queue, reason='None given', states=None):\n    \"\"\"Terminates/cancels all RUNNING, RUNNABLE, and STARTING jobs.\"\"\"\n    if states is None:\n        states = ['STARTING', 'RUNNABLE', 'RUNNING']\n    batch = boto3.client('batch')\n    runnable = batch.list_jobs(jobQueue=job_queue, jobStatus='RUNNABLE')\n    job_info = runnable.get('jobSummaryList')\n    if job_info:\n        job_ids = [job['jobId'] for job in job_info]\n        # Cancel jobs\n        for job_id in job_ids:\n            batch.cancel_job(jobId=job_id, reason=reason)\n    res_list = []\n    for status in states:\n        running = batch.list_jobs(jobQueue=job_queue, jobStatus=status)\n        job_info = running.get('jobSummaryList')\n        if job_info:\n            job_ids = [job['jobId'] for job in job_info]\n            for job_id in job_ids:\n                logger.info('Killing %s' % job_id)\n                res = batch.terminate_job(jobId=job_id, reason=reason)\n                res_list.append(res)\n    return res_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntags a single ec2 instance.", "response": "def tag_instance(instance_id, **tags):\n    \"\"\"Tag a single ec2 instance.\"\"\"\n    logger.debug(\"Got request to add tags %s to instance %s.\"\n                 % (str(tags), instance_id))\n    ec2 = boto3.resource('ec2')\n    instance = ec2.Instance(instance_id)\n\n    # Remove None's from `tags`\n    filtered_tags = {k: v for k, v in tags.items() if v and k}\n\n    # Check for existing tags\n    if instance.tags is not None:\n        existing_tags = {tag.get('Key'): tag.get('Value')\n                         for tag in instance.tags}\n        logger.debug(\"Ignoring existing tags; %s\" % str(existing_tags))\n        for tag_key in existing_tags.keys():\n            filtered_tags.pop(tag_key, None)\n\n    # If we have new tags to add, add them.\n    tag_list = [{'Key': k, 'Value': v} for k, v in filtered_tags.items()]\n    if len(tag_list):\n        logger.info('Adding project tags \"%s\" to instance %s'\n                    % (filtered_tags, instance_id))\n        instance.create_tags(Tags=tag_list)\n    else:\n        logger.info('No new tags from: %s' % str(tags))\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions run when indra is used in an EC2 instance to apply tags.", "response": "def tag_myself(project='cwc', **other_tags):\n    \"\"\"Function run when indra is used in an EC2 instance to apply tags.\"\"\"\n    base_url = \"http://169.254.169.254\"\n    try:\n        resp = requests.get(base_url + \"/latest/meta-data/instance-id\")\n    except requests.exceptions.ConnectionError:\n        logger.warning(\"Could not connect to service. Note this should only \"\n                       \"be run from within a batch job.\")\n        return\n    instance_id = resp.text\n    tag_instance(instance_id, project=project, **other_tags)\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the command appropriate for running something on batch.", "response": "def get_batch_command(command_list, project=None, purpose=None):\n    \"\"\"Get the command appropriate for running something on batch.\"\"\"\n    command_str = ' '.join(command_list)\n    ret = ['python', '-m', 'indra.util.aws', 'run_in_batch', command_str]\n    if not project and has_config('DEFAULT_AWS_PROJECT'):\n        project = get_config('DEFAULT_AWS_PROJECT')\n    if project:\n        ret += ['--project', project]\n    if purpose:\n        ret += ['--purpose', purpose]\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of dicts with jobName and jobId for each job with the given status.", "response": "def get_jobs(job_queue='run_reach_queue', job_status='RUNNING'):\n    \"\"\"Returns a list of dicts with jobName and jobId for each job with the\n    given status.\"\"\"\n    batch = boto3.client('batch')\n    jobs = batch.list_jobs(jobQueue=job_queue, jobStatus=job_status)\n    return jobs.get('jobSummaryList')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_job_log(job_info, log_group_name='/aws/batch/job',\n                write_file=True, verbose=False):\n    \"\"\"Gets the Cloudwatch log associated with the given job.\n\n    Parameters\n    ----------\n    job_info : dict\n        dict containing entries for 'jobName' and 'jobId', e.g., as returned\n        by get_jobs()\n    log_group_name : string\n        Name of the log group; defaults to '/aws/batch/job'\n    write_file : boolean\n        If True, writes the downloaded log to a text file with the filename\n        '%s_%s.log' % (job_name, job_id)\n\n\n    Returns\n    -------\n    list of strings\n        The event messages in the log, with the earliest events listed first.\n    \"\"\"\n    job_name = job_info['jobName']\n    job_id = job_info['jobId']\n    logs = boto3.client('logs')\n    batch = boto3.client('batch')\n    resp = batch.describe_jobs(jobs=[job_id])\n    job_desc = resp['jobs'][0]\n    job_def_name = job_desc['jobDefinition'].split('/')[-1].split(':')[0]\n    task_arn_id = job_desc['container']['taskArn'].split('/')[-1]\n    log_stream_name = '%s/default/%s' % (job_def_name, task_arn_id)\n    stream_resp = logs.describe_log_streams(\n                            logGroupName=log_group_name,\n                            logStreamNamePrefix=log_stream_name)\n    streams = stream_resp.get('logStreams')\n    if not streams:\n        logger.warning('No streams for job')\n        return None\n    elif len(streams) > 1:\n        logger.warning('More than 1 stream for job, returning first')\n    log_stream_name = streams[0]['logStreamName']\n    if verbose:\n        logger.info(\"Getting log for %s/%s\" % (job_name, job_id))\n    out_file = ('%s_%s.log' % (job_name, job_id)) if write_file else None\n    lines = get_log_by_name(log_group_name, log_stream_name, out_file, verbose)\n    return lines", "response": "Returns the Cloudwatch log associated with the given job."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload a log from the given log group and stream name.", "response": "def get_log_by_name(log_group_name, log_stream_name, out_file=None,\n                    verbose=True):\n    \"\"\"Download a log given the log's group and stream name.\n\n    Parameters\n    ----------\n    log_group_name : str\n        The name of the log group, e.g. /aws/batch/job.\n\n    log_stream_name : str\n        The name of the log stream, e.g. run_reach_jobdef/default/<UUID>\n\n    Returns\n    -------\n    lines : list[str]\n        The lines of the log as a list.\n    \"\"\"\n    logs = boto3.client('logs')\n    kwargs = {'logGroupName': log_group_name,\n              'logStreamName': log_stream_name,\n              'startFromHead': True}\n    lines = []\n    while True:\n        response = logs.get_log_events(**kwargs)\n        # If we've gotten all the events already, the nextForwardToken for\n        # this call will be the same as the last one\n        if response.get('nextForwardToken') == kwargs.get('nextToken'):\n            break\n        else:\n            events = response.get('events')\n            if events:\n                lines += ['%s: %s\\n' % (evt['timestamp'], evt['message'])\n                          for evt in events]\n            kwargs['nextToken'] = response.get('nextForwardToken')\n        if verbose:\n            logger.info('%d %s' % (len(lines), lines[-1]))\n    if out_file:\n        with open(out_file, 'wt') as f:\n            for line in lines:\n                f.write(line)\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite logs for all jobs with given the status to files.", "response": "def dump_logs(job_queue='run_reach_queue', job_status='RUNNING'):\n    \"\"\"Write logs for all jobs with given the status to files.\"\"\"\n    jobs = get_jobs(job_queue, job_status)\n    for job in jobs:\n        get_job_log(job, write_file=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\novercome s3 response limit and return NestedDict object for file directory structure.", "response": "def get_s3_file_tree(s3, bucket, prefix):\n    \"\"\"Overcome s3 response limit and return NestedDict tree of paths.\n\n    The NestedDict object also allows the user to search by the ends of a path.\n\n    The tree mimics a file directory structure, with the leave nodes being the\n    full unbroken key. For example, 'path/to/file.txt' would be retrieved by\n\n        ret['path']['to']['file.txt']['key']\n\n    The NestedDict object returned also has the capability to get paths that\n    lead to a certain value. So if you wanted all paths that lead to something\n    called 'file.txt', you could use\n\n        ret.get_paths('file.txt')\n\n    For more details, see the NestedDict docs.\n    \"\"\"\n    def get_some_keys(keys, marker=None):\n        if marker:\n            relevant_files = s3.list_objects(Bucket=bucket, Prefix=prefix,\n                                             Marker=marker)\n        else:\n            relevant_files = s3.list_objects(Bucket=bucket, Prefix=prefix)\n        keys.extend([entry['Key'] for entry in relevant_files['Contents']\n                     if entry['Key'] != marker])\n        return relevant_files['IsTruncated']\n\n    file_keys = []\n    marker = None\n    while get_some_keys(file_keys, marker):\n        marker = file_keys[-1]\n\n    file_tree = NestedDict()\n    pref_path = prefix.split('/')[:-1]   # avoid the trailing empty str.\n    for key in file_keys:\n        full_path = key.split('/')\n        relevant_path = full_path[len(pref_path):]\n        curr = file_tree\n        for step in relevant_path:\n            curr = curr[step]\n        curr['key'] = key\n    return file_tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_model(self, use_name_as_key=False, include_mods=False,\n                   include_complexes=False):\n        \"\"\"Assemble the graph from the assembler's list of INDRA Statements.\n\n        Parameters\n        ----------\n        use_name_as_key : boolean\n            If True, uses the name of the agent as the key to the nodes in\n            the network. If False (default) uses the matches_key() of the\n            agent.\n        include_mods : boolean\n            If True, adds Modification statements into the graph as directed\n            edges. Default is False.\n        include_complexes : boolean\n            If True, creates two edges (in both directions) between all pairs\n            of nodes in Complex statements. Default is False.\n        \"\"\"\n        self.graph = nx.DiGraph()\n        self._use_name_as_key = use_name_as_key\n        for st in self.stmts:\n            support_all = len(st.evidence)\n            support_pmid = len(set([ev.pmid for ev in st.evidence\n                                    if ev.pmid is not None]))\n            attr = {'polarity': 'unknown', 'support_all': support_all,\n                    'support_pmid': support_pmid}\n            if isinstance(st, RegulateActivity):\n                attr['polarity'] = ('positive' if st.is_activation\n                                    else 'negative')\n                self._add_node_edge(st.subj, st.obj, attr)\n            elif include_mods and isinstance(st, Modification):\n                self._add_node_edge(st.agent_list()[0], st.agent_list()[1], attr)\n            elif include_mods and \\\n                 (isinstance(st, Gap) or isinstance(st, DecreaseAmount)):\n                attr['polarity'] = 'negative'\n                self._add_node_edge(st.agent_list()[0], st.agent_list()[1], attr)\n            elif include_mods and \\\n                 (isinstance(st, Gef) or isinstance(st, IncreaseAmount)):\n                attr['polarity'] = 'positive'\n                self._add_node_edge(st.agent_list()[0], st.agent_list()[1], attr)\n            elif include_complexes and isinstance(st, Complex):\n                # Create s->t edges between all possible pairs of complex\n                # members\n                for node1, node2 in itertools.permutations(st.members, 2):\n                    self._add_node_edge(node1, node2, attr)", "response": "Assemble the graph from the INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a SIF string of the assembled model.", "response": "def print_model(self, include_unsigned_edges=False):\n        \"\"\"Return a SIF string of the assembled model.\n\n        Parameters\n        ----------\n        include_unsigned_edges : bool\n            If True, includes edges with an unknown activating/inactivating\n            relationship (e.g., most PTMs). Default is False.\n        \"\"\"\n        sif_str = ''\n        for edge in self.graph.edges(data=True):\n            n1 = edge[0]\n            n2 = edge[1]\n            data = edge[2]\n            polarity = data.get('polarity')\n            if polarity == 'negative':\n                rel = '-1'\n            elif polarity == 'positive':\n                rel = '1'\n            elif include_unsigned_edges:\n                rel = '0'\n            else:\n                continue\n            sif_str += '%s %s %s\\n' % (n1, rel, n2)\n        return sif_str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the assembled model into a file.", "response": "def save_model(self, fname, include_unsigned_edges=False):\n        \"\"\"Save the assembled model's SIF string into a file.\n\n        Parameters\n        ----------\n        fname : str\n            The name of the file to save the SIF into.\n        include_unsigned_edges : bool\n            If True, includes edges with an unknown activating/inactivating\n            relationship (e.g., most PTMs). Default is False.\n        \"\"\"\n        sif_str = self.print_model(include_unsigned_edges)\n        with open(fname, 'wb') as fh:\n            fh.write(sif_str.encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_loopy(self, as_url=True):\n        init_str = ''\n        node_id = 1\n        node_list = {}\n        for node, data in self.graph.nodes(data=True):\n            node_name = data['name']\n            nodex = int(500*numpy.random.rand())\n            nodey = int(500*numpy.random.rand())\n            hue = int(5*numpy.random.rand())\n            node_attr = [node_id, nodex, nodey, 1, node_name, hue]\n            node_list[node] = node_attr\n            node_id += 1\n        nodes = list(node_list.values())\n\n        edges = []\n        for s, t, data in self.graph.edges(data=True):\n            s_id = node_list[s][0]\n            t_id = node_list[t][0]\n            if data['polarity'] == 'positive':\n                pol = 1\n            else:\n                pol = -1\n            edge = [s_id, t_id, 89, pol, 0]\n            edges.append(edge)\n\n        labels = []\n        components = [nodes, edges, labels]\n        model = json.dumps(components, separators=(',', ':'))\n        if as_url:\n            model = 'http://ncase.me/loopy/v1/?data=' + model\n        return model", "response": "Print a Loopy network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_boolean_net(self, out_file=None):\n        init_str = ''\n        for node_key in self.graph.nodes():\n            node_name = self.graph.node[node_key]['name']\n            init_str += '%s = False\\n' % node_name\n        rule_str = ''\n        for node_key in self.graph.nodes():\n            node_name = self.graph.node[node_key]['name']\n            in_edges = self.graph.in_edges(node_key)\n            if not in_edges:\n                continue\n            parents = [e[0] for e in in_edges]\n            polarities = [self.graph.edge[e[0]][node_key]['polarity']\n                          for e in in_edges]\n            pos_parents = [par for par, pol in zip(parents, polarities) if\n                           pol == 'positive']\n            neg_parents = [par for par, pol in zip(parents, polarities) if\n                           pol == 'negative']\n\n            rhs_pos_parts = []\n            for par in pos_parents:\n                rhs_pos_parts.append(self.graph.node[par]['name'])\n            rhs_pos_str = ' or '.join(rhs_pos_parts)\n\n            rhs_neg_parts = []\n            for par in neg_parents:\n                rhs_neg_parts.append(self.graph.node[par]['name'])\n            rhs_neg_str = ' or '.join(rhs_neg_parts)\n\n            if rhs_pos_str:\n                if rhs_neg_str:\n                    rhs_str = '(' + rhs_pos_str + \\\n                              ') and not (' + rhs_neg_str + ')'\n                else:\n                    rhs_str = rhs_pos_str\n            else:\n                rhs_str = 'not (' + rhs_neg_str + ')'\n\n            node_eq = '%s* = %s\\n' % (node_name, rhs_str)\n            rule_str += node_eq\n        full_str = init_str + '\\n' + rule_str\n        if out_file is not None:\n            with open(out_file, 'wt') as fh:\n                fh.write(full_str)\n        return full_str", "response": "Return a string representation of the Boolean network."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap Elsevier methods which directly use the API keys.", "response": "def _ensure_api_keys(task_desc, failure_ret=None):\n    \"\"\"Wrap Elsevier methods which directly use the API keys.\n\n    Ensure that the keys are retrieved from the environment or config file when\n    first called, and store global scope. Subsequently use globally stashed\n    results and check for required ids.\n    \"\"\"\n    def check_func_wrapper(func):\n        @wraps(func)\n        def check_api_keys(*args, **kwargs):\n            global ELSEVIER_KEYS\n            if ELSEVIER_KEYS is None:\n                ELSEVIER_KEYS = {}\n                # Try to read in Elsevier API keys. For each key, first check\n                # the environment variables, then check the INDRA config file.\n                if not has_config(INST_KEY_ENV_NAME):\n                    logger.warning('Institution API key %s not found in config '\n                                   'file or environment variable: this will '\n                                   'limit access for %s'\n                                   % (INST_KEY_ENV_NAME, task_desc))\n                ELSEVIER_KEYS['X-ELS-Insttoken'] = get_config(INST_KEY_ENV_NAME)\n\n                if not has_config(API_KEY_ENV_NAME):\n                    logger.error('API key %s not found in configuration file '\n                                 'or environment variable: cannot %s'\n                                 % (API_KEY_ENV_NAME, task_desc))\n                    return failure_ret\n                ELSEVIER_KEYS['X-ELS-APIKey'] = get_config(API_KEY_ENV_NAME)\n            elif 'X-ELS-APIKey' not in ELSEVIER_KEYS.keys():\n                logger.error('No Elsevier API key %s found: cannot %s'\n                             % (API_KEY_ENV_NAME, task_desc))\n                return failure_ret\n            return func(*args, **kwargs)\n        return check_api_keys\n    return check_func_wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_entitlement(doi):\n    if doi.lower().startswith('doi:'):\n        doi = doi[4:]\n    url = '%s/%s' % (elsevier_entitlement_url, doi)\n    params = {'httpAccept': 'text/xml'}\n    res = requests.get(url, params, headers=ELSEVIER_KEYS)\n    if not res.status_code == 200:\n        logger.error('Could not check entitlements for article %s: '\n                     'status code %d' % (doi, res.status_code))\n        logger.error('Response content: %s' % res.text)\n        return False\n    return True", "response": "Check whether IP and credentials enable access to content for a given doi."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_article(id_val, id_type='doi', on_retry=False):\n    if id_type == 'pmid':\n        id_type = 'pubmed_id'\n    url = '%s/%s' % (elsevier_article_url_fmt % id_type, id_val)\n    params = {'httpAccept': 'text/xml'}\n    res = requests.get(url, params, headers=ELSEVIER_KEYS)\n    if res.status_code == 404:\n        logger.info(\"Resource for %s not available on elsevier.\" % url)\n        return None\n    elif res.status_code == 429:\n        if not on_retry:\n            logger.warning(\"Broke the speed limit. Waiting half a second then \"\n                           \"trying again...\")\n            sleep(0.5)\n            return download_article(id_val, id_type, True)\n        else:\n            logger.error(\"Still breaking speed limit after waiting.\")\n            logger.error(\"Elsevier response: %s\" % res.text)\n            return None\n    elif res.status_code != 200:\n        logger.error('Could not download article %s: status code %d' %\n                     (url, res.status_code))\n        logger.error('Elsevier response: %s' % res.text)\n        return None\n    else:\n        content_str = res.content.decode('utf-8')\n        if content_str.startswith('<service-error>'):\n            logger.error('Got a service error with 200 status: %s'\n                         % content_str)\n            return None\n    # Return the XML content as a unicode string, assuming UTF-8 encoding\n    return content_str", "response": "Low level function to get an XML article for a particular id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_article_from_ids(**id_dict):\n    valid_id_types = ['eid', 'doi', 'pmid', 'pii']\n    assert all([k in valid_id_types for k in id_dict.keys()]),\\\n        (\"One of these id keys is invalid: %s Valid keys are: %s.\"\n         % (list(id_dict.keys()), valid_id_types))\n    if 'doi' in id_dict.keys() and id_dict['doi'].lower().startswith('doi:'):\n        id_dict['doi'] = id_dict['doi'][4:]\n    content = None\n    for id_type in valid_id_types:\n        if id_type in id_dict.keys():\n            content = download_article(id_dict[id_type], id_type)\n            if content is not None:\n                break\n    else:\n        logger.error(\"Could not download article with any of the ids: %s.\"\n                     % str(id_dict))\n    return content", "response": "Download an article from Elsevier matching the set of ids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_abstract(doi):\n    xml_string = download_article(doi)\n    if xml_string is None:\n        return None\n    assert isinstance(xml_string, str)\n    xml_tree = ET.XML(xml_string.encode('utf-8'), parser=UTB())\n    if xml_tree is None:\n        return None\n    coredata = xml_tree.find('article:coredata', elsevier_ns)\n    abstract = coredata.find('dc:description', elsevier_ns)\n    abs_text = abstract.text\n    return abs_text", "response": "Get the abstract text of an article from Elsevier given a doi."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_article(doi, output_format='txt'):\n    xml_string = download_article(doi)\n    if output_format == 'txt' and xml_string is not None:\n        text = extract_text(xml_string)\n        return text\n    return xml_string", "response": "Get the full body of an article from Elsevier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_paragraphs(xml_string):\n    assert isinstance(xml_string, str)\n    xml_tree = ET.XML(xml_string.encode('utf-8'), parser=UTB())\n    full_text = xml_tree.find('article:originalText', elsevier_ns)\n    if full_text is None:\n        logger.info('Could not find full text element article:originalText')\n        return None\n    article_body = _get_article_body(full_text)\n    if article_body:\n        return article_body\n    raw_text = _get_raw_text(full_text)\n    if raw_text:\n        return [raw_text]\n    return None", "response": "Get paragraphs from the body of the given Elsevier xml."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching ScienceDirect through the API for articles.", "response": "def get_dois(query_str, count=100):\n    \"\"\"Search ScienceDirect through the API for articles.\n\n    See http://api.elsevier.com/content/search/fields/scidir for constructing a\n    query string to pass here.  Example: 'abstract(BRAF) AND all(\"colorectal\n    cancer\")'\n    \"\"\"\n    url = '%s/%s' % (elsevier_search_url, query_str)\n    params = {'query': query_str,\n              'count': count,\n              'httpAccept': 'application/xml',\n              'sort': '-coverdate',\n              'field': 'doi'}\n    res = requests.get(url, params)\n    if not res.status_code == 200:\n        return None\n    tree = ET.XML(res.content, parser=UTB())\n    doi_tags = tree.findall('atom:entry/prism:doi', elsevier_ns)\n    dois = [dt.text for dt in doi_tags]\n    return dois"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_piis(query_str):\n    dates = range(1960, datetime.datetime.now().year)\n    all_piis = flatten([get_piis_for_date(query_str, date) for date in dates])\n    return all_piis", "response": "Search ScienceDirect through API for articles and return PIIs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_piis_for_date(query_str, date):\n    count = 200\n    params = {'query': query_str,\n              'count': count,\n              'start': 0,\n              'sort': '-coverdate',\n              'date': date,\n              'field': 'pii'}\n    all_piis = []\n    while True:\n        res = requests.get(elsevier_search_url, params, headers=ELSEVIER_KEYS)\n        if not res.status_code == 200:\n            logger.info('Got status code: %d' % res.status_code)\n            break\n        res_json = res.json()\n        entries = res_json['search-results']['entry']\n        logger.info(res_json['search-results']['opensearch:totalResults'])\n        if entries == [{'@_fa': 'true', 'error': 'Result set was empty'}]:\n            logger.info('Search result was empty')\n            return []\n        piis = [entry['pii'] for entry in entries]\n        all_piis += piis\n        # Get next batch\n        links = res_json['search-results'].get('link', [])\n        cont = False\n        for link in links:\n            if link.get('@ref') == 'next':\n                logger.info('Found link to next batch of results.')\n                params['start'] += count\n                cont = True\n                break\n        if not cont:\n            break\n    return all_piis", "response": "Search ScienceDirect with a query string constrained to a given year."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_from_search(query_str, folder, do_extract_text=True,\n                         max_results=None):\n    \"\"\"Save raw text files based on a search for papers on ScienceDirect.\n\n    This performs a search to get PIIs, downloads the XML corresponding to\n    the PII, extracts the raw text and then saves the text into a file\n    in the designated folder.\n\n    Parameters\n    ----------\n    query_str : str\n        The query string to search with\n    folder : str\n        The local path to an existing folder in which the text files\n        will be dumped\n    do_extract_text : bool\n        Choose whether to extract text from the xml, or simply save the raw xml\n        files. Default is True, so text is extracted.\n    max_results : int or None\n        Default is None. If specified, limit the number of results to the given\n        maximum.\n    \"\"\"\n    piis = get_piis(query_str)\n    for pii in piis[:max_results]:\n        if os.path.exists(os.path.join(folder, '%s.txt' % pii)):\n            continue\n        logger.info('Downloading %s' % pii)\n        xml = download_article(pii, 'pii')\n        sleep(1)\n        if do_extract_text:\n            txt = extract_text(xml)\n            if not txt:\n                continue\n\n            with open(os.path.join(folder, '%s.txt' % pii), 'wb') as fh:\n                fh.write(txt.encode('utf-8'))\n        else:\n            with open(os.path.join(folder, '%s.xml' % pii), 'wb') as fh:\n                fh.write(xml.encode('utf-8'))\n    return", "response": "Download the raw text files corresponding to a given query string and save them into a folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_statement_from_query_result(self, res):\n        agent_start, agent_end, affected_start, affected_end = res\n\n        # Convert from rdflib literals to python integers so we can use\n        # them to index strings\n        agent_start = int(agent_start)\n        agent_end = int(agent_end)\n        affected_start = int(affected_start)\n        affected_end = int(affected_end)\n\n        # Find the text corresponding to these indices\n        agent = self.text[agent_start:agent_end]\n        affected = self.text[affected_start:affected_end]\n\n        # Strip off surrounding whitespace\n        agent = agent.lstrip().rstrip()\n        affected = affected.lstrip().rstrip()\n\n        # Make an Agent object for both the subject and the object\n        subj = Agent(agent, db_refs={'TEXT': agent})\n        obj = Agent(affected, db_refs={'TEXT': affected})\n\n        statement = Influence(subj=subj, obj=obj)\n\n        # Add the statement to the list of statements\n        self.statements.append(statement)", "response": "Adds a statement based on one element of a SPARQL query result."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts INDRA statements from the RDF graph.", "response": "def extract_statements(self):\n        \"\"\"Extracts INDRA statements from the RDF graph via SPARQL queries.\n        \"\"\"\n\n        # Look for events that have an AGENT and an AFFECTED, and get the\n        # start and ending text indices for each.\n        query = prefixes + \"\"\"\n        SELECT\n            ?agent_start\n            ?agent_end\n            ?affected_start\n            ?affected_end\n        WHERE {\n            ?rel role:AGENT ?agent .\n            ?rel role:AFFECTED ?affected .\n            ?agent lf:start ?agent_start .\n            ?agent lf:end ?agent_end .\n            ?affected lf:start ?affected_start .\n            ?affected lf:end ?affected_end .\n        }\n        \"\"\"\n        results = self.graph.query(query)\n        for res in results:\n            # Make a statement for each query match\n            self.extract_statement_from_query_result(res)\n\n        # Look for events that have an AGENT and a RESULT, and get the start\n        # and ending text indices for each.\n        query = query.replace('role:AFFECTED', 'role:RESULT')\n        results = self.graph.query(query)\n        for res in results:\n            # Make a statement for each query match\n            self.extract_statement_from_query_result(res)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlook up the constitutents of a complex and expands until all constituents are not complexes.", "response": "def _recursively_lookup_complex(self, complex_id):\n        \"\"\"Looks up the constitutents of a complex. If any constituent is\n        itself a complex, recursively expands until all constituents are\n        not complexes.\"\"\"\n        assert complex_id in self.complex_map\n\n        expanded_agent_strings = []\n        expand_these_next = [complex_id]\n        while len(expand_these_next) > 0:\n            # Pop next element\n            c = expand_these_next[0]\n            expand_these_next = expand_these_next[1:]\n\n            # If a complex, add expanding it to the end of the queue\n            # If an agent string, add it to the agent string list immediately\n            assert c in self.complex_map\n            for s in self.complex_map[c]:\n                if s in self.complex_map:\n                    expand_these_next.append(s)\n                else:\n                    expanded_agent_strings.append(s)\n        return expanded_agent_strings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_complex_agents(self, complex_id):\n        agents = []\n        components = self._recursively_lookup_complex(complex_id)\n\n        for c in components:\n            db_refs = {}\n            name = uniprot_client.get_gene_name(c)\n            if name is None:\n                db_refs['SIGNOR'] = c\n            else:\n                db_refs['UP'] = c\n                hgnc_id = hgnc_client.get_hgnc_id(name)\n                if hgnc_id:\n                    db_refs['HGNC'] = hgnc_id\n\n            famplex_key = ('SIGNOR', c)\n            if famplex_key in famplex_map:\n                db_refs['FPLX'] = famplex_map[famplex_key]\n                if not name:\n                    name = db_refs['FPLX']  # Set agent name to Famplex name if\n                                            # the Uniprot name is not available\n            elif not name:\n                # We neither have a Uniprot nor Famplex grounding\n                logger.info('Have neither a Uniprot nor Famplex grounding ' + \\\n                            'for ' + c)\n                if not name:\n                    name = db_refs['SIGNOR']  # Set the agent name to the\n                                              # Signor name if neither the\n                                              # Uniprot nor Famplex names are\n                                              # available\n            assert(name is not None)\n            agents.append(Agent(name, db_refs=db_refs))\n        return agents", "response": "Returns a list of agents corresponding to each of the constituents\n        in a SIGNOR complex."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stmts_from_json(json_in, on_missing_support='handle'):\n\n    stmts = []\n    uuid_dict = {}\n    for json_stmt in json_in:\n        try:\n            st = Statement._from_json(json_stmt)\n        except Exception as e:\n            logger.warning(\"Error creating statement: %s\" % e)\n            continue\n        stmts.append(st)\n        uuid_dict[st.uuid] = st\n    for st in stmts:\n        _promote_support(st.supports, uuid_dict, on_missing_support)\n        _promote_support(st.supported_by, uuid_dict, on_missing_support)\n    return stmts", "response": "Returns a list of INDRA Statements from a json list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing a list of INDRA Statements into a JSON file.", "response": "def stmts_to_json_file(stmts, fname):\n    \"\"\"Serialize a list of INDRA Statements into a JSON file.\n\n    Parameters\n    ----------\n    stmts : list[indra.statement.Statements]\n        The list of INDRA Statements to serialize into the JSON file.\n    fname : str\n        Path to the JSON file to serialize Statements into.\n    \"\"\"\n    with open(fname, 'w') as fh:\n        json.dump(stmts_to_json(stmts), fh, indent=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stmts_to_json(stmts_in, use_sbo=False):\n    if not isinstance(stmts_in, list):\n        json_dict = stmts_in.to_json(use_sbo=use_sbo)\n        return json_dict\n    else:\n        json_dict = [st.to_json(use_sbo=use_sbo) for st in stmts_in]\n    return json_dict", "response": "Return the JSON - serialized form of one or more INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _promote_support(sup_list, uuid_dict, on_missing='handle'):\n    valid_handling_choices = ['handle', 'error', 'ignore']\n    if on_missing not in valid_handling_choices:\n        raise InputError('Invalid option for `on_missing_support`: \\'%s\\'\\n'\n                         'Choices are: %s.'\n                         % (on_missing, str(valid_handling_choices)))\n    for idx, uuid in enumerate(sup_list):\n        if uuid in uuid_dict.keys():\n            sup_list[idx] = uuid_dict[uuid]\n        elif on_missing == 'handle':\n            sup_list[idx] = Unresolved(uuid)\n        elif on_missing == 'ignore':\n            sup_list.remove(uuid)\n        elif on_missing == 'error':\n            raise UnresolvedUuidError(\"Uuid %s not found in stmt jsons.\"\n                                      % uuid)\n    return", "response": "Promote the list of support - related uuids to Statements if possible."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef draw_stmt_graph(stmts):\n    import networkx\n    try:\n        import matplotlib.pyplot as plt\n    except Exception:\n        logger.error('Could not import matplotlib, not drawing graph.')\n        return\n    try:  # This checks whether networkx has this package to work with.\n        import pygraphviz\n    except Exception:\n        logger.error('Could not import pygraphviz, not drawing graph.')\n        return\n    import numpy\n    g = networkx.compose_all([stmt.to_graph() for stmt in stmts])\n    plt.figure()\n    plt.ion()\n    g.graph['graph'] = {'rankdir': 'LR'}\n    pos = networkx.drawing.nx_agraph.graphviz_layout(g, prog='dot')\n    g = g.to_undirected()\n\n    # Draw nodes\n    options = {\n        'marker': 'o',\n        's': 200,\n        'c': [0.85, 0.85, 1],\n        'facecolor': '0.5',\n        'lw': 0,\n    }\n    ax = plt.gca()\n    nodelist = list(g)\n    xy = numpy.asarray([pos[v] for v in nodelist])\n    node_collection = ax.scatter(xy[:, 0], xy[:, 1], **options)\n    node_collection.set_zorder(2)\n    # Draw edges\n    networkx.draw_networkx_edges(g, pos, arrows=False, edge_color='0.5')\n    # Draw labels\n    edge_labels = {(e[0], e[1]): e[2].get('label') for e in g.edges(data=True)}\n    networkx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels)\n    node_labels = {n[0]: n[1].get('label') for n in g.nodes(data=True)}\n    for key, label in node_labels.items():\n        if len(label) > 25:\n            parts = label.split(' ')\n            parts.insert(int(len(parts)/2), '\\n')\n            label = ' '.join(parts)\n            node_labels[key] = label\n    networkx.draw_networkx_labels(g, pos, labels=node_labels)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.show()", "response": "Render the attributes of a list of INDRA Statements as directed graphs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fix_json_agents(ag_obj):\n    if isinstance(ag_obj, str):\n        logger.info(\"Fixing string agent: %s.\" % ag_obj)\n        ret = {'name': ag_obj, 'db_refs': {'TEXT': ag_obj}}\n    elif isinstance(ag_obj, list):\n        # Recursive for complexes and similar.\n        ret = [_fix_json_agents(ag) for ag in ag_obj]\n    elif isinstance(ag_obj, dict) and 'TEXT' in ag_obj.keys():\n        ret = deepcopy(ag_obj)\n        text = ret.pop('TEXT')\n        ret['db_refs']['TEXT'] = text\n    else:\n        ret = ag_obj\n    return ret", "response": "Fix the json representation of an agent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the evidence PMID of Statements that have been extracted by the processor.", "response": "def set_statements_pmid(self, pmid):\n        \"\"\"Set the evidence PMID of Statements that have been extracted.\n\n        Parameters\n        ----------\n        pmid : str or None\n            The PMID to be used in the Evidence objects of the Statements\n            that were extracted by the processor.\n        \"\"\"\n        # Replace PMID value in JSON dict first\n        for stmt in self.json_stmts:\n            evs = stmt.get('evidence', [])\n            for ev in evs:\n                ev['pmid'] = pmid\n        # Replace PMID value in extracted Statements next\n        for stmt in self.statements:\n            for ev in stmt.evidence:\n                ev.pmid = pmid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_args(node):\n    arg_roles = {}\n    args = node.findall('arg') + \\\n        [node.find('arg1'), node.find('arg2'), node.find('arg3')]\n    for arg in args:\n        if arg is not None:\n            id = arg.attrib.get('id')\n            if id is not None:\n                arg_roles[arg.attrib['role']] = (arg.attrib['id'], arg)\n    # Now look at possible inevent links\n    if node.find('features') is not None:\n        inevents = node.findall('features/inevent')\n        for inevent in inevents:\n            if 'id' in inevent.attrib:\n                arg_roles['inevent'] = (inevent.attrib['id'], inevent)\n\n        ptms = node.findall('features/ptm') + node.findall('features/no-ptm')\n        for ptm in ptms:\n            if 'id' in inevent.attrib:\n                arg_roles['ptm'] = (inevent.attrib['id'], ptm)\n\n    # And also look for assoc-with links\n    aw = node.find('assoc-with')\n    if aw is not None:\n        aw_id = aw.attrib['id']\n        arg_roles['assoc-with'] = (aw_id, aw)\n    return arg_roles", "response": "Return the arguments of a node in the event graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the types of a and b are compatible False otherwise.", "response": "def type_match(a, b):\n    \"\"\"Return True of the types of a and b are compatible, False otherwise.\"\"\"\n    # If the types are the same, return True\n    if a['type'] == b['type']:\n        return True\n    # Otherwise, look at some special cases\n    eq_groups = [\n        {'ONT::GENE-PROTEIN', 'ONT::GENE', 'ONT::PROTEIN'},\n        {'ONT::PHARMACOLOGIC-SUBSTANCE', 'ONT::CHEMICAL'}\n        ]\n    for eq_group in eq_groups:\n        if a['type'] in eq_group and b['type'] in eq_group:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_graph(patterns, G):\n    if not patterns:\n        patterns.append([G])\n        return\n    for i, graphs in enumerate(patterns):\n        if networkx.is_isomorphic(graphs[0], G, node_match=type_match,\n                                  edge_match=type_match):\n            patterns[i].append(G)\n            return\n    patterns.append([G])", "response": "Add a graph to a set of unique patterns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing a graph and save it into a file", "response": "def draw(graph, fname):\n    \"\"\"Draw a graph and save it into a file\"\"\"\n    ag = networkx.nx_agraph.to_agraph(graph)\n    ag.draw(fname, prog='dot')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_patterns(fnames):\n    patterns = []\n    for fn in fnames:\n        et = ET.parse(fn)\n        res = et.findall('CC') + et.findall('EVENT')\n        for event in res:\n            G = networkx.DiGraph()\n            build_event_graph(G, et, event)\n            add_graph(patterns, G)\n    patterns = sorted(patterns, key=lambda x: len(x[0]), reverse=True)\n    return patterns", "response": "Build a list of CC or EVENT graph patterns from a list of EKB files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a DiGraph of a specific event structure.", "response": "def build_event_graph(graph, tree, node):\n    \"\"\"Return a DiGraph of a specific event structure, built recursively\"\"\"\n    # If we have already added this node then let's return\n    if node_key(node) in graph:\n        return\n    type = get_type(node)\n    text = get_text(node)\n    label = '%s (%s)' % (type, text)\n    graph.add_node(node_key(node), type=type, label=label, text=text)\n    args = get_args(node)\n    for arg_role, (arg_id, arg_tag) in args.items():\n        arg = get_node_by_id(tree, arg_id)\n        if arg is None:\n            arg = arg_tag\n        build_event_graph(graph, tree, arg)\n        graph.add_edge(node_key(node), node_key(arg), type=arg_role,\n                       label=arg_role)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a full list of all extracted event IDs from a list of EKB files", "response": "def get_extracted_events(fnames):\n    \"\"\"Get a full list of all extracted event IDs from a list of EKB files\"\"\"\n    event_list = []\n    for fn in fnames:\n        tp = trips.process_xml_file(fn)\n        ed = tp.extracted_events\n        for k, v in ed.items():\n            event_list += v\n    return event_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the ratio of patterns that were extracted.", "response": "def check_event_coverage(patterns, event_list):\n    \"\"\"Calculate the ratio of patterns that were extracted.\"\"\"\n    proportions = []\n    for pattern_list in patterns:\n        proportion = 0\n        for pattern in pattern_list:\n            for node in pattern.nodes():\n                if node in event_list:\n                    proportion += 1.0 / len(pattern_list)\n                    break\n        proportions.append(proportion)\n    return proportions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_wm_map(exclude_auto=None):\n    exclude_auto = [] if not exclude_auto else exclude_auto\n    path_here = os.path.dirname(os.path.abspath(__file__))\n    ontomap_file = os.path.join(path_here, '../resources/wm_ontomap.tsv')\n    mappings = {}\n\n    def make_hume_prefix_map():\n        hume_ont = os.path.join(path_here, '../sources/hume/hume_ontology.rdf')\n        graph = rdflib.Graph()\n        graph.parse(os.path.abspath(hume_ont), format='nt')\n        entry_map = {}\n        for node in graph.all_nodes():\n            entry = node.split('#')[1]\n            # Handle \"event\" and other top-level entries\n            if '/' not in entry:\n                entry_map[entry] = None\n                continue\n            parts = entry.split('/')\n            prefix, real_entry = parts[0], '/'.join(parts[1:])\n            entry_map[real_entry] = prefix\n        return entry_map\n\n    hume_prefix_map = make_hume_prefix_map()\n\n    def add_hume_prefix(hume_entry):\n        \"\"\"We need to do this because the HUME prefixes are missing\"\"\"\n        prefix = hume_prefix_map[hume_entry]\n        return '%s/%s' % (prefix, hume_entry)\n\n    def map_entry(reader, entry):\n        \"\"\"Remap the readers and entries to match our internal standards.\"\"\"\n        if reader == 'eidos':\n            namespace = 'UN'\n            entry = entry.replace(' ', '_')\n            entry_id = entry\n        elif reader == 'BBN':\n            namespace = 'HUME'\n            entry = entry.replace(' ', '_')\n            entry_id = add_hume_prefix(entry)\n        elif reader == 'sofia':\n            namespace = 'SOFIA'\n            # First chop off the Event/Entity prefix\n            parts = entry.split('/')[1:]\n            # Now we split each part by underscore and capitalize\n            # each piece of each part\n            parts = ['_'.join([p.capitalize() for p in part.split('_')])\n                     for part in parts]\n            # Finally we stick the entry back together separated by slashes\n            entry_id = '/'.join(parts)\n        else:\n            return reader, entry\n        return namespace, entry_id\n\n    with open(ontomap_file, 'r') as fh:\n        for line in fh.readlines():\n            # Get each entry from the line\n            s, se, t, te, score = line.strip().split('\\t')\n            score = float(score)\n            # Map the entries to our internal naming standards\n            s, se = map_entry(s, se)\n            t, te = map_entry(t, te)\n            # Skip automated mappings when they should be excluded\n            if (s, t) not in exclude_auto:\n                # We first do the forward mapping\n                if (s, se, t) in mappings:\n                    if mappings[(s, se, t)][1] < score:\n                        mappings[(s, se, t)] = ((t, te), score)\n                else:\n                    mappings[(s, se, t)] = ((t, te), score)\n            # Then we add the reverse mapping\n            if (t, s) not in exclude_auto:\n                if (t, te, s) in mappings:\n                    if mappings[(t, te, s)][1] < score:\n                        mappings[(t, te, s)] = ((s, se), score)\n                else:\n                    mappings[(t, te, s)] = ((s, se), score)\n    ontomap = []\n    for s, ts in mappings.items():\n        ontomap.append(((s[0], s[1]), ts[0], ts[1]))\n\n    # Now apply the Hume -> Eidos override\n    override_file = os.path.join(path_here, '../resources/wm_ontomap.bbn.tsv')\n    override_mappings = []\n    with open(override_file, 'r') as fh:\n        for row in fh.readlines():\n            if 'BBN' not in row:\n                continue\n            # Order is target first, source second\n            _, te, _, se = row.strip().split('\\t')\n            # Map the entries to our internal naming standards\n            s = 'HUME'\n            t = 'UN'\n            se = se.replace(' ', '_')\n            te = te.replace(' ', '_')\n            if se.startswith('/'):\n                se = se[1:]\n            override_mappings.append((s, se, t, te))\n    for s, se, t, te in override_mappings:\n        found = False\n        for idx, ((so, seo), (eo, teo), score) in enumerate(ontomap):\n            if (s, se, t) == (so, seo, eo):\n                # Override when a match is found\n                ontomap[idx] = ((s, se), (t, te), 1.0)\n                found = True\n        if not found:\n            ontomap.append(((s, se), (t, te), 1.0))\n    return ontomap", "response": "Load an ontology map for world models."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map_statements(self):\n        for stmt in self.statements:\n            for agent in stmt.agent_list():\n                if agent is None:\n                    continue\n                all_mappings = []\n                for db_name, db_id in agent.db_refs.items():\n                    if isinstance(db_id, list):\n                        db_id = db_id[0][0]\n                    mappings = self._map_id(db_name, db_id)\n                    all_mappings += mappings\n                for map_db_name, map_db_id, score, orig_db_name in all_mappings:\n                    if map_db_name in agent.db_refs:\n                        continue\n                    if self.scored:\n                        # If the original one is a scored grounding,\n                        # we take that score and multiply it with the mapping\n                        # score. Otherwise we assume the original score is 1.\n                        try:\n                            orig_score = agent.db_refs[orig_db_name][0][1]\n                        except Exception:\n                            orig_score = 1.0\n                        agent.db_refs[map_db_name] = \\\n                            [(map_db_id, score * orig_score)]\n                    else:\n                        if map_db_name in ('UN', 'HUME'):\n                            agent.db_refs[map_db_name] = [(map_db_id, 1.0)]\n                        else:\n                            agent.db_refs[map_db_name] = map_db_id", "response": "Run the ontology mapping on the statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_grounding_map(grounding_map_path, ignore_path=None,\n                       lineterminator='\\r\\n'):\n    \"\"\"Return a grounding map dictionary loaded from a csv file.\n\n    In the file pointed to by grounding_map_path, the number of name_space ID\n    pairs can vary per row and commas are\n    used to pad out entries containing fewer than the maximum amount of\n    name spaces appearing in the file. Lines should be terminated with \\r\\n\n    both a carriage return and a new line by default.\n\n    Optionally, one can specify another csv file (pointed to by ignore_path)\n    containing agent texts that are degenerate and should be filtered out.\n\n    Parameters\n    ----------\n    grounding_map_path : str\n        Path to csv file containing grounding map information. Rows of the file\n        should be of the form <agent_text>,<name_space_1>,<ID_1>,...\n        <name_space_n>,<ID_n>\n    ignore_path : Optional[str]\n        Path to csv file containing terms that should be filtered out during\n        the grounding mapping process. The file Should be of the form\n        <agent_text>,,..., where the number of commas that\n        appear is the same as in the csv file at grounding_map_path.\n        Default: None\n    lineterminator : Optional[str]\n        Line terminator used in input csv file. Default: \\r\\n\n\n    Returns\n    -------\n    g_map : dict\n        The grounding map constructed from the given files.\n    \"\"\"\n    g_map = {}\n    map_rows = read_unicode_csv(grounding_map_path, delimiter=',',\n                                quotechar='\"',\n                                quoting=csv.QUOTE_MINIMAL,\n                                lineterminator='\\r\\n')\n    if ignore_path and os.path.exists(ignore_path):\n        ignore_rows = read_unicode_csv(ignore_path, delimiter=',',\n                                       quotechar='\"',\n                                       quoting=csv.QUOTE_MINIMAL,\n                                       lineterminator=lineterminator)\n    else:\n        ignore_rows = []\n    csv_rows = chain(map_rows, ignore_rows)\n    for row in csv_rows:\n        key = row[0]\n        db_refs = {'TEXT': key}\n        keys = [entry for entry in row[1::2] if entry != '']\n        values = [entry for entry in row[2::2] if entry != '']\n        if len(keys) != len(values):\n            logger.info('ERROR: Mismatched keys and values in row %s' %\n                        str(row))\n            continue\n        else:\n            db_refs.update(dict(zip(keys, values)))\n            if len(db_refs.keys()) > 1:\n                g_map[key] = db_refs\n            else:\n                g_map[key] = None\n    return g_map", "response": "Load a grounding map from a csv file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all_agents(stmts):\n    agents = []\n    for stmt in stmts:\n        for agent in stmt.agent_list():\n            # Agents don't always have a TEXT db_refs entry (for instance\n            # in the case of Statements from databases) so we check for this.\n            if agent is not None and agent.db_refs.get('TEXT') is not None:\n                agents.append(agent)\n    return agents", "response": "Return a list of all of the agents in a list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of evidence sentences with a given agent text from a list of INDRA Statements.", "response": "def get_sentences_for_agent(text, stmts, max_sentences=None):\n    \"\"\"Returns evidence sentences with a given agent text from a list of statements\n\n    Parameters\n    ----------\n    text : str\n        An agent text\n\n    stmts : list of :py:class:`indra.statements.Statement`\n        INDRA Statements to search in for evidence statements.\n\n    max_sentences : Optional[int/None]\n        Cap on the number of evidence sentences to return. Default: None\n\n    Returns\n    -------\n    sentences : list of str\n        Evidence sentences from the list of statements containing\n        the given agent text.\n    \"\"\"\n    sentences = []\n    for stmt in stmts:\n        for agent in stmt.agent_list():\n            if agent is not None and agent.db_refs.get('TEXT') == text:\n                sentences.append((stmt.evidence[0].pmid,\n                                  stmt.evidence[0].text))\n                if max_sentences is not None and \\\n                   len(sentences) >= max_sentences:\n                    return sentences\n    return sentences"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef agent_texts_with_grounding(stmts):\n    allag = all_agents(stmts)\n    # Convert PFAM-DEF lists into tuples so that they are hashable and can\n    # be tabulated with a Counter\n    for ag in allag:\n        pfam_def = ag.db_refs.get('PFAM-DEF')\n        if pfam_def is not None:\n            ag.db_refs['PFAM-DEF'] = tuple(pfam_def)\n    refs = [tuple(ag.db_refs.items()) for ag in allag]\n    refs_counter = Counter(refs)\n    refs_counter_dict = [(dict(entry[0]), entry[1])\n                         for entry in refs_counter.items()]\n    # First, sort by text so that we can do a groupby\n    refs_counter_dict.sort(key=lambda x: x[0].get('TEXT'))\n\n    # Then group by text\n    grouped_by_text = []\n    for k, g in groupby(refs_counter_dict, key=lambda x: x[0].get('TEXT')):\n        # Total occurrences of this agent text\n        total = 0\n        entry = [k]\n        db_ref_list = []\n        for db_refs, count in g:\n            # Check if TEXT is our only key, indicating no grounding\n            if list(db_refs.keys()) == ['TEXT']:\n                db_ref_list.append((None, None, count))\n            # Add any other db_refs (not TEXT)\n            for db, db_id in db_refs.items():\n                if db == 'TEXT':\n                    continue\n                else:\n                    db_ref_list.append((db, db_id, count))\n            total += count\n        # Sort the db_ref_list by the occurrences of each grounding\n        entry.append(tuple(sorted(db_ref_list, key=lambda x: x[2],\n                     reverse=True)))\n        # Now add the total frequency to the entry\n        entry.append(total)\n        # And add the entry to the overall list\n        grouped_by_text.append(tuple(entry))\n    # Sort the list by the total number of occurrences of each unique key\n    grouped_by_text.sort(key=lambda x: x[2], reverse=True)\n    return grouped_by_text", "response": "Return a list of tuples of the form agent_text count where the count is the number of times an agent with the given agent_text appears grounded with the particular name space and ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all ungrounded entities ordered by number of mentions", "response": "def ungrounded_texts(stmts):\n    \"\"\"Return a list of all ungrounded entities ordered by number of mentions\n\n    Parameters\n    ----------\n    stmts : list of :py:class:`indra.statements.Statement`\n\n    Returns\n    -------\n    ungroundc : list of tuple\n       list of tuples of the form (text: str, count: int) sorted in descending\n       order by count.\n    \"\"\"\n    ungrounded = [ag.db_refs['TEXT']\n                  for s in stmts\n                  for ag in s.agent_list()\n                  if ag is not None and list(ag.db_refs.keys()) == ['TEXT']]\n    ungroundc = Counter(ungrounded)\n    ungroundc = ungroundc.items()\n    ungroundc = sorted(ungroundc, key=lambda x: x[1], reverse=True)\n    return ungroundc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all agents within a list of statements with a particular name.", "response": "def get_agents_with_name(name, stmts):\n    \"\"\"Return all agents within a list of statements with a particular name.\"\"\"\n    return [ag for stmt in stmts for ag in stmt.agent_list()\n            if ag is not None and ag.name == name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_base_map(filename, grouped_by_text):\n    rows = []\n    for group in grouped_by_text:\n        text_string = group[0]\n        for db, db_id, count in group[1]:\n            if db == 'UP':\n                name = uniprot_client.get_mnemonic(db_id)\n            else:\n                name = ''\n            row = [text_string, db, db_id, count, name]\n            rows.append(row)\n\n    write_unicode_csv(filename, rows, delimiter=',', quotechar='\"',\n                      quoting=csv.QUOTE_MINIMAL, lineterminator='\\r\\n')", "response": "Dump a list of agents along with groundings and counts into a csv file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef protein_map_from_twg(twg):\n\n    protein_map = {}\n    unmatched = 0\n    matched = 0\n    logger.info('Building grounding map for human proteins')\n    for agent_text, grounding_list, _ in twg:\n        # If 'UP' (Uniprot) not one of the grounding entries for this text,\n        # then we skip it.\n        if 'UP' not in [entry[0] for entry in grounding_list]:\n            continue\n        # Otherwise, collect all the Uniprot IDs for this protein.\n        uniprot_ids = [entry[1] for entry in grounding_list\n                       if entry[0] == 'UP']\n        # For each Uniprot ID, look up the species\n        for uniprot_id in uniprot_ids:\n            # If it's not a human protein, skip it\n            mnemonic = uniprot_client.get_mnemonic(uniprot_id)\n            if mnemonic is None or not mnemonic.endswith('_HUMAN'):\n                continue\n            # Otherwise, look up the gene name in HGNC and match against the\n            # agent text\n            gene_name = uniprot_client.get_gene_name(uniprot_id)\n            if gene_name is None:\n                unmatched += 1\n                continue\n            if agent_text.upper() == gene_name.upper():\n                matched += 1\n                protein_map[agent_text] = {'TEXT': agent_text,\n                                           'UP': uniprot_id}\n            else:\n                unmatched += 1\n    logger.info('Exact matches for %d proteins' % matched)\n    logger.info('No match (or no gene name) for %d proteins' % unmatched)\n    return protein_map", "response": "Build a grounding map from a list of statements containing the entity texts that have grounding to a human protein."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_sentences(twg, stmts, filename, agent_limit=300):\n    sentences = []\n    unmapped_texts = [t[0] for t in twg]\n    counter = 0\n    logger.info('Getting sentences for top %d unmapped agent texts.' %\n                agent_limit)\n    for text in unmapped_texts:\n        agent_sentences = get_sentences_for_agent(text, stmts)\n        sentences += map(lambda tup: (text,) + tup, agent_sentences)\n        counter += 1\n        if counter >= agent_limit:\n            break\n    # Write sentences to CSV file\n    write_unicode_csv(filename, sentences, delimiter=',', quotechar='\"',\n                      quoting=csv.QUOTE_MINIMAL, lineterminator='\\r\\n')", "response": "Write evidence sentences for ungrounded agents to a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_text_for_grounding(stmt, agent_text):\n    text = None\n    # First we will try to get content from the DB\n    try:\n        from indra_db.util.content_scripts \\\n            import get_text_content_from_text_refs\n        from indra.literature.deft_tools import universal_extract_text\n        refs = stmt.evidence[0].text_refs\n        # Prioritize the pmid attribute if given\n        if stmt.evidence[0].pmid:\n            refs['PMID'] = stmt.evidence[0].pmid\n        logger.info('Obtaining text for disambiguation with refs: %s' %\n                    refs)\n        content = get_text_content_from_text_refs(refs)\n        text = universal_extract_text(content, contains=agent_text)\n        if text:\n            return text\n    except Exception as e:\n        logger.info('Could not get text for disambiguation from DB.')\n    # If that doesn't work, we try PubMed next\n    if text is None:\n        from indra.literature import pubmed_client\n        pmid = stmt.evidence[0].pmid\n        if pmid:\n            logger.info('Obtaining abstract for disambiguation for PMID%s' %\n                        pmid)\n            text = pubmed_client.get_abstract(pmid)\n            if text:\n                return text\n    # Finally, falling back on the evidence sentence\n    if text is None:\n        logger.info('Falling back on sentence-based disambiguation')\n        text = stmt.evidence[0].text\n        return text\n    return None", "response": "Get the text for the given agent text for the given statement."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the db_refs of an agent using the grounding map.", "response": "def update_agent_db_refs(self, agent, agent_text, do_rename=True):\n        \"\"\"Update db_refs of agent using the grounding map\n\n        If the grounding map is missing one of the HGNC symbol or Uniprot ID,\n        attempts to reconstruct one from the other.\n\n        Parameters\n        ----------\n        agent : :py:class:`indra.statements.Agent`\n            The agent whose db_refs will be updated\n        agent_text : str\n            The agent_text to find a grounding for in the grounding map\n            dictionary. Typically this will be agent.db_refs['TEXT'] but\n            there may be situations where a different value should be used.\n        do_rename: Optional[bool]\n            If True, the Agent name is updated based on the mapped grounding.\n            If do_rename is True the priority for setting the name is\n            FamPlex ID, HGNC symbol, then the gene name\n            from Uniprot. Default: True\n\n        Raises\n        ------\n        ValueError\n            If the the grounding map contains and HGNC symbol for\n            agent_text but no HGNC ID can be found for it.\n        ValueError\n            If the grounding map contains both an HGNC symbol and a\n            Uniprot ID, but the HGNC symbol and the gene name associated with\n            the gene in Uniprot do not match or if there is no associated gene\n            name in Uniprot.\n        \"\"\"\n        map_db_refs = deepcopy(self.gm.get(agent_text))\n        self.standardize_agent_db_refs(agent, map_db_refs, do_rename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new Statement whose agents have been grounding mapped.", "response": "def map_agents_for_stmt(self, stmt, do_rename=True):\n        \"\"\"Return a new Statement whose agents have been grounding mapped.\n\n        Parameters\n        ----------\n        stmt : :py:class:`indra.statements.Statement`\n            The Statement whose agents need mapping.\n        do_rename: Optional[bool]\n            If True, the Agent name is updated based on the mapped grounding.\n            If do_rename is True the priority for setting the name is\n            FamPlex ID, HGNC symbol, then the gene name\n            from Uniprot. Default: True\n\n        Returns\n        -------\n        mapped_stmt : :py:class:`indra.statements.Statement`\n            The mapped Statement.\n        \"\"\"\n        mapped_stmt = deepcopy(stmt)\n        # Iterate over the agents\n\n        # Update agents directly participating in the statement\n        agent_list = mapped_stmt.agent_list()\n        for idx, agent in enumerate(agent_list):\n            if agent is None:\n                continue\n            agent_txt = agent.db_refs.get('TEXT')\n            if agent_txt is None:\n                continue\n\n            new_agent, maps_to_none = self.map_agent(agent, do_rename)\n\n            # Check if a deft model exists for agent text\n            if self.use_deft and agent_txt in deft_disambiguators:\n                try:\n                    run_deft_disambiguation(mapped_stmt, agent_list, idx,\n                                            new_agent, agent_txt)\n                except Exception as e:\n                    logger.error('There was an error during Deft'\n                                 ' disambiguation.')\n                    logger.error(e)\n\n            if maps_to_none:\n                # Skip the entire statement if the agent maps to None in the\n                # grounding map\n                return None\n\n            # If the old agent had bound conditions, but the new agent does\n            # not, copy the bound conditions over\n            if new_agent is not None and len(new_agent.bound_conditions) == 0:\n                new_agent.bound_conditions = agent.bound_conditions\n\n            agent_list[idx] = new_agent\n\n        mapped_stmt.set_agent_list(agent_list)\n\n        # Update agents in the bound conditions\n        for agent in agent_list:\n            if agent is not None:\n                for bc in agent.bound_conditions:\n                    bc.agent, maps_to_none = self.map_agent(bc.agent,\n                                                            do_rename)\n                    if maps_to_none:\n                        # Skip the entire statement if the agent maps to None\n                        # in the grounding map\n                        return None\n\n        return mapped_stmt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the given Agent with its grounding mapped.", "response": "def map_agent(self, agent, do_rename):\n        \"\"\"Return the given Agent with its grounding mapped.\n\n        This function grounds a single agent. It returns the new Agent object\n        (which might be a different object if we load a new agent state\n        from json) or the same object otherwise.\n\n        Parameters\n        ----------\n        agent : :py:class:`indra.statements.Agent`\n            The Agent to map.\n        do_rename: bool\n            If True, the Agent name is updated based on the mapped grounding.\n            If do_rename is True the priority for setting the name is\n            FamPlex ID, HGNC symbol, then the gene name\n            from Uniprot.\n\n        Returns\n        -------\n        grounded_agent : :py:class:`indra.statements.Agent`\n            The grounded Agent.\n        maps_to_none : bool\n            True if the Agent is in the grounding map and maps to None.\n        \"\"\"\n\n        agent_text = agent.db_refs.get('TEXT')\n        mapped_to_agent_json = self.agent_map.get(agent_text)\n        if mapped_to_agent_json:\n            mapped_to_agent = \\\n                Agent._from_json(mapped_to_agent_json['agent'])\n            return mapped_to_agent, False\n        # Look this string up in the grounding map\n        # If not in the map, leave agent alone and continue\n        if agent_text in self.gm.keys():\n            map_db_refs = self.gm[agent_text]\n        else:\n            return agent, False\n\n        # If it's in the map but it maps to None, then filter out\n        # this statement by skipping it\n        if map_db_refs is None:\n            # Increase counter if this statement has not already\n            # been skipped via another agent\n            logger.debug(\"Skipping %s\" % agent_text)\n            return None, True\n        # If it has a value that's not None, map it and add it\n        else:\n            # Otherwise, update the agent's db_refs field\n            self.update_agent_db_refs(agent, agent_text, do_rename)\n        return agent, False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_agents(self, stmts, do_rename=True):\n        # Make a copy of the stmts\n        mapped_stmts = []\n        num_skipped = 0\n        # Iterate over the statements\n        for stmt in stmts:\n            mapped_stmt = self.map_agents_for_stmt(stmt, do_rename)\n            # Check if we should skip the statement\n            if mapped_stmt is not None:\n                mapped_stmts.append(mapped_stmt)\n            else:\n                num_skipped += 1\n        logger.info('%s statements filtered out' % num_skipped)\n        return mapped_stmts", "response": "Returns a list of statements whose agents have been mapped to the Uniprot names."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rename_agents(self, stmts):\n        # Make a copy of the stmts\n        mapped_stmts = deepcopy(stmts)\n        # Iterate over the statements\n        for _, stmt in enumerate(mapped_stmts):\n            # Iterate over the agents\n            for agent in stmt.agent_list():\n                if agent is None:\n                    continue\n                # If there's a FamPlex ID, prefer that for the name\n                if agent.db_refs.get('FPLX'):\n                    agent.name = agent.db_refs.get('FPLX')\n                # Take a HGNC name from Uniprot next\n                elif agent.db_refs.get('UP'):\n                    # Try for the gene name\n                    gene_name = uniprot_client.get_gene_name(\n                                                    agent.db_refs.get('UP'),\n                                                    web_fallback=False)\n                    if gene_name:\n                        agent.name = gene_name\n                        hgnc_id = hgnc_client.get_hgnc_id(gene_name)\n                        if hgnc_id:\n                            agent.db_refs['HGNC'] = hgnc_id\n                    # Take the text string\n                    #if agent.db_refs.get('TEXT'):\n                    #    agent.name = agent.db_refs.get('TEXT')\n                    # If this fails, then we continue with no change\n                # Fall back to the text string\n                #elif agent.db_refs.get('TEXT'):\n                #    agent.name = agent.db_refs.get('TEXT')\n        return mapped_stmts", "response": "Returns a list of statements with updated Agent names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_complexes(self, cplx_df):\n        # Group the agents for the complex\n        logger.info('Processing complexes...')\n        for cplx_id, this_cplx in cplx_df.groupby('CPLX_ID'):\n            agents = []\n            for hprd_id in this_cplx.HPRD_ID:\n                ag = self._make_agent(hprd_id)\n                if ag is not None:\n                    agents.append(ag)\n            # Make sure we got some agents!\n            if not agents:\n                continue\n            # Get evidence info from first member of complex\n            row0 = this_cplx.iloc[0]\n            isoform_id = '%s_1' % row0.HPRD_ID\n            ev_list = self._get_evidence(row0.HPRD_ID, isoform_id, row0.PMIDS,\n                                         row0.EVIDENCE, 'interactions')\n            stmt = Complex(agents, evidence=ev_list)\n            self.statements.append(stmt)", "response": "Generate Complex Statements from the HPRD protein complexes data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating Modification statements from the HPRD PTM data.", "response": "def get_ptms(self, ptm_df):\n        \"\"\"Generate Modification statements from the HPRD PTM data.\n\n        Parameters\n        ----------\n        ptm_df : pandas.DataFrame\n            DataFrame loaded from the POST_TRANSLATIONAL_MODIFICATIONS.txt file.\n        \"\"\"\n        logger.info('Processing PTMs...')\n        # Iterate over the rows of the dataframe\n        for ix, row in ptm_df.iterrows():\n            # Check the modification type; if we can't make an INDRA statement\n            # for it, then skip it\n            ptm_class = _ptm_map[row['MOD_TYPE']]\n            if ptm_class is None:\n                continue\n            # Use the Refseq protein ID for the substrate to make sure that\n            # we get the right Uniprot ID for the isoform\n            sub_ag = self._make_agent(row['HPRD_ID'],\n                                      refseq_id=row['REFSEQ_PROTEIN'])\n\n            # If we couldn't get the substrate, skip the statement\n            if sub_ag is None:\n                continue\n            enz_id = _nan_to_none(row['ENZ_HPRD_ID'])\n            enz_ag = self._make_agent(enz_id)\n            res = _nan_to_none(row['RESIDUE'])\n            pos = _nan_to_none(row['POSITION'])\n            if pos is not None and ';' in pos:\n                pos, dash = pos.split(';')\n                assert dash == '-'\n            # As a fallback for later site mapping, we also get the protein\n            # sequence information in case there was a problem with the\n            # RefSeq->Uniprot mapping\n            assert res\n            assert pos\n            motif_dict = self._get_seq_motif(row['REFSEQ_PROTEIN'], res, pos)\n            # Get evidence\n            ev_list = self._get_evidence(\n                    row['HPRD_ID'], row['HPRD_ISOFORM'], row['PMIDS'],\n                    row['EVIDENCE'], 'ptms', motif_dict)\n            stmt = ptm_class(enz_ag, sub_ag, res, pos, evidence=ev_list)\n            self.statements.append(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating Complex Statements from the HPRD PPI data.", "response": "def get_ppis(self, ppi_df):\n        \"\"\"Generate Complex Statements from the HPRD PPI data.\n\n        Parameters\n        ----------\n        ppi_df : pandas.DataFrame\n            DataFrame loaded from the BINARY_PROTEIN_PROTEIN_INTERACTIONS.txt\n            file.\n        \"\"\"\n        logger.info('Processing PPIs...')\n        for ix, row in ppi_df.iterrows():\n            agA = self._make_agent(row['HPRD_ID_A'])\n            agB = self._make_agent(row['HPRD_ID_B'])\n            # If don't get valid agents for both, skip this PPI\n            if agA is None or agB is None:\n                continue\n            isoform_id = '%s_1' % row['HPRD_ID_A']\n            ev_list = self._get_evidence(\n                    row['HPRD_ID_A'], isoform_id, row['PMIDS'],\n                    row['EVIDENCE'], 'interactions')\n            stmt = Complex([agA, agB], evidence=ev_list)\n            self.statements.append(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_verb_statement_mapping():\n    path_this = os.path.dirname(os.path.abspath(__file__))\n    map_path = os.path.join(path_this, 'isi_verb_to_indra_statement_type.tsv')\n    with open(map_path, 'r') as f:\n        first_line = True\n        verb_to_statement_type = {}\n        for line in f:\n            if not first_line:\n                line = line[:-1]\n                tokens = line.split('\\t')\n\n                if len(tokens) == 2 and len(tokens[1]) > 0:\n                    verb = tokens[0]\n                    s_type = tokens[1]\n                    try:\n                        statement_class = getattr(ist, s_type)\n                        verb_to_statement_type[verb] = statement_class\n                    except Exception:\n                        pass\n            else:\n                first_line = False\n    return verb_to_statement_type", "response": "Build the mapping between ISI verb strings and INDRA statement classes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_statements(self):\n        for k, v in self.reader_output.items():\n            for interaction in v['interactions']:\n                self._process_interaction(k, interaction, v['text'], self.pmid,\n                                          self.extra_annotations)", "response": "Process reader output to produce INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess an interaction JSON tuple from the ISI output and add it to the list of extracted statements.", "response": "def _process_interaction(self, source_id, interaction, text, pmid,\n                             extra_annotations):\n        \"\"\"Process an interaction JSON tuple from the ISI output, and adds up\n        to one statement to the list of extracted statements.\n\n        Parameters\n        ----------\n        source_id : str\n            the JSON key corresponding to the sentence in the ISI output\n            interaction: the JSON list with subject/verb/object information\n            about the event in the ISI output\n        text : str\n            the text of the sentence\n        pmid : str\n            the PMID of the article from which the information was extracted\n        extra_annotations : dict\n            Additional annotations to add to the statement's evidence,\n            potentially containing metadata about the source. Annotations\n            with the key \"interaction\" will be overridden by the JSON\n            interaction tuple from the ISI output\n        \"\"\"\n        verb = interaction[0].lower()\n        subj = interaction[-2]\n        obj = interaction[-1]\n\n        # Make ungrounded agent objects for the subject and object\n        # Grounding will happen after all statements are extracted in __init__\n        subj = self._make_agent(subj)\n        obj = self._make_agent(obj)\n\n        # Make an evidence object\n        annotations = deepcopy(extra_annotations)\n        if 'interaction' in extra_annotations:\n            logger.warning(\"'interaction' key of extra_annotations ignored\" +\n                           \" since this is reserved for storing the raw ISI \" +\n                           \"input.\")\n        annotations['source_id'] = source_id\n        annotations['interaction'] = interaction\n        ev = ist.Evidence(source_api='isi',\n                          pmid=pmid,\n                          text=text.rstrip(),\n                          annotations=annotations)\n\n        # For binding time interactions, it is said that a catayst might be\n        # specified. We don't use this for now, but extract in case we want\n        # to in the future\n        cataylst_specified = False\n        if len(interaction) == 4:\n            catalyst = interaction[1]\n            if catalyst is not None:\n                cataylst_specified = True\n        self.verbs.add(verb)\n\n        statement = None\n        if verb in verb_to_statement_type:\n            statement_class = verb_to_statement_type[verb]\n\n            if statement_class == ist.Complex:\n                statement = ist.Complex([subj, obj], evidence=ev)\n            else:\n                statement = statement_class(subj, obj, evidence=ev)\n\n        if statement is not None:\n            # For Complex statements, the ISI reader produces two events:\n            # binds(A, B) and binds(B, A)\n            # We want only one Complex statement for each sentence, so check\n            # to see if we already have a Complex for this source_id with the\n            # same members\n            already_have = False\n            if type(statement) == ist.Complex:\n                for old_s in self.statements:\n                    old_id = statement.evidence[0].source_id\n                    new_id = old_s.evidence[0].source_id\n                    if type(old_s) == ist.Complex and old_id == new_id:\n                        old_statement_members = \\\n                                [m.db_refs['TEXT'] for m in old_s.members]\n                        old_statement_members = sorted(old_statement_members)\n\n                        new_statement_members = [m.db_refs['TEXT']\n                                                 for m in statement.members]\n                        new_statement_members = sorted(new_statement_members)\n\n                        if old_statement_members == new_statement_members:\n                            already_have = True\n                            break\n\n            if not already_have:\n                self.statements.append(statement)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_annotation(self):\n        annotation = dict()\n\n        # Put all properties of the action object into the annotation\n        for item in dir(self):\n            if len(item) > 0 and item[0] != '_' and \\\n                    not inspect.ismethod(getattr(self, item)):\n                annotation[item] = getattr(self, item)\n\n        return annotation", "response": "Returns a dictionary with all properties of the action mention."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an array consisting of the elements obtained from a pattern search cast into their appropriate classes.", "response": "def _match_to_array(m):\n    \"\"\" Returns an array consisting of the elements obtained from a pattern\n    search cast into their appropriate classes. \"\"\"\n    return [_cast_biopax_element(m.get(i)) for i in range(m.varSize())]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _is_complex(pe):\n    val = isinstance(pe, _bp('Complex')) or \\\n            isinstance(pe, _bpimpl('Complex'))\n    return val", "response": "Return True if the physical entity is a complex"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_protein(pe):\n    val = isinstance(pe, _bp('Protein')) or \\\n            isinstance(pe, _bpimpl('Protein')) or \\\n            isinstance(pe, _bp('ProteinReference')) or \\\n            isinstance(pe, _bpimpl('ProteinReference'))\n    return val", "response": "Return True if the element is a protein"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the element is an RNA", "response": "def _is_rna(pe):\n    \"\"\"Return True if the element is an RNA\"\"\"\n    val = isinstance(pe, _bp('Rna')) or isinstance(pe, _bpimpl('Rna'))\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_small_molecule(pe):\n    val = isinstance(pe, _bp('SmallMolecule')) or \\\n            isinstance(pe, _bpimpl('SmallMolecule')) or \\\n            isinstance(pe, _bp('SmallMoleculeReference')) or \\\n            isinstance(pe, _bpimpl('SmallMoleculeReference'))\n    return val", "response": "Return True if the element is a small molecule"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the element is a physical entity.", "response": "def _is_physical_entity(pe):\n    \"\"\"Return True if the element is a physical entity\"\"\"\n    val = isinstance(pe, _bp('PhysicalEntity')) or \\\n            isinstance(pe, _bpimpl('PhysicalEntity'))\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_modification_or_activity(feature):\n    if not (isinstance(feature, _bp('ModificationFeature')) or \\\n            isinstance(feature, _bpimpl('ModificationFeature'))):\n        return None\n    mf_type = feature.getModificationType()\n    if mf_type is None:\n        return None\n    mf_type_terms = mf_type.getTerm().toArray()\n    for term in mf_type_terms:\n        if term in ('residue modification, active',\n                    'residue modification, inactive',\n                    'active', 'inactive'):\n            return 'activity'\n    return 'modification'", "response": "Return True if the feature is a modification or activity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the element is an entity reference.", "response": "def _is_reference(bpe):\n    \"\"\"Return True if the element is an entity reference.\"\"\"\n    if isinstance(bpe, _bp('ProteinReference')) or \\\n        isinstance(bpe, _bpimpl('ProteinReference')) or \\\n        isinstance(bpe, _bp('SmallMoleculeReference')) or \\\n        isinstance(bpe, _bpimpl('SmallMoleculeReference')) or \\\n        isinstance(bpe, _bp('RnaReference')) or \\\n        isinstance(bpe, _bpimpl('RnaReference')) or \\\n        isinstance(bpe, _bp('EntityReference')) or \\\n        isinstance(bpe, _bpimpl('EntityReference')):\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_entity(bpe):\n    if isinstance(bpe, _bp('Protein')) or \\\n        isinstance(bpe, _bpimpl('Protein')) or \\\n        isinstance(bpe, _bp('SmallMolecule')) or \\\n        isinstance(bpe, _bpimpl('SmallMolecule')) or \\\n        isinstance(bpe, _bp('Complex')) or \\\n        isinstance(bpe, _bpimpl('Complex')) or \\\n        isinstance(bpe, _bp('Rna')) or \\\n        isinstance(bpe, _bpimpl('Rna')) or \\\n        isinstance(bpe, _bp('RnaRegion')) or \\\n        isinstance(bpe, _bpimpl('RnaRegion')) or \\\n        isinstance(bpe, _bp('DnaRegion')) or \\\n        isinstance(bpe, _bpimpl('DnaRegion')) or \\\n        isinstance(bpe, _bp('PhysicalEntity')) or \\\n        isinstance(bpe, _bpimpl('PhysicalEntity')):\n        return True\n    else:\n        return False", "response": "Return True if the element is a physical entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_catalysis(bpe):\n    if isinstance(bpe, _bp('Catalysis')) or \\\n        isinstance(bpe, _bpimpl('Catalysis')):\n        return True\n    else:\n        return False", "response": "Return True if the element is Catalysis."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints all INDRA Statements collected by the processors.", "response": "def print_statements(self):\n        \"\"\"Print all INDRA Statements collected by the processors.\"\"\"\n        for i, stmt in enumerate(self.statements):\n            print(\"%s: %s\" % (i, stmt))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the BioPAX model object in an OWL file.", "response": "def save_model(self, file_name=None):\n        \"\"\"Save the BioPAX model object in an OWL file.\n\n        Parameters\n        ----------\n        file_name : Optional[str]\n            The name of the OWL file to save the model in.\n        \"\"\"\n        if file_name is None:\n            logger.error('Missing file name')\n            return\n        pcc.model_to_owl(self.model, file_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef eliminate_exact_duplicates(self):\n        # Here we use the deep hash of each Statement, and by making a dict,\n        # we effectively keep only one Statement with a given deep hash\n        self.statements = list({stmt.get_hash(shallow=False, refresh=True): stmt\n                                for stmt in self.statements}.values())", "response": "Eliminate Statements that were extracted multiple times."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_complexes(self):\n        for obj in self.model.getObjects().toArray():\n            bpe = _cast_biopax_element(obj)\n            if not _is_complex(bpe):\n                continue\n            ev = self._get_evidence(bpe)\n\n            members = self._get_complex_members(bpe)\n            if members is not None:\n                if len(members) > 10:\n                    logger.debug('Skipping complex with more than 10 members.')\n                    continue\n                complexes = _get_combinations(members)\n                for c in complexes:\n                    self.statements.append(decode_obj(Complex(c, ev),\n                                                      encoding='utf-8'))", "response": "Extract INDRA Complex Statements from the BioPAX model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_modifications(self):\n        for modtype, modclass in modtype_to_modclass.items():\n            # TODO: we could possibly try to also extract generic\n            # modifications here\n            if modtype == 'modification':\n                continue\n            stmts = self._get_generic_modification(modclass)\n            self.statements += stmts", "response": "Extract INDRA Modification Statements from the BioPAX model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_activity_modification(self):\n        mod_filter = 'residue modification, active'\n        for is_active in [True, False]:\n            p = self._construct_modification_pattern()\n            rel = mcct.GAIN if is_active else mcct.LOSS\n            p.add(mcc(rel, mod_filter),\n                  \"input simple PE\", \"output simple PE\")\n\n            s = _bpp('Searcher')\n            res = s.searchPlain(self.model, p)\n            res_array = [_match_to_array(m) for m in res.toArray()]\n\n            for r in res_array:\n                reaction = r[p.indexOf('Conversion')]\n                activity = 'activity'\n                input_spe = r[p.indexOf('input simple PE')]\n                output_spe = r[p.indexOf('output simple PE')]\n\n                # Get the modifications\n                mod_in = \\\n                    BiopaxProcessor._get_entity_mods(input_spe)\n                mod_out = \\\n                    BiopaxProcessor._get_entity_mods(output_spe)\n\n                mod_shared = _get_mod_intersection(mod_in, mod_out)\n                gained_mods = _get_mod_difference(mod_out, mod_in)\n\n                # Here we get the evidence for the BiochemicalReaction\n                ev = self._get_evidence(reaction)\n\n                agents = self._get_agents_from_entity(output_spe)\n                for agent in _listify(agents):\n                    static_mods = _get_mod_difference(agent.mods,\n                                                      gained_mods)\n                    # NOTE: with the ActiveForm representation we cannot\n                    # separate static_mods and gained_mods. We assume here\n                    # that the static_mods are inconsequential and therefore\n                    # are not mentioned as an Agent condition, following\n                    # don't care don't write semantics. Therefore only the\n                    # gained_mods are listed in the ActiveForm as Agent\n                    # conditions.\n                    if gained_mods:\n                        agent.mods = gained_mods\n                        stmt = ActiveForm(agent, activity, is_active,\n                                          evidence=ev)\n                        self.statements.append(decode_obj(stmt,\n                                                          encoding='utf-8'))", "response": "Extract INDRA ActiveForm Statements that are due to a specific modification state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Activation and Inhibition INDRA Statements from the model and reuses the corresponding INDRA statements.", "response": "def get_regulate_activities(self):\n        \"\"\"Get Activation/Inhibition INDRA Statements from the BioPAX model.\n\n        This method extracts Activation/Inhibition Statements and reuses the\n        structure of BioPAX Pattern's\n        org.biopax.paxtools.pattern.PatternBox.constrolsStateChange pattern\n        with additional constraints to specify the gain or loss of\n        activity state but assuring that the activity change is not due to\n        a modification state change (which are extracted by get_modifications\n        and get_activity_modification).\n        \"\"\"\n        mcc = _bpp('constraint.ModificationChangeConstraint')\n        mcct = _bpp('constraint.ModificationChangeConstraint$Type')\n        mod_filter = 'residue modification, active'\n        # Start with a generic modification pattern\n        p = BiopaxProcessor._construct_modification_pattern()\n        stmts = []\n        for act_class, gain_loss in zip([Activation, Inhibition],\n                                        [mcct.GAIN, mcct.LOSS]):\n            p.add(mcc(gain_loss, mod_filter),\n                      \"input simple PE\", \"output simple PE\")\n            s = _bpp('Searcher')\n            res = s.searchPlain(self.model, p)\n            res_array = [_match_to_array(m) for m in res.toArray()]\n            for r in res_array:\n                controller_pe = r[p.indexOf('controller PE')]\n                input_pe = r[p.indexOf('input PE')]\n                input_spe = r[p.indexOf('input simple PE')]\n                output_spe = r[p.indexOf('output simple PE')]\n                reaction = r[p.indexOf('Conversion')]\n                control = r[p.indexOf('Control')]\n\n                if not _is_catalysis(control):\n                    continue\n                cat_dir = control.getCatalysisDirection()\n                if cat_dir is not None and cat_dir.name() != 'LEFT_TO_RIGHT':\n                    logger.debug('Unexpected catalysis direction: %s.' % \\\n                        control.getCatalysisDirection())\n                    continue\n\n                subjs = BiopaxProcessor._get_primary_controller(controller_pe)\n                if not subjs:\n                    continue\n\n                '''\n                if _is_complex(input_pe):\n                    # TODO: It is possible to find which member of the complex\n                    # is actually activated. That member will be the substrate\n                    # and all other members of the complex will be bound to it.\n                    logger.info('Cannot handle complex subjects.')\n                    continue\n                '''\n                objs = BiopaxProcessor._get_agents_from_entity(input_spe,\n                                                               expand_pe=False)\n\n                ev = self._get_evidence(control)\n                for subj, obj in itertools.product(_listify(subjs),\n                                                   _listify(objs)):\n                    # Get the modifications\n                    mod_in = \\\n                        BiopaxProcessor._get_entity_mods(input_spe)\n                    mod_out = \\\n                        BiopaxProcessor._get_entity_mods(output_spe)\n\n                    # We assume if modifications change then this is not really\n                    # a pure activation event\n                    gained_mods = _get_mod_difference(mod_out, mod_in)\n                    lost_mods = _get_mod_difference(mod_in, mod_out)\n                    if gained_mods or lost_mods:\n                        continue\n\n                    stmt = act_class(subj, obj, 'activity', evidence=ev)\n                    self.statements.append(decode_obj(stmt, encoding='utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_regulate_amounts(self):\n        p = pb.controlsExpressionWithTemplateReac()\n        s = _bpp('Searcher')\n        res = s.searchPlain(self.model, p)\n        res_array = [_match_to_array(m) for m in res.toArray()]\n        stmts = []\n        for res in res_array:\n            # FIXME: for some reason labels are not accessible\n            # for these queries. It would be more reliable\n            # to get results by label instead of index.\n            '''\n            controller_er = res[p.indexOf('controller ER')]\n            generic_controller_er = res[p.indexOf('generic controller ER')]\n            controller_simple_pe = res[p.indexOf('controller simple PE')]\n            controller_pe = res[p.indexOf('controller PE')]\n            control = res[p.indexOf('Control')]\n            conversion = res[p.indexOf('Conversion')]\n            input_pe = res[p.indexOf('input PE')]\n            input_simple_pe = res[p.indexOf('input simple PE')]\n            changed_generic_er = res[p.indexOf('changed generic ER')]\n            output_pe = res[p.indexOf('output PE')]\n            output_simple_pe = res[p.indexOf('output simple PE')]\n            changed_er = res[p.indexOf('changed ER')]\n            '''\n            # TODO: here, res[3] is the complex physical entity\n            # for instance http://pathwaycommons.org/pc2/\n            # Complex_43c6b8330562c1b411d21e9d1185bae9\n            # consists of 3 components: JUN, FOS and NFAT\n            # where NFAT further contains 3 member physical entities.\n            #\n            # However, res[2] iterates over all 5 member physical entities\n            # of the complex which doesn't represent the underlying\n            # structure faithfully. It would be better to use res[3]\n            # (the complex itself) and look at components and then\n            # members. However, then, it would not be clear how to\n            # construct an INDRA Agent for the controller.\n            controller = self._get_agents_from_entity(res[2])\n            controlled_pe = res[6]\n            controlled = self._get_agents_from_entity(controlled_pe)\n\n            conversion = res[5]\n            direction = conversion.getTemplateDirection()\n            if direction is not None:\n                direction = direction.name()\n                if direction != 'FORWARD':\n                    logger.warning('Unhandled conversion direction %s' %\n                                   direction)\n                    continue\n            # Sometimes interaction type is annotated as\n            # term=='TRANSCRIPTION'. Other times this is not\n            # annotated.\n            int_type = conversion.getInteractionType().toArray()\n            if int_type:\n                for it in int_type:\n                    for term in it.getTerm().toArray():\n                        pass\n            control = res[4]\n            control_type = control.getControlType()\n            if control_type:\n                control_type = control_type.name()\n            ev = self._get_evidence(control)\n            for subj, obj in itertools.product(_listify(controller),\n                                               _listify(controlled)):\n                subj_act = ActivityCondition('transcription', True)\n                subj.activity = subj_act\n                if control_type == 'ACTIVATION':\n                    st = IncreaseAmount(subj, obj, evidence=ev)\n                elif control_type == 'INHIBITION':\n                    st = DecreaseAmount(subj, obj, evidence=ev)\n                else:\n                    logger.warning('Unhandled control type %s' % control_type)\n                    continue\n                st_dec = decode_obj(st, encoding='utf-8')\n                self.statements.append(st_dec)", "response": "Extract INDRA RegulateAmount Statements from the BioPAX model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_conversions(self):\n        # NOTE: This pattern gets all reactions in which a protein is the\n        # controller and chemicals are converted. But with this pattern only\n        # a single chemical is extracted from each side. This can be misleading\n        # since we want to capture all inputs and all outputs of the\n        # conversion. So we need to step back to the conversion itself and\n        # enumerate all inputs/outputs, make sure they constitute the kind\n        # of conversion we can capture here and then extract as a Conversion\n        # Statement. Another issue here is that the same reaction will be\n        # extracted multiple times if there is more then one input or output.\n        # Therefore we need to cache the ID of the reactions that have already\n        # been handled.\n        p = _bpp('Pattern')(_bpimpl('PhysicalEntity')().getModelInterface(),\n                            'controller PE')\n        # Getting the control itself\n        p.add(cb.peToControl(), \"controller PE\", \"Control\")\n        # Make sure the controller is a protein\n        # TODO: possibly allow Complex too\n        p.add(tp(_bpimpl('Protein')().getModelInterface()), \"controller PE\")\n        # Link the control to the conversion that it controls\n        p.add(cb.controlToConv(), \"Control\", \"Conversion\")\n        # Make sure this is a BiochemicalRection (as opposed to, for instance,\n        # ComplexAssembly)\n        p.add(tp(_bpimpl('BiochemicalReaction')().getModelInterface()),\n                         \"Conversion\")\n        # The controller shouldn't be a participant of the conversion\n        p.add(_bpp('constraint.NOT')(cb.participant()),\n              \"Conversion\", \"controller PE\")\n        # Get the input participant of the conversion\n        p.add(pt(rt.INPUT, True), \"Control\", \"Conversion\", \"input PE\")\n        # Link to the other side of the conversion\n        p.add(cs(cst.OTHER_SIDE), \"input PE\", \"Conversion\", \"output PE\")\n        # Make sure the two sides are not the same\n        p.add(_bpp('constraint.Equality')(False), \"input PE\", \"output PE\")\n        # Make sure the input/output is a chemical\n        p.add(tp(_bpimpl('SmallMolecule')().getModelInterface()), \"input PE\")\n        p.add(tp(_bpimpl('SmallMolecule')().getModelInterface()), \"output PE\")\n\n        s = _bpp('Searcher')\n        res = s.searchPlain(self.model, p)\n        res_array = [_match_to_array(m) for m in res.toArray()]\n        stmts = []\n        reaction_extracted = set()\n        for r in res_array:\n            controller_pe = r[p.indexOf('controller PE')]\n            reaction = r[p.indexOf('Conversion')]\n            control = r[p.indexOf('Control')]\n            input_pe = r[p.indexOf('input PE')]\n            output_pe = r[p.indexOf('output PE')]\n            if control.getUri() in reaction_extracted:\n                continue\n            # Get controller\n            subj_list = self._get_agents_from_entity(controller_pe)\n            # Get inputs and outputs\n            left = reaction.getLeft().toArray()\n            right = reaction.getRight().toArray()\n            # Skip this if not all participants are chemicals\n            if any([not _is_small_molecule(e) for e in left]):\n                continue\n            if any([not _is_small_molecule(e) for e in right]):\n                continue\n            obj_left = []\n            obj_right = []\n            for participant in left:\n                agent = self._get_agents_from_entity(participant)\n                if isinstance(agent, list):\n                    obj_left += agent\n                else:\n                    obj_left.append(agent)\n            for participant in right:\n                agent = self._get_agents_from_entity(participant)\n                if isinstance(agent, list):\n                    obj_right += agent\n                else:\n                    obj_right.append(agent)\n            ev = self._get_evidence(control)\n            for subj in _listify(subj_list):\n                st = Conversion(subj, obj_left, obj_right, evidence=ev)\n                st_dec = decode_obj(st, encoding='utf-8')\n                self.statements.append(st_dec)\n            reaction_extracted.add(control.getUri())", "response": "Extract Conversion INDRA Statements from the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract Gef INDRA Statements from the model.", "response": "def get_gef(self):\n        \"\"\"Extract Gef INDRA Statements from the BioPAX model.\n\n        This method uses a custom BioPAX Pattern\n        (one that is not implemented PatternBox) to query for controlled\n        BiochemicalReactions in which the same protein is in complex with\n        GDP on the left hand side and in complex with GTP on the\n        right hand side. This implies that the controller is a GEF for the\n        GDP/GTP-bound protein.\n        \"\"\"\n        p = self._gef_gap_base()\n        s = _bpp('Searcher')\n        res = s.searchPlain(self.model, p)\n        res_array = [_match_to_array(m) for m in res.toArray()]\n        for r in res_array:\n            controller_pe = r[p.indexOf('controller PE')]\n            input_pe = r[p.indexOf('input PE')]\n            input_spe = r[p.indexOf('input simple PE')]\n            output_pe = r[p.indexOf('output PE')]\n            output_spe = r[p.indexOf('output simple PE')]\n            reaction = r[p.indexOf('Conversion')]\n            control = r[p.indexOf('Control')]\n\n            # Make sure the GEF is not a complex\n            # TODO: it could be possible to extract certain complexes here, for\n            # instance ones that only have a single protein\n            if _is_complex(controller_pe):\n                continue\n\n            members_in = self._get_complex_members(input_pe)\n            members_out = self._get_complex_members(output_pe)\n            if not (members_in and members_out):\n                continue\n            # Make sure the outgoing complex has exactly 2 members\n            # TODO: by finding matching proteins on either side, in principle\n            # it would be possible to find Gef relationships in complexes\n            # with more members\n            if len(members_out) != 2:\n                continue\n            # Make sure complex starts with GDP that becomes GTP\n            gdp_in = False\n            for member in members_in:\n                if isinstance(member, Agent) and member.name == 'GDP':\n                    gdp_in = True\n            gtp_out = False\n            for member in members_out:\n                if isinstance(member, Agent) and member.name == 'GTP':\n                    gtp_out = True\n            if not (gdp_in and gtp_out):\n                continue\n            ras_list = self._get_agents_from_entity(input_spe)\n            gef_list = self._get_agents_from_entity(controller_pe)\n            ev = self._get_evidence(control)\n            for gef, ras in itertools.product(_listify(gef_list),\n                                               _listify(ras_list)):\n                st = Gef(gef, ras, evidence=ev)\n                st_dec = decode_obj(st, encoding='utf-8')\n                self.statements.append(st_dec)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_gap(self):\n        p = self._gef_gap_base()\n        s = _bpp('Searcher')\n        res = s.searchPlain(self.model, p)\n        res_array = [_match_to_array(m) for m in res.toArray()]\n        for r in res_array:\n            controller_pe = r[p.indexOf('controller PE')]\n            input_pe = r[p.indexOf('input PE')]\n            input_spe = r[p.indexOf('input simple PE')]\n            output_pe = r[p.indexOf('output PE')]\n            output_spe = r[p.indexOf('output simple PE')]\n            reaction = r[p.indexOf('Conversion')]\n            control = r[p.indexOf('Control')]\n\n            # Make sure the GAP is not a complex\n            # TODO: it could be possible to extract certain complexes here, for\n            # instance ones that only have a single protein\n            if _is_complex(controller_pe):\n                continue\n\n            members_in = self._get_complex_members(input_pe)\n            members_out = self._get_complex_members(output_pe)\n            if not (members_in and members_out):\n                continue\n            # Make sure the outgoing complex has exactly 2 members\n            # TODO: by finding matching proteins on either side, in principle\n            # it would be possible to find Gap relationships in complexes\n            # with more members\n            if len(members_out) != 2:\n                continue\n            # Make sure complex starts with GDP that becomes GTP\n            gtp_in = False\n            for member in members_in:\n                if isinstance(member, Agent) and member.name == 'GTP':\n                    gtp_in = True\n            gdp_out = False\n            for member in members_out:\n                if isinstance(member, Agent) and member.name == 'GDP':\n                    gdp_out = True\n            if not (gtp_in and gdp_out):\n                continue\n            ras_list = self._get_agents_from_entity(input_spe)\n            gap_list = self._get_agents_from_entity(controller_pe)\n            ev = self._get_evidence(control)\n            for gap, ras in itertools.product(_listify(gap_list),\n                                               _listify(ras_list)):\n                st = Gap(gap, ras, evidence=ev)\n                st_dec = decode_obj(st, encoding='utf-8')\n                self.statements.append(st_dec)", "response": "Extract Gap INDRA Statements from the model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_entity_mods(bpe):\n        if _is_entity(bpe):\n            features = bpe.getFeature().toArray()\n        else:\n            features = bpe.getEntityFeature().toArray()\n        mods = []\n        for feature in features:\n            if not _is_modification(feature):\n                continue\n            mc = BiopaxProcessor._extract_mod_from_feature(feature)\n            if mc is not None:\n                mods.append(mc)\n        return mods", "response": "Get all the modifications of an entity in INDRA format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_generic_modification(self, mod_class):\n        mod_type = modclass_to_modtype[mod_class]\n        if issubclass(mod_class, RemoveModification):\n            mod_gain_const = mcct.LOSS\n            mod_type = modtype_to_inverse[mod_type]\n        else:\n            mod_gain_const = mcct.GAIN\n        mod_filter = mod_type[:5]\n        # Start with a generic modification pattern\n        p = BiopaxProcessor._construct_modification_pattern()\n        p.add(mcc(mod_gain_const, mod_filter),\n                  \"input simple PE\", \"output simple PE\")\n        s = _bpp('Searcher')\n        res = s.searchPlain(self.model, p)\n        res_array = [_match_to_array(m) for m in res.toArray()]\n        stmts = []\n        for r in res_array:\n            controller_pe = r[p.indexOf('controller PE')]\n            input_pe = r[p.indexOf('input PE')]\n            input_spe = r[p.indexOf('input simple PE')]\n            output_spe = r[p.indexOf('output simple PE')]\n            reaction = r[p.indexOf('Conversion')]\n            control = r[p.indexOf('Control')]\n\n            if not _is_catalysis(control):\n                continue\n            cat_dir = control.getCatalysisDirection()\n            if cat_dir is not None and cat_dir.name() != 'LEFT_TO_RIGHT':\n                logger.debug('Unexpected catalysis direction: %s.' % \\\n                    control.getCatalysisDirection())\n                continue\n\n            enzs = BiopaxProcessor._get_primary_controller(controller_pe)\n            if not enzs:\n                continue\n            '''\n            if _is_complex(input_pe):\n                sub_members_in = self._get_complex_members(input_pe)\n                sub_members_out = self._get_complex_members(output_pe)\n                # TODO: It is possible to find which member of the complex is\n                # actually modified. That member will be the substrate and\n                # all other members of the complex will be bound to it.\n                logger.info('Cannot handle complex substrates.')\n                continue\n            '''\n            subs = BiopaxProcessor._get_agents_from_entity(input_spe,\n                                                           expand_pe=False)\n \n            ev = self._get_evidence(control)\n            for enz, sub in itertools.product(_listify(enzs), _listify(subs)):\n                # Get the modifications\n                mod_in = \\\n                    BiopaxProcessor._get_entity_mods(input_spe)\n                mod_out = \\\n                    BiopaxProcessor._get_entity_mods(output_spe)\n\n                sub.mods = _get_mod_intersection(mod_in, mod_out)\n\n                if issubclass(mod_class, AddModification):\n                    gained_mods = _get_mod_difference(mod_out, mod_in)\n                else:\n                    gained_mods = _get_mod_difference(mod_in, mod_out)\n\n                for mod in gained_mods:\n                    # Is it guaranteed that these are all modifications\n                    # of the type we are extracting?\n                    if mod.mod_type not in (mod_type,\n                                            modtype_to_inverse[mod_type]):\n                        continue\n                    stmt = mod_class(enz, sub, mod.residue, mod.position,\n                                     evidence=ev)\n                    stmts.append(decode_obj(stmt, encoding='utf-8'))\n        return stmts", "response": "Get all generic Modification reactions given a Modification class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct the BioPAX pattern to extract modification reactions.", "response": "def _construct_modification_pattern():\n        \"\"\"Construct the BioPAX pattern to extract modification reactions.\"\"\"\n        # The following constraints were pieced together based on the\n        # following two higher level constrains: pb.controlsStateChange(),\n        # pb.controlsPhosphorylation().\n        p = _bpp('Pattern')(_bpimpl('PhysicalEntity')().getModelInterface(),\n                            'controller PE')\n        # Getting the control itself\n        p.add(cb.peToControl(), \"controller PE\", \"Control\")\n        # Link the control to the conversion that it controls\n        p.add(cb.controlToConv(), \"Control\", \"Conversion\")\n        # The controller shouldn't be a participant of the conversion\n        p.add(_bpp('constraint.NOT')(cb.participant()),\n              \"Conversion\", \"controller PE\")\n        # Get the input participant of the conversion\n        p.add(pt(rt.INPUT, True), \"Control\", \"Conversion\", \"input PE\")\n        # Get the specific PhysicalEntity\n        p.add(cb.linkToSpecific(), \"input PE\", \"input simple PE\")\n        # Link to ER\n        p.add(cb.peToER(), \"input simple PE\", \"input simple ER\")\n        # Make sure the participant is a protein\n        p.add(tp(_bpimpl('Protein')().getModelInterface()), \"input simple PE\")\n        # Link to the other side of the conversion\n        p.add(cs(cst.OTHER_SIDE), \"input PE\", \"Conversion\", \"output PE\")\n        # Make sure the two sides are not the same\n        p.add(_bpp('constraint.Equality')(False), \"input PE\", \"output PE\")\n        # Get the specific PhysicalEntity\n        p.add(cb.linkToSpecific(), \"output PE\", \"output simple PE\")\n        # Link to ER\n        p.add(cb.peToER(), \"output simple PE\", \"output simple ER\")\n        p.add(_bpp('constraint.Equality')(True), \"input simple ER\",\n              \"output simple ER\")\n        # Make sure the output is a Protein\n        p.add(tp(_bpimpl('Protein')().getModelInterface()), \"output simple PE\")\n        p.add(_bpp('constraint.NOT')(cb.linkToSpecific()),\n              \"input PE\", \"output simple PE\")\n        p.add(_bpp('constraint.NOT')(cb.linkToSpecific()),\n              \"output PE\", \"input simple PE\")\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the type of modification and position from a ModificationFeature object in the INDRA format.", "response": "def _extract_mod_from_feature(mf):\n        \"\"\"Extract the type of modification and the position from\n        a ModificationFeature object in the INDRA format.\"\"\"\n        # ModificationFeature / SequenceModificationVocabulary\n        mf_type = mf.getModificationType()\n        if mf_type is None:\n            return None\n        mf_type_terms = mf_type.getTerm().toArray()\n        known_mf_type = None\n        for t in mf_type_terms:\n            if t.startswith('MOD_RES '):\n                t = t[8:]\n            mf_type_indra = _mftype_dict.get(t)\n            if mf_type_indra is not None:\n                known_mf_type = mf_type_indra\n                break\n        if not known_mf_type:\n            logger.debug('Skipping modification with unknown terms: %s' %\n                        ', '.join(mf_type_terms))\n            return None\n\n        mod_type, residue = known_mf_type\n\n        # getFeatureLocation returns SequenceLocation, which is the\n        # generic parent class of SequenceSite and SequenceInterval.\n        # Here we need to cast to SequenceSite in order to get to\n        # the sequence position.\n        mf_pos = mf.getFeatureLocation()\n        if mf_pos is not None:\n            # If it is not a SequenceSite we can't handle it\n            if not mf_pos.modelInterface.getName() == \\\n                'org.biopax.paxtools.model.level3.SequenceSite':\n                mod_pos = None\n            else:\n                mf_site = cast(_bp('SequenceSite'), mf_pos)\n                mf_pos_status = mf_site.getPositionStatus()\n                if mf_pos_status is None:\n                    mod_pos = None\n                elif mf_pos_status and mf_pos_status.toString() != 'EQUAL':\n                    logger.debug('Modification site position is %s' %\n                                mf_pos_status.toString())\n                else:\n                    mod_pos = mf_site.getSequencePosition()\n                    mod_pos = '%s' % mod_pos\n        else:\n            mod_pos = None\n        mc = ModCondition(mod_type, residue, mod_pos, True)\n        return mc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_entref(bpe):\n        if not _is_reference(bpe):\n            try:\n                er = bpe.getEntityReference()\n            except AttributeError:\n                return None\n            return er\n        else:\n            return bpe", "response": "Returns the entity reference of an entity that was passed in as argument."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying an event location to the Agents in the corresponding Statement.", "response": "def _stmt_location_to_agents(stmt, location):\n    \"\"\"Apply an event location to the Agents in the corresponding Statement.\n\n    If a Statement is in a given location we represent that by requiring all\n    Agents in the Statement to be in that location.\n    \"\"\"\n    if location is None:\n        return\n    agents = stmt.agent_list()\n    for a in agents:\n        if a is not None:\n            a.location = location"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_db_refs(term):\n    db_refs = {}\n    # Here we extract the text name of the Agent\n    # There are two relevant tags to consider here.\n    # The <text> tag typically contains a larger phrase surrounding the\n    # term but it contains the term in a raw, non-canonicalized form.\n    # The <name> tag only contains the name of the entity but it is\n    # canonicalized. For instance, MAP2K1 appears as MAP-2-K-1.\n    agent_text_tag = term.find('name')\n    if agent_text_tag is not None:\n        db_refs['TEXT'] = agent_text_tag.text\n        # If we have some drum-terms, the matched-name of the first\n        # drum-term (e.g. \"MAP2K1\") is a better value for TEXT than\n        # the name of the TERM (e.g. \"MAP-2-K-1\") so we put that in there\n        drum_terms = term.findall('drum-terms/drum-term')\n        if drum_terms:\n            matched_name = drum_terms[0].attrib.get('matched-name')\n            if matched_name:\n                db_refs['TEXT'] = matched_name\n\n    # We make a list of scored grounding terms from the DRUM terms\n    grounding_terms = _get_grounding_terms(term)\n    if not grounding_terms:\n        # This is for backwards compatibility with EKBs without drum-term\n        # scored entries. It is important to keep for Bioagents\n        # compatibility.\n        dbid = term.attrib.get('dbid')\n        if dbid:\n            dbids = dbid.split('|')\n            for dbname, dbid in [d.split(':') for d in dbids]:\n                if not db_refs.get(dbname):\n                    db_refs[dbname] = dbid\n        return db_refs, None, []\n\n\n    # This is the INDRA prioritization of grounding name spaces. Lower score\n    # takes precedence.\n    ns_priority = {\n        'HGNC': 1,\n        'UP': 1,\n        'FPLX': 2,\n        'CHEBI': 3,\n        'PC': 3,\n        'GO': 4,\n        'FA': 5,\n        'XFAM': 5,\n        'NCIT': 5\n    }\n    # We get the top priority entry from each score group\n    score_groups = itertools.groupby(grounding_terms, lambda x: x['score'])\n    top_per_score_group = []\n    ambiguities = []\n    for score, group in score_groups:\n        entries = list(group)\n        for entry in entries:\n            priority = 100\n            for ref_ns, ref_id in entry['refs'].items():\n                # Skip etc UP entries\n                if ref_ns == 'UP' and ref_id == 'etc':\n                    continue\n                try:\n                    priority = min(priority, ns_priority[ref_ns])\n                except KeyError:\n                    pass\n                if ref_ns == 'UP':\n                    if not up_client.is_human(ref_id):\n                        priority = 4\n            entry['priority'] = priority\n        if len(entries) > 1:\n            top_entry = entries[0]\n            top_idx = 0\n            for i, entry in enumerate(entries):\n                # We take the lowest priority entry within the score group\n                # as the top entry\n                if entry['priority'] < top_entry['priority']:\n                    # This is a corner case in which a protein family\n                    # should be prioritized over a specific protein,\n                    # specifically when HGNC was mapped from NCIT but\n                    # FPLX was not mapped from NCIT, the HGNC shouldn't\n                    # take precedence.\n                    if entry.get('comment') == 'HGNC_FROM_NCIT' and \\\n                        'FPLX' in top_entry['refs'] and \\\n                        top_entry.get('comment') != 'FPLX_FROM_NCIT':\n                        continue\n                    top_entry = entry\n                    top_idx = i\n            for i, entry in enumerate(entries):\n                if i == top_idx:\n                    continue\n                if (entry['priority'] - top_entry['priority']) <= 1:\n                    ambiguities.append((top_entry, entry))\n        else:\n            top_entry = entries[0]\n        top_per_score_group.append(top_entry)\n    # Get the top priority for each score group\n    priorities = [entry['priority'] for entry in top_per_score_group]\n\n    # By default, we coose the top priority entry from the highest score group\n    top_grounding = top_per_score_group[0]\n    # Sometimes the top grounding has much lower priority and not much higher\n    # score than the second grounding. Typically 1.0 vs 0.82857 and 5 vs 2.\n    # In this case we take the second entry. A special case is handled where\n    # a FPLX entry was mapped from FA, in which case priority difference of < 2\n    # is also accepted.\n    if len(top_per_score_group) > 1:\n        score_diff = top_per_score_group[0]['score'] - \\\n                     top_per_score_group[1]['score']\n        priority_diff = top_per_score_group[0]['priority'] - \\\n                        top_per_score_group[1]['priority']\n        if score_diff < 0.2 and (priority_diff >= 2 or \\\n            top_per_score_group[0].get('comment') == 'FPLX_FROM_FA'):\n            top_grounding = top_per_score_group[1]\n    relevant_ambiguities = []\n    for amb in ambiguities:\n        if top_grounding not in amb:\n            continue\n        if top_grounding == amb[0]:\n            relevant_ambiguities.append({'preferred': amb[0],\n                                         'alternative': amb[1]})\n        else:\n            relevant_ambiguities.append({'preferred': amb[1],\n                                         'alternative': amb[0]})\n\n    for k, v in top_grounding['refs'].items():\n        db_refs[k] = v\n\n    # Now standardize db_refs to the INDRA standards\n    # We need to add a prefix for CHEBI\n    chebi_id = db_refs.get('CHEBI')\n    if chebi_id and not chebi_id.startswith('CHEBI:'):\n        db_refs['CHEBI'] = 'CHEBI:%s' % chebi_id\n    # We need to strip the trailing version number for XFAM and rename to PF\n    pfam_id = db_refs.get('XFAM')\n    if pfam_id:\n        pfam_id = pfam_id.split('.')[0]\n        db_refs.pop('XFAM', None)\n        db_refs['PF'] = pfam_id\n    # We need to add GO prefix if it is missing\n    go_id = db_refs.get('GO')\n    if go_id:\n        if not go_id.startswith('GO:'):\n            db_refs['GO'] = 'GO:%s' % go_id\n    # We need to deal with Nextprot families\n    nxp_id = db_refs.get('FA')\n    if nxp_id:\n        db_refs.pop('FA', None)\n        db_refs['NXPFA'] = nxp_id\n    # We need to rename PC to PUBCHEM\n    pc_id = db_refs.get('PC')\n    if pc_id:\n        db_refs.pop('PC', None)\n        db_refs['PUBCHEM'] = pc_id\n\n    # Here we also get and return the type, which is a TRIPS\n    # ontology type. This is to be used in the context of\n    # Bioagents.\n    ont_type = top_grounding['type']\n\n    return db_refs, ont_type, relevant_ambiguities", "response": "Extract database references for a term."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a list of all events in the TRIPS EKB.", "response": "def get_all_events(self):\n        \"\"\"Make a list of all events in the TRIPS EKB.\n\n        The events are stored in self.all_events.\n        \"\"\"\n        self.all_events = {}\n        events = self.tree.findall('EVENT')\n        events += self.tree.findall('CC')\n        for e in events:\n            event_id = e.attrib['id']\n            if event_id in self._static_events:\n                continue\n            event_type = e.find('type').text\n            try:\n                self.all_events[event_type].append(event_id)\n            except KeyError:\n                self.all_events[event_type] = [event_id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract direct Activation INDRA Statements.", "response": "def get_activations(self):\n        \"\"\"Extract direct Activation INDRA Statements.\"\"\"\n        act_events = self.tree.findall(\"EVENT/[type='ONT::ACTIVATE']\")\n        inact_events = self.tree.findall(\"EVENT/[type='ONT::DEACTIVATE']\")\n        inact_events += self.tree.findall(\"EVENT/[type='ONT::INHIBIT']\")\n        for event in (act_events + inact_events):\n            event_id = event.attrib['id']\n            if event_id in self._static_events:\n                continue\n            # Get the activating agent in the event\n            agent = event.find(\".//*[@role=':AGENT']\")\n            if agent is None:\n                continue\n            agent_id = agent.attrib.get('id')\n            if agent_id is None:\n                logger.debug(\n                    'Skipping activation with missing activator agent')\n                continue\n            activator_agent = self._get_agent_by_id(agent_id, event_id)\n            if activator_agent is None:\n                continue\n\n            # Get the activated agent in the event\n            affected = event.find(\".//*[@role=':AFFECTED']\")\n            if affected is None:\n                logger.debug(\n                    'Skipping activation with missing affected agent')\n                continue\n            affected_id = affected.attrib.get('id')\n            if affected_id is None:\n                logger.debug(\n                    'Skipping activation with missing affected agent')\n                continue\n\n            affected_agent = self._get_agent_by_id(affected_id, event_id)\n            if affected_agent is None:\n                logger.debug(\n                    'Skipping activation with missing affected agent')\n                continue\n\n            is_activation = True\n            if _is_type(event, 'ONT::ACTIVATE'):\n                self._add_extracted('ONT::ACTIVATE', event.attrib['id'])\n            elif _is_type(event, 'ONT::INHIBIT'):\n                is_activation = False\n                self._add_extracted('ONT::INHIBIT', event.attrib['id'])\n            elif _is_type(event, 'ONT::DEACTIVATE'):\n                is_activation = False\n                self._add_extracted('ONT::DEACTIVATE', event.attrib['id'])\n            ev = self._get_evidence(event)\n            location = self._get_event_location(event)\n            for a1, a2 in _agent_list_product((activator_agent,\n                                               affected_agent)):\n                if is_activation:\n                    st = Activation(a1, a2, evidence=[deepcopy(ev)])\n                else:\n                    st = Inhibition(a1, a2, evidence=[deepcopy(ev)])\n                _stmt_location_to_agents(st, location)\n                self.statements.append(st)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_activations_causal(self):\n        # Search for causal connectives of type ONT::CAUSE\n        ccs = self.tree.findall(\"CC/[type='ONT::CAUSE']\")\n        for cc in ccs:\n            factor = cc.find(\"arg/[@role=':FACTOR']\")\n            outcome = cc.find(\"arg/[@role=':OUTCOME']\")\n            # If either the factor or the outcome is missing, skip\n            if factor is None or outcome is None:\n                continue\n            factor_id = factor.attrib.get('id')\n            # Here, implicitly, we require that the factor is a TERM\n            # and not an EVENT\n            factor_term = self.tree.find(\"TERM/[@id='%s']\" % factor_id)\n            outcome_id = outcome.attrib.get('id')\n            # Here it is implicit that the outcome is an event not\n            # a TERM\n            outcome_event = self.tree.find(\"EVENT/[@id='%s']\" % outcome_id)\n            if factor_term is None or outcome_event is None:\n                continue\n            factor_term_type = factor_term.find('type')\n            # The factor term must be a molecular entity\n            if factor_term_type is None or \\\n               factor_term_type.text not in molecule_types:\n                continue\n            factor_agent = self._get_agent_by_id(factor_id, None)\n            if factor_agent is None:\n                continue\n            outcome_event_type = outcome_event.find('type')\n            if outcome_event_type is None:\n                continue\n            # Construct evidence\n            ev = self._get_evidence(cc)\n            ev.epistemics['direct'] = False\n            location = self._get_event_location(outcome_event)\n            if outcome_event_type.text in ['ONT::ACTIVATE', 'ONT::ACTIVITY',\n                                           'ONT::DEACTIVATE']:\n                if outcome_event_type.text in ['ONT::ACTIVATE',\n                                               'ONT::DEACTIVATE']:\n                    agent_tag = outcome_event.find(\".//*[@role=':AFFECTED']\")\n                elif outcome_event_type.text == 'ONT::ACTIVITY':\n                    agent_tag = outcome_event.find(\".//*[@role=':AGENT']\")\n                if agent_tag is None or agent_tag.attrib.get('id') is None:\n                    continue\n                outcome_agent = self._get_agent_by_id(agent_tag.attrib['id'],\n                                                      outcome_id)\n                if outcome_agent is None:\n                    continue\n                if outcome_event_type.text == 'ONT::DEACTIVATE':\n                    is_activation = False\n                else:\n                    is_activation = True\n                for a1, a2 in _agent_list_product((factor_agent,\n                                                   outcome_agent)):\n                    if is_activation:\n                        st = Activation(a1, a2, evidence=[deepcopy(ev)])\n                    else:\n                        st = Inhibition(a1, a2, evidence=[deepcopy(ev)])\n                    _stmt_location_to_agents(st, location)\n                    self.statements.append(st)", "response": "Extract INDRA Statements from the XML file for Activations causal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_activations_stimulate(self):\n        # TODO: extract to other patterns:\n        # - Stimulation by EGF activates ERK\n        # - Stimulation by EGF leads to ERK activation\n        # Search for stimulation event\n        stim_events = self.tree.findall(\"EVENT/[type='ONT::STIMULATE']\")\n        for event in stim_events:\n            event_id = event.attrib.get('id')\n            if event_id in self._static_events:\n                continue\n            controller = event.find(\"arg1/[@role=':AGENT']\")\n            affected = event.find(\"arg2/[@role=':AFFECTED']\")\n            # If either the controller or the affected is missing, skip\n            if controller is None or affected is None:\n                continue\n            controller_id = controller.attrib.get('id')\n            # Here, implicitly, we require that the controller is a TERM\n            # and not an EVENT\n            controller_term = self.tree.find(\"TERM/[@id='%s']\" % controller_id)\n            affected_id = affected.attrib.get('id')\n            # Here it is implicit that the affected is an event not\n            # a TERM\n            affected_event = self.tree.find(\"EVENT/[@id='%s']\" % affected_id)\n            if controller_term is None or affected_event is None:\n                continue\n            controller_term_type = controller_term.find('type')\n            # The controller term must be a molecular entity\n            if controller_term_type is None or \\\n               controller_term_type.text not in molecule_types:\n                continue\n            controller_agent = self._get_agent_by_id(controller_id, None)\n            if controller_agent is None:\n                continue\n            affected_event_type = affected_event.find('type')\n            if affected_event_type is None:\n                continue\n            # Construct evidence\n            ev = self._get_evidence(event)\n            ev.epistemics['direct'] = False\n            location = self._get_event_location(affected_event)\n            if affected_event_type.text == 'ONT::ACTIVATE':\n                affected = affected_event.find(\".//*[@role=':AFFECTED']\")\n                if affected is None:\n                    continue\n                affected_agent = self._get_agent_by_id(affected.attrib['id'],\n                                                       affected_id)\n                if affected_agent is None:\n                    continue\n                for a1, a2 in _agent_list_product((controller_agent,\n                                                   affected_agent)):\n                    st = Activation(a1, a2, evidence=[deepcopy(ev)])\n                    _stmt_location_to_agents(st, location)\n                    self.statements.append(st)\n            elif affected_event_type.text == 'ONT::ACTIVITY':\n                agent_tag = affected_event.find(\".//*[@role=':AGENT']\")\n                if agent_tag is None:\n                    continue\n                affected_agent = self._get_agent_by_id(agent_tag.attrib['id'],\n                                                       affected_id)\n                if affected_agent is None:\n                    continue\n                for a1, a2 in _agent_list_product((controller_agent,\n                                                   affected_agent)):\n                    st = Activation(a1, a2, evidence=[deepcopy(ev)])\n                    _stmt_location_to_agents(st, location)\n                    self.statements.append(st)", "response": "Extract Activation INDRA Statements via stimulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_degradations(self):\n        deg_events = self.tree.findall(\"EVENT/[type='ONT::CONSUME']\")\n        for event in deg_events:\n            if event.attrib['id'] in self._static_events:\n                continue\n            affected = event.find(\".//*[@role=':AFFECTED']\")\n            if affected is None:\n                msg = 'Skipping degradation event with no affected term.'\n                logger.debug(msg)\n                continue\n\n            # Make sure the degradation is affecting a molecule type\n            # Temporarily removed for CwC compatibility with no type tag\n            #affected_type = affected.find('type')\n            #if affected_type is None or \\\n            #   affected_type.text not in molecule_types:\n            #    continue\n\n            affected_id = affected.attrib.get('id')\n            if affected_id is None:\n                logger.debug(\n                    'Skipping degradation event with missing affected agent')\n                continue\n\n            affected_agent = self._get_agent_by_id(affected_id,\n                                                   event.attrib['id'])\n            if affected_agent is None:\n                logger.debug(\n                    'Skipping degradation event with missing affected agent')\n                continue\n\n            agent = event.find(\".//*[@role=':AGENT']\")\n            if agent is None:\n                agent_agent = None\n            else:\n                agent_id = agent.attrib.get('id')\n                if agent_id is None:\n                    agent_agent = None\n                else:\n                    agent_agent = self._get_agent_by_id(agent_id,\n                                                        event.attrib['id'])\n            ev = self._get_evidence(event)\n            location = self._get_event_location(event)\n            for subj, obj in \\\n                    _agent_list_product((agent_agent, affected_agent)):\n                st = DecreaseAmount(subj, obj, evidence=deepcopy(ev))\n                _stmt_location_to_agents(st, location)\n                self.statements.append(st)\n            self._add_extracted(_get_type(event), event.attrib['id'])", "response": "Extract Degradation INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract Increase and DecreaseAmount Statements.", "response": "def get_regulate_amounts(self):\n        \"\"\"Extract Increase/DecreaseAmount Statements.\"\"\"\n        pos_events = []\n        neg_events = []\n        pattern = \"EVENT/[type='ONT::STIMULATE']/arg2/[type='ONT::TRANSCRIBE']/..\"\n        pos_events += self.tree.findall(pattern)\n        pattern = \"EVENT/[type='ONT::INCREASE']/arg2/[type='ONT::TRANSCRIBE']/..\"\n        pos_events += self.tree.findall(pattern)\n        pattern = \"EVENT/[type='ONT::INHIBIT']/arg2/[type='ONT::TRANSCRIBE']/..\"\n        neg_events += self.tree.findall(pattern)\n        pattern = \"EVENT/[type='ONT::DECREASE']/arg2/[type='ONT::TRANSCRIBE']/..\"\n        neg_events += self.tree.findall(pattern)\n        # Look at polarity\n        pattern = \"EVENT/[type='ONT::MODULATE']/arg2/[type='ONT::TRANSCRIBE']/..\"\n        mod_events = self.tree.findall(pattern)\n        for event in mod_events:\n            pol = event.find('polarity')\n            if pol is not None:\n                if pol.text == 'ONT::POSITIVE':\n                    pos_events.append(event)\n                elif pol.text == 'ONT::NEGATIVE':\n                    neg_events.append(event)\n        combs = zip([pos_events, neg_events], [IncreaseAmount, DecreaseAmount])\n        for events, cls in combs:\n            for event in events:\n                if event.attrib['id'] in self._static_events:\n                    continue\n                if event.attrib['id'] in self._subsumed_events:\n                    continue\n                # The agent has to exist and be a protein type\n                agent = event.find(\".//*[@role=':AGENT']\")\n                if agent is None:\n                    continue\n                if agent.find('type') is None or \\\n                    (agent.find('type').text not in protein_types):\n                    continue\n                agent_id = agent.attrib.get('id')\n                if agent_id is None:\n                    continue\n                agent_agent = self._get_agent_by_id(agent_id,\n                                                    event.attrib['id'])\n\n                # The affected, we already know is ONT::TRANSCRIPTION\n                affected_arg = event.find(\".//*[@role=':AFFECTED']\")\n                if affected_arg is None:\n                    continue\n                affected_id = affected_arg.attrib.get('id')\n                affected_event = self.tree.find(\"EVENT/[@id='%s']\" %\n                                                affected_id)\n                if affected_event is None:\n                    continue\n                affected = \\\n                    affected_event.find(\".//*[@role=':AFFECTED-RESULT']\")\n                if affected is None:\n                    affected = \\\n                        affected_event.find(\".//*[@role=':AFFECTED']\")\n                    if affected is None:\n                        continue\n                affected_id = affected.attrib.get('id')\n                if affected_id is None:\n                    continue\n                affected_agent = \\\n                        self._get_agent_by_id(affected_id,\n                                              affected_event.attrib['id'])\n                ev = self._get_evidence(event)\n                location = self._get_event_location(event)\n                for subj, obj in \\\n                        _agent_list_product((agent_agent, affected_agent)):\n                    if obj is None:\n                        continue\n                    st = cls(subj, obj, evidence=deepcopy(ev))\n                    _stmt_location_to_agents(st, location)\n                    self.statements.append(st)\n                self._add_extracted(_get_type(event), event.attrib['id'])\n                self._subsumed_events.append(affected_event.attrib['id'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts ActiveForm INDRA Statements.", "response": "def get_active_forms(self):\n        \"\"\"Extract ActiveForm INDRA Statements.\"\"\"\n        act_events = self.tree.findall(\"EVENT/[type='ONT::ACTIVATE']\")\n        def _agent_is_basic(agent):\n            if not agent.mods and not agent.mutations \\\n                    and not agent.bound_conditions and not agent.location:\n                return True\n            return False\n        for event in act_events:\n            if event.attrib['id'] in self._static_events:\n                continue\n            agent = event.find(\".//*[@role=':AGENT']\")\n            if agent is not None:\n                # In this case this is not an ActiveForm statement\n                continue\n            affected = event.find(\".//*[@role=':AFFECTED']\")\n            if affected is None:\n                msg = 'Skipping active form event with no affected term.'\n                logger.debug(msg)\n                continue\n\n            affected_id = affected.attrib.get('id')\n            if affected_id is None:\n                logger.debug(\n                    'Skipping active form event with missing affected agent')\n                continue\n\n            affected_agent = self._get_agent_by_id(affected_id,\n                                                   event.attrib['id'])\n            # If it is a list of agents, skip them for now\n            if not isinstance(affected_agent, Agent):\n                continue\n            if _agent_is_basic(affected_agent):\n                continue\n            # The affected agent has to be protein-like type\n            affected_type = affected.find('type')\n            if affected_type is None or \\\n               affected_type.text not in protein_types:\n                continue\n            # If the Agent state is at the base state then this is not an\n            # ActiveForm statement\n            if _is_base_agent_state(affected_agent):\n                continue\n            ev = self._get_evidence(event)\n            location = self._get_event_location(event)\n            st = ActiveForm(affected_agent, 'activity', True, evidence=ev)\n            _stmt_location_to_agents(st, location)\n            self.statements.append(st)\n            self._add_extracted('ONT::ACTIVATE', event.attrib['id'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts ActiveForm INDRA Statements.", "response": "def get_active_forms_state(self):\n        \"\"\"Extract ActiveForm INDRA Statements.\"\"\"\n        for term in self._isolated_terms:\n            act = term.find('features/active')\n            if act is None:\n                continue\n            if act.text == 'TRUE':\n                is_active = True\n            elif act.text == 'FALSE':\n                is_active = False\n            else:\n                logger.warning('Unhandled term activity feature %s' % act.text)\n            agent = self._get_agent_by_id(term.attrib['id'], None)\n            # Skip aggregates for now\n            if not isinstance(agent, Agent):\n                continue\n            # If the Agent state is at the base state then this is not an\n            # ActiveForm statement\n            if _is_base_agent_state(agent):\n                continue\n            # Remove the activity flag since it's irrelevant here\n            agent.activity = None\n            text_term = term.find('text')\n            if text_term is not None:\n                ev_text = text_term.text\n            else:\n                ev_text = None\n            ev = Evidence(source_api='trips', text=ev_text, pmid=self.doc_id)\n            st = ActiveForm(agent, 'activity', is_active, evidence=[ev])\n            self.statements.append(st)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_complexes(self):\n        bind_events = self.tree.findall(\"EVENT/[type='ONT::BIND']\")\n        bind_events += self.tree.findall(\"EVENT/[type='ONT::INTERACT']\")\n        for event in bind_events:\n            if event.attrib['id'] in self._static_events:\n                continue\n\n            arg1 = event.find(\"arg1\")\n            arg2 = event.find(\"arg2\")\n            # EKB-AGENT\n            if arg1 is None and arg2 is None:\n                args = list(event.findall('arg'))\n                if len(args) < 2:\n                    continue\n                arg1 = args[0]\n                arg2 = args[1]\n\n            if (arg1 is None or arg1.attrib.get('id') is None) or \\\n               (arg2 is None or arg2.attrib.get('id') is None):\n                logger.debug('Skipping complex with less than 2 members')\n                continue\n\n            agent1 = self._get_agent_by_id(arg1.attrib['id'],\n                                           event.attrib['id'])\n            agent2 = self._get_agent_by_id(arg2.attrib['id'],\n                                           event.attrib['id'])\n            if agent1 is None or agent2 is None:\n                logger.debug('Skipping complex with less than 2 members')\n                continue\n\n            # Information on binding site is either attached to the agent term\n            # in a features/site tag or attached to the event itself in\n            # a site tag\n            '''\n            site_feature = self._find_in_term(arg1.attrib['id'], 'features/site')\n            if site_feature is not None:\n                sites, positions = self._get_site_by_id(site_id)\n                print sites, positions\n\n            site_feature = self._find_in_term(arg2.attrib['id'], 'features/site')\n            if site_feature is not None:\n                sites, positions = self._get_site_by_id(site_id)\n                print sites, positions\n\n            site = event.find(\"site\")\n            if site is not None:\n                sites, positions = self._get_site_by_id(site.attrib['id'])\n                print sites, positions\n            '''\n            ev = self._get_evidence(event)\n            location = self._get_event_location(event)\n            for a1, a2 in _agent_list_product((agent1, agent2)):\n                st = Complex([a1, a2], evidence=deepcopy(ev))\n                _stmt_location_to_agents(st, location)\n                self.statements.append(st)\n            self._add_extracted(_get_type(event), event.attrib['id'])", "response": "Extract Complex INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_modifications(self):\n        # Get all the specific mod types\n        mod_event_types = list(ont_to_mod_type.keys())\n        # Add ONT::PTMs as a special case\n        mod_event_types += ['ONT::PTM']\n        mod_events = []\n        for mod_event_type in mod_event_types:\n            events = self.tree.findall(\"EVENT/[type='%s']\" % mod_event_type)\n            mod_extracted = self.extracted_events.get(mod_event_type, [])\n            for event in events:\n                event_id = event.attrib.get('id')\n                if event_id not in mod_extracted:\n                    mod_events.append(event)\n\n        # Iterate over all modification events\n        for event in mod_events:\n            stmts = self._get_modification_event(event)\n            if stmts:\n                for stmt in stmts:\n                    self.statements.append(stmt)", "response": "Extract all types of Modification INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_modifications_indirect(self):\n        # Get all the specific mod types\n        mod_event_types = list(ont_to_mod_type.keys())\n        # Add ONT::PTMs as a special case\n        mod_event_types += ['ONT::PTM']\n\n        def get_increase_events(mod_event_types):\n            mod_events = []\n            events = self.tree.findall(\"EVENT/[type='ONT::INCREASE']\")\n            for event in events:\n                affected = event.find(\".//*[@role=':AFFECTED']\")\n                if affected is None:\n                    continue\n                affected_id = affected.attrib.get('id')\n                if not affected_id:\n                    continue\n                pattern = \"EVENT/[@id='%s']\" % affected_id\n                affected_event = self.tree.find(pattern)\n                if affected_event is not None:\n                    affected_type = affected_event.find('type')\n                    if affected_type is not None and \\\n                        affected_type.text in mod_event_types:\n                        mod_events.append(event)\n            return mod_events\n\n        def get_cause_events(mod_event_types):\n            mod_events = []\n            ccs = self.tree.findall(\"CC/[type='ONT::CAUSE']\")\n            for cc in ccs:\n                outcome = cc.find(\".//*[@role=':OUTCOME']\")\n                if outcome is None:\n                    continue\n                outcome_id = outcome.attrib.get('id')\n                if not outcome_id:\n                    continue\n                pattern = \"EVENT/[@id='%s']\" % outcome_id\n                outcome_event = self.tree.find(pattern)\n                if outcome_event is not None:\n                    outcome_type = outcome_event.find('type')\n                    if outcome_type is not None and \\\n                        outcome_type.text in mod_event_types:\n                        mod_events.append(cc)\n            return mod_events\n\n        mod_events = get_increase_events(mod_event_types)\n        mod_events += get_cause_events(mod_event_types)\n\n        # Iterate over all modification events\n        for event in mod_events:\n            event_id = event.attrib['id']\n            if event_id in self._static_events:\n                continue\n            event_type = _get_type(event)\n\n            # Get enzyme Agent\n            enzyme = event.find(\".//*[@role=':AGENT']\")\n            if enzyme is None:\n                enzyme = event.find(\".//*[@role=':FACTOR']\")\n                if enzyme is None:\n                    return\n\n            enzyme_id = enzyme.attrib.get('id')\n            if enzyme_id is None:\n                continue\n            enzyme_agent = self._get_agent_by_id(enzyme_id, event_id)\n\n            affected_event_tag = event.find(\".//*[@role=':AFFECTED']\")\n            if affected_event_tag is None:\n                affected_event_tag = event.find(\".//*[@role=':OUTCOME']\")\n                if affected_event_tag is None:\n                    return\n            affected_id = affected_event_tag.attrib.get('id')\n            if not affected_id:\n                return\n            affected_event = self.tree.find(\"EVENT/[@id='%s']\" % affected_id)\n            if affected_event is None:\n                return\n\n            # Iterate over all enzyme agents if there are multiple ones\n            for enz_t in _agent_list_product((enzyme_agent, )):\n                # enz_t comes out as a tuple so we need to take the first\n                # element here\n                enz = enz_t[0]\n                # Note that we re-run the extraction code here potentially\n                # multiple times. This is mainly to make sure each Statement\n                # object created here is independent (i.e. has different UUIDs)\n                # without having to manipulate it after creation.\n                stmts = self._get_modification_event(affected_event)\n                stmts_to_make = []\n                if stmts:\n                    for stmt in stmts:\n                        # The affected event should have no enzyme but should\n                        # have a substrate\n                        if stmt.enz is None and stmt.sub is not None:\n                            stmts_to_make.append(stmt)\n\n                for stmt in stmts_to_make:\n                    stmt.enz = enz\n                    for ev in stmt.evidence:\n                        ev.epistemics['direct'] = False\n                    self.statements.append(stmt)\n\n            self._add_extracted(event_type, event.attrib['id'])\n            self._add_extracted(affected_event.find('type').text, affected_id)", "response": "Extract indirect Modification INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_agents(self):\n        agents_dict = self.get_term_agents()\n        agents = [a for a in agents_dict.values() if a is not None]\n        return agents", "response": "Returns list of INDRA Agents corresponding to TERMs in the EKB."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_term_agents(self):\n        terms = self.tree.findall('TERM')\n        agents = {}\n        assoc_links = []\n        for term in terms:\n            term_id = term.attrib.get('id')\n            if term_id:\n                agent = self._get_agent_by_id(term_id, None)\n                agents[term_id] = agent\n                # Handle assoc-with links\n                aw = term.find('assoc-with')\n                if aw is not None:\n                    aw_id = aw.attrib.get('id')\n                    if aw_id:\n                        assoc_links.append((term_id, aw_id))\n        # We only keep the target end of assoc with links if both\n        # source and target are in the list\n        for source, target in assoc_links:\n            if target in agents and source in agents:\n                agents.pop(source)\n        return agents", "response": "Returns a dictionary of INDRA Agents keyed by corresponding TERMs in the EKB."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the evidence text for an event.", "response": "def _get_evidence_text(self, event_tag):\n        \"\"\"Extract the evidence for an event.\n\n        Pieces of text linked to an EVENT are fragments of a sentence. The\n        EVENT refers to the paragraph ID and the \"uttnum\", which corresponds\n        to a sentence ID. Here we find and return the full sentence from which\n        the event was taken.\n        \"\"\"\n        par_id = event_tag.attrib.get('paragraph')\n        uttnum = event_tag.attrib.get('uttnum')\n        event_text = event_tag.find('text')\n        if self.sentences is not None and uttnum is not None:\n            sentence = self.sentences[uttnum]\n        elif event_text is not None:\n            sentence = event_text.text\n        else:\n            sentence = None\n        return sentence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an agent to the corresponding PyBEL DSL object.", "response": "def _get_agent_grounding(agent):\n    \"\"\"Convert an agent to the corresponding PyBEL DSL object (to be filled with variants later).\"\"\"\n    def _get_id(_agent, key):\n        _id = _agent.db_refs.get(key)\n        if isinstance(_id, list):\n            _id = _id[0]\n        return _id\n\n    hgnc_id = _get_id(agent, 'HGNC')\n    if hgnc_id:\n        hgnc_name = hgnc_client.get_hgnc_name(hgnc_id)\n        if not hgnc_name:\n            logger.warning('Agent %s with HGNC ID %s has no HGNC name.',\n                           agent, hgnc_id)\n            return\n        return protein('HGNC', hgnc_name)\n\n    uniprot_id = _get_id(agent, 'UP')\n    if uniprot_id:\n        return protein('UP', uniprot_id)\n\n    fplx_id = _get_id(agent, 'FPLX')\n    if fplx_id:\n        return protein('FPLX', fplx_id)\n\n    pfam_id = _get_id(agent, 'PF')\n    if pfam_id:\n        return protein('PFAM', pfam_id)\n\n    ip_id = _get_id(agent, 'IP')\n    if ip_id:\n        return protein('IP', ip_id)\n\n    fa_id = _get_id(agent, 'FA')\n    if fa_id:\n        return protein('NXPFA', fa_id)\n\n    chebi_id = _get_id(agent, 'CHEBI')\n    if chebi_id:\n        if chebi_id.startswith('CHEBI:'):\n            chebi_id = chebi_id[len('CHEBI:'):]\n        return abundance('CHEBI', chebi_id)\n\n    pubchem_id = _get_id(agent, 'PUBCHEM')\n    if pubchem_id:\n        return abundance('PUBCHEM', pubchem_id)\n\n    go_id = _get_id(agent, 'GO')\n    if go_id:\n        return bioprocess('GO', go_id)\n\n    mesh_id = _get_id(agent, 'MESH')\n    if mesh_id:\n        return bioprocess('MESH', mesh_id)\n\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the causal polar edge with the correct contact.", "response": "def get_causal_edge(stmt, activates):\n    \"\"\"Returns the causal, polar edge with the correct \"contact\".\"\"\"\n    any_contact = any(\n        evidence.epistemics.get('direct', False)\n        for evidence in stmt.evidence\n    )\n    if any_contact:\n        return pc.DIRECTLY_INCREASES if activates else pc.DIRECTLY_DECREASES\n\n    return pc.INCREASES if activates else pc.DECREASES"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_database(self, manager=None):\n        network = pybel.to_database(self.model, manager=manager)\n        return network", "response": "Send the model to the PyBEL database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_web(self, host=None, user=None, password=None):\n        response = pybel.to_web(self.model, host=host, user=user,\n                                password=password)\n        return response", "response": "Send the model to BEL Commons by wrapping pybel. to_web"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_model(self, path, output_format=None):\n        if output_format == 'pickle':\n            pybel.to_pickle(self.model, path)\n        else:\n            with open(path, 'w') as fh:\n                if output_format == 'json':\n                    pybel.to_json_file(self.model, fh)\n                elif output_format == 'cx':\n                    pybel.to_cx_file(self.model, fh)\n                else: # output_format == 'bel':\n                    pybel.to_bel(self.model, fh)", "response": "Save the BELGraph using one of the outputs from\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_nodes_edges(self, subj_agent, obj_agent, relation, evidences):\n        subj_data, subj_edge = _get_agent_node(subj_agent)\n        obj_data, obj_edge = _get_agent_node(obj_agent)\n        # If we failed to create nodes for subject or object, skip it\n        if subj_data is None or obj_data is None:\n            return\n        subj_node = self.model.add_node_from_data(subj_data)\n        obj_node = self.model.add_node_from_data(obj_data)\n        edge_data_list = \\\n            _combine_edge_data(relation, subj_edge, obj_edge, evidences)\n        for edge_data in edge_data_list:\n            self.model.add_edge(subj_node, obj_node, **edge_data)", "response": "Given subj and obj agents relation and evidence add nodes and edges."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassembling RegulateActivity statement into a new object.", "response": "def _assemble_regulate_activity(self, stmt):\n        \"\"\"Example: p(HGNC:MAP2K1) => act(p(HGNC:MAPK1))\"\"\"\n        act_obj = deepcopy(stmt.obj)\n        act_obj.activity = stmt._get_activity_condition()\n        # We set is_active to True here since the polarity is encoded\n        # in the edge (decreases/increases)\n        act_obj.activity.is_active = True\n        activates = isinstance(stmt, Activation)\n        relation = get_causal_edge(stmt, activates)\n        self._add_nodes_edges(stmt.subj, act_obj, relation, stmt.evidence)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nassembling Modification statements into a single node.", "response": "def _assemble_modification(self, stmt):\n        \"\"\"Example: p(HGNC:MAP2K1) => p(HGNC:MAPK1, pmod(Ph, Thr, 185))\"\"\"\n        sub_agent = deepcopy(stmt.sub)\n        sub_agent.mods.append(stmt._get_mod_condition())\n        activates = isinstance(stmt, AddModification)\n        relation = get_causal_edge(stmt, activates)\n        self._add_nodes_edges(stmt.enz, sub_agent, relation, stmt.evidence)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassemble RegulateAmount statements into a single node.", "response": "def _assemble_regulate_amount(self, stmt):\n        \"\"\"Example: p(HGNC:ELK1) => p(HGNC:FOS)\"\"\"\n        activates = isinstance(stmt, IncreaseAmount)\n        relation = get_causal_edge(stmt, activates)\n        self._add_nodes_edges(stmt.subj, stmt.obj, relation, stmt.evidence)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _assemble_gef(self, stmt):\n        gef = deepcopy(stmt.gef)\n        gef.activity = ActivityCondition('gef', True)\n        ras = deepcopy(stmt.ras)\n        ras.activity = ActivityCondition('gtpbound', True)\n        self._add_nodes_edges(gef, ras, pc.DIRECTLY_INCREASES, stmt.evidence)", "response": "Assemble a Gef statement into a set of INDRA statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _assemble_gap(self, stmt):\n        gap = deepcopy(stmt.gap)\n        gap.activity = ActivityCondition('gap', True)\n        ras = deepcopy(stmt.ras)\n        ras.activity = ActivityCondition('gtpbound', True)\n        self._add_nodes_edges(gap, ras, pc.DIRECTLY_DECREASES, stmt.evidence)", "response": "Assemble Gap statement into a single EntryNode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassemble the ActiveForm statement into a single Agent.", "response": "def _assemble_active_form(self, stmt):\n        \"\"\"Example: p(HGNC:ELK1, pmod(Ph)) => act(p(HGNC:ELK1), ma(tscript))\"\"\"\n        act_agent = Agent(stmt.agent.name, db_refs=stmt.agent.db_refs)\n        act_agent.activity = ActivityCondition(stmt.activity, True)\n        activates = stmt.is_active\n        relation = get_causal_edge(stmt, activates)\n        self._add_nodes_edges(stmt.agent, act_agent, relation, stmt.evidence)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _assemble_complex(self, stmt):\n        complex_data, _ = _get_complex_node(stmt.members)\n        if complex_data is None:\n            logger.info('skip adding complex with no members: %s', stmt.members)\n            return\n        self.model.add_node_from_data(complex_data)", "response": "Assemble a Complex statement into the internal structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassemble the conversion information into the model.", "response": "def _assemble_conversion(self, stmt):\n        \"\"\"Example: p(HGNC:HK1) => rxn(reactants(a(CHEBI:\"CHEBI:17634\")),\n                                       products(a(CHEBI:\"CHEBI:4170\")))\"\"\"\n        pybel_lists = ([], [])\n        for pybel_list, agent_list in \\\n                            zip(pybel_lists, (stmt.obj_from, stmt.obj_to)):\n            for agent in agent_list:\n                node = _get_agent_grounding(agent)\n                # TODO check for missing grounding?\n                pybel_list.append(node)\n\n        rxn_node_data = reaction(\n            reactants=pybel_lists[0],\n            products=pybel_lists[1],\n        )\n        obj_node = self.model.add_node_from_data(rxn_node_data)\n        obj_edge = None  # TODO: Any edge information possible here?\n        # Add node for controller, if there is one\n        if stmt.subj is not None:\n            subj_attr, subj_edge = _get_agent_node(stmt.subj)\n            subj_node = self.model.add_node_from_data(subj_attr)\n            edge_data_list = _combine_edge_data(pc.DIRECTLY_INCREASES,\n                                           subj_edge, obj_edge, stmt.evidence)\n            for edge_data in edge_data_list:\n                self.model.add_edge(subj_node, obj_node, **edge_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _assemble_autophosphorylation(self, stmt):\n        sub_agent = deepcopy(stmt.enz)\n        mc = stmt._get_mod_condition()\n        sub_agent.mods.append(mc)\n        # FIXME Ignore any bound conditions on the substrate!!!\n        # This is because if they are included, a complex node will be returned,\n        # which (at least currently) won't incorporate any protein\n        # modifications.\n        sub_agent.bound_conditions = []\n        # FIXME\n        self._add_nodes_edges(stmt.enz, sub_agent, pc.DIRECTLY_INCREASES,\n                              stmt.evidence)", "response": "Assemble the autocophosphorylation statements into a single resource set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _assemble_transphosphorylation(self, stmt):\n        # Check our assumptions about the bound condition of the enzyme\n        assert len(stmt.enz.bound_conditions) == 1\n        assert stmt.enz.bound_conditions[0].is_bound\n        # Create a modified protein node for the bound target\n        sub_agent = deepcopy(stmt.enz.bound_conditions[0].agent)\n        sub_agent.mods.append(stmt._get_mod_condition())\n        self._add_nodes_edges(stmt.enz, sub_agent, pc.DIRECTLY_INCREASES,\n                              stmt.evidence)", "response": "Assemble the transphosphorylation statements into a single protein node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_binding_site_name(agent):\n    # Try to construct a binding site name based on parent\n    grounding = agent.get_grounding()\n    if grounding != (None, None):\n        uri = hierarchies['entity'].get_uri(grounding[0], grounding[1])\n        # Get highest level parents in hierarchy\n        parents = hierarchies['entity'].get_parents(uri, 'top')\n        if parents:\n            # Choose the first parent if there are more than one\n            parent_uri = sorted(parents)[0]\n            parent_agent = _agent_from_uri(parent_uri)\n            binding_site = _n(parent_agent.name).lower()\n            return binding_site\n    # Fall back to Agent's own name if one from parent can't be constructed\n    binding_site = _n(agent.name).lower()\n    return binding_site", "response": "Return a binding site name from a given agent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn site names for a modification.", "response": "def get_mod_site_name(mod_condition):\n    \"\"\"Return site names for a modification.\"\"\"\n    if mod_condition.residue is None:\n        mod_str = abbrevs[mod_condition.mod_type]\n    else:\n        mod_str = mod_condition.residue\n    mod_pos = mod_condition.position if \\\n        mod_condition.position is not None else ''\n    name = ('%s%s' % (mod_str, mod_pos))\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the given list of flat files and returns INDRA Statements.", "response": "def process_flat_files(id_mappings_file, complexes_file=None, ptm_file=None,\n                       ppi_file=None, seq_file=None, motif_window=7):\n    \"\"\"Get INDRA Statements from HPRD data.\n\n    Of the arguments, `id_mappings_file` is required, and at least one of\n    `complexes_file`, `ptm_file`, and `ppi_file` must also be given.  If\n    `ptm_file` is given, `seq_file` must also be given.\n\n    Note that many proteins (> 1,600) in the HPRD content are associated with\n    outdated RefSeq IDs that cannot be mapped to Uniprot IDs. For these, the\n    Uniprot ID obtained from the HGNC ID (itself obtained from the Entrez ID)\n    is used. Because the sequence referenced by the Uniprot ID obtained this\n    way may be different from the (outdated) RefSeq sequence included with the\n    HPRD content, it is possible that this will lead to invalid site positions\n    with respect to the Uniprot IDs.\n\n    To allow these site positions to be mapped during assembly, the\n    Modification statements produced by the HprdProcessor include an additional\n    key in the `annotations` field of their Evidence object. The annotations\n    field is called 'site_motif' and it maps to a dictionary with three\n    elements: 'motif', 'respos', and 'off_by_one'. 'motif' gives the peptide\n    sequence obtained from the RefSeq sequence included with HPRD. 'respos'\n    indicates the position in the peptide sequence containing the residue.\n    Note that these positions are ONE-INDEXED (not zero-indexed). Finally, the\n    'off-by-one' field contains a boolean value indicating whether the correct\n    position was inferred as being an off-by-one (methionine cleavage) error.\n    If True, it means that the given residue could not be found in the HPRD\n    RefSeq sequence at the given position, but a matching residue was found at\n    position+1, suggesting a sequence numbering based on the methionine-cleaved\n    sequence. The peptide included in the 'site_motif' dictionary is based on\n    this updated position.\n\n    Parameters\n    ----------\n    id_mappings_file : str\n        Path to HPRD_ID_MAPPINGS.txt file.\n    complexes_file : Optional[str]\n        Path to PROTEIN_COMPLEXES.txt file.\n    ptm_file : Optional[str]\n        Path to POST_TRANSLATIONAL_MODIFICATIONS.txt file.\n    ppi_file : Optional[str]\n        Path to BINARY_PROTEIN_PROTEIN_INTERACTIONS.txt file.\n    seq_file : Optional[str]\n        Path to PROTEIN_SEQUENCES.txt file.\n    motif_window : int\n        Number of flanking amino acids to include on each side of the\n        PTM target residue in the 'site_motif' annotations field of the\n        Evidence for Modification Statements. Default is 7.\n\n    Returns\n    -------\n    HprdProcessor\n        An HprdProcessor object which contains a list of extracted INDRA\n        Statements in its statements attribute.\n    \"\"\"\n    id_df = pd.read_csv(id_mappings_file, delimiter='\\t', names=_hprd_id_cols,\n                        dtype='str')\n    id_df = id_df.set_index('HPRD_ID')\n    if complexes_file is None and ptm_file is None and ppi_file is None:\n        raise ValueError('At least one of complexes_file, ptm_file, or '\n                         'ppi_file must be given.')\n    if ptm_file and not seq_file:\n        raise ValueError('If ptm_file is given, seq_file must also be given.')\n    # Load complexes into dataframe\n    cplx_df = None\n    if complexes_file:\n        cplx_df = pd.read_csv(complexes_file, delimiter='\\t', names=_cplx_cols,\n                              dtype='str', na_values=['-', 'None'])\n    # Load ptm data into dataframe\n    ptm_df = None\n    seq_dict = None\n    if ptm_file:\n        ptm_df = pd.read_csv(ptm_file, delimiter='\\t', names=_ptm_cols,\n                             dtype='str', na_values='-')\n        # Load protein sequences as a dict keyed by RefSeq ID\n        seq_dict = load_fasta_sequences(seq_file, id_index=2)\n    # Load the PPI data into dataframe\n    ppi_df = None\n    if ppi_file:\n        ppi_df = pd.read_csv(ppi_file, delimiter='\\t', names=_ppi_cols,\n                             dtype='str')\n    # Create the processor\n    return HprdProcessor(id_df, cplx_df, ptm_df, ppi_df, seq_dict, motif_window)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _gather_active_forms(self):\n        for stmt in self.statements:\n            if isinstance(stmt, ActiveForm):\n                base_agent = self.agent_set.get_create_base_agent(stmt.agent)\n                # Handle the case where an activity flag is set\n                agent_to_add = stmt.agent\n                if stmt.agent.activity:\n                    new_agent = fast_deepcopy(stmt.agent)\n                    new_agent.activity = None\n                    agent_to_add = new_agent\n                base_agent.add_activity_form(agent_to_add, stmt.is_active)", "response": "Collect all the active forms of each Agent in the Statements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing ative flags with Agent states when possible.", "response": "def replace_activities(self):\n        \"\"\"Replace ative flags with Agent states when possible.\"\"\"\n        logger.debug('Running PySB Preassembler replace activities')\n        # TODO: handle activity hierarchies\n        new_stmts = []\n\n        def has_agent_activity(stmt):\n            \"\"\"Return True if any agents in the Statement have activity.\"\"\"\n            for agent in stmt.agent_list():\n                if isinstance(agent, Agent) and agent.activity is not None:\n                    return True\n            return False\n        # First collect all explicit active forms\n        self._gather_active_forms()\n        # Iterate over all statements\n        for j, stmt in enumerate(self.statements):\n            logger.debug('%d/%d %s' % (j + 1, len(self.statements), stmt))\n            # If the Statement doesn't have any activities, we can just\n            # keep it and move on\n            if not has_agent_activity(stmt):\n                new_stmts.append(stmt)\n                continue\n            stmt_agents = stmt.agent_list()\n            num_agents = len(stmt_agents)\n            # Make a list with an empty list for each Agent so that later\n            # we can build combinations of Agent forms\n            agent_forms = [[] for a in stmt_agents]\n            for i, agent in enumerate(stmt_agents):\n                # This is the case where there is an activity flag on an\n                # Agent which we will attempt to replace with an explicit\n                # active form\n                if agent is not None and isinstance(agent, Agent) and \\\n                        agent.activity is not None:\n                    base_agent = self.agent_set.get_create_base_agent(agent)\n                    # If it is an \"active\" state\n                    if agent.activity.is_active:\n                        active_forms = base_agent.active_forms\n                        # If no explicit active forms are known then we use\n                        # the generic one\n                        if not active_forms:\n                            active_forms = [agent]\n                    # If it is an \"inactive\" state\n                    else:\n                        active_forms = base_agent.inactive_forms\n                        # If no explicit inactive forms are known then we use\n                        # the generic one\n                        if not active_forms:\n                            active_forms = [agent]\n                    # We now iterate over the active agent forms and create\n                    # new agents\n                    for af in active_forms:\n                        new_agent = fast_deepcopy(agent)\n                        self._set_agent_context(af, new_agent)\n                        agent_forms[i].append(new_agent)\n                # Otherwise we just copy over the agent as is\n                else:\n                    agent_forms[i].append(agent)\n            # Now create all possible combinations of the agents and create new\n            # statements as needed\n            agent_combs = itertools.product(*agent_forms)\n            for agent_comb in agent_combs:\n                new_stmt = fast_deepcopy(stmt)\n                new_stmt.set_agent_list(agent_comb)\n                new_stmts.append(new_stmt)\n        self.statements = new_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_reverse_effects(self):\n        # TODO: generalize to other modification sites\n        pos_mod_sites = {}\n        neg_mod_sites = {}\n        syntheses = []\n        degradations = []\n        for stmt in self.statements:\n            if isinstance(stmt, Phosphorylation):\n                agent = stmt.sub.name\n                try:\n                    pos_mod_sites[agent].append((stmt.residue, stmt.position))\n                except KeyError:\n                    pos_mod_sites[agent] = [(stmt.residue, stmt.position)]\n            elif isinstance(stmt, Dephosphorylation):\n                agent = stmt.sub.name\n                try:\n                    neg_mod_sites[agent].append((stmt.residue, stmt.position))\n                except KeyError:\n                    neg_mod_sites[agent] = [(stmt.residue, stmt.position)]\n            elif isinstance(stmt, Influence):\n                if stmt.overall_polarity() == 1:\n                    syntheses.append(stmt.obj.name)\n                elif stmt.overall_polarity() == -1:\n                    degradations.append(stmt.obj.name)\n            elif isinstance(stmt, IncreaseAmount):\n                syntheses.append(stmt.obj.name)\n            elif isinstance(stmt, DecreaseAmount):\n                degradations.append(stmt.obj.name)\n\n        new_stmts = []\n        for agent_name, pos_sites in pos_mod_sites.items():\n            neg_sites = neg_mod_sites.get(agent_name, [])\n            no_neg_site = set(pos_sites).difference(set(neg_sites))\n            for residue, position in no_neg_site:\n                st = Dephosphorylation(Agent('phosphatase'),\n                                           Agent(agent_name),\n                                           residue, position)\n                new_stmts.append(st)\n        for agent_name in syntheses:\n            if agent_name not in degradations:\n                st = DecreaseAmount(None, Agent(agent_name))\n                new_stmts.append(st)\n\n        self.statements += new_stmts", "response": "Add Statements for the reverse effects of some Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_uniprot_id(agent):\n    up_id = agent.db_refs.get('UP')\n    hgnc_id = agent.db_refs.get('HGNC')\n    if up_id is None:\n        if hgnc_id is None:\n            # If both UniProt and HGNC refs are missing we can't\n            # sequence check and so don't report a failure.\n            return None\n        # Try to get UniProt ID from HGNC\n        up_id = hgnc_client.get_uniprot_id(hgnc_id)\n        # If this fails, again, we can't sequence check\n        if up_id is None:\n            return None\n    # If the UniProt ID is a list then choose the first one.\n    if not isinstance(up_id, basestring) and \\\n       isinstance(up_id[0], basestring):\n        up_id = up_id[0]\n    return up_id", "response": "Return the UniProt ID for an agent looking up in HGNC if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks a set of statements for invalid modification sites.", "response": "def map_sites(self, stmts):\n        \"\"\"Check a set of statements for invalid modification sites.\n\n        Statements are checked against Uniprot reference sequences to determine\n        if residues referred to by post-translational modifications exist at\n        the given positions.\n\n        If there is nothing amiss with a statement (modifications on any of the\n        agents, modifications made in the statement, etc.), then the statement\n        goes into the list of valid statements. If there is a problem with the\n        statement, the offending modifications are looked up in the site map\n        (:py:attr:`site_map`), and an instance of :py:class:`MappedStatement`\n        is added to the list of mapped statements.\n\n        Parameters\n        ----------\n        stmts : list of :py:class:`indra.statement.Statement`\n            The statements to check for site errors.\n\n        Returns\n        -------\n        tuple\n            2-tuple containing (valid_statements, mapped_statements). The first\n            element of the tuple is a list of valid statements\n            (:py:class:`indra.statement.Statement`) that were not found to\n            contain any site errors. The second element of the tuple is a list\n            of mapped statements (:py:class:`MappedStatement`) with information\n            on the incorrect sites and corresponding statements with correctly\n            mapped sites.\n        \"\"\"\n        valid_statements = []\n        mapped_statements = []\n\n        for stmt in stmts:\n            mapped_stmt = self.map_stmt_sites(stmt)\n            # If we got a MappedStatement as a return value, we add that to the\n            # list of mapped statements, otherwise, the original Statement is\n            # not invalid so we add it to the other list directly.\n            if mapped_stmt is not None:\n                mapped_statements.append(mapped_stmt)\n            else:\n                valid_statements.append(stmt)\n\n        return valid_statements, mapped_statements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _map_agent_sites(self, agent):\n        # If there are no modifications on this agent, then we can return the\n        # copy of the agent\n        if agent is None or not agent.mods:\n            return [], agent\n        new_agent = deepcopy(agent)\n        mapped_sites = []\n        # Now iterate over all the modifications and map each one\n        for idx, mod_condition in enumerate(agent.mods):\n            mapped_site = \\\n                self._map_agent_mod(agent, mod_condition)\n            # If we couldn't do the mapping or the mapped site isn't invalid\n            # then we don't need to change the existing ModCondition\n            if not mapped_site or mapped_site.not_invalid():\n                continue\n            # Otherwise, if there is a mapping, we replace the old ModCondition\n            # with the new one where only the residue and position are updated,\n            # the mod type and the is modified flag are kept.\n            if mapped_site.has_mapping():\n                mc = ModCondition(mod_condition.mod_type,\n                                  mapped_site.mapped_res,\n                                  mapped_site.mapped_pos,\n                                  mod_condition.is_modified)\n                new_agent.mods[idx] = mc\n            # Finally, whether or not we have a mapping, we keep track of mapped\n            # sites and make them available to the caller\n            mapped_sites.append(mapped_site)\n        return mapped_sites, new_agent", "response": "Map the agent s invalid sites and modify the agent s modified sites."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps a single modification condition on an agent.", "response": "def _map_agent_mod(self, agent, mod_condition):\n        \"\"\"Map a single modification condition on an agent.\n\n        Parameters\n        ----------\n        agent : :py:class:`indra.statements.Agent`\n            Agent to check for invalid modification sites.\n        mod_condition : :py:class:`indra.statements.ModCondition`\n            Modification to check for validity and map.\n\n        Returns\n        -------\n        protmapper.MappedSite or None\n            A MappedSite object is returned if a UniProt ID was found for the\n            agent, and if both the position and residue for the modification\n            condition were available. Otherwise None is returned.\n        \"\"\"\n        # Get the UniProt ID of the agent, if not found, return\n        up_id = _get_uniprot_id(agent)\n        if not up_id:\n            logger.debug(\"No uniprot ID for %s\" % agent.name)\n            return None\n        # If no site information for this residue, skip\n        if mod_condition.position is None or mod_condition.residue is None:\n            return None\n        # Otherwise, try to map it and return the mapped site\n        mapped_site = \\\n            self.map_to_human_ref(up_id, 'uniprot',\n                mod_condition.residue,\n                mod_condition.position,\n                do_methionine_offset=self.do_methionine_offset,\n                do_orthology_mapping=self.do_orthology_mapping,\n                do_isoform_mapping=self.do_isoform_mapping)\n        return mapped_site"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of transitive reductions on a BaseAgent.", "response": "def _get_graph_reductions(graph):\n    \"\"\"Return transitive reductions on a DAG.\n\n    This is used to reduce the set of activities of a BaseAgent to the most\n    specific one(s) possible. For instance, if a BaseAgent is know to have\n    'activity', 'catalytic' and 'kinase' activity, then this function will\n    return {'activity': 'kinase', 'catalytic': 'kinase', 'kinase': 'kinase'}\n    as the set of reductions.\n    \"\"\"\n    def frontier(g, nd):\n        \"\"\"Return the nodes after nd in the topological sort that are at the\n        lowest possible level of the topological sort.\"\"\"\n        if g.out_degree(nd) == 0:\n            return set([nd])\n        else:\n            frontiers = set()\n            for n in g.successors(nd):\n                frontiers = frontiers.union(frontier(graph, n))\n            return frontiers\n    reductions = {}\n    nodes_sort = list(networkx.algorithms.dag.topological_sort(graph))\n    frontiers = [frontier(graph, n) for n in nodes_sort]\n    # This loop ensures that if a node n2 comes after node n1 in the topological\n    # sort, and their frontiers are identical then n1 can be reduced to n2.\n    # If their frontiers aren't identical, the reduction cannot be done.\n    for i, n1 in enumerate(nodes_sort):\n        for j, n2 in enumerate(nodes_sort):\n            if i > j:\n                continue\n            if frontiers[i] == frontiers[j]:\n                reductions[n1] = n2\n    return reductions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gather_explicit_activities(self):\n        for stmt in self.statements:\n            agents = stmt.agent_list()\n            # Activity types given as ActivityConditions\n            for agent in agents:\n                if agent is not None and agent.activity is not None:\n                    agent_base = self._get_base(agent)\n                    agent_base.add_activity(agent.activity.activity_type)\n            # Object activities given in RegulateActivity statements\n            if isinstance(stmt, RegulateActivity):\n                if stmt.obj is not None:\n                    obj_base = self._get_base(stmt.obj)\n                    obj_base.add_activity(stmt.obj_activity)\n            # Activity types given in ActiveForms\n            elif isinstance(stmt, ActiveForm):\n                agent_base = self._get_base(stmt.agent)\n                agent_base.add_activity(stmt.activity)\n                if stmt.is_active:\n                    agent_base.add_active_state(stmt.activity, stmt.agent,\n                                                stmt.evidence)\n                else:\n                    agent_base.add_inactive_state(stmt.activity, stmt.agent,\n                                                  stmt.evidence)", "response": "Aggregate all explicit activities and active forms of Agents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngathering all implicit activities and active forms of Agents and collect only explicit activities.", "response": "def gather_implicit_activities(self):\n        \"\"\"Aggregate all implicit activities and active forms of Agents.\n\n        Iterate over self.statements and collect the implied activities\n        and active forms of Agents that appear in the Statements.\n\n        Note that using this function to collect implied Agent activities can\n        be risky. Assume, for instance, that a Statement from a reading\n        system states that EGF bound to EGFR phosphorylates ERK. This would\n        be interpreted as implicit evidence for the EGFR-bound form of EGF\n        to have 'kinase' activity, which is clearly incorrect.\n\n        In contrast the alternative pair of this function:\n        gather_explicit_activities collects only explicitly stated activities.\n        \"\"\"\n        for stmt in self.statements:\n            if isinstance(stmt, Phosphorylation) or \\\n                isinstance(stmt, Transphosphorylation) or \\\n                isinstance(stmt, Autophosphorylation):\n                if stmt.enz is not None:\n                    enz_base = self._get_base(stmt.enz)\n                    enz_base.add_activity('kinase')\n                    enz_base.add_active_state('kinase', stmt.enz.mods)\n            elif isinstance(stmt, Dephosphorylation):\n                if stmt.enz is not None:\n                    enz_base = self._get_base(stmt.enz)\n                    enz_base.add_activity('phosphatase')\n                    enz_base.add_active_state('phosphatase', stmt.enz.mods)\n            elif isinstance(stmt, Modification):\n                if stmt.enz is not None:\n                    enz_base = self._get_base(stmt.enz)\n                    enz_base.add_activity('catalytic')\n                    enz_base.add_active_state('catalytic', stmt.enz.mods)\n            elif isinstance(stmt, SelfModification):\n                if stmt.enz is not None:\n                    enz_base = self._get_base(stmt.enz)\n                    enz_base.add_activity('catalytic')\n                    enz_base.add_active_state('catalytic', stmt.enz.mods)\n            elif isinstance(stmt, Gef):\n                if stmt.gef is not None:\n                    gef_base = self._get_base(stmt.gef)\n                    gef_base.add_activity('gef')\n                    if stmt.gef.activity is not None:\n                        act = stmt.gef.activity.activity_type\n                    else:\n                        act = 'activity'\n                    gef_base.add_active_state(act, stmt.gef.mods)\n            elif isinstance(stmt, Gap):\n                if stmt.gap is not None:\n                    gap_base = self._get_base(stmt.gap)\n                    gap_base.add_activity('gap')\n                    if stmt.gap.activity is not None:\n                        act = stmt.gap.activity.activity_type\n                    else:\n                        act = 'activity'\n                    gap_base.add_active_state('act', stmt.gap.mods)\n            elif isinstance(stmt, RegulateActivity):\n                if stmt.subj is not None:\n                    subj_base = self._get_base(stmt.subj)\n                    subj_base.add_activity(stmt.j)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef require_active_forms(self):\n        logger.info('Setting required active forms on %d statements...' %\n                    len(self.statements))\n        new_stmts = []\n        for stmt in self.statements:\n            if isinstance(stmt, Modification):\n                if stmt.enz is None:\n                    new_stmts.append(stmt)\n                    continue\n                enz_base = self._get_base(stmt.enz)\n                active_forms = enz_base.get_active_forms()\n                if not active_forms:\n                    new_stmts.append(stmt)\n                else:\n                    for af in active_forms:\n                        new_stmt = fast_deepcopy(stmt)\n                        new_stmt.uuid = str(uuid.uuid4())\n                        evs = af.apply_to(new_stmt.enz)\n                        new_stmt.partial_evidence = evs\n                        new_stmts.append(new_stmt)\n            elif isinstance(stmt, RegulateAmount) or \\\n                isinstance(stmt, RegulateActivity):\n                if stmt.subj is None:\n                    new_stmts.append(stmt)\n                    continue\n                subj_base = self._get_base(stmt.subj)\n                active_forms = subj_base.get_active_forms()\n                if not active_forms:\n                    new_stmts.append(stmt)\n                else:\n                    for af in active_forms:\n                        new_stmt = fast_deepcopy(stmt)\n                        new_stmt.uuid = str(uuid.uuid4())\n                        evs = af.apply_to(new_stmt.subj)\n                        new_stmt.partial_evidence = evs\n                        new_stmts.append(new_stmt)\n            else:\n                new_stmts.append(stmt)\n        self.statements = new_stmts\n        return new_stmts", "response": "This function requires that all statements with Agents in active positions are in active positions."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reduce_activities(self):\n        for stmt in self.statements:\n            agents = stmt.agent_list()\n            for agent in agents:\n                if agent is not None and agent.activity is not None:\n                    agent_base = self._get_base(agent)\n                    act_red = agent_base.get_activity_reduction(\n                                                agent.activity.activity_type)\n                    if act_red is not None:\n                        agent.activity.activity_type = act_red\n            if isinstance(stmt, RegulateActivity):\n                if stmt.obj is not None:\n                    obj_base = self._get_base(stmt.obj)\n                    act_red = \\\n                        obj_base.get_activity_reduction(stmt.obj_activity)\n                    if act_red is not None:\n                        stmt.obj_activity = act_red\n            elif isinstance(stmt, ActiveForm):\n                agent_base = self._get_base(stmt.agent)\n                act_red = agent_base.get_activity_reduction(stmt.activity)\n                if act_red is not None:\n                    stmt.activity = act_red", "response": "Rewrite the activity types referenced in Statements for consistency."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns inferred Complexes from Statements implying physical interaction.", "response": "def infer_complexes(stmts):\n        \"\"\"Return inferred Complex from Statements implying physical interaction.\n\n        Parameters\n        ----------\n        stmts : list[indra.statements.Statement]\n            A list of Statements to infer Complexes from.\n\n        Returns\n        -------\n        linked_stmts : list[indra.mechlinker.LinkedStatement]\n            A list of LinkedStatements representing the inferred Statements.\n        \"\"\"\n        interact_stmts = _get_statements_by_type(stmts, Modification)\n        linked_stmts = []\n        for mstmt in interact_stmts:\n            if mstmt.enz is None:\n                continue\n            st = Complex([mstmt.enz, mstmt.sub], evidence=mstmt.evidence)\n            linked_stmts.append(st)\n        return linked_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninferring RegulateActivity from Modification + ActiveForm.", "response": "def infer_activations(stmts):\n        \"\"\"Return inferred RegulateActivity from Modification + ActiveForm.\n\n        This function looks for combinations of Modification and ActiveForm\n        Statements and infers Activation/Inhibition Statements from them.\n        For example, if we know that A phosphorylates B, and the\n        phosphorylated form of B is active, then we can infer that\n        A activates B. This can also be viewed as having \"explained\" a given\n        Activation/Inhibition Statement with a combination of more mechanistic\n        Modification + ActiveForm Statements.\n\n        Parameters\n        ----------\n        stmts : list[indra.statements.Statement]\n            A list of Statements to infer RegulateActivity from.\n\n        Returns\n        -------\n        linked_stmts : list[indra.mechlinker.LinkedStatement]\n            A list of LinkedStatements representing the inferred Statements.\n        \"\"\"\n        linked_stmts = []\n        af_stmts = _get_statements_by_type(stmts, ActiveForm)\n        mod_stmts = _get_statements_by_type(stmts, Modification)\n        for af_stmt, mod_stmt in itertools.product(*(af_stmts, mod_stmts)):\n            # There has to be an enzyme and the substrate and the\n            # agent of the active form have to match\n            if mod_stmt.enz is None or \\\n                (not af_stmt.agent.entity_matches(mod_stmt.sub)):\n                continue\n            # We now check the modifications to make sure they are consistent\n            if not af_stmt.agent.mods:\n                continue\n            found = False\n            for mc in af_stmt.agent.mods:\n                if mc.mod_type == modclass_to_modtype[mod_stmt.__class__] and \\\n                    mc.residue == mod_stmt.residue and \\\n                    mc.position == mod_stmt.position:\n                    found = True\n            if not found:\n                continue\n            # Collect evidence\n            ev = mod_stmt.evidence\n            # Finally, check the polarity of the ActiveForm\n            if af_stmt.is_active:\n                st = Activation(mod_stmt.enz, mod_stmt.sub, af_stmt.activity,\n                                evidence=ev)\n            else:\n                st = Inhibition(mod_stmt.enz, mod_stmt.sub, af_stmt.activity,\n                                evidence=ev)\n            linked_stmts.append(LinkedStatement([af_stmt, mod_stmt], st))\n        return linked_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninfer ActiveForm from RegulateActivity + Modification.", "response": "def infer_active_forms(stmts):\n        \"\"\"Return inferred ActiveForm from RegulateActivity + Modification.\n\n        This function looks for combinations of Activation/Inhibition\n        Statements and Modification Statements, and infers an ActiveForm\n        from them. For example, if we know that A activates B and\n        A phosphorylates B, then we can infer that the phosphorylated form\n        of B is active.\n\n        Parameters\n        ----------\n        stmts : list[indra.statements.Statement]\n            A list of Statements to infer ActiveForms from.\n\n        Returns\n        -------\n        linked_stmts : list[indra.mechlinker.LinkedStatement]\n            A list of LinkedStatements representing the inferred Statements.\n        \"\"\"\n        linked_stmts = []\n        for act_stmt in _get_statements_by_type(stmts, RegulateActivity):\n            # TODO: revise the conditions here\n            if not (act_stmt.subj.activity is not None and\n                act_stmt.subj.activity.activity_type == 'kinase' and\n                act_stmt.subj.activity.is_active):\n                continue\n            matching = []\n            ev = act_stmt.evidence\n            for mod_stmt in _get_statements_by_type(stmts, Modification):\n                if mod_stmt.enz is not None:\n                    if mod_stmt.enz.entity_matches(act_stmt.subj) and \\\n                        mod_stmt.sub.entity_matches(act_stmt.obj):\n                        matching.append(mod_stmt)\n                        ev.extend(mod_stmt.evidence)\n            if not matching:\n                continue\n            mods = []\n            for mod_stmt in matching:\n                mod_type_name = mod_stmt.__class__.__name__.lower()\n                if isinstance(mod_stmt, AddModification):\n                    is_modified = True\n                else:\n                    is_modified = False\n                    mod_type_name = mod_type_name[2:]\n                mc = ModCondition(mod_type_name, mod_stmt.residue,\n                                  mod_stmt.position, is_modified)\n                mods.append(mc)\n            source_stmts = [act_stmt] + [m for m in matching]\n            st = ActiveForm(Agent(act_stmt.obj.name, mods=mods,\n                                  db_refs=act_stmt.obj.db_refs),\n                            act_stmt.obj_activity, act_stmt.is_activation,\n                            evidence=ev)\n            linked_stmts.append(LinkedStatement(source_stmts, st))\n            logger.info('inferred: %s' % st)\n        return linked_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef infer_modifications(stmts):\n        linked_stmts = []\n        for act_stmt in _get_statements_by_type(stmts, RegulateActivity):\n            for af_stmt in _get_statements_by_type(stmts, ActiveForm):\n                if not af_stmt.agent.entity_matches(act_stmt.obj):\n                    continue\n                mods = af_stmt.agent.mods\n                # Make sure the ActiveForm only involves modified sites\n                if af_stmt.agent.mutations or \\\n                    af_stmt.agent.bound_conditions or \\\n                    af_stmt.agent.location:\n                    continue\n                if not af_stmt.agent.mods:\n                    continue\n                for mod in af_stmt.agent.mods:\n                    evs = act_stmt.evidence + af_stmt.evidence\n                    for ev in evs:\n                        ev.epistemics['direct'] = False\n                    if mod.is_modified:\n                        mod_type_name = mod.mod_type\n                    else:\n                        mod_type_name = modtype_to_inverse[mod.mod_type]\n                    mod_class = modtype_to_modclass[mod_type_name]\n                    if not mod_class:\n                        continue\n                    st = mod_class(act_stmt.subj,\n                                   act_stmt.obj,\n                                   mod.residue, mod.position,\n                                   evidence=evs)\n                    ls = LinkedStatement([act_stmt, af_stmt], st)\n                    linked_stmts.append(ls)\n                    logger.info('inferred: %s' % st)\n        return linked_stmts", "response": "Infer Modification from RegulateActivity + ActiveForm."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves Complex Statements that can be inferred out. This function iterates over self.statements and looks for Complex Statements that either match or are refined by inferred Complex Statements that were linked (provided as the linked_stmts argument). It removes Complex Statements from self.statements that can be explained by the linked statements. Parameters ---------- linked_stmts : Optional[list[indra.mechlinker.LinkedStatement]] A list of linked statements, optionally passed from outside. If None is passed, the MechLinker runs self.infer_complexes to infer Complexes and obtain a list of LinkedStatements that are then used for removing existing Complexes in self.statements.", "response": "def replace_complexes(self, linked_stmts=None):\n        \"\"\"Remove Complex Statements that can be inferred out.\n\n        This function iterates over self.statements and looks for Complex\n        Statements that either match or are refined by inferred Complex\n        Statements that were linked (provided as the linked_stmts argument).\n        It removes Complex Statements from self.statements that can be\n        explained by the linked statements.\n\n        Parameters\n        ----------\n        linked_stmts : Optional[list[indra.mechlinker.LinkedStatement]]\n            A list of linked statements, optionally passed from outside.\n            If None is passed, the MechLinker runs self.infer_complexes to\n            infer Complexes and obtain a list of LinkedStatements that are\n            then used for removing existing Complexes in self.statements.\n        \"\"\"\n        if linked_stmts is None:\n            linked_stmts = self.infer_complexes(self.statements)\n        new_stmts = []\n        for stmt in self.statements:\n            if not isinstance(stmt, Complex):\n                new_stmts.append(stmt)\n                continue\n            found = False\n            for linked_stmt in linked_stmts:\n                if linked_stmt.refinement_of(stmt, hierarchies):\n                    found = True\n            if not found:\n                new_stmts.append(stmt)\n            else:\n                logger.info('Removing complex: %s' % stmt)\n        self.statements = new_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves RegulateActivity Statements that can be inferred out. This function iterates over self.statements and looks for RegulateActivity Statements that either match or are refined by inferred RegulateActivity Statements that were linked (provided as the linked_stmts argument). It removes RegulateActivity Statements from self.statements that can be explained by the linked statements. Parameters ---------- linked_stmts : Optional[list[indra.mechlinker.LinkedStatement]] A list of linked statements, optionally passed from outside. If None is passed, the MechLinker runs self.infer_activations to infer RegulateActivities and obtain a list of LinkedStatements that are then used for removing existing Complexes in self.statements.", "response": "def replace_activations(self, linked_stmts=None):\n        \"\"\"Remove RegulateActivity Statements that can be inferred out.\n\n        This function iterates over self.statements and looks for\n        RegulateActivity Statements that either match or are refined by\n        inferred RegulateActivity Statements that were linked\n        (provided as the linked_stmts argument).\n        It removes RegulateActivity Statements from self.statements that can be\n        explained by the linked statements.\n\n        Parameters\n        ----------\n        linked_stmts : Optional[list[indra.mechlinker.LinkedStatement]]\n            A list of linked statements, optionally passed from outside.\n            If None is passed, the MechLinker runs self.infer_activations to\n            infer RegulateActivities and obtain a list of LinkedStatements\n            that are then used for removing existing Complexes\n            in self.statements.\n        \"\"\"\n        if linked_stmts is None:\n            linked_stmts = self.infer_activations(self.statements)\n        new_stmts = []\n        for stmt in self.statements:\n            if not isinstance(stmt, RegulateActivity):\n                new_stmts.append(stmt)\n                continue\n            found = False\n            for linked_stmt in linked_stmts:\n                inferred_stmt = linked_stmt.inferred_stmt\n                if stmt.is_activation == inferred_stmt.is_activation and \\\n                    stmt.subj.entity_matches(inferred_stmt.subj) and \\\n                    stmt.obj.entity_matches(inferred_stmt.obj):\n                        found = True\n            if not found:\n                new_stmts.append(stmt)\n            else:\n                logger.info('Removing regulate activity: %s' % stmt)\n        self.statements = new_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_create_base_agent(self, agent):\n        try:\n            base_agent = self.agents[agent.name]\n        except KeyError:\n            base_agent = BaseAgent(agent.name)\n            self.agents[agent.name] = base_agent\n\n        return base_agent", "response": "Return BaseAgent from an Agent creating it if needed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply this object s state to an Agent.", "response": "def apply_to(self, agent):\n        \"\"\"Apply this object's state to an Agent.\n\n        Parameters\n        ----------\n        agent : indra.statements.Agent\n            The agent to which the state should be applied\n        \"\"\"\n        agent.bound_conditions = self.bound_conditions\n        agent.mods = self.mods\n        agent.mutations = self.mutations\n        agent.location = self.location\n        return self.evidence"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submit_curation():\n    if request.json is None:\n        abort(Response('Missing application/json header.', 415))\n    # Get input parameters\n    corpus_id = request.json.get('corpus_id')\n    curations = request.json.get('curations', {})\n    try:\n        curator.submit_curation(corpus_id, curations)\n    except InvalidCorpusError:\n        abort(Response('The corpus_id \"%s\" is unknown.' % corpus_id, 400))\n        return\n    return jsonify({})", "response": "Submit curations for a given corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_beliefs():\n    if request.json is None:\n        abort(Response('Missing application/json header.', 415))\n    # Get input parameters\n    corpus_id = request.json.get('corpus_id')\n    try:\n        belief_dict = curator.update_beliefs(corpus_id)\n    except InvalidCorpusError:\n        abort(Response('The corpus_id \"%s\" is unknown.' % corpus_id, 400))\n        return\n    return jsonify(belief_dict)", "response": "Return updated beliefs based on current probability model."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_scorer(self):\n        self.scorer = get_eidos_bayesian_scorer()\n        for corpus_id, corpus in self.corpora.items():\n            corpus.curations = {}", "response": "Reset the scorer used for couration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a corpus given an ID.", "response": "def get_corpus(self, corpus_id):\n        \"\"\"Return a corpus given an ID.\n\n        If the corpus ID cannot be found, an InvalidCorpusError is raised.\n\n        Parameters\n        ----------\n        corpus_id : str\n            The ID of the corpus to return.\n\n        Returns\n        -------\n        Corpus\n            The corpus with the given ID.\n        \"\"\"\n        try:\n            corpus = self.corpora[corpus_id]\n            return corpus\n        except KeyError:\n            raise InvalidCorpusError"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsubmit correct and incorrect curations for a given corpus.", "response": "def submit_curation(self, corpus_id, curations):\n        \"\"\"Submit correct/incorrect curations fo a given corpus.\n\n        Parameters\n        ----------\n        corpus_id : str\n            The ID of the corpus to which the curations apply.\n        curations : dict\n            A dict of curations with keys corresponding to Statement UUIDs and\n            values corresponding to correct/incorrect feedback.\n        \"\"\"\n        corpus = self.get_corpus(corpus_id)\n        # Start tabulating the curation counts\n        prior_counts = {}\n        subtype_counts = {}\n        # Take each curation from the input\n        for uuid, correct in curations.items():\n            # Save the curation in the corpus\n            # TODO: handle already existing curation\n            stmt = corpus.statements.get(uuid)\n            if stmt is None:\n                logger.warning('%s is not in the corpus.' % uuid)\n                continue\n            corpus.curations[uuid] = correct\n            # Now take all the evidences of the statement and assume that\n            # they follow the correctness of the curation and contribute to\n            # counts for their sources\n            for ev in stmt.evidence:\n                # Make the index in the curation count list\n                idx = 0 if correct else 1\n                extraction_rule = ev.annotations.get('found_by')\n                # If there is no extraction rule then we just score the source\n                if not extraction_rule:\n                    try:\n                        prior_counts[ev.source_api][idx] += 1\n                    except KeyError:\n                        prior_counts[ev.source_api] = [0, 0]\n                        prior_counts[ev.source_api][idx] += 1\n                # Otherwise we score the specific extraction rule\n                else:\n                    try:\n                        subtype_counts[ev.source_api][extraction_rule][idx] \\\n                            += 1\n                    except KeyError:\n                        if ev.source_api not in subtype_counts:\n                            subtype_counts[ev.source_api] = {}\n                        subtype_counts[ev.source_api][extraction_rule] = [0, 0]\n                        subtype_counts[ev.source_api][extraction_rule][idx] \\\n                            += 1\n        # Finally, we update the scorer with the new curation counts\n        self.scorer.update_counts(prior_counts, subtype_counts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of belief scores corresponding to Statement UUIDs and values to new belief scores for the given corpus.", "response": "def update_beliefs(self, corpus_id):\n        \"\"\"Return updated belief scores for a given corpus.\n\n        Parameters\n        ----------\n        corpus_id : str\n            The ID of the corpus for which beliefs are to be updated.\n\n        Returns\n        -------\n        dict\n            A dictionary of belief scores with keys corresponding to Statement\n            UUIDs and values to new belief scores.\n        \"\"\"\n        corpus = self.get_corpus(corpus_id)\n        be = BeliefEngine(self.scorer)\n        stmts = list(corpus.statements.values())\n        be.set_prior_probs(stmts)\n        # Here we set beliefs based on actual curation\n        for uuid, correct in corpus.curations.items():\n            stmt = corpus.statements.get(uuid)\n            if stmt is None:\n                logger.warning('%s is not in the corpus.' % uuid)\n                continue\n            stmt.belief = correct\n        belief_dict = {st.uuid: st.belief for st in stmts}\n        return belief_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_python_list(scala_list):\n    python_list = []\n    for i in range(scala_list.length()):\n        python_list.append(scala_list.apply(i))\n    return python_list", "response": "Return list from elements of scala. collection. immutable. List"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict from entries in a scala. collection. immutable. Map", "response": "def get_python_dict(scala_map):\n    \"\"\"Return a dict from entries in a scala.collection.immutable.Map\"\"\"\n    python_dict = {}\n    keys = get_python_list(scala_map.keys().toList())\n    for key in keys:\n        python_dict[key] = scala_map.apply(key)\n    return python_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_python_json(scala_json):\n    def convert_node(node):\n        if node.__class__.__name__ in ('org.json4s.JsonAST$JValue',\n                                       'org.json4s.JsonAST$JObject'):\n            # Make a dictionary and then convert each value\n            values_raw = get_python_dict(node.values())\n            values = {}\n            for k, v in values_raw.items():\n                values[k] = convert_node(v)\n            return values\n        elif node.__class__.__name__.startswith('scala.collection.immutable.Map') or \\\n            node.__class__.__name__ == \\\n                'scala.collection.immutable.HashMap$HashTrieMap':\n            values_raw = get_python_dict(node)\n            values = {}\n            for k, v in values_raw.items():\n                values[k] = convert_node(v)\n            return values\n        elif node.__class__.__name__ == 'org.json4s.JsonAST$JArray':\n            entries_raw = get_python_list(node.values())\n            entries = []\n            for entry in entries_raw:\n                entries.append(convert_node(entry))\n            return entries\n        elif node.__class__.__name__ == 'scala.collection.immutable.$colon$colon':\n            entries_raw = get_python_list(node)\n            entries = []\n            for entry in entries_raw:\n                entries.append(convert_node(entry))\n            return entries\n        elif node.__class__.__name__ == 'scala.math.BigInt':\n            return node.intValue()\n        elif node.__class__.__name__ == 'scala.None$':\n            return None\n        elif node.__class__.__name__ == 'scala.collection.immutable.Nil$':\n            return []\n        elif isinstance(node, (str, int, float)):\n            return node\n        else:\n            logger.error('Cannot convert %s into Python' %\n                         node.__class__.__name__)\n            return node.__class__.__name__\n\n    python_json = convert_node(scala_json)\n    return python_json", "response": "Convert a org. json4s. JsonAST into a Python dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the identifier of a heat kernel calculated for a given network.", "response": "def get_heat_kernel(network_id):\n    \"\"\"Return the identifier of a heat kernel calculated for a given network.\n\n    Parameters\n    ----------\n    network_id : str\n        The UUID of the network in NDEx.\n\n    Returns\n    -------\n    kernel_id : str\n        The identifier of the heat kernel calculated for the given network.\n    \"\"\"\n    url = ndex_relevance + '/%s/generate_ndex_heat_kernel' % network_id\n    res = ndex_client.send_request(url, {}, is_json=True, use_get=True)\n    if res is None:\n        logger.error('Could not get heat kernel for network %s.' % network_id)\n        return None\n\n    kernel_id = res.get('kernel_id')\n    if kernel_id is None:\n        logger.error('Could not get heat kernel for network %s.' % network_id)\n        return None\n    return kernel_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_relevant_nodes(network_id, query_nodes):\n    url = ndex_relevance + '/rank_entities'\n    kernel_id = get_heat_kernel(network_id)\n    if kernel_id is None:\n        return None\n    if isinstance(query_nodes, basestring):\n        query_nodes = [query_nodes]\n    params = {'identifier_set': query_nodes,\n              'kernel_id': kernel_id}\n    res = ndex_client.send_request(url, params, is_json=True)\n    if res is None:\n        logger.error(\"ndex_client.send_request returned None.\")\n        return None\n    ranked_entities = res.get('ranked_entities')\n    if ranked_entities is None:\n        logger.error('Could not get ranked entities.')\n        return None\n    return ranked_entities", "response": "Returns a set of network nodes relevant to a given query set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_belief_package(stmt):\n    # This list will contain the belief packages for the given statement\n    belief_packages = []\n    # Iterate over all the support parents\n    for st in stmt.supports:\n        # Recursively get all the belief packages of the parent\n        parent_packages = _get_belief_package(st)\n        package_stmt_keys = [pkg.statement_key for pkg in belief_packages]\n        for package in parent_packages:\n            # Only add this belief package if it hasn't already been added\n            if package.statement_key not in package_stmt_keys:\n                belief_packages.append(package)\n    # Now make the Statement's own belief package and append it to the list\n    belief_package = BeliefPackage(stmt.matches_key(), stmt.evidence)\n    belief_packages.append(belief_package)\n    return belief_packages", "response": "Return the belief packages of a given statement recursively."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of INDRA Statements sampled according to belief scores.", "response": "def sample_statements(stmts, seed=None):\n    \"\"\"Return statements sampled according to belief.\n\n    Statements are sampled independently according to their\n    belief scores. For instance, a Staement with a belief\n    score of 0.7 will end up in the returned Statement list\n    with probability 0.7.\n\n    Parameters\n    ----------\n    stmts : list[indra.statements.Statement]\n        A list of INDRA Statements to sample.\n    seed : Optional[int]\n        A seed for the random number generator used for sampling.\n\n    Returns\n    -------\n    new_stmts : list[indra.statements.Statement]\n        A list of INDRA Statements that were chosen by random sampling\n        according to their respective belief scores.\n    \"\"\"\n    if seed:\n        numpy.random.seed(seed)\n    new_stmts = []\n    r = numpy.random.rand(len(stmts))\n    for i, stmt in enumerate(stmts):\n        if r[i] < stmt.belief:\n            new_stmts.append(stmt)\n    return new_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evidence_random_noise_prior(evidence, type_probs, subtype_probs):\n    (stype, subtype) = tag_evidence_subtype(evidence)\n    # Get the subtype, if available\n\n    # Return the subtype random noise prior, if available\n    if subtype_probs is not None:\n        if stype in subtype_probs:\n            if subtype in subtype_probs[stype]:\n                return subtype_probs[stype][subtype]\n\n    # Fallback to just returning the overall evidence type random noise prior\n    return type_probs[stype]", "response": "Determines the random noise prior probability for this evidence."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn belief score given a list of supporting evidences.", "response": "def score_evidence_list(self, evidences):\n        \"\"\"Return belief score given a list of supporting evidences.\"\"\"\n        def _score(evidences):\n            if not evidences:\n                return 0\n            # Collect all unique sources\n            sources = [ev.source_api for ev in evidences]\n            uniq_sources = numpy.unique(sources)\n            # Calculate the systematic error factors given unique sources\n            syst_factors = {s: self.prior_probs['syst'][s]\n                            for s in uniq_sources}\n            # Calculate the radom error factors for each source\n            rand_factors = {k: [] for k in uniq_sources}\n            for ev in evidences:\n                rand_factors[ev.source_api].append(\n                    evidence_random_noise_prior(\n                        ev,\n                        self.prior_probs['rand'],\n                        self.subtype_probs))\n            # The probability of incorrectness is the product of the\n            # source-specific probabilities\n            neg_prob_prior = 1\n            for s in uniq_sources:\n                neg_prob_prior *= (syst_factors[s] +\n                                   numpy.prod(rand_factors[s]))\n            # Finally, the probability of correctness is one minus incorrect\n            prob_prior = 1 - neg_prob_prior\n            return prob_prior\n        pos_evidence = [ev for ev in evidences if\n                        not ev.epistemics.get('negated')]\n        neg_evidence = [ev for ev in evidences if\n                        ev.epistemics.get('negated')]\n        pp = _score(pos_evidence)\n        np = _score(neg_evidence)\n        # The basic assumption is that the positive and negative evidence\n        # can't simultaneously be correct.\n        # There are two cases to consider. (1) If the positive evidence is\n        # incorrect then there is no Statement and the belief should be 0,\n        # irrespective of the negative evidence.\n        # (2) If the positive evidence is correct and the negative evidence\n        # is incorrect.\n        # This amounts to the following formula:\n        # 0 * (1-pp) + 1 * (pp * (1-np)) which we simplify below\n        score = pp * (1 - np)\n        return score"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef score_statement(self, st, extra_evidence=None):\n        if extra_evidence is None:\n            extra_evidence = []\n        all_evidence = st.evidence + extra_evidence\n        return self.score_evidence_list(all_evidence)", "response": "Computes the prior belief probability for a INDRA Statement."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises an exception if BeliefEngine parameter is missing.", "response": "def check_prior_probs(self, statements):\n        \"\"\"Throw Exception if BeliefEngine parameter is missing.\n\n        Make sure the scorer has all the information needed to compute\n        belief scores of each statement in the provided list, and raises an\n        exception otherwise.\n\n        Parameters\n        ----------\n        statements : list[indra.statements.Statement]\n            List of statements to check\n        \"\"\"\n        sources = set()\n        for stmt in statements:\n            sources |= set([ev.source_api for ev in stmt.evidence])\n        for err_type in ('rand', 'syst'):\n            for source in sources:\n                if source not in self.prior_probs[err_type]:\n                    msg = 'BeliefEngine missing probability parameter' + \\\n                        ' for source: %s' % source\n                    raise Exception(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_probs(self):\n        # We deal with the prior probsfirst\n        # This is a fixed assumed value for systematic error\n        syst_error = 0.05\n        prior_probs = {'syst': {}, 'rand': {}}\n        for source, (p, n) in self.prior_counts.items():\n            # Skip if there are no actual counts\n            if n + p == 0:\n                continue\n            prior_probs['syst'][source] = syst_error\n            prior_probs['rand'][source] = \\\n                1 - min((float(p) / (n + p), 1-syst_error)) - syst_error\n        # Next we deal with subtype probs based on counts\n        subtype_probs = {}\n        for source, entry in self.subtype_counts.items():\n            for rule, (p, n) in entry.items():\n                # Skip if there are no actual counts\n                if n + p == 0:\n                    continue\n                if source not in subtype_probs:\n                    subtype_probs[source] = {}\n                subtype_probs[source][rule] = \\\n                    1 - min((float(p) / (n + p), 1-syst_error)) - syst_error\n        # Finally we propagate this into the full probability\n        # data structures of the parent class\n        super(BayesianScorer, self).update_probs(prior_probs, subtype_probs)", "response": "Update the internal probability values given the counts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_counts(self, prior_counts, subtype_counts):\n        for source, (pos, neg) in prior_counts.items():\n            if source not in self.prior_counts:\n                self.prior_counts[source] = [0, 0]\n            self.prior_counts[source][0] += pos\n            self.prior_counts[source][1] += neg\n        for source, subtype_dict in subtype_counts.items():\n            if source not in self.subtype_counts:\n                self.subtype_counts[source] = {}\n            for subtype, (pos, neg) in subtype_dict.items():\n                if subtype not in self.subtype_counts[source]:\n                    self.subtype_counts[source][subtype] = [0, 0]\n                self.subtype_counts[source][subtype][0] += pos\n                self.subtype_counts[source][subtype][1] += neg\n        self.update_probs()", "response": "Update the internal counts based on given new counts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_prior_probs(self, statements):\n        self.scorer.check_prior_probs(statements)\n        for st in statements:\n            st.belief = self.scorer.score_statement(st)", "response": "Sets the prior probabilities for INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_hierarchy_probs(self, statements):\n        def build_hierarchy_graph(stmts):\n            \"\"\"Return a DiGraph based on matches keys and Statement supports\"\"\"\n            g = networkx.DiGraph()\n            for st1 in stmts:\n                g.add_node(st1.matches_key(), stmt=st1)\n                for st2 in st1.supported_by:\n                    g.add_node(st2.matches_key(), stmt=st2)\n                    g.add_edge(st2.matches_key(), st1.matches_key())\n            return g\n\n        def get_ranked_stmts(g):\n            \"\"\"Return a topological sort of statement matches keys from a graph.\n            \"\"\"\n            node_ranks = networkx.algorithms.dag.topological_sort(g)\n            node_ranks = reversed(list(node_ranks))\n            stmts = [g.node[n]['stmt'] for n in node_ranks]\n            return stmts\n\n        def assert_no_cycle(g):\n            \"\"\"If the graph has cycles, throws AssertionError.\"\"\"\n            try:\n                cyc = networkx.algorithms.cycles.find_cycle(g)\n            except networkx.exception.NetworkXNoCycle:\n                return\n            msg = 'Cycle found in hierarchy graph: %s' % cyc\n            assert False, msg\n\n        g = build_hierarchy_graph(statements)\n        assert_no_cycle(g)\n        ranked_stmts = get_ranked_stmts(g)\n        for st in ranked_stmts:\n            bps = _get_belief_package(st)\n            supporting_evidences = []\n            # NOTE: the last belief package in the list is this statement's own\n            for bp in bps[:-1]:\n                # Iterate over all the parent evidences and add only\n                # non-negated ones\n                for ev in bp.evidences:\n                    if not ev.epistemics.get('negated'):\n                        supporting_evidences.append(ev)\n            # Now add the Statement's own evidence\n            # Now score all the evidences\n            belief = self.scorer.score_statement(st, supporting_evidences)\n            st.belief = belief", "response": "Sets the hierarchical belief probabilities for INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the belief probabilities for a list of INDRA Statements.", "response": "def set_linked_probs(self, linked_statements):\n        \"\"\"Sets the belief probabilities for a list of linked INDRA Statements.\n\n        The list of LinkedStatement objects is assumed to come from the\n        MechanismLinker. The belief probability of the inferred Statement is\n        assigned the joint probability of its source Statements.\n\n        Parameters\n        ----------\n        linked_statements : list[indra.mechlinker.LinkedStatement]\n            A list of INDRA LinkedStatements whose belief scores are to\n            be calculated. The belief attribute of the inferred Statement in\n            the LinkedStatement object is updated by this function.\n        \"\"\"\n        for st in linked_statements:\n            source_probs = [s.belief for s in st.source_stmts]\n            st.inferred_stmt.belief = numpy.prod(source_probs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an INDRA Agent by processing an entity_info dict.", "response": "def get_agent_from_entity_info(entity_info):\n    \"\"\"Return an INDRA Agent by processing an entity_info dict.\"\"\"\n    # This will be the default name. If we get a gene name, it will\n    # override this rawtext name.\n    raw_text = entity_info['entityText']\n    name = raw_text\n\n    # Get the db refs.\n    refs = {'TEXT': raw_text}\n\n    ref_counts = Counter([entry['source'] for entry in\n                          entity_info['entityId']])\n    for source, count in ref_counts.items():\n        if source in ('Entrez', 'UniProt') and count > 1:\n            logger.info('%s has %d entries for %s, skipping'\n                        % (raw_text, count, source))\n            return None, None\n    muts = []\n    for id_dict in entity_info['entityId']:\n        if id_dict['source'] == 'Entrez':\n            refs['EGID'] = id_dict['idString']\n            hgnc_id = hgnc_client.get_hgnc_from_entrez(id_dict['idString'])\n            if hgnc_id is not None:\n                # Check against what we may have already inferred from\n                # UniProt. If it disagrees with this, let it be. Inference\n                # from Entrez isn't as reliable.\n                if 'HGNC' in refs.keys():\n                    if refs['HGNC'] != hgnc_id:\n                        msg = ('HGNC:%s previously set does not'\n                               ' match HGNC:%s from EGID:%s') % \\\n                               (refs['HGNC'], hgnc_id, refs['EGID'])\n                        logger.info(msg)\n                else:\n                    refs['HGNC'] = hgnc_id\n        elif id_dict['source'] == 'UniProt':\n            refs['UP'] = id_dict['idString']\n            gene_name = uniprot_client.get_gene_name(id_dict['idString'])\n            if gene_name is not None:\n                name = gene_name\n                hgnc_id = hgnc_client.get_hgnc_id(gene_name)\n                if hgnc_id is not None:\n                    # Check to see if we have a conflict with an HGNC id\n                    # found from the Entrez id. If so, overwrite with this\n                    # one, in which we have greater faith.\n                    if 'HGNC' in refs.keys() and refs['HGNC'] != hgnc_id:\n                        msg = ('Inferred HGNC:%s from UP:%s does not'\n                               ' match HGNC:%s from EGID:%s') % \\\n                               (refs['HGNC'], refs['UP'], hgnc_id,\n                                refs['EGID'])\n                        logger.info(msg)\n                    refs['HGNC'] = hgnc_id\n        elif id_dict['source'] in ('Tax', 'NCBI'):\n            refs['TAX'] = id_dict['idString']\n        elif id_dict['source'] == 'CHEBI':\n            refs['CHEBI'] = 'CHEBI:%s' % id_dict['idString']\n        # These we take as is\n        elif id_dict['source'] in ('MESH', 'OMIM', 'CTD'):\n            refs[id_dict['source']] = id_dict['idString']\n        # Handle mutations\n        elif id_dict['source'] == 'Unk' and \\\n                id_dict['entityType'] == 'ProteinMutation':\n            # {'idString': 'p|SUB|Y|268|A', 'source': 'Unk',\n            #  'tool': 'PubTator', 'entityType': 'ProteinMutation'}\n            # Mpk1(Y268A)'\n            if id_dict['idString'].startswith('p|SUB|'):\n                try:\n                    # Handle special cases like p|SUB|A|30|P;RS#:104893878\n                    parts = id_dict['idString'].split(';')[0].split('|')\n                    residue_from, pos, residue_to = parts[2:5]\n                    mut = MutCondition(pos, residue_from, residue_to)\n                    muts.append(mut)\n                except Exception as e:\n                    logger.info('Could not process mutation %s' %\n                                id_dict['idString'])\n            else:\n                logger.info('Unhandled mutation: %s' % id_dict['idString'])\n        else:\n            logger.warning(\"Unhandled id type: {source}={idString}\"\n                           .format(**id_dict))\n\n    raw_coords = (entity_info['charStart'], entity_info['charEnd'])\n    return Agent(name, db_refs=refs, mutations=muts), raw_coords"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_statements(self):\n        for p_info in self._json:\n            para = RlimspParagraph(p_info, self.doc_id_type)\n            self.statements.extend(para.get_statements())\n        return", "response": "Extract the statements from the json."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_agent(self, entity_id):\n        if entity_id is None:\n            return None\n\n        entity_info = self._entity_dict.get(entity_id)\n        if entity_info is None:\n            logger.warning(\"Entity key did not resolve to entity.\")\n            return None\n        return get_agent_from_entity_info(entity_info)", "response": "Convert the entity dictionary into an INDRA Agent."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_evidence(self, trigger_id, args, agent_coords, site_coords):\n        trigger_info = self._entity_dict[trigger_id]\n\n        # Get the sentence index from the trigger word.\n        s_idx_set = {self._entity_dict[eid]['sentenceIndex']\n                     for eid in args.values()\n                     if 'sentenceIndex' in self._entity_dict[eid]}\n        if s_idx_set:\n            i_min = min(s_idx_set)\n            i_max = max(s_idx_set)\n\n            text = '. '.join(self._sentences[i_min:(i_max+1)]) + '.'\n\n            s_start = self._sentence_starts[i_min]\n            annotations = {\n                'agents': {'coords': [_fix_coords(coords, s_start)\n                                      for coords in agent_coords]},\n                'trigger': {'coords': _fix_coords([trigger_info['charStart'],\n                                                   trigger_info['charEnd']],\n                                                  s_start)}\n                }\n        else:\n            logger.info('Unable to get sentence index')\n            annotations = {}\n            text = None\n        if site_coords:\n            annotations['site'] = {'coords': _fix_coords(site_coords, s_start)}\n\n        return Evidence(text_refs=self._text_refs.copy(), text=text,\n                        source_api='rlimsp', pmid=self._text_refs.get('PMID'),\n                        annotations=annotations)", "response": "Get the evidence using the info in the trigger entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all childless the descendants of a parent class recursively.", "response": "def get_reader_classes(parent=Reader):\n    \"\"\"Get all childless the descendants of a parent class, recursively.\"\"\"\n    children = parent.__subclasses__()\n    descendants = children[:]\n    for child in children:\n        grandchildren = get_reader_classes(child)\n        if grandchildren:\n            descendants.remove(child)\n            descendants.extend(grandchildren)\n    return descendants"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_reader_class(reader_name):\n    for reader_class in get_reader_classes():\n        if reader_class.name.lower() == reader_name.lower():\n            return reader_class\n    else:\n        logger.error(\"No such reader: %s\" % reader_name)\n        return None", "response": "Get a particular reader class by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_file(cls, file_path, compressed=False, encoded=False):\n        file_id = '.'.join(path.basename(file_path).split('.')[:-1])\n        file_format = file_path.split('.')[-1]\n        content = cls(file_id, file_format, compressed, encoded)\n        content.file_exists = True\n        content._location = path.dirname(file_path)\n        return content", "response": "Create a content object from a file path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_string(cls, id, format, raw_content, compressed=False,\n                    encoded=False):\n        \"\"\"Create a Content object from string/bytes content.\"\"\"\n        content = cls(id, format, compressed, encoded)\n        content._raw_content = raw_content\n        return content", "response": "Create a Content object from a string content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef change_id(self, new_id):\n        self._load_raw_content()\n        self._id = new_id\n        self.get_filename(renew=True)\n        self.get_filepath(renew=True)\n        return", "response": "Change the id of this content."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the format label of this content.", "response": "def change_format(self, new_format):\n        \"\"\"Change the format label of this content.\n\n        Note that this does NOT actually alter the format of the content, only\n        the label.\n        \"\"\"\n        self._load_raw_content()\n        self._format = new_format\n        self.get_filename(renew=True)\n        self.get_filepath(renew=True)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_location(self, new_location):\n        self._load_raw_content()\n        self._location = new_location\n        self.get_filepath(renew=True)\n        return", "response": "Set the location of this content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_text(self):\n        self._load_raw_content()\n        if self._text is None:\n            assert self._raw_content is not None\n            ret_cont = self._raw_content\n            if self.compressed:\n                ret_cont = zlib.decompress(ret_cont, zlib.MAX_WBITS+16)\n            if self.encoded:\n                ret_cont = ret_cont.decode('utf-8')\n            self._text = ret_cont\n        assert self._text is not None\n        return self._text", "response": "Get the loaded decompressed and decoded text of this content."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the filename of this content.", "response": "def get_filename(self, renew=False):\n        \"\"\"Get the filename of this content.\n\n        If the file name doesn't already exist, we created it as {id}.{format}.\n        \"\"\"\n        if self._fname is None or renew:\n            self._fname = '%s.%s' % (self._id, self._format)\n        return self._fname"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the file path for this object.", "response": "def get_filepath(self, renew=False):\n        \"\"\"Get the file path, joining the name and location for this file.\n\n        If no location is given, it is assumed to be \"here\", e.g. \".\".\n        \"\"\"\n        if self._location is None or renew:\n            self._location = '.'\n        return path.join(self._location, self.get_filename())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_result(self, content_id, content, **kwargs):\n        result_object = self.ResultClass(content_id, self.name, self.version,\n                                         formats.JSON, content, **kwargs)\n        self.results.append(result_object)\n        return", "response": "Add a result to the list of results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_content(self, content_str):\n        if self.do_content_check:\n            space_ratio = float(content_str.count(' '))/len(content_str)\n            if space_ratio > self.max_space_ratio:\n                return \"space-ratio: %f > %f\" % (space_ratio,\n                                                 self.max_space_ratio)\n            if len(content_str) > self.input_character_limit:\n                return \"too long: %d > %d\" % (len(content_str),\n                                              self.input_character_limit)\n        return None", "response": "Check if the content is likely to be successfully read."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\njoins different REACH output JSON files into a single JSON object.", "response": "def _join_json_files(cls, prefix, clear=False):\n        \"\"\"Join different REACH output JSON files into a single JSON object.\n\n        The output of REACH is broken into three files that need to be joined\n        before processing. Specifically, there will be three files of the form:\n        `<prefix>.uaz.<subcategory>.json`.\n\n        Parameters\n        ----------\n        prefix : str\n            The absolute path up to the extensions that reach will add.\n        clear : bool\n            Default False - if True, delete the files as soon as they are\n            loaded.\n\n        Returns\n        -------\n        json_obj : dict\n            The result of joining the files, keyed by the three subcategories.\n        \"\"\"\n        filetype_list = ['entities', 'events', 'sentences']\n        json_dict = {}\n        try:\n            for filetype in filetype_list:\n                fname = prefix + '.uaz.' + filetype + '.json'\n                with open(fname, 'rt') as f:\n                    json_dict[filetype] = json.load(f)\n                if clear:\n                    remove(fname)\n                    logger.debug(\"Removed %s.\" % fname)\n        except IOError as e:\n            logger.error(\n                'Failed to open JSON files for %s; REACH error?' % prefix\n                )\n            logger.exception(e)\n            return None\n        return json_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking that the environment supports runnig reach.", "response": "def _check_reach_env():\n        \"\"\"Check that the environment supports runnig reach.\"\"\"\n        # Get the path to the REACH JAR\n        path_to_reach = get_config('REACHPATH')\n        if path_to_reach is None:\n            path_to_reach = environ.get('REACHPATH', None)\n        if path_to_reach is None or not path.exists(path_to_reach):\n            raise ReachError(\n                'Reach path unset or invalid. Check REACHPATH environment var '\n                'and/or config file.'\n                )\n\n        logger.debug('Using REACH jar at: %s' % path_to_reach)\n\n        # Get the reach version.\n        reach_version = get_config('REACH_VERSION')\n        if reach_version is None:\n            reach_version = environ.get('REACH_VERSION', None)\n        if reach_version is None:\n            logger.debug('REACH version not set in REACH_VERSION')\n            m = re.match('reach-(.*?)\\.jar', path.basename(path_to_reach))\n            reach_version = re.sub('-SNAP.*?$', '', m.groups()[0])\n\n        logger.debug('Using REACH version: %s' % reach_version)\n        return path_to_reach, reach_version"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prep_input(self, read_list):\n        logger.info(\"Prepping input.\")\n        i = 0\n        for content in read_list:\n            # Check the quality of the text, and skip if there are any issues.\n            quality_issue = self._check_content(content.get_text())\n            if quality_issue is not None:\n                logger.warning(\"Skipping %d due to: %s\"\n                               % (content.get_id(), quality_issue))\n                continue\n\n            # Look for things that are more like file names, rather than ids.\n            cid = content.get_id()\n            if isinstance(cid, str) and re.match('^\\w*?\\d+$', cid) is None:\n                new_id = 'FILE%06d' % i\n                i += 1\n                self.id_maps[new_id] = cid\n                content.change_id(new_id)\n                new_fpath = content.copy_to(self.input_dir)\n            else:\n                # Put the content in the appropriate directory.\n                new_fpath = content.copy_to(self.input_dir)\n            self.num_input += 1\n            logger.debug('%s saved for reading by reach.'\n                         % new_fpath)\n        return", "response": "Apply the readers to the content."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the output of a reading job as a list of filenames.", "response": "def get_output(self):\n        \"\"\"Get the output of a reading job as a list of filenames.\"\"\"\n        logger.info(\"Getting outputs.\")\n        # Get the set of prefixes (each will correspond to three json files.)\n        json_files = glob.glob(path.join(self.output_dir, '*.json'))\n        json_prefixes = set()\n        for json_file in json_files:\n            # Remove .uaz.<subfile type>.json\n            prefix = '.'.join(path.basename(json_file).split('.')[:-3])\n            json_prefixes.add(path.join(self.output_dir, prefix))\n\n        # Join each set of json files and store the json dict.\n        for prefix in json_prefixes:\n            base_prefix = path.basename(prefix)\n            if base_prefix.isdecimal():\n                base_prefix = int(base_prefix)\n            elif base_prefix in self.id_maps.keys():\n                base_prefix = self.id_maps[base_prefix]\n            try:\n                content = self._join_json_files(prefix, clear=True)\n            except Exception as e:\n                logger.exception(e)\n                logger.error(\"Could not load result for prefix %s.\" % prefix)\n                content = None\n            self.add_result(base_prefix, content)\n            logger.debug('Joined files for prefix %s.' % base_prefix)\n        return self.results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_input(self):\n        for item in listdir(self.input_dir):\n            item_path = path.join(self.input_dir, item)\n            if path.isfile(item_path):\n                remove(item_path)\n                logger.debug('Removed input %s.' % item_path)\n        return", "response": "Remove all the input files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the content of the content file and returns a list of ReadingData objects.", "response": "def read(self, read_list, verbose=False, log=False):\n        \"\"\"Read the content, returning a list of ReadingData objects.\"\"\"\n        ret = []\n        mem_tot = _get_mem_total()\n        if mem_tot is not None and mem_tot <= self.REACH_MEM + self.MEM_BUFFER:\n            logger.error(\n                \"Too little memory to run reach. At least %s required.\" %\n                (self.REACH_MEM + self.MEM_BUFFER)\n                )\n            logger.info(\"REACH not run.\")\n            return ret\n\n        # Prep the content\n        self.prep_input(read_list)\n\n        if self.num_input > 0:\n            # Run REACH!\n            logger.info(\"Beginning reach.\")\n            args = [\n                'java',\n                '-Dconfig.file=%s' % self.conf_file_path,\n                '-jar', self.exec_path\n                ]\n            p = subprocess.Popen(args, stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE)\n            log_file_str = ''\n            for line in iter(p.stdout.readline, b''):\n                log_line = 'REACH: ' + line.strip().decode('utf8')\n                if verbose:\n                    logger.info(log_line)\n                if log:\n                    log_file_str += log_line + '\\n'\n            if log:\n                with open('reach_run.log', 'ab') as f:\n                    f.write(log_file_str.encode('utf8'))\n            p_out, p_err = p.communicate()\n            if p.returncode:\n                logger.error('Problem running REACH:')\n                logger.error('Stdout: %s' % p_out.decode('utf-8'))\n                logger.error('Stderr: %s' % p_err.decode('utf-8'))\n                raise ReachError(\"Problem running REACH\")\n            logger.info(\"Reach finished.\")\n            ret = self.get_output()\n            self.clear_input()\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare the list of files or text content objects to be read.", "response": "def prep_input(self, read_list):\n        \"Prepare the list of files or text content objects to be read.\"\n        logger.info('Prepping input for sparser.')\n\n        self.file_list = []\n\n        for content in read_list:\n            quality_issue = self._check_content(content.get_text())\n            if quality_issue is not None:\n                logger.warning(\"Skipping %d due to: %s\"\n                               % (content.get_id(), quality_issue))\n                continue\n\n            if content.is_format('nxml'):\n                # If it is already an nxml, we just need to adjust the\n                # name a bit, if anything.\n                if not content.get_filename().startswith('PMC'):\n                    content.change_id('PMC' + str(content.get_id()))\n                fpath = content.copy_to(self.tmp_dir)\n                self.file_list.append(fpath)\n            elif content.is_format('txt', 'text'):\n                # Otherwise we need to frame the content in xml and put it\n                # in a new file with the appropriate name.\n                nxml_str = sparser.make_nxml_from_text(content.get_text())\n                new_content = Content.from_string('PMC' + str(content.get_id()),\n                                                  'nxml', nxml_str)\n                fpath = new_content.copy_to(self.tmp_dir)\n                self.file_list.append(fpath)\n            else:\n                raise SparserError(\"Unrecognized format %s.\"\n                                   % content.format)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_output(self, output_files, clear=True):\n        \"Get the output files as an id indexed dict.\"\n        patt = re.compile(r'(.*?)-semantics.*?')\n        for outpath in output_files:\n            if outpath is None:\n                logger.warning(\"Found outpath with value None. Skipping.\")\n                continue\n\n            re_out = patt.match(path.basename(outpath))\n            if re_out is None:\n                raise SparserError(\"Could not get prefix from output path %s.\"\n                                   % outpath)\n            prefix = re_out.groups()[0]\n            if prefix.startswith('PMC'):\n                prefix = prefix[3:]\n            if prefix.isdecimal():\n                # In this case we assume the prefix is a tcid.\n                prefix = int(prefix)\n\n            try:\n                with open(outpath, 'rt') as f:\n                    content = json.load(f)\n            except Exception as e:\n                logger.exception(e)\n                logger.error(\"Could not load reading content from %s.\"\n                             % outpath)\n                content = None\n\n            self.add_result(prefix, content)\n\n            if clear:\n                input_path = outpath.replace('-semantics.json', '.nxml')\n                try:\n                    remove(outpath)\n                    remove(input_path)\n                except Exception as e:\n                    logger.exception(e)\n                    logger.error(\"Could not remove sparser files %s and %s.\"\n                                 % (outpath, input_path))\n        return self.results", "response": "Get the output files as an id indexed dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a few readings.", "response": "def read_some(self, fpath_list, outbuf=None, verbose=False):\n        \"Perform a few readings.\"\n        outpath_list = []\n        for fpath in fpath_list:\n            output, outbuf = self.read_one(fpath, outbuf, verbose)\n            if output is not None:\n                outpath_list.append(output)\n        return outpath_list, outbuf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, read_list, verbose=False, log=False, n_per_proc=None):\n        \"Perform the actual reading.\"\n        ret = []\n        self.prep_input(read_list)\n        L = len(self.file_list)\n        if L == 0:\n            return ret\n\n        logger.info(\"Beginning to run sparser.\")\n        output_file_list = []\n        if log:\n            log_name = 'sparser_run_%s.log' % _time_stamp()\n            outbuf = open(log_name, 'wb')\n        else:\n            outbuf = None\n        try:\n            if self.n_proc == 1:\n                for fpath in self.file_list:\n                    outpath, _ = self.read_one(fpath, outbuf, verbose)\n                    if outpath is not None:\n                        output_file_list.append(outpath)\n            else:\n                if n_per_proc is None:\n                    n_per_proc = max(1, min(1000, L//self.n_proc//2))\n                pool = None\n                try:\n                    pool = Pool(self.n_proc)\n                    if n_per_proc is not 1:\n                        batches = [self.file_list[n*n_per_proc:(n+1)*n_per_proc]\n                                   for n in range(L//n_per_proc + 1)]\n                        out_lists_and_buffs = pool.map(self.read_some,\n                                                       batches)\n                    else:\n                        out_files_and_buffs = pool.map(self.read_one,\n                                                       self.file_list)\n                        out_lists_and_buffs = [([out_files], buffs)\n                                               for out_files, buffs\n                                               in out_files_and_buffs]\n                finally:\n                    if pool is not None:\n                        pool.close()\n                        pool.join()\n                for i, (out_list, buff) in enumerate(out_lists_and_buffs):\n                    if out_list is not None:\n                        output_file_list += out_list\n                    if log:\n                        outbuf.write(b'Log for producing output %d/%d.\\n'\n                                     % (i, len(out_lists_and_buffs)))\n                        if buff is not None:\n                            buff.seek(0)\n                            outbuf.write(buff.read() + b'\\n')\n                        else:\n                            outbuf.write(b'ERROR: no buffer was None. '\n                                         b'No logs available.\\n')\n                        outbuf.flush()\n        finally:\n            if log:\n                outbuf.close()\n                if verbose:\n                    logger.info(\"Sparser logs may be found at %s.\" %\n                                log_name)\n        ret = self.get_output(output_file_list)\n        return ret", "response": "Perform the actual reading."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_text(text, pmid=None, cleanup=True, add_grounding=True):\n    # Create a temporary directory to store the proprocessed input\n    pp_dir = tempfile.mkdtemp('indra_isi_pp_output')\n\n    pp = IsiPreprocessor(pp_dir)\n    extra_annotations = {}\n    pp.preprocess_plain_text_string(text, pmid, extra_annotations)\n\n    # Run the ISI reader and extract statements\n    ip = process_preprocessed(pp)\n    if add_grounding:\n        ip.add_grounding()\n\n    if cleanup:\n        # Remove temporary directory with processed input\n        shutil.rmtree(pp_dir)\n    else:\n        logger.info('Not cleaning up %s' % pp_dir)\n\n    return ip", "response": "Process a string using the ISI reader and extract INDRA statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess an NXML file using the ISI reader and extract INDRA Statements.", "response": "def process_nxml(nxml_filename, pmid=None, extra_annotations=None,\n                 cleanup=True, add_grounding=True):\n    \"\"\"Process an NXML file using the ISI reader\n\n    First converts NXML to plain text and preprocesses it, then runs the ISI\n    reader, and processes the output to extract INDRA Statements.\n\n    Parameters\n    ----------\n    nxml_filename : str\n        nxml file to process\n    pmid : Optional[str]\n        pmid of this nxml file, to be added to the Evidence object of the\n        extracted INDRA statements\n    extra_annotations : Optional[dict]\n        Additional annotations to add to the Evidence object of all extracted\n        INDRA statements. Extra annotations called 'interaction' are ignored\n        since this is used by the processor to store the corresponding\n        raw ISI output.\n    cleanup : Optional[bool]\n        If True, the temporary folders created for preprocessed reading input\n        and output are removed. Default: True\n    add_grounding : Optional[bool]\n        If True the extracted Statements' grounding is mapped\n\n    Returns\n    -------\n    ip : indra.sources.isi.processor.IsiProcessor\n        A processor containing extracted Statements\n    \"\"\"\n    if extra_annotations is None:\n        extra_annotations = {}\n\n    # Create a temporary directory to store the proprocessed input\n    pp_dir = tempfile.mkdtemp('indra_isi_pp_output')\n\n    pp = IsiPreprocessor(pp_dir)\n    extra_annotations = {}\n    pp.preprocess_nxml_file(nxml_filename, pmid, extra_annotations)\n\n    # Run the ISI reader and extract statements\n    ip = process_preprocessed(pp)\n    if add_grounding:\n        ip.add_grounding()\n\n    if cleanup:\n        # Remove temporary directory with processed input\n        shutil.rmtree(pp_dir)\n    else:\n        logger.info('Not cleaning up %s' % pp_dir)\n\n    return ip"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_preprocessed(isi_preprocessor, num_processes=1,\n                         output_dir=None, cleanup=True, add_grounding=True):\n    \"\"\"Process a directory of abstracts and/or papers preprocessed using the\n    specified IsiPreprocessor, to produce a list of extracted INDRA statements.\n\n    Parameters\n    ----------\n    isi_preprocessor : indra.sources.isi.preprocessor.IsiPreprocessor\n        Preprocessor object that has already preprocessed the documents we\n        want to read and process with the ISI reader\n    num_processes : Optional[int]\n        Number of processes to parallelize over\n    output_dir : Optional[str]\n        The directory into which to put reader output; if omitted or None,\n        uses a temporary directory.\n    cleanup : Optional[bool]\n        If True, the temporary folders created for preprocessed reading input\n        and output are removed. Default: True\n    add_grounding : Optional[bool]\n        If True the extracted Statements' grounding is mapped\n\n    Returns\n    -------\n    ip : indra.sources.isi.processor.IsiProcessor\n        A processor containing extracted statements\n    \"\"\"\n\n    # Create a temporary directory to store the output\n    if output_dir is None:\n        output_dir = tempfile.mkdtemp('indra_isi_processor_output')\n    else:\n        output_dir = os.path.abspath(output_dir)\n    tmp_dir = tempfile.mkdtemp('indra_isi_processor_tmp')\n\n    # Form the command to invoke the ISI reader via Docker\n    dir_name = isi_preprocessor.preprocessed_dir\n    # We call realpath on all these paths so that any symbolic links\n    # are generated out - this is needed on Mac\n    input_binding = os.path.realpath(dir_name) + ':/input:ro'\n    output_binding = os.path.realpath(output_dir) + ':/output:rw'\n    tmp_binding = os.path.realpath(tmp_dir) + ':/temp:rw'\n    command = ['docker', 'run', '-it', '--rm',\n               '-v', input_binding, '-v', output_binding, '-v', tmp_binding,\n               'sahilgar/bigmechisi', './myprocesspapers.sh',\n               '-c', str(num_processes)]\n\n    # Invoke the ISI reader\n    logger.info('Running command:')\n    logger.info(' '.join(command))\n    ret = subprocess.call(command)\n    if ret != 0:\n        logger.error('Docker returned non-zero status code')\n\n    ips = []\n    for basename, pmid in isi_preprocessor.pmids.items():\n        fname = os.path.join(output_dir, '%s.json' % basename)\n        ip = process_json_file(fname, pmid=pmid,\n            extra_annotations=isi_preprocessor.extra_annotations.get(fname, {}),\n            add_grounding=False)\n        ips.append(ip)\n\n    # Remove the temporary output directory\n    if output_dir is None:\n        if cleanup:\n            shutil.rmtree(output_dir)\n        else:\n            logger.info('Not cleaning up %s' % output_dir)\n    if cleanup:\n        shutil.rmtree(tmp_dir)\n    else:\n        logger.info('Not cleaning up %s' % output_dir)\n\n    if len(ips) > 1:\n        for ip in ips[1:]:\n            ips[0].statements += ip.statements\n\n    if ips:\n        if add_grounding:\n            ips[0].add_grounding()\n        return ips[0]\n    else:\n        return None", "response": "Processes a directory of abstracts and papers preprocessed using the IsiPreprocessor object that has already preprocessed the INDRA statements and writes them to a temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_output_folder(folder_path, pmids=None, extra_annotations=None,\n                          add_grounding=True):\n    \"\"\"Recursively extracts statements from all ISI output files in the\n    given directory and subdirectories.\n\n    Parameters\n    ----------\n    folder_path : str\n        The directory to traverse\n    pmids : Optional[str]\n        PMID mapping to be added to the Evidence of the extracted INDRA\n        Statements\n    extra_annotations : Optional[dict]\n        Additional annotations to add to the Evidence object of all extracted\n        INDRA statements. Extra annotations called 'interaction' are ignored\n        since this is used by the processor to store the corresponding\n        raw ISI output.\n    add_grounding : Optional[bool]\n        If True the extracted Statements' grounding is mapped\n    \"\"\"\n    pmids = pmids if pmids is not None else {}\n    extra_annotations = extra_annotations if \\\n        extra_annotations is not None else {}\n    ips = []\n    for entry in glob.glob(os.path.join(folder_path, '*.json')):\n        entry_key = os.path.splitext(os.path.basename(entry))[0]\n        # Extract the corresponding file id\n        pmid = pmids.get(entry_key)\n        extra_annotation = extra_annotations.get(entry_key)\n        ip = process_json_file(entry, pmid, extra_annotation, False)\n        ips.append(ip)\n\n    if len(ips) > 1:\n        for ip in ips[1:]:\n            ips[0].statements += ip.statements\n\n    if ips:\n        if add_grounding:\n            ips[0].add_grounding()\n        return ips[0]\n    else:\n        return None", "response": "Recursively extracts INDRA statements from all ISI output files in the folder_path and returns a list of INDRA statements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting statements from the given ISI output file.", "response": "def process_json_file(file_path, pmid=None, extra_annotations=None,\n                      add_grounding=True):\n    \"\"\"Extracts statements from the given ISI output file.\n\n    Parameters\n    ----------\n    file_path : str\n        The ISI output file from which to extract statements\n    pmid : int\n        The PMID of the document being preprocessed, or None if not\n        specified\n    extra_annotations : dict\n        Extra annotations to be added to each statement from this document\n        (can be the empty dictionary)\n    add_grounding : Optional[bool]\n        If True the extracted Statements' grounding is mapped\n    \"\"\"\n    logger.info('Extracting from %s' % file_path)\n    with open(file_path, 'rb') as fh:\n        jd = json.load(fh)\n        ip = IsiProcessor(jd, pmid, extra_annotations)\n        ip.get_statements()\n        if add_grounding:\n            ip.add_grounding()\n        return ip"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing text using the CWMS web service.", "response": "def process_text(text, save_xml='cwms_output.xml'):\n    \"\"\"Processes text using the CWMS web service.\n\n    Parameters\n    ----------\n    text : str\n        Text to process\n\n    Returns\n    -------\n    cp : indra.sources.cwms.CWMSProcessor\n        A CWMSProcessor, which contains a list of INDRA statements in its\n        statements attribute.\n    \"\"\"\n    xml = client.send_query(text, 'cwmsreader')\n\n    # There are actually two EKBs in the xml document. Extract the second.\n    first_end = xml.find('</ekb>')  # End of first EKB\n    second_start = xml.find('<ekb', first_end)  # Start of second EKB\n    second_end = xml.find('</ekb>', second_start)  # End of second EKB\n    second_ekb = xml[second_start:second_end+len('</ekb>')]  # second EKB\n    if save_xml:\n        with open(save_xml, 'wb') as fh:\n            fh.write(second_ekb.encode('utf-8'))\n    return process_ekb(second_ekb)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_ekb_file(fname):\n    # Process EKB XML file into statements\n    with open(fname, 'rb') as fh:\n        ekb_str = fh.read().decode('utf-8')\n    return process_ekb(ekb_str)", "response": "Processes an EKB file produced by CWMS and returns a list of INDRA statements in the base CWMSProcessor."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a networkx graph from Kappy s influence map JSON.", "response": "def im_json_to_graph(im_json):\n    \"\"\"Return networkx graph from Kappy's influence map JSON.\n\n    Parameters\n    ----------\n    im_json : dict\n        A JSON dict which contains an influence map generated by Kappy.\n\n    Returns\n    -------\n    graph : networkx.MultiDiGraph\n        A graph representing the influence map.\n    \"\"\"\n    imap_data = im_json['influence map']['map']\n\n    # Initialize the graph\n    graph = MultiDiGraph()\n\n    id_node_dict = {}\n    # Add each node to the graph\n    for node_dict in imap_data['nodes']:\n        # There is always just one entry here with the node type e.g. \"rule\"\n        # as key, and all the node data as the value\n        node_type, node = list(node_dict.items())[0]\n        # Add the node to the graph with its label and type\n        attrs = {'fillcolor': '#b7d2ff' if node_type == 'rule' else '#cdffc9',\n                 'shape': 'box' if node_type == 'rule' else 'oval',\n                 'style': 'filled'}\n        graph.add_node(node['label'], node_type=node_type, **attrs)\n        # Save the key of the node to refer to it later\n        new_key = '%s%s' % (node_type, node['id'])\n        id_node_dict[new_key] = node['label']\n\n    def add_edges(link_list, edge_sign):\n        attrs = {'sign': edge_sign,\n                 'color': 'green' if edge_sign == 1 else 'red',\n                 'arrowhead': 'normal' if edge_sign == 1 else 'tee'}\n        for link_dict in link_list:\n            source = link_dict['source']\n            for target_dict in link_dict['target map']:\n                target = target_dict['target']\n                src_id = '%s%s' % list(source.items())[0]\n                tgt_id = '%s%s' % list(target.items())[0]\n                graph.add_edge(id_node_dict[src_id], id_node_dict[tgt_id],\n                               **attrs)\n\n    # Add all the edges from the positive and negative influences\n    add_edges(imap_data['wake-up map'], 1)\n    add_edges(imap_data['inhibition map'], -1)\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns pygraphviz. Agraph from Kappy s contact map JSON.", "response": "def cm_json_to_graph(im_json):\n    \"\"\"Return pygraphviz Agraph from Kappy's contact map JSON.\n\n    Parameters\n    ----------\n    im_json : dict\n        A JSON dict which contains a contact map generated by Kappy.\n\n    Returns\n    -------\n    graph : pygraphviz.Agraph\n        A graph representing the contact map.\n    \"\"\"\n    cmap_data = im_json['contact map']['map']\n\n    # Initialize the graph\n    graph = AGraph()\n\n    # In this loop we add sites as nodes and clusters around sites to the\n    # graph. We also collect edges to be added between sites later.\n    edges = []\n    for node_idx, node in enumerate(cmap_data):\n        sites_in_node = []\n        for site_idx, site in enumerate(node['node_sites']):\n            # We map the unique ID of the site to its name\n            site_key = (node_idx, site_idx)\n            sites_in_node.append(site_key)\n            graph.add_node(site_key, label=site['site_name'], style='filled',\n                           shape='ellipse')\n            # Each port link is an edge from the current site to the\n            # specified site\n            if not site['site_type'] or not site['site_type'][0] == 'port':\n                continue\n            for port_link in site['site_type'][1]['port_links']:\n                edge = (site_key, tuple(port_link))\n                edges.append(edge)\n        graph.add_subgraph(sites_in_node,\n                           name='cluster_%s' % node['node_type'],\n                           label=node['node_type'])\n\n    # Finally we add the edges between the sites\n    for source, target in edges:\n        graph.add_edge(source, target)\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_email(M, msg_id):\n    res, data = M.fetch(msg_id, '(RFC822)')\n    if res == 'OK':\n        # Data here is a list with 1 element containing a tuple\n        # whose 2nd element is a long string containing the email\n        # The content is a bytes that must be decoded\n        raw_msg_txt = data[0][1]\n        # In Python3, we call message_from_bytes, but this function doesn't\n        # exist in Python 2.\n        try:\n            msg = email.message_from_bytes(raw_msg_txt)\n        except AttributeError:\n            msg = email.message_from_string(raw_msg_txt)\n        # At this point, we have a message containing bytes (not unicode)\n        # fields that will still need to be decoded, ideally according to the\n        # character set specified in the message.\n        return msg\n    else:\n        return None", "response": "Returns the given email message as a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_headers(msg):\n    headers = {}\n    for k in msg.keys():\n        # decode_header decodes header but does not convert charset, so these\n        # may still be bytes, even in Python 3. However, if it's ASCII\n        # only (hence unambiguous encoding), the header fields come back\n        # as str (unicode) in Python 3.\n        (header_txt, charset) = email.header.decode_header(msg[k])[0]\n        if charset is not None:\n            header_txt = header_txt.decode(charset)\n        headers[k] = header_txt\n    return headers", "response": "Takes email. message. Message object initialized from unicode string returns dict with header fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate_config_dict(config_path):\n    try:\n        config_dict = {}\n        parser = RawConfigParser()\n        parser.optionxform = lambda x: x\n        parser.read(config_path)\n        sections = parser.sections()\n        for section in sections:\n            options = parser.options(section)\n            for option in options:\n                config_dict[option] = str(parser.get(section, option))\n    except Exception as e:\n        logger.warning(\"Could not load configuration file due to exception. \"\n                       \"Only environment variable equivalents will be used.\")\n        return None\n\n    for key in config_dict.keys():\n        if config_dict[key] == '':\n            config_dict[key] = None\n        elif isinstance(config_dict[key], str):\n            config_dict[key] = os.path.expanduser(config_dict[key])\n    return config_dict", "response": "Load the configuration file into the config_file dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_config(key, failure_ok=True):\n    err_msg = \"Key %s not in environment or config file.\" % key\n    if key in os.environ:\n        return os.environ[key]\n    elif key in CONFIG_DICT:\n        val = CONFIG_DICT[key]\n        # We interpret an empty value in the config file as a failure\n        if val is None and not failure_ok:\n            msg = 'Key %s is set to an empty value in config file.' % key\n            raise IndraConfigError(msg)\n        else:\n            return val\n    elif not failure_ok:\n        raise IndraConfigError(err_msg)\n    else:\n        logger.warning(err_msg)\n        return None", "response": "Get the value by key from the config file or environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the unicode fileobj into a generator of unicode strings.", "response": "def read_unicode_csv_fileobj(fileobj, delimiter=',', quotechar='\"',\n                             quoting=csv.QUOTE_MINIMAL, lineterminator='\\n',\n                             encoding='utf-8', skiprows=0):\n    \"\"\"fileobj can be a StringIO in Py3, but should be a BytesIO in Py2.\"\"\"\n    # Python 3 version\n    if sys.version_info[0] >= 3:\n        # Next, get the csv reader, with unicode delimiter and quotechar\n        csv_reader = csv.reader(fileobj, delimiter=delimiter,\n                                quotechar=quotechar, quoting=quoting,\n                                lineterminator=lineterminator)\n        # Now, return the (already decoded) unicode csv_reader generator\n        # Skip rows if necessary\n        for skip_ix in range(skiprows):\n            next(csv_reader)\n        for row in csv_reader:\n            yield row\n    # Python 2 version\n    else:\n        # Next, get the csv reader, passing delimiter and quotechar as\n        # bytestrings rather than unicode\n        csv_reader = csv.reader(fileobj, delimiter=delimiter.encode(encoding),\n                             quotechar=quotechar.encode(encoding),\n                             quoting=quoting, lineterminator=lineterminator)\n        # Iterate over the file and decode each string into unicode\n        # Skip rows if necessary\n        for skip_ix in range(skiprows):\n            next(csv_reader)\n        for row in csv_reader:\n            yield [cell.decode(encoding) for cell in row]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fast_deepcopy(obj):\n    with BytesIO() as buf:\n        pickle.dump(obj, buf)\n        buf.seek(0)\n        obj_new = pickle.load(buf)\n    return obj_new", "response": "This is a fast implementation of deepcopy via pickle."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten(l):\n    return sum(map(flatten, l), []) \\\n        if isinstance(l, list) or isinstance(l, tuple) else [l]", "response": "Flatten a nested list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef batch_iter(iterator, batch_size, return_func=None, padding=None):\n    for batch in zip_longest(*[iter(iterator)]*batch_size, fillvalue=padding):\n        gen = (thing for thing in batch if thing is not padding)\n        if return_func is None:\n            yield gen\n        else:\n            yield return_func(gen)", "response": "Yields a list of items from an iterable into batches of size batch_size."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_pmid_sentences(pmid_sentences, **drum_args):\n    def _set_pmid(statements, pmid):\n        for stmt in statements:\n            for evidence in stmt.evidence:\n                evidence.pmid = pmid\n\n    # See if we need to start DRUM as a subprocess\n    run_drum = drum_args.get('run_drum', False)\n    drum_process = None\n    all_statements = {}\n    # Iterate over all the keys and sentences to read\n    for pmid, sentences in pmid_sentences.items():\n        logger.info('================================')\n        logger.info('Processing %d sentences for %s' % (len(sentences), pmid))\n        ts = time.time()\n        # Make a DrumReader instance\n        drum_args['name'] = 'DrumReader%s' % pmid\n        dr = DrumReader(**drum_args)\n        time.sleep(3)\n        # If there is no DRUM process set yet, we get the one that was\n        # just started by the DrumReader\n        if run_drum and drum_process is None:\n            drum_args.pop('run_drum', None)\n            drum_process = dr.drum_system\n            # By setting this, we ensuer that the reference to the\n            # process is passed in to all future DrumReaders\n            drum_args['drum_system'] = drum_process\n        # Now read each sentence for this key\n        for sentence in sentences:\n            dr.read_text(sentence)\n        # Start receiving results and exit when done\n        try:\n            dr.start()\n        except SystemExit:\n            pass\n        statements = []\n        # Process all the extractions into INDRA Statements\n        for extraction in dr.extractions:\n            # Sometimes we get nothing back\n            if not extraction:\n                continue\n            tp = process_xml(extraction)\n            statements += tp.statements\n        # Set the PMIDs for the evidences of the Statements\n        _set_pmid(statements, pmid)\n        te = time.time()\n        logger.info('Reading took %d seconds and produced %d Statements.' %\n                    (te-ts, len(statements)))\n        all_statements[pmid] = statements\n    # If we were running a DRUM process, we should kill it\n    if drum_process and dr.drum_system:\n        dr._kill_drum()\n    return all_statements", "response": "Read sentences from a dictonary and return all INDRA Statements for a given PMID."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a graph query on PathwayCommons.", "response": "def graph_query(kind, source, target=None, neighbor_limit=1,\n                database_filter=None):\n    \"\"\"Perform a graph query on PathwayCommons.\n\n    For more information on these queries, see\n    http://www.pathwaycommons.org/pc2/#graph\n\n    Parameters\n    ----------\n    kind : str\n        The kind of graph query to perform. Currently 3 options are\n        implemented, 'neighborhood', 'pathsbetween' and 'pathsfromto'.\n    source : list[str]\n        A list of gene names which are the source set for the graph query.\n    target : Optional[list[str]]\n        A list of gene names which are the target set for the graph query.\n        Only needed for 'pathsfromto' queries.\n    neighbor_limit : Optional[int]\n        This limits the length of the longest path considered in\n        the graph query. Default: 1\n\n    Returns\n    -------\n    model : org.biopax.paxtools.model.Model\n        A BioPAX model (java object).\n    \"\"\"\n\n    default_databases = ['wp', 'smpdb', 'reconx', 'reactome', 'psp', 'pid',\n                         'panther', 'netpath', 'msigdb', 'mirtarbase', 'kegg',\n                         'intact', 'inoh', 'humancyc', 'hprd',\n                         'drugbank', 'dip', 'corum']\n    if not database_filter:\n        query_databases = default_databases\n    else:\n        query_databases = database_filter\n    # excluded: ctd\n\n    params = {}\n    params['format'] = 'BIOPAX'\n    params['organism'] = '9606'\n    params['datasource'] = query_databases\n    # Get the \"kind\" string\n    kind_str = kind.lower()\n    if kind not in ['neighborhood', 'pathsbetween', 'pathsfromto']:\n        logger.warn('Invalid query type %s' % kind_str)\n        return None\n    params['kind'] = kind_str\n    # Get the source string\n    if isinstance(source, basestring):\n        source_str = source\n    else:\n        source_str = ','.join(source)\n    params['source'] = source_str\n    try:\n        neighbor_limit = int(neighbor_limit)\n        params['limit'] = neighbor_limit\n    except (TypeError, ValueError):\n        logger.warn('Invalid neighborhood limit %s' % neighbor_limit)\n        return None\n    if target is not None:\n        if isinstance(target, basestring):\n            target_str = target\n        else:\n            target_str = ','.join(target)\n        params['target'] = target_str\n    logger.info('Sending Pathway Commons query with parameters: ')\n    for k, v in params.items():\n        logger.info(' %s: %s' % (k, v))\n\n    logger.info('Sending Pathway Commons query...')\n    res = requests.get(pc2_url + 'graph', params=params)\n    if not res.status_code == 200:\n        logger.error('Response is HTTP code %d.' % res.status_code)\n        if res.status_code == 500:\n            logger.error('Note: HTTP code 500 can mean empty '\n                         'results for a valid query.')\n        return None\n    # We don't decode to Unicode here because owl_str_to_model expects\n    # a byte stream\n    model = owl_str_to_model(res.content)\n    if model is not None:\n        logger.info('Pathway Commons query returned a model...')\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef owl_str_to_model(owl_str):\n    io_class = autoclass('org.biopax.paxtools.io.SimpleIOHandler')\n    io = io_class(autoclass('org.biopax.paxtools.model.BioPAXLevel').L3)\n    bais = autoclass('java.io.ByteArrayInputStream')\n    scs = autoclass('java.nio.charset.StandardCharsets')\n    jstr = autoclass('java.lang.String')\n    istream = bais(owl_str)\n    biopax_model = io.convertFromOWL(istream)\n    return biopax_model", "response": "Return a BioPAX model object from an OWL string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef owl_to_model(fname):\n    io_class = autoclass('org.biopax.paxtools.io.SimpleIOHandler')\n    io = io_class(autoclass('org.biopax.paxtools.model.BioPAXLevel').L3)\n\n    try:\n        file_is = autoclass('java.io.FileInputStream')(fname)\n    except JavaException:\n        logger.error('Could not open data file %s' % fname)\n        return\n    try:\n        biopax_model = io.convertFromOWL(file_is)\n    except JavaException as e:\n        logger.error('Could not convert data file %s to BioPax model' % fname)\n        logger.error(e)\n        return\n\n    file_is.close()\n\n    return biopax_model", "response": "Returns a BioPAX model object from an OWL file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef model_to_owl(model, fname):\n    io_class = autoclass('org.biopax.paxtools.io.SimpleIOHandler')\n    io = io_class(autoclass('org.biopax.paxtools.model.BioPAXLevel').L3)\n\n    try:\n        fileOS = autoclass('java.io.FileOutputStream')(fname)\n    except JavaException:\n        logger.error('Could not open data file %s' % fname)\n        return\n    l3_factory = autoclass('org.biopax.paxtools.model.BioPAXLevel').L3.getDefaultFactory()\n    model_out = l3_factory.createModel()\n    for r in model.getObjects().toArray():\n        model_out.add(r)\n    io.convertToOWL(model_out, fileOS)\n\n    fileOS.close()", "response": "Save a BioPAX model object as an OWL file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_model(self, *args, **kwargs):\n        for stmt in self.statements:\n            if isinstance(stmt, RegulateActivity):\n                self._add_regulate_activity(stmt)\n            elif isinstance(stmt, RegulateAmount):\n                self._add_regulate_amount(stmt)\n            elif isinstance(stmt, Modification):\n                self._add_modification(stmt)\n            elif isinstance(stmt, SelfModification):\n                self._add_selfmodification(stmt)\n            elif isinstance(stmt, Gef):\n                self._add_gef(stmt)\n            elif isinstance(stmt, Gap):\n                self._add_gap(stmt)\n            elif isinstance(stmt, Complex):\n                self._add_complex(stmt)\n            else:\n                logger.warning('Unhandled statement type: %s' %\n                               stmt.__class__.__name__)\n        if kwargs.get('grouping'):\n            self._group_nodes()\n            self._group_edges()\n        return self.print_cyjs_graph()", "response": "Assemble a Cytoscape JS model from INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_gene_names(self):\n        # Collect all gene names in network\n        gene_names = []\n        for node in self._nodes:\n            members = node['data'].get('members')\n            if members:\n                gene_names += list(members.keys())\n            else:\n                if node['data']['name'].startswith('Group'):\n                    continue\n                gene_names.append(node['data']['name'])\n        self._gene_names = gene_names", "response": "Gather all gene names of all nodes and node members"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_CCLE_context(self, cell_types):\n        self.get_gene_names()\n\n        # Get expression and mutations from context client\n        exp_values = \\\n            context_client.get_protein_expression(self._gene_names, cell_types)\n        mut_values = \\\n            context_client.get_mutations(self._gene_names, cell_types)\n\n        # Make a dict of presence/absence of mutations\n        muts = {cell_line: {} for cell_line in cell_types}\n        for cell_line, entries in mut_values.items():\n            if entries is not None:\n                for gene, mutations in entries.items():\n                    if mutations:\n                        muts[cell_line][gene] = 1\n                    else:\n                        muts[cell_line][gene] = 0\n\n        # Create bins for the exp values\n        # because colorbrewer only does 3-9 bins and I don't feel like\n        # reinventing color scheme theory, this will only bin 3-9 bins\n        def bin_exp(expression_dict):\n            d = expression_dict\n            exp_values = []\n            for line in d:\n                for gene in d[line]:\n                    val = d[line][gene]\n                    if val is not None:\n                        exp_values.append(val)\n            thr_dict = {}\n            for n_bins in range(3, 10):\n                bin_thr = np.histogram(np.log10(exp_values), n_bins)[1][1:]\n                thr_dict[n_bins] = bin_thr\n            # this dict isn't yet binned, that happens in the loop\n            binned_dict = {x: deepcopy(expression_dict) for x in range(3, 10)}\n            for n_bins in binned_dict:\n                for line in binned_dict[n_bins]:\n                    for gene in binned_dict[n_bins][line]:\n                        # last bin is reserved for None\n                        if binned_dict[n_bins][line][gene] is None:\n                            binned_dict[n_bins][line][gene] = n_bins\n                        else:\n                            val = np.log10(binned_dict[n_bins][line][gene])\n                            for thr_idx, thr in enumerate(thr_dict[n_bins]):\n                                if val <= thr:\n                                    binned_dict[n_bins][line][gene] = thr_idx\n                                    break\n            return binned_dict\n        binned_exp = bin_exp(exp_values)\n\n        context = {'bin_expression': binned_exp,\n                   'mutation': muts}\n        self._context['CCLE'] = context", "response": "Set the context of all nodes and node members from CCLE."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the assembled Cytoscape JS network as a json string.", "response": "def print_cyjs_graph(self):\n        \"\"\"Return the assembled Cytoscape JS network as a json string.\n\n        Returns\n        -------\n        cyjs_str : str\n            A json string representation of the Cytoscape JS network.\n        \"\"\"\n        cyjs_dict = {'edges': self._edges, 'nodes': self._nodes}\n        cyjs_str = json.dumps(cyjs_dict, indent=1, sort_keys=True)\n        return cyjs_str"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_cyjs_context(self):\n        context = self._context\n        context_str = json.dumps(context, indent=1, sort_keys=True)\n        return context_str", "response": "Return a list of node names and their respective context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the assembled Cytoscape JS network in a json file.", "response": "def save_json(self, fname_prefix='model'):\n        \"\"\"Save the assembled Cytoscape JS network in a json file.\n\n        This method saves two files based on the file name prefix given.\n        It saves one json file with the graph itself, and another json\n        file with the context.\n\n        Parameters\n        ----------\n        fname_prefix : Optional[str]\n            The prefix of the files to save the Cytoscape JS network and\n            context to.\n            Default: model\n        \"\"\"\n        cyjs_str = self.print_cyjs_graph()\n        # outputs the graph\n        with open(fname_prefix + '.json', 'wb') as fh:\n            fh.write(cyjs_str.encode('utf-8'))\n        # outputs the context of graph nodes\n        context_str = self.print_cyjs_context()\n        with open(fname_prefix + '_context.json', 'wb') as fh:\n            fh.write(context_str.encode('utf-8'))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_model(self, fname='model.js'):\n        exp_colorscale_str = json.dumps(self._exp_colorscale)\n        mut_colorscale_str = json.dumps(self._mut_colorscale)\n        cyjs_dict = {'edges': self._edges, 'nodes': self._nodes}\n        model_str = json.dumps(cyjs_dict, indent=1, sort_keys=True)\n        model_dict = {'exp_colorscale_str': exp_colorscale_str,\n                      'mut_colorscale_str': mut_colorscale_str,\n                      'model_elements_str': model_str}\n        s = ''\n        s += 'var exp_colorscale = %s;\\n' % model_dict['exp_colorscale_str']\n        s += 'var mut_colorscale = %s;\\n' % model_dict['mut_colorscale_str']\n        s += 'var model_elements = %s;\\n' % model_dict['model_elements_str']\n        with open(fname, 'wb') as fh:\n            fh.write(s.encode('utf-8'))", "response": "Save the assembled Cytoscape JS network in a js file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_edge_dict(self):\n        edge_dict = collections.defaultdict(lambda: [])\n        if len(self._edges) > 0:\n            for e in self._edges:\n                data = e['data']\n                key = tuple([data['i'], data['source'],\n                            data['target'], data['polarity']])\n                edge_dict[key] = data['id']\n        return edge_dict", "response": "Return a dict of edges.\n        Keyed tuples of source target polarity and edge ids with lists of edge ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_node_key(self, node_dict_item):\n        s = tuple(sorted(node_dict_item['sources']))\n        t = tuple(sorted(node_dict_item['targets']))\n        return (s, t)", "response": "Return a tuple of sorted sources and targets given a node dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_node_groups(self):\n        node_dict = {node['data']['id']: {'sources': [], 'targets': []}\n                     for node in self._nodes}\n        for edge in self._edges:\n            # Add edge as a source for its target node\n            edge_data = (edge['data']['i'], edge['data']['polarity'],\n                         edge['data']['source'])\n            node_dict[edge['data']['target']]['sources'].append(edge_data)\n            # Add edge as target for its source node\n            edge_data = (edge['data']['i'], edge['data']['polarity'],\n                         edge['data']['target'])\n            node_dict[edge['data']['source']]['targets'].append(edge_data)\n        # Make a dictionary of nodes based on source/target as a key\n        node_key_dict = collections.defaultdict(lambda: [])\n        for node_id, node_d in node_dict.items():\n            key = self._get_node_key(node_d)\n            node_key_dict[key].append(node_id)\n        # Constrain the groups to ones that have more than 1 member\n        node_groups = [g for g in node_key_dict.values() if (len(g) > 1)]\n        return node_groups", "response": "Return a list of node id lists that are topologically identical."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _group_edges(self):\n        # edit edges on parent nodes and make new edges for them\n        edges_to_add = [[], []]  # [group_edges, uuid_lists]\n        for e in self._edges:\n            new_edge = deepcopy(e)\n            new_edge['data'].pop('id', None)\n            uuid_list = new_edge['data'].pop('uuid_list', [])\n            # Check if edge source or target are contained in a parent\n            # If source or target in parent edit edge\n            # Nodes may only point within their container\n            source = e['data']['source']\n            target = e['data']['target']\n            source_node = [x for x in self._nodes if\n                           x['data']['id'] == source][0]\n            target_node = [x for x in self._nodes if\n                           x['data']['id'] == target][0]\n            # If the source node is in a group, we change the source of this\n            # edge to the group\n            if source_node['data']['parent'] != '':\n                new_edge['data']['source'] = source_node['data']['parent']\n                e['data']['i'] = 'Virtual'\n            # If the targete node is in a group, we change the target of this\n            # edge to the group\n            if target_node['data']['parent'] != '':\n                new_edge['data']['target'] = target_node['data']['parent']\n                e['data']['i'] = 'Virtual'\n            if e['data']['i'] == 'Virtual':\n                if new_edge not in edges_to_add[0]:\n                    edges_to_add[0].append(new_edge)\n                    edges_to_add[1].append(uuid_list)\n                else:\n                    idx = edges_to_add[0].index(new_edge)\n                    edges_to_add[1][idx] += uuid_list\n                    edges_to_add[1][idx] = list(set(edges_to_add[1][idx]))\n        for ze in zip(*edges_to_add):\n            edge = ze[0]\n            edge['data']['id'] = self._get_new_id()\n            edge['data']['uuid_list'] = ze[1]\n            self._edges.append(edge)", "response": "Group all edges that are topologically identical."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_stmt(stmt_cls, tf_agent, target_agent, pmid):\n    ev = Evidence(source_api='trrust', pmid=pmid)\n    return stmt_cls(deepcopy(tf_agent), deepcopy(target_agent),\n                    evidence=[ev])", "response": "Return a Statement based on its type agents and PMID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_grounded_agent(gene_name):\n    db_refs = {'TEXT': gene_name}\n    if gene_name in hgnc_map:\n        gene_name = hgnc_map[gene_name]\n    hgnc_id = hgnc_client.get_hgnc_id(gene_name)\n    if hgnc_id:\n        db_refs['HGNC'] = hgnc_id\n        up_id = hgnc_client.get_uniprot_id(hgnc_id)\n        if up_id:\n            db_refs['UP'] = up_id\n    agent = Agent(gene_name, db_refs=db_refs)\n    return agent", "response": "Return a grounded Agent based on an HGNC symbol."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_statements(self):\n        for _, (tf, target, effect, refs) in self.df.iterrows():\n            tf_agent = get_grounded_agent(tf)\n            target_agent = get_grounded_agent(target)\n            if effect == 'Activation':\n                stmt_cls = IncreaseAmount\n            elif effect == 'Repression':\n                stmt_cls = DecreaseAmount\n            else:\n                continue\n            pmids = refs.split(';')\n            for pmid in pmids:\n                stmt = make_stmt(stmt_cls, tf_agent, target_agent, pmid)\n                self.statements.append(stmt)", "response": "Process the table to extract Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_paper(model_name, pmid):\n    json_directory = os.path.join(model_name, 'jsons')\n    json_path = os.path.join(json_directory, 'PMID%s.json' % pmid)\n\n    if pmid.startswith('api') or pmid.startswith('PMID'):\n        logger.warning('Invalid PMID: %s' % pmid)\n    # If the paper has been read, use the json output file\n    if os.path.exists(json_path):\n        rp = reach.process_json_file(json_path, citation=pmid)\n        txt_format = 'existing_json'\n    # If the paper has not been read, download the text and read\n    else:\n        try:\n            txt, txt_format = get_full_text(pmid, 'pmid')\n        except Exception:\n            return None, None\n\n        if txt_format == 'pmc_oa_xml':\n            rp = reach.process_nxml_str(txt, citation=pmid, offline=True,\n                                        output_fname=json_path)\n        elif txt_format == 'elsevier_xml':\n            # Extract the raw text from the Elsevier XML\n            txt = elsevier_client.extract_text(txt)\n            rp = reach.process_text(txt, citation=pmid, offline=True,\n                                    output_fname=json_path)\n        elif txt_format == 'abstract':\n            rp = reach.process_text(txt, citation=pmid, offline=True,\n                                    output_fname=json_path)\n        else:\n            rp = None\n    if rp is not None:\n        check_pmids(rp.statements)\n    return rp, txt_format", "response": "Process a paper with the given pubmed identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap processing a paper by either a local service or remote service .", "response": "def process_paper_helper(model_name, pmid, start_time_local):\n    \"\"\"Wraps processing a paper by either a local or remote service\n    and caches any uncaught exceptions\"\"\"\n    try:\n        if not aws_available:\n            rp, txt_format = process_paper(model_name, pmid)\n        else:\n            rp, txt_format = process_paper_aws(pmid, start_time_local)\n    except:\n        logger.exception('uncaught exception while processing %s', pmid)\n        return None, None\n\n    return rp, txt_format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_with_search_helper(model_path, config, num_days=None):\n    logger.info('-------------------------')\n    logger.info(time.strftime('%c'))\n\n    if not os.path.isdir(model_path):\n        logger.error('%s is not a directory', model_path)\n        sys.exit()\n\n    default_config_fname = os.path.join(model_path, 'config.yaml')\n\n    if config:\n        config = get_machine_config(config)\n    elif os.path.exists(default_config_fname):\n        logger.info('Loading default configuration from %s',\n                    default_config_fname)\n        config = get_machine_config(default_config_fname)\n    else:\n        logger.error('Configuration file argument missing.')\n        sys.exit()\n\n    # Probability cutoff for filtering statements\n    default_belief_threshold = 0.95\n    belief_threshold = config.get('belief_threshold')\n    if belief_threshold is None:\n        belief_threshold = default_belief_threshold\n        msg = 'Belief threshold argument (belief_threshold) not specified.' + \\\n              ' Using default belief threshold %.2f' % default_belief_threshold\n        logger.info(msg)\n    else:\n        logger.info('Using belief threshold: %.2f' % belief_threshold)\n\n    twitter_cred = get_twitter_cred(config)\n    if twitter_cred:\n        logger.info('Using Twitter with given credentials.')\n    else:\n        logger.info('Not using Twitter due to missing credentials.')\n\n    gmail_cred = get_gmail_cred(config)\n    if gmail_cred:\n        logger.info('Using Gmail with given credentials.')\n    else:\n        logger.info('Not using Gmail due to missing credentials.')\n\n    ndex_cred = get_ndex_cred(config)\n    if ndex_cred:\n        logger.info('Using NDEx with given credentials.')\n    else:\n        logger.info('Not using NDEx due to missing information.')\n\n    pmids = {}\n    # Get email PMIDs\n    if gmail_cred:\n        logger.info('Getting PMIDs from emails.')\n        try:\n            email_pmids = get_email_pmids(gmail_cred)\n            # Put the email_pmids into the pmids dictionary\n            pmids['Gmail'] = email_pmids\n            logger.info('Collected %d PMIDs from Gmail', len(email_pmids))\n        except Exception:\n            logger.exception('Could not get PMIDs from Gmail, continuing.')\n\n    # Get PMIDs for general search_terms and genes\n    search_genes = config.get('search_genes')\n    search_terms = config.get('search_terms')\n    if not search_terms:\n        logger.info('No search terms argument (search_terms) specified.')\n    else:\n        if search_genes is not None:\n            search_terms += search_genes\n        logger.info('Using search terms: %s' % ', '.join(search_terms))\n\n        if num_days is None:\n            num_days = int(config.get('search_terms_num_days', 5))\n        logger.info('Searching the last %d days', num_days)\n\n        pmids_term = get_searchterm_pmids(search_terms, num_days=num_days)\n        num_pmids = len(set(itt.chain.from_iterable(pmids_term.values())))\n        logger.info('Collected %d PMIDs from PubMed search_terms.', num_pmids)\n        pmids = _extend_dict(pmids, pmids_term)\n\n    # Get optional grounding map\n    gm_path = config.get('grounding_map_path')\n    if gm_path:\n        try:\n            from indra.preassembler.grounding_mapper import load_grounding_map\n            grounding_map = load_grounding_map(gm_path)\n        except Exception as e:\n            logger.error('Could not load grounding map from %s' % gm_path)\n            logger.error(e)\n            grounding_map = None\n    else:\n        grounding_map = None\n\n    '''\n    # Get PMIDs for search_genes\n    # Temporarily removed because Entrez-based article searches\n    # are lagging behind and cannot be time-limited\n    if not search_genes:\n        logger.info('No search genes argument (search_genes) specified.')\n    else:\n        logger.info('Using search genes: %s' % ', '.join(search_genes))\n        pmids_gene = get_searchgenes_pmids(search_genes, num_days=5)\n        num_pmids = sum([len(pm) for pm in pmids_gene.values()])\n        logger.info('Collected %d PMIDs from PubMed search_genes.' % num_pmids)\n        pmids = _extend_dict(pmids, pmids_gene)\n    '''\n    run_machine(\n        model_path,\n        pmids,\n        belief_threshold,\n        search_genes=search_genes,\n        ndex_cred=ndex_cred,\n        twitter_cred=twitter_cred,\n        grounding_map=grounding_map\n    )", "response": "Run a search helper for the given model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _load_data():\n    # Get the cwv reader object.\n    csv_path = path.join(HERE, path.pardir, path.pardir, 'resources',\n                         DATAFILE_NAME)\n    data_iter = list(read_unicode_csv(csv_path))\n\n    # Get the headers.\n    headers = data_iter[0]\n\n    # For some reason this heading is oddly formatted and inconsistent with the\n    # rest, or with the usual key-style for dicts.\n    headers[headers.index('Approved.Symbol')] = 'approved_symbol'\n    return [{header: val for header, val in zip(headers, line)}\n            for line in data_iter[1:]]", "response": "Load the data from the CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a given enpoint of Eidos through the command line.", "response": "def run_eidos(endpoint, *args):\n    \"\"\"Run a given enpoint of Eidos through the command line.\n\n    Parameters\n    ----------\n    endpoint : str\n        The class within the Eidos package to run, for instance\n        'apps.ExtractFromDirectory' will run\n        'org.clulab.wm.eidos.apps.ExtractFromDirectory'\n    *args\n        Any further arguments to be passed as inputs to the class\n        being run.\n    \"\"\"\n    # Make the full path to the class that should be used\n    call_class = '%s.%s' % (eidos_package, endpoint)\n    # Assemble the command line command and append optonal args\n    cmd = ['java', '-Xmx12G', '-cp', eip, call_class] + list(args)\n    logger.info('Running Eidos with command \"%s\"' % (' '.join(cmd)))\n    subprocess.call(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_from_directory(path_in, path_out):\n    path_in = os.path.realpath(os.path.expanduser(path_in))\n    path_out = os.path.realpath(os.path.expanduser(path_out))\n    logger.info('Running Eidos on input folder %s' % path_in)\n    run_eidos('apps.ExtractFromDirectory', path_in, path_out)", "response": "Run Eidos on a set of text files in a folder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_and_process(path_in, path_out):\n    path_in = os.path.realpath(os.path.expanduser(path_in))\n    path_out = os.path.realpath(os.path.expanduser(path_out))\n    extract_from_directory(path_in, path_out)\n    jsons = glob.glob(os.path.join(path_out, '*.jsonld'))\n    logger.info('Found %d JSON-LD files to process in %s' %\n                (len(jsons), path_out))\n    stmts = []\n    for json in jsons:\n        ep = process_json_file(json)\n        if ep:\n            stmts += ep.statements\n    return stmts", "response": "Extract INDRA statements from a set of text files and process them with INDRA."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting INDRA Statements from the INDRA DB.", "response": "def get_statements(subject=None, object=None, agents=None, stmt_type=None,\n                   use_exact_type=False, persist=True, timeout=None,\n                   simple_response=False, ev_limit=10, best_first=True, tries=2,\n                   max_stmts=None):\n    \"\"\"Get a processor for the INDRA DB web API matching given agents and type.\n\n    There are two types of responses available. You can just get a list of\n    INDRA Statements, or you can get an IndraDBRestProcessor object, which allow\n    Statements to be loaded in a background thread, providing a sample of the\n    best* content available promptly in the sample_statements attribute, and\n    populates the statements attribute when the paged load is complete.\n\n    The latter should be used in all new code, and where convenient the prior\n    should be converted to use the processor, as this option may be removed in\n    the future.\n\n    * In the sense of having the most supporting evidence.\n\n    Parameters\n    ----------\n    subject/object : str\n        Optionally specify the subject and/or object of the statements in\n        you wish to get from the database. By default, the namespace is assumed\n        to be HGNC gene names, however you may specify another namespace by\n        including `@<namespace>` at the end of the name string. For example, if\n        you want to specify an agent by chebi, you could use `CHEBI:6801@CHEBI`,\n        or if you wanted to use the HGNC id, you could use `6871@HGNC`.\n    agents : list[str]\n        A list of agents, specified in the same manner as subject and object,\n        but without specifying their grammatical position.\n    stmt_type : str\n        Specify the types of interactions you are interested in, as indicated\n        by the sub-classes of INDRA's Statements. This argument is *not* case\n        sensitive. If the statement class given has sub-classes\n        (e.g. RegulateAmount has IncreaseAmount and DecreaseAmount), then both\n        the class itself, and its subclasses, will be queried, by default. If\n        you do not want this behavior, set use_exact_type=True. Note that if\n        max_stmts is set, it is possible only the exact statement type will\n        be returned, as this is the first searched. The processor then cycles\n        through the types, getting a page of results for each type and adding it\n        to the quota, until the max number of statements is reached.\n    use_exact_type : bool\n        If stmt_type is given, and you only want to search for that specific\n        statement type, set this to True. Default is False.\n    persist : bool\n        Default is True. When False, if a query comes back limited (not all\n        results returned), just give up and pass along what was returned.\n        Otherwise, make further queries to get the rest of the data (which may\n        take some time).\n    timeout : positive int or None\n        If an int, block until the work is done and statements are retrieved, or\n        until the timeout has expired, in which case the results so far will be\n        returned in the response object, and further results will be added in\n        a separate thread as they become available. If simple_response is True,\n        all statements available will be returned. Otherwise (if None), block\n        indefinitely until all statements are retrieved. Default is None.\n    simple_response : bool\n        If True, a simple list of statements is returned (thus block should also\n        be True). If block is False, only the original sample will be returned\n        (as though persist was False), until the statements are done loading, in\n        which case the rest should appear in the list. This behavior is not\n        encouraged. Default is False (which breaks backwards compatibility with\n        usage of INDRA versions from before 1/22/2019). WE ENCOURAGE ALL NEW\n        USE-CASES TO USE THE PROCESSOR, AS THIS FEATURE MAY BE REMOVED AT A\n        LATER DATE.\n    ev_limit : int or None\n        Limit the amount of evidence returned per Statement. Default is 10.\n    best_first : bool\n        If True, the preassembled statements will be sorted by the amount of\n        evidence they have, and those with the most evidence will be\n        prioritized. When using `max_stmts`, this means you will get the \"best\"\n        statements. If False, statements will be queried in arbitrary order.\n    tries : int > 0\n        Set the number of times to try the query. The database often caches\n        results, so if a query times out the first time, trying again after a\n        timeout will often succeed fast enough to avoid a timeout. This can also\n        help gracefully handle an unreliable connection, if you're willing to\n        wait. Default is 2.\n    max_stmts : int or None\n        Select the maximum number of statements to return. When set less than\n        1000 the effect is much the same as setting persist to false, and will\n        guarantee a faster response. Default is None.\n\n    Returns\n    -------\n    processor : :py:class:`IndraDBRestProcessor`\n        An instance of the IndraDBRestProcessor, which has an attribute\n        `statements` which will be populated when the query/queries are done.\n        This is the default behavior, and is encouraged in all future cases,\n        however a simple list of statements may be returned using the\n        `simple_response` option described above.\n    \"\"\"\n    processor = IndraDBRestProcessor(subject, object, agents, stmt_type,\n                                     use_exact_type, persist, timeout,\n                                     ev_limit, best_first, tries, max_stmts)\n\n    # Format the result appropriately.\n    if simple_response:\n        ret = processor.statements\n    else:\n        ret = processor\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget fully formed statements from a list of hashes.", "response": "def get_statements_by_hash(hash_list, ev_limit=100, best_first=True, tries=2):\n    \"\"\"Get fully formed statements from a list of hashes.\n\n    Parameters\n    ----------\n    hash_list : list[int or str]\n        A list of statement hashes.\n    ev_limit : int or None\n        Limit the amount of evidence returned per Statement. Default is 100.\n    best_first : bool\n        If True, the preassembled statements will be sorted by the amount of\n        evidence they have, and those with the most evidence will be\n        prioritized. When using `max_stmts`, this means you will get the \"best\"\n        statements. If False, statements will be queried in arbitrary order.\n    tries : int > 0\n        Set the number of times to try the query. The database often caches\n        results, so if a query times out the first time, trying again after a\n        timeout will often succeed fast enough to avoid a timeout. This can\n        also help gracefully handle an unreliable connection, if you're\n        willing to wait. Default is 2.\n    \"\"\"\n    if not isinstance(hash_list, list):\n        raise ValueError(\"The `hash_list` input is a list, not %s.\"\n                         % type(hash_list))\n    if not hash_list:\n        return []\n    if isinstance(hash_list[0], str):\n        hash_list = [int(h) for h in hash_list]\n    if not all([isinstance(h, int) for h in hash_list]):\n        raise ValueError(\"Hashes must be ints or strings that can be \"\n                         \"converted into ints.\")\n    resp = submit_statement_request('post', 'from_hashes', ev_limit=ev_limit,\n                                    data={'hashes': hash_list},\n                                    best_first=best_first, tries=tries)\n    return stmts_from_json(resp.json()['statements'].values())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the set of INDRA Statements extracted from a paper given by the id.", "response": "def get_statements_for_paper(ids, ev_limit=10, best_first=True, tries=2,\n                             max_stmts=None):\n    \"\"\"Get the set of raw Statements extracted from a paper given by the id.\n\n    Parameters\n    ----------\n    ids : list[(<id type>, <id value>)]\n        A list of tuples with ids and their type. The type can be any one of\n        'pmid', 'pmcid', 'doi', 'pii', 'manuscript id', or 'trid', which is the\n        primary key id of the text references in the database.\n    ev_limit : int or None\n        Limit the amount of evidence returned per Statement. Default is 10.\n    best_first : bool\n        If True, the preassembled statements will be sorted by the amount of\n        evidence they have, and those with the most evidence will be\n        prioritized. When using `max_stmts`, this means you will get the \"best\"\n        statements. If False, statements will be queried in arbitrary order.\n    tries : int > 0\n        Set the number of times to try the query. The database often caches\n        results, so if a query times out the first time, trying again after a\n        timeout will often succeed fast enough to avoid a timeout. This can also\n        help gracefully handle an unreliable connection, if you're willing to\n        wait. Default is 2.\n    max_stmts : int or None\n        Select a maximum number of statements to be returned. Default is None.\n\n    Returns\n    -------\n    stmts : list[:py:class:`indra.statements.Statement`]\n        A list of INDRA Statement instances.\n    \"\"\"\n    id_l = [{'id': id_val, 'type': id_type} for id_type, id_val in ids]\n    resp = submit_statement_request('post', 'from_papers', data={'ids': id_l},\n                                    ev_limit=ev_limit, best_first=best_first,\n                                    tries=tries, max_stmts=max_stmts)\n    stmts_json = resp.json()['statements']\n    return stmts_from_json(stmts_json.values())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submit_curation(hash_val, tag, curator, text=None,\n                    source='indra_rest_client', ev_hash=None, is_test=False):\n    \"\"\"Submit a curation for the given statement at the relevant level.\n\n    Parameters\n    ----------\n    hash_val : int\n        The hash corresponding to the statement.\n    tag : str\n        A very short phrase categorizing the error or type of curation,\n        e.g. \"grounding\" for a grounding error, or \"correct\" if you are\n        marking a statement as correct.\n    curator : str\n        The name or identifier for the curator.\n    text : str\n        A brief description of the problem.\n    source : str\n        The name of the access point through which the curation was performed.\n        The default is 'direct_client', meaning this function was used\n        directly. Any higher-level application should identify itself here.\n    ev_hash : int\n        A hash of the sentence and other evidence information. Elsewhere\n        referred to as `source_hash`.\n    is_test : bool\n        Used in testing. If True, no curation will actually be added to the\n        database.\n    \"\"\"\n    data = {'tag': tag, 'text': text, 'curator': curator, 'source': source,\n            'ev_hash': ev_hash}\n    url = 'curation/submit/%s' % hash_val\n    if is_test:\n        qstr = '?test'\n    else:\n        qstr = ''\n    return make_db_rest_request('post', url, qstr, data=data)", "response": "Submit a curation for the given statement at the relevant level."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the queries used to search based on a list of INDRA Statements.", "response": "def get_statement_queries(stmts, **params):\n    \"\"\"Get queries used to search based on a statement.\n\n    In addition to the stmts, you can enter any parameters standard to the\n    query. See https://github.com/indralab/indra_db/rest_api for a full list.\n\n    Parameters\n    ----------\n    stmts : list[Statement]\n        A list of INDRA statements.\n    \"\"\"\n\n    def pick_ns(ag):\n        for ns in ['HGNC', 'FPLX', 'CHEMBL', 'CHEBI', 'GO', 'MESH']:\n            if ns in ag.db_refs.keys():\n                dbid = ag.db_refs[ns]\n                break\n        else:\n            ns = 'TEXT'\n            dbid = ag.name\n        return '%s@%s' % (dbid, ns)\n\n    queries = []\n    url_base = get_url_base('statements/from_agents')\n    non_binary_statements = [Complex, SelfModification, ActiveForm]\n    for stmt in stmts:\n        kwargs = {}\n        if type(stmt) not in non_binary_statements:\n            for pos, ag in zip(['subject', 'object'], stmt.agent_list()):\n                if ag is not None:\n                    kwargs[pos] = pick_ns(ag)\n        else:\n            for i, ag in enumerate(stmt.agent_list()):\n                if ag is not None:\n                    kwargs['agent%d' % i] = pick_ns(ag)\n        kwargs['type'] = stmt.__class__.__name__\n        kwargs.update(params)\n        query_str = '?' + '&'.join(['%s=%s' % (k, v) for k, v in kwargs.items()\n                                    if v is not None])\n        queries.append(url_base + query_str)\n    return queries"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, model_fname='model.pkl'):\n        with open(model_fname, 'wb') as fh:\n            pickle.dump(self.stmts, fh, protocol=4)", "response": "Save the state of the IncrementalModel in a pickle file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding INDRA Statements to the incremental model indexed by PMID.", "response": "def add_statements(self, pmid, stmts):\n        \"\"\"Add INDRA Statements to the incremental model indexed by PMID.\n\n        Parameters\n        ----------\n        pmid : str\n            The PMID of the paper from which statements were extracted.\n        stmts : list[indra.statements.Statement]\n            A list of INDRA Statements to be added to the model.\n        \"\"\"\n        if pmid not in self.stmts:\n            self.stmts[pmid] = stmts\n        else:\n            self.stmts[pmid] += stmts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preassemble(self, filters=None, grounding_map=None):\n        stmts = self.get_statements()\n\n        # Filter out hypotheses\n        stmts = ac.filter_no_hypothesis(stmts)\n\n        # Fix grounding\n        if grounding_map is not None:\n            stmts = ac.map_grounding(stmts, grounding_map=grounding_map)\n        else:\n            stmts = ac.map_grounding(stmts)\n\n        if filters and ('grounding' in filters):\n            stmts = ac.filter_grounded_only(stmts)\n\n        # Fix sites\n        stmts = ac.map_sequence(stmts)\n\n        if filters and 'human_only' in filters:\n            stmts = ac.filter_human_only(stmts)\n\n        # Run preassembly\n        stmts = ac.run_preassembly(stmts, return_toplevel=False)\n\n        # Run relevance filter\n        stmts = self._relevance_filter(stmts, filters)\n\n        # Save Statements\n        self.assembled_stmts = stmts", "response": "Preassemble the Statements collected in the IncrementalModel and save the result in the class attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of all Agents from all Statements.", "response": "def get_model_agents(self):\n        \"\"\"Return a list of all Agents from all Statements.\n\n        Returns\n        -------\n        agents : list[indra.statements.Agent]\n           A list of Agents that are in the model.\n        \"\"\"\n        model_stmts = self.get_statements()\n        agents = []\n        for stmt in model_stmts:\n            for a in stmt.agent_list():\n                if a is not None:\n                    agents.append(a)\n        return agents"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_statements(self):\n        stmt_lists = [v for k, v in self.stmts.items()]\n        stmts = []\n        for s in stmt_lists:\n            stmts += s\n        return stmts", "response": "Return a list of all INDRA Statements in a single list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all INDRA Statements in a single list.", "response": "def get_statements_noprior(self):\n        \"\"\"Return a list of all non-prior Statements in a single list.\n\n        Returns\n        -------\n        stmts : list[indra.statements.Statement]\n            A list of all the INDRA Statements in the model (excluding\n            the prior).\n        \"\"\"\n        stmt_lists = [v for k, v in self.stmts.items() if k != 'prior']\n        stmts = []\n        for s in stmt_lists:\n            stmts += s\n        return stmts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a BelRdfProcessor for an NDEx network neighborhood. Parameters ---------- gene_names : list A list of HGNC gene symbols to search the neighborhood of. Example: ['BRAF', 'MAP2K1'] network_id : Optional[str] The UUID of the network in NDEx. By default, the BEL Large Corpus network is used. rdf_out : Optional[str] Name of the output file to save the RDF returned by the web service. This is useful for debugging purposes or to repeat the same query on an offline RDF file later. Default: bel_output.rdf Returns ------- bp : BelRdfProcessor A BelRdfProcessor object which contains INDRA Statements in bp.statements. Notes ----- This function calls process_belrdf to the returned RDF string from the webservice.", "response": "def process_ndex_neighborhood(gene_names, network_id=None,\n                              rdf_out='bel_output.rdf', print_output=True):\n    \"\"\"Return a BelRdfProcessor for an NDEx network neighborhood.\n\n    Parameters\n    ----------\n    gene_names : list\n        A list of HGNC gene symbols to search the neighborhood of.\n        Example: ['BRAF', 'MAP2K1']\n    network_id : Optional[str]\n        The UUID of the network in NDEx. By default, the BEL Large Corpus\n        network is used.\n    rdf_out : Optional[str]\n        Name of the output file to save the RDF returned by the web service.\n        This is useful for debugging purposes or to repeat the same query\n        on an offline RDF file later. Default: bel_output.rdf\n\n    Returns\n    -------\n    bp : BelRdfProcessor\n        A BelRdfProcessor object which contains INDRA Statements in bp.statements.\n\n    Notes\n    -----\n    This function calls process_belrdf to the returned RDF string from the\n    webservice.\n    \"\"\"\n    logger.warning('This method is deprecated and the results are not '\n                   'guaranteed to be correct. Please use '\n                   'process_pybel_neighborhood instead.')\n    if network_id is None:\n        network_id = '9ea3c170-01ad-11e5-ac0f-000c29cb28fb'\n    url = ndex_bel2rdf + '/network/%s/asBELRDF/query' % network_id\n    params = {'searchString': ' '.join(gene_names)}\n    # The ndex_client returns the rdf as the content of a json dict\n    res_json = ndex_client.send_request(url, params, is_json=True)\n    if not res_json:\n        logger.error('No response for NDEx neighborhood query.')\n        return None\n    if res_json.get('error'):\n        error_msg = res_json.get('message')\n        logger.error('BEL/RDF response contains error: %s' % error_msg)\n        return None\n    rdf = res_json.get('content')\n    if not rdf:\n        logger.error('BEL/RDF response is empty.')\n        return None\n\n    with open(rdf_out, 'wb') as fh:\n        fh.write(rdf.encode('utf-8'))\n    bp = process_belrdf(rdf, print_output=print_output)\n    return bp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_pybel_neighborhood(gene_names, network_file=None,\n                               network_type='belscript', **kwargs):\n    \"\"\"Return PybelProcessor around neighborhood of given genes in a network.\n\n    This function processes the given network file and filters the returned\n    Statements to ones that contain genes in the given list.\n\n    Parameters\n    ----------\n    network_file : Optional[str]\n        Path to the network file to process. If not given, by default, the\n        BEL Large Corpus is used.\n    network_type : Optional[str]\n        This function allows processing both BEL Script files and JSON files.\n        This argument controls which type is assumed to be processed, and the\n        value can be either 'belscript' or 'json'. Default: bel_script\n\n    Returns\n    -------\n    bp : PybelProcessor\n        A PybelProcessor object which contains INDRA Statements in\n        bp.statements.\n    \"\"\"\n    if network_file is None:\n        # Use large corpus as base network\n        network_file = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                                    os.path.pardir, os.path.pardir,\n                                    os.path.pardir, 'data', 'large_corpus.bel')\n    if network_type == 'belscript':\n        bp = process_belscript(network_file, **kwargs)\n    elif network_type == 'json':\n        bp = process_json_file(network_file)\n\n    filtered_stmts = []\n    for stmt in bp.statements:\n        found = False\n        for agent in stmt.agent_list():\n            if agent is not None:\n                if agent.name in gene_names:\n                    found = True\n        if found:\n            filtered_stmts.append(stmt)\n\n    bp.statements = filtered_stmts\n\n    return bp", "response": "Return a PybelProcessor around neighborhood of given genes in a network."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_belrdf(rdf_str, print_output=True):\n    g = rdflib.Graph()\n    try:\n        g.parse(data=rdf_str, format='nt')\n    except ParseError as e:\n        logger.error('Could not parse rdf: %s' % e)\n        return None\n    # Build INDRA statements from RDF\n    bp = BelRdfProcessor(g)\n    bp.get_complexes()\n    bp.get_activating_subs()\n    bp.get_modifications()\n    bp.get_activating_mods()\n    bp.get_transcription()\n    bp.get_activation()\n    bp.get_conversions()\n\n    # Print some output about the process\n    if print_output:\n        bp.print_statement_coverage()\n        bp.print_statements()\n    return bp", "response": "Return a BelRdfProcessor for a BEL - RDF string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a PybelProcessor by processing a PyBEL graph.", "response": "def process_pybel_graph(graph):\n    \"\"\"Return a PybelProcessor by processing a PyBEL graph.\n\n    Parameters\n    ----------\n    graph : pybel.struct.BELGraph\n        A PyBEL graph to process\n\n    Returns\n    -------\n    bp : PybelProcessor\n        A PybelProcessor object which contains INDRA Statements in\n        bp.statements.\n    \"\"\"\n    bp = PybelProcessor(graph)\n    bp.get_statements()\n    if bp.annot_manager.failures:\n        logger.warning('missing %d annotation pairs',\n                       sum(len(v)\n                           for v in bp.annot_manager.failures.values()))\n    return bp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_belscript(file_name, **kwargs):\n    if 'citation_clearing' not in kwargs:\n        kwargs['citation_clearing'] = False\n    if 'no_identifier_validation' not in kwargs:\n        kwargs['no_identifier_validation'] = True\n    pybel_graph = pybel.from_path(file_name, **kwargs)\n    return process_pybel_graph(pybel_graph)", "response": "Return a PybelProcessor by processing a BEL script file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a PybelProcessor by processing a Node - Link JSON file.", "response": "def process_json_file(file_name):\n    \"\"\"Return a PybelProcessor by processing a Node-Link JSON file.\n\n    For more information on this format, see:\n    http://pybel.readthedocs.io/en/latest/io.html#node-link-json\n\n    Parameters\n    ----------\n    file_name : str\n        The path to a Node-Link JSON file.\n\n    Returns\n    -------\n    bp : PybelProcessor\n        A PybelProcessor object which contains INDRA Statements in\n        bp.statements.\n    \"\"\"\n    with open(file_name, 'rt') as fh:\n        pybel_graph = pybel.from_json_file(fh, False)\n    return process_pybel_graph(pybel_graph)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_cbn_jgif_file(file_name):\n    with open(file_name, 'r') as jgf:\n        return process_pybel_graph(pybel.from_cbn_jgif(json.load(jgf)))", "response": "Return a PybelProcessor by processing a CBN JGIF JSON file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_famplex():\n    famplex_url_pattern = \\\n        'https://raw.githubusercontent.com/sorgerlab/famplex/master/%s.csv'\n    csv_names = ['entities', 'equivalences', 'gene_prefixes',\n                 'grounding_map', 'relations']\n    for csv_name in csv_names:\n        url = famplex_url_pattern % csv_name\n        save_from_http(url, os.path.join(path,'famplex/%s.csv' % csv_name))", "response": "Update all the CSV files that form the FamPlex resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the csv of LINCS small molecule metadata into a dict.", "response": "def update_lincs_small_molecules():\n    \"\"\"Load the csv of LINCS small molecule metadata into a dict.\n\n    Produces a dict keyed by HMS LINCS small molecule ids, with the metadata\n    contained in a dict of row values keyed by the column headers extracted\n    from the csv.\n    \"\"\"\n    url = 'http://lincs.hms.harvard.edu/db/sm/'\n    sm_data = load_lincs_csv(url)\n    sm_dict = {d['HMS LINCS ID']: d.copy() for d in sm_data}\n    assert len(sm_dict) == len(sm_data), \"We lost data.\"\n    fname = os.path.join(path, 'lincs_small_molecules.json')\n    with open(fname, 'w') as fh:\n        json.dump(sm_dict, fh, indent=1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the csv of LINCS protein metadata into a dict.", "response": "def update_lincs_proteins():\n    \"\"\"Load the csv of LINCS protein metadata into a dict.\n\n    Produces a dict keyed by HMS LINCS protein ids, with the metadata\n    contained in a dict of row values keyed by the column headers extracted\n    from the csv.\n    \"\"\"\n    url = 'http://lincs.hms.harvard.edu/db/proteins/'\n    prot_data = load_lincs_csv(url)\n    prot_dict = {d['HMS LINCS ID']: d.copy() for d in prot_data}\n    assert len(prot_dict) == len(prot_data), \"We lost data.\"\n    fname = os.path.join(path, 'lincs_proteins.json')\n    with open(fname, 'w') as fh:\n        json.dump(prot_dict, fh, indent=1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_is_direct(stmt):\n    '''Returns true if there is evidence that the statement is a direct\n    interaction. If any of the evidences associated with the statement\n    indicates a direct interatcion then we assume the interaction\n    is direct. If there is no evidence for the interaction being indirect\n    then we default to direct.'''\n    any_indirect = False\n    for ev in stmt.evidence:\n        if ev.epistemics.get('direct') is True:\n            return True\n        elif ev.epistemics.get('direct') is False:\n            # This guarantees that we have seen at least\n            # some evidence that the statement is indirect\n            any_indirect = True\n    if any_indirect:\n        return False\n    return True", "response": "Returns true if the statement contains a direct interaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_model(self):\n        for stmt in self.statements:\n            if isinstance(stmt, Modification):\n                card = assemble_modification(stmt)\n            elif isinstance(stmt, SelfModification):\n                card = assemble_selfmodification(stmt)\n            elif isinstance(stmt, Complex):\n                card = assemble_complex(stmt)\n            elif isinstance(stmt, Translocation):\n                card = assemble_translocation(stmt)\n            elif isinstance(stmt, RegulateActivity):\n                card = assemble_regulate_activity(stmt)\n            elif isinstance(stmt, RegulateAmount):\n                card = assemble_regulate_amount(stmt)\n            else:\n                continue\n            if card is not None:\n                card.card['meta'] = {'id': stmt.uuid, 'belief': stmt.belief}\n                if self.pmc_override is not None:\n                    card.card['pmc_id'] = self.pmc_override\n                else:\n                    card.card['pmc_id'] = get_pmc_id(stmt)\n                self.cards.append(card)", "response": "Assemble statements into index cards."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_model(self):\n        cards = [c.card for c in self.cards]\n        # If there is only one card, print it as a single\n        # card not as a list\n        if len(cards) == 1:\n            cards = cards[0]\n        cards_json = json.dumps(cards, indent=1)\n        return cards_json", "response": "Return the assembled cards as a JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an INDRA statement corresponding to the Geneways action type.", "response": "def geneways_action_to_indra_statement_type(actiontype, plo):\n    \"\"\"Return INDRA Statement corresponding to Geneways action type.\n\n    Parameters\n    ----------\n    actiontype : str\n        The verb extracted by the Geneways processor\n    plo : str\n        A one character string designating whether Geneways classifies\n        this verb as a physical, logical, or other interaction\n\n    Returns\n    -------\n    statement_generator :\n        If there is no mapping to INDRA statements from this action type\n        the return value is None.\n        If there is such a mapping, statement_generator is an anonymous\n        function that takes in the subject agent, object agent, and evidence,\n        in that order, and returns an INDRA statement object.\n    \"\"\"\n    actiontype = actiontype.lower()\n\n    statement_generator = None\n    is_direct = (plo == 'P')\n\n    if actiontype == 'bind':\n        statement_generator = lambda substance1, substance2, evidence: \\\n                Complex([substance1, substance2], evidence=evidence)\n        is_direct = True\n    elif actiontype == 'phosphorylate':\n        statement_generator = lambda substance1, substance2, evidence: \\\n                Phosphorylation(substance1, substance2, evidence=evidence)\n        is_direct = True\n\n    return (statement_generator, is_direct)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake an INDRA statement from a Geneways action and action mention.", "response": "def make_statement(self, action, mention):\n        \"\"\"Makes an INDRA statement from a Geneways action and action mention.\n\n        Parameters\n        ----------\n        action : GenewaysAction\n            The mechanism that the Geneways mention maps to. Note that\n            several text mentions can correspond to the same action if they are\n            referring to the same relationship - there may be multiple\n            Geneways action mentions corresponding to each action.\n        mention : GenewaysActionMention\n            The Geneways action mention object corresponding to a single\n            mention of a mechanism in a specific text. We make a new INDRA\n            statement corresponding to each action mention.\n\n        Returns\n        -------\n        statement : indra.statements.Statement\n            An INDRA statement corresponding to the provided Geneways action\n            mention, or None if the action mention's type does not map onto\n            any INDRA statement type in geneways_action_type_mapper.\n        \"\"\"\n        (statement_generator, is_direct) = \\\n            geneways_action_to_indra_statement_type(mention.actiontype,\n                                                    action.plo)\n\n        if statement_generator is None:\n            # Geneways statement does not map onto an indra statement\n            return None\n\n        # Try to find the full-text sentence\n        # Unfortunately, the sentence numbers in the Geneways dataset\n        # don't correspond to an obvious sentence segmentation.\n        # This code looks for sentences with the subject, object, and verb\n        # listed by the Geneways action mention table and only includes\n        # it in the evidence if there is exactly one such sentence\n        text = None\n        if self.get_ft_mention:\n            try:\n                content, content_type = get_full_text(mention.pmid, 'pmid')\n                if content is not None:\n                    ftm = FullTextMention(mention, content)\n                    sentences = ftm.find_matching_sentences()\n                    if len(sentences) == 1:\n                        text = sentences[0]\n            except Exception:\n                logger.warning('Could not fetch full text for PMID ' +\n                               mention.pmid)\n\n        # Make an evidence object\n        epistemics = dict()\n        epistemics['direct'] = is_direct\n        annotations = mention.make_annotation()\n        annotations['plo'] = action.plo  # plo only in action table\n        evidence = Evidence(source_api='geneways',\n                            source_id=mention.actionmentionid,\n                            pmid=mention.pmid, text=text,\n                            epistemics=epistemics,\n                            annotations=annotations)\n\n        # Construct the grounded and name standardized agents\n        # Note that this involves grounding the agent by\n        # converting the Entrez ID listed in the Geneways data with\n        # HGNC and UniProt\n        upstream_agent = get_agent(mention.upstream, action.up)\n        downstream_agent = get_agent(mention.downstream, action.dn)\n\n        # Make the statement\n        return statement_generator(upstream_agent, downstream_agent, evidence)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_from_rdf_file(self, rdf_file):\n        self.graph = rdflib.Graph()\n        self.graph.parse(os.path.abspath(rdf_file), format='nt')\n        self.initialize()", "response": "Initialize given an RDF input file representing the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_from_rdf_string(self, rdf_str):\n        self.graph = rdflib.Graph()\n        self.graph.parse(data=rdf_str, format='nt')\n        self.initialize()", "response": "Initialize given an RDF string representing the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extend_with(self, rdf_file):\n        self.graph.parse(os.path.abspath(rdf_file), format='nt')\n        self.initialize()", "response": "Extend the current HierarchyManager with another RDF file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_transitive_closures(self):\n        self.component_counter = 0\n        for rel, tc_dict in ((self.isa_objects, self.isa_closure),\n                             (self.partof_objects, self.partof_closure),\n                             (self.isa_or_partof_objects,\n                                 self.isa_or_partof_closure)):\n            self.build_transitive_closure(rel, tc_dict)", "response": "Build the transitive closures of the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a transitive closure for a given relation in a given dict.", "response": "def build_transitive_closure(self, rel, tc_dict):\n        \"\"\"Build a transitive closure for a given relation in a given dict.\"\"\"\n        # Make a function with the righ argument structure\n        rel_fun = lambda node, graph: rel(node)\n        for x in self.graph.all_nodes():\n            rel_closure = self.graph.transitiveClosure(rel_fun, x)\n            xs = x.toPython()\n            for y in rel_closure:\n                ys = y.toPython()\n                if xs == ys:\n                    continue\n                try:\n                    tc_dict[xs].append(ys)\n                except KeyError:\n                    tc_dict[xs] = [ys]\n                if rel == self.isa_or_partof_objects:\n                    self._add_component(xs, ys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_entity(self, x):\n\n        qstr = self.prefixes + \"\"\"\n            SELECT ?x WHERE {{\n                ?x rn:hasName \"{0}\" .\n            }}\n            \"\"\".format(x)\n        res = self.graph.query(qstr)\n        if list(res):\n            en = list(res)[0][0].toPython()\n            return en\n        else:\n            return None", "response": "Find the entity that has the specified name or synonym."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef directly_or_indirectly_related(self, ns1, id1, ns2, id2, closure_dict,\n                                       relation_func):\n        \"\"\"Return True if two entities have the speicified relationship.\n\n        This relation is constructed possibly through multiple links connecting\n        the two entities directly or indirectly.\n\n        Parameters\n        ----------\n        ns1 : str\n            Namespace code for an entity.\n        id1 : str\n            URI for an entity.\n        ns2 : str\n            Namespace code for an entity.\n        id2 : str\n            URI for an entity.\n        closure_dict: dict\n            A dictionary mapping node names to nodes that have the\n            specified relationship, directly or indirectly. Empty if this\n            has not been precomputed.\n        relation_func: function\n            Function with arguments (node, graph) that generates objects\n            with some relationship with node on the given graph.\n\n        Returns\n        -------\n        bool\n            True if t1 has the specified relationship with t2, either\n            directly or through a series of intermediates; False otherwise.\n        \"\"\"\n        # if id2 is None, or both are None, then it's by definition isa:\n        if id2 is None or (id2 is None and id1 is None):\n            return True\n        # If only id1 is None, then it cannot be isa\n        elif id1 is None:\n            return False\n\n        if closure_dict:\n            term1 = self.get_uri(ns1, id1)\n            term2 = self.get_uri(ns2, id2)\n            ec = closure_dict.get(term1)\n            if ec is not None and term2 in ec:\n                return True\n            else:\n                return False\n        else:\n            if not self.uri_as_name:\n                e1 = self.find_entity(id1)\n                e2 = self.find_entity(id2)\n                if e1 is None or e2 is None:\n                    return False\n                t1 = rdflib.term.URIRef(e1)\n                t2 = rdflib.term.URIRef(e2)\n            else:\n                u1 = self.get_uri(ns1, id1)\n                u2 = self.get_uri(ns2, id2)\n                t1 = rdflib.term.URIRef(u1)\n                t2 = rdflib.term.URIRef(u2)\n\n            to = self.graph.transitiveClosure(relation_func, t1)\n            if t2 in to:\n                return True\n            else:\n                return False", "response": "Returns True if two entities have the speicified relationship with t1 or t2 directly or indirectly."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef isa(self, ns1, id1, ns2, id2):\n        rel_fun = lambda node, graph: self.isa_objects(node)\n        return self.directly_or_indirectly_related(ns1, id1, ns2, id2,\n                                                   self.isa_closure,\n                                                   rel_fun)", "response": "Return True if one entity has an isa relationship to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partof(self, ns1, id1, ns2, id2):\n        rel_fun = lambda node, graph: self.partof_objects(node)\n        return self.directly_or_indirectly_related(ns1, id1, ns2, id2,\n                                                   self.partof_closure,\n                                                   rel_fun)", "response": "Return True if one entity is partof another."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef isa_or_partof(self, ns1, id1, ns2, id2):\n        rel_fun = lambda node, graph: self.isa_or_partof_objects(node)\n        return self.directly_or_indirectly_related(ns1, id1, ns2, id2,\n                                                   self.isa_or_partof_closure,\n                                                   rel_fun)", "response": "Return True if two entities are in an isa or partof relationship."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_opposite(self, ns1, id1, ns2, id2):\n        u1 = self.get_uri(ns1, id1)\n        u2 = self.get_uri(ns2, id2)\n        t1 = rdflib.term.URIRef(u1)\n        t2 = rdflib.term.URIRef(u2)\n\n        rel = rdflib.term.URIRef(self.relations_prefix + 'is_opposite')\n        to = self.graph.objects(t1, rel)\n        if t2 in to:\n            return True\n        return False", "response": "Return True if two entities are in an is_opposite relationship with t1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_parents(self, uri, type='all'):\n        # First do a quick dict lookup to see if there are any parents\n        all_parents = set(self.isa_or_partof_closure.get(uri, []))\n        # If there are no parents or we are looking for all, we can return here\n        if not all_parents or type == 'all':\n            return all_parents\n\n        # If we need immediate parents, we search again, this time knowing that\n        # the uri is definitely in the graph since it has some parents\n        if type == 'immediate':\n            node = rdflib.term.URIRef(uri)\n            immediate_parents = list(set(self.isa_or_partof_objects(node)))\n            return [p.toPython() for p in immediate_parents]\n        elif type == 'top':\n            top_parents = [p for p in all_parents if\n                           not self.isa_or_partof_closure.get(p)]\n            return top_parents", "response": "Return the parents of a given entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_perf(text, msg_id):\n    msg = KQMLPerformative('REQUEST')\n    msg.set('receiver', 'READER')\n    content = KQMLList('run-text')\n    content.sets('text', text)\n    msg.set('content', content)\n    msg.set('reply-with', msg_id)\n    return msg", "response": "Return a request message for a given text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a given PMC article.", "response": "def read_pmc(self, pmcid):\n        \"\"\"Read a given PMC article.\n\n        Parameters\n        ----------\n        pmcid : str\n            The PMC ID of the article to read. Note that only\n            articles in the open-access subset of PMC will work.\n        \"\"\"\n        msg = KQMLPerformative('REQUEST')\n        msg.set('receiver', 'READER')\n        content = KQMLList('run-pmcid')\n        content.sets('pmcid', pmcid)\n        content.set('reply-when-done', 'true')\n        msg.set('content', content)\n        msg.set('reply-with', 'P-%s' % pmcid)\n        self.reply_counter += 1\n        self.send(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_text(self, text):\n        logger.info('Reading: \"%s\"' % text)\n        msg_id = 'RT000%s' % self.msg_counter\n        kqml_perf = _get_perf(text, msg_id)\n        self.reply_counter += 1\n        self.msg_counter += 1\n        self.send(kqml_perf)", "response": "Read a given text phrase."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling replies with reading results.", "response": "def receive_reply(self, msg, content):\n        \"\"\"Handle replies with reading results.\"\"\"\n        reply_head = content.head()\n        if reply_head == 'error':\n            comment = content.gets('comment')\n            logger.error('Got error reply: \"%s\"' % comment)\n        else:\n            extractions = content.gets('ekb')\n            self.extractions.append(extractions)\n        self.reply_counter -= 1\n        if self.reply_counter == 0:\n            self.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_long_sentence(sentence, words_per_line):\n    words = sentence.split(' ')\n    split_sentence = ''\n    for i in range(len(words)):\n        split_sentence = split_sentence + words[i]\n        if (i+1) % words_per_line == 0:\n            split_sentence = split_sentence + '\\n'\n        elif i != len(words) - 1:\n            split_sentence = split_sentence + \" \"\n    return split_sentence", "response": "Takes a sentence and adds a newline every words_per_line words. Returns a string that is a list of strings each contains the word that is a single word."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shorter_name(key):\n    key_short = key\n    for sep in ['#', '/']:\n        ind = key_short.rfind(sep)\n        if ind is not None:\n            key_short = key_short[ind+1:]\n        else:\n            key_short = key_short\n    return key_short.replace('-', '_').replace('.', '_')", "response": "Return a shorter name for an id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd edges to the graph for event properties.", "response": "def add_event_property_edges(event_entity, entries):\n    \"\"\"Adds edges to the graph for event properties.\"\"\"\n    do_not_log = ['@type', '@id',\n            'http://worldmodelers.com/DataProvenance#sourced_from']\n\n    for prop in event_entity:\n        if prop not in do_not_log:\n            value = event_entity[prop]\n            value_entry = None\n            value_str = None\n            if '@id' in value[0]:\n                value = value[0]['@id']\n\n                if value in entries:\n                    value_str = get_entry_compact_text_repr(entries[value],\n                                                            entries)\n                #get_entry_compact_text_repr(entry, entries)\n\n            if value_str is not None:\n                edges.append([shorter_name(event_entity['@id']),\n                                           shorter_name(value),\n                                           shorter_name(prop)])\n                node_labels[shorter_name(value)] = value_str"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of values from the source_from attribute", "response": "def get_sourced_from(entry):\n    \"\"\"Get a list of values from the source_from attribute\"\"\"\n    sourced_from = 'http://worldmodelers.com/DataProvenance#sourced_from'\n\n    if sourced_from in entry:\n        values = entry[sourced_from]\n        values = [i['@id'] for i in values]\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_entry_compact_text_repr(entry, entries):\n    text = get_shortest_text_value(entry)\n    if text is not None:\n        return text\n    else:\n        sources = get_sourced_from(entry)\n        # There are a lot of references to this entity, each of which refer\n        # to it by a different text label. For the sake of visualization,\n        # let's pick one of these labels (in this case, the shortest one)\n        if sources is not None:\n            texts = []\n            for source in sources:\n                source_entry = entries[source]\n                texts.append(get_shortest_text_value(source_entry))\n            return get_shortest_string(texts)", "response": "Get the compact text representation of an entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a JSON - LD entry returns the abbreviated type and the shortest length of the entry.", "response": "def get_entity_type(entry):\n    \"\"\"Given a JSON-LD entry, returns the abbreviated @type and the\n    text attribute that has the shortest length.\n\n    Parameters\n    ----------\n    entry: dict\n        A JSON-LD entry parsed into a nested python dictionary via the json\n        module\n\n    Returns\n    -------\n    short_type: str\n        The shortest type\n    short_text: str\n        Of the text values, the shortest one\n    \"\"\"\n    entry_type = entry['@type']\n    entry_type = [shorter_name(t) for t in entry_type]\n    entry_type = repr(entry_type)\n    return entry_type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn processor with Statements extracted by reading text with Sparser.", "response": "def process_text(text, output_fmt='json', outbuf=None, cleanup=True, key='',\n                 **kwargs):\n    \"\"\"Return processor with Statements extracted by reading text with Sparser.\n\n    Parameters\n    ----------\n    text : str\n        The text to be processed\n    output_fmt: Optional[str]\n        The output format to obtain from Sparser, with the two options being\n        'json' and 'xml'. Default: 'json'\n    outbuf : Optional[file]\n        A file like object that the Sparser output is written to.\n    cleanup : Optional[bool]\n        If True, the temporary file created, which is used as an input\n        file for Sparser, as well as the output file created by Sparser\n        are removed. Default: True\n    key : Optional[str]\n        A key which is embedded into the name of the temporary file\n        passed to Sparser for reading. Default is empty string.\n\n    Returns\n    -------\n    SparserXMLProcessor or SparserJSONProcessor depending on what output\n    format was chosen.\n    \"\"\"\n    nxml_str = make_nxml_from_text(text)\n    return process_nxml_str(nxml_str, output_fmt, outbuf, cleanup, key,\n                            **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_nxml_str(nxml_str, output_fmt='json', outbuf=None, cleanup=True,\n                     key='', **kwargs):\n    \"\"\"Return processor with Statements extracted by reading an NXML string.\n\n    Parameters\n    ----------\n    nxml_str : str\n        The string value of the NXML-formatted paper to be read.\n    output_fmt: Optional[str]\n        The output format to obtain from Sparser, with the two options being\n        'json' and 'xml'. Default: 'json'\n    outbuf : Optional[file]\n        A file like object that the Sparser output is written to.\n    cleanup : Optional[bool]\n        If True, the temporary file created in this function,\n        which is used as an input file for Sparser, as well as the\n        output file created by Sparser are removed. Default: True\n    key : Optional[str]\n        A key which is embedded into the name of the temporary file\n        passed to Sparser for reading. Default is empty string.\n\n    Returns\n    -------\n    SparserXMLProcessor or SparserJSONProcessor depending on what output\n    format was chosen.\n    \"\"\"\n    tmp_fname = 'PMC%s_%d.nxml' % (key, mp.current_process().pid)\n    with open(tmp_fname, 'wb') as fh:\n        fh.write(nxml_str.encode('utf-8'))\n    try:\n        sp = process_nxml_file(tmp_fname, output_fmt, outbuf, cleanup,\n                               **kwargs)\n    finally:\n        if cleanup and os.path.exists(tmp_fname):\n            os.remove(tmp_fname)\n    return sp", "response": "Return processor with Statements extracted by reading an NXML string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_nxml_file(fname, output_fmt='json', outbuf=None, cleanup=True,\n                      **kwargs):\n    \"\"\"Return processor with Statements extracted by reading an NXML file.\n\n    Parameters\n    ----------\n    fname : str\n        The path to the NXML file to be read.\n    output_fmt: Optional[str]\n        The output format to obtain from Sparser, with the two options being\n        'json' and 'xml'. Default: 'json'\n    outbuf : Optional[file]\n        A file like object that the Sparser output is written to.\n    cleanup : Optional[bool]\n        If True, the output file created by Sparser is removed.\n        Default: True\n\n    Returns\n    -------\n    sp : SparserXMLProcessor or SparserJSONProcessor depending on what output\n    format was chosen.\n    \"\"\"\n    sp = None\n    out_fname = None\n    try:\n        out_fname = run_sparser(fname, output_fmt, outbuf, **kwargs)\n        sp = process_sparser_output(out_fname, output_fmt)\n    except Exception as e:\n        logger.error(\"Sparser failed to run on %s.\" % fname)\n        logger.exception(e)\n    finally:\n        if out_fname is not None and os.path.exists(out_fname) and cleanup:\n            os.remove(out_fname)\n\n    return sp", "response": "Return processor with Statements extracted by reading an NXML file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a processor with Statements extracted from Sparser XML or JSON.", "response": "def process_sparser_output(output_fname, output_fmt='json'):\n    \"\"\"Return a processor with Statements extracted from Sparser XML or JSON\n\n    Parameters\n    ----------\n    output_fname : str\n        The path to the Sparser output file to be processed. The file can\n        either be JSON or XML output from Sparser, with the output_fmt\n        parameter defining what format is assumed to be processed.\n    output_fmt : Optional[str]\n        The format of the Sparser output to be processed, can either be\n        'json' or 'xml'. Default: 'json'\n\n    Returns\n    -------\n    sp : SparserXMLProcessor or SparserJSONProcessor depending on what output\n    format was chosen.\n    \"\"\"\n    if output_fmt not in ['json', 'xml']:\n        logger.error(\"Unrecognized output format '%s'.\" % output_fmt)\n        return None\n\n    sp = None\n    with open(output_fname, 'rt') as fh:\n        if output_fmt == 'json':\n            json_dict = json.load(fh)\n            sp = process_json_dict(json_dict)\n        else:\n            xml_str = fh.read()\n            sp = process_xml(xml_str)\n    return sp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_xml(xml_str):\n    try:\n        tree = ET.XML(xml_str, parser=UTB())\n    except ET.ParseError as e:\n        logger.error('Could not parse XML string')\n        logger.error(e)\n        return None\n    sp = _process_elementtree(tree)\n    return sp", "response": "Return a SparserXMLProcessor with Statements extracted from a Sparser XML string."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the Sparser executable and returns the path to the output file.", "response": "def run_sparser(fname, output_fmt, outbuf=None, timeout=600):\n    \"\"\"Return the path to reading output after running Sparser reading.\n\n    Parameters\n    ----------\n    fname : str\n        The path to an input file to be processed. Due to the Spaser\n        executable's assumptions, the file name needs to start with PMC\n        and should be an NXML formatted file.\n    output_fmt : Optional[str]\n        The format in which Sparser should produce its output, can either be\n        'json' or 'xml'.\n    outbuf : Optional[file]\n        A file like object that the Sparser output is written to.\n    timeout : int\n        The number of seconds to wait until giving up on this one reading. The\n        default is 600 seconds (i.e. 10 minutes). Sparcer is a fast reader and\n        the typical type to read a single full text is a matter of seconds.\n\n    Returns\n    -------\n    output_path : str\n        The path to the output file created by Sparser.\n    \"\"\"\n    if not sparser_path or not os.path.exists(sparser_path):\n        logger.error('Sparser executable not set in %s' % sparser_path_var)\n        return None\n    if output_fmt == 'xml':\n        format_flag = '-x'\n        suffix = '.xml'\n    elif output_fmt == 'json':\n        format_flag = '-j'\n        suffix = '.json'\n    else:\n        logger.error('Unknown output format: %s' % output_fmt)\n        return None\n    sparser_exec_path = os.path.join(sparser_path, 'save-semantics.sh')\n    output_path = fname.split('.')[0] + '-semantics' + suffix\n    for fpath in [sparser_exec_path, fname]:\n        if not os.path.exists(fpath):\n            raise Exception(\"'%s' is not a valid path.\" % fpath)\n\n    cmd_list = [sparser_exec_path, format_flag, fname]\n\n    # This is mostly a copy of the code found in subprocess.run, with the\n    # key change that proc.kill is replaced with os.killpg. This allows the\n    # process to be killed even if it has children. Solution developed from:\n    # https://stackoverflow.com/questions/36952245/subprocess-timeout-failure\n    with sp.Popen(cmd_list, stdout=sp.PIPE) as proc:\n        try:\n            stdout, stderr = proc.communicate(timeout=timeout)\n        except sp.TimeoutExpired:\n            # Yes, this is about as bad as it looks. But it is the only way to\n            # be sure the script actually dies.\n            sp.check_call(['pkill', '-f', 'r3.core.*%s' % fname])\n            stdout, stderr = proc.communicate()\n            raise sp.TimeoutExpired(proc.args, timeout, output=stdout,\n                                    stderr=stderr)\n        except BaseException:\n            # See comment on above instance.\n            sp.check_call(['pkill', '-f', fname])\n            proc.wait()\n            raise\n        retcode = proc.poll()\n        if retcode:\n            raise sp.CalledProcessError(retcode, proc.args, output=stdout,\n                                        stderr=stderr)\n    if outbuf is not None:\n        outbuf.write(stdout)\n        outbuf.flush()\n    assert os.path.exists(output_path),\\\n        'No output file \\\"%s\\\" created by sparser.' % output_path\n    return output_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_version():\n    assert sparser_path is not None, \"Sparser path is not defined.\"\n    with open(os.path.join(sparser_path, 'version.txt'), 'r') as f:\n        version = f.read().strip()\n    return version", "response": "Return the version of the Sparser executable on the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a NXML string from a raw text.", "response": "def make_nxml_from_text(text):\n    \"\"\"Return raw text wrapped in NXML structure.\n\n    Parameters\n    ----------\n    text : str\n        The raw text content to be wrapped in an NXML structure.\n\n    Returns\n    -------\n    nxml_str : str\n        The NXML string wrapping the raw text input.\n    \"\"\"\n    text = _escape_xml(text)\n    header = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>' + \\\n        '<OAI-PMH><article><body><sec id=\"s1\"><p>'\n    footer = '</p></sec></body></article></OAI-PMH>'\n    nxml_str = header + text + footer\n    return nxml_str"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the HGNC symbol corresponding to the given HGNC ID.", "response": "def get_hgnc_name(hgnc_id):\n    \"\"\"Return the HGNC symbol corresponding to the given HGNC ID.\n\n    Parameters\n    ----------\n    hgnc_id : str\n        The HGNC ID to be converted.\n\n    Returns\n    -------\n    hgnc_name : str\n        The HGNC symbol corresponding to the given HGNC ID.\n    \"\"\"\n    try:\n        hgnc_name = hgnc_names[hgnc_id]\n    except KeyError:\n        xml_tree = get_hgnc_entry(hgnc_id)\n        if xml_tree is None:\n            return None\n        hgnc_name_tag =\\\n            xml_tree.find(\"result/doc/str[@name='symbol']\")\n        if hgnc_name_tag is None:\n            return None\n        hgnc_name = hgnc_name_tag.text.strip()\n    return hgnc_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the HGNC ID corresponding to the given current or outdated HGNC symbol.", "response": "def get_current_hgnc_id(hgnc_name):\n    \"\"\"Return the HGNC ID(s) corresponding to a current or outdate HGNC symbol.\n\n    Parameters\n    ----------\n    hgnc_name : str\n        The HGNC symbol to be converted, possibly an outdated symbol.\n\n    Returns\n    -------\n    str or list of str or None\n        If there is a single HGNC ID corresponding to the given current or\n        outdated HGNC symbol, that ID is returned as a string. If the symbol\n        is outdated and maps to multiple current IDs, a list of these\n        IDs is returned. If the given name doesn't correspond to either\n        a current or an outdated HGNC symbol, None is returned.\n    \"\"\"\n    hgnc_id = get_hgnc_id(hgnc_name)\n    if hgnc_id:\n        return hgnc_id\n    hgnc_id = prev_sym_map.get(hgnc_name)\n    return hgnc_id"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the HGNC entry for the given HGNC ID from the web service.", "response": "def get_hgnc_entry(hgnc_id):\n    \"\"\"Return the HGNC entry for the given HGNC ID from the web service.\n\n    Parameters\n    ----------\n    hgnc_id : str\n        The HGNC ID to be converted.\n\n    Returns\n    -------\n    xml_tree : ElementTree\n        The XML ElementTree corresponding to the entry for the\n        given HGNC ID.\n    \"\"\"\n    url = hgnc_url + 'hgnc_id/%s' % hgnc_id\n    headers = {'Accept': '*/*'}\n    res = requests.get(url, headers=headers)\n    if not res.status_code == 200:\n        return None\n    xml_tree = ET.XML(res.content, parser=UTB())\n    return xml_tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning unifinished PMIDs given a log file name.", "response": "def analyze_reach_log(log_fname=None, log_str=None):\n    \"\"\"Return unifinished PMIDs given a log file name.\"\"\"\n    assert bool(log_fname) ^ bool(log_str), 'Must specify log_fname OR log_str'\n    started_patt = re.compile('Starting ([\\d]+)')\n    # TODO: it might be interesting to get the time it took to read\n    # each paper here\n    finished_patt = re.compile('Finished ([\\d]+)')\n\n    def get_content_nums(txt):\n        pat = 'Retrieved content for ([\\d]+) / ([\\d]+) papers to be read'\n        res = re.match(pat, txt)\n        has_content, total = res.groups() if res else None, None\n        return has_content, total\n\n    if log_fname:\n        with open(log_fname, 'r') as fh:\n            log_str = fh.read()\n    # has_content, total = get_content_nums(log_str)  # unused\n    pmids = {}\n    pmids['started'] = started_patt.findall(log_str)\n    pmids['finished'] = finished_patt.findall(log_str)\n    pmids['not_done'] = set(pmids['started']) - set(pmids['finished'])\n    return pmids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the logs stashed on s3 for a particular reading.", "response": "def get_logs_from_db_reading(job_prefix, reading_queue='run_db_reading_queue'):\n    \"\"\"Get the logs stashed on s3 for a particular reading.\"\"\"\n    s3 = boto3.client('s3')\n    gen_prefix = 'reading_results/%s/logs/%s' % (job_prefix, reading_queue)\n    job_log_data = s3.list_objects_v2(Bucket='bigmech',\n                                      Prefix=join(gen_prefix, job_prefix))\n    # TODO: Track success/failure\n    log_strs = []\n    for fdict in job_log_data['Contents']:\n        resp = s3.get_object(Bucket='bigmech', Key=fdict['Key'])\n        log_strs.append(resp['Body'].read().decode('utf-8'))\n    return log_strs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of reach logs from the overall logs.", "response": "def separate_reach_logs(log_str):\n    \"\"\"Get the list of reach logs from the overall logs.\"\"\"\n    log_lines = log_str.splitlines()\n    reach_logs = []\n    reach_lines = []\n    adding_reach_lines = False\n    for l in log_lines[:]:\n        if not adding_reach_lines and 'Beginning reach' in l:\n            adding_reach_lines = True\n        elif adding_reach_lines and 'Reach finished' in l:\n            adding_reach_lines = False\n            reach_logs.append(('SUCCEEDED', '\\n'.join(reach_lines)))\n            reach_lines = []\n        elif adding_reach_lines:\n            reach_lines.append(l.split('readers - ')[1])\n            log_lines.remove(l)\n    if adding_reach_lines:\n        reach_logs.append(('FAILURE', '\\n'.join(reach_lines)))\n    return '\\n'.join(log_lines), reach_logs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the set of tcids for which no statements were created.", "response": "def get_unyielding_tcids(log_str):\n    \"\"\"Extract the set of tcids for which no statements were created.\"\"\"\n    tcid_strs = re.findall('INFO: \\[.*?\\].*? - Got no statements for (\\d+).*',\n                           log_str)\n    return {int(tcid_str) for tcid_str in tcid_strs}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef analyze_db_reading(job_prefix, reading_queue='run_db_reading_queue'):\n    # Analyze reach failures\n    log_strs = get_logs_from_db_reading(job_prefix, reading_queue)\n    indra_log_strs = []\n    all_reach_logs = []\n    log_stats = []\n    for log_str in log_strs:\n        log_str, reach_logs = separate_reach_logs(log_str)\n        all_reach_logs.extend(reach_logs)\n        indra_log_strs.append(log_str)\n        log_stats.append(get_reading_stats(log_str))\n\n    # Analayze the reach failures.\n    failed_reach_logs = [reach_log_str\n                         for result, reach_log_str in all_reach_logs\n                         if result == 'FAILURE']\n    failed_id_dicts = [analyze_reach_log(log_str=reach_log)\n                       for reach_log in failed_reach_logs if bool(reach_log)]\n    tcids_unfinished = {id_dict['not_done'] for id_dict in failed_id_dicts}\n    print(\"Found %d unfinished tcids.\" % len(tcids_unfinished))\n\n    # Summarize the global stats\n    if log_stats:\n        sum_dict = dict.fromkeys(log_stats[0].keys())\n        for log_stat in log_stats:\n            for k in log_stat.keys():\n                if isinstance(log_stat[k], list):\n                    if k not in sum_dict.keys():\n                        sum_dict[k] = [0]*len(log_stat[k])\n                    sum_dict[k] = [sum_dict[k][i] + log_stat[k][i]\n                                   for i in range(len(log_stat[k]))]\n                else:\n                    if k not in sum_dict.keys():\n                        sum_dict[k] = 0\n                    sum_dict[k] += log_stat[k]\n    else:\n        sum_dict = {}\n\n    return tcids_unfinished, sum_dict, log_stats", "response": "Run various analysis on a particular reading job."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_pc_neighborhood(gene_names, neighbor_limit=1,\n                            database_filter=None):\n    \"\"\"Returns a BiopaxProcessor for a PathwayCommons neighborhood query.\n\n    The neighborhood query finds the neighborhood around a set of source genes.\n\n    http://www.pathwaycommons.org/pc2/#graph\n\n    http://www.pathwaycommons.org/pc2/#graph_kind\n\n    Parameters\n    ----------\n    gene_names : list\n        A list of HGNC gene symbols to search the neighborhood of.\n        Examples: ['BRAF'], ['BRAF', 'MAP2K1']\n    neighbor_limit : Optional[int]\n        The number of steps to limit the size of the neighborhood around\n        the gene names being queried. Default: 1\n    database_filter : Optional[list]\n        A list of database identifiers to which the query is restricted.\n        Examples: ['reactome'], ['biogrid', 'pid', 'psp']\n        If not given, all databases are used in the query. For a full\n        list of databases see http://www.pathwaycommons.org/pc2/datasources\n\n    Returns\n    -------\n    bp : BiopaxProcessor\n        A BiopaxProcessor containing the obtained BioPAX model in bp.model.\n    \"\"\"\n    model = pcc.graph_query('neighborhood', gene_names,\n                            neighbor_limit=neighbor_limit,\n                            database_filter=database_filter)\n    if model is not None:\n        return process_model(model)", "response": "Returns a BiopaxProcessor for a PathwayCommons neighborhood query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_pc_pathsbetween(gene_names, neighbor_limit=1,\n                            database_filter=None, block_size=None):\n    \"\"\"Returns a BiopaxProcessor for a PathwayCommons paths-between query.\n\n    The paths-between query finds the paths between a set of genes. Here\n    source gene names are given in a single list and all directions of paths\n    between these genes are considered.\n\n    http://www.pathwaycommons.org/pc2/#graph\n\n    http://www.pathwaycommons.org/pc2/#graph_kind\n\n    Parameters\n    ----------\n    gene_names : list\n        A list of HGNC gene symbols to search for paths between.\n        Examples: ['BRAF', 'MAP2K1']\n    neighbor_limit : Optional[int]\n        The number of steps to limit the length of the paths between\n        the gene names being queried. Default: 1\n    database_filter : Optional[list]\n        A list of database identifiers to which the query is restricted.\n        Examples: ['reactome'], ['biogrid', 'pid', 'psp']\n        If not given, all databases are used in the query. For a full\n        list of databases see http://www.pathwaycommons.org/pc2/datasources\n    block_size : Optional[int]\n        Large paths-between queries (above ~60 genes) can error on the server\n        side. In this case, the query can be replaced by a series of\n        smaller paths-between and paths-from-to queries each of which contains\n        block_size genes.\n\n    Returns\n    -------\n    bp : BiopaxProcessor\n        A BiopaxProcessor containing the obtained BioPAX model in bp.model.\n    \"\"\"\n    if not block_size:\n        model = pcc.graph_query('pathsbetween', gene_names,\n                                neighbor_limit=neighbor_limit,\n                                database_filter=database_filter)\n        if model is not None:\n            return process_model(model)\n    else:\n        gene_blocks = [gene_names[i:i + block_size] for i in\n                       range(0, len(gene_names), block_size)]\n        stmts = []\n        # Run pathsfromto between pairs of blocks and pathsbetween\n        # within each block. This breaks up a single call with N genes into\n        # (N/block_size)*(N/blocksize) calls with block_size genes\n        for genes1, genes2 in itertools.product(gene_blocks, repeat=2):\n            if genes1 == genes2:\n                bp = process_pc_pathsbetween(genes1,\n                                             database_filter=database_filter,\n                                             block_size=None)\n            else:\n                bp = process_pc_pathsfromto(genes1, genes2,\n                                            database_filter=database_filter)\n            stmts += bp.statements", "response": "Returns a BiopaxProcessor for a PathwayCommons paths - between query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a BiopaxProcessor for a PathwayCommons paths - from - to query.", "response": "def process_pc_pathsfromto(source_genes, target_genes, neighbor_limit=1,\n                           database_filter=None):\n    \"\"\"Returns a BiopaxProcessor for a PathwayCommons paths-from-to query.\n\n    The paths-from-to query finds the paths from a set of source genes to\n    a set of target genes.\n\n    http://www.pathwaycommons.org/pc2/#graph\n\n    http://www.pathwaycommons.org/pc2/#graph_kind\n\n    Parameters\n    ----------\n    source_genes : list\n        A list of HGNC gene symbols that are the sources of paths being\n        searched for.\n        Examples: ['BRAF', 'RAF1', 'ARAF']\n    target_genes : list\n        A list of HGNC gene symbols that are the targets of paths being\n        searched for.\n        Examples: ['MAP2K1', 'MAP2K2']\n    neighbor_limit : Optional[int]\n        The number of steps to limit the length of the paths\n        between the source genes and target genes being queried. Default: 1\n    database_filter : Optional[list]\n        A list of database identifiers to which the query is restricted.\n        Examples: ['reactome'], ['biogrid', 'pid', 'psp']\n        If not given, all databases are used in the query. For a full\n        list of databases see http://www.pathwaycommons.org/pc2/datasources\n\n    Returns\n    -------\n    bp : BiopaxProcessor\n        A BiopaxProcessor containing the obtained BioPAX model in bp.model.\n    \"\"\"\n    model = pcc.graph_query('pathsfromto', source_genes,\n                             target_genes, neighbor_limit=neighbor_limit,\n                             database_filter=database_filter)\n    if model is not None:\n        return process_model(model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a BiopaxProcessor for a BioPAX model object.", "response": "def process_model(model):\n    \"\"\"Returns a BiopaxProcessor for a BioPAX model object.\n\n    Parameters\n    ----------\n    model : org.biopax.paxtools.model.Model\n        A BioPAX model object.\n\n    Returns\n    -------\n    bp : BiopaxProcessor\n        A BiopaxProcessor containing the obtained BioPAX model in bp.model.\n    \"\"\"\n    bp = BiopaxProcessor(model)\n    bp.get_modifications()\n    bp.get_regulate_activities()\n    bp.get_regulate_amounts()\n    bp.get_activity_modification()\n    bp.get_gef()\n    bp.get_gap()\n    bp.get_conversions()\n    # bp.get_complexes()\n    bp.eliminate_exact_duplicates()\n    return bp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the agent is a protein or chemical.", "response": "def is_protein_or_chemical(agent):\n    '''Return True if the agent is a protein/protein family or chemical.'''\n    # Default is True if agent is None\n    if agent is None:\n        return True\n    dbs = set(['UP', 'HGNC', 'CHEBI', 'PFAM-DEF', 'IP', 'INDRA', 'PUBCHEM',\n               'CHEMBL'])\n    agent_refs = set(agent.db_refs.keys())\n    if agent_refs.intersection(dbs):\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if Statement is only supported by background knowledge.", "response": "def is_background_knowledge(stmt):\n    '''Return True if Statement is only supported by background knowledge.'''\n    any_background = False\n    # Iterate over all evidence for the statement\n    for ev in stmt.evidence:\n        epi = ev.epistemics\n        if epi is not None:\n            sec = epi.get('section_type')\n            # If there is at least one evidence not from a \n            # background section then we consider this to be\n            # a non-background knowledge finding.\n            if sec is not None and sec not in background_secs:\n                return False\n            # If there is at least one evidence that is explicitly\n            # from a background section then we keep track of that.\n            elif sec in background_secs:\n                any_background = True\n    # If there is any explicit evidence for this statement being\n    # background info (and no evidence otherwise) then we return\n    # True, otherwise (for instnace of there is no section info at all)\n    # we return False.\n    return any_background"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multiple_sources(stmt):\n    '''Return True if statement is supported by multiple sources.\n\n    Note: this is currently not used and replaced by BeliefEngine score cutoff\n    '''\n    sources = list(set([e.source_api for e in stmt.evidence]))\n    if len(sources) > 1:\n        return True\n    return False", "response": "Return True if statement is supported by multiple sources."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_assembly(stmts, folder, pmcid, background_assertions=None):\n    '''Run assembly on a list of statements, for a given PMCID.'''\n    # Folder for index card output (scored submission)\n    indexcard_prefix = folder + '/index_cards/' + pmcid\n    # Folder for other outputs (for analysis, debugging)\n    otherout_prefix = folder + '/other_outputs/' + pmcid\n\n    # Do grounding mapping here\n    # Load the TRIPS-specific grounding map and add to the default\n    # (REACH-oriented) grounding map:\n    trips_gm = load_grounding_map('trips_grounding_map.csv')\n    default_grounding_map.update(trips_gm)\n    gm = GroundingMapper(default_grounding_map)\n\n    mapped_agent_stmts = gm.map_agents(stmts)\n    renamed_agent_stmts = gm.rename_agents(mapped_agent_stmts)\n\n    # Filter for grounding\n    grounded_stmts = []\n    for st in renamed_agent_stmts:\n        if all([is_protein_or_chemical(a) for a in st.agent_list()]):\n            grounded_stmts.append(st)\n\n    # Instantiate the Preassembler\n    pa = Preassembler(hierarchies)\n    pa.add_statements(grounded_stmts)\n    print('== %s ====================' % pmcid)\n    print('%d statements collected in total.' % len(pa.stmts))\n\n    # Combine duplicates\n    unique_stmts = pa.combine_duplicates()\n    print('%d statements after combining duplicates.' % len(unique_stmts))\n\n    # Run BeliefEngine on unique statements\n    epe = BeliefEngine()\n    epe.set_prior_probs(pa.unique_stmts)\n\n    # Build statement hierarchy\n    related_stmts = pa.combine_related()\n    # Run BeliefEngine on hierarchy\n    epe.set_hierarchy_probs(related_stmts)\n    print('%d statements after combining related.' % len(related_stmts))\n\n    # Instantiate the mechanism linker\n    # Link statements\n    linked_stmts = MechLinker.infer_active_forms(related_stmts)\n    linked_stmts += MechLinker.infer_modifications(related_stmts)\n    linked_stmts += MechLinker.infer_activations(related_stmts)\n    # Run BeliefEngine on linked statements\n    epe.set_linked_probs(linked_stmts)\n    # Print linked statements for debugging purposes\n    print('Linked\\n=====')\n    for ls in linked_stmts:\n        print(ls.inferred_stmt.belief, ls.inferred_stmt)\n    print('=============')\n\n    # Combine all statements including linked ones\n    all_statements = related_stmts + [ls.inferred_stmt for ls in linked_stmts]\n\n    # Instantiate a new preassembler\n    pa = Preassembler(hierarchies, all_statements)\n    # Build hierarchy again\n    pa.combine_duplicates()\n    # Choose the top-level statements\n    related_stmts = pa.combine_related()\n\n    # Remove top-level statements that came only from the prior\n    if background_assertions is not None:\n        nonbg_stmts = [stmt for stmt in related_stmts\n                       if stmt not in background_assertions]\n    else:\n        nonbg_stmts = related_stmts\n\n    # Dump top-level statements in a pickle\n    with open(otherout_prefix + '.pkl', 'wb') as fh:\n        pickle.dump(nonbg_stmts, fh)\n\n    # Flatten evidence for statements\n    flattened_evidence_stmts = flatten_evidence(nonbg_stmts)\n\n    # Start a card counter\n    card_counter = 1\n    # We don't limit the number of cards reported in this round\n    card_lim = float('inf')\n    top_stmts = []\n    ###############################################\n    # The belief cutoff for statements\n    belief_cutoff = 0.3\n    ###############################################\n    # Sort by amount of evidence\n    for st in sorted(flattened_evidence_stmts,\n                     key=lambda x: x.belief, reverse=True):\n        if st.belief >= belief_cutoff:\n            print(st.belief, st)\n        if st.belief < belief_cutoff:\n            print('SKIP', st.belief, st)\n\n        # If it's background knowledge, we skip the statement\n        if is_background_knowledge(st):\n            print('This statement is background knowledge - skipping.')\n            continue\n\n        # Assemble IndexCards\n        ia = IndexCardAssembler([st], pmc_override=pmcid)\n        ia.make_model()\n        # If the index card was actually made \n        # (not all statements can be assembled into index cards to\n        # this is often not the case)\n        if ia.cards:\n            # Save the index card json\n            ia.save_model(indexcard_prefix + '-%d.json' % card_counter)\n            card_counter += 1\n            top_stmts.append(st)\n            if card_counter > card_lim:\n                break\n\n    # Print the English-assembled model for debugging purposes\n    ea = EnglishAssembler(top_stmts)\n    print('=======================')\n    print(ea.make_model().encode('utf-8'))\n    print('=======================')\n\n    # Print the statement graph\n    graph = render_stmt_graph(nonbg_stmts)\n    graph.draw(otherout_prefix + '_graph.pdf', prog='dot')\n    # Print statement diagnostics\n    print_stmts(pa.stmts, otherout_prefix + '_statements.tsv')\n    print_stmts(related_stmts, otherout_prefix + '_related_statements.tsv')", "response": "Run assembly on a list of statements for a given PMCID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef symbol_to_id(self, symbol):\n\n        if symbol not in self.symbols_to_ids:\n            m = 'Could not look up Entrez ID for Geneways symbol ' + symbol\n            raise Exception(m)\n        return self.symbols_to_ids[symbol]", "response": "Returns the list of Entrez IDs for a given Geneways symbol."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef id_to_symbol(self, entrez_id):\n\n        entrez_id = str(entrez_id)\n        if entrez_id not in self.ids_to_symbols:\n            m = 'Could not look up symbol for Entrez ID ' + entrez_id\n            raise Exception(m)\n        return self.ids_to_symbols[entrez_id]", "response": "Gives the symbol for a given entrez id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats a namespace and ID pair for display and curation.", "response": "def _format_id(ns, id):\n    \"\"\"Format a namespace/ID pair for display and curation.\"\"\"\n    label = '%s:%s' % (ns, id)\n    label = label.replace(' ', '_')\n    url = get_identifiers_url(ns, id)\n    return (label, url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport the statements into a tab - separated text file.", "response": "def make_model(self, output_file, add_curation_cols=False, up_only=False):\n        \"\"\"Export the statements into a tab-separated text file.\n\n        Parameters\n        ----------\n        output_file : str\n            Name of the output file.\n        add_curation_cols : bool\n            Whether to add columns to facilitate statement curation. Default\n            is False (no additional columns).\n        up_only : bool\n            Whether to include identifiers.org links *only* for the Uniprot\n            grounding of an agent when one is available. Because most\n            spreadsheets allow only a single hyperlink per cell, this can makes\n            it easier to link to Uniprot information pages for curation\n            purposes. Default is False.\n        \"\"\"\n        stmt_header = ['INDEX', 'UUID', 'TYPE', 'STR',\n                       'AG_A_TEXT', 'AG_A_LINKS', 'AG_A_STR',\n                       'AG_B_TEXT', 'AG_B_LINKS', 'AG_B_STR',\n                       'PMID', 'TEXT', 'IS_HYP', 'IS_DIRECT']\n        if add_curation_cols:\n            stmt_header = stmt_header + \\\n                          ['AG_A_IDS_CORRECT', 'AG_A_STATE_CORRECT',\n                           'AG_B_IDS_CORRECT', 'AG_B_STATE_CORRECT',\n                           'EVENT_CORRECT',\n                           'RES_CORRECT', 'POS_CORRECT', 'SUBJ_ACT_CORRECT',\n                           'OBJ_ACT_CORRECT', 'HYP_CORRECT', 'DIRECT_CORRECT']\n        rows = [stmt_header]\n\n        for ix, stmt in enumerate(self.statements):\n            # Complexes\n            if len(stmt.agent_list()) > 2:\n                logger.info(\"Skipping statement with more than two members: %s\"\n                            % stmt)\n                continue\n            # Self-modifications, ActiveForms\n            elif len(stmt.agent_list()) == 1:\n                ag_a = stmt.agent_list()[0]\n                ag_b = None\n            # All others\n            else:\n                (ag_a, ag_b) = stmt.agent_list()\n            # Put together the data row\n            row = [ix+1, stmt.uuid, stmt.__class__.__name__, str(stmt)] + \\\n                  _format_agent_entries(ag_a, up_only) + \\\n                  _format_agent_entries(ag_b, up_only) + \\\n                  [stmt.evidence[0].pmid, stmt.evidence[0].text,\n                   stmt.evidence[0].epistemics.get('hypothesis', ''),\n                   stmt.evidence[0].epistemics.get('direct', '')]\n            if add_curation_cols:\n                row = row + ([''] * 11)\n            rows.append(row)\n        # Write to file\n        write_unicode_csv(output_file, rows, delimiter='\\t')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning base agent with given name creating it if needed.", "response": "def get_create_base_agent(self, agent):\n        \"\"\"Return base agent with given name, creating it if needed.\"\"\"\n        try:\n            base_agent = self.agents[_n(agent.name)]\n        except KeyError:\n            base_agent = BaseAgent(_n(agent.name))\n            self.agents[_n(agent.name)] = base_agent\n\n        # If it's a molecular agent\n        if isinstance(agent, Agent):\n            # Handle bound conditions\n            for bc in agent.bound_conditions:\n                bound_base_agent = self.get_create_base_agent(bc.agent)\n                bound_base_agent.create_site(get_binding_site_name(agent))\n                base_agent.create_site(get_binding_site_name(bc.agent))\n\n            # Handle modification conditions\n            for mc in agent.mods:\n                base_agent.create_mod_site(mc)\n\n            # Handle mutation conditions\n            for mc in agent.mutations:\n                res_from = mc.residue_from if mc.residue_from else 'mut'\n                res_to = mc.residue_to if mc.residue_to else 'X'\n                if mc.position is None:\n                    mut_site_name = res_from\n                else:\n                    mut_site_name = res_from + mc.position\n\n                base_agent.create_site(mut_site_name, states=['WT', res_to])\n\n            # Handle location condition\n            if agent.location is not None:\n                base_agent.create_site('loc', [_n(agent.location)])\n\n            # Handle activity\n            if agent.activity is not None:\n                site_name = agent.activity.activity_type\n                base_agent.create_site(site_name, ['inactive', 'active'])\n\n        # There might be overwrites here\n        for db_name, db_ref in agent.db_refs.items():\n            base_agent.db_refs[db_name] = db_ref\n\n        return base_agent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_site(self, site, states=None):\n        if site not in self.sites:\n            self.sites.append(site)\n        if states is not None:\n            self.site_states.setdefault(site, [])\n            try:\n                states = list(states)\n            except TypeError:\n                return\n            self.add_site_states(site, states)", "response": "Create a new site on an agent if it doesn t already exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating modification site for the BaseAgent from a ModCondition.", "response": "def create_mod_site(self, mc):\n        \"\"\"Create modification site for the BaseAgent from a ModCondition.\"\"\"\n        site_name = get_mod_site_name(mc)\n        (unmod_site_state, mod_site_state) = states[mc.mod_type]\n        self.create_site(site_name, (unmod_site_state, mod_site_state))\n        site_anns = [Annotation((site_name, mod_site_state), mc.mod_type,\n                                'is_modification')]\n        if mc.residue:\n            site_anns.append(Annotation(site_name, mc.residue, 'is_residue'))\n        if mc.position:\n            site_anns.append(Annotation(site_name, mc.position, 'is_position'))\n        self.site_annotations += site_anns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding states to an agent site.", "response": "def add_site_states(self, site, states):\n        \"\"\"Create new states on an agent site if the state doesn't exist.\"\"\"\n        for state in states:\n            if state not in self.site_states[site]:\n                self.site_states[site].append(state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_activity_form(self, activity_pattern, is_active):\n        if is_active:\n            if activity_pattern not in self.active_forms:\n                self.active_forms.append(activity_pattern)\n        else:\n            if activity_pattern not in self.inactive_forms:\n                self.inactive_forms.append(activity_pattern)", "response": "Adds the given pattern as an active or inactive form to an Agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_activity_type(self, activity_type):\n        if activity_type not in self.activity_types:\n            self.activity_types.append(activity_type)", "response": "Adds an activity type to an Agent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_annotation(self):\n        annotation = dict()\n\n        # Put all properties of the action object into the annotation\n        for item in dir(self):\n            if len(item) > 0 and item[0] != '_' and \\\n                    not inspect.ismethod(getattr(self, item)):\n                annotation[item] = getattr(self, item)\n\n        # Add properties of each action mention\n        annotation['action_mentions'] = list()\n        for action_mention in self.action_mentions:\n            annotation_mention = action_mention.make_annotation()\n            annotation['action_mentions'].append(annotation_mention)\n\n        return annotation", "response": "Returns a dictionary with all properties of the action\n        and each of its action mentions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for a given file in the specified directory.", "response": "def _search_path(self, directory_name, filename):\n        \"\"\"Searches for a given file in the specified directory.\"\"\"\n        full_path = path.join(directory_name, filename)\n        if path.exists(full_path):\n            return full_path\n\n        # Could not find the requested file in any of the directories\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the file and populates the data.", "response": "def _init_action_list(self, action_filename):\n        \"\"\"Parses the file and populates the data.\"\"\"\n\n        self.actions = list()\n        self.hiid_to_action_index = dict()\n\n        f = codecs.open(action_filename, 'r', encoding='latin-1')\n        first_line = True\n        for line in f:\n            line = line.rstrip()\n            if first_line:\n                # Ignore the first line\n                first_line = False\n            else:\n                self.actions.append(GenewaysAction(line))\n\n                latestInd = len(self.actions)-1\n                hiid = self.actions[latestInd].hiid\n                if hiid in self.hiid_to_action_index:\n                    raise Exception('action hiid not unique: %d' % hiid)\n                self.hiid_to_action_index[hiid] = latestInd"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _link_to_action_mentions(self, actionmention_filename):\n        parser = GenewaysActionMentionParser(actionmention_filename)\n        self.action_mentions = parser.action_mentions\n\n        for action_mention in self.action_mentions:\n            hiid = action_mention.hiid\n            if hiid not in self.hiid_to_action_index:\n                m1 = 'Parsed action mention has hiid %d, which does not exist'\n                m2 = ' in table of action hiids'\n                raise Exception((m1 + m2) % hiid)\n            else:\n                idx = self.hiid_to_action_index[hiid]\n                self.actions[idx].action_mentions.append(action_mention)", "response": "Link action mentions to the action table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _lookup_symbols(self, symbols_filename):\n        symbol_lookup = GenewaysSymbols(symbols_filename)\n        for action in self.actions:\n            action.up_symbol = symbol_lookup.id_to_symbol(action.up)\n            action.dn_symbol = symbol_lookup.id_to_symbol(action.dn)", "response": "Look up symbols for actions and action mentions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the top N actions by count.", "response": "def get_top_n_action_types(self, top_n):\n        \"\"\"Returns the top N actions by count.\"\"\"\n        # Count action types\n        action_type_to_counts = dict()\n        for action in self.actions:\n            actiontype = action.actiontype\n            if actiontype not in action_type_to_counts:\n                action_type_to_counts[actiontype] = 1\n            else:\n                action_type_to_counts[actiontype] = \\\n                        action_type_to_counts[actiontype] + 1\n\n        # Convert the dictionary representation into a pair of lists\n        action_types = list()\n        counts = list()\n        for actiontype in action_type_to_counts.keys():\n            action_types.append(actiontype)\n            counts.append(action_type_to_counts[actiontype])\n\n        # How many actions in total?\n        num_actions = len(self.actions)\n        num_actions2 = 0\n        for count in counts:\n            num_actions2 = num_actions2 + count\n        if num_actions != num_actions2:\n            raise(Exception('Problem counting everything up!'))\n\n        # Sort action types by count (lowest to highest)\n        sorted_inds = np.argsort(counts)\n        last_ind = len(sorted_inds)-1\n\n        # Return the top N actions\n        top_actions = list()\n        if top_n > len(sorted_inds):\n            raise Exception('Asked for top %d action types, ' +\n                            'but there are only %d action types'\n                            % (top_n, len(sorted_inds)))\n        for i in range(top_n):\n            top_actions.append(action_types[sorted_inds[last_ind-i]])\n        return top_actions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassembling the graph from the assembler s list of INDRA Statements.", "response": "def make_model(self):\n        \"\"\"Assemble the graph from the assembler's list of INDRA Statements.\"\"\"\n        # Assemble in two stages.\n        # First, create the nodes of the graph\n        for stmt in self.statements:\n            # Skip SelfModification (self loops) -- has one node\n            if isinstance(stmt, SelfModification) or \\\n               isinstance(stmt, Translocation) or \\\n               isinstance(stmt, ActiveForm):\n                continue\n            # Special handling for Associations -- more than 1 node and members\n            # are Events\n            elif isinstance(stmt, Association):\n                for m in stmt.members:\n                    self._add_node(m.concept)\n            # Special handling for Complexes -- more than 1 node\n            elif isinstance(stmt, Complex):\n                for m in stmt.members:\n                    self._add_node(m)\n            # All else should have exactly 2 nodes\n            elif all([ag is not None for ag in stmt.agent_list()]):\n                if not len(stmt.agent_list()) == 2:\n                    logger.warning(\n                        '%s has less/more than the expected 2 agents.' % stmt)\n                    continue\n                for ag in stmt.agent_list():\n                    self._add_node(ag)\n        # Second, create the edges of the graph\n        for stmt in self.statements:\n            # Skip SelfModification (self loops) -- has one node\n            if isinstance(stmt, SelfModification) or \\\n               isinstance(stmt, Translocation) or \\\n               isinstance(stmt, ActiveForm):\n                continue\n            elif isinstance(stmt, Association):\n                self._add_complex(stmt.members, is_association=True)\n            elif isinstance(stmt, Complex):\n                self._add_complex(stmt.members)\n            elif all([ag is not None for ag in stmt.agent_list()]):\n                self._add_stmt_edge(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_string(self):\n        graph_string = self.graph.to_string()\n        graph_string = graph_string.replace('\\\\N', '\\\\n')\n        return graph_string", "response": "Return the assembled graph as a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save_dot(self, file_name='graph.dot'):\n        s = self.get_string()\n        with open(file_name, 'wt') as fh:\n            fh.write(s)", "response": "Save the current state of the current node in a graphviz dot file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw the graph and save as an image or pdf file.", "response": "def save_pdf(self, file_name='graph.pdf', prog='dot'):\n        \"\"\"Draw the graph and save as an image or pdf file.\n\n        Parameters\n        ----------\n        file_name : Optional[str]\n            The name of the file to save the graph as. Default: graph.pdf\n        prog : Optional[str]\n            The graphviz program to use for graph layout. Default: dot\n        \"\"\"\n        self.graph.draw(file_name, prog=prog)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an edge to the graph.", "response": "def _add_edge(self, source, target, **kwargs):\n        \"\"\"Add an edge to the graph.\"\"\"\n        # Start with default edge properties\n        edge_properties = self.edge_properties\n        # Overwrite ones that are given in function call explicitly\n        for k, v in kwargs.items():\n            edge_properties[k] = v\n        self.graph.add_edge(source, target, **edge_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_node(self, agent):\n        if agent is None:\n            return\n        node_label = _get_node_label(agent)\n        if isinstance(agent, Agent) and agent.bound_conditions:\n            bound_agents = [bc.agent for bc in agent.bound_conditions if\n                            bc.is_bound]\n            if bound_agents:\n                bound_names = [_get_node_label(a) for a in bound_agents]\n                node_label = _get_node_label(agent) + '/' + \\\n                             '/'.join(bound_names)\n                self._complex_nodes.append([agent] + bound_agents)\n            else:\n                node_label = _get_node_label(agent)\n        node_key = _get_node_key(agent)\n        if node_key in self.existing_nodes:\n            return\n        self.existing_nodes.append(node_key)\n        self.graph.add_node(node_key,\n                            label=node_label,\n                            **self.node_properties)", "response": "Add an Agent as a node to the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassemble a Modification statement.", "response": "def _add_stmt_edge(self, stmt):\n        \"\"\"Assemble a Modification statement.\"\"\"\n        # Skip statements with None in the subject position\n        source = _get_node_key(stmt.agent_list()[0])\n        target = _get_node_key(stmt.agent_list()[1])\n        edge_key = (source, target, stmt.__class__.__name__)\n        if edge_key in self.existing_edges:\n            return\n        self.existing_edges.append(edge_key)\n        if isinstance(stmt, RemoveModification) or \\\n             isinstance(stmt, Inhibition) or \\\n             isinstance(stmt, DecreaseAmount) or \\\n             isinstance(stmt, Gap) or \\\n             (isinstance(stmt, Influence) and stmt.overall_polarity() == -1):\n            color = '#ff0000'\n        else:\n            color = '#000000'\n        params = {'color': color,\n                  'arrowhead': 'normal',\n                  'dir': 'forward'}\n        self._add_edge(source, target, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassembling a Complex statement.", "response": "def _add_complex(self, members, is_association=False):\n        \"\"\"Assemble a Complex statement.\"\"\"\n        params = {'color': '#0000ff',\n                  'arrowhead': 'dot',\n                  'arrowtail': 'dot',\n                  'dir': 'both'}\n        for m1, m2 in itertools.combinations(members, 2):\n            if self._has_complex_node(m1, m2):\n                continue\n            if is_association:\n                m1_key = _get_node_key(m1.concept)\n                m2_key = _get_node_key(m2.concept)\n            else:\n                m1_key = _get_node_key(m1)\n                m2_key = _get_node_key(m2)\n            edge_key = (set([m1_key, m2_key]), 'complex')\n            if edge_key in self.existing_edges:\n                return\n            self.existing_edges.append(edge_key)\n            self._add_edge(m1_key, m2_key, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing Signor interaction data from CSV files.", "response": "def process_from_file(signor_data_file, signor_complexes_file=None):\n    \"\"\"Process Signor interaction data from CSV files.\n\n    Parameters\n    ----------\n    signor_data_file : str\n        Path to the Signor interaction data file in CSV format.\n    signor_complexes_file : str\n        Path to the Signor complexes data in CSV format. If unspecified,\n        Signor complexes will not be expanded to their constitutents.\n\n    Returns\n    -------\n    indra.sources.signor.SignorProcessor\n        SignorProcessor containing Statements extracted from the Signor data.\n    \"\"\"\n    # Get generator over the CSV file\n    data_iter = read_unicode_csv(signor_data_file, delimiter=';', skiprows=1)\n    complexes_iter = None\n    if signor_complexes_file:\n        complexes_iter = read_unicode_csv(signor_complexes_file, delimiter=';',\n                                          skiprows=1)\n    else:\n        logger.warning('Signor complex mapping file not provided, Statements '\n                       'involving complexes will not be expanded to members.')\n    return _processor_from_data(data_iter, complexes_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _handle_response(res, delimiter):\n    if res.status_code == 200:\n        # Python 2 -- csv.reader will need bytes\n        if sys.version_info[0] < 3:\n            csv_io = BytesIO(res.content)\n        # Python 3 -- csv.reader needs str\n        else:\n            csv_io = StringIO(res.text)\n        data_iter = read_unicode_csv_fileobj(csv_io, delimiter=delimiter,\n                                             skiprows=1)\n    else:\n        raise Exception('Could not download Signor data.')\n    return data_iter", "response": "Get an iterator over the CSV data from the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_protein_expression(gene_names, cell_types):\n    A = 0.2438361\n    B = 3.0957627\n    mrna_amounts = cbio_client.get_ccle_mrna(gene_names, cell_types)\n    protein_amounts = copy(mrna_amounts)\n    for cell_type in cell_types:\n        amounts = mrna_amounts.get(cell_type)\n        if amounts is None:\n            continue\n        for gene_name, amount in amounts.items():\n            if amount is not None:\n                protein_amount = 10**(A * amount + B)\n                protein_amounts[cell_type][gene_name] = protein_amount\n    return protein_amounts", "response": "Get the protein expression levels of genes in cell types."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an aspect given the name of the aspect", "response": "def get_aspect(cx, aspect_name):\n    \"\"\"Return an aspect given the name of the aspect\"\"\"\n    if isinstance(cx, dict):\n        return cx.get(aspect_name)\n    for entry in cx:\n        if list(entry.keys())[0] == aspect_name:\n            return entry[aspect_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef classify_nodes(graph, hub):\n    node_stats = defaultdict(lambda: defaultdict(list))\n    for u, v, data in graph.edges(data=True):\n        # This means the node is downstream of the hub\n        if hub == u:\n            h, o = u, v\n            if data['i'] != 'Complex':\n                node_stats[o]['up'].append(-1)\n            else:\n                node_stats[o]['up'].append(0)\n        # This means the node is upstream of the hub\n        elif hub == v:\n            h, o = v, u\n            if data['i'] != 'Complex':\n                node_stats[o]['up'].append(1)\n            else:\n                node_stats[o]['up'].append(0)\n        else:\n            continue\n        node_stats[o]['interaction'].append(edge_type_to_class(data['i']))\n\n    node_classes = {}\n    for node_id, stats in node_stats.items():\n        up = max(set(stats['up']), key=stats['up'].count)\n        # Special case: if up is not 0 then we should exclude complexes\n        # from the edge_type states so that we don't end up with\n        # (-1, complex, ...) or (1, complex, ...) as the node class\n        interactions = [i for i in stats['interaction'] if\n                        not (up != 0 and i == 'complex')]\n        edge_type = max(set(interactions), key=interactions.count)\n        node_type = graph.nodes[node_id]['type']\n        node_classes[node_id] = (up, edge_type, node_type)\n    return node_classes", "response": "Classify each node based on its type and relationship to the hub."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_attributes(aspect, id):\n    attributes = {}\n    for entry in aspect:\n        if entry['po'] == id:\n            attributes[entry['n']] = entry['v']\n    return attributes", "response": "Return the attributes pointing to a given ID in a given aspect."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a MultiDiGraph representation of a CX network.", "response": "def cx_to_networkx(cx):\n    \"\"\"Return a MultiDiGraph representation of a CX network.\"\"\"\n    graph = networkx.MultiDiGraph()\n    for node_entry in get_aspect(cx, 'nodes'):\n        id = node_entry['@id']\n        attrs = get_attributes(get_aspect(cx, 'nodeAttributes'), id)\n        attrs['n'] = node_entry['n']\n        graph.add_node(id, **attrs)\n    for edge_entry in get_aspect(cx, 'edges'):\n        id = edge_entry['@id']\n        attrs = get_attributes(get_aspect(cx, 'edgeAttributes'), id)\n        attrs['i'] = edge_entry['i']\n        graph.add_edge(edge_entry['s'], edge_entry['t'], key=id, **attrs)\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the ID of the segment of the plane corresponding to a class.", "response": "def get_quadrant_from_class(node_class):\n    \"\"\"Return the ID of the segment of the plane corresponding to a class.\"\"\"\n    up, edge_type, _ = node_class\n    if up == 0:\n        return 0 if random.random() < 0.5 else 7\n    mappings = {(-1, 'modification'): 1,\n                (-1, 'amount'): 2,\n                (-1, 'activity'): 3,\n                (1, 'activity'): 4,\n                (1, 'amount'): 5,\n                (1, 'modification'): 6}\n    return mappings[(up, edge_type)]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate coordinates for a node in a given class.", "response": "def get_coordinates(node_class):\n    \"\"\"Generate coordinates for a node in a given class.\"\"\"\n    quadrant_size = (2 * math.pi / 8.0)\n    quadrant = get_quadrant_from_class(node_class)\n    begin_angle = quadrant_size * quadrant\n    r = 200 + 800*random.random()\n    alpha = begin_angle + random.random() * quadrant_size\n    x = r * math.cos(alpha)\n    y = r * math.sin(alpha)\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the full layout aspect with coordinates for each node.", "response": "def get_layout_aspect(hub, node_classes):\n    \"\"\"Get the full layout aspect with coordinates for each node.\"\"\"\n    aspect = [{'node': hub, 'x': 0.0, 'y': 0.0}]\n    for node, node_class in node_classes.items():\n        if node == hub:\n            continue\n        x, y = get_coordinates(node_class)\n        aspect.append({'node': node, 'x': x, 'y': y})\n    return aspect"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a node ID given its name.", "response": "def get_node_by_name(graph, name):\n    \"\"\"Return a node ID given its name.\"\"\"\n    for id, attrs in graph.nodes(data=True):\n        if attrs['n'] == name:\n            return id"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattach a layout aspect to a CX network given a hub node.", "response": "def add_semantic_hub_layout(cx, hub):\n    \"\"\"Attach a layout aspect to a CX network given a hub node.\"\"\"\n    graph = cx_to_networkx(cx)\n    hub_node = get_node_by_name(graph, hub)\n    node_classes = classify_nodes(graph, hub_node)\n    layout_aspect = get_layout_aspect(hub_node, node_classes)\n    cx['cartesianLayout'] = layout_aspect"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the metadata of an article given its DOI from CrossRef as a JSON dict", "response": "def get_metadata(doi):\n    \"\"\"Returns the metadata of an article given its DOI from CrossRef\n    as a JSON dict\"\"\"\n    url = crossref_url + 'works/' + doi\n    res = requests.get(url)\n    if res.status_code != 200:\n        logger.info('Could not get CrossRef metadata for DOI %s, code %d' %\n                    (doi, res.status_code))\n        return None\n    raw_message = res.json()\n    metadata = raw_message.get('message')\n    return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fulltext_links(doi):\n    metadata = get_metadata(doi)\n    if metadata is None:\n        return None\n    links = metadata.get('link')\n    return links", "response": "Return a list of links to the full text of an article given its DOI."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries Pubmed for a given PMID and return the first search_limit results.", "response": "def doi_query(pmid, search_limit=10):\n    \"\"\"Get the DOI for a PMID by matching CrossRef and Pubmed metadata.\n\n    Searches CrossRef using the article title and then accepts search hits only\n    if they have a matching journal ISSN and page number with what is obtained\n    from the Pubmed database.\n    \"\"\"\n    # Get article metadata from PubMed\n    pubmed_meta_dict = pubmed_client.get_metadata_for_ids([pmid],\n                                                        get_issns_from_nlm=True)\n    if pubmed_meta_dict is None or pubmed_meta_dict.get(pmid) is None:\n        logger.warning('No metadata found in Pubmed for PMID%s' % pmid)\n        return None\n    # The test above ensures we've got this now\n    pubmed_meta = pubmed_meta_dict[pmid]\n    # Check if we already got a DOI from Pubmed itself!\n    if pubmed_meta.get('doi'):\n        return pubmed_meta.get('doi')\n    # Check for the title, which we'll need for the CrossRef search\n    pm_article_title = pubmed_meta.get('title')\n    if pm_article_title is None:\n        logger.warning('No article title found in Pubmed for PMID%s' % pmid)\n        return None\n    # Get the ISSN list\n    pm_issn_list = pubmed_meta.get('issn_list')\n    if not pm_issn_list:\n        logger.warning('No ISSNs found in Pubmed for PMID%s' % pmid)\n        return None\n    # Get the page number\n    pm_page = pubmed_meta.get('page')\n    if not pm_page:\n        logger.debug('No page number found in Pubmed for PMID%s' % pmid)\n        return None\n    # Now query CrossRef using the title we've got\n    url = crossref_search_url\n    params = {'q': pm_article_title, 'sort': 'score'}\n    try:\n        res = requests.get(crossref_search_url, params)\n    except requests.exceptions.ConnectionError as e:\n        logger.error('CrossRef service could not be reached.')\n        logger.error(e)\n        return None\n    except Exception as e:\n        logger.error('Error accessing CrossRef service: %s' % str(e))\n        return None\n    if res.status_code != 200:\n        logger.info('PMID%s: no search results from CrossRef, code %d' %\n                    (pmid, res.status_code))\n        return None\n    raw_message = res.json()\n    mapped_doi = None\n    # Iterate over the search results, looking up XREF metadata\n    for result_ix, result in enumerate(raw_message):\n        if result_ix > search_limit:\n            logger.info('PMID%s: No match found within first %s results, '\n                        'giving up!' % (pmid, search_limit))\n            break\n        xref_doi_url = result['doi']\n        # Strip the URL prefix off of the DOI\n        m = re.match('^http://dx.doi.org/(.*)$', xref_doi_url)\n        xref_doi = m.groups()[0]\n        # Get the XREF metadata using the DOI\n        xref_meta = get_metadata(xref_doi)\n        if xref_meta is None:\n            continue\n        xref_issn_list = xref_meta.get('ISSN')\n        xref_page = xref_meta.get('page')\n        # If there's no ISSN info for this article, skip to the next result\n        if not xref_issn_list:\n            logger.debug('No ISSN found for DOI %s, skipping' % xref_doi_url)\n            continue\n        # If there's no page info for this article, skip to the next result\n        if not xref_page:\n            logger.debug('No page number found for DOI %s, skipping' %\n                          xref_doi_url)\n            continue\n        # Now check for an ISSN match by looking for the set intersection\n        # between the Pubmed ISSN list and the CrossRef ISSN list.\n        matching_issns = set(pm_issn_list).intersection(set(xref_issn_list))\n        # Before comparing page numbers, regularize the page numbers a bit.\n        # Note that we only compare the first page number, since frequently\n        # the final page number will simply be missing in one of the data\n        # sources. We also canonicalize page numbers of the form '14E' to\n        # 'E14' (which is the format used by Pubmed).\n        pm_start_page = pm_page.split('-')[0].upper()\n        xr_start_page = xref_page.split('-')[0].upper()\n        if xr_start_page.endswith('E'):\n            xr_start_page = 'E' + xr_start_page[:-1]\n        # Now compare the ISSN list and page numbers\n        if matching_issns and pm_start_page == xr_start_page:\n            # We found a match!\n            mapped_doi = xref_doi\n            break\n        # Otherwise, keep looking through the results...\n    # Return a DOI, or None if we didn't find one that met our matching\n    # criteria\n    return mapped_doi"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing a string from an Agent as part of a PySB rule name.", "response": "def get_agent_rule_str(agent):\n    \"\"\"Construct a string from an Agent as part of a PySB rule name.\"\"\"\n    rule_str_list = [_n(agent.name)]\n    # If it's a molecular agent\n    if isinstance(agent, ist.Agent):\n        for mod in agent.mods:\n            mstr = abbrevs[mod.mod_type]\n            if mod.residue is not None:\n                mstr += mod.residue\n            if mod.position is not None:\n                mstr += mod.position\n            rule_str_list.append('%s' % mstr)\n        for mut in agent.mutations:\n            res_from = mut.residue_from if mut.residue_from else 'mut'\n            res_to = mut.residue_to if mut.residue_to else 'X'\n            if mut.position is None:\n                mut_site_name = res_from\n            else:\n                mut_site_name = res_from + mut.position\n            mstr = mut_site_name + res_to\n            rule_str_list.append(mstr)\n        if agent.bound_conditions:\n            for b in agent.bound_conditions:\n                if b.is_bound:\n                    rule_str_list.append(_n(b.agent.name))\n                else:\n                    rule_str_list.append('n' + _n(b.agent.name))\n        if agent.location is not None:\n            rule_str_list.append(_n(agent.location))\n        if agent.activity is not None:\n            if agent.activity.is_active:\n                rule_str_list.append(agent.activity.activity_type[:3])\n            else:\n                rule_str_list.append(agent.activity.activity_type[:3] + '_inact')\n    rule_str = '_'.join(rule_str_list)\n    return rule_str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a rule to a PySB model and handle duplicate component errors.", "response": "def add_rule_to_model(model, rule, annotations=None):\n    \"\"\"Add a Rule to a PySB model and handle duplicate component errors.\"\"\"\n    try:\n        model.add_component(rule)\n        # If the rule was actually added, also add the annotations\n        if annotations:\n            model.annotations += annotations\n    # If this rule is already in the model, issue a warning and continue\n    except ComponentDuplicateNameError:\n        msg = \"Rule %s already in model! Skipping.\" % rule.name\n        logger.debug(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a parameter with given name creating it if needed.", "response": "def get_create_parameter(model, param):\n    \"\"\"Return parameter with given name, creating it if needed.\n\n    If unique is false and the parameter exists, the value is not changed; if\n    it does not exist, it will be created. If unique is true then upon conflict\n    a number is added to the end of the parameter name.\n\n    Parameters\n    ----------\n    model : pysb.Model\n        The model to add the parameter to\n    param : Param\n        An assembly parameter object\n    \"\"\"\n    norm_name = _n(param.name)\n    parameter = model.parameters.get(norm_name)\n\n    if not param.unique and parameter is not None:\n        return parameter\n\n    if param.unique:\n        pnum = 1\n        while True:\n            pname = norm_name + '_%d' % pnum\n            if model.parameters.get(pname) is None:\n                break\n            pnum += 1\n    else:\n        pname = norm_name\n\n    parameter = Parameter(pname, param.value)\n    model.add_component(parameter)\n    return parameter"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstructing the unconditional state of an Agent.", "response": "def get_uncond_agent(agent):\n    \"\"\"Construct the unconditional state of an Agent.\n\n    The unconditional Agent is a copy of the original agent but\n    without any bound conditions and modification conditions.\n    Mutation conditions, however, are preserved since they are static.\n    \"\"\"\n    agent_uncond = ist.Agent(_n(agent.name), mutations=agent.mutations)\n    return agent_uncond"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a generator of MonomerPatterns that can be used to find the patterns for the given agent in the given model.", "response": "def grounded_monomer_patterns(model, agent, ignore_activities=False):\n    \"\"\"Get monomer patterns for the agent accounting for grounding information.\n\n    Parameters\n    ----------\n    model : pysb.core.Model\n        The model to search for MonomerPatterns matching the given Agent.\n    agent : indra.statements.Agent\n        The Agent to find matching MonomerPatterns for.\n    ignore_activites : bool\n        Whether to ignore any ActivityConditions on the agent when determining\n        the required site conditions for the MonomerPattern. For example, if\n        set to True, will find a match for the agent `MAPK1(activity=kinase)`\n        even if the corresponding MAPK1 Monomer in the model has no site\n        named `kinase`. Default is False (more stringent matching).\n\n    Returns\n    -------\n    generator of MonomerPatterns\n    \"\"\"\n    # If it's not a molecular agent\n    if not isinstance(agent, ist.Agent):\n        monomer = model.monomers.get(agent.name)\n        if not monomer:\n            return\n        yield monomer()\n    # Iterate over all model annotations to identify the monomer associated\n    # with this agent\n    monomer = None\n    for ann in model.annotations:\n        if monomer:\n            break\n        if not ann.predicate == 'is':\n            continue\n        if not isinstance(ann.subject, Monomer):\n            continue\n        (ns, id) = parse_identifiers_url(ann.object)\n        if ns is None and id is None:\n            continue\n        # We now have an identifiers.org namespace/ID for a given monomer;\n        # we check to see if there is a matching identifier in the db_refs\n        # for this agent\n        for db_ns, db_id in agent.db_refs.items():\n            # We've found a match! Return first match\n            # FIXME Could also update this to check for alternative\n            # FIXME matches, or make sure that all grounding IDs match,\n            # FIXME etc.\n            if db_ns == ns and db_id == id:\n                monomer = ann.subject\n                break\n    # We looked at all the annotations in the model and didn't find a\n    # match\n    if monomer is None:\n        logger.info('No monomer found corresponding to agent %s' % agent)\n        return\n    # Now that we have a monomer for the agent, look for site/state\n    # combinations corresponding to the state of the agent. For every one of\n    # the modifications specified in the agent signature, check to see if it\n    # can be satisfied based on the agent's annotations.  For every one we find\n    # that is consistent, we yield it--there may be more than one.\n    # FIXME\n    # Create a list of tuples, each one representing the site conditions\n    # that can satisfy a particular agent condition. Each entry in the list\n    # will contain a list of dicts associated with a particular mod/activity\n    # condition. Each dict will represent a site/state combination satisfying\n    # the constraints imposed by that mod/activity condition.\n    sc_list = []\n    for mod in agent.mods:\n        # Find all site/state combinations that have the appropriate\n        # modification type\n        # As we iterate, build up a dict identifying the annotations of\n        # particular sites\n        mod_sites = {}\n        res_sites = set([])\n        pos_sites = set([])\n        for ann in monomer.site_annotations:\n            # Don't forget to handle Nones!\n            if ann.predicate == 'is_modification' and \\\n               ann.object == mod.mod_type:\n                site_state = ann.subject\n                assert isinstance(site_state, tuple)\n                assert len(site_state) == 2\n                mod_sites[site_state[0]] = site_state[1]\n            elif ann.predicate == 'is_residue' and \\\n                 ann.object == mod.residue:\n                res_sites.add(ann.subject)\n            elif ann.predicate == 'is_position' and \\\n                 ann.object == mod.position:\n                pos_sites.add(ann.subject)\n        # If the residue field of the agent is specified,\n        viable_sites = set(mod_sites.keys())\n        if mod.residue is not None:\n            viable_sites = viable_sites.intersection(res_sites)\n        if mod.position is not None:\n            viable_sites = viable_sites.intersection(pos_sites)\n        # If there are no viable sites annotated in the model matching the\n        # available info in the mod condition, then we won't be able to\n        # satisfy the conditions on this agent\n        if not viable_sites:\n            return\n        # Otherwise, update the \n        # If there are any sites left after we subject them to residue\n        # and position constraints, then return the relevant monomer patterns!\n        pattern_list = []\n        for site_name in viable_sites:\n            pattern_list.append({site_name: (mod_sites[site_name], WILD)})\n        sc_list.append(pattern_list)\n    # Now check for monomer patterns satisfying the agent's activity condition\n    if agent.activity and not ignore_activities:\n        # Iterate through annotations with this monomer as the subject\n        # and a has_active_pattern or has_inactive_pattern relationship\n        # FIXME: Currently activity type is not annotated/checked\n        # FIXME act_type = agent.activity.activity_type\n        rel_type = 'has_active_pattern' if agent.activity.is_active \\\n                                        else 'has_inactive_pattern'\n        active_form_list = []\n        for ann in model.annotations:\n            if ann.subject == monomer and ann.predicate == rel_type:\n                # The annotation object contains the active/inactive pattern\n                active_form_list.append(ann.object)\n        sc_list.append(active_form_list)\n    # Now that we've got a list of conditions\n    for pattern_combo in itertools.product(*sc_list):\n        mp_sc = {}\n        for pattern in pattern_combo:\n            mp_sc.update(pattern)\n        if mp_sc:\n            yield monomer(**mp_sc)\n    if not sc_list:\n        yield monomer()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a PySB MonomerPattern from an Agent.", "response": "def get_monomer_pattern(model, agent, extra_fields=None):\n    \"\"\"Construct a PySB MonomerPattern from an Agent.\"\"\"\n    try:\n        monomer = model.monomers[_n(agent.name)]\n    except KeyError as e:\n        logger.warning('Monomer with name %s not found in model' %\n                       _n(agent.name))\n        return None\n    # Get the agent site pattern\n    pattern = get_site_pattern(agent)\n    if extra_fields is not None:\n        for k, v in extra_fields.items():\n            # This is an important assumption, it only sets the given pattern\n            # on the monomer if that site/key is not already specified at the\n            # Agent level. For instance, if the Agent is specified to have\n            # 'activity', that site will not be updated here.\n            if k not in pattern:\n                pattern[k] = v\n    # If a model is given, return the Monomer with the generated pattern,\n    # otherwise just return the pattern\n    try:\n        monomer_pattern = monomer(**pattern)\n    except Exception as e:\n        logger.info(\"Invalid site pattern %s for monomer %s\" %\n                      (pattern, monomer))\n        return None\n    return monomer_pattern"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_site_pattern(agent):\n    if not isinstance(agent, ist.Agent):\n        return {}\n    pattern = {}\n    # Handle bound conditions\n    for bc in agent.bound_conditions:\n        # Here we make the assumption that the binding site\n        # is simply named after the binding partner\n        if bc.is_bound:\n            pattern[get_binding_site_name(bc.agent)] = ANY\n        else:\n            pattern[get_binding_site_name(bc.agent)] = None\n\n    # Handle modifications\n    for mod in agent.mods:\n        mod_site_str = abbrevs[mod.mod_type]\n        if mod.residue is not None:\n            mod_site_str = mod.residue\n        mod_pos_str = mod.position if mod.position is not None else ''\n        mod_site = ('%s%s' % (mod_site_str, mod_pos_str))\n        site_states = states[mod.mod_type]\n        if mod.is_modified:\n            pattern[mod_site] = (site_states[1], WILD)\n        else:\n            pattern[mod_site] = (site_states[0], WILD)\n\n    # Handle mutations\n    for mc in agent.mutations:\n        res_from = mc.residue_from if mc.residue_from else 'mut'\n        res_to = mc.residue_to if mc.residue_to else 'X'\n        if mc.position is None:\n            mut_site_name = res_from\n        else:\n            mut_site_name = res_from + mc.position\n        pattern[mut_site_name] = res_to\n\n    # Handle location\n    if agent.location is not None:\n        pattern['loc'] = _n(agent.location)\n\n    # Handle activity\n    if agent.activity is not None:\n        active_site_name = agent.activity.activity_type\n        if agent.activity.is_active:\n            active_site_state = 'active'\n        else:\n            active_site_state = 'inactive'\n        pattern[active_site_name] = active_site_state\n\n    return pattern", "response": "Construct a dictionary of Monomer site states from an Agent object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset an initial condition for a monomer in its default state.", "response": "def set_base_initial_condition(model, monomer, value):\n    \"\"\"Set an initial condition for a monomer in its 'default' state.\"\"\"\n    # Build up monomer pattern dict\n    sites_dict = {}\n    for site in monomer.sites:\n        if site in monomer.site_states:\n            if site == 'loc' and 'cytoplasm' in monomer.site_states['loc']:\n                sites_dict['loc'] = 'cytoplasm'\n            else:\n                sites_dict[site] = monomer.site_states[site][0]\n        else:\n            sites_dict[site] = None\n    mp = monomer(**sites_dict)\n    pname = monomer.name + '_0'\n    try:\n        p = model.parameters[pname]\n        p.value = value\n    except KeyError:\n        p = Parameter(pname, value)\n        model.add_component(p)\n        model.initial(mp, p)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_annotation(component, db_name, db_ref):\n    url = get_identifiers_url(db_name, db_ref)\n    if not url:\n        return None\n    subj = component\n    ann = Annotation(subj, url, 'is')\n    return ann", "response": "Construct model Annotations for each component."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_identifiers_url(url):\n    url_pattern = 'http://identifiers.org/([A-Za-z]+)/([A-Za-z0-9:]+)'\n    match = re.match(url_pattern, url)\n    if match is not None:\n        g = match.groups()\n        if not len(g) == 2:\n            return (None, None)\n        ns_map = {'hgnc': 'HGNC', 'uniprot': 'UP', 'chebi':'CHEBI',\n                  'interpro':'IP', 'pfam':'XFAM', 'fplx': 'FPLX',\n                  'go': 'GO', 'mesh': 'MESH', 'pubchem.compound': 'PUBCHEM'}\n        ns = g[0]\n        id = g[1]\n        if not ns in ns_map.keys():\n            return (None, None)\n        if ns == 'hgnc':\n            if id.startswith('HGNC:'):\n                id = id[5:]\n            else:\n                logger.warning('HGNC URL missing \"HGNC:\" prefix: %s' % url)\n                return (None, None)\n        indra_ns = ns_map[ns]\n        return (indra_ns, id)\n    return (None, None)", "response": "Parse an identifiers. org URL into a namespace and ID tuple."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassemble the PySB model from the set of INDRA Statements.", "response": "def make_model(self, policies=None, initial_conditions=True,\n                   reverse_effects=False, model_name='indra_model'):\n        \"\"\"Assemble the PySB model from the collected INDRA Statements.\n\n        This method assembles a PySB model from the set of INDRA Statements.\n        The assembled model is both returned and set as the assembler's\n        model argument.\n\n        Parameters\n        ----------\n        policies : Optional[Union[str, dict]]\n            A string or dictionary that defines one or more assembly policies.\n\n            If policies is a string, it defines a global assembly policy\n            that applies to all Statement types.\n            Example: one_step, interactions_only\n\n            A dictionary of policies has keys corresponding to Statement types\n            and values to the policy to be applied to that type of Statement.\n            For Statement types whose policy is undefined, the 'default'\n            policy is applied.\n            Example: {'Phosphorylation': 'two_step'}\n        initial_conditions : Optional[bool]\n            If True, default initial conditions are generated for the\n            Monomers in the model. Default: True\n        reverse_effects : Optional[bool]\n            If True, reverse rules are added to the model for activity,\n            modification and amount regulations that have no corresponding\n            reverse effects. Default: False\n        model_name : Optional[str]\n            The name attribute assigned to the PySB Model object.\n            Default: \"indra_model\"\n\n        Returns\n        -------\n        model : pysb.Model\n            The assembled PySB model object.\n        \"\"\"\n        ppa = PysbPreassembler(self.statements)\n        self.processed_policies = self.process_policies(policies)\n        ppa.replace_activities()\n        if reverse_effects:\n            ppa.add_reverse_effects()\n        self.statements = ppa.statements\n        self.model = Model()\n        self.model.name = model_name\n        self.agent_set = BaseAgentSet()\n        # Collect information about the monomers/self.agent_set from the\n        # statements\n        self._monomers()\n        # Add the monomers to the model based on our BaseAgentSet\n        for agent_name, agent in self.agent_set.items():\n            m = Monomer(_n(agent_name), agent.sites, agent.site_states)\n            m.site_annotations = agent.site_annotations\n            self.model.add_component(m)\n            for db_name, db_ref in agent.db_refs.items():\n                a = get_annotation(m, db_name, db_ref)\n                if a is not None:\n                    self.model.add_annotation(a)\n            # Iterate over the active_forms\n            for af in agent.active_forms:\n                self.model.add_annotation(Annotation(m, af,\n                                                     'has_active_pattern'))\n            for iaf in agent.inactive_forms:\n                self.model.add_annotation(Annotation(m, iaf,\n                                                     'has_inactive_pattern'))\n            for at in agent.activity_types:\n                act_site_cond = {at: 'active'}\n                self.model.add_annotation(Annotation(m, act_site_cond,\n                                                     'has_active_pattern'))\n                inact_site_cond = {at: 'inactive'}\n                self.model.add_annotation(Annotation(m, inact_site_cond,\n                                                     'has_inactive_pattern'))\n\n        # Iterate over the statements to generate rules\n        self._assemble()\n        # Add initial conditions\n        if initial_conditions:\n            self.add_default_initial_conditions()\n\n        return self.model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_default_initial_conditions(self, value=None):\n        if value is not None:\n            try:\n                value_num = float(value)\n            except ValueError:\n                logger.error('Invalid initial condition value.')\n                return\n        else:\n            value_num = self.default_initial_amount\n        if self.model is None:\n            return\n        for m in self.model.monomers:\n            set_base_initial_condition(self.model, m, value_num)", "response": "Set default initial conditions in the PySB model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the initial conditions of the monomers in the given context to the given expression amount.", "response": "def set_expression(self, expression_dict):\n        \"\"\"Set protein expression amounts as initial conditions\n\n        Parameters\n        ----------\n        expression_dict : dict\n            A dictionary in which the keys are gene names and the\n            values are numbers representing the absolute amount\n            (count per cell) of proteins expressed. Proteins that\n            are not expressed can be represented as nan. Entries\n            that are not in the dict or are in there but resolve\n            to None, are set to the default initial amount.\n            Example: {'EGFR': 12345, 'BRAF': 4567, 'ESR1': nan}\n        \"\"\"\n        if self.model is None:\n            return\n\n        monomers_found = []\n        monomers_notfound = []\n        # Iterate over all the monomers\n        for m in self.model.monomers:\n            if (m.name in expression_dict and\n                expression_dict[m.name] is not None):\n                # Try to get the expression amount from the dict\n                init = expression_dict[m.name]\n                # We interpret nan and None as not expressed\n                if math.isnan(init):\n                    init = 0\n                init_round = round(init)\n                set_base_initial_condition(self.model, m, init_round)\n                monomers_found.append(m.name)\n            else:\n                set_base_initial_condition(self.model, m,\n                                           self.default_initial_amount)\n                monomers_notfound.append(m.name)\n        logger.info('Monomers set to given context')\n        logger.info('-----------------------------')\n        for m in monomers_found:\n            logger.info('%s' % m)\n        if monomers_notfound:\n            logger.info('')\n            logger.info('Monomers not found in given context')\n            logger.info('-----------------------------------')\n            for m in monomers_notfound:\n                logger.info('%s' % m)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_context(self, cell_type):\n        if self.model is None:\n            return\n        monomer_names = [m.name for m in self.model.monomers]\n        res = context_client.get_protein_expression(monomer_names, [cell_type])\n        amounts = res.get(cell_type)\n        if not amounts:\n            logger.warning('Could not get context for %s cell type.' %\n                           cell_type)\n            self.add_default_initial_conditions()\n            return\n        self.set_expression(amounts)", "response": "Set protein expression amounts for a given cell type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_model(self, format, file_name=None):\n        # Handle SBGN as special case\n        if format == 'sbgn':\n            exp_str = export_sbgn(self.model)\n        elif format == 'kappa_im':\n            # NOTE: this export is not a str, rather a graph object\n            return export_kappa_im(self.model, file_name)\n        elif format == 'kappa_cm':\n            # NOTE: this export is not a str, rather a graph object\n            return export_kappa_cm(self.model, file_name)\n        else:\n            try:\n                exp_str = pysb.export.export(self.model, format)\n            except KeyError:\n                logging.error('Unknown export format: %s' % format)\n                return None\n\n        if file_name:\n            with open(file_name, 'wb') as fh:\n                fh.write(exp_str.encode('utf-8'))\n        return exp_str", "response": "Save the assembled model in a modeling formalism other than PySB."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the assembled model as an RST file for literate modeling.", "response": "def save_rst(self, file_name='pysb_model.rst', module_name='pysb_module'):\n        \"\"\"Save the assembled model as an RST file for literate modeling.\n\n        Parameters\n        ----------\n        file_name : Optional[str]\n            The name of the file to save the RST in.\n            Default: pysb_model.rst\n        module_name : Optional[str]\n            The name of the python function defining the module.\n            Default: pysb_module\n        \"\"\"\n        if self.model is not None:\n            with open(file_name, 'wt') as fh:\n                fh.write('.. _%s:\\n\\n' % module_name)\n                fh.write('Module\\n======\\n\\n')\n                fh.write('INDRA-assembled model\\n---------------------\\n\\n')\n                fh.write('::\\n\\n')\n                model_str = pysb.export.export(self.model, 'pysb_flat')\n                model_str = '\\t' + model_str.replace('\\n', '\\n\\t')\n                fh.write(model_str)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct and call an assembly function based on the type of statement and the corresponding policy and the stage of assembly.", "response": "def _dispatch(self, stmt, stage, *args):\n        \"\"\"Construct and call an assembly function.\n\n        This function constructs the name of the assembly function based on\n        the type of statement, the corresponding policy and the stage\n        of assembly. It then calls that function to perform the assembly\n        task.\"\"\"\n        policy = self.processed_policies[stmt.uuid]\n        class_name = stmt.__class__.__name__.lower()\n        # We map remove modifications to their positive counterparts\n        if isinstance(stmt, ist.RemoveModification):\n            class_name = ist.modclass_to_modtype[stmt.__class__]\n        # We handle any kind of activity regulation in regulateactivity\n        if isinstance(stmt, ist.RegulateActivity):\n            class_name = 'regulateactivity'\n        func_name = '%s_%s_%s' % (class_name, stage, policy.name)\n        func = globals().get(func_name)\n        if func is None:\n            # The specific policy is not implemented for the\n            # given statement type.\n            # We try to apply a default policy next.\n            func_name = '%s_%s_default' % (class_name, stage)\n            func = globals().get(func_name)\n            if func is None:\n                # The given statement type doesn't have a default\n                # policy.\n                raise UnknownPolicyError('%s function %s not defined' %\n                                         (stage, func_name))\n        return func(stmt, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _monomers(self):\n        for stmt in self.statements:\n            if _is_whitelisted(stmt):\n                self._dispatch(stmt, 'monomers', self.agent_set)", "response": "Calls the appropriate monomers method based on policies."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling the appropriate assemble method based on policies.", "response": "def _assemble(self):\n        \"\"\"Calls the appropriate assemble method based on policies.\"\"\"\n        for stmt in self.statements:\n            pol = self.processed_policies[stmt.uuid]\n            if _is_whitelisted(stmt):\n                self._dispatch(stmt, 'assemble', self.model, self.agent_set,\n                               pol.parameters)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a query to the TRIPS web service.", "response": "def send_query(text, service_endpoint='drum', query_args=None):\n    \"\"\"Send a query to the TRIPS web service.\n\n    Parameters\n    ----------\n    text : str\n        The text to be processed.\n    service_endpoint : Optional[str]\n        Selects the TRIPS/DRUM web service endpoint to use. Is a choice between\n        \"drum\" (default), \"drum-dev\", a nightly build, and \"cwms\" for use with\n        more general knowledge extraction.\n    query_args : Optional[dict]\n        A dictionary of arguments to be passed with the query.\n\n    Returns\n    -------\n    html : str\n        The HTML result returned by the web service.\n    \"\"\"\n    if service_endpoint in ['drum', 'drum-dev', 'cwms', 'cwmsreader']:\n        url = base_url + service_endpoint\n    else:\n        logger.error('Invalid service endpoint: %s' % service_endpoint)\n        return ''\n    if query_args is None:\n        query_args = {}\n    query_args.update({'input': text})\n    res = requests.get(url, query_args, timeout=3600)\n    if not res.status_code == 200:\n        logger.error('Problem with TRIPS query: status code %s' %\n                     res.status_code)\n        return ''\n    # Gets unicode content\n    return res.text"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the content XML from the HTML output of the TRIPS web service.", "response": "def get_xml(html, content_tag='ekb', fail_if_empty=False):\n    \"\"\"Extract the content XML from the HTML output of the TRIPS web service.\n\n    Parameters\n    ----------\n    html : str\n        The HTML output from the TRIPS web service.\n    content_tag : str\n        The xml tag used to label the content. Default is 'ekb'.\n    fail_if_empty : bool\n        If True, and if the xml content found is an empty string, raise an\n        exception. Default is False.\n\n    Returns\n    -------\n    The extraction knowledge base (e.g. EKB) XML that contains the event and\n    term extractions.\n    \"\"\"\n    cont = re.findall(r'<%(tag)s(.*?)>(.*?)</%(tag)s>' % {'tag': content_tag},\n                      html, re.MULTILINE | re.DOTALL)\n    if cont:\n        events_terms = ''.join([l.strip() for l in cont[0][1].splitlines()])\n        if 'xmlns' in cont[0][0]:\n            meta = ' '.join([l.strip() for l in cont[0][0].splitlines()])\n        else:\n            meta = ''\n    else:\n        events_terms = ''\n        meta = ''\n\n    if fail_if_empty:\n        assert events_terms != '',\\\n            \"Got empty string for events content from html:\\n%s\" % html\n\n    header = ('<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\"?><%s%s>'\n              % (content_tag, meta))\n    footer = '</%s>' % content_tag\n    return header + events_terms.replace('\\n', '') + footer"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the TRIPS EKB XML string to a file.", "response": "def save_xml(xml_str, file_name, pretty=True):\n    \"\"\"Save the TRIPS EKB XML in a file.\n\n    Parameters\n    ----------\n    xml_str : str\n        The TRIPS EKB XML string to be saved.\n    file_name : str\n        The name of the file to save the result in.\n    pretty : Optional[bool]\n        If True, the XML is pretty printed.\n    \"\"\"\n    try:\n        fh = open(file_name, 'wt')\n    except IOError:\n        logger.error('Could not open %s for writing.' % file_name)\n        return\n    if pretty:\n        xmld = xml.dom.minidom.parseString(xml_str)\n        xml_str_pretty = xmld.toprettyxml()\n        fh.write(xml_str_pretty)\n    else:\n        fh.write(xml_str)\n    fh.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn processor by processing a given table.", "response": "def process_table(fname):\n    \"\"\"Return processor by processing a given sheet of a spreadsheet file.\n\n    Parameters\n    ----------\n    fname : str\n        The name of the Excel file (typically .xlsx extension) to process\n\n    Returns\n    -------\n    sp : indra.sources.sofia.processor.SofiaProcessor\n        A SofiaProcessor object which has a list of extracted INDRA\n        Statements as its statements attribute.\n    \"\"\"\n    book = openpyxl.load_workbook(fname, read_only=True)\n    try:\n        rel_sheet = book['Relations']\n    except Exception as e:\n        rel_sheet = book['Causal']\n    event_sheet = book['Events']\n    entities_sheet = book['Entities']\n    sp = SofiaExcelProcessor(rel_sheet.rows, event_sheet.rows,\n                             entities_sheet.rows)\n    return sp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn processor by processing text given as a string.", "response": "def process_text(text, out_file='sofia_output.json', auth=None):\n    \"\"\"Return processor by processing text given as a string.\n\n    Parameters\n    ----------\n    text : str\n        A string containing the text to be processed with Sofia.\n    out_file : Optional[str]\n        The path to a file to save the reader's output into.\n        Default: sofia_output.json\n    auth : Optional[list]\n        A username/password pair for the Sofia web service. If not given,\n        the SOFIA_USERNAME and SOFIA_PASSWORD values are loaded from either\n        the INDRA config or the environment.\n\n    Returns\n    -------\n    sp : indra.sources.sofia.processor.SofiaProcessor\n        A SofiaProcessor object which has a list of extracted INDRA\n        Statements as its statements attribute. If the API did not process\n        the text, None is returned.\n    \"\"\"\n    text_json = {'text': text}\n    if not auth:\n        user, password = _get_sofia_auth()\n    else:\n        user, password = auth\n    if not user or not password:\n        raise ValueError('Could not use SOFIA web service since'\n                         ' authentication information is missing. Please'\n                         ' set SOFIA_USERNAME and SOFIA_PASSWORD in the'\n                         ' INDRA configuration file or as environmental'\n                         ' variables.')\n    json_response, status_code, process_status = \\\n        _text_processing(text_json=text_json, user=user, password=password)\n\n    # Check response status\n    if process_status != 'Done' or status_code != 200:\n        return None\n\n    # Cache reading output\n    if out_file:\n        with open(out_file, 'w') as fh:\n            json.dump(json_response, fh, indent=1)\n\n    return process_json(json_response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_dict_from_list(dict_key, list_of_dicts):\n    the_dict = [cur_dict for cur_dict in list_of_dicts\n                if cur_dict.get(dict_key)]\n    if not the_dict:\n        raise ValueError('Could not find a dict with key %s' % dict_key)\n    return the_dict[0][dict_key]", "response": "Retrieve a specific dict from a list of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing internal dicts containing node information.", "response": "def _initialize_node_agents(self):\n        \"\"\"Initialize internal dicts containing node information.\"\"\"\n        nodes = _get_dict_from_list('nodes', self.cx)\n        invalid_genes = []\n        for node in nodes:\n            id = node['@id']\n            cx_db_refs = self.get_aliases(node)\n            up_id = cx_db_refs.get('UP')\n            if up_id:\n                gene_name = uniprot_client.get_gene_name(up_id)\n                hgnc_id = hgnc_client.get_hgnc_id(gene_name)\n                db_refs = {'UP': up_id, 'HGNC': hgnc_id, 'TEXT': gene_name}\n                agent = Agent(gene_name, db_refs=db_refs)\n                self._node_names[id] = gene_name\n                self._node_agents[id] = agent\n                continue\n            else:\n                node_name = node['n']\n                self._node_names[id] = node_name\n                hgnc_id = hgnc_client.get_hgnc_id(node_name)\n                db_refs = {'TEXT': node_name}\n                if not hgnc_id:\n                    if not self.require_grounding:\n                        self._node_agents[id] = \\\n                                Agent(node_name, db_refs=db_refs)\n                    invalid_genes.append(node_name)\n                else:\n                    db_refs.update({'HGNC': hgnc_id})\n                    up_id = hgnc_client.get_uniprot_id(hgnc_id)\n                    # It's possible that a valid HGNC ID will not have a\n                    # Uniprot ID, as in the case of HOTAIR (HOX transcript\n                    # antisense RNA, HGNC:33510)\n                    if up_id:\n                        db_refs.update({'UP': up_id})\n                    self._node_agents[id] = Agent(node_name, db_refs=db_refs)\n        if invalid_genes:\n            verb = 'Skipped' if self.require_grounding else 'Included'\n            logger.info('%s invalid gene symbols: %s' %\n                        (verb, ', '.join(invalid_genes)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pmids(self):\n        pmids = []\n        for ea in self._edge_attributes.values():\n            edge_pmids = ea.get('pmids')\n            if edge_pmids:\n                pmids += edge_pmids\n        return list(set(pmids))", "response": "Get list of all PMIDs associated with edges in the network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting network edges into INDRA Statements.", "response": "def get_statements(self):\n        \"\"\"Convert network edges into Statements.\n\n        Returns\n        -------\n        list of Statements\n            Converted INDRA Statements.\n        \"\"\"\n        edges = _get_dict_from_list('edges', self.cx)\n        for edge in edges:\n            edge_type = edge.get('i')\n            if not edge_type:\n                continue\n            stmt_type = _stmt_map.get(edge_type)\n            if stmt_type:\n                id = edge['@id']\n                source_agent = self._node_agents.get(edge['s'])\n                target_agent = self._node_agents.get(edge['t'])\n                if not source_agent or not target_agent:\n                    logger.info(\"Skipping edge %s->%s: %s\" %\n                                (self._node_names[edge['s']],\n                                 self._node_names[edge['t']], edge))\n                    continue\n                ev = self._create_evidence(id)\n                if stmt_type == Complex:\n                    stmt = stmt_type([source_agent, target_agent], evidence=ev)\n                else:\n                    stmt = stmt_type(source_agent, target_agent, evidence=ev)\n                self.statements.append(stmt)\n        return self.statements"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an evidence object for a specific edge in the network.", "response": "def _create_evidence(self, edge_id):\n        \"\"\"Create Evidence object for a specific edge/Statement in the network.\n\n        Parameters\n        ----------\n        edge_id : int\n            ID of the edge in the underlying NDEx network.\n        \"\"\"\n        pmids = None\n        edge_attr = self._edge_attributes.get(edge_id)\n        if edge_attr:\n            pmids = edge_attr.get('pmids')\n        if not pmids:\n            return [Evidence(source_api='ndex',\n                             source_id=self._network_info['externalId'],\n                             annotations={'edge_id': edge_id})]\n        else:\n            evidence = []\n            for pmid in pmids:\n                evidence.append(\n                        Evidence(source_api='ndex',\n                                 source_id=self._network_info['externalId'],\n                                 pmid=pmid,\n                                 annotations={'edge_id': edge_id}))\n            return evidence"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking for an edge from node_name to some other node with the specified edge_label. Returns None if the edge does not exist.", "response": "def node_has_edge_with_label(self, node_name, edge_label):\n        \"\"\"Looks for an edge from node_name to some other node with the specified\n        label. Returns the node to which this edge points if it exists, or None\n        if it doesn't.\n\n        Parameters\n        ----------\n        G :\n            The graph object\n        node_name :\n            Node that the edge starts at\n        edge_label :\n            The text in the relation property of the edge\n        \"\"\"\n        G = self.G\n        for edge in G.edges(node_name):\n            to = edge[1]\n\n            relation_name = G.edges[node_name, to]['relation']\n            if relation_name == edge_label:\n                return to\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef general_node_label(self, node):\n        G = self.G\n        if G.node[node]['is_event']:\n            return 'event type=' + G.node[node]['type']\n        else:\n            return 'entity text=' + G.node[node]['text']", "response": "Used for debugging - gives a short text description of a\n            graph node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses for debugging - prints a short description of a node its children its parents and its parents and children.", "response": "def print_parent_and_children_info(self, node):\n        \"\"\"Used for debugging - prints a short description of a a node, its\n        children, its parents, and its parents' children.\"\"\"\n        G = self.G\n        parents = G.predecessors(node)\n        children = G.successors(node)\n\n        print(general_node_label(G, node))\n        tabs = '\\t'\n        for parent in parents:\n            relation = G.edges[parent, node]['relation']\n            print(tabs + 'Parent (%s): %s' % (relation,\n                  general_node_label(G, parent)))\n            for cop in G.successors(parent):\n                if cop != node:\n                    relation = G.edges[parent, cop]['relation']\n                    print(tabs + 'Child of parent (%s): %s' % (relation,\n                          general_node_label(G, cop)))\n        for child in children:\n            relation = G.edges[node, child]['relation']\n            print(tabs + 'Child (%s): (%s)' % (relation,\n                                               general_node_label(G, child)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_event_parent_with_event_child(self, parent_name, child_name):\n        G = self.G\n        matches = []\n        for n in G.node.keys():\n            if G.node[n]['is_event'] and G.node[n]['type'] == parent_name:\n                children = G.successors(n)\n                for child in children:\n                    if G.node[child]['is_event'] and \\\n                            G.node[child]['type'] == child_name:\n                        matches.append((n, child))\n                        break\n        return list(set(matches))", "response": "Finds all event nodes that are_event node attribute is True that are\n        of the type parent_name that have a child event node with the type\n        child_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_event_with_outgoing_edges(self, event_name, desired_relations):\n\n        G = self.G\n        desired_relations = set(desired_relations)\n\n        desired_event_nodes = []\n\n        for node in G.node.keys():\n            if G.node[node]['is_event'] and G.node[node]['type'] == event_name:\n                has_relations = [G.edges[node, edge[1]]['relation'] for\n                                 edge in G.edges(node)]\n                has_relations = set(has_relations)\n                # Did the outgoing edges from this node have all of the\n                # desired relations?\n                if desired_relations.issubset(has_relations):\n                    desired_event_nodes.append(node)\n        return desired_event_nodes", "response": "Returns a list of event nodes with the specified event_name and outgoing edges annotated with each of the specified relations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlooks for an edge from node to some other node such that the edge COOKIE is annotated with the given relation. Returns None if no such edge exists.", "response": "def get_related_node(self, node, relation):\n        \"\"\"Looks for an edge from node to some other node, such that the edge\n        is annotated with the given relation. If there exists such an edge,\n        returns the name of the node it points to. Otherwise, returns None.\"\"\"\n        G = self.G\n        for edge in G.edges(node):\n            to = edge[1]\n\n            to_relation = G.edges[node, to]['relation']\n            if to_relation == relation:\n                return to\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks for an edge from node to some other node that is annotated with the given relation. Returns None if no such edge is found.", "response": "def get_entity_text_for_relation(self, node, relation):\n        \"\"\"Looks for an edge from node to some other node, such that the edge is\n        annotated with the given relation. If there exists such an edge, and\n        the node at the other edge is an entity, return that entity's text.\n        Otherwise, returns None.\"\"\"\n\n        G = self.G\n        related_node = self.get_related_node(node, relation)\n        if related_node is not None:\n            if not G.node[related_node]['is_event']:\n                return G.node[related_node]['text']\n            else:\n                return None\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the InDRA statements for Positive_Regulation events with a specified Cause and Gene_Expression theme and returns a list of INDRA statements.", "response": "def process_increase_expression_amount(self):\n        \"\"\"Looks for Positive_Regulation events with a specified Cause\n        and a Gene_Expression theme, and processes them into INDRA statements.\n        \"\"\"\n        statements = []\n\n        pwcs = self.find_event_parent_with_event_child(\n                'Positive_regulation', 'Gene_expression')\n        for pair in pwcs:\n            pos_reg = pair[0]\n            expression = pair[1]\n\n            cause = self.get_entity_text_for_relation(pos_reg, 'Cause')\n            target = self.get_entity_text_for_relation(expression, 'Theme')\n\n            if cause is not None and target is not None:\n                theme_node = self.get_related_node(expression, 'Theme')\n                assert(theme_node is not None)\n                evidence = self.node_to_evidence(theme_node, is_direct=False)\n\n                statements.append(IncreaseAmount(s2a(cause), s2a(target),\n                                  evidence=evidence))\n        return statements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_phosphorylation_statements(self):\n        G = self.G\n        statements = []\n\n        pwcs = self.find_event_parent_with_event_child('Positive_regulation',\n                                                       'Phosphorylation')\n        for pair in pwcs:\n            (pos_reg, phos) = pair\n            cause = self.get_entity_text_for_relation(pos_reg, 'Cause')\n            theme = self.get_entity_text_for_relation(phos, 'Theme')\n            print('Cause:', cause, 'Theme:', theme)\n\n            # If the trigger word is dephosphorylate or similar, then we\n            # extract a dephosphorylation statement\n            trigger_word = self.get_entity_text_for_relation(phos,\n                                                             'Phosphorylation')\n            if 'dephos' in trigger_word:\n                deph = True\n            else:\n                deph = False\n\n            site = self.get_entity_text_for_relation(phos, 'Site')\n\n            theme_node = self.get_related_node(phos, 'Theme')\n            assert(theme_node is not None)\n            evidence = self.node_to_evidence(theme_node, is_direct=False)\n\n            if theme is not None:\n                if deph:\n                    statements.append(Dephosphorylation(s2a(cause),\n                                      s2a(theme), site, evidence=evidence))\n                else:\n                    statements.append(Phosphorylation(s2a(cause),\n                                      s2a(theme), site, evidence=evidence))\n        return statements", "response": "Processes Phosphorylation events in the graph and extracts them into INDRA statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_binding_statements(self):\n        G = self.G\n        statements = []\n\n        binding_nodes = self.find_event_with_outgoing_edges('Binding',\n                                                            ['Theme',\n                                                                'Theme2'])\n\n        for node in binding_nodes:\n            theme1 = self.get_entity_text_for_relation(node, 'Theme')\n            theme1_node = self.get_related_node(node, 'Theme')\n            theme2 = self.get_entity_text_for_relation(node, 'Theme2')\n\n            assert(theme1 is not None)\n            assert(theme2 is not None)\n\n            evidence = self.node_to_evidence(theme1_node, is_direct=True)\n            statements.append(Complex([s2a(theme1), s2a(theme2)],\n                              evidence=evidence))\n\n        return statements", "response": "Processes the Binding statements in the INDRA INDRA"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes an evidence object for a given node.", "response": "def node_to_evidence(self, entity_node, is_direct):\n        \"\"\"Computes an evidence object for a statement.\n\n        We assume that the entire event happens within a single statement, and\n        get the text of the sentence by getting the text of the sentence\n        containing the provided node that corresponds to one of the entities\n        participanting in the event.\n\n        The Evidence's pmid is whatever was provided to the constructor\n        (perhaps None), and the annotations are the subgraph containing the\n        provided node, its ancestors, and its descendants.\n        \"\"\"\n\n        # We assume that the entire event is within a single sentence, and\n        # get this sentence by getting the sentence containing one of the\n        # entities\n        sentence_text = self.G.node[entity_node]['sentence_text']\n\n        # Make annotations object containing the fully connected subgraph\n        # containing these nodes\n        subgraph = self.connected_subgraph(entity_node)\n        edge_properties = {}\n        for edge in subgraph.edges():\n            edge_properties[edge] = subgraph.edges[edge]\n\n        annotations = {'node_properties': subgraph.node,\n                       'edge_properties': edge_properties}\n\n        # Make evidence object\n        epistemics = dict()\n        evidence = Evidence(source_api='tees',\n                            pmid=self.pmid,\n                            text=sentence_text,\n                            epistemics={'direct': is_direct},\n                            annotations=annotations)\n        return evidence"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connected_subgraph(self, node):\n        G = self.G\n\n        subgraph_nodes = set()\n        subgraph_nodes.add(node)\n        subgraph_nodes.update(dag.ancestors(G, node))\n        subgraph_nodes.update(dag.descendants(G, node))\n\n        # Keep adding the ancesotrs and descendants on nodes of the graph\n        # until we can't do so any longer\n        graph_changed = True\n        while graph_changed:\n            initial_count = len(subgraph_nodes)\n\n            old_nodes = set(subgraph_nodes)\n            for n in old_nodes:\n                subgraph_nodes.update(dag.ancestors(G, n))\n                subgraph_nodes.update(dag.descendants(G, n))\n\n            current_count = len(subgraph_nodes)\n            graph_changed = current_count > initial_count\n\n        return G.subgraph(subgraph_nodes)", "response": "Returns the subgraph containing the given node its ancestors and its descendants."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a text string and return a TripsProcessor by processing it.", "response": "def process_text(text, save_xml_name='trips_output.xml', save_xml_pretty=True,\n                 offline=False, service_endpoint='drum'):\n    \"\"\"Return a TripsProcessor by processing text.\n\n    Parameters\n    ----------\n    text : str\n        The text to be processed.\n    save_xml_name : Optional[str]\n        The name of the file to save the returned TRIPS extraction knowledge\n        base XML. Default: trips_output.xml\n    save_xml_pretty : Optional[bool]\n        If True, the saved XML is pretty-printed. Some third-party tools\n        require non-pretty-printed XMLs which can be obtained by setting this\n        to False. Default: True\n    offline : Optional[bool]\n        If True, offline reading is used with a local instance of DRUM, if\n        available. Default: False\n    service_endpoint : Optional[str]\n        Selects the TRIPS/DRUM web service endpoint to use. Is a choice between\n        \"drum\" (default) and \"drum-dev\", a nightly build.\n\n    Returns\n    -------\n    tp : TripsProcessor\n        A TripsProcessor containing the extracted INDRA Statements\n        in tp.statements.\n    \"\"\"\n    if not offline:\n        html = client.send_query(text, service_endpoint)\n        xml = client.get_xml(html)\n    else:\n        if offline_reading:\n            try:\n                dr = DrumReader()\n                if dr is None:\n                    raise Exception('DrumReader could not be instantiated.')\n            except BaseException as e:\n                logger.error(e)\n                logger.error('Make sure drum/bin/trips-drum is running in'\n                              ' a separate process')\n                return None\n            try:\n                dr.read_text(text)\n                dr.start()\n            except SystemExit:\n                pass\n            xml = dr.extractions[0]\n        else:\n            logger.error('Offline reading with TRIPS/DRUM not available.')\n            logger.error('Error message was: %s' % offline_err)\n            msg = \"\"\"\n                To install DRUM locally, follow instructions at\n                https://github.com/wdebeaum/drum.\n                Next, install the pykqml package either from pip or from\n                https://github.com/bgyori/pykqml.\n                Once installed, run drum/bin/trips-drum in a separate process.\n                \"\"\"\n            logger.error(msg)\n            return None\n    if save_xml_name:\n        client.save_xml(xml, save_xml_name, save_xml_pretty)\n    return process_xml(xml)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_xml_file(file_name):\n    with open(file_name, 'rb') as fh:\n        ekb = fh.read().decode('utf-8')\n    return process_xml(ekb)", "response": "Returns a TripsProcessor by processing a TRIPS EKB XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_xml(xml_string):\n    tp = TripsProcessor(xml_string)\n    if tp.tree is None:\n        return None\n    tp.get_modifications_indirect()\n    tp.get_activations_causal()\n    tp.get_activations_stimulate()\n    tp.get_complexes()\n    tp.get_modifications()\n    tp.get_active_forms()\n    tp.get_active_forms_state()\n    tp.get_activations()\n    tp.get_translocation()\n    tp.get_regulate_amounts()\n    tp.get_degradations()\n    tp.get_syntheses()\n    tp.get_conversions()\n    tp.get_simple_increase_decrease()\n    return tp", "response": "Returns a TripsProcessor by processing a TRIPS EKB XML string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_eidos_curation_table():\n    url = 'https://raw.githubusercontent.com/clulab/eidos/master/' + \\\n        'src/main/resources/org/clulab/wm/eidos/english/confidence/' + \\\n        'rule_summary.tsv'\n    # Load the table of scores from the URL above into a data frame\n    res = StringIO(requests.get(url).text)\n    table = pandas.read_table(res, sep='\\t')\n    # Drop the last \"Grant total\" row\n    table = table.drop(table.index[len(table)-1])\n    return table", "response": "Return a pandas table of Eidos curation data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a BayesianScorer based on Eidos curation counts.", "response": "def get_eidos_bayesian_scorer(prior_counts=None):\n    \"\"\"Return a BayesianScorer based on Eidos curation counts.\"\"\"\n    table = load_eidos_curation_table()\n    subtype_counts = {'eidos': {r: [c, i] for r, c, i in\n                              zip(table['RULE'], table['Num correct'],\n                                  table['Num incorrect'])}}\n    prior_counts = prior_counts if prior_counts else copy.deepcopy(\n        default_priors)\n\n    scorer = BayesianScorer(prior_counts=prior_counts,\n                            subtype_counts=subtype_counts)\n    return scorer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_eidos_scorer():\n    table = load_eidos_curation_table()\n\n    # Get the overall precision\n    total_num = table['COUNT of RULE'].sum()\n    weighted_sum = table['COUNT of RULE'].dot(table['% correct'])\n    precision = weighted_sum / total_num\n    # We have to divide this into a random and systematic component, for now\n    # in an ad-hoc manner\n    syst_error = 0.05\n    rand_error = 1 - precision - syst_error\n    prior_probs = {'rand': {'eidos': rand_error}, 'syst': {'eidos': syst_error}}\n\n    # Get a dict of rule-specific errors.\n    subtype_probs = {'eidos':\n                     {k: 1.0-min(v, 0.95)-syst_error for k, v\n                      in zip(table['RULE'], table['% correct'])}}\n    scorer = SimpleScorer(prior_probs, subtype_probs)\n    return scorer", "response": "Return a SimpleScorer based on Eidos curated precision estimates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a TrrustProcessor based on the online interaction table.", "response": "def process_from_web():\n    \"\"\"Return a TrrustProcessor based on the online interaction table.\n\n    Returns\n    -------\n    TrrustProcessor\n        A TrrustProcessor object that has a list of INDRA Statements in its\n        statements attribute.\n    \"\"\"\n    logger.info('Downloading table from %s' % trrust_human_url)\n    res = requests.get(trrust_human_url)\n    res.raise_for_status()\n    df = pandas.read_table(io.StringIO(res.text))\n    tp = TrrustProcessor(df)\n    tp.extract_statements()\n    return tp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an output from RLIMS - P for the given PubMed ID or PMC ID.", "response": "def process_from_webservice(id_val, id_type='pmcid', source='pmc',\n                            with_grounding=True):\n    \"\"\"Return an output from RLIMS-p for the given PubMed ID or PMC ID.\n\n    Parameters\n    ----------\n    id_val : str\n        A PMCID, with the prefix PMC, or pmid, with no prefix, of the paper to\n        be \"read\".\n    id_type : str\n        Either 'pmid' or 'pmcid'. The default is 'pmcid'.\n    source : str\n        Either 'pmc' or 'medline', whether you want pmc fulltext or medline\n        abstracts.\n    with_grounding : bool\n        The RLIMS-P web service provides two endpoints, one pre-grounded, the\n        other not so much. The grounded endpoint returns far less content, and\n        may perform some grounding that can be handled by the grounding mapper.\n\n    Returns\n    -------\n    :py:class:`indra.sources.rlimsp.processor.RlimspProcessor`\n        An RlimspProcessor which contains a list of extracted INDRA Statements\n        in its statements attribute.\n    \"\"\"\n    if with_grounding:\n        fmt = '%s.normed/%s/%s'\n    else:\n        fmt = '%s/%s/%s'\n\n    resp = requests.get(RLIMSP_URL + fmt % (source, id_type, id_val))\n\n    if resp.status_code != 200:\n        raise RLIMSP_Error(\"Bad status code: %d - %s\"\n                           % (resp.status_code, resp.reason))\n\n    rp = RlimspProcessor(resp.json())\n    rp.extract_statements()\n    return rp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing RLIMS - P extractions from a bulk - download JSON file.", "response": "def process_from_json_file(filename, doc_id_type=None):\n    \"\"\"Process RLIMSP extractions from a bulk-download JSON file.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the JSON file.\n    doc_id_type : Optional[str]\n        In some cases the RLIMS-P paragraph info doesn't contain 'pmid' or\n        'pmcid' explicitly, instead if contains a 'docId' key. This parameter\n        allows defining what ID type 'docId' sould be interpreted as. Its\n        values should be 'pmid' or 'pmcid' or None if not used.\n\n    Returns\n    -------\n    :py:class:`indra.sources.rlimsp.processor.RlimspProcessor`\n        An RlimspProcessor which contains a list of extracted INDRA Statements\n        in its statements attribute.\n    \"\"\"\n    with open(filename, 'rt') as f:\n        lines = f.readlines()\n        json_list = []\n        for line in lines:\n            json_list.append(json.loads(line))\n        rp = RlimspProcessor(json_list, doc_id_type=doc_id_type)\n        rp.extract_statements()\n    return rp"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_dict(self):\n        \"Convert this into an ordinary dict (of dicts).\"\n        return {k: v.export_dict() if isinstance(v, self.__class__) else v\n                for k, v in self.items()}", "response": "Convert this into an ordinary dict ( of dicts).\"\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the first value within the tree which has the key.", "response": "def get(self, key):\n        \"Find the first value within the tree which has the key.\"\n        if key in self.keys():\n            return self[key]\n        else:\n            res = None\n            for v in self.values():\n                # This could get weird if the actual expected returned value\n                # is None, especially in teh case of overlap. Any ambiguity\n                # would be resolved by get_path(s).\n                if hasattr(v, 'get'):\n                    res = v.get(key)\n                if res is not None:\n                    break\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_path(self, key):\n        \"Like `get`, but also return the path taken to the value.\"\n        if key in self.keys():\n            return (key,), self[key]\n        else:\n            key_path, res = (None, None)\n            for sub_key, v in self.items():\n                if isinstance(v, self.__class__):\n                    key_path, res = v.get_path(key)\n                elif hasattr(v, 'get'):\n                    res = v.get(key)\n                    key_path = (key,) if res is not None else None\n                if res is not None and key_path is not None:\n                    key_path = (sub_key,) + key_path\n                    break\n            return key_path, res", "response": "Like get but also return the path taken to the value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef gets(self, key):\n        \"Like `get`, but return all matches, not just the first.\"\n        result_list = []\n        if key in self.keys():\n            result_list.append(self[key])\n        for v in self.values():\n            if isinstance(v, self.__class__):\n                sub_res_list = v.gets(key)\n                for res in sub_res_list:\n                    result_list.append(res)\n            elif isinstance(v, dict):\n                if key in v.keys():\n                    result_list.append(v[key])\n        return result_list", "response": "Like get but return all matches not just the first."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_paths(self, key):\n        \"Like `gets`, but include the paths, like `get_path` for all matches.\"\n        result_list = []\n        if key in self.keys():\n            result_list.append(((key,), self[key]))\n        for sub_key, v in self.items():\n            if isinstance(v, self.__class__):\n                sub_res_list = v.get_paths(key)\n                for key_path, res in sub_res_list:\n                    result_list.append(((sub_key,) + key_path, res))\n            elif isinstance(v, dict):\n                if key in v.keys():\n                    result_list.append(((sub_key, key), v[key]))\n        return result_list", "response": "Like gets but include the paths like get_path for all matches."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_leaves(self):\n        ret_set = set()\n        for val in self.values():\n            if isinstance(val, self.__class__):\n                ret_set |= val.get_leaves()\n            elif isinstance(val, dict):\n                ret_set |= set(val.values())\n            elif isinstance(val, list):\n                ret_set |= set(val)\n            elif isinstance(val, set):\n                ret_set |= val\n            else:\n                ret_set.add(val)\n        return ret_set", "response": "Get the deepest entries as a flat set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _read_reach_rule_regexps():\n    reach_rule_filename = \\\n        os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                     'reach_rule_regexps.txt')\n    with open(reach_rule_filename, 'r') as f:\n        reach_rule_regexp = []\n        for line in f:\n            reach_rule_regexp.append(line.rstrip())\n    return reach_rule_regexp", "response": "Load in a file with the regular expressions corresponding to each available reach rule. Why regular expression matching?\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine the category of reach rule from the reach rule instance.", "response": "def determine_reach_subtype(event_name):\n    \"\"\"Returns the category of reach rule from the reach rule instance.\n\n    Looks at a list of regular\n    expressions corresponding to reach rule types, and returns the longest\n    regexp that matches, or None if none of them match.\n\n    Parameters\n    ----------\n    evidence : indra.statements.Evidence\n        A reach evidence object to subtype\n\n    Returns\n    -------\n    best_match : str\n        A regular expression corresponding to the reach rule that was used to\n        extract this evidence\n    \"\"\"\n\n    best_match_length = None\n    best_match = None\n    for ss in reach_rule_regexps:\n        if re.search(ss, event_name):\n            if best_match is None or len(ss) > best_match_length:\n                best_match = ss\n                best_match_length = len(ss)\n\n    return best_match"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_event_statistics(self):\n        logger.info('All events by type')\n        logger.info('-------------------')\n        for k, v in self.all_events.items():\n            logger.info('%s, %s' % (k, len(v)))\n        logger.info('-------------------')", "response": "Print the number of events in the REACH output by type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_events(self):\n        self.all_events = {}\n        events = self.tree.execute(\"$.events.frames\")\n        if events is None:\n            return\n        for e in events:\n            event_type = e.get('type')\n            frame_id = e.get('frame_id')\n            try:\n                self.all_events[event_type].append(frame_id)\n            except KeyError:\n                self.all_events[event_type] = [frame_id]", "response": "Gather all event IDs in the REACH output by type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_modifications(self):\n        # Find all event frames that are a type of protein modification\n        qstr = \"$.events.frames[(@.type is 'protein-modification')]\"\n        res = self.tree.execute(qstr)\n        if res is None:\n            return\n        # Extract each of the results when possible\n        for r in res:\n            # The subtype of the modification\n            modification_type = r.get('subtype')\n\n            # Skip negated events (i.e. something doesn't happen)\n            epistemics = self._get_epistemics(r)\n            if epistemics.get('negated'):\n                continue\n\n            annotations, context = self._get_annot_context(r)\n            frame_id = r['frame_id']\n            args = r['arguments']\n            site = None\n            theme = None\n\n            # Find the substrate (the \"theme\" agent here) and the\n            # site and position it is modified on\n            for a in args:\n                if self._get_arg_type(a) == 'theme':\n                    theme = a['arg']\n                elif self._get_arg_type(a) == 'site':\n                    site = a['text']\n            theme_agent, theme_coords = self._get_agent_from_entity(theme)\n            if site is not None:\n                mods = self._parse_site_text(site)\n            else:\n                mods = [(None, None)]\n\n            for mod in mods:\n                # Add up to one statement for each site\n                residue, pos = mod\n\n                # Now we need to look for all regulation event to get to the\n                # enzymes (the \"controller\" here)\n                qstr = \"$.events.frames[(@.type is 'regulation') and \" + \\\n                       \"(@.arguments[0].arg is '%s')]\" % frame_id\n                reg_res = self.tree.execute(qstr)\n                reg_res = list(reg_res)\n                for reg in reg_res:\n                    controller_agent, controller_coords = None, None\n                    for a in reg['arguments']:\n                        if self._get_arg_type(a) == 'controller':\n                            controller = a.get('arg')\n                            if controller is not None:\n                                controller_agent, controller_coords = \\\n                                    self._get_agent_from_entity(controller)\n                                break\n                    # Check the polarity of the regulation and if negative,\n                    # flip the modification type.\n                    # For instance, negative-regulation of a phosphorylation\n                    # will become an (indirect) dephosphorylation\n                    reg_subtype = reg.get('subtype')\n                    if reg_subtype == 'negative-regulation':\n                        modification_type = \\\n                            modtype_to_inverse.get(modification_type)\n                        if not modification_type:\n                            logger.warning('Unhandled modification type: %s' %\n                                           modification_type)\n                            continue \n\n                    sentence = reg['verbose-text']\n                    annotations['agents']['coords'] = [controller_coords,\n                                                       theme_coords]\n                    ev = Evidence(source_api='reach', text=sentence,\n                                  annotations=annotations, pmid=self.citation,\n                                  context=context, epistemics=epistemics)\n                    args = [controller_agent, theme_agent, residue, pos, ev]\n\n                    # Here ModStmt is a sub-class of Modification\n                    ModStmt = modtype_to_modclass.get(modification_type)\n                    if ModStmt is None:\n                        logger.warning('Unhandled modification type: %s' %\n                                       modification_type)\n                    else:\n                        # Handle this special case here because only\n                        # enzyme argument is needed\n                        if modification_type == 'autophosphorylation':\n                            args = [theme_agent, residue, pos, ev]\n                        self.statements.append(ModStmt(*args))", "response": "Extract Modification INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts RegulateAmount INDRA Statements.", "response": "def get_regulate_amounts(self):\n        \"\"\"Extract RegulateAmount INDRA Statements.\"\"\"\n        qstr = \"$.events.frames[(@.type is 'transcription')]\"\n        res = self.tree.execute(qstr)\n        all_res = []\n        if res is not None:\n            all_res += list(res)\n        qstr = \"$.events.frames[(@.type is 'amount')]\"\n        res = self.tree.execute(qstr)\n        if res is not None:\n            all_res += list(res)\n\n        for r in all_res:\n            subtype = r.get('subtype')\n            epistemics = self._get_epistemics(r)\n            if epistemics.get('negated'):\n                continue\n            annotations, context = self._get_annot_context(r)\n            frame_id = r['frame_id']\n            args = r['arguments']\n            theme = None\n            for a in args:\n                if self._get_arg_type(a) == 'theme':\n                    theme = a['arg']\n                    break\n            if theme is None:\n                continue\n            theme_agent, theme_coords = self._get_agent_from_entity(theme)\n            qstr = \"$.events.frames[(@.type is 'regulation') and \" + \\\n                   \"(@.arguments[0].arg is '%s')]\" % frame_id\n            reg_res = self.tree.execute(qstr)\n            for reg in reg_res:\n                controller_agent, controller_coords = None, None\n                for a in reg['arguments']:\n                    if self._get_arg_type(a) == 'controller':\n                        controller_agent, controller_coords = \\\n                            self._get_controller_agent(a)\n                sentence = reg['verbose-text']\n                annotations['agents']['coords'] = [controller_coords,\n                                                   theme_coords]\n                ev = Evidence(source_api='reach', text=sentence,\n                              annotations=annotations, pmid=self.citation,\n                              context=context, epistemics=epistemics)\n                args = [controller_agent, theme_agent, ev]\n                subtype = reg.get('subtype')\n                if subtype == 'positive-regulation':\n                    st = IncreaseAmount(*args)\n                else:\n                    st = DecreaseAmount(*args)\n                self.statements.append(st)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_complexes(self):\n        qstr = \"$.events.frames[@.type is 'complex-assembly']\"\n        res = self.tree.execute(qstr)\n        if res is None:\n            return\n\n        for r in res:\n            epistemics = self._get_epistemics(r)\n            if epistemics.get('negated'):\n                continue\n            # Due to an issue with the REACH output serialization\n            # (though seemingly not with the raw mentions), sometimes\n            # a redundant complex-assembly event is reported which can\n            # be recognized by the missing direct flag, which we can filter\n            # for here\n            if epistemics.get('direct') is None:\n                continue\n            annotations, context = self._get_annot_context(r)\n            args = r['arguments']\n            sentence = r['verbose-text']\n            members = []\n            agent_coordinates = []\n            for a in args:\n                agent, coords = self._get_agent_from_entity(a['arg'])\n                members.append(agent)\n                agent_coordinates.append(coords)\n            annotations['agents']['coords'] = agent_coordinates\n            ev = Evidence(source_api='reach', text=sentence,\n                          annotations=annotations, pmid=self.citation,\n                          context=context, epistemics=epistemics)\n            stmt = Complex(members, ev)\n            self.statements.append(stmt)", "response": "Extract INDRA Complex Statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract INDRA Activation Statements.", "response": "def get_activation(self):\n        \"\"\"Extract INDRA Activation Statements.\"\"\"\n        qstr = \"$.events.frames[@.type is 'activation']\"\n        res = self.tree.execute(qstr)\n        if res is None:\n            return\n        for r in res:\n            epistemics = self._get_epistemics(r)\n            if epistemics.get('negated'):\n                continue\n            sentence = r['verbose-text']\n            annotations, context = self._get_annot_context(r)\n            ev = Evidence(source_api='reach', text=sentence,\n                          pmid=self.citation, annotations=annotations,\n                          context=context, epistemics=epistemics)\n            args = r['arguments']\n            for a in args:\n                if self._get_arg_type(a) == 'controller':\n                    controller_agent, controller_coords = \\\n                        self._get_controller_agent(a)\n                if self._get_arg_type(a) == 'controlled':\n                    controlled = a['arg']\n            controlled_agent, controlled_coords = \\\n                self._get_agent_from_entity(controlled)\n            annotations['agents']['coords'] = [controller_coords,\n                                               controlled_coords]\n            if r['subtype'] == 'positive-activation':\n                st = Activation(controller_agent, controlled_agent,\n                                evidence=ev)\n            else:\n                st = Inhibition(controller_agent, controlled_agent,\n                                evidence=ev)\n            self.statements.append(st)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts INDRA Translocation Statements.", "response": "def get_translocation(self):\n        \"\"\"Extract INDRA Translocation Statements.\"\"\"\n        qstr = \"$.events.frames[@.type is 'translocation']\"\n        res = self.tree.execute(qstr)\n        if res is None:\n            return\n        for r in res:\n            epistemics = self._get_epistemics(r)\n            if epistemics.get('negated'):\n                continue\n            sentence = r['verbose-text']\n            annotations, context = self._get_annot_context(r)\n            args = r['arguments']\n            from_location = None\n            to_location = None\n            for a in args:\n                if self._get_arg_type(a) == 'theme':\n                    agent, theme_coords = self._get_agent_from_entity(a['arg'])\n                    if agent is None:\n                        continue\n                elif self._get_arg_type(a) == 'source':\n                    from_location = self._get_location_by_id(a['arg'])\n                elif self._get_arg_type(a) == 'destination':\n                    to_location = self._get_location_by_id(a['arg'])\n            annotations['agents']['coords'] = [theme_coords]\n            ev = Evidence(source_api='reach', text=sentence,\n                          pmid=self.citation, annotations=annotations,\n                          context=context, epistemics=epistemics)\n            st = Translocation(agent, from_location, to_location,\n                               evidence=ev)\n            self.statements.append(st)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of ModConditions given a mod term dict.", "response": "def _get_mod_conditions(self, mod_term):\n        \"\"\"Return a list of ModConditions given a mod term dict.\"\"\"\n        site = mod_term.get('site')\n        if site is not None:\n            mods = self._parse_site_text(site)\n        else:\n            mods = [Site(None, None)]\n\n        mcs = []\n        for mod in mods:\n            mod_res, mod_pos = mod\n            mod_type_str = mod_term['type'].lower()\n            mod_state = agent_mod_map.get(mod_type_str)\n            if mod_state is not None:\n                mc = ModCondition(mod_state[0], residue=mod_res,\n                                  position=mod_pos, is_modified=mod_state[1])\n                mcs.append(mc)\n            else:\n                logger.warning('Unhandled entity modification type: %s'\n                               % mod_type_str)\n        return mcs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_entity_coordinates(self, entity_term):\n        # The following lines get the starting coordinate of the sentence\n        # containing the entity.\n        sent_id = entity_term.get('sentence')\n        if sent_id is None:\n            return None\n        qstr = \"$.sentences.frames[(@.frame_id is \\'%s')]\" % sent_id\n        res = self.tree.execute(qstr)\n        if res is None:\n            return None\n        try:\n            sentence = next(res)\n        except StopIteration:\n            return None\n        sent_start = sentence.get('start-pos')\n        if sent_start is None:\n            return None\n        sent_start = sent_start.get('offset')\n        if sent_start is None:\n            return None\n        # Get the entity coordinate in the entire text and subtract the\n        # coordinate of the first character in the associated sentence to\n        # get the sentence coordinate of the entity. Return None if entity\n        # coordinates are missing\n        entity_start = entity_term.get('start-pos')\n        entity_stop = entity_term.get('end-pos')\n        if entity_start is None or entity_stop is None:\n            return None\n        entity_start = entity_start.get('offset')\n        entity_stop = entity_stop.get('offset')\n        if entity_start is None or entity_stop is None:\n            return None\n        return (entity_start - sent_start, entity_stop - sent_start)", "response": "Get the entity coordinates for a given entity term."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_section(self, event):\n        sentence_id = event.get('sentence')\n        section = None\n        if sentence_id:\n            qstr = \"$.sentences.frames[(@.frame_id is \\'%s\\')]\" % sentence_id\n            res = self.tree.execute(qstr)\n            if res:\n                sentence_frame = list(res)[0]\n                passage_id = sentence_frame.get('passage')\n                if passage_id:\n                    qstr = \"$.sentences.frames[(@.frame_id is \\'%s\\')]\" % \\\n                            passage_id\n                    res = self.tree.execute(qstr)\n                    if res:\n                        passage_frame = list(res)[0]\n                        section = passage_frame.get('section-id')\n        # If the section is in the standard list, return as is\n        if section in self._section_list:\n            return section\n        # Next, handle a few special cases that come up in practice\n        elif section.startswith('fig'):\n            return 'figure'\n        elif section.startswith('supm'):\n            return 'supplementary'\n        elif section == 'article-title':\n            return 'title'\n        elif section in ['subjects|methods', 'methods|subjects']:\n            return 'methods'\n        elif section == 'conclusions':\n            return 'conclusion'\n        elif section == 'intro':\n            return 'introduction'\n        else:\n            return None", "response": "Get the section of the paper that the event is from."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_controller_agent(self, arg):\n        controller_agent = None\n        controller = arg.get('arg')\n        # There is either a single controller here\n        if controller is not None:\n            controller_agent, coords = self._get_agent_from_entity(controller)\n        # Or the controller is a complex\n        elif arg['argument-type'] == 'complex':\n            controllers = list(arg.get('args').values())\n            controller_agent, coords = \\\n                self._get_agent_from_entity(controllers[0])\n            bound_agents = [self._get_agent_from_entity(c)[0]\n                            for c in controllers[1:]]\n            bound_conditions = [BoundCondition(ba, True) for\n                                ba in bound_agents]\n            controller_agent.bound_conditions = bound_conditions\n        return controller_agent, coords", "response": "Return a single or a complex controller agent."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _sanitize(text):\n    d = {'-LRB-': '(', '-RRB-': ')'}\n    return re.sub('|'.join(d.keys()), lambda m: d[m.group(0)], text)", "response": "Return sanitized Eidos text field for human readability."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a datetime object from a timex constraint start or end entry.", "response": "def _get_time_stamp(entry):\n    \"\"\"Return datetime object from a timex constraint start/end entry.\n\n    Example string format to convert: 2018-01-01T00:00\n    \"\"\"\n    if not entry or entry == 'Undef':\n        return None\n    try:\n        dt = datetime.datetime.strptime(entry, '%Y-%m-%dT%H:%M')\n    except Exception as e:\n        logger.debug('Could not parse %s format' % entry)\n        return None\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a RefContext object given a geoloc entry.", "response": "def ref_context_from_geoloc(geoloc):\n    \"\"\"Return a RefContext object given a geoloc entry.\"\"\"\n    text = geoloc.get('text')\n    geoid = geoloc.get('geoID')\n    rc = RefContext(name=text, db_refs={'GEOID': geoid})\n    return rc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a TimeContext object given a timex entry.", "response": "def time_context_from_timex(timex):\n    \"\"\"Return a TimeContext object given a timex entry.\"\"\"\n    time_text = timex.get('text')\n    constraint = timex['intervals'][0]\n    start = _get_time_stamp(constraint.get('start'))\n    end = _get_time_stamp(constraint.get('end'))\n    duration = constraint['duration']\n    tc = TimeContext(text=time_text, start=start, end=end,\n                     duration=duration)\n    return tc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_args(event, arg_type):\n    args = event.get('arguments', {})\n    obj_tags = [arg for arg in args if arg['type'] == arg_type]\n    if obj_tags:\n        return [o['value']['@id'] for o in obj_tags]\n    else:\n        return []", "response": "Return IDs of all arguments of a given type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_causal_relations(self):\n        # Get the extractions that are labeled as directed and causal\n        relations = [e for e in self.doc.extractions if\n                     'DirectedRelation' in e['labels'] and\n                     'Causal' in e['labels']]\n        # For each relation, we try to extract an INDRA Statement and\n        # save it if its valid\n        for relation in relations:\n            stmt = self.get_causal_relation(relation)\n            if stmt is not None:\n                self.statements.append(stmt)", "response": "Extract causal relations as Statements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_evidence(self, relation):\n        provenance = relation.get('provenance')\n\n        # First try looking up the full sentence through provenance\n        text = None\n        context = None\n        if provenance:\n            sentence_tag = provenance[0].get('sentence')\n            if sentence_tag and '@id' in sentence_tag:\n                sentence_id = sentence_tag['@id']\n                sentence = self.doc.sentences.get(sentence_id)\n                if sentence is not None:\n                    text = _sanitize(sentence['text'])\n                # Get temporal constraints if available\n                timexes = sentence.get('timexes', [])\n                if timexes:\n                    # We currently handle just one timex per statement\n                    timex = timexes[0]\n                    tc = time_context_from_timex(timex)\n                    context = WorldContext(time=tc)\n                # Get geolocation if available\n                geolocs = sentence.get('geolocs', [])\n                if geolocs:\n                    geoloc = geolocs[0]\n                    rc = ref_context_from_geoloc(geoloc)\n                    if context:\n                        context.geo_location = rc\n                    else:\n                        context = WorldContext(geo_location=rc)\n\n            # Here we try to get the title of the document and set it\n            # in the provenance\n            doc_id = provenance[0].get('document', {}).get('@id')\n            if doc_id:\n                title = self.doc.documents.get(doc_id, {}).get('title')\n                if title:\n                    provenance[0]['document']['title'] = title\n\n        annotations = {'found_by': relation.get('rule'),\n                       'provenance': provenance}\n        if self.doc.dct is not None:\n            annotations['document_creation_time'] = self.doc.dct.to_json()\n\n        epistemics = {}\n        negations = self.get_negation(relation)\n        hedgings = self.get_hedging(relation)\n        if hedgings:\n            epistemics['hedgings'] = hedgings\n        if negations:\n            # This is the INDRA standard to show negation\n            epistemics['negated'] = True\n            # But we can also save the texts associated with the negation\n            # under annotations, just in case it's needed\n            annotations['negated_texts'] = negations\n\n        # If that fails, we can still get the text of the relation\n        if text is None:\n            text = _sanitize(event.get('text'))\n\n        ev = Evidence(source_api='eidos', text=text, annotations=annotations,\n                      context=context, epistemics=epistemics)\n        return ev", "response": "Get the Evidence object for the INDRA Statment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the negation attached to an event.", "response": "def get_negation(event):\n        \"\"\"Return negation attached to an event.\n\n        Example: \"states\": [{\"@type\": \"State\", \"type\": \"NEGATION\",\n                             \"text\": \"n't\"}]\n        \"\"\"\n        states = event.get('states', [])\n        if not states:\n            return []\n        negs = [state for state in states\n                if state.get('type') == 'NEGATION']\n        neg_texts = [neg['text'] for neg in negs]\n        return neg_texts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the hedging markers attached to an event.", "response": "def get_hedging(event):\n        \"\"\"Return hedging markers attached to an event.\n\n        Example: \"states\": [{\"@type\": \"State\", \"type\": \"HEDGE\",\n                             \"text\": \"could\"}\n        \"\"\"\n        states = event.get('states', [])\n        if not states:\n            return []\n        hedgings = [state for state in states\n                    if state.get('type') == 'HEDGE']\n        hedging_texts = [hedging['text'] for hedging in hedgings]\n        return hedging_texts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the groundings as db_refs for an entity.", "response": "def get_groundings(entity):\n        \"\"\"Return groundings as db_refs for an entity.\"\"\"\n        def get_grounding_entries(grounding):\n            if not grounding:\n                return None\n\n            entries = []\n            values = grounding.get('values', [])\n            # Values could still have been a None entry here\n            if values:\n                for entry in values:\n                    ont_concept = entry.get('ontologyConcept')\n                    value = entry.get('value')\n                    if ont_concept is None or value is None:\n                        continue\n                    entries.append((ont_concept, value))\n            return entries\n\n        # Save raw text and Eidos scored groundings as db_refs\n        db_refs = {'TEXT': entity['text']}\n        groundings = entity.get('groundings')\n        if not groundings:\n            return db_refs\n        for g in groundings:\n            entries = get_grounding_entries(g)\n            # Only add these groundings if there are actual values listed\n            if entries:\n                key = g['name'].upper()\n                if key == 'UN':\n                    db_refs[key] = [(s[0].replace(' ', '_'), s[1])\n                                    for s in entries]\n                else:\n                    db_refs[key] = entries\n        return db_refs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_concept(entity):\n        # Use the canonical name as the name of the Concept\n        name = entity['canonicalName']\n        db_refs = EidosProcessor.get_groundings(entity)\n        concept = Concept(name, db_refs=db_refs)\n        return concept", "response": "Return Concept from an Eidos entity."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef time_context_from_ref(self, timex):\n        # If the timex has a value set, it means that it refers to a DCT or\n        # a TimeExpression e.g. \"value\": {\"@id\": \"_:DCT_1\"} and the parameters\n        # need to be taken from there\n        value = timex.get('value')\n        if value:\n            # Here we get the TimeContext directly from the stashed DCT\n            # dictionary\n            tc = self.doc.timexes.get(value['@id'])\n            return tc\n        return None", "response": "Return a time context object given a timex reference entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geo_context_from_ref(self, ref):\n        value = ref.get('value')\n        if value:\n            # Here we get the RefContext from the stashed geoloc dictionary\n            rc = self.doc.geolocs.get(value['@id'])\n            return rc\n        return None", "response": "Return a RefContext object given a location reference entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a time context object given a DCT entry.", "response": "def time_context_from_dct(dct):\n        \"\"\"Return a time context object given a DCT entry.\"\"\"\n        time_text = dct.get('text')\n        start = _get_time_stamp(dct.get('start'))\n        end = _get_time_stamp(dct.get('end'))\n        duration = dct.get('duration')\n        tc = TimeContext(text=time_text, start=start, end=end,\n                         duration=duration)\n        return tc"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_hash(s, n_bytes):\n    raw_h = int(md5(s.encode('utf-8')).hexdigest()[:n_bytes], 16)\n    # Make it a signed int.\n    return 16**n_bytes//2 - raw_h", "response": "Make the hash from a matches key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse an a1 file and returns a dictionary mapping TEES identifiers to TEESEntity objects.", "response": "def parse_a1(a1_text):\n    \"\"\"Parses an a1 file, the file TEES outputs that lists the entities in\n    the extracted events.\n\n    Parameters\n    ----------\n    a1_text : str\n        Text of the TEES a1 output file, specifying the entities\n\n    Returns\n    -------\n    entities : Dictionary mapping TEES identifiers to TEESEntity objects\n        describing each entity. Each row of the .a1 file corresponds to one\n        TEESEntity object.\n    \"\"\"\n    entities = {}\n\n    for line in a1_text.split('\\n'):\n        if len(line) == 0:\n            continue\n        tokens = line.rstrip().split('\\t')\n        if len(tokens) != 3:\n            raise Exception('Expected three tab-seperated tokens per line ' +\n                            'in the a1 file output from TEES.')\n\n        identifier = tokens[0]\n        entity_info = tokens[1]\n        entity_name = tokens[2]\n\n        info_tokens = entity_info.split()\n        if len(info_tokens) != 3:\n            raise Exception('Expected three space-seperated tokens in the ' + \n                            'second column of the a2 file output from TEES.')\n        entity_type = info_tokens[0]\n        first_offset = int(info_tokens[1])\n        second_offset = int(info_tokens[2])\n        offsets = (first_offset, second_offset)\n\n        entities[identifier] = TEESEntity(\n                identifier,\n                entity_type,\n                entity_name,\n                offsets)\n\n    return entities"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract events from a TEES a2 output into a networkx directed graph.", "response": "def parse_a2(a2_text, entities, tees_sentences):\n    \"\"\"Extracts events from a TEES a2 output into a networkx directed graph.\n\n    Parameters\n    ----------\n    a2_text : str\n        Text of the TEES a2 file output, specifying the event graph\n    sentences_xml_gz : str\n        Filename with the TEES sentence segmentation in a gzipped xml format\n\n    Returns\n    -------\n    events :\n        A networkx graph of events. Node names are entity and event labels\n        in the original A2 file (such as \"E2\" or \"T1\") and edges between nodes\n        are the various properties. Text nodes (ex. \"T1\") have a text node\n        property that gives the text.\n    \"\"\"\n    G = nx.DiGraph()\n    event_names = set()\n\n    # Put entities into the graph\n    for entity_name in entities.keys():\n        offset0 = entities[entity_name].offsets[0]\n        G.add_node(entity_name, text=entities[entity_name].entity_name,\n                   type=entities[entity_name].entity_type, is_event=False,\n                   sentence_text=tees_sentences.index_to_sentence(offset0))\n\n    for line in a2_text.split('\\n'):\n        if len(line) == 0:\n            continue\n        if line[0] == 'T':  # New text\n            tokens = line.rstrip().split('\\t')\n            identifier = tokens[0]\n            text = tokens[2]\n\n            if identifier not in G.node:\n                G.add_node(identifier)\n            G.node[identifier]['text'] = text\n            G.node[identifier]['is_event'] = False\n\n        elif line[0] == 'E':  # New event\n            tokens = line.rstrip().split('\\t')\n            if len(tokens) != 2:\n                raise Exception('Expected two tab-separated tokens per line ' +\n                                'in TEES a2 file.')\n\n            event_identifier = tokens[0]\n\n            # In the second tab-separated token, we have a series of keys\n            # and values separated by the colon\n            key_value_pairs = tokens[1].split()\n            event_name = key_value_pairs[0].split(':')[0]\n            properties = dict()\n            for pair in key_value_pairs:\n                key_and_value = pair.split(':')\n                if len(key_and_value) != 2:\n                    raise Exception('Expected two colon-separated tokens ' + \n                                    'in the second column of the a2 file ' + \n                                    'output from TEES.')\n                properties[key_and_value[0]] = key_and_value[1]\n\n            # Add event to the graph if we haven't added it yet\n            if event_identifier not in G.node:\n                G.add_node(event_identifier)\n\n            # Add edges\n            for key in properties.keys():\n                G.add_edge(event_identifier, properties[key],\n                           relation=key)\n\n            # We assume that node is not negated unless a event modifier\n            # later says otherwise\n            G.node[event_identifier]['negated'] = False\n            G.node[event_identifier]['speculation'] = False\n            G.node[event_identifier]['type'] = event_name\n            G.node[event_identifier]['is_event'] = True\n\n            event_names.add(event_name)\n\n        elif line[0] == 'M':  # Event modification\n            tokens = line.split('\\t')\n            if len(tokens) == 2:\n                raise Exception('Expected two tab-separated tokens per line ' +\n                                'in the a2 file output from TEES.')\n\n            tokens2 = tokens[1].split()\n            if len(tokens2) == 2:\n                raise Exception('Expected two space-separated tokens per ' + \n                                'line in the a2 file output from TEES.')\n            modification_type = tokens2[0]\n            modified = tokens2[1]\n\n            # But assuming this is a negation modifier, we'll need to\n            # handle it\n            if modification_type == 'Negation':\n                G.node[modified]['negated'] = True\n            elif modification_type == 'Speculation':\n                G.node[modified]['speculation'] = True\n            else:\n                # I've only seen negation event modifiers in these outputs\n                # If there are other types of modifications,\n                # we'll need to handle them, since it could\n                # affect whether we want to process them into statements\n                print('Unknown negation event: %s' % line)\n                assert(False)\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the TEES output of TEES reader and returns a networkx graph with the entities events and relationship between the TEES events.", "response": "def parse_output(a1_text, a2_text, sentence_segmentations):\n    \"\"\"Parses the output of the TEES reader and returns a networkx graph\n    with the event information.\n\n    Parameters\n    ----------\n    a1_text : str\n        Contents of the TEES a1 output, specifying the entities\n    a1_text : str\n        Contents of the TEES a2 output, specifying the event graph\n    sentence_segmentations : str\n        Concents of the TEES sentence segmentation output XML\n\n    Returns\n    -------\n    events : networkx.DiGraph\n        networkx graph with the entities, events, and relationship between\n        extracted by TEES\n    \"\"\"\n\n    # Parse the sentence segmentation document\n    tees_sentences = TEESSentences(sentence_segmentations)\n\n    # Parse the a1 (entities) file\n    entities = parse_a1(a1_text)\n\n    # Parse the a2 (events) file\n    events = parse_a2(a2_text, entities, tees_sentences)\n\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tees_parse_networkx_to_dot(G, output_file, subgraph_nodes):\n\n    with codecs.open(output_file, 'w', encoding='utf-8') as f:\n        f.write('digraph teesParse {\\n')\n\n        mentioned_nodes = set()\n\n        for from_node in subgraph_nodes:\n            for edge in G.edges(from_node):\n                to_node = edge[1]\n\n                mentioned_nodes.add(from_node)\n                mentioned_nodes.add(to_node)\n                relation = G.edges[from_node, to_node]['relation']\n                f.write('%s -> %s [ label = \"%s\" ];\\n' % (from_node, to_node,\n                        relation))\n\n        for node in mentioned_nodes:\n            is_event = G.node[node]['is_event']\n            if is_event:\n                node_type = G.node[node]['type']\n                negated = G.node[node]['negated']\n                speculation = G.node[node]['speculation']\n\n                # Add a tag to the label if the event is negated or speculation\n                if negated and speculation:\n                    tag = ' {NS}'\n                elif negated:\n                    tag = ' {N}'\n                elif speculation:\n                    tag = ' {S}'\n                else:\n                    tag = ''\n\n                node_label = node_type + tag\n            else:\n                node_label = G.node[node]['text']\n            f.write('%s [label=\"%s\"];\\n' % (node, node_label))\n\n        f.write('}\\n')", "response": "Converts TEES extractions stored in a networkx graph into a graphviz DOT file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_event(self, event, find_str):\n        # Get the term with the given element id\n        element = event.find(find_str)\n        if element is None:\n            return None\n        element_id = element.attrib.get('id')\n        element_term = self.tree.find(\"*[@id='%s']\" % element_id)\n        if element_term is None:\n            return None\n        time, location = self._extract_time_loc(element_term)\n\n        # Now see if there is a modifier like assoc-with connected\n        # to the main concept\n        assoc_with = self._get_assoc_with(element_term)\n\n        # Get the element's text and use it to construct a Concept\n        element_text_element = element_term.find('text')\n        if element_text_element is None:\n            return None\n        element_text = element_text_element.text\n        element_db_refs = {'TEXT': element_text}\n        element_name = sanitize_name(element_text)\n\n        element_type_element = element_term.find('type')\n        if element_type_element is not None:\n            element_db_refs['CWMS'] = element_type_element.text\n            # If there's an assoc-with, we tack it on as extra grounding\n            if assoc_with is not None:\n                element_db_refs['CWMS'] += ('|%s' % assoc_with)\n\n        concept = Concept(element_name, db_refs=element_db_refs)\n        if time or location:\n            context = WorldContext(time=time, geo_location=location)\n        else:\n            context = None\n        event_obj = Event(concept, context=context)\n        return event_obj", "response": "Get a concept referred from the event by the given string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting the time and loc from a term.", "response": "def _extract_time_loc(self, term):\n        \"\"\"Get the location from a term (CC or TERM)\"\"\"\n        loc = term.find('location')\n        if loc is None:\n            loc_context = None\n        else:\n            loc_id = loc.attrib.get('id')\n            loc_term = self.tree.find(\"*[@id='%s']\" % loc_id)\n            text = loc_term.findtext('text')\n            name = loc_term.findtext('name')\n            loc_context = RefContext(name=text)\n        time = term.find('time')\n        if time is None:\n            time_context = None\n        else:\n            time_id = time.attrib.get('id')\n            time_term = self.tree.find(\"*[@id='%s']\" % time_id)\n            if time_term is not None:\n                text = time_term.findtext('text')\n                timex = time_term.find('timex')\n                if timex is not None:\n                    year = timex.findtext('year')\n                    try:\n                        year = int(year)\n                    except Exception:\n                        year = None\n                    month = timex.findtext('month')\n                    day = timex.findtext('day')\n                    if year and (month or day):\n                        try:\n                            month = int(month)\n                        except Exception:\n                            month = 1\n                        try:\n                            day = int(day)\n                        except Exception:\n                            day = 1\n                        start = datetime(year, month, day)\n                        time_context = TimeContext(text=text, start=start)\n                    else:\n                        time_context = TimeContext(text=text)\n                else:\n                    time_context = TimeContext(text=text)\n            else:\n                time_context = None\n        return time_context, loc_context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a networkx MultiDiGraph representing a causal analysis graph.", "response": "def make_model(self, grounding_ontology='UN', grounding_threshold=None):\n        \"\"\"Return a networkx MultiDiGraph representing a causal analysis graph.\n\n        Parameters\n        ----------\n        grounding_ontology : Optional[str]\n            The ontology from which the grounding should be taken\n            (e.g. UN, FAO)\n        grounding_threshold : Optional[float]\n            Minimum threshold score for Eidos grounding.\n\n        Returns\n        -------\n        nx.MultiDiGraph\n            The assembled CAG.\n        \"\"\"\n        if grounding_threshold is not None:\n            self.grounding_threshold = grounding_threshold\n\n        self.grounding_ontology = grounding_ontology\n\n        # Filter to Influence Statements which are currently supported\n        statements = [stmt for stmt in self.statements if\n                      isinstance(stmt, Influence)]\n\n        # Initialize graph\n        self.CAG = nx.MultiDiGraph()\n\n        # Add nodes and edges to the graph\n        for s in statements:\n            # Get standardized name of subject and object\n            # subj, obj = (self._node_name(s.subj), self._node_name(s.obj))\n\n            # See if both subject and object have polarities given\n            has_both_polarity = (s.subj.delta['polarity'] is not None and\n                                 s.obj.delta['polarity'] is not None)\n\n            # Add the nodes to the graph\n            for node, delta in zip((s.subj.concept, s.obj.concept),\n                                   (s.subj.delta, s.obj.delta)):\n                self.CAG.add_node(self._node_name(node),\n                                  simulable=has_both_polarity,\n                                  mods=delta['adjectives'])\n\n            # Edge is solid if both nodes have polarity given\n            linestyle = 'solid' if has_both_polarity else 'dotted'\n            if has_both_polarity:\n                same_polarity = (s.subj.delta['polarity'] ==\n                                 s.obj.delta['polarity'])\n                if same_polarity:\n                    target_arrow_shape, linecolor = ('circle', 'green')\n                else:\n                    target_arrow_shape, linecolor = ('tee', 'maroon')\n            else:\n                target_arrow_shape, linecolor = ('triangle', 'maroon')\n\n            # Add edge to the graph with metadata from statement\n            provenance = []\n            if s.evidence:\n                provenance = s.evidence[0].annotations.get('provenance', [])\n                if provenance:\n                    provenance[0]['text'] = s.evidence[0].text\n            self.CAG.add_edge(\n                    self._node_name(s.subj.concept),\n                    self._node_name(s.obj.concept),\n                    subj_polarity=s.subj.delta['polarity'],\n                    subj_adjectives=s.subj.delta['adjectives'],\n                    obj_polarity=s.obj.delta['polarity'],\n                    obj_adjectives=s.obj.delta['adjectives'],\n                    linestyle=linestyle,\n                    linecolor=linecolor,\n                    targetArrowShape=target_arrow_shape,\n                    provenance=provenance,\n                )\n\n        return self.CAG"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting the CAG to CytoscapeJS format.", "response": "def export_to_cytoscapejs(self):\n        \"\"\"Return CAG in format readable by CytoscapeJS.\n\n        Return\n        ------\n        dict\n            A JSON-like dict representing the graph for use with\n            CytoscapeJS.\n        \"\"\"\n        def _create_edge_data_dict(e):\n            \"\"\"Return a dict from a MultiDiGraph edge for CytoscapeJS export.\"\"\"\n            # A hack to get rid of the redundant 'Provenance' label.\n            if e[3].get('provenance'):\n                tooltip = e[3]['provenance'][0]\n                if tooltip.get('@type'):\n                    del tooltip['@type']\n            else:\n                tooltip = None\n            edge_data_dict = {\n                    'id'               : e[0]+'_'+e[1],\n                    'source'           : e[0],\n                    'target'           : e[1],\n                    'linestyle'        : e[3][\"linestyle\"],\n                    'linecolor'        : e[3][\"linecolor\"],\n                    'targetArrowShape' : e[3][\"targetArrowShape\"],\n                    'subj_adjectives'  : e[3][\"subj_adjectives\"],\n                    'subj_polarity'    : e[3][\"subj_polarity\"],\n                    'obj_adjectives'   : e[3][\"obj_adjectives\"],\n                    'obj_polarity'     : e[3][\"obj_polarity\"],\n                    'tooltip'          : tooltip,\n                    'simulable'        : False if (\n                        e[3]['obj_polarity'] is None or\n                        e[3]['subj_polarity'] is None) else True,\n                    }\n            return edge_data_dict\n\n        return {\n                'nodes': [{'data': {\n                    'id': n[0],\n                    'simulable': n[1]['simulable'],\n                    'tooltip': 'Modifiers: '+json.dumps(n[1]['mods'])}\n                    } for n in self.CAG.nodes(data=True)],\n\n                'edges': [{'data': _create_edge_data_dict(e)}\n                          for e in self.CAG.edges(data=True, keys=True)]\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a Jupyter notebook cell s Javascript from a template file.", "response": "def generate_jupyter_js(self, cyjs_style=None, cyjs_layout=None):\n        \"\"\"Generate Javascript from a template to run in Jupyter notebooks.\n\n        Parameters\n        ----------\n        cyjs_style : Optional[dict]\n            A dict that sets CytoscapeJS style as specified in\n            https://github.com/cytoscape/cytoscape.js/blob/master/documentation/md/style.md.\n\n        cyjs_layout : Optional[dict]\n            A dict that sets CytoscapeJS\n            `layout parameters <http://js.cytoscape.org/#core/layout>`_.\n\n        Returns\n        -------\n        str\n            A Javascript string to be rendered in a Jupyter notebook cell.\n        \"\"\"\n        # First, export the CAG to CyJS\n        cyjs_elements = self.export_to_cytoscapejs()\n        # Load the Javascript template\n        tempf = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                             'cag_template.js')\n        with open(tempf, 'r') as fh:\n            template = fh.read()\n        # Load the default style and layout\n        stylef = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n                              'cag_style.json')\n        with open(stylef, 'r') as fh:\n            style = json.load(fh)\n        # Apply style and layout only if arg wasn't passed in\n        if cyjs_style is None:\n            cyjs_style = style['style']\n        if cyjs_layout is None:\n            cyjs_layout = style['layout']\n        # Now fill in the template\n        formatted_args = tuple(json.dumps(x, indent=2) for x in\n                               (cyjs_elements, cyjs_style, cyjs_layout))\n        js_str = template % formatted_args\n        return js_str"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a standardized name for a node given a Concept.", "response": "def _node_name(self, concept):\n        \"\"\"Return a standardized name for a node given a Concept.\"\"\"\n        if (# grounding threshold is specified\n            self.grounding_threshold is not None\n            # The particular eidos ontology grounding (un/wdi/fao) is present\n            and concept.db_refs[self.grounding_ontology]\n            # The grounding score is above the grounding threshold\n            and (concept.db_refs[self.grounding_ontology][0][1] >\n                 self.grounding_threshold)):\n                entry = concept.db_refs[self.grounding_ontology][0][0]\n                return entry.split('/')[-1].replace('_', ' ').capitalize()\n        else:\n            return concept.name.capitalize()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the entity namespace from the URI.", "response": "def namespace_from_uri(uri):\n    \"\"\"Return the entity namespace from the URI. Examples:\n    http://www.openbel.org/bel/p_HGNC_RAF1 -> HGNC\n    http://www.openbel.org/bel/p_RGD_Raf1 -> RGD\n    http://www.openbel.org/bel/p_PFH_MEK1/2_Family -> PFH\n    \"\"\"\n    patterns = ['http://www.openbel.org/bel/[pragm]_([A-Za-z]+)_.*',\n                'http://www.openbel.org/bel/[a-z]+_[pr]_([A-Za-z]+)_.*',\n                'http://www.openbel.org/bel/[a-z]+_complex_([A-Za-z]+)_.*',\n                'http://www.openbel.org/bel/complex_([A-Za-z]+)_.*']\n    for pr in patterns:\n        match = re.match(pr, uri)\n        if match is not None:\n            return match.groups()[0]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove prepended URI information from terms.", "response": "def term_from_uri(uri):\n    \"\"\"Removes prepended URI information from terms.\"\"\"\n    if uri is None:\n        return None\n    # This insures that if we get a Literal with an integer value (as we\n    # do for modification positions), it will get converted to a string,\n    # not an integer.\n    if isinstance(uri, rdflib.Literal):\n        uri = str(uri.toPython())\n    # This is to handle URIs like\n    # http://www.openbel.org/bel/namespace//MAPK%20Erk1/3%20Family\n    # or\n    # http://www.openbel.org/bel/namespace/MAPK%20Erk1/3%20Family\n    # In the current implementation, the order of the patterns\n    # matters.\n    patterns = ['http://www.openbel.org/bel/namespace//(.*)',\n                'http://www.openbel.org/vocabulary//(.*)',\n                'http://www.openbel.org/bel//(.*)',\n                'http://www.openbel.org/bel/namespace/(.*)',\n                'http://www.openbel.org/vocabulary/(.*)',\n                'http://www.openbel.org/bel/(.*)']\n    for pr in patterns:\n        match = re.match(pr, uri)\n        if match is not None:\n            term = match.groups()[0]\n            term = unquote(term)\n            return term\n    # If none of the patterns match then the URI is actually a simple term\n    # for instance a site: \"341\" or a substitution: \"sub(V,600,E)\"\n    return uri"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_modifications(self):\n\n        # Get statements where the subject is an activity\n        q_phospho1 = prefixes + \"\"\"\n            SELECT ?enzName ?substrateName ?mod ?pos\n                   ?stmt ?enzyme ?substrate ?rel\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subject .\n                ?stmt belvoc:hasObject ?object .\n                ?subject a belvoc:AbundanceActivity .\n                ?subject belvoc:hasChild ?enzyme .\n                ?enzyme a belvoc:ProteinAbundance .\n                ?enzyme belvoc:hasConcept ?enzName .\n                ?object a belvoc:ModifiedProteinAbundance .\n                ?object belvoc:hasModificationType ?mod .\n                ?object belvoc:hasChild ?substrate .\n                ?substrate belvoc:hasConcept ?substrateName .\n                OPTIONAL { ?object belvoc:hasModificationPosition ?pos . }\n            }\n        \"\"\"\n        # Get statements where the subject is a protein abundance\n        q_phospho2 = prefixes + \"\"\"\n            SELECT ?enzName ?substrateName ?mod ?pos\n                   ?stmt ?enzyme ?substrate ?rel\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?enzyme .\n                ?stmt belvoc:hasObject ?object .\n                ?enzyme a belvoc:ProteinAbundance .\n                ?enzyme belvoc:hasConcept ?enzName .\n                ?object a belvoc:ModifiedProteinAbundance .\n                ?object belvoc:hasModificationType ?mod .\n                ?object belvoc:hasChild ?substrate .\n                ?substrate belvoc:hasConcept ?substrateName .\n                OPTIONAL { ?object belvoc:hasModificationPosition ?pos . }\n            }\n        \"\"\"\n        for q_phospho in (q_phospho1, q_phospho2):\n            # Run the query\n            res_phospho = self.g.query(q_phospho)\n\n            for stmt in res_phospho:\n                # Parse out the elements of the query\n                evidence = self._get_evidence(stmt[4])\n                enz = self._get_agent(stmt[0], stmt[5])\n                #act_type = name_from_uri(stmt[1])\n                sub = self._get_agent(stmt[1], stmt[6])\n                mod = term_from_uri(stmt[2])\n                residue = self._get_residue(mod)\n                mod_pos = term_from_uri(stmt[3])\n                stmt_str = strip_statement(stmt[4])\n                # Get the relationship (increases/decreases, etc.)\n                rel = term_from_uri(stmt[7])\n                if rel == 'DirectlyIncreases' or rel == 'DirectlyDecreases':\n                    is_direct = True\n                else:\n                    is_direct = False\n\n                # Build the INDRA statement\n                # Handle PhosphorylationSerine, etc.\n                if mod.startswith('Phosphorylation'):\n                    modtype = 'phosphorylation'\n                else:\n                    modtype = mod.lower()\n                # Get the class and invert if needed\n                modclass = modtype_to_modclass[modtype]\n                if rel == 'DirectlyDecreases' or rel == 'Decreases':\n                    modclass = modclass_to_inverse[modclass]\n                stmt = modclass(enz, sub, residue, mod_pos, evidence)\n                if is_direct:\n                    self.statements.append(stmt)\n                    self.converted_direct_stmts.append(stmt_str)\n                else:\n                    self.converted_indirect_stmts.append(stmt_str)\n                    self.indirect_stmts.append(stmt)\n        return", "response": "Extract INDRA Modification Statements from BEL."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_activating_mods(self):\n        q_mods = prefixes + \"\"\"\n            SELECT ?speciesName ?actType ?mod ?pos ?rel ?stmt ?species\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subject .\n                ?stmt belvoc:hasObject ?object .\n                ?object belvoc:hasActivityType ?actType .\n                ?object belvoc:hasChild ?species .\n                ?species a belvoc:ProteinAbundance .\n                ?species belvoc:hasConcept ?speciesName .\n                ?subject a belvoc:ModifiedProteinAbundance .\n                ?subject belvoc:hasModificationType ?mod .\n                ?subject belvoc:hasChild ?species .\n                OPTIONAL { ?subject belvoc:hasModificationPosition ?pos . }\n                FILTER (?rel = belvoc:DirectlyIncreases ||\n                        ?rel = belvoc:DirectlyDecreases)\n            }\n        \"\"\"\n\n        # Now make the PySB for the phosphorylation\n        res_mods = self.g.query(q_mods)\n\n        for stmt in res_mods:\n            evidence = self._get_evidence(stmt[5])\n            # Parse out the elements of the query\n            species = self._get_agent(stmt[0], stmt[6])\n            act_type = term_from_uri(stmt[1]).lower()\n            mod = term_from_uri(stmt[2])\n            mod_pos = term_from_uri(stmt[3])\n            mc = self._get_mod_condition(mod, mod_pos)\n            species.mods = [mc]\n            rel = term_from_uri(stmt[4])\n            if rel == 'DirectlyDecreases':\n                is_active = False\n            else:\n                is_active = True\n            stmt_str = strip_statement(stmt[5])\n            # Mark this as a converted statement\n            self.converted_direct_stmts.append(stmt_str)\n            st = ActiveForm(species, act_type, is_active, evidence)\n            self.statements.append(st)", "response": "Extract INDRA ActiveForm Statements with a single mod from BEL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_complexes(self):\n        q_cmplx = prefixes + \"\"\"\n            SELECT ?complexTerm ?childName ?child ?stmt\n            WHERE {\n                {\n                {?stmt belvoc:hasSubject ?complexTerm}\n                UNION\n                {?stmt belvoc:hasObject ?complexTerm .}\n                UNION\n                {?stmt belvoc:hasSubject ?term .\n                ?term belvoc:hasChild ?complexTerm .}\n                UNION\n                {?stmt belvoc:hasObject ?term .\n                ?term belvoc:hasChild ?complexTerm .}\n                }\n                ?complexTerm a belvoc:Term .\n                ?complexTerm a belvoc:ComplexAbundance .\n                ?complexTerm belvoc:hasChild ?child .\n                ?child belvoc:hasConcept ?childName .\n            }\n        \"\"\"\n        # Run the query\n        res_cmplx = self.g.query(q_cmplx)\n\n        # Store the members of each complex in a dict of lists, keyed by the\n        # term for the complex\n        cmplx_dict = collections.defaultdict(list)\n        cmplx_ev = {}\n        for stmt in res_cmplx:\n            stmt_uri = stmt[3]\n            ev = self._get_evidence(stmt_uri)\n            for e in ev:\n                e.epistemics['direct'] = True\n            cmplx_name = term_from_uri(stmt[0])\n            cmplx_id = stmt_uri + '#' + cmplx_name\n            child = self._get_agent(stmt[1], stmt[2])\n            cmplx_dict[cmplx_id].append(child)\n            # This might be written multiple times but with the same\n            # evidence\n            cmplx_ev[cmplx_id] = ev\n        # Now iterate over the stored complex information and create binding\n        # statements\n        for cmplx_id, cmplx_list in cmplx_dict.items():\n            if len(cmplx_list) < 2:\n                msg = 'Complex %s has less than 2 members! Skipping.' % \\\n                       cmplx_name\n                logger.warning(msg)\n            else:\n                self.statements.append(Complex(cmplx_list,\n                                               evidence=cmplx_ev[cmplx_id]))", "response": "Extract INDRA Complex Statements from BEL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_activating_subs(self):\n        q_mods = prefixes + \"\"\"\n            SELECT ?enzyme_name ?sub_label ?act_type ?rel ?stmt ?subject\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subject .\n                ?stmt belvoc:hasObject ?object .\n                ?subject a belvoc:ProteinAbundance .\n                ?subject belvoc:hasConcept ?enzyme_name .\n                ?subject belvoc:hasChild ?sub_expr .\n                ?sub_expr rdfs:label ?sub_label .\n                ?object a belvoc:AbundanceActivity .\n                ?object belvoc:hasActivityType ?act_type .\n                ?object belvoc:hasChild ?enzyme .\n                ?enzyme a belvoc:ProteinAbundance .\n                ?enzyme belvoc:hasConcept ?enzyme_name .\n            }\n        \"\"\"\n\n        # Now make the PySB for the phosphorylation\n        res_mods = self.g.query(q_mods)\n\n        for stmt in res_mods:\n            evidence = self._get_evidence(stmt[4])\n            # Parse out the elements of the query\n            enz = self._get_agent(stmt[0], stmt[5])\n            sub_expr = term_from_uri(stmt[1])\n            act_type = term_from_uri(stmt[2]).lower()\n            # Parse the WT and substituted residues from the node label.\n            # Strangely, the RDF for substituted residue doesn't break the\n            # terms of the BEL expression down into their meaning, as happens\n            # for modified protein abundances. Instead, the substitution\n            # just comes back as a string, e.g., \"sub(V,600,E)\". This code\n            # parses the arguments back out using a regular expression.\n            match = re.match('sub\\(([A-Z]),([0-9]*),([A-Z])\\)', sub_expr)\n            if match:\n                matches = match.groups()\n                wt_residue = matches[0]\n                position = matches[1]\n                sub_residue = matches[2]\n            else:\n                logger.warning(\"Could not parse substitution expression %s\" %\n                               sub_expr)\n                continue\n            mc = MutCondition(position, wt_residue, sub_residue)\n            enz.mutations = [mc]\n            rel = strip_statement(stmt[3])\n            if rel == 'DirectlyDecreases':\n                is_active = False\n            else:\n                is_active = True\n\n            stmt_str = strip_statement(stmt[4])\n            # Mark this as a converted statement\n            self.converted_direct_stmts.append(stmt_str)\n            st = ActiveForm(enz, act_type, is_active, evidence)\n            self.statements.append(st)", "response": "Extract INDRA ActiveForm Statements based on a mutation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_activation(self):\n        q_stmts = prefixes + \"\"\"\n            SELECT ?subjName ?subjActType ?rel ?objName ?objActType\n                   ?stmt ?subj ?obj\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subj .\n                {?subj belvoc:hasActivityType ?subjActType .\n                 ?subj belvoc:hasChild ?subjProt .\n                 ?subjProt belvoc:hasConcept ?subjName .}\n                UNION\n                {?subj a belvoc:Abundance .\n                 ?subj belvoc:hasConcept ?subjName .}\n                ?stmt belvoc:hasObject ?obj .\n                ?obj belvoc:hasActivityType ?objActType .\n                ?obj belvoc:hasChild ?objProt .\n                ?objProt belvoc:hasConcept ?objName .\n                FILTER (?rel = belvoc:DirectlyIncreases ||\n                        ?rel = belvoc:DirectlyDecreases)\n            }\n        \"\"\"\n        res_stmts = self.g.query(q_stmts)\n\n        for stmt in res_stmts:\n            evidence = self._get_evidence(stmt[5])\n            subj = self._get_agent(stmt[0], stmt[6])\n            subj_activity = stmt[1]\n            if subj_activity:\n                subj_activity = term_from_uri(stmt[1]).lower()\n                subj.activity = ActivityCondition(subj_activity, True)\n            rel = term_from_uri(stmt[2])\n            if rel == 'DirectlyDecreases':\n                is_activation = False\n            else:\n                is_activation = True\n            obj = self._get_agent(stmt[3], stmt[7])\n            obj_activity = term_from_uri(stmt[4]).lower()\n            stmt_str = strip_statement(stmt[5])\n            # Mark this as a converted statement\n            self.converted_direct_stmts.append(stmt_str)\n\n            # Distinguish the case when the activator is a GTPase\n            # (since this may involve unique and stereotyped mechanisms)\n            if subj_activity == 'gtpbound':\n                if not is_activation:\n                    logger.warning('GtpActivation only handles positive '\n                                   'activation.')\n                    continue\n                self.statements.append(\n                     GtpActivation(subj, obj, obj_activity, evidence))\n            # If the object is a GTPase, and the subject *increases*\n            # its GtpBound activity, then the subject is a GEF\n            elif obj_activity == 'gtpbound' and rel == 'DirectlyIncreases':\n                self.statements.append(\n                        Gef(subj, obj, evidence))\n            # If the object is a GTPase, and the subject *decreases*\n            # its GtpBound activity, then the subject is a GAP\n            elif obj_activity == 'gtpbound' and rel == 'DirectlyDecreases':\n                self.statements.append(\n                        Gap(subj, obj, evidence))\n            # Otherwise, create a generic Activity->Activity statement\n            else:\n                if rel == 'DirectlyDecreases':\n                    st = Inhibition(subj, obj, obj_activity, evidence)\n                else:\n                    st = Activation(subj, obj, obj_activity, evidence)\n                self.statements.append(st)", "response": "Extract INDRA Inhibition and Activation Statements from BEL."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts Increase and DecreaseAmount INDRA Statements from BEL.", "response": "def get_transcription(self):\n        \"\"\"Extract Increase/DecreaseAmount INDRA Statements from BEL.\n\n        Three distinct SPARQL patterns are used to extract amount\n        regulations from BEL.\n\n        - q_tscript1 searches for a subject which is a Transcription\n          ActivityType of a ProteinAbundance and an object which is\n          an RNAAbundance that is either increased or decreased.\n\n          Examples:\n\n              transcriptionalActivity(proteinAbundance(HGNC:FOXP2))\n              directlyIncreases\n              rnaAbundance(HGNC:SYK)\n\n              transcriptionalActivity(proteinAbundance(HGNC:FOXP2))\n              directlyDecreases\n              rnaAbundance(HGNC:CALCRL)\n\n        - q_tscript2 searches for a subject which is a ProteinAbundance\n          and an object which is an RNAAbundance. Note that this pattern\n          typically exists in an indirect form (i.e. increases/decreases).\n\n          Example:\n\n              proteinAbundance(HGNC:MTF1) directlyIncreases\n              rnaAbundance(HGNC:LCN1)\n\n        - q_tscript3 searches for a subject which is a\n          ModifiedProteinAbundance, with an object which is an RNAAbundance.\n          In the BEL large corpus, this pattern is found for\n          subjects which are protein families or mouse/rat proteins, and\n          the predicate in an indirect increase.\n\n          Example:\n\n              proteinAbundance(PFR:\"Akt Family\",proteinModification(P))\n              increases\n              rnaAbundance(RGD:Cald1)\n        \"\"\"\n        q_tscript1 = prefixes + \"\"\"\n            SELECT ?tfName ?targetName ?stmt ?tf ?target ?rel\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subject .\n                ?stmt belvoc:hasObject ?target .\n                ?subject a belvoc:AbundanceActivity .\n                ?subject belvoc:hasActivityType belvoc:Transcription .\n                ?subject belvoc:hasChild ?tf .\n                ?tf a belvoc:ProteinAbundance .\n                ?tf belvoc:hasConcept ?tfName .\n                ?target a belvoc:RNAAbundance .\n                ?target belvoc:hasConcept ?targetName .\n            }\n        \"\"\"\n        q_tscript2 = prefixes + \"\"\"\n            SELECT ?tfName ?targetName ?stmt ?tf ?target ?rel\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?tf .\n                ?stmt belvoc:hasObject ?target .\n                ?tf a belvoc:ProteinAbundance .\n                ?tf belvoc:hasConcept ?tfName .\n                ?target a belvoc:RNAAbundance .\n                ?target belvoc:hasConcept ?targetName .\n            }\n        \"\"\"\n        q_tscript3 = prefixes + \"\"\"\n            SELECT ?tfName ?targetName ?stmt ?tf ?target ?rel ?mod ?pos\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subject .\n                ?stmt belvoc:hasObject ?target .\n                ?subject a belvoc:ModifiedProteinAbundance .\n                ?subject belvoc:hasModificationType ?mod .\n                ?subject belvoc:hasChild ?tf .\n                ?tf belvoc:hasConcept ?tfName .\n                ?target a belvoc:RNAAbundance .\n                ?target belvoc:hasConcept ?targetName .\n                OPTIONAL { ?subject belvoc:hasModificationPosition ?pos . }\n            }\n        \"\"\"\n        for q_tscript in (q_tscript1, q_tscript2, q_tscript3):\n            res_tscript = self.g.query(q_tscript)\n            for stmt in res_tscript:\n                # Get modifications on the subject, if any\n                if q_tscript == q_tscript1:\n                    tf = self._get_agent(stmt[0], stmt[3])\n                    tf.activity = ActivityCondition('transcription', True)\n                elif q_tscript == q_tscript3:\n                    mod = term_from_uri(stmt[6])\n                    mod_pos = term_from_uri(stmt[7])\n                    mc = self._get_mod_condition(mod, mod_pos)\n                    if mc is None:\n                        continue\n                    tf = self._get_agent(stmt[0], stmt[3])\n                    tf.mods = mods=[mc]\n                else:\n                    tf = self._get_agent(stmt[0], stmt[3])\n                # Parse out the elements of the query\n                evidence = self._get_evidence(stmt[2])\n                target = self._get_agent(stmt[1], stmt[4])\n                stmt_str = strip_statement(stmt[2])\n                # Get the relationship (increases/decreases, etc.)\n                rel = term_from_uri(stmt[5])\n                if rel == 'DirectlyIncreases' or rel == 'DirectlyDecreases':\n                    is_direct = True\n                else:\n                    is_direct = False\n                # Build the INDRA statement\n                stmt = None\n                if rel == 'DirectlyIncreases' or rel == 'Increases':\n                    stmt = IncreaseAmount(tf, target, evidence)\n                elif rel == 'DirectlyDecreases' or rel == 'Decreases':\n                    stmt = DecreaseAmount(tf, target, evidence)\n                # If we've matched a pattern, mark this as a converted statement\n                if stmt is not None:\n                    if is_direct:\n                        self.statements.append(stmt)\n                        self.converted_direct_stmts.append(stmt_str)\n                    else:\n                        self.indirect_stmts.append(stmt)\n                        self.converted_indirect_stmts.append(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_conversions(self):\n        query = prefixes + \"\"\"\n            SELECT DISTINCT ?controller ?controllerName ?controllerActivity\n                ?product ?productName ?reactant ?reactantName ?stmt\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasRelationship ?rel .\n                ?stmt belvoc:hasSubject ?subject .\n                ?stmt belvoc:hasObject ?rxn .\n                ?subject a belvoc:AbundanceActivity .\n                ?subject belvoc:hasActivityType ?controllerActivity .\n                ?subject belvoc:hasChild ?controller .\n                ?controller belvoc:hasConcept ?controllerName .\n                ?rxn a belvoc:Reaction .\n                ?rxn belvoc:hasChild ?reactants .\n                ?reactants rdfs:label ?reactLabel .\n                FILTER (regex(?reactLabel, \"^reactants.*\"))\n                ?rxn belvoc:hasChild ?products .\n                ?products rdfs:label ?prodLabel .\n                FILTER (regex(?prodLabel, \"^products.*\"))\n                ?reactants belvoc:hasChild ?reactant .\n                ?products belvoc:hasChild ?product .\n                ?reactant belvoc:hasConcept ?reactantName .\n                ?product belvoc:hasConcept ?productName .\n            }\n            \"\"\"\n        res = self.g.query(query)\n        # We need to collect all pieces of the same statement so that we can\n        # collect multiple reactants and products\n        stmt_map = collections.defaultdict(list)\n        for stmt in res:\n            stmt_map[stmt[-1]].append(stmt)\n        for stmts in stmt_map.values():\n            # First we get the shared part of the Statement\n            stmt = stmts[0]\n            subj = self._get_agent(stmt[1], stmt[0])\n            evidence = self._get_evidence(stmt[-1])\n            stmt_str = strip_statement(stmt[-1])\n            # Now we collect the participants\n            obj_from_map = {}\n            obj_to_map = {}\n            for stmt in stmts:\n                reactant_name = stmt[6]\n                product_name = stmt[4]\n                if reactant_name not in obj_from_map:\n                    obj_from_map[reactant_name] = \\\n                        self._get_agent(stmt[6], stmt[5])\n                if product_name not in obj_to_map:\n                    obj_to_map[product_name] = \\\n                        self._get_agent(stmt[4], stmt[3])\n            obj_from = list(obj_from_map.values())\n            obj_to = list(obj_to_map.values())\n            st = Conversion(subj, obj_from, obj_to, evidence=evidence)\n            # If we've matched a pattern, mark this as a converted statement\n            self.statements.append(st)\n            self.converted_direct_stmts.append(stmt_str)", "response": "Extract Conversion INDRA Statements from BEL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all direct statements in BEL database.", "response": "def get_all_direct_statements(self):\n        \"\"\"Get all directlyIncreases/Decreases BEL statements.\n\n        This method stores the results of the query in self.all_direct_stmts\n        as a list of strings. The SPARQL query used to find direct BEL\n        statements searches for all statements whose predicate is either\n        DirectyIncreases or DirectlyDecreases.\n        \"\"\"\n        logger.info(\"Getting all direct statements...\\n\")\n        q_stmts = prefixes + \"\"\"\n            SELECT ?stmt\n            WHERE {\n                ?stmt a belvoc:Statement .\n                {\n                  { ?stmt belvoc:hasRelationship belvoc:DirectlyIncreases . }\n                  UNION\n                  { ?stmt belvoc:hasRelationship belvoc:DirectlyDecreases . }\n                }\n            }\n        \"\"\"\n\n        res_stmts = self.g.query(q_stmts)\n        self.all_direct_stmts = [strip_statement(stmt[0]) for stmt in res_stmts]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_indirect_statements(self):\n        q_stmts = prefixes + \"\"\"\n            SELECT ?stmt\n            WHERE {\n                ?stmt a belvoc:Statement .\n                {\n                  { ?stmt belvoc:hasRelationship belvoc:Increases . }\n                  UNION\n                  { ?stmt belvoc:hasRelationship belvoc:Decreases . }\n                }\n            }\n        \"\"\"\n\n        res_stmts = self.g.query(q_stmts)\n        self.all_indirect_stmts = [strip_statement(stmt[0]) for stmt in res_stmts]", "response": "Get all indirect increases and decreases BEL statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all degenerate BEL statements.", "response": "def get_degenerate_statements(self):\n        \"\"\"Get all degenerate BEL statements.\n\n        Stores the results of the query in self.degenerate_stmts.\n        \"\"\"\n        logger.info(\"Checking for 'degenerate' statements...\\n\")\n        # Get rules of type protein X -> activity Y\n        q_stmts = prefixes + \"\"\"\n            SELECT ?stmt\n            WHERE {\n                ?stmt a belvoc:Statement .\n                ?stmt belvoc:hasSubject ?subj .\n                ?stmt belvoc:hasObject ?obj .\n                {\n                  { ?stmt belvoc:hasRelationship belvoc:DirectlyIncreases . }\n                  UNION\n                  { ?stmt belvoc:hasRelationship belvoc:DirectlyDecreases . }\n                }\n                {\n                  { ?subj a belvoc:ProteinAbundance . }\n                  UNION\n                  { ?subj a belvoc:ModifiedProteinAbundance . }\n                }\n                ?subj belvoc:hasConcept ?xName .\n                {\n                  {\n                    ?obj a belvoc:ProteinAbundance .\n                    ?obj belvoc:hasConcept ?yName .\n                  }\n                  UNION\n                  {\n                    ?obj a belvoc:ModifiedProteinAbundance .\n                    ?obj belvoc:hasChild ?proteinY .\n                    ?proteinY belvoc:hasConcept ?yName .\n                  }\n                  UNION\n                  {\n                    ?obj a belvoc:AbundanceActivity .\n                    ?obj belvoc:hasChild ?objChild .\n                    ?objChild a belvoc:ProteinAbundance .\n                    ?objChild belvoc:hasConcept ?yName .\n                  }\n                }\n                FILTER (?xName != ?yName)\n            }\n        \"\"\"\n        res_stmts = self.g.query(q_stmts)\n\n        logger.info(\"Protein -> Protein/Activity statements:\")\n        logger.info(\"---------------------------------------\")\n        for stmt in res_stmts:\n            stmt_str = strip_statement(stmt[0])\n            logger.info(stmt_str)\n            self.degenerate_stmts.append(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay how many of the direct statements have been converted and not converted.", "response": "def print_statement_coverage(self):\n        \"\"\"Display how many of the direct statements have been converted.\n\n        Also prints how many are considered 'degenerate' and not converted.\"\"\"\n\n        if not self.all_direct_stmts:\n            self.get_all_direct_statements()\n        if not self.degenerate_stmts:\n            self.get_degenerate_statements()\n        if not self.all_indirect_stmts:\n            self.get_all_indirect_statements()\n\n        logger.info('')\n        logger.info(\"Total indirect statements: %d\" %\n                     len(self.all_indirect_stmts))\n        logger.info(\"Converted indirect statements: %d\" %\n                     len(self.converted_indirect_stmts))\n        logger.info(\">> Unhandled indirect statements: %d\" %\n                     (len(self.all_indirect_stmts) -\n                      len(self.converted_indirect_stmts)))\n        logger.info('')\n        logger.info(\"Total direct statements: %d\" % len(self.all_direct_stmts))\n        logger.info(\"Converted direct statements: %d\" %\n                    len(self.converted_direct_stmts))\n        logger.info(\"Degenerate direct statements: %d\" %\n                    len(self.degenerate_stmts))\n        logger.info(\">> Unhandled direct statements: %d\" %\n                     (len(self.all_direct_stmts) -\n                      len(self.converted_direct_stmts) -\n                      len(self.degenerate_stmts)))\n\n        logger.info('')\n        logger.info(\"--- Unhandled direct statements ---------\")\n        for stmt in self.all_direct_stmts:\n            if not (stmt in self.converted_direct_stmts or\n                    stmt in self.degenerate_stmts):\n                logger.info(stmt)\n        logger.info('')\n        logger.info(\"--- Unhandled indirect statements ---------\")\n        for stmt in self.all_indirect_stmts:\n            if not (stmt in self.converted_indirect_stmts or\n                    stmt in self.degenerate_stmts):\n                logger.info(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint all extracted INDRA Statements.", "response": "def print_statements(self):\n        \"\"\"Print all extracted INDRA Statements.\"\"\"\n        logger.info('--- Direct INDRA statements ----------')\n        for i, stmt in enumerate(self.statements):\n            logger.info(\"%s: %s\" % (i, stmt))\n        logger.info('--- Indirect INDRA statements ----------')\n        for i, stmt in enumerate(self.indirect_stmts):\n            logger.info(\"%s: %s\" % (i, stmt))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a directory containing CSXML files and returns INDRA statements sorted by pmid.", "response": "def process_directory_statements_sorted_by_pmid(directory_name):\n    \"\"\"Processes a directory filled with CSXML files, first normalizing the\n    character encoding to utf-8, and then processing into INDRA statements\n    sorted by pmid.\n\n    Parameters\n    ----------\n    directory_name : str\n        The name of a directory filled with csxml files to process\n\n    Returns\n    -------\n    pmid_dict : dict\n        A dictionary mapping pmids to a list of statements corresponding to\n        that pmid\n    \"\"\"\n    s_dict = defaultdict(list)\n    mp = process_directory(directory_name, lazy=True)\n\n    for statement in mp.iter_statements():\n        s_dict[statement.evidence[0].pmid].append(statement)\n    return s_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a directory filled with CSXML files and returns a MedscanProcessor populated with INDRA statements extracted from the CSXML files.", "response": "def process_directory(directory_name, lazy=False):\n    \"\"\"Processes a directory filled with CSXML files, first normalizing the\n    character encodings to utf-8, and then processing into a list of INDRA\n    statements.\n\n    Parameters\n    ----------\n    directory_name : str\n        The name of a directory filled with csxml files to process\n    lazy : bool\n        If True, the statements will not be generated immediately, but rather\n        a generator will be formulated, and statements can be retrieved by\n        using `iter_statements`. If False, the `statements` attribute will be\n        populated immediately. Default is False.\n\n    Returns\n    -------\n    mp : indra.sources.medscan.processor.MedscanProcessor\n        A MedscanProcessor populated with INDRA statements extracted from the\n        csxml files\n    \"\"\"\n\n    # Parent Medscan processor containing extractions from all files\n    mp = MedscanProcessor()\n    mp.process_directory(directory_name, lazy)\n    return mp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing a csxml file and returns a dictionary mapping pmids to a list of statements corresponding to that pmid.", "response": "def process_file_sorted_by_pmid(file_name):\n    \"\"\"Processes a file and returns a dictionary mapping pmids to a list of\n    statements corresponding to that pmid.\n\n    Parameters\n    ----------\n    file_name : str\n        A csxml file to process\n\n    Returns\n    -------\n    s_dict : dict\n        Dictionary mapping pmids to a list of statements corresponding to\n        that pmid\n    \"\"\"\n    s_dict = defaultdict(list)\n    mp = process_file(file_name, lazy=True)\n\n    for statement in mp.iter_statements():\n        s_dict[statement.evidence[0].pmid].append(statement)\n    return s_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a CSXML file for its relevant information.", "response": "def process_file(filename, interval=None, lazy=False):\n    \"\"\"Process a CSXML file for its relevant information.\n\n    Consider running the fix_csxml_character_encoding.py script in\n    indra/sources/medscan to fix any encoding issues in the input file before\n    processing.\n\n    Attributes\n    ----------\n    filename : str\n        The csxml file, containing Medscan XML, to process\n    interval : (start, end) or None\n        Select the interval of documents to read, starting with the\n        `start`th document and ending before the `end`th document. If\n        either is None, the value is considered undefined. If the value\n        exceeds the bounds of available documents, it will simply be\n        ignored.\n    lazy : bool\n        If True, the statements will not be generated immediately, but rather\n        a generator will be formulated, and statements can be retrieved by\n        using `iter_statements`. If False, the `statements` attribute will be\n        populated immediately. Default is False.\n\n    Returns\n    -------\n    mp : MedscanProcessor\n        A MedscanProcessor object containing extracted statements\n    \"\"\"\n    mp = MedscanProcessor()\n    mp.process_csxml_file(filename, interval, lazy)\n    return mp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns source Statements corresponding to a path in a PySB model.", "response": "def stmts_from_path(path, model, stmts):\n    \"\"\"Return source Statements corresponding to a path in a model.\n\n    Parameters\n    ----------\n    path : list[tuple[str, int]]\n        A list of tuples where the first element of the tuple is the\n        name of a rule, and the second is the associated polarity along\n        a path.\n    model : pysb.core.Model\n        A PySB model which contains the rules along the path.\n    stmts : list[indra.statements.Statement]\n        A list of INDRA Statements from which the model was assembled.\n\n    Returns\n    -------\n    path_stmts : list[indra.statements.Statement]\n        The Statements from which the rules along the path were obtained.\n    \"\"\"\n    path_stmts = []\n    for path_rule, sign in path:\n        for rule in model.rules:\n            if rule.name == path_rule:\n                stmt = stmt_from_rule(path_rule, model, stmts)\n                assert stmt is not None\n                path_stmts.append(stmt)\n    return path_stmts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_context(annotations, annot_manager):\n    def get_annot(annotations, key):\n        \"\"\"Return a specific annotation given a key.\"\"\"\n        val = annotations.pop(key, None)\n        if val:\n            val_list = [v for v, tf in val.items() if tf]\n            if len(val_list) > 1:\n                logger.warning('More than one \"%s\" in annotations' % key)\n            elif not val_list:\n                return None\n            return val_list[0]\n        return None\n\n    bc = BioContext()\n    species = get_annot(annotations, 'Species')\n    if species:\n        name = annot_manager.get_mapping('Species', species)\n        bc.species = RefContext(name=name, db_refs={'TAXONOMY': species})\n\n    mappings = (('CellLine', 'cell_line', None),\n                ('Disease', 'disease', None),\n                ('Anatomy', 'organ', None),\n                ('Cell', 'cell_type', None),\n                ('CellStructure', 'location', 'MESH'))\n    for bel_name, indra_name, ns in mappings:\n        ann = get_annot(annotations, bel_name)\n        if ann:\n            ref = annot_manager.get_mapping(bel_name, ann)\n            if ref is None:\n                continue\n            if not ns:\n                db_ns, db_id = ref.split('_', 1)\n            else:\n                db_ns, db_id = ns, ref\n            setattr(bc, indra_name,\n                    RefContext(name=ann, db_refs={db_ns: db_id}))\n    # Overwrite blank BioContext\n    if not bc:\n        bc = None\n    return bc", "response": "Extract a single BioContext from the annotations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_axis(ax, label_padding=2, tick_padding=0, yticks_position='left'):\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position(yticks_position)\n    ax.yaxis.set_tick_params(which='both', direction='out', labelsize=fontsize,\n                             pad=tick_padding, length=2, width=0.5)\n    ax.xaxis.set_tick_params(which='both', direction='out', labelsize=fontsize,\n                             pad=tick_padding, length=2, width=0.5)\n    ax.xaxis.labelpad = label_padding\n    ax.yaxis.labelpad = label_padding\n    ax.xaxis.label.set_size(fontsize)\n    ax.yaxis.label.set_size(fontsize)", "response": "Set standardized axis formatting for figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tag_text(text, tag_info_list):\n    # Check to tags for overlap and if there is any, return the subsumed\n    # range. Return None if no overlap.\n    def overlap(t1, t2):\n        if range(max(t1[0], t2[0]), min(t1[1]-1, t2[1]-1)+1):\n            if t1[1] - t1[0] >= t2[1] - t2[0]:\n                return t2\n            else:\n                return t1\n        else:\n            return None\n    # Remove subsumed tags\n    for t1, t2 in list(itertools.combinations(tag_info_list, 2)):\n        subsumed_tag = overlap(t1, t2)\n        if subsumed_tag is not None:\n            # Delete the subsumed tag from the list\n            try:\n                tag_ix = tag_info_list.index(subsumed_tag)\n                del tag_info_list[tag_ix]\n            # Ignore case where tag has already been deleted\n            except ValueError:\n                pass\n    # Sort the indices by their start position\n    tag_info_list.sort(key=lambda x: x[0])\n    # Now, add the marker text for each occurrence of the strings\n    format_text = ''\n    start_pos = 0\n    for i, j, ag_text, tag_start, tag_close in tag_info_list:\n        # Add the text before this agent, if any\n        format_text += text[start_pos:i]\n        # Add wrapper for this entity\n        format_text += tag_start + ag_text + tag_close\n        # Now set the next start position\n        start_pos = j\n    # Add the last section of text\n    format_text += text[start_pos:]\n    return format_text", "response": "Apply start and end tags to the given text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the assembled HTML content as a string.", "response": "def make_model(self):\n        \"\"\"Return the assembled HTML content as a string.\n\n        Returns\n        -------\n        str\n            The assembled HTML as a string.\n        \"\"\"\n        stmts_formatted = []\n        stmt_rows = group_and_sort_statements(self.statements,\n                                              self.ev_totals if self.ev_totals else None)\n        for key, verb, stmts in stmt_rows:\n            # This will now be ordered by prevalence and entity pairs.\n            stmt_info_list = []\n            for stmt in stmts:\n                stmt_hash = stmt.get_hash(shallow=True)\n                ev_list = self._format_evidence_text(stmt)\n                english = self._format_stmt_text(stmt)\n                if self.ev_totals:\n                    total_evidence = self.ev_totals.get(int(stmt_hash), '?')\n                    if total_evidence == '?':\n                        logger.warning('The hash %s was not found in the '\n                                       'evidence totals dict.' % stmt_hash)\n                    evidence_count_str = '%s / %s' % (len(ev_list), total_evidence)\n                else:\n                    evidence_count_str = str(len(ev_list))\n                stmt_info_list.append({\n                    'hash': stmt_hash,\n                    'english': english,\n                    'evidence': ev_list,\n                    'evidence_count': evidence_count_str})\n            short_name = make_string_from_sort_key(key, verb)\n            short_name_key = str(uuid.uuid4())\n            stmts_formatted.append((short_name, short_name_key, stmt_info_list))\n        metadata = {k.replace('_', ' ').title(): v\n                    for k, v in self.metadata.items()}\n        if self.db_rest_url and not self.db_rest_url.endswith('statements'):\n            db_rest_url = self.db_rest_url + '/statements'\n        else:\n            db_rest_url = '.'\n        self.model = template.render(stmt_data=stmts_formatted,\n                                     metadata=metadata, title=self.title,\n                                     db_rest_url=db_rest_url)\n        return self.model"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nappend a warning message to the model to expose issues.", "response": "def append_warning(self, msg):\n        \"\"\"Append a warning message to the model to expose issues.\"\"\"\n        assert self.model is not None, \"You must already have run make_model!\"\n        addendum = ('\\t<span style=\"color:red;\">(CAUTION: %s occurred when '\n                    'creating this page.)</span>' % msg)\n        self.model = self.model.replace(self.title, self.title + addendum)\n        return self.model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_model(self, fname):\n        if self.model is None:\n            self.make_model()\n\n        with open(fname, 'wb') as fh:\n            fh.write(self.model.encode('utf-8'))", "response": "Save the assembled HTML into a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _format_evidence_text(stmt):\n        def get_role(ag_ix):\n            if isinstance(stmt, Complex) or \\\n               isinstance(stmt, SelfModification) or \\\n               isinstance(stmt, ActiveForm) or isinstance(stmt, Conversion) or\\\n               isinstance(stmt, Translocation):\n                return 'other'\n            else:\n                assert len(stmt.agent_list()) == 2, (len(stmt.agent_list()),\n                                                     type(stmt))\n                return 'subject' if ag_ix == 0 else 'object'\n\n        ev_list = []\n        for ix, ev in enumerate(stmt.evidence):\n            # Expand the source api to include the sub-database\n            if ev.source_api == 'biopax' and \\\n               'source_sub_id' in ev.annotations and \\\n               ev.annotations['source_sub_id']:\n               source_api = '%s:%s' % (ev.source_api,\n                                       ev.annotations['source_sub_id'])\n            else:\n                source_api = ev.source_api\n            # Prepare the evidence text\n            if ev.text is None:\n                format_text = None\n            else:\n                indices = []\n                for ix, ag in enumerate(stmt.agent_list()):\n                    if ag is None:\n                        continue\n                    # If the statement has been preassembled, it will have\n                    # this entry in annotations\n                    try:\n                        ag_text = ev.annotations['agents']['raw_text'][ix]\n                        if ag_text is None:\n                            raise KeyError\n                    # Otherwise we try to get the agent text from db_refs\n                    except KeyError:\n                        ag_text = ag.db_refs.get('TEXT')\n                    if ag_text is None:\n                        continue\n                    role = get_role(ix)\n                    # Get the tag with the correct badge\n                    tag_start = '<span class=\"badge badge-%s\">' % role\n                    tag_close = '</span>'\n                    # Build up a set of indices\n                    indices += [(m.start(), m.start() + len(ag_text),\n                                 ag_text, tag_start, tag_close)\n                                 for m in re.finditer(re.escape(ag_text),\n                                                      ev.text)]\n                format_text = tag_text(ev.text, indices)\n\n            ev_list.append({'source_api': source_api,\n                            'pmid': ev.pmid,\n                            'text_refs': ev.text_refs,\n                            'text': format_text,\n                            'source_hash': ev.source_hash })\n\n        return ev_list", "response": "Returns the evidence metadata with highlighted text."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_pmc(pmc_id, offline=False, output_fname=default_output_fname):\n    xml_str = pmc_client.get_xml(pmc_id)\n    if xml_str is None:\n        return None\n    fname = pmc_id + '.nxml'\n    with open(fname, 'wb') as fh:\n        fh.write(xml_str.encode('utf-8'))\n    ids = id_lookup(pmc_id, 'pmcid')\n    pmid = ids.get('pmid')\n    rp = process_nxml_file(fname, citation=pmid, offline=offline,\n                           output_fname=output_fname)\n    return rp", "response": "Return a ReachProcessor by processing a paper with a given PMC id."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a ReachProcessor by processing an abstract with a given Pubmed id.", "response": "def process_pubmed_abstract(pubmed_id, offline=False,\n                            output_fname=default_output_fname, **kwargs):\n    \"\"\"Return a ReachProcessor by processing an abstract with a given Pubmed id.\n\n    Uses the Pubmed client to get the abstract. If that fails, None is\n    returned.\n\n    Parameters\n    ----------\n    pubmed_id : str\n        The ID of a Pubmed article. The string may start with PMID but\n        passing just the ID also works.\n        Examples: 27168024, PMID27168024\n        https://www.ncbi.nlm.nih.gov/pubmed/\n    offline : Optional[bool]\n        If set to True, the REACH system is ran offline. Otherwise (by default)\n        the web service is called. Default: False\n    output_fname : Optional[str]\n        The file to output the REACH JSON output to.\n        Defaults to reach_output.json in current working directory.\n    **kwargs : keyword arguments\n        All other keyword arguments are passed directly to `process_text`.\n\n    Returns\n    -------\n    rp : ReachProcessor\n        A ReachProcessor containing the extracted INDRA Statements\n        in rp.statements.\n    \"\"\"\n    abs_txt = pubmed_client.get_abstract(pubmed_id)\n    if abs_txt is None:\n        return None\n    rp = process_text(abs_txt, citation=pubmed_id, offline=offline,\n                      output_fname=output_fname, **kwargs)\n    if rp and rp.statements:\n        for st in rp.statements:\n            for ev in st.evidence:\n                ev.epistemics['section_type'] = 'abstract'\n    return rp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a text string and return a ReachProcessor by processing it.", "response": "def process_text(text, citation=None, offline=False,\n                 output_fname=default_output_fname, timeout=None):\n    \"\"\"Return a ReachProcessor by processing the given text.\n\n    Parameters\n    ----------\n    text : str\n        The text to be processed.\n    citation : Optional[str]\n        A PubMed ID passed to be used in the evidence for the extracted INDRA\n        Statements. This is used when the text to be processed comes from\n        a publication that is not otherwise identified. Default: None\n    offline : Optional[bool]\n        If set to True, the REACH system is ran offline. Otherwise (by default)\n        the web service is called. Default: False\n    output_fname : Optional[str]\n        The file to output the REACH JSON output to.\n        Defaults to reach_output.json in current working directory.\n    timeout : Optional[float]\n        This only applies when reading online (`offline=False`). Only wait for\n        `timeout` seconds for the api to respond.\n\n    Returns\n    -------\n    rp : ReachProcessor\n        A ReachProcessor containing the extracted INDRA Statements\n        in rp.statements.\n    \"\"\"\n    if offline:\n        if not try_offline:\n            logger.error('Offline reading is not available.')\n            return None\n        try:\n            api_ruler = reach_reader.get_api_ruler()\n        except ReachOfflineReadingError as e:\n            logger.error(e)\n            logger.error('Cannot read offline because the REACH ApiRuler '\n                         'could not be instantiated.')\n            return None\n        try:\n            result_map = api_ruler.annotateText(text, 'fries')\n        except JavaException as e:\n            logger.error('Could not process text.')\n            logger.error(e)\n            return None\n        # REACH version < 1.3.3\n        json_str = result_map.get('resultJson')\n        if not json_str:\n            # REACH version >= 1.3.3\n            json_str = result_map.get('result')\n        if not isinstance(json_str, bytes):\n            json_str = json_str.encode('utf-8')\n    else:\n        data = {'text': text.encode('utf-8')}\n        try:\n            res = requests.post(reach_text_url, data, timeout=timeout)\n        except requests.exceptions.RequestException as e:\n            logger.error('Could not connect to REACH service:')\n            logger.error(e)\n            return None\n        # TODO: we could use res.json() here to get a dict\n        # directly\n        # This is a byte string\n        json_str = res.content\n    if not isinstance(json_str, bytes):\n        raise TypeError('{} is {} instead of {}'.format(json_str, json_str.__class__, bytes))\n\n    with open(output_fname, 'wb') as fh:\n        fh.write(json_str)\n    return process_json_str(json_str.decode('utf-8'), citation)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a ReachProcessor by processing the given NXML string.", "response": "def process_nxml_str(nxml_str, citation=None, offline=False,\n                     output_fname=default_output_fname):\n    \"\"\"Return a ReachProcessor by processing the given NXML string.\n\n    NXML is the format used by PubmedCentral for papers in the open\n    access subset.\n\n    Parameters\n    ----------\n    nxml_str : str\n        The NXML string to be processed.\n    citation : Optional[str]\n        A PubMed ID passed to be used in the evidence for the extracted INDRA\n        Statements. Default: None\n    offline : Optional[bool]\n        If set to True, the REACH system is ran offline. Otherwise (by default)\n        the web service is called. Default: False\n    output_fname : Optional[str]\n        The file to output the REACH JSON output to.\n        Defaults to reach_output.json in current working directory.\n\n    Returns\n    -------\n    rp : ReachProcessor\n        A ReachProcessor containing the extracted INDRA Statements\n        in rp.statements.\n    \"\"\"\n    if offline:\n        if not try_offline:\n            logger.error('Offline reading is not available.')\n            return None\n        try:\n            api_ruler = reach_reader.get_api_ruler()\n        except ReachOfflineReadingError as e:\n            logger.error(e)\n            logger.error('Cannot read offline because the REACH ApiRuler '\n                         'could not be instantiated.')\n            return None\n        try:\n            result_map = api_ruler.annotateNxml(nxml_str, 'fries')\n        except JavaException as e:\n            logger.error('Could not process NXML.')\n            logger.error(e)\n            return None\n        # REACH version < 1.3.3\n        json_str = result_map.get('resultJson')\n        if not json_str:\n            # REACH version >= 1.3.3\n            json_str = result_map.get('result')\n        if json_str is None:\n            logger.warning('No results retrieved')\n            return None\n        if isinstance(json_str, bytes):\n            json_str = json_str.decode('utf-8')\n        return process_json_str(json_str, citation)\n    else:\n        data = {'nxml': nxml_str}\n        try:\n            res = requests.post(reach_nxml_url, data)\n        except requests.exceptions.RequestException as e:\n            logger.error('Could not connect to REACH service:')\n            logger.error(e)\n            return None\n        if res.status_code != 200:\n            logger.error('Could not process NXML via REACH service.'\n                         + 'Status code: %d' % res.status_code)\n            return None\n        json_str = res.text\n\n        with open(output_fname, 'wb') as fh:\n            fh.write(json_str.encode('utf-8'))\n        return process_json_str(json_str, citation)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_nxml_file(file_name, citation=None, offline=False,\n                      output_fname=default_output_fname):\n    \"\"\"Return a ReachProcessor by processing the given NXML file.\n\n    NXML is the format used by PubmedCentral for papers in the open\n    access subset.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the NXML file to be processed.\n    citation : Optional[str]\n        A PubMed ID passed to be used in the evidence for the extracted INDRA\n        Statements. Default: None\n    offline : Optional[bool]\n        If set to True, the REACH system is ran offline. Otherwise (by default)\n        the web service is called. Default: False\n    output_fname : Optional[str]\n        The file to output the REACH JSON output to.\n        Defaults to reach_output.json in current working directory.\n\n    Returns\n    -------\n    rp : ReachProcessor\n        A ReachProcessor containing the extracted INDRA Statements\n        in rp.statements.\n    \"\"\"\n    with open(file_name, 'rb') as f:\n        nxml_str = f.read().decode('utf-8')\n        return process_nxml_str(nxml_str, citation, False, output_fname)", "response": "Return a ReachProcessor by processing the given NXML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a ReachProcessor by processing the given REACH json file.", "response": "def process_json_file(file_name, citation=None):\n    \"\"\"Return a ReachProcessor by processing the given REACH json file.\n\n    The output from the REACH parser is in this json format. This function is\n    useful if the output is saved as a file and needs to be processed.\n    For more information on the format, see: https://github.com/clulab/reach\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the json file to be processed.\n    citation : Optional[str]\n        A PubMed ID passed to be used in the evidence for the extracted INDRA\n        Statements. Default: None\n\n    Returns\n    -------\n    rp : ReachProcessor\n        A ReachProcessor containing the extracted INDRA Statements\n        in rp.statements.\n    \"\"\"\n    try:\n        with open(file_name, 'rb') as fh:\n            json_str = fh.read().decode('utf-8')\n            return process_json_str(json_str, citation)\n    except IOError:\n        logger.error('Could not read file %s.' % file_name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_json_str(json_str, citation=None):\n    if not isinstance(json_str, basestring):\n        raise TypeError('{} is {} instead of {}'.format(json_str,\n                                                        json_str.__class__,\n                                                        basestring))\n\n    json_str = json_str.replace('frame-id', 'frame_id')\n    json_str = json_str.replace('argument-label', 'argument_label')\n    json_str = json_str.replace('object-meta', 'object_meta')\n    json_str = json_str.replace('doc-id', 'doc_id')\n    json_str = json_str.replace('is-hypothesis', 'is_hypothesis')\n    json_str = json_str.replace('is-negated', 'is_negated')\n    json_str = json_str.replace('is-direct', 'is_direct')\n    json_str = json_str.replace('found-by', 'found_by')\n    try:\n        json_dict = json.loads(json_str)\n    except ValueError:\n        logger.error('Could not decode JSON string.')\n        return None\n    rp = ReachProcessor(json_dict, citation)\n    rp.get_modifications()\n    rp.get_complexes()\n    rp.get_activation()\n    rp.get_translocation()\n    rp.get_regulate_amounts()\n    return rp", "response": "Return a ReachProcessor by processing a REACH json string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the parser for this script.", "response": "def make_parser():\n    \"\"\"Generate the parser for this script.\"\"\"\n    parser = ArgumentParser(\n        'wait_for_complete.py',\n        usage='%(prog)s [-h] queue_name [options]',\n        description=('Wait for a set of batch jobs to complete, and monitor '\n                     'them as they run.'),\n        epilog=('Jobs can also be monitored, terminated, and otherwise '\n                'managed on the AWS website. However this tool will also tag '\n                'the instances, and should be run whenever a job is submitted '\n                'to AWS.')\n    )\n    parser.add_argument(\n        dest='queue_name',\n        help=('The name of the queue to watch and wait for completion. If no '\n              'jobs are specified, this will wait until all jobs in the queue '\n              'are completed (either SUCCEEDED or FAILED).')\n    )\n    parser.add_argument(\n        '--watch', '-w',\n        dest='job_list',\n        metavar='JOB_ID',\n        nargs='+',\n        help=('Specify particular jobs using their job ids, as reported by '\n              'the submit command. Many ids may be specified.')\n    )\n    parser.add_argument(\n        '--prefix', '-p',\n        dest='job_name_prefix',\n        help='Specify a prefix for the name of the jobs to watch and wait for.'\n    )\n    parser.add_argument(\n        '--interval', '-i',\n        dest='poll_interval',\n        default=10,\n        type=int,\n        help=('The time interval to wait between job status checks, in '\n              'seconds (default: %(default)d seconds).')\n    )\n    parser.add_argument(\n        '--timeout', '-T',\n        metavar='TIMEOUT',\n        type=int,\n        help=('If the logs are not updated for %(metavar)s seconds, '\n              'print a warning. If `--kill_on_log_timeout` flag is set, then '\n              'the offending jobs will be automatically terminated.')\n    )\n    parser.add_argument(\n        '--kill_on_timeout', '-K',\n        action='store_true',\n        help='If a log times out, terminate the offending job.'\n    )\n    parser.add_argument(\n        '--stash_log_method', '-l',\n        choices=['s3', 'local'],\n        metavar='METHOD',\n        help=('Select a method from: [%(choices)s] to store the job logs. '\n              'If no method is specified, the logs will not be '\n              'loaded off of AWS. If \\'s3\\' is specified, then '\n              '`job_name_prefix` must also be given, as this will indicate '\n              'where on s3 to store the logs.')\n    )\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef id_lookup(paper_id, idtype):\n    if idtype not in ('pmid', 'pmcid', 'doi'):\n        raise ValueError(\"Invalid idtype %s; must be 'pmid', 'pmcid', \"\n                         \"or 'doi'.\" % idtype)\n\n    ids = {'doi': None, 'pmid': None, 'pmcid': None}\n    pmc_id_results = pmc_client.id_lookup(paper_id, idtype)\n    # Start with the results of the PMC lookup and then override with the\n    # provided ID\n    ids['pmid'] = pmc_id_results.get('pmid')\n    ids['pmcid'] = pmc_id_results.get('pmcid')\n    ids['doi'] = pmc_id_results.get('doi')\n    ids[idtype] = paper_id\n    # If we gave a DOI, then our work is done after looking for PMID and PMCID\n    if idtype == 'doi':\n        return ids\n    # If we gave a PMID or PMCID, we need to check to see if we got a DOI.\n    # If we got a DOI back, we're done.\n    elif ids.get('doi'):\n        return ids\n    # If we get here, then we've given PMID or PMCID and don't have a DOI yet.\n    # If we gave a PMCID and have neither a PMID nor a DOI, then we'll run\n    # into problems later on when we try to the reverse lookup using CrossRef.\n    # So we bail here and return what we have (PMCID only) with a warning.\n    if ids.get('pmcid') and ids.get('doi') is None and ids.get('pmid') is None:\n        logger.warning('%s: PMCID without PMID or DOI' % ids.get('pmcid'))\n        return ids\n    # To clarify the state of things at this point:\n    assert ids.get('pmid') is not None\n    assert ids.get('doi') is None\n    # As a last result, we try to get the DOI from CrossRef (which internally\n    # tries to get the DOI from Pubmed in the process of collecting the\n    # necessary metadata for the lookup):\n    ids['doi'] = crossref_client.doi_query(ids['pmid'])\n    # It may still be None, but at this point there's nothing we can do...\n    return ids", "response": "Look up an ID of type PMID PMCID or DOI and return the corresponding ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_full_text(paper_id, idtype, preferred_content_type='text/xml'):\n    if preferred_content_type not in \\\n            ('text/xml', 'text/plain', 'application/pdf'):\n        raise ValueError(\"preferred_content_type must be one of 'text/xml', \"\n                         \"'text/plain', or 'application/pdf'.\")\n    ids = id_lookup(paper_id, idtype)\n    pmcid = ids.get('pmcid')\n    pmid = ids.get('pmid')\n    doi = ids.get('doi')\n    # First try to find paper via PMC\n    if pmcid:\n        nxml = pmc_client.get_xml(pmcid)\n        if nxml:\n            return nxml, 'pmc_oa_xml'\n    # If we got here, it means we didn't find the full text in PMC, so we'll\n    # need either the DOI (for lookup in CrossRef) and/or the PMID (so we\n    # can fall back on the abstract. If by some strange turn we have neither,\n    # give up now.\n    if not doi and not pmid:\n        return (None, None)\n\n    # If it does not have PMC NXML then we attempt to obtain the full-text\n    # through the CrossRef Click-through API\n    if doi:\n        # Get publisher\n        publisher = crossref_client.get_publisher(doi)\n\n        # First check for whether this is Elsevier--if so, use the Elsevier\n        # client directly, because the Clickthrough API key seems unreliable.\n        # Return full XML.\n        if publisher == 'Elsevier BV':\n            logger.info('Elsevier: %s' % pmid)\n            #article = elsevier_client.get_article(doi, output='txt')\n            try:\n                article_xml = elsevier_client.download_article(doi)\n            except Exception as e:\n                logger.error(\"Error downloading Elsevier article: %s\" % e)\n                article_xml = None\n            if article_xml is not None:\n                return (article_xml, 'elsevier_xml')\n\n        # FIXME FIXME FIXME\n        # Because we don't yet have a way to process non-Elsevier content\n        # obtained from CrossRef, which includes both XML of unknown format\n        # and PDFs, we just comment this section out for now\n        \"\"\"\n        # Check if there are any full text links\n        links = crossref_client.get_fulltext_links(doi)\n        if links:\n            headers = {}\n            # Set the Cross Ref Clickthrough API key in the header, if we've\n            # got one\n            cr_api_key = crossref_client.get_api_key()\n            if cr_api_key is not None:\n                headers['CR-Clickthrough-Client-Token'] = cr_api_key\n            # Utility function to get particular links by content-type\n            def lookup_content_type(link_list, content_type):\n                content_list = [l.get('URL') for l in link_list\n                                if l.get('content-type') == content_type]\n                return None if not content_list else content_list[0]\n            # First check for what the user asked for\n            if lookup_content_type(links, preferred_content_type):\n                req = requests.get(lookup_content_type(links,\n                                                       preferred_content_type),\n                                   headers=headers)\n                if req.status_code == 200:\n                    req_content_type = req.headers['Content-Type']\n                    return req.text, req_content_type\n                elif req.status_code == 400:\n                    logger.warning('Full text query returned 400 (Bad Request): '\n                                  'Perhaps missing CrossRef Clickthrough API '\n                                  'key?')\n                    return (None, None)\n            # Check for XML first\n            if lookup_content_type(links, 'text/xml'):\n                req = requests.get(lookup_content_type(links, 'text/xml'),\n                                   headers=headers)\n                if req.status_code == 200:\n                    req_content_type = req.headers['Content-Type']\n                    return req.text, req_content_type\n                elif req.status_code == 400:\n                    logger.warning('Full text query returned 400 (Bad Request):'\n                                  'Perhaps missing CrossRef Clickthrough API '\n                                  'key?')\n                    return (None, None)\n            # Next, plain text\n            elif lookup_content_type(links, 'text/plain'):\n                req = requests.get(lookup_content_type(links, 'text/plain'),\n                                   headers=headers)\n                if req.status_code == 200:\n                    req_content_type = req.headers['Content-Type']\n                    return req.text, req_content_type\n                elif req.status_code == 400:\n                    logger.warning('Full text query returned 400 (Bad Request):'\n                                  'Perhaps missing CrossRef Clickthrough API '\n                                  'key?')\n                    return (None, None)\n            elif lookup_content_type(links, 'application/pdf'):\n                pass\n            # Wiley's links are often of content-type 'unspecified'.\n            elif lookup_content_type(links, 'unspecified'):\n                req = requests.get(lookup_content_type(links, 'unspecified'),\n                                   headers=headers)\n                if req.status_code == 200:\n                    req_content_type = req.headers['Content-Type']\n                    return 'foo', req_content_type\n                elif req.status_code == 400:\n                    logger.warning('Full text query returned 400 (Bad Request):'\n                                  'Perhaps missing CrossRef Clickthrough API '\n                                  'key?')\n                    return (None, None)\n                elif req.status_code == 401:\n                    logger.warning('Full text query returned 401 (Unauthorized)')\n                    return (None, None)\n                elif req.status_code == 403:\n                    logger.warning('Full text query returned 403 (Forbidden)')\n                    return (None, None)\n            else:\n                raise Exception(\"Unknown content type(s): %s\" % links)\n        elif publisher == 'American Society for Biochemistry & Molecular ' \\\n                          'Biology (ASBMB)':\n            url = crossref_client.get_url(doi)\n            return get_asbmb_full_text(url)\n        \"\"\"\n        # end FIXME FIXME FIXME\n\n        # No full text links and not a publisher we support. We'll have to\n        # fall back to the abstract.\n        #elif pmid:\n        if pmid:\n            abstract = pubmed_client.get_abstract(pmid)\n            if abstract is None:\n                return (None, None)\n            else:\n                return abstract, 'abstract'\n        # We have a useless DOI and no PMID. Give up.\n        else:\n            return (None, None)\n    # We don't have a DOI but we're guaranteed to have a PMID at this point,\n    # so we fall back to the abstract:\n    else:\n        abstract = pubmed_client.get_abstract(pmid)\n        if abstract is None:\n            return (None, None)\n        else:\n            return abstract, 'abstract'\n    # We'll only get here if we've missed a combination of conditions\n    assert False", "response": "Return the content and content type of an article."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the existing reader if it exists or launch a new one.", "response": "def get_api_ruler(self):\n        \"\"\"Return the existing reader if it exists or launch a new one.\n\n        Returns\n        -------\n        api_ruler : org.clulab.reach.apis.ApiRuler\n            An instance of the REACH ApiRuler class (java object).\n        \"\"\"\n        if self.api_ruler is None:\n            try:\n                self.api_ruler = \\\n                    autoclass('org.clulab.reach.export.apis.ApiRuler')\n            except JavaException as e:\n                raise ReachOfflineReadingError(e)\n        return self.api_ruler"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _download_biogrid_data(url):\n    res = requests.get(biogrid_file_url)\n    if res.status_code != 200:\n        raise Exception('Unable to download Biogrid data: status code %s'\n                        % res.status_code)\n    zip_bytes = BytesIO(res.content)\n    zip_file = ZipFile(zip_bytes)\n    zip_info_list = zip_file.infolist()\n    # There should be only one file in this zip archive\n    if len(zip_info_list) != 1:\n        raise Exception('There should be exactly zipfile in BioGrid zip '\n                        'archive: %s' % str(zip_info_list))\n    unzipped_bytes = zip_file.read(zip_info_list[0]) # Unzip the file\n    biogrid_str = StringIO(unzipped_bytes.decode('utf8')) # Make file-like obj\n    csv_reader = csv.reader(biogrid_str, delimiter='\\t') # Get csv reader\n    next(csv_reader) # Skip the header\n    return csv_reader", "response": "Downloads zipped Biogrid data in tab - separated format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake an Agent object appropriately grounded.", "response": "def _make_agent(self, entrez_id, text_id):\n        \"\"\"Make an Agent object, appropriately grounded.\n\n        Parameters\n        ----------\n        entrez_id : str\n            Entrez id number\n        text_id : str\n            A plain text systematic name, or None if not listed.\n\n        Returns\n        -------\n        agent : indra.statements.Agent\n            A grounded agent object.\n        \"\"\"\n        hgnc_name, db_refs = self._make_db_refs(entrez_id, text_id)\n        if hgnc_name is not None:\n            name = hgnc_name\n        elif text_id is not None:\n            name = text_id\n        # Handle case where the name is None\n        else:\n            return None\n\n        return Agent(name, db_refs=db_refs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_db_refs(self, entrez_id, text_id):\n        db_refs = {}\n        if text_id != '-' and text_id is not None:\n            db_refs['TEXT'] = text_id\n\n        hgnc_id = hgnc_client.get_hgnc_from_entrez(entrez_id)\n        hgnc_name = hgnc_client.get_hgnc_name(hgnc_id)\n        if hgnc_id is not None:\n            db_refs['HGNC'] = hgnc_id\n            up_id = hgnc_client.get_uniprot_id(hgnc_id)\n            if up_id is not None:\n                db_refs['UP'] = up_id\n        return (hgnc_name, db_refs)", "response": "Creates the db_refs dictionary for the AgentCOOKIE object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_model(self, policies=None, initial_conditions=True,\n                   reverse_effects=False):\n        \"\"\"Assemble the Kami model from the collected INDRA Statements.\n\n        This method assembles a Kami model from the set of INDRA Statements.\n        The assembled model is both returned and set as the assembler's\n        model argument.\n\n        Parameters\n        ----------\n        policies : Optional[Union[str, dict]]\n            A string or dictionary of policies, as defined in\n            :py:class:`indra.assemblers.KamiAssembler`. This set of policies\n            locally supersedes the default setting in the assembler. This\n            is useful when this function is called multiple times with\n            different policies.\n        initial_conditions : Optional[bool]\n            If True, default initial conditions are generated for the\n            agents in the model.\n\n        Returns\n        -------\n        model : dict\n            The assembled Kami model.\n        \"\"\"\n        self.processed_policies = self.process_policies(policies)\n        ppa = PysbPreassembler(self.statements)\n        ppa.replace_activities()\n        if reverse_effects:\n            ppa.add_reverse_effects()\n        self.statements = ppa.statements\n        # Set local policies for this make_model call that overwrite\n        # the global policies of the Kami assembler\n        if policies is not None:\n            global_policies = self.policies\n            if isinstance(policies, basestring):\n                local_policies = {'other': policies}\n            else:\n                local_policies = {'other': 'default'}\n                local_policies.update(policies)\n            self.policies = local_policies\n\n        self.model = {}\n        graphs = []\n        self.model['graphs'] = graphs\n        self.model['typing'] = []\n\n        # Action graph generated here\n        action_graph = {'id': 'action_graph',\n                        'attrs': {'name': 'action_graph'}}\n        action_graph['graph'] = {'nodes': [], 'edges': []}\n        graphs.append(action_graph)\n\n        # Iterate over the statements to generate rules\n        self._assemble()\n        # Add initial conditions\n        #if initial_conditions:\n        #    self.add_default_initial_conditions()\n\n        # If local policies were applied, revert to the global one\n        if policies is not None:\n            self.policies = global_policies\n\n        return self.model", "response": "Assemble the Kami model from the set of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls the appropriate assemble method based on policies.", "response": "def _assemble(self):\n        \"\"\"Calls the appropriate assemble method based on policies.\"\"\"\n        for stmt in self.statements:\n            if _is_whitelisted(stmt):\n                self._dispatch(stmt, 'assemble', self.model)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dispatch(self, stmt, stage, *args):\n        class_name = stmt.__class__.__name__\n        policy = self.processed_policies[stmt.uuid]\n        func_name = '%s_%s_%s' % (class_name.lower(), stage, policy)\n        func = globals().get(func_name)\n        if func is None:\n            # The specific policy is not implemented for the\n            # given statement type.\n            # We try to apply a default policy next.\n            func_name = '%s_%s_default' % (class_name.lower(), stage)\n            func = globals().get(func_name)\n            if func is None:\n                # The given statement type doesn't have a default\n                # policy.\n                #raise UnknownPolicyError('%s function %s not defined' %\n                #                         (stage, func_name))\n                logger.warning('%s function %s not defined' %\n                               (stage, func_name))\n                return\n        return func(stmt, *args)", "response": "Construct and call an assembly function based on the type of statement and the corresponding policy and the stage of assembly."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_agent(self, agent):\n        agent_id = self.add_node(agent.name)\n        self.add_typing(agent_id, 'agent')\n        # Handle bound conditions\n        for bc in agent.bound_conditions:\n            # Here we make the assumption that the binding site\n            # is simply named after the binding partner\n            if bc.is_bound:\n                test_type = 'is_bnd'\n            else:\n                test_type = 'is_free'\n            bound_name = bc.agent.name\n            agent_bs = get_binding_site_name(bc.agent)\n            test_name = '%s_bound_to_%s_test' % (agent_id, bound_name)\n            agent_bs_id = self.add_node(agent_bs)\n            test_id = self.add_node(test_name)\n            self.add_edge(agent_bs_id, agent_id)\n            self.add_edge(agent_bs_id, test_id)\n            self.add_typing(agent_bs_id, 'locus')\n            self.add_typing(test_id, test_type)\n\n        for mod in agent.mods:\n            mod_site_str = abbrevs[mod.mod_type]\n            if mod.residue is not None:\n                mod_site_str = mod.residue\n            mod_pos_str = mod.position if mod.position is not None else ''\n            mod_site = ('%s%s' % (mod_site_str, mod_pos_str))\n            site_states = states[mod.mod_type]\n            if mod.is_modified:\n                val = site_states[1]\n            else:\n                val = site_states[0]\n            mod_site_id = self.add_node(mod_site, {'val': val})\n            self.add_edge(mod_site_id, agent_id)\n            self.add_typing(mod_site_id, 'state')\n        return agent_id", "response": "Adds an INDRA Agent and its conditions to the Nugget."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a node with a given base name to the Nugget and return ID.", "response": "def add_node(self, name_base, attrs=None):\n        \"\"\"Add a node with a given base name to the Nugget and return ID.\"\"\"\n        if name_base not in self.counters:\n            node_id = name_base\n        else:\n            node_id = '%s_%d' % (name_base, self.counters[name_base])\n        node = {'id': node_id}\n        if attrs:\n            node['attrs'] = attrs\n        self.nodes.append(node)\n        self.counters[node_id] += 1\n        return node_id"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nugget_dict(self):\n        nugget_dict = \\\n            {'id': self.id,\n             'graph': {\n                 'nodes': self.nodes,\n                 'edges': self.edges\n                 },\n             'attrs': {\n                 'name': self.name,\n                 'rate': self.rate\n                 }\n            }\n        return nugget_dict", "response": "Return the Nugget as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_text(text, pmid=None, python2_path=None):\n \n    # Try to locate python2 in one of the directories of the PATH environment\n    # variable if it is not provided\n    if python2_path is None:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            proposed_python2_path = os.path.join(path, 'python2.7')\n            if os.path.isfile(proposed_python2_path):\n                python2_path = proposed_python2_path\n                print('Found python 2 interpreter at', python2_path)\n                break\n    if python2_path is None:\n        raise Exception('Could not find python2 in the directories ' +\n                        'listed in the PATH environment variable. ' +\n                        'Need python2 to run TEES.')\n\n    # Run TEES\n    a1_text, a2_text, sentence_segmentations = run_on_text(text,\n                                                                python2_path)\n\n    # Run the TEES processor\n    tp = TEESProcessor(a1_text, a2_text, sentence_segmentations, pmid)\n    return tp", "response": "Processes the specified text with TEES and converts it to a TEES object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning TEES on the given text and returns a temporary directory and INDRA statements.", "response": "def run_on_text(text, python2_path):\n    \"\"\"Runs TEES on the given text in a temporary directory and returns a\n    temporary directory with TEES output.\n    \n    The caller should delete this directory when done with it. This function\n    runs TEES and produces TEES output files but does not process TEES output\n    into INDRA statements.\n\n    Parameters\n    ----------\n    text : str\n        Text from which to extract relationships\n    python2_path : str\n        The path to the python 2 interpreter\n\n    Returns\n    -------\n    output_dir : str\n        Temporary directory with TEES output. The caller should delete this\n        directgory when done with it.\n    \"\"\"\n    tees_path = get_config('TEES_PATH')\n\n    if tees_path is None:\n        # If TEES directory is not specifies, see if any of the candidate paths\n        # exist and contain all of the files expected for a TEES installation.\n        for cpath in tees_candidate_paths:\n            cpath = os.path.expanduser(cpath)\n            if os.path.isdir(cpath):\n                # Check to see if it has all of the expected files and\n                # directories\n                has_expected_files = True\n                for f in tees_installation_files:\n                    fpath = os.path.join(cpath, f)\n                    present = os.path.isfile(fpath)\n                    has_expected_files = has_expected_files and present\n\n                has_expected_dirs = True\n                for d in tees_installation_dirs:\n                    dpath = os.path.join(cpath, d)\n                    present = os.path.isdir(dpath)\n                    has_expected_dirs = has_expected_dirs and present\n\n                if has_expected_files and has_expected_dirs:\n                    # We found a directory with all of the files and\n                    # directories  we expected in a TEES installation - let's\n                    # assume it's a TEES installation\n                    tees_path = cpath\n                    print('Found TEES installation at ' + cpath)\n                    break\n\n    # Make sure the provided TEES directory exists\n    if not os.path.isdir(tees_path):\n        raise Exception('Provided TEES directory does not exist.')\n\n    # Make sure the classify.py script exists within this directory\n    classify_path = 'classify.py'\n    # if not os.path.isfile(classify_path):\n    #    raise Exception('classify.py does not exist in provided TEES path.')\n\n    # Create a temporary directory to tag the shared-task files\n    tmp_dir = tempfile.mkdtemp(suffix='indra_tees_processor')\n\n    pwd = os.path.abspath(os.getcwd())\n\n    try:\n        # Write text to a file in the temporary directory\n        text_path = os.path.join(tmp_dir, 'text.txt')\n        # Had some trouble with non-ascii characters. A possible TODO item in\n        # the future is to look into resolving this, for now just ignoring\n        # non-latin-1 characters\n        with codecs.open(text_path, 'w', encoding='latin-1', errors='ignore') \\\n                as f:\n            f.write(text)\n\n        # Run TEES\n        output_path = os.path.join(tmp_dir, 'output')\n        model_path = os.path.join(tees_path, 'tees_data/models/GE11-test/')\n        command = [python2_path, classify_path, '-m', model_path,\n                   '-i', text_path,\n                   '-o', output_path]\n        try:\n            pwd = os.path.abspath(os.getcwd())\n            os.chdir(tees_path)  # Change to TEES directory\n            # print('cwd is:', os.getcwd())\n            # out = subprocess.check_output(command, stderr=subprocess.STDOUT)\n            p = subprocess.Popen(command, stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE, cwd=tees_path)\n            p.wait()\n            (so, se) = p.communicate()\n            print(so)\n            print(se)\n            os.chdir(pwd)  # Change back to previous directory\n            # print('cwd is:', os.getcwd())\n            # print(out.decode('utf-8'))\n\n        except BaseException as e:\n            # If there's an error, print it out and then propagate the\n            # exception\n            os.chdir(pwd)  # Change back to previous directory\n            # print (e.output.decode('utf-8'))\n            raise e\n\n    except BaseException as e:\n        # If there was an exception, delete the temporary directory and\n        # pass on the exception\n        shutil.rmtree(tmp_dir)\n        raise e\n    # Return the temporary directory with the TEES output\n    output_tuple = extract_output(tmp_dir)\n    shutil.rmtree(tmp_dir)\n    return output_tuple"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_output(output_dir):\n\n    # Locate the file of sentences segmented by the TEES system, described\n    # in a compressed xml document\n    sentences_glob = os.path.join(output_dir, '*-preprocessed.xml.gz')\n    sentences_filename_candidates = glob.glob(sentences_glob)\n\n    # Make sure there is exactly one such file\n    if len(sentences_filename_candidates) != 1:\n        m = 'Looking for exactly one file matching %s but found %d matches'\n        raise Exception(m % (\n            sentences_glob, len(sentences_filename_candidates)))\n        return None, None, None\n\n    # Read in the sentence segmentation XML\n    sentence_segmentation_filename = sentences_filename_candidates[0]\n    with gzip.GzipFile(sentences_filename_candidates[0], 'r') as f:\n        sentence_segmentations = f.read().decode('utf-8')\n\n    # Create a temporary directory to which to extract the a1 and a2 files from\n    # the tarball\n    tmp_dir = tempfile.mkdtemp(suffix='indra_tees_processor')\n\n    try:\n        # Make sure the tarfile with the extracted events is in shared task\n        # format is in the output directory\n        tarfile_glob = os.path.join(output_dir, '*-events.tar.gz')\n        candidate_tarfiles = glob.glob(tarfile_glob)\n        if len(candidate_tarfiles) != 1:\n            raise Exception('Expected exactly one match for glob %s' %\n                            tarfile_glob)\n            return None, None, None\n\n        # Decide what tar files to extract\n        # (We're not blindly extracting all files because of the security\n        # warning in the documentation for TarFile.extractall\n        # In particular, we want to make sure that the filename doesn't\n        # try to specify a relative or absolute path other than the current\n        # directory by making sure the filename starts with an alphanumeric\n        # character.\n        # We're also only interested in files with the .a1 or .a2 extension\n        tar_file = tarfile.open(candidate_tarfiles[0])\n        a1_file = None\n        a2_file = None\n        extract_these = []\n        for m in tar_file.getmembers():\n            if re.match('[a-zA-Z0-9].*.a[12]', m.name):\n                extract_these.append(m)\n\n                if m.name.endswith('.a1'):\n                    a1_file = m.name\n                elif m.name.endswith('.a2'):\n                    a2_file = m.name\n                else:\n                    assert(False)\n\n        # There should be exactly two files that match these criteria\n        if len(extract_these) != 2 or a1_file is None or a2_file is None:\n            raise Exception('We thought there would be one .a1 and one .a2' +\n                            ' file in the tarball, but we got %d files total' %\n                            len(extract_these))\n            return None, None, None\n\n        # Extract the files that we decided to extract\n        tar_file.extractall(path=tmp_dir, members=extract_these)\n\n        # Read the text of the a1 (entities) file\n        with codecs.open(os.path.join(tmp_dir, a1_file), 'r',\n                         encoding='utf-8') as f:\n            a1_text = f.read()\n\n        # Read the text of the a2 (events) file\n        with codecs.open(os.path.join(tmp_dir, a2_file), 'r',\n                         encoding='utf-8') as f:\n            a2_text = f.read()\n\n        # Now that we're done, remove the temporary directory\n        shutil.rmtree(tmp_dir)\n\n        # Return the extracted text\n        return a1_text, a2_text, sentence_segmentations\n    except BaseException as e:\n        # If there was an exception, delete the temporary directory and\n        # pass on the exception\n        print('Not removing temporary directory: ' + tmp_dir)\n        shutil.rmtree(tmp_dir)\n        raise e\n        return None, None, None", "response": "Extract the text of the a1 a2 and sentence segmentation files from TEES output directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _list_to_seq(lst):\n    ml = autoclass('scala.collection.mutable.MutableList')()\n    for element in lst:\n        ml.appendElem(element)\n    return ml", "response": "Return a scala. collection. Seq from a Python list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a JSON object given text.", "response": "def process_text(self, text, format='json'):\n        \"\"\"Return a mentions JSON object given text.\n\n        Parameters\n        ----------\n        text : str\n            Text to be processed.\n        format : str\n            The format of the output to produce, one of \"json\" or \"json_ld\".\n            Default: \"json\"\n\n        Returns\n        -------\n        json_dict : dict\n            A JSON object of mentions extracted from text.\n        \"\"\"\n        if self.eidos_reader is None:\n            self.initialize_reader()\n        default_arg = lambda x: autoclass('scala.Some')(x)\n        today = datetime.date.today().strftime(\"%Y-%m-%d\")\n        fname = 'default_file_name'\n\n        annot_doc = self.eidos_reader.extractFromText(\n            text,\n            True, # keep text\n            False, # CAG-relevant only\n            default_arg(today), # doc creation time\n            default_arg(fname) # file name\n            )\n        if format == 'json':\n            mentions = annot_doc.odinMentions()\n            ser = autoclass(eidos_package +\n                            '.serialization.json.WMJSONSerializer')\n            mentions_json = ser.toJsonStr(mentions)\n        elif format == 'json_ld':\n            # We need to get a Scala Seq of annot docs here\n            ml = _list_to_seq([annot_doc])\n            # We currently do not need toinstantiate the adjective grounder\n            # if we want to reinstate it, we would need to do the following\n            # ag = EidosAdjectiveGrounder.fromConfig(\n            #   EidosSystem.defaultConfig.getConfig(\"adjectiveGrounder\"))\n            # We now create a JSON-LD corpus\n            jc = autoclass(eidos_package + '.serialization.json.JLDCorpus')\n            corpus = jc(ml)\n            # Finally, serialize the corpus into JSON string\n            mentions_json = corpus.toJsonStr()\n        json_dict = json.loads(mentions_json)\n        return json_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an EidosProcessor by processing the given text.", "response": "def process_text(text, out_format='json_ld', save_json='eidos_output.json',\n                 webservice=None):\n    \"\"\"Return an EidosProcessor by processing the given text.\n\n    This constructs a reader object via Java and extracts mentions\n    from the text. It then serializes the mentions into JSON and\n    processes the result with process_json.\n\n    Parameters\n    ----------\n    text : str\n        The text to be processed.\n    out_format : Optional[str]\n        The type of Eidos output to read into and process. Currently only\n        'json-ld' is supported which is also the default value used.\n    save_json : Optional[str]\n        The name of a file in which to dump the JSON output of Eidos.\n    webservice : Optional[str]\n        An Eidos reader web service URL to send the request to.\n        If None, the reading is assumed to be done with the Eidos JAR rather\n        than via a web service. Default: None\n\n    Returns\n    -------\n    ep : EidosProcessor\n        An EidosProcessor containing the extracted INDRA Statements in its\n        statements attribute.\n    \"\"\"\n    if not webservice:\n        if eidos_reader is None:\n            logger.error('Eidos reader is not available.')\n            return None\n        json_dict = eidos_reader.process_text(text, out_format)\n    else:\n        res = requests.post('%s/process_text' % webservice,\n                            json={'text': text})\n        json_dict = res.json()\n    if save_json:\n        with open(save_json, 'wt') as fh:\n            json.dump(json_dict, fh, indent=2)\n    return process_json(json_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_json_file(file_name):\n    try:\n        with open(file_name, 'rb') as fh:\n            json_str = fh.read().decode('utf-8')\n            return process_json_str(json_str)\n    except IOError:\n        logger.exception('Could not read file %s.' % file_name)", "response": "Return an EidosProcessor by processing the given Eidos JSON - LD file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_json(json_dict):\n    ep = EidosProcessor(json_dict)\n    ep.extract_causal_relations()\n    ep.extract_correlations()\n    ep.extract_events()\n    return ep", "response": "Return an EidosProcessor by processing a JSON - LD dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_drug_inhibition_stmts(drug):\n    chebi_id = drug.db_refs.get('CHEBI')\n    mesh_id = drug.db_refs.get('MESH')\n    if chebi_id:\n        drug_chembl_id = chebi_client.get_chembl_id(chebi_id)\n    elif mesh_id:\n        drug_chembl_id = get_chembl_id(mesh_id)\n    else:\n        logger.error('Drug missing ChEBI or MESH grounding.')\n        return None\n    logger.info('Drug: %s' % (drug_chembl_id))\n    query_dict = {'query': 'activity',\n                  'params': {'molecule_chembl_id': drug_chembl_id,\n                             'limit': 10000}\n                  }\n    res = send_query(query_dict)\n    activities = res['activities']\n    targ_act_dict = activities_by_target(activities)\n    target_chembl_ids = [x for x in targ_act_dict]\n    protein_targets = get_protein_targets_only(target_chembl_ids)\n    filtered_targ_act_dict = {t: targ_act_dict[t]\n                              for t in [x for x in protein_targets]}\n    stmts = []\n    for target_chembl_id in filtered_targ_act_dict:\n        target_activity_ids = filtered_targ_act_dict[target_chembl_id]\n        target_activites = [x for x in activities\n                            if x['activity_id'] in target_activity_ids]\n        target_upids = []\n        targ_comp = protein_targets[target_chembl_id]['target_components']\n        for t_c in targ_comp:\n            target_upids.append(t_c['accession'])\n        evidence = []\n        for assay in target_activites:\n            ev = get_evidence(assay)\n            if not ev:\n                continue\n            evidence.append(ev)\n        if len(evidence) > 0:\n            for target_upid in target_upids:\n                agent_name = uniprot_client.get_gene_name(target_upid)\n                target_agent = Agent(agent_name, db_refs={'UP': target_upid})\n                st = Inhibition(drug, target_agent, evidence=evidence)\n                stmts.append(st)\n    return stmts", "response": "Query ChEMBL for kinetics data given a drug as Agent get back statements of INDRA statements"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a query to ChEMBL API", "response": "def send_query(query_dict):\n    \"\"\"Query ChEMBL API\n\n    Parameters\n    ----------\n    query_dict : dict\n        'query' : string of the endpoint to query\n        'params' : dict of params for the query\n\n    Returns\n    -------\n    js : dict\n        dict parsed from json that is unique to the submitted query\n    \"\"\"\n    query = query_dict['query']\n    params = query_dict['params']\n    url = 'https://www.ebi.ac.uk/chembl/api/data/' + query + '.json'\n    r = requests.get(url, params=params)\n    r.raise_for_status()\n    js = r.json()\n    return js"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nqueries ChEMBL API target by id", "response": "def query_target(target_chembl_id):\n    \"\"\"Query ChEMBL API target by id\n\n    Parameters\n    ----------\n    target_chembl_id : str\n\n    Returns\n    -------\n    target : dict\n        dict parsed from json that is unique for the target\n    \"\"\"\n    query_dict = {'query': 'target',\n                  'params': {'target_chembl_id': target_chembl_id,\n                             'limit': 1}}\n    res = send_query(query_dict)\n    target = res['targets'][0]\n    return target"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef activities_by_target(activities):\n    targ_act_dict = defaultdict(lambda: [])\n    for activity in activities:\n        target_chembl_id = activity['target_chembl_id']\n        activity_id = activity['activity_id']\n        targ_act_dict[target_chembl_id].append(activity_id)\n    for target_chembl_id in targ_act_dict:\n        targ_act_dict[target_chembl_id] = \\\n            list(set(targ_act_dict[target_chembl_id]))\n    return targ_act_dict", "response": "Get back lists of activities in a dict keyed by ChEMBL target id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving list of ChEMBL target ids return dict of SINGLE PROTEIN targets", "response": "def get_protein_targets_only(target_chembl_ids):\n    \"\"\"Given list of ChEMBL target ids, return dict of SINGLE PROTEIN targets\n\n    Parameters\n    ----------\n    target_chembl_ids : list\n        list of chembl_ids as strings\n\n    Returns\n    -------\n    protein_targets : dict\n        dictionary keyed to ChEMBL target ids with lists of activity ids\n    \"\"\"\n    protein_targets = {}\n    for target_chembl_id in target_chembl_ids:\n        target = query_target(target_chembl_id)\n        if 'SINGLE PROTEIN' in target['target_type']:\n            protein_targets[target_chembl_id] = target\n    return protein_targets"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_evidence(assay):\n    kin = get_kinetics(assay)\n    source_id = assay.get('assay_chembl_id')\n    if not kin:\n        return None\n    annotations = {'kinetics': kin}\n    chembl_doc_id = str(assay.get('document_chembl_id'))\n    pmid = get_pmid(chembl_doc_id)\n    ev = Evidence(source_api='chembl', pmid=pmid, source_id=source_id,\n                  annotations=annotations)\n    return ev", "response": "Given an activity return an INDRA Evidence object containing the kinetics of the activity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_kinetics(assay):\n    try:\n        val = float(assay.get('standard_value'))\n    except TypeError:\n        logger.warning('Invalid assay value: %s' % assay.get('standard_value'))\n        return None\n    unit = assay.get('standard_units')\n    if unit == 'nM':\n        unit_sym = 1e-9 * units.mol / units.liter\n    elif unit == 'uM':\n        unit_sym = 1e-6 * units.mol / units.liter\n    else:\n        logger.warning('Unhandled unit: %s' % unit)\n        return None\n    param_type = assay.get('standard_type')\n    if param_type not in ['IC50', 'EC50', 'INH', 'Potency', 'Kd']:\n        logger.warning('Unhandled parameter type: %s' % param_type)\n        logger.info(str(assay))\n        return None\n    kin = {param_type: val * unit_sym}\n    return kin", "response": "Given an activity return its kinetics values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget PMID from document_chembl_id", "response": "def get_pmid(doc_id):\n    \"\"\"Get PMID from document_chembl_id\n\n    Parameters\n    ----------\n    doc_id : str\n\n    Returns\n    -------\n    pmid : str\n    \"\"\"\n    url_pmid = 'https://www.ebi.ac.uk/chembl/api/data/document.json'\n    params = {'document_chembl_id': doc_id}\n    res = requests.get(url_pmid, params=params)\n    js = res.json()\n    pmid = str(js['documents'][0]['pubmed_id'])\n    return pmid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_target_chemblid(target_upid):\n    url = 'https://www.ebi.ac.uk/chembl/api/data/target.json'\n    params = {'target_components__accession': target_upid}\n    r = requests.get(url, params=params)\n    r.raise_for_status()\n    js = r.json()\n    target_chemblid = js['targets'][0]['target_chembl_id']\n    return target_chemblid", "response": "Get ChEMBL ID from UniProt upid\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget MESH ID from NLM MESH", "response": "def get_mesh_id(nlm_mesh):\n    \"\"\"Get MESH ID from NLM MESH\n\n    Parameters\n    ----------\n    nlm_mesh : str\n\n    Returns\n    -------\n    mesh_id : str\n    \"\"\"\n    url_nlm2mesh = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi'\n    params = {'db': 'mesh', 'term': nlm_mesh, 'retmode': 'JSON'}\n    r = requests.get(url_nlm2mesh, params=params)\n    res = r.json()\n    mesh_id = res['esearchresult']['idlist'][0]\n    return mesh_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets PC ID from MESH ID", "response": "def get_pcid(mesh_id):\n    \"\"\"Get PC ID from MESH ID\n\n    Parameters\n    ----------\n    mesh : str\n\n    Returns\n    -------\n    pcid : str\n    \"\"\"\n    url_mesh2pcid = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi'\n    params = {'dbfrom': 'mesh', 'id': mesh_id,\n              'db': 'pccompound', 'retmode': 'JSON'}\n    r = requests.get(url_mesh2pcid, params=params)\n    res = r.json()\n    pcid = res['linksets'][0]['linksetdbs'][0]['links'][0]\n    return pcid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_chembl_id(nlm_mesh):\n    mesh_id = get_mesh_id(nlm_mesh)\n    pcid = get_pcid(mesh_id)\n    url_mesh2pcid = 'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/' + \\\n                    'cid/%s/synonyms/JSON' % pcid\n    r = requests.get(url_mesh2pcid)\n    res = r.json()\n    synonyms = res['InformationList']['Information'][0]['Synonym']\n    chembl_id = [syn for syn in synonyms\n                 if 'CHEMBL' in syn and 'SCHEMBL' not in syn][0]\n    return chembl_id", "response": "Get ChEMBL ID from NLM MESH"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of plain - text sentences by iterating through the root element and adding the block_tags to the list of block_tags.", "response": "def get_sentences(self, root_element, block_tags):\n        \"\"\"Returns a list of plain-text sentences by iterating through\n        XML tags except for those listed in block_tags.\"\"\"\n        sentences = []\n        for element in root_element:\n            if not self.any_ends_with(block_tags, element.tag):\n                # tag not in block_tags\n                if element.text is not None and not re.match('^\\s*$',\n                                                             element.text):\n                    sentences.extend(self.sentence_tokenize(element.text))\n                sentences.extend(self.get_sentences(element, block_tags))\n\n        f = open('sentence_debug.txt', 'w')\n        for s in sentences:\n            f.write(s.lower() + '\\n')\n        f.close()\n        return sentences"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef any_ends_with(self, string_list, pattern):\n        try:\n            s_base = basestring\n        except:\n            s_base = str\n        is_string = isinstance(pattern, s_base)\n\n        if not is_string:\n            return False\n        for s in string_list:\n            if pattern.endswith(s):\n                return True\n\n        return False", "response": "Returns true iff one of the strings in string_list ends in pattern."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_tag_names(self):\n        root = etree.fromstring(self.xml_full_text.encode('utf-8'))\n        return self.get_children_tag_names(root)", "response": "Returns the set of tag names present in the XML."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_children_tag_names(self, xml_element):\n        tags = set()\n        tags.add(self.remove_namespace_from_tag(xml_element.tag))\n\n        for element in xml_element.iter(tag=etree.Element):\n            if element != xml_element:\n                new_tags = self.get_children_tag_names(element)\n                if new_tags is not None:\n                    tags.update(new_tags)\n        return tags", "response": "Returns all tag names of xml element and its children."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if two strings match modulo their whitespace.", "response": "def string_matches_sans_whitespace(self, str1, str2_fuzzy_whitespace):\n        \"\"\"Check if two strings match, modulo their whitespace.\"\"\"\n        str2_fuzzy_whitespace = re.sub('\\s+', '\\s*', str2_fuzzy_whitespace)\n        return re.search(str2_fuzzy_whitespace, str1) is not None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sentence_matches(self, sentence_text):\n        has_upstream = False\n        has_downstream = False\n        has_verb = False\n\n        # Get the first word of the action type and assume this is the verb\n        # (Ex. get depends for depends on)\n        actiontype_words = word_tokenize(self.mention.actiontype)\n        actiontype_verb_stemmed = stem(actiontype_words[0])\n\n        words = word_tokenize(sentence_text)\n\n        if self.string_matches_sans_whitespace(sentence_text.lower(),\n            self.mention.upstream.lower()):\n            has_upstream = True\n\n        if self.string_matches_sans_whitespace(sentence_text.lower(),\n            self.mention.downstream.lower()):\n            has_downstream = True\n\n        for word in words:\n            if actiontype_verb_stemmed == stem(word):\n                has_verb = True\n\n        return has_upstream and has_downstream and has_verb", "response": "Returns true iff the sentence contains this mention s upstream\n        and downstream participants and if one of the stemmed verbs in\n        and the sentence is the same as the stemmed action type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an identifiers. org URL corresponding to a given database name and ID.", "response": "def get_identifiers_url(db_name, db_id):\n    \"\"\"Return an identifiers.org URL for a given database name and ID.\n\n    Parameters\n    ----------\n    db_name : str\n        An internal database name: HGNC, UP, CHEBI, etc.\n    db_id : str\n        An identifier in the given database.\n\n    Returns\n    -------\n    url : str\n        An identifiers.org URL corresponding to the given database name and ID.\n    \"\"\"\n    identifiers_url = 'http://identifiers.org/'\n    bel_scai_url = 'https://arty.scai.fraunhofer.de/artifactory/bel/namespace/'\n    if db_name == 'UP':\n        url = identifiers_url + 'uniprot/%s' % db_id\n    elif db_name == 'HGNC':\n        url = identifiers_url + 'hgnc/HGNC:%s' % db_id\n    elif db_name == 'IP':\n        url = identifiers_url + 'interpro/%s' % db_id\n    elif db_name == 'IPR':\n        url = identifiers_url + 'interpro/%s' % db_id\n    elif db_name == 'CHEBI':\n        url = identifiers_url + 'chebi/%s' % db_id\n    elif db_name == 'NCIT':\n        url = identifiers_url + 'ncit/%s' % db_id\n    elif db_name == 'GO':\n        if db_id.startswith('GO:'):\n            url = identifiers_url + 'go/%s' % db_id\n        else:\n            url = identifiers_url + 'go/GO:%s' % db_id\n    elif db_name in ('PUBCHEM', 'PCID'):  # Assuming PCID = PubChem compound ID\n        if db_id.startswith('PUBCHEM:'):\n            db_id = db_id[8:]\n        elif db_id.startswith('PCID:'):\n            db_id = db_id[5:]\n        url = identifiers_url + 'pubchem.compound/%s' % db_id\n    elif db_name == 'PF':\n        url = identifiers_url + 'pfam/%s' % db_id\n    elif db_name == 'MIRBASEM':\n        url = identifiers_url + 'mirbase.mature/%s' % db_id\n    elif db_name == 'MIRBASE':\n        url = identifiers_url + 'mirbase/%s' % db_id\n    elif db_name == 'MESH':\n        url = identifiers_url + 'mesh/%s' % db_id\n    elif db_name == 'EGID':\n        url = identifiers_url + 'ncbigene/%s' % db_id\n    elif db_name == 'HMDB':\n        url = identifiers_url + 'hmdb/%s' % db_id\n    elif db_name == 'LINCS':\n        if db_id.startswith('LSM-'):  # Lincs Small Molecule ID\n            url = identifiers_url + 'lincs.smallmolecule/%s' % db_id\n        elif db_id.startswith('LCL-'):  # Lincs Cell Line ID\n            url = identifiers_url + 'lincs.cell/%s' % db_id\n        else:  # Assume LINCS Protein\n            url = identifiers_url + 'lincs.protein/%s' % db_id\n    elif db_name == 'HMS-LINCS':\n        url = 'http://lincs.hms.harvard.edu/db/sm/%s-101' % db_id\n\n    # Special cases with no identifiers entry\n    elif db_name == 'SCHEM':\n        url = bel_scai_url + 'selventa-legacy-chemicals/' + \\\n            'selventa-legacy-chemicals-20150601.belns'\n    elif db_name == 'SCOMP':\n        url = bel_scai_url + 'selventa-named-complexes/' + \\\n            'selventa-named-complexes-20150601.belns'\n    elif db_name == 'SFAM':\n        url = bel_scai_url + 'selventa-protein-families/' + \\\n            'selventa-protein-families-20150601.belns'\n    elif db_name == 'FPLX':\n        url = 'http://identifiers.org/fplx/%s' % db_id\n    elif db_name == 'LNCRNADB':\n        if db_id.startswith('ENSG'):\n            url = 'http://www.lncrnadb.org/search/?q=%s' % db_id\n        else:  # Assmuing HGNC symbol\n            url = 'http://www.lncrnadb.org/%s/' % db_id\n    elif db_name == 'NXPFA':\n        url = 'https://www.nextprot.org/term/FA-%s' % db_id\n    elif db_name in ('UN', 'WDI', 'FAO'):\n        url = 'https://github.com/clulab/eidos/wiki/JSON-LD#Grounding/%s' % \\\n                db_id\n    elif db_name == 'HUME':\n        url = ('https://github.com/BBN-E/Hume/blob/master/resource/ontologies/'\n               'hume_ontology/%s' % db_id)\n    elif db_name == 'CWMS':\n        url = 'http://trips.ihmc.us/%s' % db_id\n    elif db_name == 'SIGNOR':  # Assuming db_id == Primary ID\n        url = 'https://signor.uniroma2.it/relation_result.php?id=%s' % db_id\n    elif db_name == 'SOFIA':\n        url = 'http://cs.cmu.edu/sofia/%s' % db_id\n    elif db_name == 'CHEMBL':\n        if not db_id.startswith('CHEMBL'):\n            db_id = 'CHEMBL%s' % db_id\n        url = identifiers_url + 'chembl.compound/%s' % db_id\n    elif db_name == 'NONCODE':\n        url = 'http://www.noncode.org/show_gene.php?id=NONHSAG%s' % db_id\n    elif db_name == 'TEXT':\n        return None\n    else:\n        logger.warning('Unhandled name space %s' % db_name)\n        url = None\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndump a list of statements into a pickle file.", "response": "def dump_statements(stmts, fname, protocol=4):\n    \"\"\"Dump a list of statements into a pickle file.\n\n    Parameters\n    ----------\n    fname : str\n        The name of the pickle file to dump statements into.\n    protocol : Optional[int]\n        The pickle protocol to use (use 2 for Python 2 compatibility).\n        Default: 4\n    \"\"\"\n    logger.info('Dumping %d statements into %s...' % (len(stmts), fname))\n    with open(fname, 'wb') as fh:\n        pickle.dump(stmts, fh, protocol=protocol)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a list of statements from a pickle file.", "response": "def load_statements(fname, as_dict=False):\n    \"\"\"Load statements from a pickle file.\n\n    Parameters\n    ----------\n    fname : str\n        The name of the pickle file to load statements from.\n    as_dict : Optional[bool]\n        If True and the pickle file contains a dictionary of statements, it\n        is returned as a dictionary. If False, the statements are always\n        returned in a list. Default: False\n\n    Returns\n    -------\n    stmts : list\n        A list or dict of statements that were loaded.\n    \"\"\"\n    logger.info('Loading %s...' % fname)\n    with open(fname, 'rb') as fh:\n        # Encoding argument not available in pickle for Python 2\n        if sys.version_info[0] < 3:\n            stmts = pickle.load(fh)\n        # Encoding argument specified here to enable compatibility with\n        # pickle files created with Python 2\n        else:\n            stmts = pickle.load(fh, encoding='latin1')\n\n    if isinstance(stmts, dict):\n        if as_dict:\n            return stmts\n        st = []\n        for pmid, st_list in stmts.items():\n            st += st_list\n        stmts = st\n    logger.info('Loaded %d statements' % len(stmts))\n    return stmts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_grounding(stmts_in, **kwargs):\n    from indra.preassembler.grounding_mapper import GroundingMapper\n    from indra.preassembler.grounding_mapper import gm as grounding_map\n    from indra.preassembler.grounding_mapper import \\\n        default_agent_map as agent_map\n    logger.info('Mapping grounding on %d statements...' % len(stmts_in))\n    do_rename = kwargs.get('do_rename')\n    gm = kwargs.get('grounding_map', grounding_map)\n    if do_rename is None:\n        do_rename = True\n    gm = GroundingMapper(gm, agent_map, use_deft=kwargs.get('use_deft', True))\n    stmts_out = gm.map_agents(stmts_in, do_rename=do_rename)\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Map grounding on a list of statements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngather and merge original grounding information from evidences.", "response": "def merge_groundings(stmts_in):\n    \"\"\"Gather and merge original grounding information from evidences.\n\n    Each Statement's evidences are traversed to find original grounding\n    information. These groundings are then merged into an overall consensus\n    grounding dict with as much detail as possible.\n\n    The current implementation is only applicable to Statements whose\n    concept/agent roles are fixed. Complexes, Associations and Conversions\n    cannot be handled correctly.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of INDRA Statements whose groundings should be merged. These\n        Statements are meant to have been preassembled and potentially have\n        multiple pieces of evidence.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        The list of Statements now with groundings merged at the Statement\n        level.\n    \"\"\"\n    def surface_grounding(stmt):\n        # Find the \"best\" grounding for a given concept and its evidences\n        # and surface that\n        for idx, concept in enumerate(stmt.agent_list()):\n            if concept is None:\n                continue\n            aggregate_groundings = {}\n            for ev in stmt.evidence:\n                if 'agents' in ev.annotations:\n                    groundings = ev.annotations['agents']['raw_grounding'][idx]\n                    for ns, value in groundings.items():\n                        if ns not in aggregate_groundings:\n                            aggregate_groundings[ns] = []\n                        if isinstance(value, list):\n                            aggregate_groundings[ns] += value\n                        else:\n                            aggregate_groundings[ns].append(value)\n            best_groundings = get_best_groundings(aggregate_groundings)\n            concept.db_refs = best_groundings\n\n    def get_best_groundings(aggregate_groundings):\n        best_groundings = {}\n        for ns, values in aggregate_groundings.items():\n            # There are 3 possibilities here\n            # 1. All the entries in the list are scored in which case we\n            # get unique entries and sort them by score\n            if all([isinstance(v, (tuple, list)) for v in values]):\n                best_groundings[ns] = []\n                for unique_value in {v[0] for v in values}:\n                    scores = [v[1] for v in values if v[0] == unique_value]\n                    best_groundings[ns].append((unique_value, max(scores)))\n\n                best_groundings[ns] = \\\n                    sorted(best_groundings[ns], key=lambda x: x[1],\n                           reverse=True)\n            # 2. All the entries in the list are unscored in which case we\n            # get the highest frequency entry\n            elif all([not isinstance(v, (tuple, list)) for v in values]):\n                best_groundings[ns] = max(set(values), key=values.count)\n            # 3. There is a mixture, which can happen when some entries were\n            # mapped with scores and others had no scores to begin with.\n            # In this case, we again pick the highest frequency non-scored\n            # entry assuming that the unmapped version is more reliable.\n            else:\n                unscored_vals = [v for v in values\n                                 if not isinstance(v, (tuple, list))]\n                best_groundings[ns] = max(set(unscored_vals),\n                                          key=unscored_vals.count)\n        return best_groundings\n\n    stmts_out = []\n    for stmt in stmts_in:\n        if not isinstance(stmt, (Complex, Conversion)):\n            surface_grounding(stmt)\n        stmts_out.append(stmt)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_deltas(stmts_in):\n    stmts_out = []\n    for stmt in stmts_in:\n        # This operation is only applicable to Influences\n        if not isinstance(stmt, Influence):\n            stmts_out.append(stmt)\n            continue\n        # At this point this is guaranteed to be an Influence\n        deltas = {}\n        for role in ('subj', 'obj'):\n            for info in ('polarity', 'adjectives'):\n                key = (role, info)\n                deltas[key] = []\n                for ev in stmt.evidence:\n                    entry = ev.annotations.get('%s_%s' % key)\n                    deltas[key].append(entry if entry else None)\n        # POLARITY\n        # For polarity we need to work in pairs\n        polarity_pairs = list(zip(deltas[('subj', 'polarity')],\n                                  deltas[('obj', 'polarity')]))\n        # If we have some fully defined pairs, we take the most common one\n        both_pols = [pair for pair in polarity_pairs if pair[0] is not None and\n                     pair[1] is not None]\n        if both_pols:\n            subj_pol, obj_pol = max(set(both_pols), key=both_pols.count)\n            stmt.subj.delta['polarity'] = subj_pol\n            stmt.obj.delta['polarity'] = obj_pol\n        # Otherwise we prefer the case when at least one entry of the\n        # pair is given\n        else:\n            one_pol = [pair for pair in polarity_pairs if pair[0] is not None or\n                       pair[1] is not None]\n            if one_pol:\n                subj_pol, obj_pol = max(set(one_pol), key=one_pol.count)\n                stmt.subj.delta['polarity'] = subj_pol\n                stmt.obj.delta['polarity'] = obj_pol\n\n        # ADJECTIVES\n        for attr, role in ((stmt.subj.delta, 'subj'), (stmt.obj.delta, 'obj')):\n            all_adjectives = []\n            for adj in deltas[(role, 'adjectives')]:\n                if isinstance(adj, list):\n                    all_adjectives += adj\n                elif adj is not None:\n                    all_adjectives.append(adj)\n            attr['adjectives'] = all_adjectives\n        stmts_out.append(stmt)\n    return stmts_out", "response": "Gather and merge original Influence delta information from INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping a list of statements into a single entry in the cache.", "response": "def map_sequence(stmts_in, **kwargs):\n    \"\"\"Map sequences using the SiteMapper.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to map.\n    do_methionine_offset : boolean\n        Whether to check for off-by-one errors in site position (possibly)\n        attributable to site numbering from mature proteins after\n        cleavage of the initial methionine. If True, checks the reference\n        sequence for a known modification at 1 site position greater\n        than the given one; if there exists such a site, creates the\n        mapping. Default is True.\n    do_orthology_mapping : boolean\n        Whether to check sequence positions for known modification sites\n        in mouse or rat sequences (based on PhosphoSitePlus data). If a\n        mouse/rat site is found that is linked to a site in the human\n        reference sequence, a mapping is created. Default is True.\n    do_isoform_mapping : boolean\n        Whether to check sequence positions for known modifications\n        in other human isoforms of the protein (based on PhosphoSitePlus\n        data). If a site is found that is linked to a site in the human\n        reference sequence, a mapping is created. Default is True.\n    use_cache : boolean\n        If True, a cache will be created/used from the laction specified by\n        SITEMAPPER_CACHE_PATH, defined in your INDRA config or the environment.\n        If False, no cache is used. For more details on the cache, see the\n        SiteMapper class definition.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of mapped statements.\n    \"\"\"\n    from indra.preassembler.sitemapper import SiteMapper, default_site_map\n    logger.info('Mapping sites on %d statements...' % len(stmts_in))\n    kwarg_list = ['do_methionine_offset', 'do_orthology_mapping',\n                  'do_isoform_mapping']\n    sm = SiteMapper(default_site_map,\n                    use_cache=kwargs.pop('use_cache', False),\n                    **_filter(kwargs, kwarg_list))\n    valid, mapped = sm.map_sites(stmts_in)\n    correctly_mapped_stmts = []\n    for ms in mapped:\n        correctly_mapped = all([mm.has_mapping() for mm in ms.mapped_mods])\n        if correctly_mapped:\n            correctly_mapped_stmts.append(ms.mapped_stmt)\n    stmts_out = valid + correctly_mapped_stmts\n    logger.info('%d statements with valid sites' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    del sm\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns preassembly on a list of statements.", "response": "def run_preassembly(stmts_in, **kwargs):\n    \"\"\"Run preassembly on a list of statements.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to preassemble.\n    return_toplevel : Optional[bool]\n        If True, only the top-level statements are returned. If False,\n        all statements are returned irrespective of level of specificity.\n        Default: True\n    poolsize : Optional[int]\n        The number of worker processes to use to parallelize the\n        comparisons performed by the function. If None (default), no\n        parallelization is performed. NOTE: Parallelization is only\n        available on Python 3.4 and above.\n    size_cutoff : Optional[int]\n        Groups with size_cutoff or more statements are sent to worker\n        processes, while smaller groups are compared in the parent process.\n        Default value is 100. Not relevant when parallelization is not\n        used.\n    belief_scorer : Optional[indra.belief.BeliefScorer]\n        Instance of BeliefScorer class to use in calculating Statement\n        probabilities. If None is provided (default), then the default\n        scorer is used.\n    hierarchies : Optional[dict]\n        Dict of hierarchy managers to use for preassembly\n    flatten_evidence : Optional[bool]\n        If True, evidences are collected and flattened via supports/supported_by\n        links. Default: False\n    flatten_evidence_collect_from : Optional[str]\n        String indicating whether to collect and flatten evidence from the\n        `supports` attribute of each statement or the `supported_by` attribute.\n        If not set, defaults to 'supported_by'.\n        Only relevant when flatten_evidence is True.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n    save_unique : Optional[str]\n        The name of a pickle file to save the unique statements into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of preassembled top-level statements.\n    \"\"\"\n    dump_pkl_unique = kwargs.get('save_unique')\n    belief_scorer = kwargs.get('belief_scorer')\n    use_hierarchies = kwargs['hierarchies'] if 'hierarchies' in kwargs else \\\n        hierarchies\n    be = BeliefEngine(scorer=belief_scorer)\n    pa = Preassembler(hierarchies, stmts_in)\n    run_preassembly_duplicate(pa, be, save=dump_pkl_unique)\n\n    dump_pkl = kwargs.get('save')\n    return_toplevel = kwargs.get('return_toplevel', True)\n    poolsize = kwargs.get('poolsize', None)\n    size_cutoff = kwargs.get('size_cutoff', 100)\n    options = {'save': dump_pkl, 'return_toplevel': return_toplevel,\n               'poolsize': poolsize, 'size_cutoff': size_cutoff,\n               'flatten_evidence': kwargs.get('flatten_evidence', False),\n               'flatten_evidence_collect_from':\n                   kwargs.get('flatten_evidence_collect_from', 'supported_by')\n               }\n    stmts_out = run_preassembly_related(pa, be, **options)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun deduplication stage of preassembly on a list of statements.", "response": "def run_preassembly_duplicate(preassembler, beliefengine, **kwargs):\n    \"\"\"Run deduplication stage of preassembly on a list of statements.\n\n    Parameters\n    ----------\n    preassembler : indra.preassembler.Preassembler\n        A Preassembler instance\n    beliefengine : indra.belief.BeliefEngine\n        A BeliefEngine instance.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of unique statements.\n    \"\"\"\n    logger.info('Combining duplicates on %d statements...' %\n                len(preassembler.stmts))\n    dump_pkl = kwargs.get('save')\n    stmts_out = preassembler.combine_duplicates()\n    beliefengine.set_prior_probs(stmts_out)\n    logger.info('%d unique statements' % len(stmts_out))\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning related stage of preassembly on a list of statements.", "response": "def run_preassembly_related(preassembler, beliefengine, **kwargs):\n    \"\"\"Run related stage of preassembly on a list of statements.\n\n    Parameters\n    ----------\n    preassembler : indra.preassembler.Preassembler\n        A Preassembler instance which already has a set of unique statements\n        internally.\n    beliefengine : indra.belief.BeliefEngine\n        A BeliefEngine instance.\n    return_toplevel : Optional[bool]\n        If True, only the top-level statements are returned. If False,\n        all statements are returned irrespective of level of specificity.\n        Default: True\n    poolsize : Optional[int]\n        The number of worker processes to use to parallelize the\n        comparisons performed by the function. If None (default), no\n        parallelization is performed. NOTE: Parallelization is only\n        available on Python 3.4 and above.\n    size_cutoff : Optional[int]\n        Groups with size_cutoff or more statements are sent to worker\n        processes, while smaller groups are compared in the parent process.\n        Default value is 100. Not relevant when parallelization is not\n        used.\n    flatten_evidence : Optional[bool]\n        If True, evidences are collected and flattened via supports/supported_by\n        links. Default: False\n    flatten_evidence_collect_from : Optional[str]\n        String indicating whether to collect and flatten evidence from the\n        `supports` attribute of each statement or the `supported_by` attribute.\n        If not set, defaults to 'supported_by'.\n        Only relevant when flatten_evidence is True.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of preassembled top-level statements.\n    \"\"\"\n    logger.info('Combining related on %d statements...' %\n                len(preassembler.unique_stmts))\n    return_toplevel = kwargs.get('return_toplevel', True)\n    poolsize = kwargs.get('poolsize', None)\n    size_cutoff = kwargs.get('size_cutoff', 100)\n    stmts_out = preassembler.combine_related(return_toplevel=False,\n                                             poolsize=poolsize,\n                                             size_cutoff=size_cutoff)\n    # Calculate beliefs\n    beliefengine.set_hierarchy_probs(stmts_out)\n\n    # Flatten evidence if needed\n    do_flatten_evidence = kwargs.get('flatten_evidence', False)\n    if do_flatten_evidence:\n        flatten_evidences_collect_from = \\\n            kwargs.get('flatten_evidence_collect_from', 'supported_by')\n        stmts_out = flatten_evidence(stmts_out, flatten_evidences_collect_from)\n\n    # Filter to top if needed\n    stmts_top = filter_top_level(stmts_out)\n    if return_toplevel:\n        stmts_out = stmts_top\n        logger.info('%d top-level statements' % len(stmts_out))\n    else:\n        logger.info('%d statements out of which %d are top-level' %\n                    (len(stmts_out), len(stmts_top)))\n\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_by_type(stmts_in, stmt_type, **kwargs):\n    invert = kwargs.get('invert', False)\n    logger.info('Filtering %d statements for type %s%s...' %\n                (len(stmts_in), 'not ' if invert else '',\n                 stmt_type.__name__))\n    if not invert:\n        stmts_out = [st for st in stmts_in if isinstance(st, stmt_type)]\n    else:\n        stmts_out = [st for st in stmts_in if not isinstance(st, stmt_type)]\n\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter to a given statement type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_bound_conditions(agent, keep_criterion):\n    new_bc = []\n    for ind in range(len(agent.bound_conditions)):\n        if keep_criterion(agent.bound_conditions[ind].agent):\n            new_bc.append(agent.bound_conditions[ind])\n    agent.bound_conditions = new_bc", "response": "Removes bound conditions of agent that are not in keep_criterion."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _any_bound_condition_fails_criterion(agent, criterion):\n    bc_agents = [bc.agent for bc in agent.bound_conditions]\n    for b in bc_agents:\n        if not criterion(b):\n            return True\n    return False", "response": "Returns True if any of the agents in the specified agent s bound conditions fail to meet the specified criterion."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter to statements that have grounded agents.", "response": "def filter_grounded_only(stmts_in, **kwargs):\n    \"\"\"Filter to statements that have grounded agents.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    score_threshold : Optional[float]\n        If scored groundings are available in a list and the highest score\n        if below this threshold, the Statement is filtered out.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n    remove_bound: Optional[bool]\n        If true, removes ungrounded bound conditions from a statement.\n        If false (default), filters out statements with ungrounded bound\n        conditions.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    remove_bound = kwargs.get('remove_bound', False)\n\n    logger.info('Filtering %d statements for grounded agents...' % \n                len(stmts_in))\n    stmts_out = []\n    score_threshold = kwargs.get('score_threshold')\n    for st in stmts_in:\n        grounded = True\n        for agent in st.agent_list():\n            if agent is not None:\n                criterion = lambda x: _agent_is_grounded(x, score_threshold)\n                if not criterion(agent):\n                    grounded = False\n                    break\n                if not isinstance(agent, Agent):\n                    continue\n                if remove_bound:\n                    _remove_bound_conditions(agent, criterion)\n                elif _any_bound_condition_fails_criterion(agent, criterion):\n                    grounded = False\n                    break\n        if grounded:\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether an agent is a gene.", "response": "def _agent_is_gene(agent, specific_only):\n    \"\"\"Returns whether an agent is for a gene.\n\n    Parameters\n    ----------\n    agent: Agent\n        The agent to evaluate\n    specific_only : Optional[bool]\n        If True, only elementary genes/proteins evaluate as genes and families\n        will be filtered out. If False, families are also included.\n\n    Returns\n    -------\n    is_gene: bool\n        Whether the agent is a gene\n    \"\"\"\n    if not specific_only:\n        if not(agent.db_refs.get('HGNC') or \\\n               agent.db_refs.get('UP') or \\\n               agent.db_refs.get('FPLX')):\n            return False\n    else:\n        if not(agent.db_refs.get('HGNC') or \\\n               agent.db_refs.get('UP')):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_genes_only(stmts_in, **kwargs):\n    remove_bound = 'remove_bound' in kwargs and kwargs['remove_bound']\n\n    specific_only = kwargs.get('specific_only')\n    logger.info('Filtering %d statements for ones containing genes only...' % \n                len(stmts_in))\n    stmts_out = []\n    for st in stmts_in:\n        genes_only = True\n        for agent in st.agent_list():\n            if agent is not None:\n                criterion = lambda a: _agent_is_gene(a, specific_only)\n                if not criterion(agent):\n                    genes_only = False\n                    break\n                if remove_bound:\n                    _remove_bound_conditions(agent, criterion)\n                else:\n                    if _any_bound_condition_fails_criterion(agent, criterion):\n                        genes_only = False\n                        break\n\n        if genes_only:\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter to statements containing genes only."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_belief(stmts_in, belief_cutoff, **kwargs):\n    dump_pkl = kwargs.get('save')\n    logger.info('Filtering %d statements to above %f belief' %\n                (len(stmts_in), belief_cutoff))\n    # The first round of filtering is in the top-level list\n    stmts_out = []\n    # Now we eliminate supports/supported-by\n    for stmt in stmts_in:\n        if stmt.belief < belief_cutoff:\n            continue\n        stmts_out.append(stmt)\n        supp_by = []\n        supp = []\n        for st in stmt.supports:\n            if st.belief >= belief_cutoff:\n                supp.append(st)\n        for st in stmt.supported_by:\n            if st.belief >= belief_cutoff:\n                supp_by.append(st)\n        stmt.supports = supp\n        stmt.supported_by = supp_by\n    logger.info('%d statements after filter...' % len(stmts_out))\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter to statements with belief above a given cutoff."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_gene_list(stmts_in, gene_list, policy, allow_families=False,\n                     **kwargs):\n    \"\"\"Return statements that contain genes given in a list.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    gene_list : list[str]\n        A list of gene symbols to filter for.\n    policy : str\n        The policy to apply when filtering for the list of genes. \"one\": keep\n        statements that contain at least one of the list of genes and\n        possibly others not in the list \"all\": keep statements that only\n        contain genes given in the list\n    allow_families : Optional[bool]\n        Will include statements involving FamPlex families containing one\n        of the genes in the gene list. Default: False\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n    remove_bound: Optional[str]\n        If true, removes bound conditions that are not genes in the list\n        If false (default), looks at agents in the bound conditions in addition\n        to those participating in the statement directly when applying the\n        specified policy.\n    invert : Optional[bool]\n        If True, the statements that do not match according to the policy\n        are returned. Default: False\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    invert = kwargs.get('invert', False)\n    remove_bound = kwargs.get('remove_bound', False)\n\n    if policy not in ('one', 'all'):\n        logger.error('Policy %s is invalid, not applying filter.' % policy)\n    else:\n        genes_str = ', '.join(gene_list)\n        inv_str = 'not ' if invert else ''\n        logger.info(('Filtering %d statements for ones %scontaining \"%s\" of: '\n                     '%s...') % (len(stmts_in), inv_str, policy, genes_str))\n\n    # If we're allowing families, make a list of all FamPlex IDs that\n    # contain members of the gene list, and add them to the filter list\n    filter_list = copy(gene_list)\n    if allow_families:\n        for hgnc_name in gene_list:\n            gene_uri = hierarchies['entity'].get_uri('HGNC', hgnc_name)\n            parents = hierarchies['entity'].get_parents(gene_uri)\n            for par_uri in parents:\n                ns, id = hierarchies['entity'].ns_id_from_uri(par_uri)\n                filter_list.append(id)\n    stmts_out = []\n\n    if remove_bound:\n        # If requested, remove agents whose names are not in the list from\n        # all bound conditions\n        if not invert:\n            keep_criterion = lambda a: a.name in filter_list\n        else:\n            keep_criterion = lambda a: a.name not in filter_list\n\n        for st in stmts_in:\n            for agent in st.agent_list():\n                _remove_bound_conditions(agent, keep_criterion)\n\n    if policy == 'one':\n        for st in stmts_in:\n            found_gene = False\n            if not remove_bound:\n                agent_list = st.agent_list_with_bound_condition_agents()\n            else:\n                agent_list = st.agent_list()\n            for agent in agent_list:\n                if agent is not None:\n                    if agent.name in filter_list:\n                        found_gene = True\n                        break\n            if (found_gene and not invert) or (not found_gene and invert):\n                stmts_out.append(st)\n    elif policy == 'all':\n        for st in stmts_in:\n            found_genes = True\n            if not remove_bound:\n                agent_list = st.agent_list_with_bound_condition_agents()\n            else:\n                agent_list = st.agent_list()\n            for agent in agent_list:\n                if agent is not None:\n                    if agent.name not in filter_list:\n                        found_genes = False\n                        break\n            if (found_genes and not invert) or (not found_genes and invert):\n                stmts_out.append(st)\n    else:\n        stmts_out = stmts_in\n\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter a list of statements that contain genes given in a list of gene symbols."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_concept_names(stmts_in, name_list, policy, **kwargs):\n    invert = kwargs.get('invert', False)\n\n    if policy not in ('one', 'all'):\n        logger.error('Policy %s is invalid, not applying filter.' % policy)\n    else:\n        name_str = ', '.join(name_list)\n        inv_str = 'not ' if invert else ''\n        logger.info(('Filtering %d statements for ones %scontaining \"%s\" of: '\n                     '%s...') % (len(stmts_in), inv_str, policy, name_str))\n\n    stmts_out = []\n\n    if policy == 'one':\n        for st in stmts_in:\n            found = False\n            agent_list = st.agent_list()\n            for agent in agent_list:\n                if agent is not None:\n                    if agent.name in name_list:\n                        found = True\n                        break\n            if (found and not invert) or (not found and invert):\n                stmts_out.append(st)\n    elif policy == 'all':\n        for st in stmts_in:\n            found = True\n            agent_list = st.agent_list()\n            for agent in agent_list:\n                if agent is not None:\n                    if agent.name not in name_list:\n                        found = False\n                        break\n            if (found and not invert) or (not found and invert):\n                stmts_out.append(st)\n    else:\n        stmts_out = stmts_in\n\n    logger.info('%d Statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Return Statements that refer to concepts and agents given as a list of names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering to Statements that have agents with a given db_refs entry.", "response": "def filter_by_db_refs(stmts_in, namespace, values, policy, **kwargs):\n    \"\"\"Filter to Statements whose agents are grounded to a matching entry.\n\n    Statements are filtered so that the db_refs entry (of the given namespace)\n    of their Agent/Concept arguments take a value in the given list of values.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of Statements to filter.\n    namespace : str\n        The namespace in db_refs to which the filter should apply.\n    values : list[str]\n        A list of values in the given namespace to which the filter should\n        apply.\n    policy : str\n        The policy to apply when filtering for the db_refs. \"one\": keep\n        Statements that contain at least one of the list of db_refs and\n        possibly others not in the list \"all\": keep Statements that only\n        contain db_refs given in the list\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n    invert : Optional[bool]\n        If True, the Statements that do not match according to the policy\n        are returned. Default: False\n    match_suffix : Optional[bool]\n        If True, the suffix of the db_refs entry is matches agains the list\n        of entries\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered Statements.\n    \"\"\"\n    invert = kwargs.get('invert', False)\n    match_suffix = kwargs.get('match_suffix', False)\n\n    if policy not in ('one', 'all'):\n        logger.error('Policy %s is invalid, not applying filter.' % policy)\n        return\n    else:\n        name_str = ', '.join(values)\n        rev_mod = 'not ' if invert else ''\n        logger.info(('Filtering %d statements for those with %s agents %s'\n                     'grounded to: %s in the %s namespace...') %\n                        (len(stmts_in), policy, rev_mod, name_str, namespace))\n\n    def meets_criterion(agent):\n        if namespace not in agent.db_refs:\n            return False\n        entry = agent.db_refs[namespace]\n        if isinstance(entry, list):\n            entry = entry[0][0]\n        ret = False\n        # Match suffix or entire entry\n        if match_suffix:\n            if any([entry.endswith(e) for e in values]):\n                ret = True\n        else:\n            if entry in values:\n                ret = True\n        # Invert if needed\n        if invert:\n            return not ret\n        else:\n            return ret\n\n    enough = all if policy == 'all' else any\n\n    stmts_out = [s for s in stmts_in\n                 if enough([meets_criterion(ag) for ag in s.agent_list()\n                            if ag is not None])]\n\n    logger.info('%d Statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_human_only(stmts_in, **kwargs):\n    from indra.databases import uniprot_client\n    if 'remove_bound' in kwargs and kwargs['remove_bound']:\n        remove_bound = True\n    else:\n        remove_bound = False\n\n    dump_pkl = kwargs.get('save')\n    logger.info('Filtering %d statements for human genes only...' %\n                len(stmts_in))\n    stmts_out = []\n\n    def criterion(agent):\n        upid = agent.db_refs.get('UP')\n        if upid and not uniprot_client.is_human(upid):\n            return False\n        else:\n            return True\n\n\n    for st in stmts_in:\n        human_genes = True\n        for agent in st.agent_list():\n            if agent is not None:\n                if not criterion(agent):\n                    human_genes = False\n                    break\n                if remove_bound:\n                    _remove_bound_conditions(agent, criterion)\n                elif _any_bound_condition_fails_criterion(agent, criterion):\n                    human_genes = False\n                    break\n        if human_genes:\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter out statements that are grounded but not to a human gene."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters to statements that are direct interactions.", "response": "def filter_direct(stmts_in, **kwargs):\n    \"\"\"Filter to statements that are direct interactions\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    def get_is_direct(stmt):\n        \"\"\"Returns true if there is evidence that the statement is a direct\n        interaction.\n\n        If any of the evidences associated with the statement\n        indicates a direct interatcion then we assume the interaction\n        is direct. If there is no evidence for the interaction being indirect\n        then we default to direct.\n        \"\"\"\n        any_indirect = False\n        for ev in stmt.evidence:\n            if ev.epistemics.get('direct') is True:\n                return True\n            elif ev.epistemics.get('direct') is False:\n                # This guarantees that we have seen at least\n                # some evidence that the statement is indirect\n                any_indirect = True\n        if any_indirect:\n            return False\n        return True\n    logger.info('Filtering %d statements to direct ones...' % len(stmts_in))\n    stmts_out = []\n    for st in stmts_in:\n        if get_is_direct(st):\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering to statements that are not marked as hypothesis in epistemics.", "response": "def filter_no_hypothesis(stmts_in, **kwargs):\n    \"\"\"Filter to statements that are not marked as hypothesis in epistemics.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    logger.info('Filtering %d statements to no hypothesis...' % len(stmts_in))\n    stmts_out = []\n    for st in stmts_in:\n        all_hypotheses = True\n        ev = None\n        for ev in st.evidence:\n            if not ev.epistemics.get('hypothesis', False):\n                all_hypotheses = False\n                break\n        if ev is None:\n            all_hypotheses = False\n        if not all_hypotheses:\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter to statements that have evidence from a given set of sources.", "response": "def filter_evidence_source(stmts_in, source_apis, policy='one', **kwargs):\n    \"\"\"Filter to statements that have evidence from a given set of sources.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    source_apis : list[str]\n        A list of sources to filter for. Examples: biopax, bel, reach\n    policy : Optional[str]\n        If 'one', a statement that hase evidence from any of the sources is\n        kept. If 'all', only those statements are kept which have evidence\n        from all the input sources specified in source_apis.\n        If 'none', only those statements are kept that don't have evidence\n        from any of the sources specified in source_apis.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    logger.info('Filtering %d statements to evidence source \"%s\" of: %s...' %\n                (len(stmts_in), policy, ', '.join(source_apis)))\n    stmts_out = []\n    for st in stmts_in:\n        sources = set([ev.source_api for ev in st.evidence])\n        if policy == 'one':\n            if sources.intersection(source_apis):\n                stmts_out.append(st)\n        if policy == 'all':\n            if sources.intersection(source_apis) == set(source_apis):\n                stmts_out.append(st)\n        if policy == 'none':\n            if not sources.intersection(source_apis):\n                stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_top_level(stmts_in, **kwargs):\n    logger.info('Filtering %d statements for top-level...' % len(stmts_in))\n    stmts_out = [st for st in stmts_in if not st.supports]\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter to statements that are at the top - level of the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter out Modifications that modify inconsequential sites in a list of INDRA Statements.", "response": "def filter_inconsequential_mods(stmts_in, whitelist=None, **kwargs):\n    \"\"\"Filter out Modifications that modify inconsequential sites\n\n    Inconsequential here means that the site is not mentioned / tested\n    in any other statement. In some cases specific sites should be\n    preserved, for instance, to be used as readouts in a model.\n    In this case, the given sites can be passed in a whitelist.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    whitelist : Optional[dict]\n        A whitelist containing agent modification sites whose\n        modifications should be preserved even if no other statement\n        refers to them. The whitelist parameter is a dictionary in which\n        the key is a gene name and the value is a list of tuples of\n        (modification_type, residue, position). Example:\n        whitelist = {'MAP2K1': [('phosphorylation', 'S', '222')]}\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    if whitelist is None:\n        whitelist = {}\n    logger.info('Filtering %d statements to remove' % len(stmts_in) +\n                ' inconsequential modifications...')\n    states_used = whitelist\n    for stmt in stmts_in:\n        for agent in stmt.agent_list():\n            if agent is not None:\n                if agent.mods:\n                    for mc in agent.mods:\n                        mod = (mc.mod_type, mc.residue, mc.position)\n                        try:\n                            states_used[agent.name].append(mod)\n                        except KeyError:\n                            states_used[agent.name] = [mod]\n    for k, v in states_used.items():\n        states_used[k] = list(set(v))\n    stmts_out = []\n    for stmt in stmts_in:\n        skip = False\n        if isinstance(stmt, Modification):\n            mod_type = modclass_to_modtype[stmt.__class__]\n            if isinstance(stmt, RemoveModification):\n                mod_type = modtype_to_inverse[mod_type]\n            mod = (mod_type, stmt.residue, stmt.position)\n            used = states_used.get(stmt.sub.name, [])\n            if mod not in used:\n                skip = True\n        if not skip:\n            stmts_out.append(stmt)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_inconsequential_acts(stmts_in, whitelist=None, **kwargs):\n    if whitelist is None:\n        whitelist = {}\n    logger.info('Filtering %d statements to remove' % len(stmts_in) +\n                ' inconsequential activations...')\n    states_used = whitelist\n    for stmt in stmts_in:\n        for agent in stmt.agent_list():\n            if agent is not None:\n                if agent.activity:\n                    act = agent.activity.activity_type\n                    try:\n                        states_used[agent.name].append(act)\n                    except KeyError:\n                        states_used[agent.name] = [act]\n    for k, v in states_used.items():\n        states_used[k] = list(set(v))\n    stmts_out = []\n    for stmt in stmts_in:\n        skip = False\n        if isinstance(stmt, RegulateActivity):\n            used = states_used.get(stmt.obj.name, [])\n            if stmt.obj_activity not in used:\n                skip = True\n        if not skip:\n            stmts_out.append(stmt)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter out Activations that modify inconsequential activities in a list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_mutation_status(stmts_in, mutations, deletions, **kwargs):\n\n    if 'remove_bound' in kwargs and kwargs['remove_bound']:\n        remove_bound = True\n    else:\n        remove_bound = False\n\n    def criterion(agent):\n        if agent is not None and agent.name in deletions:\n            return False\n        if agent is not None and agent.mutations:\n            muts = mutations.get(agent.name, [])\n            for mut in agent.mutations:\n                mut_tup = (mut.residue_from, mut.position, mut.residue_to)\n                if mut_tup not in muts:\n                    return False\n        return True\n\n\n    logger.info('Filtering %d statements for mutation status...' %\n                len(stmts_in))\n    stmts_out = []\n    for stmt in stmts_in:\n        skip = False\n        for agent in stmt.agent_list():\n            if not criterion(agent):\n                skip = True\n                break\n            if remove_bound:\n                _remove_bound_conditions(agent, criterion)\n            elif _any_bound_condition_fails_criterion(agent, criterion):\n                skip = True\n                break\n        if not skip:\n            stmts_out.append(stmt)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Filter a list of statements based on existing mutations and deletions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter Phosphorylations to ones where the enzyme is a known kinase.", "response": "def filter_enzyme_kinase(stmts_in, **kwargs):\n    \"\"\"Filter Phosphorylations to ones where the enzyme is a known kinase.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    logger.info('Filtering %d statements to remove ' % len(stmts_in) +\n                'phosphorylation by non-kinases...')\n    path = os.path.dirname(os.path.abspath(__file__))\n    kinase_table = read_unicode_csv(path + '/../resources/kinases.tsv',\n                                    delimiter='\\t')\n    gene_names = [lin[1] for lin in list(kinase_table)[1:]]\n    stmts_out = []\n    for st in stmts_in:\n        if isinstance(st, Phosphorylation):\n            if st.enz is not None:\n                if st.enz.name in gene_names:\n                    stmts_out.append(st)\n        else:\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering out RegulateAmounts where subject is not a transcription factor.", "response": "def filter_transcription_factor(stmts_in, **kwargs):\n    \"\"\"Filter out RegulateAmounts where subject is not a transcription factor.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    logger.info('Filtering %d statements to remove ' % len(stmts_in) +\n                'amount regulations by non-transcription-factors...')\n    path = os.path.dirname(os.path.abspath(__file__))\n    tf_table = \\\n        read_unicode_csv(path + '/../resources/transcription_factors.csv')\n    gene_names = [lin[1] for lin in list(tf_table)[1:]]\n    stmts_out = []\n    for st in stmts_in:\n        if isinstance(st, RegulateAmount):\n            if st.subj is not None:\n                if st.subj.name in gene_names:\n                    stmts_out.append(st)\n        else:\n            stmts_out.append(st)\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters to Statements corresponding to given UUIDs.", "response": "def filter_uuid_list(stmts_in, uuids, **kwargs):\n    \"\"\"Filter to Statements corresponding to given UUIDs\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements to filter.\n    uuids : list[str]\n        A list of UUIDs to filter for.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n    invert : Optional[bool]\n        Invert the filter to remove the Statements corresponding to the given\n        UUIDs.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of filtered statements.\n    \"\"\"\n    invert = kwargs.get('invert', False)\n    logger.info('Filtering %d statements for %d UUID%s...' %\n                (len(stmts_in), len(uuids), 's' if len(uuids) > 1 else ''))\n    stmts_out = []\n    for st in stmts_in:\n        if not invert:\n            if st.uuid in uuids:\n                stmts_out.append(st)\n        else:\n            if st.uuid not in uuids:\n                stmts_out.append(st)\n\n    logger.info('%d statements after filter...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef expand_families(stmts_in, **kwargs):\n    from indra.tools.expand_families import Expander\n    logger.info('Expanding families on %d statements...' % len(stmts_in))\n    expander = Expander(hierarchies)\n    stmts_out = expander.expand_families(stmts_in)\n    logger.info('%d statements after expanding families...' % len(stmts_out))\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Expand FamPlex Agents to individual genes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reduce_activities(stmts_in, **kwargs):\n    logger.info('Reducing activities on %d statements...' % len(stmts_in))\n    stmts_out = [deepcopy(st) for st in stmts_in]\n    ml = MechLinker(stmts_out)\n    ml.gather_explicit_activities()\n    ml.reduce_activities()\n    stmts_out = ml.statements\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Reduce the activity types in a list of statements into a single entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstrips any context on agents within each statement.", "response": "def strip_agent_context(stmts_in, **kwargs):\n    \"\"\"Strip any context on agents within each statement.\n\n    Parameters\n    ----------\n    stmts_in : list[indra.statements.Statement]\n        A list of statements whose agent context should be stripped.\n    save : Optional[str]\n        The name of a pickle file to save the results (stmts_out) into.\n\n    Returns\n    -------\n    stmts_out : list[indra.statements.Statement]\n        A list of stripped statements.\n    \"\"\"\n    logger.info('Stripping agent context on %d statements...' % len(stmts_in))\n    stmts_out = []\n    for st in stmts_in:\n        new_st = deepcopy(st)\n        for agent in new_st.agent_list():\n            if agent is None:\n                continue\n            agent.mods = []\n            agent.mutations = []\n            agent.activity = None\n            agent.location = None\n            agent.bound_conditions = []\n        stmts_out.append(new_st)\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef standardize_names_groundings(stmts):\n    print('Standardize names to groundings')\n    for stmt in stmts:\n        for concept in stmt.agent_list():\n            db_ns, db_id = concept.get_grounding()\n            if db_id is not None:\n                if isinstance(db_id, list):\n                    db_id = db_id[0][0].split('/')[-1]\n                else:\n                    db_id = db_id.split('/')[-1]\n                db_id = db_id.replace('|', ' ')\n                db_id = db_id.replace('_', ' ')\n                db_id = db_id.replace('ONT::', '')\n                db_id = db_id.capitalize()\n                concept.name = db_id\n    return stmts", "response": "Standardize the names of Concepts with respect to an ontology."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump_stmt_strings(stmts, fname):\n    with open(fname, 'wb') as fh:\n        for st in stmts:\n            fh.write(('%s\\n' % st).encode('utf-8'))", "response": "Save printed statements in a text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rename_db_ref(stmts_in, ns_from, ns_to, **kwargs):\n    logger.info('Remapping \"%s\" to \"%s\" in db_refs on %d statements...' %\n                (ns_from, ns_to, len(stmts_in)))\n    stmts_out = [deepcopy(st) for st in stmts_in]\n    for stmt in stmts_out:\n        for agent in stmt.agent_list():\n            if agent is not None and ns_from in agent.db_refs:\n                agent.db_refs[ns_to] = agent.db_refs.pop(ns_from)\n    dump_pkl = kwargs.get('save')\n    if dump_pkl:\n        dump_statements(stmts_out, dump_pkl)\n    return stmts_out", "response": "Rename an entry in the db_refs of each Agent in the list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef align_statements(stmts1, stmts2, keyfun=None):\n    def name_keyfun(stmt):\n        return tuple(a.name if a is not None else None for\n                     a in stmt.agent_list())\n    if not keyfun:\n        keyfun = name_keyfun\n    matches = []\n    keys1 = [keyfun(s) for s in stmts1]\n    keys2 = [keyfun(s) for s in stmts2]\n    for stmt, key in zip(stmts1, keys1):\n        try:\n            match_idx = keys2.index(key)\n            match_stmt = stmts2[match_idx]\n            matches.append((stmt, match_stmt))\n        except ValueError:\n            matches.append((stmt, None))\n    for stmt, key in zip(stmts2, keys2):\n        try:\n            match_idx = keys1.index(key)\n        except ValueError:\n            matches.append((None, stmt))\n    return matches", "response": "Return alignment of two lists of INDRA Statements by key."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlows level function to format the query string.", "response": "def submit_query_request(end_point, *args, **kwargs):\n    \"\"\"Low level function to format the query string.\"\"\"\n    ev_limit = kwargs.pop('ev_limit', 10)\n    best_first = kwargs.pop('best_first', True)\n    tries = kwargs.pop('tries', 2)\n    # This isn't handled by requests because of the multiple identical agent\n    # keys, e.g. {'agent': 'MEK', 'agent': 'ERK'} which is not supported in\n    # python, but is allowed and necessary in these query strings.\n    # TODO because we use the API Gateway, this feature is not longer needed.\n    # We should just use the requests parameters dict.\n    query_str = '?' + '&'.join(['%s=%s' % (k, v) for k, v in kwargs.items()\n                                if v is not None]\n                               + list(args))\n    return submit_statement_request('get', end_point, query_str,\n                                    ev_limit=ev_limit, best_first=best_first,\n                                    tries=tries)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\neven lower level function to make the request.", "response": "def submit_statement_request(meth, end_point, query_str='', data=None,\n                             tries=2, **params):\n    \"\"\"Even lower level function to make the request.\"\"\"\n    full_end_point = 'statements/' + end_point.lstrip('/')\n    return make_db_rest_request(meth, full_end_point, query_str, data, params, tries)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_stmt_graph(statements, reduce=True, english=False, rankdir=None,\n                      agent_style=None):\n    \"\"\"Render the statement hierarchy as a pygraphviz graph.\n\n    Parameters\n    ----------\n    stmts : list of :py:class:`indra.statements.Statement`\n        A list of top-level statements with associated supporting statements\n        resulting from building a statement hierarchy with\n        :py:meth:`combine_related`.\n    reduce : bool\n        Whether to perform a transitive reduction of the edges in the graph.\n        Default is True.\n    english : bool\n        If True, the statements in the graph are represented by their\n        English-assembled equivalent; otherwise they are represented as\n        text-formatted Statements.\n    rank_dir : str or None\n        Argument to pass through to the  pygraphviz `AGraph` constructor\n        specifying graph layout direction. In particular, a value of 'LR'\n        specifies a left-to-right direction. If None, the pygraphviz default\n        is used.\n    agent_style : dict or None\n        Dict of attributes specifying the visual properties of nodes. If None,\n        the following default attributes are used::\n\n            agent_style = {'color': 'lightgray', 'style': 'filled',\n                           'fontname': 'arial'}\n\n    Returns\n    -------\n    pygraphviz.AGraph\n        Pygraphviz graph with nodes representing statements and edges pointing\n        from supported statements to supported_by statements.\n\n    Examples\n    --------\n    Pattern for getting statements and rendering as a Graphviz graph:\n\n    >>> from indra.preassembler.hierarchy_manager import hierarchies\n    >>> braf = Agent('BRAF')\n    >>> map2k1 = Agent('MAP2K1')\n    >>> st1 = Phosphorylation(braf, map2k1)\n    >>> st2 = Phosphorylation(braf, map2k1, residue='S')\n    >>> pa = Preassembler(hierarchies, [st1, st2])\n    >>> pa.combine_related() # doctest:+ELLIPSIS\n    [Phosphorylation(BRAF(), MAP2K1(), S)]\n    >>> graph = render_stmt_graph(pa.related_stmts)\n    >>> graph.write('example_graph.dot') # To make the DOT file\n    >>> graph.draw('example_graph.png', prog='dot') # To make an image\n\n    Resulting graph:\n\n    .. image:: /images/example_graph.png\n        :align: center\n        :alt: Example statement graph rendered by Graphviz\n\n    \"\"\"\n    from indra.assemblers.english import EnglishAssembler\n    # Set the default agent formatting properties\n    if agent_style is None:\n        agent_style = {'color': 'lightgray', 'style': 'filled',\n                       'fontname': 'arial'}\n    # Sets to store all of the nodes and edges as we recursively process all\n    # of the statements\n    nodes = set([])\n    edges = set([])\n    stmt_dict = {}\n\n    # Recursive function for processing all statements\n    def process_stmt(stmt):\n        nodes.add(str(stmt.matches_key()))\n        stmt_dict[str(stmt.matches_key())] = stmt\n        for sby_ix, sby_stmt in enumerate(stmt.supported_by):\n            edges.add((str(stmt.matches_key()), str(sby_stmt.matches_key())))\n            process_stmt(sby_stmt)\n\n    # Process all of the top-level statements, getting the supporting statements\n    # recursively\n    for stmt in statements:\n        process_stmt(stmt)\n    # Create a networkx graph from the nodes\n    nx_graph = nx.DiGraph()\n    nx_graph.add_edges_from(edges)\n    # Perform transitive reduction if desired\n    if reduce:\n        nx_graph = nx.algorithms.dag.transitive_reduction(nx_graph)\n    # Create a pygraphviz graph from the nx graph\n    try:\n        pgv_graph = pgv.AGraph(name='statements', directed=True,\n                               rankdir=rankdir)\n    except NameError:\n        logger.error('Cannot generate graph because '\n                     'pygraphviz could not be imported.')\n        return None\n    for node in nx_graph.nodes():\n        stmt = stmt_dict[node]\n        if english:\n            ea = EnglishAssembler([stmt])\n            stmt_str = ea.make_model()\n        else:\n            stmt_str = str(stmt)\n        pgv_graph.add_node(node,\n                           label='%s (%d)' % (stmt_str, len(stmt.evidence)),\n                           **agent_style)\n    pgv_graph.add_edges_from(nx_graph.edges())\n    return pgv_graph", "response": "Render the list of statements into a Pygraphviz graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the full set of unique stms in a pre - assembled stmt graph.", "response": "def flatten_stmts(stmts):\n    \"\"\"Return the full set of unique stms in a pre-assembled stmt graph.\n\n    The flattened list of statements returned by this function can be\n    compared to the original set of unique statements to make sure no\n    statements have been lost during the preassembly process.\n\n    Parameters\n    ----------\n    stmts : list of :py:class:`indra.statements.Statement`\n        A list of top-level statements with associated supporting statements\n        resulting from building a statement hierarchy with\n        :py:meth:`combine_related`.\n\n    Returns\n    -------\n    stmts : list of :py:class:`indra.statements.Statement`\n        List of all statements contained in the hierarchical statement graph.\n\n    Examples\n    --------\n    Calling :py:meth:`combine_related` on two statements results in one\n    top-level statement; calling :py:func:`flatten_stmts` recovers both:\n\n    >>> from indra.preassembler.hierarchy_manager import hierarchies\n    >>> braf = Agent('BRAF')\n    >>> map2k1 = Agent('MAP2K1')\n    >>> st1 = Phosphorylation(braf, map2k1)\n    >>> st2 = Phosphorylation(braf, map2k1, residue='S')\n    >>> pa = Preassembler(hierarchies, [st1, st2])\n    >>> pa.combine_related() # doctest:+ELLIPSIS\n    [Phosphorylation(BRAF(), MAP2K1(), S)]\n    >>> flattened = flatten_stmts(pa.related_stmts)\n    >>> flattened.sort(key=lambda x: x.matches_key())\n    >>> flattened\n    [Phosphorylation(BRAF(), MAP2K1()), Phosphorylation(BRAF(), MAP2K1(), S)]\n    \"\"\"\n    total_stmts = set(stmts)\n    for stmt in stmts:\n        if stmt.supported_by:\n            children = flatten_stmts(stmt.supported_by)\n            total_stmts = total_stmts.union(children)\n    return list(total_stmts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flatten_evidence(stmts, collect_from=None):\n    if collect_from is None:\n        collect_from = 'supported_by'\n    if collect_from not in ('supports', 'supported_by'):\n        raise ValueError('collect_from must be one of \"supports\", '\n                         '\"supported_by\"')\n    logger.info('Flattening evidence based on %s' % collect_from)\n    # Copy all of the statements--these will be the ones where we update\n    # the evidence lists\n    stmts = fast_deepcopy(stmts)\n    for stmt in stmts:\n        # We get the original evidence keys here so we can differentiate them\n        # from ones added during flattening.\n        orig_ev_keys = [ev.matches_key() for ev in stmt.evidence]\n        # We now do the flattening\n        total_evidence = _flatten_evidence_for_stmt(stmt, collect_from)\n        # Here we add annotations for each evidence in the list,\n        # depending on whether it's an original direct evidence or one that\n        # was added during flattening\n        new_evidence = []\n        for ev in total_evidence:\n            ev_key = ev.matches_key()\n            if ev_key in orig_ev_keys:\n                ev.annotations['support_type'] = 'direct'\n                new_evidence.append(ev)\n            else:\n                ev_copy = fast_deepcopy(ev)\n                ev_copy.annotations['support_type'] = collect_from\n                new_evidence.append(ev_copy)\n        # Now set the new evidence list as the copied statement's evidence\n        stmt.evidence = new_evidence\n    return stmts", "response": "Flatten the evidence of a list of statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncombines duplicates among statements and save result in unique_stmts.", "response": "def combine_duplicates(self):\n        \"\"\"Combine duplicates among `stmts` and save result in `unique_stmts`.\n\n        A wrapper around the static method :py:meth:`combine_duplicate_stmts`.\n        \"\"\"\n        if self.unique_stmts is None:\n            self.unique_stmts = self.combine_duplicate_stmts(self.stmts)\n        return self.unique_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing the matches_key method to get sets of matching statements.", "response": "def _get_stmt_matching_groups(stmts):\n        \"\"\"Use the matches_key method to get sets of matching statements.\"\"\"\n        def match_func(x): return x.matches_key()\n\n        # Remove exact duplicates using a set() call, then make copies:\n        logger.debug('%d statements before removing object duplicates.' %\n                     len(stmts))\n        st = list(set(stmts))\n        logger.debug('%d statements after removing object duplicates.' %\n                     len(stmts))\n        # Group statements according to whether they are matches (differing\n        # only in their evidence).\n        # Sort the statements in place by matches_key()\n        st.sort(key=match_func)\n\n        return itertools.groupby(st, key=match_func)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncombine evidence from duplicate Statements.", "response": "def combine_duplicate_stmts(stmts):\n        \"\"\"Combine evidence from duplicate Statements.\n\n        Statements are deemed to be duplicates if they have the same key\n        returned by the `matches_key()` method of the Statement class. This\n        generally means that statements must be identical in terms of their\n        arguments and can differ only in their associated `Evidence` objects.\n\n        This function keeps the first instance of each set of duplicate\n        statements and merges the lists of Evidence from all of the other\n        statements.\n\n        Parameters\n        ----------\n        stmts : list of :py:class:`indra.statements.Statement`\n            Set of statements to de-duplicate.\n\n        Returns\n        -------\n        list of :py:class:`indra.statements.Statement`\n            Unique statements with accumulated evidence across duplicates.\n\n        Examples\n        --------\n        De-duplicate and combine evidence for two statements differing only\n        in their evidence lists:\n\n        >>> map2k1 = Agent('MAP2K1')\n        >>> mapk1 = Agent('MAPK1')\n        >>> stmt1 = Phosphorylation(map2k1, mapk1, 'T', '185',\n        ... evidence=[Evidence(text='evidence 1')])\n        >>> stmt2 = Phosphorylation(map2k1, mapk1, 'T', '185',\n        ... evidence=[Evidence(text='evidence 2')])\n        >>> uniq_stmts = Preassembler.combine_duplicate_stmts([stmt1, stmt2])\n        >>> uniq_stmts\n        [Phosphorylation(MAP2K1(), MAPK1(), T, 185)]\n        >>> sorted([e.text for e in uniq_stmts[0].evidence]) # doctest:+IGNORE_UNICODE\n        ['evidence 1', 'evidence 2']\n        \"\"\"\n        # Helper function to get a list of evidence matches keys\n        def _ev_keys(sts):\n            ev_keys = []\n            for stmt in sts:\n                for ev in stmt.evidence:\n                    ev_keys.append(ev.matches_key())\n            return ev_keys\n        # Iterate over groups of duplicate statements\n        unique_stmts = []\n        for _, duplicates in Preassembler._get_stmt_matching_groups(stmts):\n            ev_keys = set()\n            # Get the first statement and add the evidence of all subsequent\n            # Statements to it\n            duplicates = list(duplicates)\n            start_ev_keys = _ev_keys(duplicates)\n            for stmt_ix, stmt in enumerate(duplicates):\n                if stmt_ix is 0:\n                    new_stmt = stmt.make_generic_copy()\n                if len(duplicates) == 1:\n                    new_stmt.uuid = stmt.uuid\n                raw_text = [None if ag is None else ag.db_refs.get('TEXT')\n                            for ag in stmt.agent_list(deep_sorted=True)]\n                raw_grounding = [None if ag is None else ag.db_refs\n                                 for ag in stmt.agent_list(deep_sorted=True)]\n                for ev in stmt.evidence:\n                    ev_key = ev.matches_key() + str(raw_text) + \\\n                        str(raw_grounding)\n                    if ev_key not in ev_keys:\n                        # In case there are already agents annotations, we\n                        # just add a new key for raw_text, otherwise create\n                        # a new key\n                        if 'agents' in ev.annotations:\n                            ev.annotations['agents']['raw_text'] = raw_text\n                            ev.annotations['agents']['raw_grounding'] = \\\n                                raw_grounding\n                        else:\n                            ev.annotations['agents'] = \\\n                                {'raw_text': raw_text,\n                                 'raw_grounding': raw_grounding}\n                        if 'prior_uuids' not in ev.annotations:\n                            ev.annotations['prior_uuids'] = []\n                        ev.annotations['prior_uuids'].append(stmt.uuid)\n                        new_stmt.evidence.append(ev)\n                        ev_keys.add(ev_key)\n            end_ev_keys = _ev_keys([new_stmt])\n            if len(end_ev_keys) != len(start_ev_keys):\n                logger.debug('%d redundant evidences eliminated.' %\n                             (len(start_ev_keys) - len(end_ev_keys)))\n            # This should never be None or anything else\n            assert isinstance(new_stmt, Statement)\n            unique_stmts.append(new_stmt)\n        return unique_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngrouping Statements of stmt_type by their hierarchical relations.", "response": "def _get_stmt_by_group(self, stmt_type, stmts_this_type, eh):\n        \"\"\"Group Statements of `stmt_type` by their hierarchical relations.\"\"\"\n        # Dict of stmt group key tuples, indexed by their first Agent\n        stmt_by_first = collections.defaultdict(lambda: [])\n        # Dict of stmt group key tuples, indexed by their second Agent\n        stmt_by_second = collections.defaultdict(lambda: [])\n        # Dict of statements with None first, with second Agent as keys\n        none_first = collections.defaultdict(lambda: [])\n        # Dict of statements with None second, with first Agent as keys\n        none_second = collections.defaultdict(lambda: [])\n        # The dict of all statement groups, with tuples of components\n        # or entity_matches_keys as keys\n        stmt_by_group = collections.defaultdict(lambda: [])\n        # Here we group Statements according to the hierarchy graph\n        # components that their agents are part of\n        for stmt_tuple in stmts_this_type:\n            _, stmt = stmt_tuple\n            entities = self._get_entities(stmt, stmt_type, eh)\n            # At this point we have an entity list\n            # If we're dealing with Complexes, sort the entities and use\n            # as dict key\n            if stmt_type == Complex:\n                # There shouldn't be any statements of the type\n                # e.g., Complex([Foo, None, Bar])\n                assert None not in entities\n                assert len(entities) > 0\n                entities.sort()\n                key = tuple(entities)\n                if stmt_tuple not in stmt_by_group[key]:\n                    stmt_by_group[key].append(stmt_tuple)\n            elif stmt_type == Conversion:\n                assert len(entities) > 0\n                key = (entities[0],\n                       tuple(sorted(entities[1:len(stmt.obj_from)+1])),\n                       tuple(sorted(entities[-len(stmt.obj_to):])))\n                if stmt_tuple not in stmt_by_group[key]:\n                    stmt_by_group[key].append(stmt_tuple)\n            # Now look at all other statement types\n            # All other statements will have one or two entities\n            elif len(entities) == 1:\n                # If only one entity, we only need the one key\n                # It should not be None!\n                assert None not in entities\n                key = tuple(entities)\n                if stmt_tuple not in stmt_by_group[key]:\n                    stmt_by_group[key].append(stmt_tuple)\n            else:\n                # Make sure we only have two entities, and they are not both\n                # None\n                key = tuple(entities)\n                assert len(key) == 2\n                assert key != (None, None)\n                # First agent is None; add in the statements, indexed by\n                # 2nd\n                if key[0] is None and stmt_tuple not in none_first[key[1]]:\n                    none_first[key[1]].append(stmt_tuple)\n                # Second agent is None; add in the statements, indexed by\n                # 1st\n                elif key[1] is None and stmt_tuple not in none_second[key[0]]:\n                    none_second[key[0]].append(stmt_tuple)\n                # Neither entity is None!\n                elif None not in key:\n                    if stmt_tuple not in stmt_by_group[key]:\n                        stmt_by_group[key].append(stmt_tuple)\n                    if key not in stmt_by_first[key[0]]:\n                        stmt_by_first[key[0]].append(key)\n                    if key not in stmt_by_second[key[1]]:\n                        stmt_by_second[key[1]].append(key)\n\n        # When we've gotten here, we should have stmt_by_group entries, and\n        # we may or may not have stmt_by_first/second dicts filled out\n        # (depending on the statement type).\n        if none_first:\n            # Get the keys associated with stmts having a None first\n            # argument\n            for second_arg, stmts in none_first.items():\n                # Look for any statements with this second arg\n                second_arg_keys = stmt_by_second[second_arg]\n                # If there are no more specific statements matching this\n                # set of statements with a None first arg, then the\n                # statements with the None first arg deserve to be in\n                # their own group.\n                if not second_arg_keys:\n                    stmt_by_group[(None, second_arg)] = stmts\n                # On the other hand, if there are statements with a matching\n                # second arg component, we need to add the None first\n                # statements to all groups with the matching second arg\n                for second_arg_key in second_arg_keys:\n                    stmt_by_group[second_arg_key] += stmts\n        # Now do the corresponding steps for the statements with None as the\n        # second argument:\n        if none_second:\n            for first_arg, stmts in none_second.items():\n                # Look for any statements with this first arg\n                first_arg_keys = stmt_by_first[first_arg]\n                # If there are no more specific statements matching this\n                # set of statements with a None second arg, then the\n                # statements with the None second arg deserve to be in\n                # their own group.\n                if not first_arg_keys:\n                    stmt_by_group[(first_arg, None)] = stmts\n                # On the other hand, if there are statements with a matching\n                # first arg component, we need to add the None second\n                # statements to all groups with the matching first arg\n                for first_arg_key in first_arg_keys:\n                    stmt_by_group[first_arg_key] += stmts\n        return stmt_by_group"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the ID maps for the unique statements.", "response": "def _generate_id_maps(self, unique_stmts, poolsize=None,\n                          size_cutoff=100, split_idx=None):\n        \"\"\"Connect statements using their refinement relationships.\"\"\"\n        # Check arguments relating to multiprocessing\n        if poolsize is None:\n            logger.debug('combine_related: poolsize not set, '\n                         'not using multiprocessing.')\n            use_mp = False\n        elif sys.version_info[0] >= 3 and sys.version_info[1] >= 4:\n            use_mp = True\n            logger.info('combine_related: Python >= 3.4 detected, '\n                        'using multiprocessing with poolsize %d, '\n                        'size_cutoff %d' % (poolsize, size_cutoff))\n        else:\n            use_mp = False\n            logger.info('combine_related: Python < 3.4 detected, '\n                        'not using multiprocessing.')\n        eh = self.hierarchies['entity']\n        # Make a list of Statement types\n        stmts_by_type = collections.defaultdict(lambda: [])\n        for idx, stmt in enumerate(unique_stmts):\n            stmts_by_type[indra_stmt_type(stmt)].append((idx, stmt))\n\n        child_proc_groups = []\n        parent_proc_groups = []\n        skipped_groups = 0\n        # Each Statement type can be preassembled independently\n        for stmt_type, stmts_this_type in stmts_by_type.items():\n            logger.info('Grouping %s (%s)' %\n                        (stmt_type.__name__, len(stmts_this_type)))\n            stmt_by_group = self._get_stmt_by_group(stmt_type, stmts_this_type,\n                                                    eh)\n\n            # Divide statements by group size\n            # If we're not using multiprocessing, then all groups are local\n            for g_name, g in stmt_by_group.items():\n                if len(g) < 2:\n                    skipped_groups += 1\n                    continue\n                if use_mp and len(g) >= size_cutoff:\n                    child_proc_groups.append(g)\n                else:\n                    parent_proc_groups.append(g)\n\n        # Now run preassembly!\n        logger.debug(\"Groups: %d parent, %d worker, %d skipped.\" %\n                     (len(parent_proc_groups), len(child_proc_groups),\n                      skipped_groups))\n\n        supports_func = functools.partial(_set_supports_stmt_pairs,\n                                          hierarchies=self.hierarchies,\n                                          split_idx=split_idx,\n                                          check_entities_match=False)\n\n        # Check if we are running any groups in child processes; note that if\n        # use_mp is False, child_proc_groups will be empty\n        if child_proc_groups:\n            # Get a multiprocessing context\n            ctx = mp.get_context('spawn')\n            pool = ctx.Pool(poolsize)\n            # Run the large groups remotely\n            logger.debug(\"Running %d groups in child processes\" %\n                         len(child_proc_groups))\n            res = pool.map_async(supports_func, child_proc_groups)\n            workers_ready = False\n        else:\n            workers_ready = True\n\n        # Run the small groups locally\n        logger.debug(\"Running %d groups in parent process\" %\n                     len(parent_proc_groups))\n        stmt_ix_map = [supports_func(stmt_tuples)\n                       for stmt_tuples in parent_proc_groups]\n        logger.debug(\"Done running parent process groups\")\n\n        while not workers_ready:\n            logger.debug(\"Checking child processes\")\n            if res.ready():\n                workers_ready = True\n                logger.debug('Child process group comparisons successful? %s' %\n                             res.successful())\n                if not res.successful():\n                    raise Exception(\"Sorry, there was a problem with \"\n                                    \"preassembly in the child processes.\")\n                else:\n                    stmt_ix_map += res.get()\n                logger.debug(\"Closing pool...\")\n                pool.close()\n                logger.debug(\"Joining pool...\")\n                pool.join()\n                logger.debug(\"Pool closed and joined.\")\n            time.sleep(1)\n        logger.debug(\"Done.\")\n        # Combine all redundant map edges\n        stmt_ix_map_set = set([])\n        for group_ix_map in stmt_ix_map:\n            for ix_pair in group_ix_map:\n                stmt_ix_map_set.add(ix_pair)\n        return stmt_ix_map_set"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects related statements based on their refinement relationships. This function takes as a starting point the unique statements (with duplicates removed) and returns a modified flat list of statements containing only those statements which do not represent a refinement of other existing statements. In other words, the more general versions of a given statement do not appear at the top level, but instead are listed in the `supports` field of the top-level statements. If :py:attr:`unique_stmts` has not been initialized with the de-duplicated statements, :py:meth:`combine_duplicates` is called internally. After this function is called the attribute :py:attr:`related_stmts` is set as a side-effect. The procedure for combining statements in this way involves a series of steps: 1. The statements are grouped by type (e.g., Phosphorylation) and each type is iterated over independently. 2. Statements of the same type are then grouped according to their Agents' entity hierarchy component identifiers. For instance, ERK, MAPK1 and MAPK3 are all in the same connected component in the entity hierarchy and therefore all Statements of the same type referencing these entities will be grouped. This grouping assures that relations are only possible within Statement groups and not among groups. For two Statements to be in the same group at this step, the Statements must be the same type and the Agents at each position in the Agent lists must either be in the same hierarchy component, or if they are not in the hierarchy, must have identical entity_matches_keys. Statements with None in one of the Agent list positions are collected separately at this stage. 3. Statements with None at either the first or second position are iterated over. For a statement with a None as the first Agent, the second Agent is examined; then the Statement with None is added to all Statement groups with a corresponding component or entity_matches_key in the second position. The same procedure is performed for Statements with None at the second Agent position. 4. The statements within each group are then compared; if one statement represents a refinement of the other (as defined by the `refinement_of()` method implemented for the Statement), then the more refined statement is added to the `supports` field of the more general statement, and the more general statement is added to the `supported_by` field of the more refined statement. 5. A new flat list of statements is created that contains only those statements that have no `supports` entries (statements containing such entries are not eliminated, because they will be retrievable from the `supported_by` fields of other statements). This list is returned to the caller. On multi-core machines, the algorithm can be parallelized by setting the poolsize argument to the desired number of worker processes. This feature is only available in Python > 3.4. .. note:: Subfamily relationships must be consistent across arguments For now, we require that merges can only occur if the *isa* relationships are all in the *same direction for all the agents* in a Statement. For example, the two statement groups: `RAF_family -> MEK1` and `BRAF -> MEK_family` would not be merged, since BRAF *isa* RAF_family, but MEK_family is not a MEK1. In the future this restriction could be revisited. Parameters ---------- return_toplevel : Optional[bool] If True only the top level statements are returned. If False, all statements are returned. Default: True poolsize : Optional[int] The number of worker processes to use to parallelize the comparisons performed by the function. If None (default), no parallelization is performed. NOTE: Parallelization is only available on Python 3.4 and above. size_cutoff : Optional[int] Groups with size_cutoff or more statements are sent to worker processes, while smaller groups are compared in the parent process. Default value is 100. Not relevant when parallelization is not used. Returns ------- list of :py:class:`indra.statement.Statement` The returned list contains Statements representing the more concrete/refined versions of the Statements involving particular entities. The attribute :py:attr:`related_stmts` is also set to this list. However, if return_toplevel is False then all statements are returned, irrespective of level of specificity. In this case the relationships between statements can be accessed via the supports/supported_by attributes. Examples -------- A more general statement with no information about a Phosphorylation site is identified as supporting a more specific statement: >>> from indra.preassembler.hierarchy_manager import hierarchies >>> braf = Agent('BRAF') >>> map2k1 = Agent('MAP2K1') >>> st1 = Phosphorylation(braf, map2k1) >>> st2 = Phosphorylation(braf, map2k1, residue='S') >>> pa = Preassembler(hierarchies, [st1, st2]) >>> combined_stmts = pa.combine_related() # doctest:+ELLIPSIS >>> combined_stmts [Phosphorylation(BRAF(), MAP2K1(), S)] >>> combined_stmts[0].supported_by [Phosphorylation(BRAF(), MAP2K1())] >>> combined_stmts[0].supported_by[0].supports [Phosphorylation(BRAF(), MAP2K1(), S)]", "response": "def combine_related(self, return_toplevel=True, poolsize=None,\n                        size_cutoff=100):\n        \"\"\"Connect related statements based on their refinement relationships.\n\n        This function takes as a starting point the unique statements (with\n        duplicates removed) and returns a modified flat list of statements\n        containing only those statements which do not represent a refinement of\n        other existing statements. In other words, the more general versions of\n        a given statement do not appear at the top level, but instead are\n        listed in the `supports` field of the top-level statements.\n\n        If :py:attr:`unique_stmts` has not been initialized with the\n        de-duplicated statements, :py:meth:`combine_duplicates` is called\n        internally.\n\n        After this function is called the attribute :py:attr:`related_stmts` is\n        set as a side-effect.\n\n        The procedure for combining statements in this way involves a series\n        of steps:\n\n        1. The statements are grouped by type (e.g., Phosphorylation) and\n           each type is iterated over independently.\n        2. Statements of the same type are then grouped according to their\n           Agents' entity hierarchy component identifiers. For instance,\n           ERK, MAPK1 and MAPK3 are all in the same connected component in the\n           entity hierarchy and therefore all Statements of the same type\n           referencing these entities will be grouped. This grouping assures\n           that relations are only possible within Statement groups and\n           not among groups. For two Statements to be in the same group at\n           this step, the Statements must be the same type and the Agents at\n           each position in the Agent lists must either be in the same\n           hierarchy component, or if they are not in the hierarchy, must have\n           identical entity_matches_keys. Statements with None in one of the\n           Agent list positions are collected separately at this stage.\n        3. Statements with None at either the first or second position are\n           iterated over. For a statement with a None as the first Agent,\n           the second Agent is examined; then the Statement with None is\n           added to all Statement groups with a corresponding component or\n           entity_matches_key in the second position. The same procedure is\n           performed for Statements with None at the second Agent position.\n        4. The statements within each group are then compared; if one\n           statement represents a refinement of the other (as defined by the\n           `refinement_of()` method implemented for the Statement), then the\n           more refined statement is added to the `supports` field of the more\n           general statement, and the more general statement is added to the\n           `supported_by` field of the more refined statement.\n        5. A new flat list of statements is created that contains only those\n           statements that have no `supports` entries (statements containing\n           such entries are not eliminated, because they will be retrievable\n           from the `supported_by` fields of other statements). This list\n           is returned to the caller.\n\n        On multi-core machines, the algorithm can be parallelized by setting\n        the poolsize argument to the desired number of worker processes.\n        This feature is only available in Python > 3.4.\n\n        .. note:: Subfamily relationships must be consistent across arguments\n\n            For now, we require that merges can only occur if the *isa*\n            relationships are all in the *same direction for all the agents* in\n            a Statement. For example, the two statement groups: `RAF_family ->\n            MEK1` and `BRAF -> MEK_family` would not be merged, since BRAF\n            *isa* RAF_family, but MEK_family is not a MEK1. In the future this\n            restriction could be revisited.\n\n        Parameters\n        ----------\n        return_toplevel : Optional[bool]\n            If True only the top level statements are returned.\n            If False, all statements are returned. Default: True\n        poolsize : Optional[int]\n            The number of worker processes to use to parallelize the\n            comparisons performed by the function. If None (default), no\n            parallelization is performed. NOTE: Parallelization is only\n            available on Python 3.4 and above.\n        size_cutoff : Optional[int]\n            Groups with size_cutoff or more statements are sent to worker\n            processes, while smaller groups are compared in the parent process.\n            Default value is 100. Not relevant when parallelization is not\n            used.\n\n        Returns\n        -------\n        list of :py:class:`indra.statement.Statement`\n            The returned list contains Statements representing the more\n            concrete/refined versions of the Statements involving particular\n            entities. The attribute :py:attr:`related_stmts` is also set to\n            this list. However, if return_toplevel is False then all\n            statements are returned, irrespective of level of specificity.\n            In this case the relationships between statements can\n            be accessed via the supports/supported_by attributes.\n\n        Examples\n        --------\n        A more general statement with no information about a Phosphorylation\n        site is identified as supporting a more specific statement:\n\n        >>> from indra.preassembler.hierarchy_manager import hierarchies\n        >>> braf = Agent('BRAF')\n        >>> map2k1 = Agent('MAP2K1')\n        >>> st1 = Phosphorylation(braf, map2k1)\n        >>> st2 = Phosphorylation(braf, map2k1, residue='S')\n        >>> pa = Preassembler(hierarchies, [st1, st2])\n        >>> combined_stmts = pa.combine_related() # doctest:+ELLIPSIS\n        >>> combined_stmts\n        [Phosphorylation(BRAF(), MAP2K1(), S)]\n        >>> combined_stmts[0].supported_by\n        [Phosphorylation(BRAF(), MAP2K1())]\n        >>> combined_stmts[0].supported_by[0].supports\n        [Phosphorylation(BRAF(), MAP2K1(), S)]\n        \"\"\"\n        if self.related_stmts is not None:\n            if return_toplevel:\n                return self.related_stmts\n            else:\n                assert self.unique_stmts is not None\n                return self.unique_stmts\n\n        # Call combine_duplicates, which lazily initializes self.unique_stmts\n        unique_stmts = self.combine_duplicates()\n\n        # Generate the index map, linking related statements.\n        idx_map = self._generate_id_maps(unique_stmts, poolsize, size_cutoff)\n\n        # Now iterate over all indices and set supports/supported by\n        for ix1, ix2 in idx_map:\n            unique_stmts[ix1].supported_by.append(unique_stmts[ix2])\n            unique_stmts[ix2].supports.append(unique_stmts[ix1])\n        # Get the top level statements\n        self.related_stmts = [st for st in unique_stmts if not st.supports]\n        logger.debug('%d top level' % len(self.related_stmts))\n        if return_toplevel:\n            return self.related_stmts\n        else:\n            return unique_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn pairs of contradicting Statements.", "response": "def find_contradicts(self):\n        \"\"\"Return pairs of contradicting Statements.\n\n        Returns\n        -------\n        contradicts : list(tuple(Statement, Statement))\n            A list of Statement pairs that are contradicting.\n        \"\"\"\n        eh = self.hierarchies['entity']\n\n        # Make a dict of Statement by type\n        stmts_by_type = collections.defaultdict(lambda: [])\n        for idx, stmt in enumerate(self.stmts):\n            stmts_by_type[indra_stmt_type(stmt)].append((idx, stmt))\n\n        # Handle Statements with polarity first\n        pos_stmts = AddModification.__subclasses__()\n        neg_stmts = [modclass_to_inverse[c] for c in pos_stmts]\n\n        pos_stmts += [Activation, IncreaseAmount]\n        neg_stmts += [Inhibition, DecreaseAmount]\n\n        contradicts = []\n        for pst, nst in zip(pos_stmts, neg_stmts):\n            poss = stmts_by_type.get(pst, [])\n            negs = stmts_by_type.get(nst, [])\n\n            pos_stmt_by_group = self._get_stmt_by_group(pst, poss, eh)\n            neg_stmt_by_group = self._get_stmt_by_group(nst, negs, eh)\n            for key, pg in pos_stmt_by_group.items():\n                ng = neg_stmt_by_group.get(key, [])\n                for (_, st1), (_, st2) in itertools.product(pg, ng):\n                    if st1.contradicts(st2, self.hierarchies):\n                        contradicts.append((st1, st2))\n\n        # Handle neutral Statements next\n        neu_stmts = [Influence, ActiveForm]\n        for stt in neu_stmts:\n            stmts = stmts_by_type.get(stt, [])\n            for (_, st1), (_, st2) in itertools.combinations(stmts, 2):\n                if st1.contradicts(st2, self.hierarchies):\n                    contradicts.append((st1, st2))\n\n        return contradicts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting text content for articles given a list of their pmids.", "response": "def get_text_content_for_pmids(pmids):\n    \"\"\"Get text content for articles given a list of their pmids\n\n    Parameters\n    ----------\n    pmids : list of str\n\n    Returns\n    -------\n    text_content : list of str\n    \"\"\"\n    pmc_pmids = set(pmc_client.filter_pmids(pmids, source_type='fulltext'))\n\n    pmc_ids = []\n    for pmid in pmc_pmids:\n        pmc_id = pmc_client.id_lookup(pmid, idtype='pmid')['pmcid']\n        if pmc_id:\n            pmc_ids.append(pmc_id)\n        else:\n            pmc_pmids.discard(pmid)\n\n    pmc_xmls = []\n    failed = set()\n    for pmc_id in pmc_ids:\n        if pmc_id is not None:\n            pmc_xmls.append(pmc_client.get_xml(pmc_id))\n        else:\n            failed.append(pmid)\n        time.sleep(0.5)\n\n    remaining_pmids = set(pmids) - pmc_pmids | failed\n    abstracts = []\n    for pmid in remaining_pmids:\n        abstract = pubmed_client.get_abstract(pmid)\n        abstracts.append(abstract)\n        time.sleep(0.5)\n\n    return [text_content for source in (pmc_xmls, abstracts)\n            for text_content in source if text_content is not None]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts paragraphs from xml that could be from different sources", "response": "def universal_extract_paragraphs(xml):\n    \"\"\"Extract paragraphs from xml that could be from  different sources\n\n    First try to parse the xml as if it came from elsevier. if we do not\n    have valid elsevier xml this will throw an exception. the text extraction\n    function in the pmc client may not throw an exception when parsing elsevier\n    xml, silently processing the xml incorrectly\n\n    Parameters\n    ----------\n    xml : str\n       Either an NLM xml, Elsevier xml or plaintext\n\n    Returns\n    -------\n    paragraphs : str\n        Extracted plaintext paragraphs from NLM or Elsevier XML\n    \"\"\"\n    try:\n        paragraphs = elsevier_client.extract_paragraphs(xml)\n    except Exception:\n        paragraphs = None\n    if paragraphs is None:\n        try:\n            paragraphs = pmc_client.extract_paragraphs(xml)\n        except Exception:\n            paragraphs = [xml]\n    return paragraphs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfilters paragraphs to only those containing one of a list of strings", "response": "def filter_paragraphs(paragraphs, contains=None):\n    \"\"\"Filter paragraphs to only those containing one of a list of strings\n\n    Parameters\n    ----------\n    paragraphs : list of str\n        List of plaintext paragraphs from an article\n\n    contains : str or list of str\n        Exclude paragraphs not containing this string as a token, or\n        at least one of the strings in contains if it is a list\n\n    Returns\n    -------\n    str\n        Plaintext consisting of all input paragraphs containing at least\n        one of the supplied tokens.\n    \"\"\"\n    if contains is None:\n        pattern = ''\n    else:\n        if isinstance(contains, str):\n            contains = [contains]\n        pattern = '|'.join(r'[^\\w]%s[^\\w]' % shortform\n                           for shortform in contains)\n    paragraphs = [p for p in paragraphs if re.search(pattern, p)]\n    return '\\n'.join(paragraphs) + '\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_valid_residue(residue):\n    if residue is not None and amino_acids.get(residue) is None:\n        res = amino_acids_reverse.get(residue.lower())\n        if res is None:\n            raise InvalidResidueError(residue)\n        else:\n            return res\n    return residue", "response": "Check if the given string represents a valid amino acid residue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the given location represents a valid cellular component.", "response": "def get_valid_location(location):\n    \"\"\"Check if the given location represents a valid cellular component.\"\"\"\n    # If we're given None, return None\n    if location is not None and cellular_components.get(location) is None:\n        loc = cellular_components_reverse.get(location)\n        if loc is None:\n            raise InvalidLocationError(location)\n        else:\n            return loc\n    return location"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread types of valid activities from a resource file.", "response": "def _read_activity_types():\n    \"\"\"Read types of valid activities from a resource file.\"\"\"\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    ac_file = os.path.join(this_dir, os.pardir, 'resources',\n                           'activity_hierarchy.rdf')\n    g = rdflib.Graph()\n    with open(ac_file, 'r'):\n        g.parse(ac_file, format='nt')\n    act_types = set()\n    for s, _, o in g:\n        subj = s.rpartition('/')[-1]\n        obj = o.rpartition('/')[-1]\n        act_types.add(subj)\n        act_types.add(obj)\n    return sorted(list(act_types))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads cellular components from a resource file.", "response": "def _read_cellular_components():\n    \"\"\"Read cellular components from a resource file.\"\"\"\n    # Here we load a patch file in addition to the current cellular components\n    # file to make sure we don't error with InvalidLocationError with some\n    # deprecated cellular location names\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    cc_file = os.path.join(this_dir, os.pardir, 'resources',\n                           'cellular_components.tsv')\n    cc_patch_file = os.path.join(this_dir, os.pardir, 'resources',\n                                 'cellular_components_patch.tsv')\n    cellular_components = {}\n    cellular_components_reverse = {}\n    with open(cc_file, 'rt') as fh:\n        lines = list(fh.readlines())\n    # We add the patch to the end of the lines list\n    with open(cc_patch_file, 'rt') as fh:\n        lines += list(fh.readlines())\n    for lin in lines[1:]:\n        terms = lin.strip().split('\\t')\n        cellular_components[terms[1]] = terms[0]\n        # If the GO -> name mapping doesn't exist yet, we add a mapping\n        # but if it already exists (i.e. the try doesn't error) then\n        # we don't add the GO -> name mapping. This ensures that names from\n        # the patch file aren't mapped to in the reverse list.\n        try:\n            cellular_components_reverse[terms[0]]\n        except KeyError:\n            cellular_components_reverse[terms[0]] = terms[1]\n    return cellular_components, cellular_components_reverse"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _read_amino_acids():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    aa_file = os.path.join(this_dir, os.pardir, 'resources', 'amino_acids.tsv')\n    amino_acids = {}\n    amino_acids_reverse = {}\n    with open(aa_file, 'rt') as fh:\n        lines = fh.readlines()\n    for lin in lines[1:]:\n        terms = lin.strip().split('\\t')\n        key = terms[2]\n        val = {'full_name': terms[0],\n               'short_name': terms[1],\n               'indra_name': terms[3]}\n        amino_acids[key] = val\n        for v in val.values():\n            amino_acids_reverse[v] = key\n    return amino_acids, amino_acids_reverse", "response": "Read the amino acid information from a resource file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an SBGN model string corresponding to the PySB model.", "response": "def export_sbgn(model):\n    \"\"\"Return an SBGN model string corresponding to the PySB model.\n\n    This function first calls generate_equations on the PySB model to obtain\n    a reaction network (i.e. individual species, reactions). It then iterates\n    over each reaction and and instantiates its reactants, products, and the\n    process itself as SBGN glyphs and arcs.\n\n    Parameters\n    ----------\n    model : pysb.core.Model\n        A PySB model to be exported into SBGN\n\n    Returns\n    -------\n    sbgn_str : str\n        An SBGN model as string\n    \"\"\"\n    import lxml.etree\n    import lxml.builder\n    from pysb.bng import generate_equations\n    from indra.assemblers.sbgn import SBGNAssembler\n\n    logger.info('Generating reaction network with BNG for SBGN export. ' +\n                'This could take a long time.')\n    generate_equations(model)\n\n    sa = SBGNAssembler()\n\n    glyphs = {}\n    for idx, species in enumerate(model.species):\n        glyph = sa._glyph_for_complex_pattern(species)\n        if glyph is None:\n            continue\n        sa._map.append(glyph)\n        glyphs[idx] = glyph\n    for reaction in model.reactions:\n        # Get all the reactions / products / controllers of the reaction\n        reactants = set(reaction['reactants']) - set(reaction['products'])\n        products = set(reaction['products']) - set(reaction['reactants'])\n        controllers = set(reaction['reactants']) & set(reaction['products'])\n        # Add glyph for reaction\n        process_glyph = sa._process_glyph('process')\n        # Connect reactants with arcs\n        if not reactants:\n            glyph_id = sa._none_glyph()\n            sa._arc('consumption', glyph_id, process_glyph)\n        else:\n            for r in reactants:\n                glyph = glyphs.get(r)\n                if glyph is None:\n                    glyph_id = sa._none_glyph()\n                else:\n                    glyph_id = glyph.attrib['id']\n                sa._arc('consumption', glyph_id, process_glyph)\n        # Connect products with arcs\n        if not products:\n            glyph_id = sa._none_glyph()\n            sa._arc('production', process_glyph, glyph_id)\n        else:\n            for p in products:\n                glyph = glyphs.get(p)\n                if glyph is None:\n                    glyph_id = sa._none_glyph()\n                else:\n                    glyph_id = glyph.attrib['id']\n                sa._arc('production', process_glyph, glyph_id)\n        # Connect controllers with arcs\n        for c in controllers:\n            glyph = glyphs[c]\n            sa._arc('catalysis', glyph.attrib['id'], process_glyph)\n\n    sbgn_str = sa.print_model().decode('utf-8')\n    return sbgn_str"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting a Kappa influence map from a PySB model.", "response": "def export_kappa_im(model, fname=None):\n    \"\"\"Return a networkx graph representing the model's Kappa influence map.\n\n    Parameters\n    ----------\n    model : pysb.core.Model\n        A PySB model to be exported into a Kappa IM.\n    fname : Optional[str]\n        A file name, typically with .png or .pdf extension in which\n        the IM is rendered using pygraphviz.\n\n    Returns\n    -------\n    networkx.MultiDiGraph\n        A graph object representing the influence map.\n    \"\"\"\n    from .kappa_util import im_json_to_graph\n    kappa = _prepare_kappa(model)\n    imap = kappa.analyses_influence_map()\n    im = im_json_to_graph(imap)\n    for param in model.parameters:\n        try:\n            im.remove_node(param.name)\n        except:\n            pass\n    if fname:\n        agraph = networkx.nx_agraph.to_agraph(im)\n        agraph.draw(fname, prog='dot')\n    return im"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports a Kappa CM from a PySB model.", "response": "def export_kappa_cm(model, fname=None):\n    \"\"\"Return a networkx graph representing the model's Kappa contact map.\n\n    Parameters\n    ----------\n    model : pysb.core.Model\n        A PySB model to be exported into a Kappa CM.\n    fname : Optional[str]\n        A file name, typically with .png or .pdf extension in which\n        the CM is rendered using pygraphviz.\n\n    Returns\n    -------\n    npygraphviz.Agraph\n        A graph object representing the contact map.\n    \"\"\"\n    from .kappa_util import cm_json_to_graph\n    kappa = _prepare_kappa(model)\n    cmap = kappa.analyses_contact_map()\n    cm = cm_json_to_graph(cmap)\n    if fname:\n        cm.draw(fname, prog='dot')\n    return cm"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a Kappa STD with the model loaded.", "response": "def _prepare_kappa(model):\n    \"\"\"Return a Kappa STD with the model loaded.\"\"\"\n    import kappy\n    kappa = kappy.KappaStd()\n    model_str = export(model, 'kappa')\n    kappa.add_model_string(model_str)\n    kappa.project_parse()\n    return kappa"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a request to the cBio portal and return a Pandas data frame on success.", "response": "def send_request(**kwargs):\n    \"\"\"Return a data frame from a web service request to cBio portal.\n\n    Sends a web service requrest to the cBio portal with arguments given in\n    the dictionary data and returns a Pandas data frame on success.\n\n    More information about the service here:\n    http://www.cbioportal.org/web_api.jsp\n\n    Parameters\n    ----------\n    kwargs : dict\n        A dict of parameters for the query. Entries map directly to web service\n        calls with the exception of the optional 'skiprows' entry, whose value\n        is used as the number of rows to skip when reading the result data\n        frame.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        Response from cBioPortal as a Pandas DataFrame.\n    \"\"\"\n    skiprows = kwargs.pop('skiprows', None)\n    res = requests.get(cbio_url, params=kwargs)\n    if res.status_code == 200:\n        # Adaptively skip rows based on number of comment lines\n        if skiprows == -1:\n            lines = res.text.split('\\n')\n            skiprows = 0\n            for line in lines:\n                if line.startswith('#'):\n                    skiprows += 1\n                else:\n                    break\n        csv_StringIO = StringIO(res.text)\n        df = pandas.read_csv(csv_StringIO, sep='\\t', skiprows=skiprows)\n        return df\n    else:\n        logger.error('Request returned with code %d' % res.status_code)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn mutations as a list of genes and list of amino acid changes.", "response": "def get_mutations(study_id, gene_list, mutation_type=None,\n                  case_id=None):\n    \"\"\"Return mutations as a list of genes and list of amino acid changes.\n\n    Parameters\n    ----------\n    study_id : str\n        The ID of the cBio study.\n        Example: 'cellline_ccle_broad' or 'paad_icgc'\n    gene_list : list[str]\n        A list of genes with their HGNC symbols.\n        Example: ['BRAF', 'KRAS']\n    mutation_type : Optional[str]\n        The type of mutation to filter to.\n        mutation_type can be one of: missense, nonsense, frame_shift_ins,\n        frame_shift_del, splice_site\n    case_id : Optional[str]\n        The case ID within the study to filter to.\n\n    Returns\n    -------\n    mutations : tuple[list]\n        A tuple of two lists, the first one containing a list of genes, and\n        the second one a list of amino acid changes in those genes.\n    \"\"\"\n    genetic_profile = get_genetic_profiles(study_id, 'mutation')[0]\n    gene_list_str = ','.join(gene_list)\n\n    data = {'cmd': 'getMutationData',\n            'case_set_id': study_id,\n            'genetic_profile_id': genetic_profile,\n            'gene_list': gene_list_str,\n            'skiprows': -1}\n    df = send_request(**data)\n    if case_id:\n        df = df[df['case_id'] == case_id]\n    res = _filter_data_frame(df, ['gene_symbol', 'amino_acid_change'],\n                             'mutation_type', mutation_type)\n    mutations = {'gene_symbol': list(res['gene_symbol'].values()),\n                 'amino_acid_change': list(res['amino_acid_change'].values())}\n    return mutations"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_case_lists(study_id):\n    data = {'cmd': 'getCaseLists',\n            'cancer_study_id': study_id}\n    df = send_request(**data)\n    case_set_ids = df['case_list_id'].tolist()\n    return case_set_ids", "response": "Return a list of the case set ids for a particular study."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_profile_data(study_id, gene_list,\n                     profile_filter, case_set_filter=None):\n    \"\"\"Return dict of cases and genes and their respective values.\n\n    Parameters\n    ----------\n    study_id : str\n        The ID of the cBio study.\n        Example: 'cellline_ccle_broad' or 'paad_icgc'\n    gene_list : list[str]\n        A list of genes with their HGNC symbols.\n        Example: ['BRAF', 'KRAS']\n    profile_filter : str\n        A string used to filter the profiles to return. Will be one of:\n        - MUTATION\n        - MUTATION_EXTENDED\n        - COPY_NUMBER_ALTERATION\n        - MRNA_EXPRESSION\n        - METHYLATION\n    case_set_filter : Optional[str]\n        A string that specifices which case_set_id to use, based on a complete\n        or partial match. If not provided, will look for study_id + '_all'\n\n    Returns\n    -------\n    profile_data : dict[dict[int]]\n        A dict keyed to cases containing a dict keyed to genes\n        containing int\n    \"\"\"\n    genetic_profiles = get_genetic_profiles(study_id, profile_filter)\n    if genetic_profiles:\n        genetic_profile = genetic_profiles[0]\n    else:\n        return {}\n    gene_list_str = ','.join(gene_list)\n    case_set_ids = get_case_lists(study_id)\n    if case_set_filter:\n        case_set_id = [x for x in case_set_ids if case_set_filter in x][0]\n    else:\n        case_set_id = study_id + '_all'\n        # based on looking at the cBioPortal, this is a common case_set_id\n    data = {'cmd': 'getProfileData',\n            'case_set_id': case_set_id,\n            'genetic_profile_id': genetic_profile,\n            'gene_list': gene_list_str,\n            'skiprows': -1}\n    df = send_request(**data)\n    case_list_df = [x for x in df.columns.tolist()\n                    if x not in ['GENE_ID', 'COMMON']]\n    profile_data = {case: {g: None for g in gene_list}\n                    for case in case_list_df}\n    for case in case_list_df:\n        profile_values = df[case].tolist()\n        df_gene_list = df['COMMON'].tolist()\n        for g, cv in zip(df_gene_list, profile_values):\n            if not pandas.isnull(cv):\n                profile_data[case][g] = cv\n    return profile_data", "response": "Return a dict of cases and genes and their respective values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the number of sequenced tumors for a given study.", "response": "def get_num_sequenced(study_id):\n    \"\"\"Return number of sequenced tumors for given study.\n\n    This is useful for calculating mutation statistics in terms of the\n    prevalence of certain mutations within a type of cancer.\n\n    Parameters\n    ----------\n    study_id : str\n        The ID of the cBio study.\n        Example: 'paad_icgc'\n\n    Returns\n    -------\n    num_case : int\n        The number of sequenced tumors in the given study\n    \"\"\"\n    data = {'cmd': 'getCaseLists',\n            'cancer_study_id': study_id}\n    df = send_request(**data)\n    if df.empty:\n        return 0\n    row_filter = df['case_list_id'].str.contains('sequenced', case=False)\n    num_case = len(df[row_filter]['case_ids'].tolist()[0].split(' '))\n    return num_case"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all the genetic profiles for a given study.", "response": "def get_genetic_profiles(study_id, profile_filter=None):\n    \"\"\"Return all the genetic profiles (data sets) for a given study.\n\n    Genetic profiles are different types of data for a given study. For\n    instance the study 'cellline_ccle_broad' has profiles such as\n    'cellline_ccle_broad_mutations' for mutations, 'cellline_ccle_broad_CNA'\n    for copy number alterations, etc.\n\n    Parameters\n    ----------\n    study_id : str\n        The ID of the cBio study.\n        Example: 'paad_icgc'\n    profile_filter : Optional[str]\n        A string used to filter the profiles to return.\n        Will be one of:\n        - MUTATION\n        - MUTATION_EXTENDED\n        - COPY_NUMBER_ALTERATION\n        - MRNA_EXPRESSION\n        - METHYLATION\n        The genetic profiles can include \"mutation\", \"CNA\", \"rppa\",\n        \"methylation\", etc.\n\n    Returns\n    -------\n    genetic_profiles : list[str]\n        A list of genetic profiles available  for the given study.\n    \"\"\"\n    data = {'cmd': 'getGeneticProfiles',\n            'cancer_study_id': study_id}\n    df = send_request(**data)\n    res = _filter_data_frame(df, ['genetic_profile_id'],\n                             'genetic_alteration_type', profile_filter)\n    genetic_profiles = list(res['genetic_profile_id'].values())\n    return genetic_profiles"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_cancer_studies(study_filter=None):\n    data = {'cmd': 'getCancerStudies'}\n    df = send_request(**data)\n    res = _filter_data_frame(df, ['cancer_study_id'],\n                             'cancer_study_id', study_filter)\n    study_ids = list(res['cancer_study_id'].values())\n    return study_ids", "response": "Return a list of cancer study identifiers optionally filtered."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cancer_types(cancer_filter=None):\n    data = {'cmd': 'getTypesOfCancer'}\n    df = send_request(**data)\n    res = _filter_data_frame(df, ['type_of_cancer_id'], 'name', cancer_filter)\n    type_ids = list(res['type_of_cancer_id'].values())\n    return type_ids", "response": "Return a list of cancer types optionally filtered."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ccle_mutations(gene_list, cell_lines, mutation_type=None):\n    mutations = {cl: {g: [] for g in gene_list} for cl in cell_lines}\n    for cell_line in cell_lines:\n        mutations_cl = get_mutations(ccle_study, gene_list,\n                                     mutation_type=mutation_type,\n                                     case_id=cell_line)\n        for gene, aa_change in zip(mutations_cl['gene_symbol'],\n                                   mutations_cl['amino_acid_change']):\n            aa_change = str(aa_change)\n            mutations[cell_line][gene].append(aa_change)\n    return mutations", "response": "Return a dict of mutations in given genes and cell lines from CCLE."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ccle_lines_for_mutation(gene, amino_acid_change):\n    data = {'cmd': 'getMutationData',\n            'case_set_id': ccle_study,\n            'genetic_profile_id': ccle_study + '_mutations',\n            'gene_list': gene,\n            'skiprows': 1}\n    df = send_request(**data)\n    df = df[df['amino_acid_change'] == amino_acid_change]\n    cell_lines = df['case_id'].unique().tolist()\n    return cell_lines", "response": "Returns a list of cell lines with a given point mutation in a given gene."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ccle_cna(gene_list, cell_lines):\n    profile_data = get_profile_data(ccle_study, gene_list,\n                                    'COPY_NUMBER_ALTERATION', 'all')\n    profile_data = dict((key, value) for key, value in profile_data.items()\n                        if key in cell_lines)\n    return profile_data", "response": "Returns a dict of CNAs in given genes and cell lines from CCLE."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget mRNA amounts in given genes and cell lines from CCLE.", "response": "def get_ccle_mrna(gene_list, cell_lines):\n    \"\"\"Return a dict of mRNA amounts in given genes and cell lines from CCLE.\n\n    Parameters\n    ----------\n    gene_list : list[str]\n        A list of HGNC gene symbols to get mRNA amounts for.\n    cell_lines : list[str]\n        A list of CCLE cell line names to get mRNA amounts for.\n\n    Returns\n    -------\n    mrna_amounts : dict[dict[float]]\n        A dict keyed to cell lines containing a dict keyed to genes\n        containing float\n    \"\"\"\n    gene_list_str = ','.join(gene_list)\n    data = {'cmd': 'getProfileData',\n            'case_set_id': ccle_study + '_mrna',\n            'genetic_profile_id': ccle_study + '_mrna',\n            'gene_list': gene_list_str,\n            'skiprows': -1}\n    df = send_request(**data)\n    mrna_amounts = {cl: {g: [] for g in gene_list} for cl in cell_lines}\n    for cell_line in cell_lines:\n        if cell_line in df.columns:\n            for gene in gene_list:\n                value_cell = df[cell_line][df['COMMON'] == gene]\n                if value_cell.empty:\n                    mrna_amounts[cell_line][gene] = None\n                elif pandas.isnull(value_cell.values[0]):\n                    mrna_amounts[cell_line][gene] = None\n                else:\n                    value = value_cell.values[0]\n                    mrna_amounts[cell_line][gene] = value\n        else:\n            mrna_amounts[cell_line] = None\n    return mrna_amounts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filter_data_frame(df, data_col, filter_col, filter_str=None):\n    if filter_str is not None:\n        relevant_cols = data_col + [filter_col]\n        df.dropna(inplace=True, subset=relevant_cols)\n        row_filter = df[filter_col].str.contains(filter_str, case=False)\n        data_list = df[row_filter][data_col].to_dict()\n    else:\n        data_list = df[data_col].to_dict()\n    return data_list", "response": "Return a filtered data frame as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef trips_process_text():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    text = body.get('text')\n    tp = trips.process_text(text)\n    return _stmts_from_proc(tp)", "response": "Process text with TRIPS and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trips_process_xml():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    xml_str = body.get('xml_str')\n    tp = trips.process_xml(xml_str)\n    return _stmts_from_proc(tp)", "response": "Process TRIPS EKB XML and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess text with REACH and return INDRA Statements.", "response": "def reach_process_text():\n    \"\"\"Process text with REACH and return INDRA Statements.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    text = body.get('text')\n    offline = True if body.get('offline') else False\n    rp = reach.process_text(text, offline=offline)\n    return _stmts_from_proc(rp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reach_process_json():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    json_str = body.get('json')\n    rp = reach.process_json_str(json_str)\n    return _stmts_from_proc(rp)", "response": "Process REACH json and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess PubMedCentral article and return INDRA Statements.", "response": "def reach_process_pmc():\n    \"\"\"Process PubMedCentral article and return INDRA Statements.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    pmcid = body.get('pmcid')\n    rp = reach.process_pmc(pmcid)\n    return _stmts_from_proc(rp)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bel_process_pybel_neighborhood():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    genes = body.get('genes')\n    bp = bel.process_pybel_neighborhood(genes)\n    return _stmts_from_proc(bp)", "response": "Process BEL Large Corpus neighborhood and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses BEL RDF and return INDRA Statements.", "response": "def bel_process_belrdf():\n    \"\"\"Process BEL RDF and return INDRA Statements.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    belrdf = body.get('belrdf')\n    bp = bel.process_belrdf(belrdf)\n    return _stmts_from_proc(bp)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef biopax_process_pc_pathsbetween():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    genes = body.get('genes')\n    bp = biopax.process_pc_pathsbetween(genes)\n    return _stmts_from_proc(bp)", "response": "Process PathwayCommons paths between genes return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef biopax_process_pc_pathsfromto():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    source = body.get('source')\n    target = body.get('target')\n    bp = biopax.process_pc_pathsfromto(source, target)\n    return _stmts_from_proc(bp)", "response": "Process PathwayCommons paths from - to genes return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef biopax_process_pc_neighborhood():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    genes = body.get('genes')\n    bp = biopax.process_pc_neighborhood(genes)\n    return _stmts_from_proc(bp)", "response": "Process PathwayCommons neighborhood return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef eidos_process_text():\n    if request.method == 'OPTIONS':\n        return {}\n    req = request.body.read().decode('utf-8')\n    body = json.loads(req)\n    text = body.get('text')\n    webservice = body.get('webservice')\n    if not webservice:\n        response.status = 400\n        response.content_type = 'application/json'\n        return json.dumps({'error': 'No web service address provided.'})\n    ep = eidos.process_text(text, webservice=webservice)\n    return _stmts_from_proc(ep)", "response": "Process text with EIDOS and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing an EIDOS JSON - LD and return INDRA Statements.", "response": "def eidos_process_jsonld():\n    \"\"\"Process an EIDOS JSON-LD and return INDRA Statements.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    eidos_json = body.get('jsonld')\n    ep = eidos.process_json_str(eidos_json)\n    return _stmts_from_proc(ep)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cwms_process_text():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    text = body.get('text')\n    cp = cwms.process_text(text)\n    return _stmts_from_proc(cp)", "response": "Process text with CWMS and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess Hume JSON - LD and return INDRA Statements.", "response": "def hume_process_jsonld():\n    \"\"\"Process Hume JSON-LD and return INDRA Statements.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    jsonld_str = body.get('jsonld')\n    jsonld = json.loads(jsonld_str)\n    hp = hume.process_jsonld(jsonld)\n    return _stmts_from_proc(hp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sofia_process_text():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    text = body.get('text')\n    auth = body.get('auth')\n    sp = sofia.process_text(text, auth=auth)\n    return _stmts_from_proc(sp)", "response": "Process text with Sofia and return INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassemble INDRA Statements and return PySB model string.", "response": "def assemble_pysb():\n    \"\"\"Assemble INDRA Statements and return PySB model string.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    export_format = body.get('export_format')\n    stmts = stmts_from_json(stmts_json)\n    pa = PysbAssembler()\n    pa.add_statements(stmts)\n    pa.make_model()\n    try:\n        for m in pa.model.monomers:\n            pysb_assembler.set_extended_initial_condition(pa.model, m, 0)\n    except Exception as e:\n        logger.exception(e)\n\n    if not export_format:\n        model_str = pa.print_model()\n    elif export_format in ('kappa_im', 'kappa_cm'):\n        fname = 'model_%s.png' % export_format\n        root = os.path.dirname(os.path.abspath(fname))\n        graph = pa.export_model(format=export_format, file_name=fname)\n        with open(fname, 'rb') as fh:\n            data = 'data:image/png;base64,%s' % \\\n                base64.b64encode(fh.read()).decode() \n            return {'image': data}\n    else:\n        try:\n            model_str = pa.export_model(format=export_format)\n        except Exception as e:\n            logger.exception(e)\n            model_str = ''\n    res = {'model': model_str}\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef assemble_cx():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    ca = CxAssembler(stmts)\n    model_str = ca.make_model()\n    res = {'model': model_str}\n    return res", "response": "Assemble INDRA Statements and return CX network json."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload the model to NDEX", "response": "def share_model_ndex():\n    \"\"\"Upload the model to NDEX\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_str = body.get('stmts')\n    stmts_json = json.loads(stmts_str)\n    stmts = stmts_from_json(stmts_json[\"statements\"])\n    ca = CxAssembler(stmts)\n    for n, v in body.items():\n        ca.cx['networkAttributes'].append({'n': n, 'v': v, 'd': 'string'})\n    ca.make_model()\n    network_id = ca.upload_model(private=False)\n    return {'network_id': network_id}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload model and associated pieces from NDEX", "response": "def fetch_model_ndex():\n    \"\"\"Download model and associated pieces from NDEX\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    network_id = body.get('network_id')\n    cx = process_ndex_network(network_id)\n    network_attr = [x for x in cx.cx if x.get('networkAttributes')]\n    network_attr = network_attr[0]['networkAttributes']\n    keep_keys = ['txt_input', 'parser',\n                 'model_elements', 'preset_pos', 'stmts',\n                 'sentences', 'evidence', 'cell_line', 'mrna', 'mutations']\n    stored_data = {}\n    for d in network_attr:\n        if d['n'] in keep_keys:\n            stored_data[d['n']] = d['v']\n    return stored_data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assemble_graph():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    ga = GraphAssembler(stmts)\n    model_str = ga.make_model()\n    res = {'model': model_str}\n    return res", "response": "Assemble INDRA Statements and return Graphviz graph dot string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assemble_cyjs():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    cja = CyJSAssembler()\n    cja.add_statements(stmts)\n    cja.make_model(grouping=True)\n    model_str = cja.print_cyjs_graph()\n    return model_str", "response": "Assemble INDRA Statements and return Cytoscape JS network."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassembles each statement into a list of sentences.", "response": "def assemble_english():\n    \"\"\"Assemble each statement into \"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    sentences = {}\n    for st in stmts:\n        enga = EnglishAssembler()\n        enga.add_statements([st])\n        model_str = enga.make_model()\n        sentences[st.uuid] = model_str\n    res = {'sentences': sentences}\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assemble_loopy():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    sa = SifAssembler(stmts)\n    sa.make_model(use_name_as_key=True)\n    model_str = sa.print_loopy(as_url=True)\n    res = {'loopy_url': model_str}\n    return res", "response": "Assemble INDRA Statements into a Loopy model using SIF Assembler."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_ccle_mrna_levels():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    gene_list = body.get('gene_list')\n    cell_lines = body.get('cell_lines')\n    mrna_amounts = cbio_client.get_ccle_mrna(gene_list, cell_lines)\n    res = {'mrna_amounts': mrna_amounts}\n    return res", "response": "Get CCLE mRNA amounts using cBioClient"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget CCLE CNA from CNA", "response": "def get_ccle_cna():\n    \"\"\"Get CCLE CNA\n    -2 = homozygous deletion\n    -1 = hemizygous deletion\n     0 = neutral / no change\n     1 = gain\n     2 = high level amplification\n    \"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    gene_list = body.get('gene_list')\n    cell_lines = body.get('cell_lines')\n    cna = cbio_client.get_ccle_cna(gene_list, cell_lines)\n    res = {'cna': cna}\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets CCLE mutations for a given list of genes and cell lines", "response": "def get_ccle_mutations():\n    \"\"\"Get CCLE mutations\n    returns the amino acid changes for a given list of genes and cell lines\n    \"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    gene_list = body.get('gene_list')\n    cell_lines = body.get('cell_lines')\n    mutations = cbio_client.get_ccle_mutations(gene_list, cell_lines)\n    res = {'mutations': mutations}\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef map_grounding():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    stmts_out = ac.map_grounding(stmts)\n    return _return_stmts(stmts_out)", "response": "Map grounding on a list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_preassembly():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    scorer = body.get('scorer')\n    return_toplevel = body.get('return_toplevel')\n    if scorer == 'wm':\n        belief_scorer = get_eidos_scorer()\n    else:\n        belief_scorer = None\n    stmts_out = ac.run_preassembly(stmts, belief_scorer=belief_scorer,\n                                   return_toplevel=return_toplevel)\n    return _return_stmts(stmts_out)", "response": "Run preassembly on a list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef map_ontologies():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmts = stmts_from_json(stmts_json)\n    om = OntologyMapper(stmts, wm_ontomap, scored=True, symmetric=False)\n    om.map_statements()\n    return _return_stmts(stmts)", "response": "Run ontology mapping on a list of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_by_type():\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    stmt_type_str = body.get('type')\n    stmt_type_str = stmt_type_str.capitalize()\n    stmt_type = getattr(sys.modules[__name__], stmt_type_str)\n    stmts = stmts_from_json(stmts_json)\n    stmts_out = ac.filter_by_type(stmts, stmt_type)\n    return _return_stmts(stmts_out)", "response": "Filter INDRA Statement types."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfilter to grounded Statements only.", "response": "def filter_grounded_only():\n    \"\"\"Filter to grounded Statements only.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    score_threshold = body.get('score_threshold')\n    if score_threshold is not None:\n        score_threshold = float(score_threshold)\n    stmts = stmts_from_json(stmts_json)\n    stmts_out = ac.filter_grounded_only(stmts, score_threshold=score_threshold)\n    return _return_stmts(stmts_out)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter to beliefs above a given threshold.", "response": "def filter_belief():\n    \"\"\"Filter to beliefs above a given threshold.\"\"\"\n    if request.method == 'OPTIONS':\n        return {}\n    response = request.body.read().decode('utf-8')\n    body = json.loads(response)\n    stmts_json = body.get('statements')\n    belief_cutoff = body.get('belief_cutoff')\n    if belief_cutoff is not None:\n        belief_cutoff = float(belief_cutoff)\n    stmts = stmts_from_json(stmts_json)\n    stmts_out = ac.filter_belief(stmts, belief_cutoff)\n    return _return_stmts(stmts_out)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_git_info():\n    start_dir = abspath(curdir)\n    try:\n        chdir(dirname(abspath(__file__)))\n        re_patt_str = (r'commit\\s+(?P<commit_hash>\\w+).*?Author:\\s+'\n                       r'(?P<author_name>.*?)\\s+<(?P<author_email>.*?)>\\s+Date:\\s+'\n                       r'(?P<date>.*?)\\n\\s+(?P<commit_msg>.*?)(?:\\ndiff.*?)?$')\n        show_out = check_output(['git', 'show']).decode('ascii')\n        revp_out = check_output(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n        revp_out = revp_out.decode('ascii').strip()\n        m = re.search(re_patt_str, show_out, re.DOTALL)\n        assert m is not None, \\\n            \"Regex pattern:\\n\\n\\\"%s\\\"\\n\\n failed to match string:\\n\\n\\\"%s\\\"\" \\\n            % (re_patt_str, show_out)\n        ret_dict = m.groupdict()\n        ret_dict['branch_name'] = revp_out\n    finally:\n        chdir(start_dir)\n    return ret_dict", "response": "Get a dict with useful git info."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_version(with_git_hash=True, refresh_hash=False):\n    version = __version__\n    if with_git_hash:\n        global INDRA_GITHASH\n        if INDRA_GITHASH is None or refresh_hash:\n            with open(devnull, 'w') as nul:\n                try:\n                    ret = check_output(['git', 'rev-parse', 'HEAD'],\n                                       cwd=dirname(__file__), stderr=nul)\n                except CalledProcessError:\n                    ret = 'UNHASHED'\n            INDRA_GITHASH = ret.strip().decode('utf-8')\n        version = '%s-%s' % (version, INDRA_GITHASH)\n    return version", "response": "Get an indra version string including a git hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_upload_content(pmid, force_fulltext_lookup=False):\n    # Make sure that the PMID doesn't start with PMID so that it doesn't\n    # screw up the literature clients\n    if pmid.startswith('PMID'):\n        pmid = pmid[4:]\n    # First, check S3:\n    (ft_content_s3, ft_content_type_s3) = get_full_text(pmid)\n    # The abstract is on S3 but there is no full text; if we're not forcing\n    # fulltext lookup, then we're done\n    if ft_content_type_s3 == 'abstract' and not force_fulltext_lookup:\n        return (ft_content_s3, ft_content_type_s3)\n    # If there's nothing (even an abstract on S3), or if there's an abstract\n    # and we're forcing fulltext lookup, do the lookup\n    elif ft_content_type_s3 is None or \\\n            (ft_content_type_s3 == 'abstract' and force_fulltext_lookup) or \\\n            (ft_content_type_s3 == 'elsevier_xml' and\n                    not elsevier_client.extract_text(ft_content_s3)):\n        if ft_content_type_s3 == 'elsevier_xml':\n            logger.info('PMID%s: elsevier_xml cached on S3 is missing full '\n                        'text element, getting again.' % pmid)\n        # Try to retrieve from literature client\n        logger.info(\"PMID%s: getting content using literature client\" % pmid)\n        (ft_content, ft_content_type) = lit.get_full_text(pmid, 'pmid')\n        assert ft_content_type in ('pmc_oa_xml', 'elsevier_xml',\n                                   'abstract', None)\n        # If we tried to get the full text and didn't even get the abstract,\n        # then there was probably a problem with the web service. Try to\n        # get the abstract instead:\n        if ft_content_type is None:\n            return (None, None)\n        # If we got the abstract, and we already had the abstract on S3, then\n        # do nothing\n        elif ft_content_type == 'abstract' and ft_content_type_s3 == 'abstract':\n            logger.info(\"PMID%s: found abstract but already had it on \" \\\n                        \"S3; skipping\" % pmid)\n            return (ft_content, ft_content_type)\n        # If we got the abstract, and we had nothing on S3, then upload\n        elif ft_content_type == 'abstract' and ft_content_type_s3 is None:\n            logger.info(\"PMID%s: found abstract, uploading to S3\" % pmid)\n            put_abstract(pmid, ft_content)\n            return (ft_content, ft_content_type)\n        # If we got elsevier_xml, but cannot get a full text element, then\n        # get and put the abstract\n        elif ft_content_type == 'elsevier_xml' and \\\n                not elsevier_client.extract_text(ft_content):\n            logger.info(\"PMID%s: Couldn't get a full text element for \"\n                        \"the elsevier_xml content; getting abstract \"\n                        % pmid)\n            abstract = pubmed_client.get_abstract(pmid)\n            # Abstract is None, so return None\n            if abstract is None:\n                logger.info(\"PMID%s: Unable to get abstract, returning None\"\n                            % pmid)\n                return (None, None)\n            # Otherwise, upload and return the abstract\n            else:\n                logger.info(\"PMID%s: Uploading and returning abstract \"\n                            % pmid)\n                put_abstract(pmid, abstract)\n                return (abstract, 'abstract')\n        # We got a viable full text\n        # (or something other than None or abstract...)\n        else:\n            logger.info(\"PMID%s: uploading and returning %s\"\n                        % (pmid, ft_content_type))\n            put_full_text(pmid, ft_content, full_text_type=ft_content_type)\n            return (ft_content, ft_content_type)\n    # Some form of full text is already on S3\n    else:\n        # TODO\n        # In future, could check for abstract even if full text is found, and\n        # upload it just to have it\n        return (ft_content_s3, ft_content_type_s3)\n    # We should always return before we get here\n    assert False", "response": "Get full text and abstract for paper and upload to S3."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _fix_evidence_text(txt):\n    txt = re.sub('[ ]?\\( xref \\)', '', txt)\n    # This is to make [ xref ] become [] to match the two readers\n    txt = re.sub('\\[ xref \\]', '[]', txt)\n    txt = re.sub('[\\(]?XREF_BIBR[\\)]?[,]?', '', txt)\n    txt = re.sub('[\\(]?XREF_FIG[\\)]?[,]?', '', txt)\n    txt = re.sub('[\\(]?XREF_SUPPLEMENT[\\)]?[,]?', '', txt)\n    txt = txt.strip()\n    return txt", "response": "Eliminate some symbols to have cleaner supporting text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassembling the CX network from the collected INDRA Statements.", "response": "def make_model(self, add_indra_json=True):\n        \"\"\"Assemble the CX network from the collected INDRA Statements.\n\n        This method assembles a CX network from the set of INDRA Statements.\n        The assembled network is set as the assembler's cx argument.\n\n        Parameters\n        ----------\n        add_indra_json : Optional[bool]\n            If True, the INDRA Statement JSON annotation is added to each\n            edge in the network. Default: True\n\n        Returns\n        -------\n        cx_str : str\n            The json serialized CX model.\n        \"\"\"\n        self.add_indra_json = add_indra_json\n        for stmt in self.statements:\n            if isinstance(stmt, Modification):\n                self._add_modification(stmt)\n            if isinstance(stmt, SelfModification):\n                self._add_self_modification(stmt)\n            elif isinstance(stmt, RegulateActivity) or \\\n                isinstance(stmt, RegulateAmount):\n                self._add_regulation(stmt)\n            elif isinstance(stmt, Complex):\n                self._add_complex(stmt)\n            elif isinstance(stmt, Gef):\n                self._add_gef(stmt)\n            elif isinstance(stmt, Gap):\n                self._add_gap(stmt)\n            elif isinstance(stmt, Influence):\n                self._add_influence(stmt)\n        network_description = ''\n        self.cx['networkAttributes'].append({'n': 'name',\n                                             'v': self.network_name})\n        self.cx['networkAttributes'].append({'n': 'description',\n                                             'v': network_description})\n        cx_str = self.print_cx()\n        return cx_str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the assembled CX network as a json string.", "response": "def print_cx(self, pretty=True):\n        \"\"\"Return the assembled CX network as a json string.\n\n        Parameters\n        ----------\n        pretty : bool\n            If True, the CX string is formatted with indentation (for human\n            viewing) otherwise no indentation is used.\n\n        Returns\n        -------\n        json_str : str\n            A json formatted string representation of the CX network.\n        \"\"\"\n        def _get_aspect_metadata(aspect):\n            count = len(self.cx.get(aspect)) if self.cx.get(aspect) else 0\n            if not count:\n                return None\n            data = {'name': aspect,\n                    'idCounter': self._id_counter,\n                    'consistencyGroup': 1,\n                    'elementCount': count}\n            return data\n        full_cx = OrderedDict()\n        full_cx['numberVerification'] = [{'longNumber': 281474976710655}]\n        aspects = ['nodes', 'edges', 'supports', 'citations', 'edgeAttributes',\n                   'edgeCitations', 'edgeSupports', 'networkAttributes',\n                   'nodeAttributes', 'cartesianLayout']\n        full_cx['metaData'] = []\n        for aspect in aspects:\n            metadata = _get_aspect_metadata(aspect)\n            if metadata:\n                full_cx['metaData'].append(metadata)\n        for k, v in self.cx.items():\n            full_cx[k] = v\n        full_cx['status'] = [{'error': '', 'success': True}]\n        full_cx = [{k: v} for k, v in full_cx.items()]\n        if pretty:\n            json_str = json.dumps(full_cx, indent=2)\n        else:\n            json_str = json.dumps(full_cx)\n        return json_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the assembled CX network in a file.", "response": "def save_model(self, file_name='model.cx'):\n        \"\"\"Save the assembled CX network in a file.\n\n        Parameters\n        ----------\n        file_name : Optional[str]\n            The name of the file to save the CX network to. Default: model.cx\n        \"\"\"\n        with open(file_name, 'wt') as fh:\n            cx_str = self.print_cx()\n            fh.write(cx_str)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload_model(self, ndex_cred=None, private=True, style='default'):\n        cx_str = self.print_cx()\n        if not ndex_cred:\n            username, password = ndex_client.get_default_ndex_cred({})\n            ndex_cred = {'user': username,\n                         'password': password}\n        network_id = ndex_client.create_network(cx_str, ndex_cred, private)\n        if network_id and style:\n            template_id = None if style == 'default' else style\n            ndex_client.set_style(network_id, ndex_cred, template_id)\n        return network_id", "response": "Uploads the assembled CX model to NDEx."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the protein expression data and mutational status as node attribute for proteins according to the given cell type.", "response": "def set_context(self, cell_type):\n        \"\"\"Set protein expression data and mutational status as node attribute\n\n        This method uses :py:mod:`indra.databases.context_client` to get\n        protein expression levels and mutational status for a given cell type\n        and set a node attribute for proteins accordingly.\n\n        Parameters\n        ----------\n        cell_type : str\n            Cell type name for which expression levels are queried.\n            The cell type name follows the CCLE database conventions.\n            Example: LOXIMVI_SKIN, BT20_BREAST\n        \"\"\"\n        node_names = [node['n'] for node in self.cx['nodes']]\n        res_expr = context_client.get_protein_expression(node_names,\n                                                         [cell_type])\n        res_mut = context_client.get_mutations(node_names,\n                                               [cell_type])\n        res_expr = res_expr.get(cell_type)\n        res_mut = res_mut.get(cell_type)\n        if not res_expr:\n            msg = 'Could not get protein expression for %s cell type.' % \\\n                  cell_type\n            logger.warning(msg)\n\n        if not res_mut:\n            msg = 'Could not get mutational status for %s cell type.' % \\\n                  cell_type\n            logger.warning(msg)\n\n        if not res_expr and not res_mut:\n            return\n\n        self.cx['networkAttributes'].append({'n': 'cellular_context',\n                                             'v': cell_type})\n        counter = 0\n        for node in self.cx['nodes']:\n            amount = res_expr.get(node['n'])\n            mut = res_mut.get(node['n'])\n            if amount is not None:\n                node_attribute = {'po': node['@id'],\n                                  'n': 'expression_amount',\n                                  'v': int(amount)}\n                self.cx['nodeAttributes'].append(node_attribute)\n            if mut is not None:\n                is_mutated = 1 if mut else 0\n                node_attribute = {'po': node['@id'],\n                                  'n': 'is_mutated',\n                                  'v': is_mutated}\n                self.cx['nodeAttributes'].append(node_attribute)\n            if mut is not None or amount is not None:\n                counter += 1\n        logger.info('Set context for %d nodes.' % counter)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_publications(gene_names, save_json_name=None):\n    if len(gene_names) != 2:\n        logger.warning('Other than 2 gene names given.')\n        return []\n    res_dict = _send_request(gene_names)\n    if not res_dict:\n        return []\n    if save_json_name is not None:\n        # The json module produces strings, not bytes, so the file should be\n        # opened in text mode\n        with open(save_json_name, 'wt') as fh:\n            json.dump(res_dict, fh, indent=1)\n    publications = _extract_publications(res_dict, gene_names)\n    return publications", "response": "Return evidence publications for interaction between the given genes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns valid PySB name.", "response": "def _n(name):\n    \"\"\"Return valid PySB name.\"\"\"\n    n = name.encode('ascii', errors='ignore').decode('ascii')\n    n = re.sub('[^A-Za-z0-9_]', '_', n)\n    n = re.sub(r'(^[0-9].*)', r'p\\1', n)\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hash_statements_dict(self):\n        res = {stmt_hash: stmts_from_json([stmt])[0]\n               for stmt_hash, stmt in self.__statement_jsons.items()}\n        return res", "response": "Return a dict of Statements keyed by hashes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmerge the results of this processor with those of another processor.", "response": "def merge_results(self, other_processor):\n        \"\"\"Merge the results of this processor with those of another.\"\"\"\n        if not isinstance(other_processor, self.__class__):\n            raise ValueError(\"Can only extend with another %s instance.\"\n                             % self.__class__.__name__)\n        self.statements.extend(other_processor.statements)\n        if other_processor.statements_sample is not None:\n            if self.statements_sample is None:\n                self.statements_sample = other_processor.statements_sample\n            else:\n                self.statements_sample.extend(other_processor.statements_sample)\n\n        self._merge_json(other_processor.__statement_jsons,\n                         other_processor.__evidence_counts)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_until_done(self, timeout=None):\n        start = datetime.now()\n        if not self.__th:\n            raise IndraDBRestResponseError(\"There is no thread waiting to \"\n                                           \"complete.\")\n        self.__th.join(timeout)\n        now = datetime.now()\n        dt = now - start\n        if self.__th.is_alive():\n            logger.warning(\"Timed out after %0.3f seconds waiting for \"\n                           \"statement load to complete.\" % dt.total_seconds())\n            ret = False\n        else:\n            logger.info(\"Waited %0.3f seconds for statements to finish loading.\"\n                        % dt.total_seconds())\n            ret = True\n        return ret", "response": "Wait until the background load to complete."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge these statement jsons with new jsons.", "response": "def _merge_json(self, stmt_json, ev_counts):\n        \"\"\"Merge these statement jsons with new jsons.\"\"\"\n        # Where there is overlap, there _should_ be agreement.\n        self.__evidence_counts.update(ev_counts)\n\n        for k, sj in stmt_json.items():\n            if k not in self.__statement_jsons:\n                self.__statement_jsons[k] = sj  # This should be most of them\n            else:\n                # This should only happen rarely.\n                for evj in sj['evidence']:\n                    self.__statement_jsons[k]['evidence'].append(evj)\n\n        if not self.__started:\n            self.statements_sample = stmts_from_json(\n                self.__statement_jsons.values())\n            self.__started = True\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses paging to get all statements requested.", "response": "def _run_queries(self, agent_strs, stmt_types, params, persist):\n        \"\"\"Use paging to get all statements requested.\"\"\"\n        self._query_over_statement_types(agent_strs, stmt_types, params)\n\n        assert len(self.__done_dict) == len(stmt_types) \\\n            or None in self.__done_dict.keys(), \\\n            \"Done dict was not initiated for all stmt_type's.\"\n\n        # Check if we want to keep going.\n        if not persist:\n            self._compile_statements()\n            return\n\n        # Get the rest of the content.\n        while not self._all_done():\n            self._query_over_statement_types(agent_strs, stmt_types, params)\n\n        # Create the actual statements.\n        self._compile_statements()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ids(search_term, **kwargs):\n    use_text_word = kwargs.pop('use_text_word', True)\n    if use_text_word:\n        search_term += '[tw]'\n    params = {'term': search_term,\n              'retmax': 100000,\n              'retstart': 0,\n              'db': 'pubmed',\n              'sort': 'pub+date'}\n    params.update(kwargs)\n    tree = send_request(pubmed_search, params)\n    if tree is None:\n        return []\n    if tree.find('ERROR') is not None:\n        logger.error(tree.find('ERROR').text)\n        return []\n    if tree.find('ErrorList') is not None:\n        for err in tree.find('ErrorList').getchildren():\n            logger.error('Error - %s: %s' % (err.tag, err.text))\n        return []\n    count = int(tree.find('Count').text)\n    id_terms = tree.findall('IdList/Id')\n    if id_terms is None:\n        return []\n    ids = [idt.text for idt in id_terms]\n    if count != len(ids):\n        logger.warning('Not all ids were retrieved for search %s;\\n'\n                       'limited at %d.' % (search_term, params['retmax']))\n    return ids", "response": "Search PubMed for paper IDs given a search term."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the number of citations in PubMed for a search query.", "response": "def get_id_count(search_term):\n    \"\"\"Get the number of citations in Pubmed for a search query.\n\n    Parameters\n    ----------\n    search_term : str\n        A term for which the PubMed search should be performed.\n\n    Returns\n    -------\n    int or None\n        The number of citations for the query, or None if the query fails.\n    \"\"\"\n    params = {'term': search_term,\n              'rettype': 'count',\n              'db': 'pubmed'}\n    tree = send_request(pubmed_search, params)\n    if tree is None:\n        return None\n    else:\n        count = tree.getchildren()[0].text\n        return int(count)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_ids_for_gene(hgnc_name, **kwargs):\n\n    # Get the HGNC ID for the HGNC name\n    hgnc_id = hgnc_client.get_hgnc_id(hgnc_name)\n    if hgnc_id is None:\n        raise ValueError('Invalid HGNC name.')\n    # Get the Entrez ID\n    entrez_id = hgnc_client.get_entrez_id(hgnc_id)\n    if entrez_id is None:\n        raise ValueError('Entrez ID not found in HGNC table.')\n    # Query the Entrez Gene database\n    params = {'db': 'gene',\n              'retmode': 'xml',\n              'id': entrez_id}\n    params.update(kwargs)\n    tree = send_request(pubmed_fetch, params)\n    if tree is None:\n        return []\n    if tree.find('ERROR') is not None:\n        logger.error(tree.find('ERROR').text)\n        return []\n    # Get all PMIDs from the XML tree\n    id_terms = tree.findall('.//PubMedId')\n    if id_terms is None:\n        return []\n    # Use a set to remove duplicate IDs\n    ids = list(set([idt.text for idt in id_terms]))\n    return ids", "response": "Get the curated set of articles for a given gene."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the XML metadata for a single article from the Pubmed database.", "response": "def get_article_xml(pubmed_id):\n    \"\"\"Get the XML metadata for a single article from the Pubmed database.\n    \"\"\"\n    if pubmed_id.upper().startswith('PMID'):\n        pubmed_id = pubmed_id[4:]\n    params = {'db': 'pubmed',\n              'retmode': 'xml',\n              'id': pubmed_id}\n    tree = send_request(pubmed_fetch, params)\n    if tree is None:\n        return None\n    article = tree.find('PubmedArticle/MedlineCitation/Article')\n    return article"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the abstract of an article in the Pubmed database.", "response": "def get_abstract(pubmed_id, prepend_title=True):\n    \"\"\"Get the abstract of an article in the Pubmed database.\"\"\"\n    article = get_article_xml(pubmed_id)\n    if article is None:\n        return None\n    return _abstract_from_article_element(article, prepend_title)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_metadata_from_xml_tree(tree, get_issns_from_nlm=False,\n                               get_abstracts=False, prepend_title=False,\n                               mesh_annotations=False):\n    \"\"\"Get metadata for an XML tree containing PubmedArticle elements.\n\n    Documentation on the XML structure can be found at:\n        - https://www.nlm.nih.gov/bsd/licensee/elements_descriptions.html\n        - https://www.nlm.nih.gov/bsd/licensee/elements_alphabetical.html\n\n    Parameters\n    ----------\n    tree : xml.etree.ElementTree\n        ElementTree containing one or more PubmedArticle elements.\n    get_issns_from_nlm : boolean\n        Look up the full list of ISSN number for the journal associated with\n        the article, which helps to match articles to CrossRef search results.\n        Defaults to False, since it slows down performance.\n    get_abstracts : boolean\n        Indicates whether to include the Pubmed abstract in the results.\n    prepend_title : boolean\n        If get_abstracts is True, specifies whether the article title should\n        be prepended to the abstract text.\n    mesh_annotations : boolean\n        If True, extract mesh annotations from the pubmed entries and include\n        in the returned data. If false, don't.\n\n    Returns\n    -------\n    dict of dicts\n        Dictionary indexed by PMID. Each value is a dict containing the\n        following fields: 'doi', 'title', 'authors', 'journal_title',\n        'journal_abbrev', 'journal_nlm_id', 'issn_list', 'page'.\n    \"\"\"\n    # Iterate over the articles and build the results dict\n    results = {}\n    pm_articles = tree.findall('./PubmedArticle')\n    for art_ix, pm_article in enumerate(pm_articles):\n        medline_citation = pm_article.find('./MedlineCitation')\n\n        article_info = _get_article_info(medline_citation,\n                                         pm_article.find('PubmedData'))\n        journal_info = _get_journal_info(medline_citation, get_issns_from_nlm)\n        context_info = _get_annotations(medline_citation)\n\n        # Build the result\n        result = {}\n        result.update(article_info)\n        result.update(journal_info)\n        result.update(context_info)\n\n        # Get the abstracts if requested\n        if get_abstracts:\n            abstract = _abstract_from_article_element(\n                medline_citation.find('Article'),\n                prepend_title=prepend_title\n                )\n            result['abstract'] = abstract\n\n        # Add to dict\n        results[article_info['pmid']] = result\n\n    return results", "response": "Get metadata for an XML tree containing PubmedArticle elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget article metadata for up to 200 PMIDs.", "response": "def get_metadata_for_ids(pmid_list, get_issns_from_nlm=False,\n                         get_abstracts=False, prepend_title=False):\n    \"\"\"Get article metadata for up to 200 PMIDs from the Pubmed database.\n\n    Parameters\n    ----------\n    pmid_list : list of PMIDs as strings\n        Can contain 1-200 PMIDs.\n    get_issns_from_nlm : boolean\n        Look up the full list of ISSN number for the journal associated with\n        the article, which helps to match articles to CrossRef search results.\n        Defaults to False, since it slows down performance.\n    get_abstracts : boolean\n        Indicates whether to include the Pubmed abstract in the results.\n    prepend_title : boolean\n        If get_abstracts is True, specifies whether the article title should\n        be prepended to the abstract text.\n\n    Returns\n    -------\n    dict of dicts\n        Dictionary indexed by PMID. Each value is a dict containing the\n        following fields: 'doi', 'title', 'authors', 'journal_title',\n        'journal_abbrev', 'journal_nlm_id', 'issn_list', 'page'.\n    \"\"\"\n    if len(pmid_list) > 200:\n        raise ValueError(\"Metadata query is limited to 200 PMIDs at a time.\")\n    params = {'db': 'pubmed',\n              'retmode': 'xml',\n              'id': pmid_list}\n    tree = send_request(pubmed_fetch, params)\n    if tree is None:\n        return None\n    return get_metadata_from_xml_tree(tree, get_issns_from_nlm, get_abstracts,\n                                      prepend_title)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_issns_for_journal(nlm_id):\n    params = {'db': 'nlmcatalog',\n              'retmode': 'xml',\n              'id': nlm_id}\n    tree = send_request(pubmed_fetch, params)\n    if tree is None:\n        return None\n    issn_list = tree.findall('.//ISSN')\n    issn_linking = tree.findall('.//ISSNLinking')\n    issns = issn_list + issn_linking\n    # No ISSNs found!\n    if not issns:\n        return None\n    else:\n        return [issn.text for issn in issns]", "response": "Get a list of the ISSN numbers for a journal given its NLM ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expand_pagination(pages):\n    # If there is no hyphen, it's a single page, and we're good to go\n    parts = pages.split('-')\n    if len(parts) == 1: # No hyphen, so no split\n        return pages\n    elif len(parts) == 2:\n        start = parts[0]\n        end = parts[1]\n        # If the end is the same number of digits as the start, then we\n        # don't change anything!\n        if len(start) == len(end):\n            return pages\n        # Otherwise, replace the last digits of start with the digits of end\n        num_end_digits = len(end)\n        new_end = start[:-num_end_digits] + end\n        return '%s-%s' % (start, new_end)\n    else: # More than one hyphen, something weird happened\n        logger.warning(\"Multiple hyphens in page number: %s\" % pages)\n        return pages", "response": "Convert a page number to long form e. g. from 456 - 7 to 456 - 457."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the set of source nodes with paths to the target.", "response": "def _find_sources_with_paths(im, target, sources, polarity):\n    \"\"\"Get the subset of source nodes with paths to the target.\n\n    Given a target, a list of sources, and a path polarity, perform a\n    breadth-first search upstream from the target to find paths to any of the\n    upstream sources.\n\n    Parameters\n    ----------\n    im : networkx.MultiDiGraph\n        Graph containing the influence map.\n    target : str\n        The node (rule name) in the influence map to start looking upstream for\n        marching sources.\n    sources : list of str\n        The nodes (rules) corresponding to the subject or upstream influence\n        being checked.\n    polarity : int\n        Required polarity of the path between source and target.\n\n    Returns\n    -------\n    generator of path\n        Yields paths as lists of nodes (rule names).  If there are no paths\n        to any of the given source nodes, the generator is empty.\n    \"\"\"\n    # First, create a list of visited nodes\n    # Adapted from\n    # http://stackoverflow.com/questions/8922060/\n    #                       how-to-trace-the-path-in-a-breadth-first-search\n    # FIXME: the sign information for the target should be associated with\n    # the observable itself\n    queue = deque([[(target, 1)]])\n    while queue:\n        # Get the first path in the queue\n        path = queue.popleft()\n        node, node_sign = path[-1]\n        # If there's only one node in the path, it's the observable we're\n        # starting from, so the path is positive\n        # if len(path) == 1:\n        #    sign = 1\n        # Because the path runs from target back to source, we have to reverse\n        # the path to calculate the overall polarity\n        #else:\n        #    sign = _path_polarity(im, reversed(path))\n        # Don't allow trivial paths consisting only of the target observable\n        if (sources is None or node in sources) and node_sign == polarity \\\n           and len(path) > 1:\n            logger.debug('Found path: %s' % str(_flip(im, path)))\n            yield tuple(path)\n        for predecessor, sign in _get_signed_predecessors(im, node, node_sign):\n            # Only add predecessors to the path if it's not already in the\n            # path--prevents loops\n            if (predecessor, sign) in path:\n                continue\n            # Otherwise, the new path is a copy of the old one plus the new\n            # predecessor\n            new_path = list(path)\n            new_path.append((predecessor, sign))\n            queue.append(new_path)\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the parameter nodes from the influence map.", "response": "def remove_im_params(model, im):\n    \"\"\"Remove parameter nodes from the influence map.\n\n    Parameters\n    ----------\n    model : pysb.core.Model\n        PySB model.\n    im : networkx.MultiDiGraph\n        Influence map.\n\n    Returns\n    -------\n    networkx.MultiDiGraph\n        Influence map with the parameter nodes removed.\n    \"\"\"\n    for param in model.parameters:\n        # If the node doesn't exist e.g., it may have already been removed),\n        # skip over the parameter without error\n        try:\n            im.remove_node(param.name)\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_sources(im, target, sources, polarity):\n    # First, create a list of visited nodes\n    # Adapted from\n    # networkx.algorithms.traversal.breadth_first_search.bfs_edges\n    visited = set([(target, 1)])\n    # Generate list of predecessor nodes with a sign updated according to the\n    # sign of the target node\n    target_tuple = (target, 1)\n    # The queue holds tuples of \"parents\" (in this case downstream nodes) and\n    # their \"children\" (in this case their upstream influencers)\n    queue = deque([(target_tuple, _get_signed_predecessors(im, target, 1), 0)])\n    while queue:\n        parent, children, path_length = queue[0]\n        try:\n            # Get the next child in the list\n            (child, sign) = next(children)\n            # Is this child one of the source nodes we're looking for? If so,\n            # yield it along with path length.\n            if (sources is None or child in sources) and sign == polarity:\n                logger.debug(\"Found path to %s from %s with desired sign %s \"\n                             \"with length %d\" %\n                             (target, child, polarity, path_length+1))\n                yield (child, sign, path_length+1)\n            # Check this child against the visited list. If we haven't visited\n            # it already (accounting for the path to the node), then add it\n            # to the queue.\n            if (child, sign) not in visited:\n                visited.add((child, sign))\n                queue.append(((child, sign),\n                              _get_signed_predecessors(im, child, sign),\n                              path_length + 1))\n        # Once we've finished iterating over the children of the current node,\n        # pop the node off and go to the next one in the queue\n        except StopIteration:\n            queue.popleft()\n    # There was no path; this will produce an empty generator\n    return", "response": "Find the subset of source nodes with paths to the target."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_signed_predecessors(im, node, polarity):\n    signed_pred_list = []\n    for pred in im.predecessors(node):\n        pred_edge = (pred, node)\n        yield (pred, _get_edge_sign(im, pred_edge) * polarity)", "response": "Get the upstream nodes along with the overall polarity of the path to the given node and the given polarity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_edge_sign(im, edge):\n    edge_data = im[edge[0]][edge[1]]\n    # Handle possible multiple edges between nodes\n    signs = list(set([v['sign'] for v in edge_data.values()\n                                  if v.get('sign')]))\n    if len(signs) > 1:\n        logger.warning(\"Edge %s has conflicting polarities; choosing \"\n                       \"positive polarity by default\" % str(edge))\n        sign = 1\n    else:\n        sign = signs[0]\n    if sign is None:\n        raise Exception('No sign attribute for edge.')\n    elif abs(sign) == 1:\n        return sign\n    else:\n        raise Exception('Unexpected edge sign: %s' % edge.attr['sign'])", "response": "Get the polarity of the influence by examining the edge sign."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_modification_to_agent(agent, mod_type, residue, position):\n    new_mod = ModCondition(mod_type, residue, position)\n    # Check if this modification already exists\n    for old_mod in agent.mods:\n        if old_mod.equals(new_mod):\n            return agent\n    new_agent = deepcopy(agent)\n    new_agent.mods.append(new_mod)\n    return new_agent", "response": "Add a modification condition to an Agent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _match_lhs(cp, rules):\n    rule_matches = []\n    for rule in rules:\n        reactant_pattern = rule.rule_expression.reactant_pattern\n        for rule_cp in reactant_pattern.complex_patterns:\n            if _cp_embeds_into(rule_cp, cp):\n                rule_matches.append(rule)\n                break\n    return rule_matches", "response": "Get rules with a left - hand side matching the given ComplexPattern."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cp_embeds_into(cp1, cp2):\n    # Check that any state in cp2 is matched in cp1\n    # If the thing we're matching to is just a monomer pattern, that makes\n    # things easier--we just need to find the corresponding monomer pattern\n    # in cp1\n    if cp1 is None or cp2 is None:\n        return False\n    cp1 = as_complex_pattern(cp1)\n    cp2 = as_complex_pattern(cp2)\n    if len(cp2.monomer_patterns) == 1:\n        mp2 = cp2.monomer_patterns[0]\n        # Iterate over the monomer patterns in cp1 and see if there is one\n        # that has the same name\n        for mp1 in cp1.monomer_patterns:\n            if _mp_embeds_into(mp1, mp2):\n                return True\n    return False", "response": "Check that any state in ComplexPattern1 is matched in ComplexPattern2.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that all conditions in MonomerPattern2 are met in MonomerPattern1.", "response": "def _mp_embeds_into(mp1, mp2):\n    \"\"\"Check that conditions in MonomerPattern2 are met in MonomerPattern1.\"\"\"\n    sc_matches = []\n    if mp1.monomer.name != mp2.monomer.name:\n        return False\n    # Check that all conditions in mp2 are met in mp1\n    for site_name, site_state in mp2.site_conditions.items():\n        if site_name not in mp1.site_conditions or \\\n           site_state != mp1.site_conditions[site_name]:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _monomer_pattern_label(mp):\n    site_strs = []\n    for site, cond in mp.site_conditions.items():\n        if isinstance(cond, tuple) or isinstance(cond, list):\n            assert len(cond) == 2\n            if cond[1] == WILD:\n                site_str = '%s_%s' % (site, cond[0])\n            else:\n                site_str = '%s_%s%s' % (site, cond[0], cond[1])\n        elif isinstance(cond, numbers.Real):\n            continue\n        else:\n            site_str = '%s_%s' % (site, cond)\n        site_strs.append(site_str)\n    return '%s_%s' % (mp.monomer.name, '_'.join(site_strs))", "response": "Return a string label for a MonomerPattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _stmt_from_rule(model, rule_name, stmts):\n    stmt_uuid = None\n    for ann in model.annotations:\n        if ann.predicate == 'from_indra_statement':\n            if ann.subject == rule_name:\n                stmt_uuid = ann.object\n                break\n    if stmt_uuid:\n        for stmt in stmts:\n            if stmt.uuid == stmt_uuid:\n                return stmt", "response": "Return the INDRA Statement corresponding to a given rule by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the influence map generated by Kappa", "response": "def generate_im(self, model):\n        \"\"\"Return a graph representing the influence map generated by Kappa\n\n        Parameters\n        ----------\n        model : pysb.Model\n            The PySB model whose influence map is to be generated\n\n        Returns\n        -------\n        graph : networkx.MultiDiGraph\n            A MultiDiGraph representing the influence map\n        \"\"\"\n        kappa = kappy.KappaStd()\n        model_str = export.export(model, 'kappa')\n        kappa.add_model_string(model_str)\n        kappa.project_parse()\n        imap = kappa.analyses_influence_map(accuracy='medium')\n        graph = im_json_to_graph(imap)\n        return graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing and save the influence map in a file.", "response": "def draw_im(self, fname):\n        \"\"\"Draw and save the influence map in a file.\n\n        Parameters\n        ----------\n        fname : str\n            The name of the file to save the influence map in.\n            The extension of the file will determine the file format,\n            typically png or pdf.\n        \"\"\"\n        im = self.get_im()\n        im_agraph = nx.nx_agraph.to_agraph(im)\n        im_agraph.draw(fname, prog='dot')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_im(self, force_update=False):\n        if self._im and not force_update:\n            return self._im\n        if not self.model:\n            raise Exception(\"Cannot get influence map if there is no model.\")\n\n        def add_obs_for_agent(agent):\n            obj_mps = list(pa.grounded_monomer_patterns(self.model, agent))\n            if not obj_mps:\n                logger.debug('No monomer patterns found in model for agent %s, '\n                            'skipping' % agent)\n                return\n            obs_list = []\n            for obj_mp in obj_mps:\n                obs_name = _monomer_pattern_label(obj_mp) + '_obs'\n                # Add the observable\n                obj_obs = Observable(obs_name, obj_mp, _export=False)\n                obs_list.append(obs_name)\n                try:\n                    self.model.add_component(obj_obs)\n                except ComponentDuplicateNameError as e:\n                    pass\n            return obs_list\n\n        # Create observables for all statements to check, and add to model\n        # Remove any existing observables in the model\n        self.model.observables = ComponentSet([])\n        for stmt in self.statements:\n            # Generate observables for Modification statements\n            if isinstance(stmt, Modification):\n                mod_condition_name = modclass_to_modtype[stmt.__class__]\n                if isinstance(stmt, RemoveModification):\n                    mod_condition_name = modtype_to_inverse[mod_condition_name]\n                # Add modification to substrate agent\n                modified_sub = _add_modification_to_agent(stmt.sub,\n                                    mod_condition_name, stmt.residue,\n                                    stmt.position)\n                obs_list = add_obs_for_agent(modified_sub)\n                # Associate this statement with this observable\n                self.stmt_to_obs[stmt] = obs_list\n            # Generate observables for Activation/Inhibition statements\n            elif isinstance(stmt, RegulateActivity):\n                regulated_obj, polarity = \\\n                        _add_activity_to_agent(stmt.obj, stmt.obj_activity,\n                                               stmt.is_activation)\n                obs_list = add_obs_for_agent(regulated_obj)\n                # Associate this statement with this observable\n                self.stmt_to_obs[stmt] = obs_list\n            elif isinstance(stmt, RegulateAmount):\n                obs_list = add_obs_for_agent(stmt.obj)\n                self.stmt_to_obs[stmt] = obs_list\n            elif isinstance(stmt, Influence):\n                obs_list = add_obs_for_agent(stmt.obj.concept)\n                self.stmt_to_obs[stmt] = obs_list\n        # Add observables for each agent\n        for ag in self.agent_obs:\n            obs_list = add_obs_for_agent(ag)\n            self.agent_to_obs[ag] = obs_list\n\n        logger.info(\"Generating influence map\")\n        self._im = self.generate_im(self.model)\n        #self._im.is_multigraph = lambda: False\n        # Now, for every rule in the model, check if there are any observables\n        # downstream; alternatively, for every observable in the model, get a\n        # list of rules.\n        # We'll need the dictionary to check if nodes are observables\n        node_attributes = nx.get_node_attributes(self._im, 'node_type')\n        for rule in self.model.rules:\n            obs_list = []\n            # Get successors of the rule node\n            for neighb in self._im.neighbors(rule.name):\n                # Check if the node is an observable\n                if node_attributes[neighb] != 'variable':\n                    continue\n                # Get the edge and check the polarity\n                edge_sign = _get_edge_sign(self._im, (rule.name, neighb))\n                obs_list.append((neighb, edge_sign))\n            self.rule_obs_dict[rule.name] = obs_list\n        return self._im", "response": "Get the influence map for the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_model(self, max_paths=1, max_path_length=5):\n        results = []\n        for stmt in self.statements:\n            result = self.check_statement(stmt, max_paths, max_path_length)\n            results.append((stmt, result))\n        return results", "response": "Check all the statements added to the ModelChecker."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck a single Statement against the model.", "response": "def check_statement(self, stmt, max_paths=1, max_path_length=5):\n        \"\"\"Check a single Statement against the model.\n\n        Parameters\n        ----------\n        stmt : indra.statements.Statement\n            The Statement to check.\n        max_paths : Optional[int]\n            The maximum number of specific paths to return for each Statement\n            to be explained. Default: 1\n        max_path_length : Optional[int]\n            The maximum length of specific paths to return. Default: 5\n\n        Returns\n        -------\n        boolean\n            True if the model satisfies the Statement.\n        \"\"\"\n        # Make sure the influence map is initialized\n        self.get_im()\n        # Check if this is one of the statement types that we can check\n        if not isinstance(stmt, (Modification, RegulateAmount,\n                                 RegulateActivity, Influence)):\n            return PathResult(False, 'STATEMENT_TYPE_NOT_HANDLED',\n                              max_paths, max_path_length)\n        # Get the polarity for the statement\n        if isinstance(stmt, Modification):\n            target_polarity = -1 if isinstance(stmt, RemoveModification) else 1\n        elif isinstance(stmt, RegulateActivity):\n            target_polarity = 1 if stmt.is_activation else -1\n        elif isinstance(stmt, RegulateAmount):\n            target_polarity = -1 if isinstance(stmt, DecreaseAmount) else 1\n        elif isinstance(stmt, Influence):\n            target_polarity = -1 if stmt.overall_polarity() == -1 else 1\n        # Get the subject and object (works also for Modifications)\n        subj, obj = stmt.agent_list()\n        # Get a list of monomer patterns matching the subject FIXME Currently\n        # this will match rules with the corresponding monomer pattern on it.\n        # In future, this statement should (possibly) also match rules in which\n        # 1) the agent is in its active form, or 2) the agent is tagged as the\n        # enzyme in a rule of the appropriate activity (e.g., a phosphorylation\n        # rule) FIXME\n        if subj is not None:\n            subj_mps = list(pa.grounded_monomer_patterns(self.model, subj,\n                                                    ignore_activities=True))\n            if not subj_mps:\n                logger.debug('No monomers found corresponding to agent %s' %\n                             subj)\n                return PathResult(False, 'SUBJECT_MONOMERS_NOT_FOUND',\n                                  max_paths, max_path_length)\n        else:\n            subj_mps = [None]\n        # Observables may not be found for an activation since there may be no\n        # rule in the model activating the object, and the object may not have\n        # an \"active\" site of the appropriate type\n        obs_names = self.stmt_to_obs[stmt]\n        if not obs_names:\n            logger.debug(\"No observables for stmt %s, returning False\" % stmt)\n            return PathResult(False, 'OBSERVABLES_NOT_FOUND',\n                              max_paths, max_path_length)\n        for subj_mp, obs_name in itertools.product(subj_mps, obs_names):\n            # NOTE: Returns on the path found for the first enz_mp/obs combo\n            result = self._find_im_paths(subj_mp, obs_name, target_polarity,\n                                         max_paths, max_path_length)\n            # If a path was found, then we return it; otherwise, that means\n            # there was no path for this observable, so we have to try the next\n            # one\n            if result.path_found:\n                return result\n        # If we got here, then there was no path for any observable\n        return PathResult(False, 'NO_PATHS_FOUND',\n                          max_paths, max_path_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for a source/target path in the influence map. Parameters ---------- subj_mp : pysb.MonomerPattern MonomerPattern corresponding to the subject of the Statement being checked. obs_name : str Name of the PySB model Observable corresponding to the object/target of the Statement being checked. target_polarity : int Whether the influence in the Statement is positive (1) or negative (-1). Returns ------- PathResult PathResult object indicating the results of the attempt to find a path.", "response": "def _find_im_paths(self, subj_mp, obs_name, target_polarity,\n                       max_paths=1, max_path_length=5):\n        \"\"\"Check for a source/target path in the influence map.\n\n        Parameters\n        ----------\n        subj_mp : pysb.MonomerPattern\n            MonomerPattern corresponding to the subject of the Statement\n            being checked.\n        obs_name : str\n            Name of the PySB model Observable corresponding to the\n            object/target of the Statement being checked.\n        target_polarity : int\n            Whether the influence in the Statement is positive (1) or negative\n            (-1).\n\n        Returns\n        -------\n        PathResult\n            PathResult object indicating the results of the attempt to find\n            a path.\n        \"\"\"\n        logger.info(('Running path finding with max_paths=%d,'\n                     ' max_path_length=%d') % (max_paths, max_path_length))\n        # Find rules in the model corresponding to the input\n        if subj_mp is None:\n            input_rule_set = None\n        else:\n            input_rule_set = self._get_input_rules(subj_mp)\n            if not input_rule_set:\n                return PathResult(False, 'INPUT_RULES_NOT_FOUND',\n                                  max_paths, max_path_length)\n        logger.info('Checking path metrics between %s and %s with polarity %s' %\n                    (subj_mp, obs_name, target_polarity))\n\n        # -- Route to the path sampling function --\n        if self.do_sampling:\n            if not has_pg:\n                raise Exception('The paths_graph package could not be '\n                                'imported.')\n            return self._sample_paths(input_rule_set, obs_name, target_polarity,\n                               max_paths, max_path_length)\n\n        # -- Do Breadth-First Enumeration --\n        # Generate the predecessors to our observable and count the paths\n        path_lengths = []\n        path_metrics = []\n        for source, polarity, path_length in \\\n                    _find_sources(self.get_im(), obs_name, input_rule_set,\n                                  target_polarity):\n\n            pm = PathMetric(source, obs_name, polarity, path_length)\n            path_metrics.append(pm)\n            path_lengths.append(path_length)\n        logger.info('Finding paths between %s and %s with polarity %s' %\n                    (subj_mp, obs_name, target_polarity))\n        # Now, look for paths\n        paths = []\n        if path_metrics and max_paths == 0:\n            pr = PathResult(True, 'MAX_PATHS_ZERO',\n                            max_paths, max_path_length)\n            pr.path_metrics = path_metrics\n            return pr\n        elif path_metrics:\n            if min(path_lengths) <= max_path_length:\n                pr = PathResult(True, 'PATHS_FOUND', max_paths, max_path_length)\n                pr.path_metrics = path_metrics\n                # Get the first path\n                path_iter = enumerate(_find_sources_with_paths(\n                                           self.get_im(), obs_name,\n                                           input_rule_set, target_polarity))\n                for path_ix, path in path_iter:\n                    flipped = _flip(self.get_im(), path)\n                    pr.add_path(flipped)\n                    if len(pr.paths) >= max_paths:\n                        break\n                return pr\n            # There are no paths shorter than the max path length, so we\n            # don't bother trying to get them\n            else:\n                pr = PathResult(True, 'MAX_PATH_LENGTH_EXCEEDED',\n                                max_paths, max_path_length)\n                pr.path_metrics = path_metrics\n                return pr\n        else:\n            return PathResult(False, 'NO_PATHS_FOUND',\n                              max_paths, max_path_length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score_paths(self, paths, agents_values, loss_of_function=False,\n                    sigma=0.15, include_final_node=False):\n        \"\"\"Return scores associated with a given set of paths.\n\n        Parameters\n        ----------\n        paths : list[list[tuple[str, int]]]\n            A list of paths obtained from path finding. Each path is a list\n            of tuples (which are edges in the path), with the first element\n            of the tuple the name of a rule, and the second element its\n            polarity in the path.\n        agents_values : dict[indra.statements.Agent, float]\n            A dictionary of INDRA Agents and their corresponding measured\n            value in a given experimental condition.\n        loss_of_function : Optional[boolean]\n            If True, flip the polarity of the path. For instance, if the effect\n            of an inhibitory drug is explained, set this to True.\n            Default: False\n        sigma : Optional[float]\n            The estimated standard deviation for the normally distributed\n            measurement error in the observation model used to score paths\n            with respect to data. Default: 0.15\n        include_final_node : Optional[boolean]\n            Determines whether the final node of the path is included in the\n            score. Default: False\n        \"\"\"\n        obs_model = lambda x: scipy.stats.norm(x, sigma)\n        # Build up dict mapping observables to values\n        obs_dict = {}\n        for ag, val in agents_values.items():\n            obs_list = self.agent_to_obs[ag]\n            if obs_list is not None:\n                for obs in obs_list:\n                    obs_dict[obs] = val\n        # For every path...\n        path_scores = []\n        for path in paths:\n            logger.info('------')\n            logger.info(\"Scoring path:\")\n            logger.info(path)\n            # Look at every node in the path, excluding the final\n            # observable...\n            path_score = 0\n            last_path_node_index = -1 if include_final_node else -2\n            for node, sign in path[:last_path_node_index]:\n                # ...and for each node check the sign to see if it matches the\n                # data. So the first thing is to look at what's downstream\n                # of the rule\n                # affected_obs is a list of observable names alogn\n                for affected_obs, rule_obs_sign in self.rule_obs_dict[node]:\n                    flip_polarity = -1 if loss_of_function else 1\n                    pred_sign = sign * rule_obs_sign * flip_polarity\n                    # Check to see if this observable is in the data\n                    logger.info('%s %s: effect %s %s' %\n                                (node, sign, affected_obs, pred_sign))\n                    measured_val = obs_dict.get(affected_obs)\n                    if measured_val:\n                        # For negative predictions use CDF (prob that given\n                        # measured value, true value lies below 0)\n                        if pred_sign <= 0:\n                            prob_correct = obs_model(measured_val).logcdf(0)\n                        # For positive predictions, use log survival function\n                        # (SF = 1 - CDF, i.e., prob that true value is\n                        # above 0)\n                        else:\n                            prob_correct = obs_model(measured_val).logsf(0)\n                        logger.info('Actual: %s, Log Probability: %s' %\n                                    (measured_val, prob_correct))\n                        path_score += prob_correct\n                if not self.rule_obs_dict[node]:\n                    logger.info('%s %s' % (node, sign))\n                    prob_correct = obs_model(0).logcdf(0)\n                    logger.info('Unmeasured node, Log Probability: %s' %\n                                (prob_correct))\n                    path_score += prob_correct\n            # Normalized path\n            #path_score = path_score / len(path)\n            logger.info(\"Path score: %s\" % path_score)\n            path_scores.append(path_score)\n        path_tuples = list(zip(paths, path_scores))\n        # Sort first by path length\n        sorted_by_length = sorted(path_tuples, key=lambda x: len(x[0]))\n        # Sort by probability; sort in reverse order to large values\n        # (higher probabilities) are ranked higher\n        scored_paths = sorted(sorted_by_length, key=lambda x: x[1],\n                              reverse=True)\n        return scored_paths", "response": "Return the score of a given set of paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves edges between rules causing problematic non - transititivity.", "response": "def prune_influence_map(self):\n        \"\"\"Remove edges between rules causing problematic non-transitivity.\n\n        First, all self-loops are removed. After this initial step, edges are\n        removed between rules when they share *all* child nodes except for each\n        other; that is, they have a mutual relationship with each other and\n        share all of the same children.\n\n        Note that edges must be removed in batch at the end to prevent edge\n        removal from affecting the lists of rule children during the comparison\n        process.\n        \"\"\"\n        im = self.get_im()\n\n        # First, remove all self-loops\n        logger.info('Removing self loops')\n        edges_to_remove = []\n        for e in im.edges():\n            if e[0] == e[1]:\n                logger.info('Removing self loop: %s', e)\n                edges_to_remove.append((e[0], e[1]))\n        # Now remove all the edges to be removed with a single call\n        im.remove_edges_from(edges_to_remove)\n\n        # Remove parameter nodes from influence map\n        remove_im_params(self.model, im)\n\n        # Now compare nodes pairwise and look for overlap between child nodes\n        logger.info('Get successorts of each node')\n        succ_dict = {}\n        for node in im.nodes():\n            succ_dict[node] = set(im.successors(node))\n        # Sort and then group nodes by number of successors\n        logger.info('Compare combinations of successors')\n        group_key_fun = lambda x: len(succ_dict[x])\n        nodes_sorted = sorted(im.nodes(), key=group_key_fun)\n        groups = itertools.groupby(nodes_sorted, key=group_key_fun)\n        # Now iterate over each group and then construct combinations\n        # within the group to check for shared sucessors\n        edges_to_remove = []\n        for gix, group in groups:\n            combos = itertools.combinations(group, 2)\n            for ix, (p1, p2) in enumerate(combos):\n                # Children are identical except for mutual relationship\n                if succ_dict[p1].difference(succ_dict[p2]) == set([p2]) and \\\n                   succ_dict[p2].difference(succ_dict[p1]) == set([p1]):\n                    for u, v in ((p1, p2), (p2, p1)):\n                        edges_to_remove.append((u, v))\n                        logger.debug('Will remove edge (%s, %s)', u, v)\n        logger.info('Removing %d edges from influence map' %\n                    len(edges_to_remove))\n        # Now remove all the edges to be removed with a single call\n        im.remove_edges_from(edges_to_remove)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npruning influence map to include only edges where the object of the upstream rule matches the subject of the downstream rule.", "response": "def prune_influence_map_subj_obj(self):\n        \"\"\"Prune influence map to include only edges where the object of the\n        upstream rule matches the subject of the downstream rule.\"\"\"\n        def get_rule_info(r):\n            result = {}\n            for ann in self.model.annotations:\n                if ann.subject == r:\n                    if ann.predicate == 'rule_has_subject':\n                        result['subject'] = ann.object\n                    elif ann.predicate == 'rule_has_object':\n                        result['object'] = ann.object\n            return result\n        im = self.get_im()\n        rules = im.nodes()\n        edges_to_prune = []\n        for r1, r2 in itertools.permutations(rules, 2):\n            if (r1, r2) not in im.edges():\n                continue\n            r1_info = get_rule_info(r1)\n            r2_info = get_rule_info(r2)\n            if 'object' not in r1_info or 'subject' not in r2_info:\n                continue\n            if r1_info['object'] != r2_info['subject']:\n                logger.info(\"Removing edge %s --> %s\" % (r1, r2))\n                edges_to_prune.append((r1, r2))\n        im.remove_edges_from(edges_to_prune)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_section(self, section_name):\n        self.section_headings.append(section_name)\n        if section_name in self.sections:\n            raise ValueError(\"Section %s already exists.\" % section_name)\n        self.sections[section_name] = []\n        return", "response": "Add a section to the report."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the order of the sections in the current page.", "response": "def set_section_order(self, section_name_list):\n        \"\"\"Set the order of the sections, which are by default unorderd.\n\n        Any unlisted sections that exist will be placed at the end of the\n        document in no particular order.\n        \"\"\"\n        self.section_headings = section_name_list[:]\n        for section_name in self.sections.keys():\n            if section_name not in section_name_list:\n                self.section_headings.append(section_name)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a text to the document.", "response": "def add_text(self, text, *args, **kwargs):\n        \"\"\"Add text to the document.\n\n        Text is shown on the final document in the order it is added, either\n        within the given section or as part of the un-sectioned list of content.\n\n        Parameters\n        ----------\n        text : str\n            The text to be added.\n        style : str\n            Choose the style of the text. Options include 'Normal', 'Code',\n            'Title', 'h1'. For others, see `getSampleStyleSheet` from\n            `reportlab.lib.styles`.\n        space : tuple (num spaces, font size)\n            The number and size of spaces to follow this section of text.\n            Default is (1, 12).\n        fontsize : int\n            The integer font size of the text (e.g. 12 for 12 point font).\n            Default is 12.\n        alignment : str\n            The alignment of the text. Options include 'left', 'right', and\n            'center'. Default is 'left'.\n        section : str\n            (This must be a keyword) Select a section in which to place this\n            text. Default is None, in which case the text will be simply be\n            added to a default list of text and images.\n        \"\"\"\n        # Pull down some kwargs.\n        section_name = kwargs.pop('section', None)\n\n        # Actually do the formatting.\n        para, sp = self._preformat_text(text, *args, **kwargs)\n\n        # Select the appropriate list to update\n        if section_name is None:\n            relevant_list = self.story\n        else:\n            relevant_list = self.sections[section_name]\n\n        # Add the new content to list.\n        relevant_list.append(para)\n        relevant_list.append(sp)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_image(self, image_path, width=None, height=None, section=None):\n        if width is not None:\n            width = width*inch\n        if height is not None:\n            height = height*inch\n        im = Image(image_path, width, height)\n        if section is None:\n            self.story.append(im)\n        else:\n            self.sections[section].append(im)\n        return", "response": "Add an image to the document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_report(self, sections_first=True, section_header_params=None):\n        full_story = list(self._preformat_text(self.title, style='Title',\n                                               fontsize=18, alignment='center'))\n\n        # Set the default section header parameters\n        if section_header_params is None:\n            section_header_params = {'style': 'h1', 'fontsize': 14,\n                                     'alignment': 'center'}\n\n        # Merge the sections and the rest of the story.\n        if sections_first:\n            full_story += self._make_sections(**section_header_params)\n            full_story += self.story\n        else:\n            full_story += self.story\n            full_story += self._make_sections(**section_header_params)\n\n        fname = self.name + '.pdf'\n        doc = SimpleDocTemplate(fname, pagesize=letter,\n                                rightMargin=72, leftMargin=72,\n                                topMargin=72, bottomMargin=18)\n        doc.build(full_story)\n        return fname", "response": "Create the pdf file with name self. name +. pdf."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_sections(self, **section_hdr_params):\n        sect_story = []\n        if not self.section_headings and len(self.sections):\n            self.section_headings = self.sections.keys()\n\n        for section_name in self.section_headings:\n            section_story = self.sections[section_name]\n            line = '-'*20\n            section_head_text = '%s %s %s' % (line, section_name, line)\n            title, title_sp = self._preformat_text(section_head_text,\n                                                   **section_hdr_params)\n            sect_story += [title, title_sp] + section_story\n        return sect_story", "response": "Flatten the sections into a single story list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _preformat_text(self, text, style='Normal', space=None, fontsize=12,\n                        alignment='left'):\n        \"\"\"Format the text for addition to a story list.\"\"\"\n        if space is None:\n            space=(1,12)\n        ptext = ('<para alignment=\\\"%s\\\"><font size=%d>%s</font></para>'\n                 % (alignment, fontsize, text))\n        para = Paragraph(ptext, self.styles[style])\n        sp = Spacer(*space)\n        return para, sp", "response": "Format the text for addition to a story list."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_mesh_name_from_web(mesh_id):\n    url = MESH_URL + mesh_id + '.json'\n    resp = requests.get(url)\n    if resp.status_code != 200:\n        return None\n    mesh_json = resp.json()\n    try:\n        label = mesh_json['@graph'][0]['label']['@value']\n    except (KeyError, IndexError) as e:\n        return None\n    return label", "response": "Get the MESH name for the given MESH ID using the NLM REST API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the MESH name for the given MESH ID.", "response": "def get_mesh_name(mesh_id, offline=False):\n    \"\"\"Get the MESH label for the given MESH ID.\n\n    Uses the mappings table in `indra/resources`; if the MESH ID is not listed\n    there, falls back on the NLM REST API.\n\n    Parameters\n    ----------\n    mesh_id : str\n        MESH Identifier, e.g. 'D003094'.\n    offline : bool\n        Whether to allow queries to the NLM REST API if the given MESH ID is not\n        contained in INDRA's internal MESH mappings file. Default is False\n        (allows REST API queries).\n\n    Returns\n    -------\n    str\n        Label for the MESH ID, or None if the query failed or no label was\n        found.\n    \"\"\"\n    indra_mesh_mapping = mesh_id_to_name.get(mesh_id)\n    if offline or indra_mesh_mapping is not None:\n        return indra_mesh_mapping\n    # Look up the MESH mapping from NLM if we don't have it locally\n    return get_mesh_name_from_web(mesh_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the MESH ID and name for the given MESH term.", "response": "def get_mesh_id_name(mesh_term, offline=False):\n    \"\"\"Get the MESH ID and name for the given MESH term.\n\n    Uses the mappings table in `indra/resources`; if the MESH term is not\n    listed there, falls back on the NLM REST API.\n\n    Parameters\n    ----------\n    mesh_term : str\n        MESH Descriptor or Concept name, e.g. 'Breast Cancer'.\n    offline : bool\n        Whether to allow queries to the NLM REST API if the given MESH term is\n        not contained in INDRA's internal MESH mappings file. Default is False\n        (allows REST API queries).\n\n    Returns\n    -------\n    tuple of strs\n        Returns a 2-tuple of the form `(id, name)` with the ID of the\n        descriptor corresponding to the MESH label, and the descriptor name\n        (which may not exactly match the name provided as an argument if it is\n        a Concept name). If the query failed, or no descriptor corresponding to\n        the name was found, returns a tuple of (None, None).\n    \"\"\"\n    indra_mesh_id = mesh_name_to_id.get(mesh_term)\n    if indra_mesh_id is not None:\n        return indra_mesh_id, mesh_term\n\n    indra_mesh_id, new_term = \\\n        mesh_name_to_id_name.get(mesh_term, (None, None))\n    if indra_mesh_id is not None:\n        return indra_mesh_id, new_term\n\n    if offline:\n        return None, None\n\n    # Look up the MESH mapping from NLM if we don't have it locally\n    return get_mesh_id_name_from_web(mesh_term)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the MESH ID and name for the given MESH term using the NLM REST API.", "response": "def get_mesh_id_name_from_web(mesh_term):\n    \"\"\"Get the MESH ID and name for the given MESH term using the NLM REST API.\n\n    Parameters\n    ----------\n    mesh_term : str\n        MESH Descriptor or Concept name, e.g. 'Breast Cancer'.\n\n    Returns\n    -------\n    tuple of strs\n        Returns a 2-tuple of the form `(id, name)` with the ID of the\n        descriptor corresponding to the MESH label, and the descriptor name\n        (which may not exactly match the name provided as an argument if it is\n        a Concept name). If the query failed, or no descriptor corresponding to\n        the name was found, returns a tuple of (None, None).\n    \"\"\"\n    url = MESH_URL + 'sparql'\n    query = \"\"\"\n        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n        PREFIX owl: <http://www.w3.org/2002/07/owl#>\n        PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#>\n        PREFIX mesh: <http://id.nlm.nih.gov/mesh/>\n        PREFIX mesh2019: <http://id.nlm.nih.gov/mesh/2019/>\n        PREFIX mesh2018: <http://id.nlm.nih.gov/mesh/2018/>\n        PREFIX mesh2017: <http://id.nlm.nih.gov/mesh/2017/>\n\n        SELECT ?d ?dName ?c ?cName \n        FROM <http://id.nlm.nih.gov/mesh>\n        WHERE {\n          ?d a meshv:Descriptor .\n          ?d meshv:concept ?c .\n          ?d rdfs:label ?dName .\n          ?c rdfs:label ?cName\n          FILTER (REGEX(?dName,'^%s$','i') || REGEX(?cName,'^%s$','i'))\n        }\n        ORDER BY ?d\n    \"\"\" % (mesh_term, mesh_term)\n    args = {'query': query, 'format': 'JSON', 'inference': 'true'}\n    # Interestingly, the following call using requests.get to package the\n    # query does not work:\n    # resp = requests.get(url, data=args)\n    # But if the query string is explicitly urlencoded using urllib, it works:\n    query_string = '%s?%s' % (url, urlencode(args))\n    resp = requests.get(query_string)\n    # Check status\n    if resp.status_code != 200:\n        return None, None\n\n    try:\n        # Try to parse the json response (this can raise exceptions if we\n        # got no response).\n        mesh_json = resp.json()\n\n        # Choose the first entry (should usually be only one)\n        id_uri = mesh_json['results']['bindings'][0]['d']['value']\n        name = mesh_json['results']['bindings'][0]['dName']['value']\n    except (KeyError, IndexError, json.decoder.JSONDecodeError) as e:\n        return None, None\n\n    # Strip the MESH prefix off the ID URI\n    m = re.match('http://id.nlm.nih.gov/mesh/([A-Za-z0-9]*)', id_uri)\n    assert m is not None\n    id = m.groups()[0]\n    return id, name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make(directory):\n\n    if os.path.exists(directory):\n        if os.path.isdir(directory):\n            click.echo('Directory already exists')\n        else:\n            click.echo('Path exists and is not a directory')\n        sys.exit()\n\n    os.makedirs(directory)\n    os.mkdir(os.path.join(directory, 'jsons'))\n    copy_default_config(os.path.join(directory, 'config.yaml'))", "response": "Makes a RAS Machine directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun with PubMed search for new papers.", "response": "def run_with_search(model_path, config, num_days):\n    \"\"\"Run with PubMed search for new papers.\"\"\"\n    from indra.tools.machine.machine import run_with_search_helper\n    run_with_search_helper(model_path, config, num_days=num_days)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning with given list of PMIDs.", "response": "def run_with_pmids(model_path, pmids):\n    \"\"\"Run with given list of PMIDs.\"\"\"\n    from indra.tools.machine.machine import run_with_pmids_helper\n    run_with_pmids_helper(model_path, pmids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef id_lookup(paper_id, idtype=None):\n    if idtype is not None and idtype not in ('pmid', 'pmcid', 'doi'):\n        raise ValueError(\"Invalid idtype %s; must be 'pmid', 'pmcid', \"\n                         \"or 'doi'.\" % idtype)\n    if paper_id.upper().startswith('PMC'):\n        idtype = 'pmcid'\n    # Strip off any prefix\n    if paper_id.upper().startswith('PMID'):\n        paper_id = paper_id[4:]\n    elif paper_id.upper().startswith('DOI'):\n        paper_id = paper_id[3:]\n    data = {'ids': paper_id}\n    if idtype is not None:\n        data['idtype'] = idtype\n    try:\n        tree = pubmed_client.send_request(pmid_convert_url, data)\n    except Exception as e:\n        logger.error('Error looking up PMID in PMC: %s' % e)\n        return {}\n    if tree is None:\n        return {}\n    record = tree.find('record')\n    if record is None:\n        return {}\n    doi = record.attrib.get('doi')\n    pmid = record.attrib.get('pmid')\n    pmcid = record.attrib.get('pmcid')\n    ids = {'doi': doi,\n           'pmid': pmid,\n           'pmcid': pmcid}\n    return ids", "response": "This function takes a Pubmed ID Pubmed Central ID or DOI\n    and uses the Pubmed ID mapping\n    service and returns all other IDs from one of these."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns XML for the article corresponding to a PMC ID.", "response": "def get_xml(pmc_id):\n    \"\"\"Returns XML for the article corresponding to a PMC ID.\"\"\"\n    if pmc_id.upper().startswith('PMC'):\n        pmc_id = pmc_id[3:]\n    # Request params\n    params = {}\n    params['verb'] = 'GetRecord'\n    params['identifier'] = 'oai:pubmedcentral.nih.gov:%s' % pmc_id\n    params['metadataPrefix'] = 'pmc'\n    # Submit the request\n    res = requests.get(pmc_url, params)\n    if not res.status_code == 200:\n        logger.warning(\"Couldn't download %s\" % pmc_id)\n        return None\n    # Read the bytestream\n    xml_bytes = res.content\n    # Check for any XML errors; xml_str should still be bytes\n    tree = ET.XML(xml_bytes, parser=UTB())\n    xmlns = \"http://www.openarchives.org/OAI/2.0/\"\n    err_tag = tree.find('{%s}error' % xmlns)\n    if err_tag is not None:\n        err_code = err_tag.attrib['code']\n        err_text = err_tag.text\n        logger.warning('PMC client returned with error %s: %s'\n                       % (err_code, err_text))\n        return None\n    # If no error, return the XML as a unicode string\n    else:\n        return xml_bytes.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts paragraphs from an XML string containing valid NLM XML.", "response": "def extract_paragraphs(xml_string):\n    \"\"\"Returns list of paragraphs in an NLM XML.\n\n    Parameters\n    ----------\n    xml_string : str\n        String containing valid NLM XML.\n\n    Returns\n    -------\n    list of str\n        List of extracted paragraphs in an NLM XML\n    \"\"\"\n    tree = etree.fromstring(xml_string.encode('utf-8'))\n\n    paragraphs = []\n    # In NLM xml, all plaintext is within <p> tags, and is the only thing\n    # that can be contained in <p> tags. To handle to possibility of namespaces\n    # uses regex to search for tags either of the form 'p' or '{<namespace>}p'\n    for element in tree.iter():\n        if isinstance(element.tag, basestring) and \\\n           re.search('(^|})[p|title]$', element.tag) and element.text:\n            paragraph = ' '.join(element.itertext())\n            paragraphs.append(paragraph)\n    return paragraphs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters a list of str objects with full text from PMC.", "response": "def filter_pmids(pmid_list, source_type):\n    \"\"\"Filter a list of PMIDs for ones with full text from PMC.\n\n    Parameters\n    ----------\n    pmid_list : list of str\n        List of PMIDs to filter.\n    source_type : string\n        One of 'fulltext', 'oa_xml', 'oa_txt', or 'auth_xml'.\n\n    Returns\n    -------\n    list of str\n        PMIDs available in the specified source/format type.\n    \"\"\"\n    global pmids_fulltext_dict\n    # Check args\n    if source_type not in ('fulltext', 'oa_xml', 'oa_txt', 'auth_xml'):\n        raise ValueError(\"source_type must be one of: 'fulltext', 'oa_xml', \"\n                         \"'oa_txt', or 'auth_xml'.\")\n    # Check if we've loaded this type, and lazily initialize\n    if pmids_fulltext_dict.get(source_type) is None:\n        fulltext_list_path = os.path.join(os.path.dirname(__file__),\n                                          'pmids_%s.txt' % source_type)\n        with open(fulltext_list_path, 'rb') as f:\n            fulltext_list = set([line.strip().decode('utf-8')\n                                 for line in f.readlines()])\n            pmids_fulltext_dict[source_type] = fulltext_list\n    return list(set(pmid_list).intersection(\n                                pmids_fulltext_dict.get(source_type)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets extractions from one of the examples in cag_examples.", "response": "def get_example_extractions(fname):\n    \"Get extractions from one of the examples in `cag_examples`.\"\n    with open(fname, 'r') as f:\n        sentences = f.read().splitlines()\n    rdf_xml_dict = {}\n    for sentence in sentences:\n        logger.info(\"Reading \\\"%s\\\"...\" % sentence)\n        html = tc.send_query(sentence, 'cwms')\n        try:\n            rdf_xml_dict[sentence] = tc.get_xml(html, 'rdf:RDF',\n                                                fail_if_empty=True)\n        except AssertionError as e:\n            logger.error(\"Got error for %s.\" % sentence)\n            logger.exception(e)\n    return rdf_xml_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes graphs from all the examples in cag_examples.", "response": "def make_example_graphs():\n    \"Make graphs from all the examples in cag_examples.\"\n    cag_example_rdfs = {}\n    for i, fname in enumerate(os.listdir('cag_examples')):\n        cag_example_rdfs[i+1] = get_example_extractions(fname)\n    return make_cag_graphs(cag_example_rdfs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassembles an Agent object to text.", "response": "def _assemble_agent_str(agent):\n    \"\"\"Assemble an Agent object to text.\"\"\"\n    agent_str = agent.name\n\n    # Only do the more detailed assembly for molecular agents\n    if not isinstance(agent, ist.Agent):\n        return agent_str\n\n    # Handle mutation conditions\n    if agent.mutations:\n        is_generic = False\n        mut_strs = []\n        for mut in agent.mutations:\n            res_to = mut.residue_to if mut.residue_to else ''\n            res_from = mut.residue_from if mut.residue_from else ''\n            pos = mut.position if mut.position else ''\n            mut_str = '%s%s%s' % (res_from, pos, res_to)\n            # If this is the only mutation and there are no details\n            # then this is a generic mutant\n            if not mut_str and len(agent.mutations) == 1:\n                is_generic = True\n                break\n            mut_strs.append(mut_str)\n        if is_generic:\n            agent_str = 'mutated ' + agent_str\n        else:\n            mut_strs = '/'.join(mut_strs)\n            agent_str = '%s-%s' % (agent_str, mut_strs)\n\n    # Handle location\n    if agent.location is not None:\n        agent_str += ' in the ' + agent.location\n\n    if not agent.mods and not agent.bound_conditions and not agent.activity:\n        return agent_str\n\n    # Handle bound conditions\n    bound_to = [bc.agent.name for bc in\n                agent.bound_conditions if bc.is_bound]\n    not_bound_to = [bc.agent.name for bc in\n                agent.bound_conditions if not bc.is_bound]\n    if bound_to:\n        agent_str += ' bound to ' + _join_list(bound_to)\n        if not_bound_to:\n            agent_str += ' and not bound to ' +\\\n                _join_list(not_bound_to)\n    else:\n        if not_bound_to:\n            agent_str += ' not bound to ' +\\\n                _join_list(not_bound_to)\n\n    # Handle modification conditions\n    if agent.mods:\n        # Special case\n        if len(agent.mods) == 1 and agent.mods[0].position is None:\n            prefix = _mod_state_str(agent.mods[0].mod_type)\n            if agent.mods[0].residue is not None:\n                residue_str =\\\n                    ist.amino_acids[agent.mods[0].residue]['full_name']\n                prefix = residue_str + '-' + prefix\n            agent_str =  prefix + ' ' + agent_str\n        else:\n            if agent.bound_conditions:\n                agent_str += ' and'\n            agent_str += ' %s on ' % _mod_state_str(agent.mods[0].mod_type)\n            mod_lst = []\n            for m in agent.mods:\n                if m.position is None:\n                    if m.residue is not None:\n                        residue_str =\\\n                            ist.amino_acids[m.residue]['full_name']\n                        mod_lst.append(residue_str)\n                    else:\n                        mod_lst.append('an unknown residue')\n                elif m.position is not None and m.residue is None:\n                    mod_lst.append('amino acid %s' % m.position)\n                else:\n                    mod_lst.append(m.residue + m.position)\n            agent_str += _join_list(mod_lst)\n\n\n    # Handle activity conditions\n    if agent.activity is not None:\n        # Get the modifier specific to the activity type, if any\n        pre_prefix = \\\n            activity_type_prefix.get(agent.activity.activity_type, '')\n        if agent.activity.is_active:\n            prefix = pre_prefix + 'active'\n        else:\n            # See if there is a special override for the inactive form\n            if agent.activity.activity_type in inactivity_type_prefix_override:\n                pre_prefix = inactivity_type_prefix_override[\n                    agent.activity.activity_type]\n            prefix = pre_prefix + 'inactive'\n        agent_str = prefix + ' ' + agent_str\n\n    return agent_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\njoin a list of words in a gramatically correct way.", "response": "def _join_list(lst, oxford=False):\n    \"\"\"Join a list of words in a gramatically correct way.\"\"\"\n    if len(lst) > 2:\n        s = ', '.join(lst[:-1])\n        if oxford:\n            s += ','\n        s += ' and ' + lst[-1]\n    elif len(lst) == 2:\n        s = lst[0] + ' and ' + lst[1]\n    elif len(lst) == 1:\n        s = lst[0]\n    else:\n        s = ''\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassemble ActiveForm statements into text.", "response": "def _assemble_activeform(stmt):\n    \"\"\"Assemble ActiveForm statements into text.\"\"\"\n    subj_str = _assemble_agent_str(stmt.agent)\n    if stmt.is_active:\n        is_active_str = 'active'\n    else:\n        is_active_str = 'inactive'\n    if stmt.activity == 'activity':\n        stmt_str = subj_str + ' is ' + is_active_str\n    elif stmt.activity == 'kinase':\n        stmt_str = subj_str + ' is kinase-' + is_active_str\n    elif stmt.activity == 'phosphatase':\n        stmt_str = subj_str + ' is phosphatase-' + is_active_str\n    elif stmt.activity == 'catalytic':\n        stmt_str = subj_str + ' is catalytically ' + is_active_str\n    elif stmt.activity == 'transcription':\n        stmt_str = subj_str + ' is transcriptionally ' + is_active_str\n    elif stmt.activity == 'gtpbound':\n        stmt_str = subj_str + ' is GTP-bound ' + is_active_str\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassembling Modification statements into text.", "response": "def _assemble_modification(stmt):\n    \"\"\"Assemble Modification statements into text.\"\"\"\n    sub_str = _assemble_agent_str(stmt.sub)\n    if stmt.enz is not None:\n        enz_str = _assemble_agent_str(stmt.enz)\n        if _get_is_direct(stmt):\n            mod_str = ' ' + _mod_process_verb(stmt) + ' '\n        else:\n            mod_str = ' leads to the ' + _mod_process_noun(stmt) + ' of '\n        stmt_str = enz_str + mod_str + sub_str\n    else:\n        stmt_str = sub_str + ' is ' + _mod_state_stmt(stmt)\n\n    if stmt.residue is not None:\n        if stmt.position is None:\n            mod_str = 'on ' + ist.amino_acids[stmt.residue]['full_name']\n        else:\n            mod_str = 'on ' + stmt.residue + stmt.position\n    else:\n        mod_str = ''\n    stmt_str += ' ' + mod_str\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _assemble_association(stmt):\n    member_strs = [_assemble_agent_str(m.concept) for m in stmt.members]\n    stmt_str = member_strs[0] + ' is associated with ' + \\\n        _join_list(member_strs[1:])\n    return _make_sentence(stmt_str)", "response": "Assemble Association statements into text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassembling Complex statements into text.", "response": "def _assemble_complex(stmt):\n    \"\"\"Assemble Complex statements into text.\"\"\"\n    member_strs = [_assemble_agent_str(m) for m in stmt.members]\n    stmt_str = member_strs[0] + ' binds ' + _join_list(member_strs[1:])\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _assemble_autophosphorylation(stmt):\n    enz_str = _assemble_agent_str(stmt.enz)\n    stmt_str = enz_str + ' phosphorylates itself'\n    if stmt.residue is not None:\n        if stmt.position is None:\n            mod_str = 'on ' + ist.amino_acids[stmt.residue]['full_name']\n        else:\n            mod_str = 'on ' + stmt.residue + stmt.position\n    else:\n        mod_str = ''\n    stmt_str += ' ' + mod_str\n    return _make_sentence(stmt_str)", "response": "Assemble Autophosphorylation statements into text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassembling RegulateActivity statements into text.", "response": "def _assemble_regulate_activity(stmt):\n    \"\"\"Assemble RegulateActivity statements into text.\"\"\"\n    subj_str = _assemble_agent_str(stmt.subj)\n    obj_str = _assemble_agent_str(stmt.obj)\n    if stmt.is_activation:\n        rel_str = ' activates '\n    else:\n        rel_str = ' inhibits '\n    stmt_str = subj_str + rel_str + obj_str\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassembles RegulateAmount statements into text.", "response": "def _assemble_regulate_amount(stmt):\n    \"\"\"Assemble RegulateAmount statements into text.\"\"\"\n    obj_str = _assemble_agent_str(stmt.obj)\n    if stmt.subj is not None:\n        subj_str = _assemble_agent_str(stmt.subj)\n        if isinstance(stmt, ist.IncreaseAmount):\n            rel_str = ' increases the amount of '\n        elif isinstance(stmt, ist.DecreaseAmount):\n            rel_str = ' decreases the amount of '\n        stmt_str = subj_str + rel_str + obj_str\n    else:\n        if isinstance(stmt, ist.IncreaseAmount):\n            stmt_str = obj_str + ' is produced'\n        elif isinstance(stmt, ist.DecreaseAmount):\n            stmt_str = obj_str + ' is degraded'\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _assemble_translocation(stmt):\n    agent_str = _assemble_agent_str(stmt.agent)\n    stmt_str = agent_str + ' translocates'\n    if stmt.from_location is not None:\n        stmt_str += ' from the ' + stmt.from_location\n    if stmt.to_location is not None:\n        stmt_str += ' to the ' + stmt.to_location\n    return _make_sentence(stmt_str)", "response": "Assemble Translocation statements into text."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassembles Gap statements into text.", "response": "def _assemble_gap(stmt):\n    \"\"\"Assemble Gap statements into text.\"\"\"\n    subj_str = _assemble_agent_str(stmt.gap)\n    obj_str = _assemble_agent_str(stmt.ras)\n    stmt_str = subj_str + ' is a GAP for ' + obj_str\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _assemble_gef(stmt):\n    subj_str = _assemble_agent_str(stmt.gef)\n    obj_str = _assemble_agent_str(stmt.ras)\n    stmt_str = subj_str + ' is a GEF for ' + obj_str\n    return _make_sentence(stmt_str)", "response": "Assemble Gef statements into text."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _assemble_conversion(stmt):\n    reactants = _join_list([_assemble_agent_str(r) for r in stmt.obj_from])\n    products = _join_list([_assemble_agent_str(r) for r in stmt.obj_to])\n\n    if stmt.subj is not None:\n        subj_str = _assemble_agent_str(stmt.subj)\n        stmt_str = '%s catalyzes the conversion of %s into %s' % \\\n            (subj_str, reactants, products)\n    else:\n        stmt_str = '%s is converted into %s' % (reactants, products)\n    return _make_sentence(stmt_str)", "response": "Assemble a Conversion statement into text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassemble an Influence statement into text.", "response": "def _assemble_influence(stmt):\n    \"\"\"Assemble an Influence statement into text.\"\"\"\n    subj_str = _assemble_agent_str(stmt.subj.concept)\n    obj_str = _assemble_agent_str(stmt.obj.concept)\n\n    # Note that n is prepended to increase to make it \"an increase\"\n    if stmt.subj.delta['polarity'] is not None:\n        subj_delta_str = ' decrease' if stmt.subj.delta['polarity'] == -1 \\\n            else 'n increase'\n        subj_str = 'a%s in %s' % (subj_delta_str, subj_str)\n\n    if stmt.obj.delta['polarity'] is not None:\n        obj_delta_str = ' decrease' if stmt.obj.delta['polarity'] == -1 \\\n            else 'n increase'\n        obj_str = 'a%s in %s' % (obj_delta_str, obj_str)\n\n    stmt_str = '%s causes %s' % (subj_str, obj_str)\n    return _make_sentence(stmt_str)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_sentence(txt):\n    #Make sure first letter is capitalized\n    txt = txt.strip(' ')\n    txt = txt[0].upper() + txt[1:] + '.'\n    return txt", "response": "Make a sentence from a piece of text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_is_hypothesis(stmt):\n    '''Returns true if there is evidence that the statement is only\n    hypothetical. If all of the evidences associated with the statement\n    indicate a hypothetical interaction then we assume the interaction\n    is hypothetical.'''\n    for ev in stmt.evidence:\n        if not ev.epistemics.get('hypothesis') is True:\n            return True\n    return False", "response": "Returns true if there is an evidence that the statement is only\n    hypothetical."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_model(self):\n        stmt_strs = []\n        for stmt in self.statements:\n            if isinstance(stmt, ist.Modification):\n                stmt_strs.append(_assemble_modification(stmt))\n            elif isinstance(stmt, ist.Autophosphorylation):\n                stmt_strs.append(_assemble_autophosphorylation(stmt))\n            elif isinstance(stmt, ist.Association):\n                stmt_strs.append(_assemble_association(stmt))\n            elif isinstance(stmt, ist.Complex):\n                stmt_strs.append(_assemble_complex(stmt))\n            elif isinstance(stmt, ist.Influence):\n                stmt_strs.append(_assemble_influence(stmt))\n            elif isinstance(stmt, ist.RegulateActivity):\n                stmt_strs.append(_assemble_regulate_activity(stmt))\n            elif isinstance(stmt, ist.RegulateAmount):\n                stmt_strs.append(_assemble_regulate_amount(stmt))\n            elif isinstance(stmt, ist.ActiveForm):\n                stmt_strs.append(_assemble_activeform(stmt))\n            elif isinstance(stmt, ist.Translocation):\n                stmt_strs.append(_assemble_translocation(stmt))\n            elif isinstance(stmt, ist.Gef):\n                stmt_strs.append(_assemble_gef(stmt))\n            elif isinstance(stmt, ist.Gap):\n                stmt_strs.append(_assemble_gap(stmt))\n            elif isinstance(stmt, ist.Conversion):\n                stmt_strs.append(_assemble_conversion(stmt))\n            else:\n                logger.warning('Unhandled statement type: %s.' % type(stmt))\n        if stmt_strs:\n            return ' '.join(stmt_strs)\n        else:\n            return ''", "response": "Assemble text from the set of INDRA Statements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd INDRA Statements to the assembler s list of statements.", "response": "def add_statements(self, stmts):\n        \"\"\"Add INDRA Statements to the assembler's list of statements.\n\n        Parameters\n        ----------\n        stmts : list[indra.statements.Statement]\n            A list of :py:class:`indra.statements.Statement`\n            to be added to the statement list of the assembler.\n        \"\"\"\n        for stmt in stmts:\n            if not self.statement_exists(stmt):\n                self.statements.append(stmt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassembling the SBGN model from the set of INDRA Statements.", "response": "def make_model(self):\n        \"\"\"Assemble the SBGN model from the collected INDRA Statements.\n\n        This method assembles an SBGN model from the set of INDRA Statements.\n        The assembled model is set as the assembler's sbgn attribute (it is\n        represented as an XML ElementTree internally). The model is returned\n        as a serialized XML string.\n\n        Returns\n        -------\n        sbgn_str : str\n            The XML serialized SBGN model.\n        \"\"\"\n        ppa = PysbPreassembler(self.statements)\n        ppa.replace_activities()\n        self.statements = ppa.statements\n        self.sbgn = emaker.sbgn()\n        self._map = emaker.map()\n        self.sbgn.append(self._map)\n        for stmt in self.statements:\n            if isinstance(stmt, Modification):\n                self._assemble_modification(stmt)\n            elif isinstance(stmt, RegulateActivity):\n                self._assemble_regulateactivity(stmt)\n            elif isinstance(stmt, RegulateAmount):\n                self._assemble_regulateamount(stmt)\n            elif isinstance(stmt, Complex):\n                self._assemble_complex(stmt)\n            elif isinstance(stmt, ActiveForm):\n                #self._assemble_activeform(stmt)\n                pass\n            else:\n                logger.warning(\"Unhandled Statement type %s\" % type(stmt))\n                continue\n        sbgn_str = self.print_model()\n        return sbgn_str"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_model(self, pretty=True, encoding='utf8'):\n        return lxml.etree.tostring(self.sbgn, pretty_print=pretty,\n                                   encoding=encoding, xml_declaration=True)", "response": "Return the assembled SBGN model as an XML string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_model(self, file_name='model.sbgn'):\n        model = self.print_model()\n        with open(file_name, 'wb') as fh:\n            fh.write(model)", "response": "Save the assembled SBGN model in a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _glyph_for_complex_pattern(self, pattern):\n        # Make the main glyph for the agent\n        monomer_glyphs = []\n        for monomer_pattern in pattern.monomer_patterns:\n            glyph = self._glyph_for_monomer_pattern(monomer_pattern)\n            monomer_glyphs.append(glyph)\n\n        if len(monomer_glyphs) > 1:\n            pattern.matches_key = lambda: str(pattern)\n            agent_id = self._make_agent_id(pattern)\n            complex_glyph = \\\n                emaker.glyph(emaker.bbox(**self.complex_style),\n                             class_('complex'), id=agent_id)\n            for glyph in monomer_glyphs:\n                glyph.attrib['id'] = agent_id + glyph.attrib['id']\n                complex_glyph.append(glyph)\n            return complex_glyph\n        return monomer_glyphs[0]", "response": "Add glyph and member glyphs for a PySB ComplexPattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a glyph for a PySB MonomerPattern.", "response": "def _glyph_for_monomer_pattern(self, pattern):\n        \"\"\"Add glyph for a PySB MonomerPattern.\"\"\"\n        pattern.matches_key = lambda: str(pattern)\n        agent_id = self._make_agent_id(pattern)\n        # Handle sources and sinks\n        if pattern.monomer.name in ('__source', '__sink'):\n            return None\n        # Handle molecules\n        glyph = emaker.glyph(emaker.label(text=pattern.monomer.name),\n                             emaker.bbox(**self.monomer_style),\n                             class_('macromolecule'), id=agent_id)\n        # Temporarily remove this\n        # Add a glyph for type\n        #type_glyph = emaker.glyph(emaker.label(text='mt:prot'),\n        #                          class_('unit of information'),\n        #                          emaker.bbox(**self.entity_type_style),\n        #                          id=self._make_id())\n        #glyph.append(type_glyph)\n        for site, value in pattern.site_conditions.items():\n            if value is None or isinstance(value, int):\n                continue\n            # Make some common abbreviations\n            if site == 'phospho':\n                site = 'p'\n            elif site == 'activity':\n                site = 'act'\n                if value == 'active':\n                    value = 'a'\n                elif value == 'inactive':\n                    value = 'i'\n            state = emaker.state(variable=site, value=value)\n            state_glyph = \\\n                emaker.glyph(state, emaker.bbox(**self.entity_state_style),\n                             class_('state variable'), id=self._make_id())\n            glyph.append(state_glyph)\n        return glyph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the GO data from an OWL file and parse into an RDF graph.", "response": "def load_go_graph(go_fname):\n    \"\"\"Load the GO data from an OWL file and parse into an RDF graph.\n\n    Parameters\n    ----------\n    go_fname : str\n        Path to the GO OWL file. Can be downloaded from\n        http://geneontology.org/ontology/go.owl.\n\n    Returns\n    -------\n    rdflib.Graph\n        RDF graph containing GO data.\n    \"\"\"\n    global _go_graph\n    if _go_graph is None:\n        _go_graph = rdflib.Graph()\n        logger.info(\"Parsing GO OWL file\")\n        _go_graph.parse(os.path.abspath(go_fname))\n    return _go_graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_id_mappings(g):\n    g = load_go_graph(go_owl_path)\n\n    query = _prefixes + \"\"\"\n        SELECT ?id ?label\n        WHERE {\n            ?class oboInOwl:id ?id .\n            ?class rdfs:label ?label\n        }\n    \"\"\"\n    logger.info(\"Querying for GO ID mappings\")\n    res = g.query(query)\n    mappings = []\n    for id_lit, label_lit in sorted(res, key=lambda x: x[0]):\n        mappings.append((id_lit.value, label_lit.value))\n    # Write to file\n    write_unicode_csv(go_mappings_file, mappings, delimiter='\\t')", "response": "Compile all ID - > label mappings and save to a TSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default_ndex_cred(ndex_cred):\n    if ndex_cred:\n        username = ndex_cred.get('user')\n        password = ndex_cred.get('password')\n\n        if username is not None and password is not None:\n            return username, password\n\n    username = get_config('NDEX_USERNAME')\n    password = get_config('NDEX_PASSWORD')\n\n    return username, password", "response": "Gets the NDEx credentials from the dict or tries the environment if None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_request(ndex_service_url, params, is_json=True, use_get=False):\n    if use_get:\n        res = requests.get(ndex_service_url, json=params)\n    else:\n        res = requests.post(ndex_service_url, json=params)\n    status = res.status_code\n    # If response is immediate, we get 200\n    if status == 200:\n        if is_json:\n            return res.json()\n        else:\n            return res.text\n    # If there is a continuation of the message we get status 300, handled below.\n    # Otherwise we return None.\n    elif status != 300:\n        logger.error('Request returned with code %d' % status)\n        return None\n    # In case the response is not immediate, a task ID can be used to get\n    # the result.\n    task_id = res.json().get('task_id')\n    logger.info('NDEx task submitted...')\n    time_used = 0\n    try:\n        while status != 200:\n            res = requests.get(ndex_base_url + '/task/' + task_id)\n            status = res.status_code\n            if status != 200:\n                time.sleep(5)\n                time_used += 5\n    except KeyError:\n        next\n        return None\n    logger.info('NDEx task complete.')\n    if is_json:\n        return res.json()\n    else:\n        return res.text", "response": "Send a request to the NDEx server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new NDEx network of the assembled CX model.", "response": "def create_network(cx_str, ndex_cred=None, private=True):\n    \"\"\"Creates a new NDEx network of the assembled CX model.\n\n    To upload the assembled CX model to NDEx, you need to have\n    a registered account on NDEx (http://ndexbio.org/) and have\n    the `ndex` python package installed. The uploaded network\n    is private by default.\n\n    Parameters\n    ----------\n    ndex_cred : dict\n        A dictionary with the following entries:\n        'user': NDEx user name\n        'password': NDEx password\n\n    Returns\n    -------\n    network_id :  str\n        The UUID of the NDEx network that was created by uploading\n        the assembled CX model.\n    \"\"\"\n    username, password = get_default_ndex_cred(ndex_cred)\n    nd = ndex2.client.Ndex2('http://public.ndexbio.org',\n                            username=username,\n                            password=password)\n    cx_stream = io.BytesIO(cx_str.encode('utf-8'))\n    try:\n        logger.info('Uploading network to NDEx.')\n        network_uri = nd.save_cx_stream_as_new_network(cx_stream)\n    except Exception as e:\n        logger.error('Could not upload network to NDEx.')\n        logger.error(e)\n        return\n\n    network_id = network_uri.rsplit('/')[-1]\n    if not private:\n        nd.make_network_public(network_id)\n    logger.info('The UUID for the uploaded network is: %s' % network_id)\n    logger.info('View at: http://ndexbio.org/#/network/%s' % network_id)\n    return network_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating an existing CX network on NDEx with new CX content.", "response": "def update_network(cx_str, network_id, ndex_cred=None):\n    \"\"\"Update an existing CX network on NDEx with new CX content.\n\n    Parameters\n    ----------\n    cx_str : str\n        String containing the CX content.\n    network_id : str\n        UUID of the network on NDEx.\n    ndex_cred : dict\n        A dictionary with the following entries:\n        'user': NDEx user name\n        'password': NDEx password\n    \"\"\"\n    server = 'http://public.ndexbio.org'\n    username, password = get_default_ndex_cred(ndex_cred)\n    nd = ndex2.client.Ndex2(server, username, password)\n\n    try:\n        logger.info('Getting network summary...')\n        summary = nd.get_network_summary(network_id)\n    except Exception as e:\n        logger.error('Could not get NDEx network summary.')\n        logger.error(e)\n        return\n\n    # Update network content\n    try:\n        logger.info('Updating network...')\n        cx_stream = io.BytesIO(cx_str.encode('utf-8'))\n        nd.update_cx_network(cx_stream, network_id)\n    except Exception as e:\n        logger.error('Could not update NDEx network.')\n        logger.error(e)\n        return\n\n    # Update network profile\n    ver_str = summary.get('version')\n    new_ver = _increment_ndex_ver(ver_str)\n    profile = {'name': summary.get('name'),\n               'description': summary.get('description'),\n               'version': new_ver,\n               }\n    logger.info('Updating NDEx network (%s) profile to %s',\n                network_id, profile)\n    profile_retries = 5\n    for _ in range(profile_retries):\n        try:\n            time.sleep(5)\n            nd.update_network_profile(network_id, profile)\n            break\n        except Exception as e:\n            logger.error('Could not update NDEx network profile.')\n            logger.error(e)\n\n    set_style(network_id, ndex_cred)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_style(network_id, ndex_cred=None, template_id=None):\n    if not template_id:\n        template_id = \"ea4ea3b7-6903-11e7-961c-0ac135e8bacf\"\n\n    server = 'http://public.ndexbio.org'\n    username, password = get_default_ndex_cred(ndex_cred)\n\n    source_network = ndex2.create_nice_cx_from_server(username=username,\n                                                      password=password,\n                                                      uuid=network_id,\n                                                      server=server)\n\n    source_network.apply_template(server, template_id)\n\n    source_network.update_to(network_id, server=server, username=username,\n                             password=password)", "response": "Set the style of the NDEx network to a given template network."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the model for simulation.", "response": "def initialize(self, cfg_file=None, mode=None):\n        \"\"\"Initialize the model for simulation, possibly given a config file.\n\n        Parameters\n        ----------\n        cfg_file : Optional[str]\n            The name of the configuration file to load, optional.\n        \"\"\"\n        self.sim = ScipyOdeSimulator(self.model)\n        self.state = numpy.array(copy.copy(self.sim.initials)[0])\n        self.time = numpy.array(0.0)\n        self.status = 'initialized'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsimulate the model for a given time interval.", "response": "def update(self, dt=None):\n        \"\"\"Simulate the model for a given time interval.\n\n        Parameters\n        ----------\n        dt : Optional[float]\n            The time step to simulate, if None, the default built-in time step\n            is used.\n        \"\"\"\n        # EMELI passes dt = -1 so we need to handle that here\n        dt = dt if (dt is not None and dt > 0) else self.dt\n        tspan = [0, dt]\n        # Run simulaton with initials set to current state\n        res = self.sim.run(tspan=tspan, initials=self.state)\n        # Set the state based on the result here\n        self.state  = res.species[-1]\n        self.time += dt\n        if self.time > self.stop_time:\n            self.DONE = True\n        print((self.time, self.state))\n        self.time_course.append((self.time.copy(), self.state.copy()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the value of a given variable in the model.", "response": "def set_value(self, var_name, value):\n        \"\"\"Set the value of a given variable to a given value.\n\n        Parameters\n        ----------\n        var_name : str\n            The name of the variable in the model whose value should be set.\n\n        value : float\n            The value the variable should be set to\n        \"\"\"\n        if var_name in self.outside_name_map:\n            var_name = self.outside_name_map[var_name]\n            print('%s=%.5f' % (var_name, 1e9*value))\n            if var_name == 'Precipitation':\n                value = 1e9*value\n        species_idx = self.species_name_map[var_name]\n        self.state[species_idx] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_value(self, var_name):\n        if var_name in self.outside_name_map:\n            var_name = self.outside_name_map[var_name]\n        species_idx = self.species_name_map[var_name]\n        return self.state[species_idx]", "response": "Return the value of a given variable in the current state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of variables names that can be set as input.", "response": "def get_input_var_names(self):\n        \"\"\"Return a list of variables names that can be set as input.\n\n        Returns\n        -------\n        var_names : list[str]\n            A list of variable names that can be set from the outside\n        \"\"\"\n        in_vars = copy.copy(self.input_vars)\n        for idx, var in enumerate(in_vars):\n            if self._map_in_out(var) is not None:\n                in_vars[idx] = self._map_in_out(var)\n        return in_vars"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of variables names that can be read as output.", "response": "def get_output_var_names(self):\n        \"\"\"Return a list of variables names that can be read as output.\n\n        Returns\n        -------\n        var_names : list[str]\n            A list of variable names that can be read from the outside\n        \"\"\"\n        # Return all the variables that aren't input variables\n        all_vars = list(self.species_name_map.keys())\n        output_vars = list(set(all_vars) - set(self.input_vars))\n        # Re-map to outside var names if needed\n        for idx, var in enumerate(output_vars):\n            if self._map_in_out(var) is not None:\n                output_vars[idx] = self._map_in_out(var)\n        return output_vars"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_repository_component(self):\n        component = etree.Element('component')\n\n        comp_name = etree.Element('comp_name')\n        comp_name.text = self.model.name\n        component.append(comp_name)\n\n        mod_path = etree.Element('module_path')\n        mod_path.text = os.getcwd()\n        component.append(mod_path)\n\n        mod_name = etree.Element('module_name')\n        mod_name.text = self.model.name\n        component.append(mod_name)\n\n        class_name = etree.Element('class_name')\n        class_name.text = 'model_class'\n        component.append(class_name)\n\n        model_name = etree.Element('model_name')\n        model_name.text = self.model.name\n        component.append(model_name)\n\n        lang = etree.Element('language')\n        lang.text = 'python'\n        component.append(lang)\n\n        ver = etree.Element('version')\n        ver.text = self.get_attribute('version')\n        component.append(ver)\n\n        au = etree.Element('author')\n        au.text = self.get_attribute('author_name')\n        component.append(au)\n\n        hu = etree.Element('help_url')\n        hu.text = 'http://github.com/sorgerlab/indra'\n        component.append(hu)\n\n        for tag in ('cfg_template', 'time_step_type', 'time_units',\n                    'grid_type', 'description', 'comp_type', 'uses_types'):\n            elem = etree.Element(tag)\n            elem.text = tag\n            component.append(elem)\n\n        return etree.tounicode(component, pretty_print=True)", "response": "Return an XML string representing this BMI in a workflow."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the model into a pickle and create a Python module that loads it.", "response": "def export_into_python(self):\n        \"\"\"Write the model into a pickle and create a module that loads it.\n\n        The model basically exports itself as a pickle file and a Python\n        file is then written which loads the pickle file. This allows importing\n        the model in the simulation workflow.\n        \"\"\"\n        pkl_path = self.model.name + '.pkl'\n        with open(pkl_path, 'wb') as fh:\n            pickle.dump(self, fh, protocol=2)\n        py_str = \"\"\"\n        import pickle\n        with open('%s', 'rb') as fh:\n            model_class = pickle.load(fh)\n        \"\"\" % os.path.abspath(pkl_path)\n        py_str = textwrap.dedent(py_str)\n        py_path = self.model.name + '.py'\n        with open(py_path, 'w') as fh:\n            fh.write(py_str)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _map_in_out(self, inside_var_name):\n        for out_name, in_name in self.outside_name_map.items():\n            if inside_var_name == in_name:\n                return out_name\n        return None", "response": "Return the external name of a variable mapped from inside."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\njoining different REACH output JSON files into a single JSON object.", "response": "def join_json_files(prefix):\n    \"\"\"Join different REACH output JSON files into a single JSON object.\n\n    The output of REACH is broken into three files that need to be joined\n    before processing. Specifically, there will be three files of the form:\n    `<prefix>.uaz.<subcategory>.json`.\n\n    Parameters\n    ----------\n    prefix : str\n        The absolute path up to the extensions that reach will add.\n\n    Returns\n    -------\n    json_obj : dict\n        The result of joining the files, keyed by the three subcategories.\n    \"\"\"\n    try:\n        with open(prefix + '.uaz.entities.json', 'rt') as f:\n            entities = json.load(f)\n        with open(prefix + '.uaz.events.json', 'rt') as f:\n            events = json.load(f)\n        with open(prefix + '.uaz.sentences.json', 'rt') as f:\n            sentences = json.load(f)\n    except IOError as e:\n        logger.error(\n            'Failed to open JSON files for %s; REACH error?' % prefix\n            )\n        logger.exception(e)\n        return None\n    return {'events': events, 'entities': entities, 'sentences': sentences}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_pmid(pmid, source, cont_path, sparser_version, outbuf=None,\n              cleanup=True):\n    \"Run sparser on a single pmid.\"\n    signal.signal(signal.SIGALRM, _timeout_handler)\n    signal.alarm(60)\n    try:\n        if (source is 'content_not_found'\n           or source.startswith('unhandled_content_type')\n           or source.endswith('failure')):\n            logger.info('No content read for %s.' % pmid)\n            return  # No real content here.\n\n        if cont_path.endswith('.nxml') and source.startswith('pmc'):\n            new_fname = 'PMC%s%d.nxml' % (pmid, mp.current_process().pid)\n            os.rename(cont_path, new_fname)\n\n            try:\n                sp = sparser.process_nxml_file(\n                    new_fname,\n                    outbuf=outbuf,\n                    cleanup=cleanup\n                    )\n            finally:\n                if cleanup and os.path.exists(new_fname):\n                    os.remove(new_fname)\n        elif cont_path.endswith('.txt'):\n            content_str = ''\n            with open(cont_path, 'r') as f:\n                content_str = f.read()\n            sp = sparser.process_text(\n                content_str,\n                outbuf=outbuf,\n                cleanup=cleanup\n                )\n        signal.alarm(0)\n    except Exception as e:\n        logger.error('Failed to process data for %s.' % pmid)\n        logger.exception(e)\n        signal.alarm(0)\n        return\n\n    if sp is None:\n        logger.error('Failed to run sparser on pmid: %s.' % pmid)\n        return\n    # At this point, we rewrite the PMID in the Evidence of Sparser\n    # Statements according to the actual PMID that was read.\n    sp.set_statements_pmid(pmid)\n    s3_client.put_reader_output('sparser', sp.json_stmts, pmid,\n                                sparser_version, source)\n    return sp.statements", "response": "Run sparser on a single pmid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_stmts(pmids_unread, cleanup=True, sparser_version=None):\n    \"Run sparser on the pmids in pmids_unread.\"\n    if sparser_version is None:\n        sparser_version = sparser.get_version()\n    stmts = {}\n    now = datetime.now()\n    outbuf_fname = 'sparser_%s_%s.log' % (\n        now.strftime('%Y%m%d-%H%M%S'),\n        mp.current_process().pid,\n        )\n    outbuf = open(outbuf_fname, 'wb')\n    try:\n        for pmid, result in pmids_unread.items():\n            logger.info('Reading %s' % pmid)\n            source = result['content_source']\n            cont_path = result['content_path']\n            outbuf.write(('\\nReading pmid %s from %s located at %s.\\n' % (\n                pmid,\n                source,\n                cont_path\n                )).encode('utf-8'))\n            outbuf.flush()\n            some_stmts = read_pmid(pmid, source, cont_path, sparser_version,\n                                   outbuf, cleanup)\n            if some_stmts is not None:\n                stmts[pmid] = some_stmts\n            else:\n                continue  # We didn't get any new statements.\n    except KeyboardInterrupt as e:\n        logger.exception(e)\n        logger.info('Caught keyboard interrupt...stopping. \\n'\n                    'Results so far will be pickled unless '\n                    'Keyboard interupt is hit again.')\n    finally:\n        outbuf.close()\n        print(\"Sparser logs may be found in %s\" % outbuf_fname)\n    return stmts", "response": "Run sparser on the pmids in pmids_unread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_sparser(pmid_list, tmp_dir, num_cores, start_index, end_index,\n                force_read, force_fulltext, cleanup=True, verbose=True):\n    'Run the sparser reader on the pmids in pmid_list.'\n    reader_version = sparser.get_version()\n    _, _, _, pmids_read, pmids_unread, _ =\\\n        get_content_to_read(\n            pmid_list, start_index, end_index, tmp_dir, num_cores,\n            force_fulltext, force_read, 'sparser', reader_version\n            )\n\n    logger.info('Adjusting num cores to length of pmid_list.')\n    num_cores = min(len(pmid_list), num_cores)\n    logger.info('Adjusted...')\n    if num_cores is 1:\n        stmts = get_stmts(pmids_unread, cleanup=cleanup)\n        stmts.update({pmid: get_stmts_from_cache(pmid)[pmid]\n                      for pmid in pmids_read.keys()})\n    elif num_cores > 1:\n        logger.info(\"Starting a pool with %d cores.\" % num_cores)\n        pool = mp.Pool(num_cores)\n        pmids_to_read = list(pmids_unread.keys())\n        N = len(pmids_unread)\n        dn = int(N/num_cores)\n        logger.info(\"Breaking pmids into batches.\")\n        batches = []\n        for i in range(num_cores):\n            batches.append({\n                k: pmids_unread[k]\n                for k in pmids_to_read[i*dn:min((i+1)*dn, N)]\n                })\n        get_stmts_func = functools.partial(\n            get_stmts,\n            cleanup=cleanup,\n            sparser_version=reader_version\n            )\n        logger.info(\"Mapping get_stmts onto pool.\")\n        unread_res = pool.map(get_stmts_func, batches)\n        logger.info('len(unread_res)=%d' % len(unread_res))\n        read_res = pool.map(get_stmts_from_cache, pmids_read.keys())\n        logger.info('len(read_res)=%d' % len(read_res))\n        pool.close()\n        logger.info('Multiprocessing pool closed.')\n        pool.join()\n        logger.info('Multiprocessing pool joined.')\n        stmts = {\n            pmid: stmt_list for res_dict in unread_res + read_res\n            for pmid, stmt_list in res_dict.items()\n            }\n        logger.info('len(stmts)=%d' % len(stmts))\n\n    return (stmts, pmids_unread)", "response": "Run the sparser reader on the pmids in pmid_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload all REACH files to S3.", "response": "def upload_process_reach_files(output_dir, pmid_info_dict, reader_version,\n                               num_cores):\n    # At this point, we have a directory full of JSON files\n    # Collect all the prefixes into a set, then iterate over the prefixes\n\n    # Collect prefixes\n    json_files = glob.glob(os.path.join(output_dir, '*.json'))\n    json_prefixes = set([])\n    for json_file in json_files:\n        filename = os.path.basename(json_file)\n        prefix = filename.split('.')[0]\n        json_prefixes.add(prefix)\n    # Make a list with PMID and source_text info\n    logger.info(\"Uploading reading results for reach.\")\n    pmid_json_tuples = []\n    for json_prefix in json_prefixes:\n        try:\n            full_json = upload_reach_readings(\n                json_prefix,\n                pmid_info_dict[json_prefix].get('content_source'),\n                reader_version,\n                output_dir\n                )\n            pmid_json_tuples.append((json_prefix, full_json))\n        except Exception as e:\n            logger.error(\"Caught an exception while trying to upload reach \"\n                         \"reading results onto s3 for %s.\" % json_prefix)\n            logger.exception(e)\n    # Create a multiprocessing pool\n    logger.info('Creating a multiprocessing pool with %d cores' % num_cores)\n    # Get a multiprocessing pool.\n    pool = mp.Pool(num_cores)\n    logger.info('Processing local REACH JSON files')\n    res = pool.map(upload_process_pmid, pmid_json_tuples)\n    stmts_by_pmid = {\n        pmid: stmts for res_dict in res for pmid, stmts in res_dict.items()\n        }\n    pool.close()\n    logger.info('Multiprocessing pool closed.')\n    pool.join()\n    logger.info('Multiprocessing pool joined.')\n    \"\"\"\n    logger.info('Uploaded REACH JSON for %d files to S3 (%d failures)' %\n        (num_uploaded, num_failures))\n    failures_file = os.path.join(output_dir, 'failures.txt')\n    with open(failures_file, 'wt') as f:\n        for fail in failures:\n            f.write('%s\\n' % fail)\n    \"\"\"\n    return stmts_by_pmid"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns reach on a list of PMIDs.", "response": "def run_reach(pmid_list, base_dir, num_cores, start_index, end_index,\n              force_read, force_fulltext, cleanup=False, verbose=True):\n    \"\"\"Run reach on a list of pmids.\"\"\"\n    logger.info('Running REACH with force_read=%s' % force_read)\n    logger.info('Running REACH with force_fulltext=%s' % force_fulltext)\n\n    # Get the path to the REACH JAR\n    path_to_reach = get_config('REACHPATH')\n    if path_to_reach is None or not os.path.exists(path_to_reach):\n        logger.warning(\n            'Reach path not set or invalid. Check REACHPATH environment var.'\n            )\n        return {}, {}\n\n    logger.info('Using REACH jar at: %s' % path_to_reach)\n\n    # Get the REACH version\n    reach_version = get_config('REACH_VERSION')\n    if reach_version is None:\n        logger.info('REACH version not set in REACH_VERSION')\n        m = re.match('reach-(.*?)\\.jar', os.path.basename(path_to_reach))\n        reach_version = re.sub('-SNAP.*?$', '', m.groups()[0])\n\n    logger.info('Using REACH version: %s' % reach_version)\n\n    tmp_dir, _, output_dir, pmids_read, pmids_unread, num_found =\\\n        get_content_to_read(\n            pmid_list, start_index, end_index, base_dir, num_cores,\n            force_fulltext, force_read, 'reach', reach_version\n            )\n\n    stmts = {}\n    mem_tot = get_mem_total()\n    if mem_tot is not None and mem_tot <= REACH_MEM + MEM_BUFFER:\n        logger.error(\n            \"Too little memory to run reach. At least %s required.\" %\n            REACH_MEM + MEM_BUFFER\n            )\n        logger.info(\"REACH not run.\")\n    elif len(pmids_unread) > 0 and num_found > 0:\n        # Create the REACH configuration file\n        with open(REACH_CONF_FMT_FNAME, 'r') as fmt_file:\n            conf_file_path = os.path.join(tmp_dir, 'indra.conf')\n            with open(conf_file_path, 'w') as conf_file:\n                conf_file.write(\n                    fmt_file.read().format(tmp_dir=os.path.abspath(tmp_dir),\n                                           num_cores=num_cores,\n                                           loglevel='INFO')\n                    )\n\n        # Run REACH!\n        logger.info(\"Beginning reach.\")\n        args = ['java', '-Xmx24000m', '-Dconfig.file=%s' % conf_file_path,\n                '-jar', path_to_reach]\n        p = subprocess.Popen(args, stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n        if verbose:\n            for line in iter(p.stdout.readline, b''):\n                logger.info(line)\n        p_out, p_err = p.communicate()\n        if p.returncode:\n            logger.error('Problem running REACH:')\n            logger.error('Stdout: %s' % p_out.decode('utf-8'))\n            logger.error('Stderr: %s' % p_err.decode('utf-8'))\n            raise Exception('REACH crashed')\n\n        # Process JSON files from local file system, process to INDRA\n        # Statements and upload to S3\n        some_stmts = upload_process_reach_files(\n            output_dir,\n            pmids_unread,\n            reach_version,\n            num_cores\n            )\n        stmts.update(some_stmts)\n        # Delete the tmp directory if desired\n        if cleanup:\n            shutil.rmtree(tmp_dir)\n\n    # Create a new multiprocessing pool for processing the REACH JSON\n    # files previously cached on S3\n    logger.info('Creating multiprocessing pool with %d cpus' % num_cores)\n    pool = mp.Pool(num_cores)\n\n    # Download and process the JSON files on S3\n    logger.info('Processing REACH JSON from S3 in parallel')\n    res = pool.map(process_reach_from_s3, pmids_read.keys())\n    pool.close()\n    logger.info('Multiprocessing pool closed.')\n    pool.join()\n    logger.info('Multiprocessing pool joined.')\n    s3_stmts = {\n        pmid: stmt_list for res_dict in res\n        for pmid, stmt_list in res_dict.items()\n        }\n    stmts.update(s3_stmts)\n\n    # Save the list of PMIDs with no content found on S3/literature client\n    '''\n    content_not_found_file = os.path.join(tmp_dir, 'content_not_found.txt')\n    with open(content_not_found_file, 'wt') as f:\n        for c in content_not_found:\n            f.write('%s\\n' % c)\n    '''\n    return stmts, pmids_unread"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_descendants(parent):\n    children = parent.__subclasses__()\n    descendants = children[:]\n    for child in children:\n        descendants += get_all_descendants(child)\n    return descendants", "response": "Get all the descendants of a parent class recursively."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_type_hierarchy(s):\n    tp = type(s) if not isinstance(s, type) else s\n    p_list = [tp]\n    for p in tp.__bases__:\n        if p is not Statement:\n            p_list.extend(get_type_hierarchy(p))\n        else:\n            p_list.append(p)\n    return p_list", "response": "Get the sequence of parents from s to Statement."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_statement_by_name(stmt_name):\n    stmt_classes = get_all_descendants(Statement)\n    for stmt_class in stmt_classes:\n        if stmt_class.__name__.lower() == stmt_name.lower():\n            return stmt_class\n    raise NotAStatementName('\\\"%s\\\" is not recognized as a statement type!'\n                            % stmt_name)", "response": "Get a statement class given the name of the statement class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting uuids unresolved in support from stmts from stmts_from_json.", "response": "def get_unresolved_support_uuids(stmts):\n    \"\"\"Get uuids unresolved in support from stmts from stmts_from_json.\"\"\"\n    return {s.uuid for stmt in stmts for s in stmt.supports + stmt.supported_by\n            if isinstance(s, Unresolved)}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning standardized backwards compatible object type String.", "response": "def stmt_type(obj, mk=True):\n    \"\"\"Return standardized, backwards compatible object type String.\n\n    This is a temporary solution to make sure type comparisons and\n    matches keys of Statements and related classes are backwards\n    compatible.\n    \"\"\"\n    if isinstance(obj, Statement) and mk:\n        return type(obj)\n    else:\n        return type(obj).__name__"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_hash(self, shallow=True, refresh=False):\n        if shallow:\n            if not hasattr(self, '_shallow_hash') or self._shallow_hash is None\\\n                    or refresh:\n                self._shallow_hash = make_hash(self.matches_key(), 14)\n            ret = self._shallow_hash\n        else:\n            if not hasattr(self, '_full_hash') or self._full_hash is None \\\n                    or refresh:\n                ev_mk_list = sorted([ev.matches_key() for ev in self.evidence])\n                self._full_hash = \\\n                    make_hash(self.matches_key() + str(ev_mk_list), 16)\n            ret = self._full_hash\n        return ret", "response": "Get a hash for this Statement."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _tag_evidence(self):\n        h = self.get_hash(shallow=False)\n        for ev in self.evidence:\n            ev.stmt_tag = h\n        return", "response": "Set all the Evidence stmt_tag to my deep matches - key hash."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef agent_list(self, deep_sorted=False):\n        ag_list = []\n        for ag_name in self._agent_order:\n            ag_attr = getattr(self, ag_name)\n            if isinstance(ag_attr, Concept) or ag_attr is None:\n                ag_list.append(ag_attr)\n            elif isinstance(ag_attr, list):\n                if not all([isinstance(ag, Concept) for ag in ag_attr]):\n                    raise TypeError(\"Expected all elements of list to be Agent \"\n                                    \"and/or Concept, but got: %s\"\n                                    % {type(ag) for ag in ag_attr})\n                if deep_sorted:\n                    ag_attr = sorted_agents(ag_attr)\n                ag_list.extend(ag_attr)\n            else:\n                raise TypeError(\"Expected type Agent, Concept, or list, got \"\n                                \"type %s.\" % type(ag_attr))\n        return ag_list", "response": "Get the canonicallized agent list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self, use_sbo=False):\n        stmt_type = type(self).__name__\n        # Original comment: For backwards compatibility, could be removed later\n        all_stmts = [self] + self.supports + self.supported_by\n        for st in all_stmts:\n            if not hasattr(st, 'uuid'):\n                st.uuid = '%s' % uuid.uuid4()\n        ##################\n        json_dict = _o(type=stmt_type)\n        json_dict['belief'] = self.belief\n        if self.evidence:\n            evidence = [ev.to_json() for ev in self.evidence]\n            json_dict['evidence'] = evidence\n        json_dict['id'] = '%s' % self.uuid\n        if self.supports:\n            json_dict['supports'] = \\\n                ['%s' % st.uuid for st in self.supports]\n        if self.supported_by:\n            json_dict['supported_by'] = \\\n                ['%s' % st.uuid for st in self.supported_by]\n\n        def get_sbo_term(cls):\n            sbo_term = stmt_sbo_map.get(cls.__name__.lower())\n            while not sbo_term:\n                cls = cls.__bases__[0]\n                sbo_term = stmt_sbo_map.get(cls.__name__.lower())\n            return sbo_term\n\n        if use_sbo:\n            sbo_term = get_sbo_term(self.__class__)\n            json_dict['sbo'] = \\\n                'http://identifiers.org/sbo/SBO:%s' % sbo_term\n        return json_dict", "response": "Return the INDRA Statement as a JSON dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns Statement as a networkx graph.", "response": "def to_graph(self):\n        \"\"\"Return Statement as a networkx graph.\"\"\"\n        def json_node(graph, element, prefix):\n            if not element:\n                return None\n            node_id = '|'.join(prefix)\n            if isinstance(element, list):\n                graph.add_node(node_id, label='')\n                # Enumerate children and add nodes and connect to anchor node\n                for i, sub_element in enumerate(element):\n                    sub_id = json_node(graph, sub_element, prefix + ['%s' % i])\n                    if sub_id:\n                        graph.add_edge(node_id, sub_id, label='')\n            elif isinstance(element, dict):\n                graph.add_node(node_id, label='')\n                # Add node recursively for each element\n                # Connect to this node with edge label according to key\n                for k, v in element.items():\n                    if k == 'id':\n                        continue\n                    elif k == 'name':\n                        graph.node[node_id]['label'] = v\n                        continue\n                    elif k == 'type':\n                        graph.node[node_id]['label'] = v\n                        continue\n\n                    sub_id = json_node(graph, v, prefix + ['%s' % k])\n                    if sub_id:\n                        graph.add_edge(node_id, sub_id, label=('%s' % k))\n            else:\n                if isinstance(element, basestring) and \\\n                   element.startswith('http'):\n                    element = element.split('/')[-1]\n                graph.add_node(node_id, label=('%s' % str(element)))\n            return node_id\n        jd = self.to_json()\n        graph = networkx.DiGraph()\n        json_node(graph, jd, ['%s' % self.uuid])\n        return graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a new matching Statement with no provenance.", "response": "def make_generic_copy(self, deeply=False):\n        \"\"\"Make a new matching Statement with no provenance.\n\n        All agents and other attributes besides evidence, belief, supports, and\n        supported_by will be copied over, and a new uuid will be assigned.\n        Thus, the new Statement will satisfy `new_stmt.matches(old_stmt)`.\n\n        If `deeply` is set to True, all the attributes will be deep-copied,\n        which is comparatively slow. Otherwise, attributes of this statement\n        may be altered by changes to the new matching statement.\n        \"\"\"\n        if deeply:\n            kwargs = deepcopy(self.__dict__)\n        else:\n            kwargs = self.__dict__.copy()\n        for attr in ['evidence', 'belief', 'uuid', 'supports', 'supported_by',\n                     'is_activation']:\n            kwargs.pop(attr, None)\n        for attr in ['_full_hash', '_shallow_hash']:\n            my_hash = kwargs.pop(attr, None)\n            my_shallow_hash = kwargs.pop(attr, None)\n        for attr in self._agent_order:\n            attr_value = kwargs.get(attr)\n            if isinstance(attr_value, list):\n                kwargs[attr] = sorted_agents(attr_value)\n        new_instance = self.__class__(**kwargs)\n        new_instance._full_hash = my_hash\n        new_instance._shallow_hash = my_shallow_hash\n        return new_instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_lincs_csv(url):\n    resp = requests.get(url, params={'output_type': '.csv'}, timeout=120)\n    resp.raise_for_status()\n    if sys.version_info[0] < 3:\n        csv_io = BytesIO(resp.content)\n    else:\n        csv_io = StringIO(resp.text)\n    data_rows = list(read_unicode_csv_fileobj(csv_io, delimiter=','))\n    headers = data_rows[0]\n    return [{header: val for header, val in zip(headers, line_elements)}\n            for line_elements in data_rows[1:]]", "response": "Helper function to turn csv rows into dicts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_small_molecule_name(self, hms_lincs_id):\n        entry = self._get_entry_by_id(self._sm_data, hms_lincs_id)\n        if not entry:\n            return None\n        name = entry['Name']\n        return name", "response": "Get the name of a small molecule from the LINCS sm metadata."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the id refs of a small molecule from the LINCS sm metadata.", "response": "def get_small_molecule_refs(self, hms_lincs_id):\n        \"\"\"Get the id refs of a small molecule from the LINCS sm metadata.\n\n        Parameters\n        ----------\n        hms_lincs_id : str\n            The HMS LINCS ID of the small molecule.\n\n        Returns\n        -------\n        dict\n            A dictionary of references.\n        \"\"\"\n        refs = {'HMS-LINCS': hms_lincs_id}\n\n        entry = self._get_entry_by_id(self._sm_data, hms_lincs_id)\n        # If there is no entry for this ID\n        if not entry:\n            return refs\n\n        # If there is an entry then fill up the refs with existing values\n        mappings = dict(chembl='ChEMBL ID', chebi='ChEBI ID',\n                        pubchem='PubChem CID', lincs='LINCS ID')\n        for k, v in mappings.items():\n            if entry.get(v):\n                refs[k.upper()] = entry.get(v)\n        return refs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_protein_refs(self, hms_lincs_id):\n        # TODO: We could get phosphorylation states from the protein data.\n        refs = {'HMS-LINCS': hms_lincs_id}\n\n        entry = self._get_entry_by_id(self._prot_data, hms_lincs_id)\n        # If there is no entry for this ID\n        if not entry:\n            return refs\n        mappings = dict(egid='Gene ID', up='UniProt ID')\n        for k, v in mappings.items():\n            if entry.get(v):\n                refs[k.upper()] = entry.get(v)\n        return refs", "response": "Get the refs for a protein from the LINCs protein metadata."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_bel_stmts(self, filter=False):\n        if self.basename is not None:\n            bel_stmt_path = '%s_bel_stmts.pkl' % self.basename\n        # Check for cached BEL stmt file\n        if self.basename is not None and os.path.isfile(bel_stmt_path):\n            logger.info(\"Loading BEL statements from %s\" % bel_stmt_path)\n            with open(bel_stmt_path, 'rb') as f:\n                bel_statements = pickle.load(f)\n        # No cache, so perform the queries\n        else:\n            bel_proc = bel.process_pybel_neighborhood(self.gene_list,\n                network_file=self.bel_corpus)\n            bel_statements = bel_proc.statements\n            # Save to pickle file if we're caching\n            if self.basename is not None:\n                with open(bel_stmt_path, 'wb') as f:\n                    pickle.dump(bel_statements, f)\n        # Optionally filter out statements not involving only our gene set\n        if filter:\n            if len(self.gene_list) > 1:\n                bel_statements = ac.filter_gene_list(bel_statements,\n                                                     self.gene_list, 'all')\n        return bel_statements", "response": "Get relevant statements from the BEL large corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_biopax_stmts(self, filter=False, query='pathsbetween',\n                         database_filter=None):\n        \"\"\"Get relevant statements from Pathway Commons.\n\n        Performs a \"paths between\" query for the genes in :py:attr:`gene_list`\n        and uses the results to build statements. This function caches two\n        files: the list of statements built from the query, which is cached in\n        `<basename>_biopax_stmts.pkl`, and the OWL file returned by the Pathway\n        Commons Web API, which is cached in `<basename>_pc_pathsbetween.owl`.\n        If these cached files are found, then the results are returned based\n        on the cached file and Pathway Commons is not queried again.\n\n        Parameters\n        ----------\n        filter : Optional[bool]\n            If True, includes only those statements that exclusively mention\n            genes in :py:attr:`gene_list`. Default is False.\n        query : Optional[str]\n            Defined what type of query is executed. The two options are\n            'pathsbetween' which finds paths between the given list of genes\n            and only works if more than 1 gene is given, and 'neighborhood'\n            which searches the immediate neighborhood of each given gene.\n            Note that for pathsbetween queries with more thatn 60 genes, the\n            query will be executed in multiple blocks for scalability.\n        database_filter: Optional[list[str]]\n            A list of PathwayCommons databases to include in the query.\n\n        Returns\n        -------\n        list of :py:class:`indra.statements.Statement`\n            List of INDRA statements extracted from Pathway Commons.\n        \"\"\"\n        # If we're using a cache, initialize the appropriate filenames\n        if self.basename is not None:\n            biopax_stmt_path = '%s_biopax_stmts.pkl' % self.basename\n            biopax_ras_owl_path = '%s_pc_pathsbetween.owl' % self.basename\n        # Check for cached Biopax stmt file at the given path\n        # if it's there, return the statements from the cache\n        if self.basename is not None and os.path.isfile(biopax_stmt_path):\n            logger.info(\"Loading Biopax statements from %s\" % biopax_stmt_path)\n            with open(biopax_stmt_path, 'rb') as f:\n                bp_statements = pickle.load(f)\n            return bp_statements\n        # Check for cached file before querying Pathway Commons Web API\n        if self.basename is not None and os.path.isfile(biopax_ras_owl_path):\n            logger.info(\"Loading Biopax from OWL file %s\" % biopax_ras_owl_path)\n            bp = biopax.process_owl(biopax_ras_owl_path)\n        # OWL file not found; do query and save to file\n        else:\n            if (len(self.gene_list) < 2) and (query == 'pathsbetween'):\n                logger.warning('Using neighborhood query for one gene.')\n                query = 'neighborhood'\n            if query == 'pathsbetween':\n                if len(self.gene_list) > 60:\n                    block_size = 60\n                else:\n                    block_size = None\n                bp = biopax.process_pc_pathsbetween(self.gene_list,\n                                                database_filter=database_filter,\n                                                block_size=block_size)\n            elif query == 'neighborhood':\n                bp = biopax.process_pc_neighborhood(self.gene_list,\n                                                database_filter=database_filter)\n            else:\n                logger.error('Invalid query type: %s' % query)\n                return []\n            # Save the file if we're caching\n            if self.basename is not None:\n                bp.save_model(biopax_ras_owl_path)\n        # Save statements to pickle file if we're caching\n        if self.basename is not None:\n            with open(biopax_stmt_path, 'wb') as f:\n                pickle.dump(bp.statements, f)\n        # Optionally filter out statements not involving only our gene set\n        if filter:\n            policy = 'one' if len(self.gene_list) > 1 else 'all'\n            stmts = ac.filter_gene_list(bp.statements, self.gene_list, policy)\n        else:\n            stmts = bp.statements\n        return stmts", "response": "Gets relevant statements from Pathway Commons."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the combined list of INDRA statements extracted from BEL and Pathway Commons.", "response": "def get_statements(self, filter=False):\n        \"\"\"Return the combined list of statements from BEL and Pathway Commons.\n\n        Internally calls :py:meth:`get_biopax_stmts` and\n        :py:meth:`get_bel_stmts`.\n\n        Parameters\n        ----------\n        filter : bool\n            If True, includes only those statements that exclusively mention\n            genes in :py:attr:`gene_list`. Default is False.\n\n        Returns\n        -------\n        list of :py:class:`indra.statements.Statement`\n            List of INDRA statements extracted the BEL large corpus and Pathway\n            Commons.\n        \"\"\"\n        bp_stmts = self.get_biopax_stmts(filter=filter)\n        bel_stmts = self.get_bel_stmts(filter=filter)\n\n        return bp_stmts + bel_stmts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_preassembly(self, stmts, print_summary=True):\n        # First round of preassembly: remove duplicates before sitemapping\n        pa1 = Preassembler(hierarchies, stmts)\n        logger.info(\"Combining duplicates\")\n        pa1.combine_duplicates()\n        # Map sites\n        logger.info(\"Mapping sites\")\n        (valid, mapped) = sm.map_sites(pa1.unique_stmts)\n        # Combine valid and successfully mapped statements into single list\n        correctly_mapped_stmts = []\n        for ms in mapped:\n            if all([True if mm[1] is not None else False\n                         for mm in ms.mapped_mods]):\n                correctly_mapped_stmts.append(ms.mapped_stmt)\n        mapped_stmts = valid + correctly_mapped_stmts \n        # Second round of preassembly: de-duplicate and combine related\n        pa2 = Preassembler(hierarchies, mapped_stmts)\n        logger.info(\"Combining duplicates again\")\n        pa2.combine_duplicates()\n        pa2.combine_related()\n        # Fill out the results dict\n        self.results = {}\n        self.results['raw'] = stmts\n        self.results['duplicates1'] = pa1.unique_stmts\n        self.results['valid'] = valid\n        self.results['mapped'] = mapped\n        self.results['mapped_stmts'] = mapped_stmts\n        self.results['duplicates2'] = pa2.unique_stmts\n        self.results['related2'] = pa2.related_stmts\n        # Print summary\n        if print_summary:\n            logger.info(\"\\nStarting number of statements: %d\" % len(stmts))\n            logger.info(\"After duplicate removal: %d\" % len(pa1.unique_stmts))\n            logger.info(\"Unique statements with valid sites: %d\" % len(valid))\n            logger.info(\"Unique statements with invalid sites: %d\" %\n                        len(mapped))\n            logger.info(\"After post-mapping duplicate removal: %d\" %\n                        len(pa2.unique_stmts))\n            logger.info(\"After combining related statements: %d\" %\n                        len(pa2.related_stmts))\n        # Save the results if we're caching\n        if self.basename is not None:\n            results_filename = '%s_results.pkl' % self.basename\n            with open(results_filename, 'wb') as f:\n                pickle.dump(self.results, f)\n        return self.results", "response": "Run complete preassembly procedure on the given list of statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_grounding(entity):\n    db_refs = {'TEXT': entity['text']}\n    groundings = entity.get('grounding')\n    if not groundings:\n        return db_refs\n\n    def get_ont_concept(concept):\n        \"\"\"Strip slash, replace spaces and remove example leafs.\"\"\"\n        # In the WM context, groundings have no URL prefix and start with /\n        # The following block does some special handling of these groundings.\n        if concept.startswith('/'):\n            concept = concept[1:]\n            concept = concept.replace(' ', '_')\n            # We eliminate any entries that aren't ontology categories\n            # these are typically \"examples\" corresponding to the category\n            while concept not in hume_onto_entries:\n                parts = concept.split('/')\n                if len(parts) == 1:\n                    break\n                concept = '/'.join(parts[:-1])\n        # Otherwise we just return the concept as is\n        return concept\n\n    # Basic collection of grounding entries\n    raw_grounding_entries = [(get_ont_concept(g['ontologyConcept']),\n                              g['value']) for g in groundings]\n\n    # Occasionally we get duplicate grounding entries, we want to\n    # eliminate those here\n    grounding_dict = {}\n    for cat, score in raw_grounding_entries:\n        if (cat not in grounding_dict) or (score > grounding_dict[cat]):\n            grounding_dict[cat] = score\n    # Then we sort the list in reverse order according to score\n    # Sometimes the exact same score appears multiple times, in this\n    # case we prioritize by the \"depth\" of the grounding which is\n    # obtained by looking at the number of /-s in the entry.\n    # However, there are still cases where the grounding depth and the score\n    # are the same. In these cases we just sort alphabetically.\n    grounding_entries = sorted(list(set(grounding_dict.items())),\n                               key=lambda x: (x[1], x[0].count('/'), x[0]),\n                               reverse=True)\n    # We could get an empty list here in which case we don't add the\n    # grounding\n    if grounding_entries:\n        db_refs['HUME'] = grounding_entries\n    return db_refs", "response": "Return the Hume grounding for the given entity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_relations(self):\n        # Get all extractions\n        extractions = \\\n            list(self.tree.execute(\"$.extractions[(@.@type is 'Extraction')]\"))\n\n        # Get relations from extractions\n        relations = []\n        for e in extractions:\n            label_set = set(e.get('labels', []))\n            # If this is a DirectedRelation\n            if 'DirectedRelation' in label_set:\n                self.relation_dict[e['@id']] = e\n                subtype = e.get('subtype')\n                if any(t in subtype for t in polarities.keys()):\n                    relations.append((subtype, e))\n            # If this is an Event or an Entity\n            if {'Event', 'Entity'} & label_set:\n                self.concept_dict[e['@id']] = e\n\n        if not relations and not self.relation_dict:\n            logger.info(\"No relations found.\")\n        else:\n            logger.info('%d relations of types %s found'\n                        % (len(relations), ', '.join(polarities.keys())))\n            logger.info('%d relations in dict.' % len(self.relation_dict))\n            logger.info('%d concepts found.' % len(self.concept_dict))\n        return relations", "response": "Find all relevant relation elements and return them in a list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_documents(self):\n        documents = self.tree.execute(\"$.documents\")\n        for doc in documents:\n            sentences = {s['@id']: s['text'] for s in doc.get('sentences', [])}\n            self.document_dict[doc['@id']] = {'sentences': sentences,\n                                              'location': doc['location']}", "response": "Populate sentences attribute with a dict keyed by document id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_context(self, entity):\n        loc_context = None\n        time_context = None\n\n        # Look for time and place contexts.\n        for argument in entity[\"arguments\"]:\n            if argument[\"type\"] == \"place\":\n                entity_id = argument[\"value\"][\"@id\"]\n                loc_entity = self.concept_dict[entity_id]\n                place = loc_entity.get(\"canonicalName\")\n                if not place:\n                    place = loc_entity['text']\n                geo_id = loc_entity.get('geoname_id')\n                loc_context = RefContext(name=place, db_refs={\"GEOID\": geo_id})\n            if argument[\"type\"] == \"time\":\n                entity_id = argument[\"value\"][\"@id\"]\n                temporal_entity = self.concept_dict[entity_id]\n                text = temporal_entity['mentions'][0]['text']\n                if len(temporal_entity.get(\"timeInterval\", [])) < 1:\n                    time_context = TimeContext(text=text)\n                    continue\n                time = temporal_entity[\"timeInterval\"][0]\n                start = datetime.strptime(time['start'], '%Y-%m-%dT%H:%M')\n                end = datetime.strptime(time['end'], '%Y-%m-%dT%H:%M')\n                duration = int(time['duration'])\n                time_context = TimeContext(text=text, start=start, end=end,\n                                           duration=duration)\n\n        # Put context together\n        context = None\n        if loc_context or time_context:\n            context = WorldContext(time=time_context, geo_location=loc_context)\n\n        return context", "response": "Get place and time info from the json for this entity."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _make_concept(self, entity):\n        # Use the canonical name as the name of the Concept by default\n        name = self._sanitize(entity['canonicalName'])\n        # But if there is a trigger head text, we prefer that since\n        # it almost always results in a cleaner name\n        # This is removed for now since the head word seems to be too\n        # minimal for some concepts, e.g. it gives us only \"security\"\n        # for \"food security\".\n        \"\"\"\n        trigger = entity.get('trigger')\n        if trigger is not None:\n            head_text = trigger.get('head text')\n            if head_text is not None:\n                name = head_text\n        \"\"\"\n        # Save raw text and Hume scored groundings as db_refs\n        db_refs = _get_grounding(entity)\n        concept = Concept(name, db_refs=db_refs)\n        metadata = {arg['type']: arg['value']['@id']\n                    for arg in entity['arguments']}\n\n        return concept, metadata", "response": "Return Concept from a Hume entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an INDRA Event based on an event entry.", "response": "def _get_event_and_context(self, event, arg_type):\n        \"\"\"Return an INDRA Event based on an event entry.\"\"\"\n        eid = _choose_id(event, arg_type)\n        ev = self.concept_dict[eid]\n        concept, metadata = self._make_concept(ev)\n        ev_delta = {'adjectives': [],\n                    'states': get_states(ev),\n                    'polarity': get_polarity(ev)}\n        context = self._make_context(ev)\n        event_obj = Event(concept, delta=ev_delta, context=context)\n        return event_obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_evidence(self, event, adjectives):\n        provenance = event.get('provenance')\n\n        # First try looking up the full sentence through provenance\n        doc_id = provenance[0]['document']['@id']\n        sent_id = provenance[0]['sentence']\n        text = self.document_dict[doc_id]['sentences'][sent_id]\n        text = self._sanitize(text)\n        bounds = [provenance[0]['documentCharPositions'][k]\n                  for k in ['start', 'end']]\n\n        annotations = {\n            'found_by': event.get('rule'),\n            'provenance': provenance,\n            'event_type': os.path.basename(event.get('type')),\n            'adjectives': adjectives,\n            'bounds': bounds\n            }\n        location = self.document_dict[doc_id]['location']\n        ev = Evidence(source_api='hume', text=text, annotations=annotations,\n                      pmid=location)\n        return [ev]", "response": "Return the Evidence object for the INDRA Statement."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_statement_in_list(new_stmt, old_stmt_list):\n    for old_stmt in old_stmt_list:\n        if old_stmt.equals(new_stmt):\n            return True\n        elif old_stmt.evidence_equals(new_stmt) and old_stmt.matches(new_stmt):\n            # If we're comparing a complex, make sure the agents are sorted.\n            if isinstance(new_stmt, Complex):\n                agent_pairs = zip(old_stmt.sorted_members(),\n                                  new_stmt.sorted_members())\n            else:\n                agent_pairs = zip(old_stmt.agent_list(), new_stmt.agent_list())\n\n            # Compare agent-by-agent.\n            for ag_old, ag_new in agent_pairs:\n                s_old = set(ag_old.db_refs.items())\n                s_new = set(ag_new.db_refs.items())\n\n                # If they're equal this isn't the one we're interested in.\n                if s_old == s_new:\n                    continue\n\n                # If the new statement has nothing new to offer, just ignore it\n                if s_old > s_new:\n                    return True\n\n                # If the new statement does have something new, add it to the\n                # existing statement. And then ignore it.\n                if s_new > s_old:\n                    ag_old.db_refs.update(ag_new.db_refs)\n                    return True\n\n                # If this is a case where different CHEBI ids were mapped to\n                # the same entity, set the agent name to the CHEBI id.\n                if _fix_different_refs(ag_old, ag_new, 'CHEBI'):\n                    # Check to make sure the newly described statement does\n                    # not match anything.\n                    return _is_statement_in_list(new_stmt, old_stmt_list)\n\n                # If this is a case, like above, but with UMLS IDs, do the same\n                # thing as above. This will likely never be improved.\n                if _fix_different_refs(ag_old, ag_new, 'UMLS'):\n                    # Check to make sure the newly described statement does\n                    # not match anything.\n                    return _is_statement_in_list(new_stmt, old_stmt_list)\n\n                logger.warning(\"Found an unexpected kind of duplicate. \"\n                               \"Ignoring it.\")\n                return True\n\n            # This means all the agents matched, which can happen if the\n            # original issue was the ordering of agents in a Complex.\n            return True\n\n        elif old_stmt.get_hash(True, True) == new_stmt.get_hash(True, True):\n            # Check to see if we can improve the annotation of the existing\n            # statement.\n            e_old = old_stmt.evidence[0]\n            e_new = new_stmt.evidence[0]\n            if e_old.annotations['last_verb'] is None:\n                e_old.annotations['last_verb'] = e_new.annotations['last_verb']\n\n            # If the evidence is \"the same\", modulo annotations, just ignore it\n            if e_old.get_source_hash(True) == e_new.get_source_hash(True):\n                return True\n\n    return False", "response": "Return True of given statement is equivalent to any statement in a list of statements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef normalize_medscan_name(name):\n    suffix = ' complex'\n\n    for i in range(2):\n        if name.endswith(suffix):\n            name = name[:-len(suffix)]\n    return name", "response": "Normalizes a Medscan agent name so that it s better corresponds with the grounding map."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _urn_to_db_refs(urn):\n    # Convert a urn to a db_refs dictionary\n    if urn is None:\n        return {}, None\n\n    m = URN_PATT.match(urn)\n    if m is None:\n        return None, None\n\n    urn_type, urn_id = m.groups()\n\n    db_refs = {}\n    db_name = None\n\n    # TODO: support more types of URNs\n    if urn_type == 'agi-cas':\n        # Identifier is CAS, convert to CHEBI\n        chebi_id = get_chebi_id_from_cas(urn_id)\n        if chebi_id:\n            db_refs['CHEBI'] = 'CHEBI:%s' % chebi_id\n            db_name = get_chebi_name_from_id(chebi_id)\n    elif urn_type == 'agi-llid':\n        # This is an Entrez ID, convert to HGNC\n        hgnc_id = get_hgnc_from_entrez(urn_id)\n        if hgnc_id is not None:\n            db_refs['HGNC'] = hgnc_id\n\n            # Convert the HGNC ID to a Uniprot ID\n            uniprot_id = get_uniprot_id(hgnc_id)\n            if uniprot_id is not None:\n                db_refs['UP'] = uniprot_id\n\n            # Try to lookup HGNC name; if it's available, set it to the\n            # agent name\n            db_name = get_hgnc_name(hgnc_id)\n    elif urn_type in ['agi-meshdis', 'agi-ncimorgan', 'agi-ncimtissue',\n                      'agi-ncimcelltype']:\n        if urn_id.startswith('C') and urn_id[1:].isdigit():\n            # Identifier is probably UMLS\n            db_refs['UMLS'] = urn_id\n        else:\n            # Identifier is MESH\n            urn_mesh_name = unquote(urn_id)\n            mesh_id, mesh_name = mesh_client.get_mesh_id_name(urn_mesh_name)\n            if mesh_id:\n                db_refs['MESH'] = mesh_id\n                db_name = mesh_name\n            else:\n                db_name = urn_mesh_name\n    elif urn_type == 'agi-gocomplex':\n        # Identifier is GO\n        db_refs['GO'] = 'GO:%s' % urn_id\n    elif urn_type == 'agi-go':\n        # Identifier is GO\n        db_refs['GO'] = 'GO:%s' % urn_id\n\n    # If we have a GO or MESH grounding, see if there is a corresponding\n    # Famplex grounding\n    db_sometimes_maps_to_famplex = ['GO', 'MESH']\n    for db in db_sometimes_maps_to_famplex:\n        if db in db_refs:\n            key = (db, db_refs[db])\n            if key in famplex_map:\n                db_refs['FPLX'] = famplex_map[key]\n\n    # If the urn corresponds to an eccode, groudn to famplex if that eccode\n    # is in the Famplex equivalences table\n    if urn.startswith('urn:agi-enz'):\n        tokens = urn.split(':')\n        eccode = tokens[2]\n        key = ('ECCODE', eccode)\n        if key in famplex_map:\n            db_refs['FPLX'] = famplex_map[key]\n\n    # If the Medscan URN itself maps to a Famplex id, add a Famplex grounding\n    key = ('MEDSCAN', urn)\n    if key in famplex_map:\n        db_refs['FPLX'] = famplex_map[key]\n\n    # If there is a Famplex grounding, use Famplex for entity name\n    if 'FPLX' in db_refs:\n        db_name = db_refs['FPLX']\n    elif 'GO' in db_refs:\n        db_name = go_client.get_go_label(db_refs['GO'])\n\n    return db_refs, db_name", "response": "Converts a Medscan URN to an INDRA db_refs dictionary with grounding information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _untag_sentence(tagged_sentence):\n    untagged_sentence = TAG_PATT.sub('\\\\2', tagged_sentence)\n    clean_sentence = JUNK_PATT.sub('', untagged_sentence)\n    return clean_sentence.strip()", "response": "Removes all tags in the tagged sentence and returns the original sentence with the Medscan annotations stripped out."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a tagged sentence extracts a dictionary mapping tags to words or phrases that they tag.", "response": "def _extract_sentence_tags(tagged_sentence):\n    \"\"\"Given a tagged sentence, extracts a dictionary mapping tags to the words\n    or phrases that they tag.\n\n    Parameters\n    ----------\n    tagged_sentence : str\n        The sentence with Medscan annotations and tags\n\n    Returns\n    -------\n    tags : dict\n        A dictionary mapping tags to the words or phrases that they tag.\n    \"\"\"\n    untagged_sentence = _untag_sentence(tagged_sentence)\n    decluttered_sentence = JUNK_PATT.sub('', tagged_sentence)\n    tags = {}\n\n    # Iteratively look for all matches of this pattern\n    endpos = 0\n    while True:\n        match = TAG_PATT.search(decluttered_sentence, pos=endpos)\n        if not match:\n            break\n        endpos = match.end()\n        text = match.group(2)\n        text = text.replace('CONTEXT', '')\n        text = text.replace('GLOSSARY', '')\n        text = text.strip()\n        start = untagged_sentence.index(text)\n        stop = start + len(text)\n\n        tag_key = match.group(1)\n        if ',' in tag_key:\n            for sub_key in tag_key.split(','):\n                if sub_key == '0':\n                    continue\n                tags[sub_key] = {'text': text, 'bounds': (start, stop)}\n        else:\n            tags[tag_key] = {'text': text, 'bounds': (start, stop)}\n    return tags"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the site - text string and return a list of sites.", "response": "def get_sites(self):\n        \"\"\"Parse the site-text string and return a list of sites.\n\n        Returns\n        -------\n        sites : list[Site]\n            A list of position-residue pairs corresponding to the site-text\n        \"\"\"\n        st = self.site_text\n        suffixes = [' residue', ' residues', ',', '/']\n        for suffix in suffixes:\n            if st.endswith(suffix):\n                st = st[:-len(suffix)]\n        assert(not st.endswith(','))\n\n        # Strip parentheses\n        st = st.replace('(', '')\n        st = st.replace(')', '')\n        st = st.replace(' or ', ' and ')  # Treat end and or the same\n\n        sites = []\n        parts = st.split(' and ')\n        for part in parts:\n            if part.endswith(','):\n                part = part[:-1]\n            if len(part.strip()) > 0:\n                sites.extend(ReachProcessor._parse_site_text(part.strip()))\n        return sites"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a filehandle to MedScan csxml output into INDRACSXML statements.", "response": "def process_csxml_file(self, filename, interval=None, lazy=False):\n        \"\"\"Processes a filehandle to MedScan csxml input into INDRA\n        statements.\n\n        The CSXML format consists of a top-level `<batch>` root element\n        containing a series of `<doc>` (document) elements, in turn containing\n        `<sec>` (section) elements, and in turn containing `<sent>` (sentence)\n        elements.\n\n        Within the `<sent>` element, a series of additional elements appear in\n        the following order:\n\n        * `<toks>`, which contains a tokenized form of the sentence in its text\n          attribute\n        * `<textmods>`, which describes any preprocessing/normalization done to\n          the underlying text\n        * `<match>` elements, each of which contains one of more `<entity>`\n          elements, describing entities in the text with their identifiers.\n          The local IDs of each entities are given in the `msid` attribute of\n          this element; these IDs are then referenced in any subsequent SVO\n          elements.\n        * `<svo>` elements, representing subject-verb-object triples. SVO\n          elements with a `type` attribute of `CONTROL` represent normalized\n          regulation relationships; they often represent the normalized\n          extraction of the immediately preceding (but unnormalized SVO\n          element). However, in some cases there can be a \"CONTROL\" SVO\n          element without its parent immediately preceding it.\n\n        Parameters\n        ----------\n        filename : string\n            The path to a Medscan csxml file.\n        interval : (start, end) or None\n            Select the interval of documents to read, starting with the\n            `start`th document and ending before the `end`th document. If\n            either is None, the value is considered undefined. If the value\n            exceeds the bounds of available documents, it will simply be\n            ignored.\n        lazy : bool\n            If True, only create a generator which can be used by the\n            `get_statements` method. If True, populate the statements list now.\n        \"\"\"\n        if interval is None:\n            interval = (None, None)\n\n        tmp_fname = tempfile.mktemp(os.path.basename(filename))\n        fix_character_encoding(filename, tmp_fname)\n\n        self.__f = open(tmp_fname, 'rb')\n        self._gen = self._iter_through_csxml_file_from_handle(*interval)\n        if not lazy:\n            for stmt in self._gen:\n                self.statements.append(stmt)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a relation into an INDRA statement.", "response": "def process_relation(self, relation, last_relation):\n        \"\"\"Process a relation into an INDRA statement.\n\n        Parameters\n        ----------\n        relation : MedscanRelation\n            The relation to process (a CONTROL svo with normalized verb)\n        last_relation : MedscanRelation\n            The relation immediately proceding the relation to process within\n            the same sentence, or None if there are no preceding relations\n            within the same sentence. This proceeding relation, if available,\n            will refer to the same interaction but with an unnormalized\n            (potentially more specific) verb, and is used when processing\n            protein modification events.\n        \"\"\"\n        subj_res = self.agent_from_entity(relation, relation.subj)\n        obj_res = self.agent_from_entity(relation, relation.obj)\n        if subj_res is None or obj_res is None:\n            # Don't extract a statement if the subject or object cannot\n            # be resolved\n            return\n        subj, subj_bounds = subj_res\n        obj, obj_bounds = obj_res\n\n        # Make evidence object\n        untagged_sentence = _untag_sentence(relation.tagged_sentence)\n        if last_relation:\n            last_verb = last_relation.verb\n        else:\n            last_verb = None\n        # Get the entity information with the character coordinates\n        annotations = {'verb': relation.verb, 'last_verb': last_verb,\n                       'agents': {'coords': [subj_bounds, obj_bounds]}}\n        epistemics = dict()\n        epistemics['direct'] = False  # Overridden later if needed\n        ev = [Evidence(source_api='medscan', source_id=relation.uri,\n                       pmid=relation.pmid, text=untagged_sentence,\n                       annotations=annotations, epistemics=epistemics)]\n\n        if relation.verb in INCREASE_AMOUNT_VERBS:\n            # If the normalized verb corresponds to an IncreaseAmount statement\n            # then make one\n            self._add_statement(IncreaseAmount(subj, obj, evidence=ev))\n        elif relation.verb in DECREASE_AMOUNT_VERBS:\n            # If the normalized verb corresponds to a DecreaseAmount statement\n            # then make one\n            self._add_statement(DecreaseAmount(subj, obj, evidence=ev))\n        elif relation.verb in ALL_ACTIVATION_VERBS:\n            # If the normalized verb corresponds to an Activation statement,\n            # then make one\n            if relation.verb in D_ACTIVATION_VERBS:\n                ev[0].epistemics['direction'] = True\n            self._add_statement(Activation(subj, obj, evidence=ev))\n        elif relation.verb in ALL_INHIBITION_VERBS:\n            # If the normalized verb corresponds to an Inhibition statement,\n            # then make one\n            if relation.verb in D_INHIBITION_VERBS:\n                ev[0].epistemics['direct'] = True\n            self._add_statement(Inhibition(subj, obj, evidence=ev))\n\n        elif relation.verb == 'ProtModification':\n            # The normalized verb 'ProtModification' is too vague to make\n            # an INDRA statement. We look at the unnormalized verb in the\n            # previous svo element, if available, to decide what type of\n            # INDRA statement to construct.\n\n            if last_relation is None:\n                # We cannot make a statement unless we have more fine-grained\n                # information on the relation type from a preceding\n                # unnormalized SVO\n                return\n\n            # Map the unnormalized verb to an INDRA statement type\n            if last_relation.verb == 'TK{phosphorylate}':\n                statement_type = Phosphorylation\n            elif last_relation.verb == 'TK{dephosphorylate}':\n                statement_type = Dephosphorylation\n            elif last_relation.verb == 'TK{ubiquitinate}':\n                statement_type = Ubiquitination\n            elif last_relation.verb == 'TK{acetylate}':\n                statement_type = Acetylation\n            elif last_relation.verb == 'TK{methylate}':\n                statement_type = Methylation\n            elif last_relation.verb == 'TK{deacetylate}':\n                statement_type = Deacetylation\n            elif last_relation.verb == 'TK{demethylate}':\n                statement_type = Demethylation\n            elif last_relation.verb == 'TK{hyperphosphorylate}':\n                statement_type = Phosphorylation\n            elif last_relation.verb == 'TK{hydroxylate}':\n                statement_type = Hydroxylation\n            elif last_relation.verb == 'TK{sumoylate}':\n                statement_type = Sumoylation\n            elif last_relation.verb == 'TK{palmitoylate}':\n                statement_type = Palmitoylation\n            elif last_relation.verb == 'TK{glycosylate}':\n                statement_type = Glycosylation\n            elif last_relation.verb == 'TK{ribosylate}':\n                statement_type = Ribosylation\n            elif last_relation.verb == 'TK{deglycosylate}':\n                statement_type = Deglycosylation\n            elif last_relation.verb == 'TK{myristylate}':\n                statement_type = Myristoylation\n            elif last_relation.verb == 'TK{farnesylate}':\n                statement_type = Farnesylation\n            elif last_relation.verb == 'TK{desumoylate}':\n                statement_type = Desumoylation\n            elif last_relation.verb == 'TK{geranylgeranylate}':\n                statement_type = Geranylgeranylation\n            elif last_relation.verb == 'TK{deacylate}':\n                statement_type = Deacetylation\n            else:\n                # This unnormalized verb is not handled, do not extract an\n                # INDRA statement\n                return\n\n            obj_text = obj.db_refs['TEXT']\n            last_info = self.last_site_info_in_sentence\n            if last_info is not None and obj_text == last_info.object_text:\n                for site in self.last_site_info_in_sentence.get_sites():\n                    r = site.residue\n                    p = site.position\n\n                    s = statement_type(subj, obj, residue=r, position=p,\n                                       evidence=ev)\n                    self._add_statement(s)\n            else:\n                self._add_statement(statement_type(subj, obj, evidence=ev))\n\n        elif relation.verb == 'Binding':\n            # The Binding normalized verb corresponds to the INDRA Complex\n            # statement.\n            self._add_statement(\n                                   Complex([subj, obj], evidence=ev)\n                                  )\n        elif relation.verb == 'ProtModification-negative':\n            pass  # TODO? These occur so infrequently so maybe not worth it\n        elif relation.verb == 'Regulation-unknown':\n            pass  # TODO? These occur so infrequently so maybe not worth it\n        elif relation.verb == 'StateEffect-positive':\n            pass\n            # self._add_statement(\n            #                       ActiveForm(subj, obj, evidence=ev)\n            #                      )\n            # TODO: disabling for now, since not sure whether we should set\n            # the is_active flag\n        elif relation.verb == 'StateEffect':\n            self.last_site_info_in_sentence = \\\n                    ProteinSiteInfo(site_text=subj.name,\n                                    object_text=obj.db_refs['TEXT'])\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an INDRA Agent object from a given entity.", "response": "def agent_from_entity(self, relation, entity_id):\n        \"\"\"Create a (potentially grounded) INDRA Agent object from a given\n        Medscan entity describing the subject or object.\n\n        Uses helper functions to convert a Medscan URN to an INDRA db_refs\n        grounding dictionary.\n\n        If the entity has properties indicating that it is a protein with\n        a mutation or modification, then constructs the needed ModCondition\n        or MutCondition.\n\n        Parameters\n        ----------\n        relation : MedscanRelation\n            The current relation being processed\n        entity_id : str\n            The ID of the entity to process\n\n        Returns\n        -------\n        agent : indra.statements.Agent\n            A potentially grounded INDRA agent representing this entity\n        \"\"\"\n        # Extract sentence tags mapping ids to the text. We refer to this\n        # mapping only if the entity doesn't appear in the grounded entity\n        # list\n        tags = _extract_sentence_tags(relation.tagged_sentence)\n\n        if entity_id is None:\n            return None\n        self.num_entities += 1\n\n        entity_id = _extract_id(entity_id)\n\n        if entity_id not in relation.entities and \\\n                entity_id not in tags:\n            # Could not find the entity in either the list of grounded\n            # entities of the items tagged in the sentence. Happens for\n            # a very small percentage of the dataset.\n            self.num_entities_not_found += 1\n            return None\n\n        if entity_id not in relation.entities:\n            # The entity is not in the grounded entity list\n            # Instead, make an ungrounded entity, with TEXT corresponding to\n            # the words with the given entity id tagged in the sentence.\n            entity_data = tags[entity_id]\n            db_refs = {'TEXT': entity_data['text']}\n            ag = Agent(normalize_medscan_name(db_refs['TEXT']),\n                       db_refs=db_refs)\n            return ag, entity_data['bounds']\n        else:\n            entity = relation.entities[entity_id]\n            bounds = (entity.ch_start, entity.ch_end)\n\n            prop = entity.properties\n            if len(prop.keys()) == 2 and 'Protein' in prop \\\n               and 'Mutation' in prop:\n                # Handle the special case where the entity is a protein\n                # with a mutation or modification, with those details\n                # described in the entity properties\n                protein = prop['Protein']\n                assert(len(protein) == 1)\n                protein = protein[0]\n\n                mutation = prop['Mutation']\n                assert(len(mutation) == 1)\n                mutation = mutation[0]\n\n                db_refs, db_name = _urn_to_db_refs(protein.urn)\n\n                if db_refs is None:\n                    return None\n                db_refs['TEXT'] = protein.name\n\n                if db_name is None:\n                    agent_name = db_refs['TEXT']\n                else:\n                    agent_name = db_name\n\n                # Check mutation.type. Only some types correspond to situations\n                # that can be represented in INDRA; return None if we cannot\n                # map to an INDRA statement (which will block processing of\n                # the statement in process_relation).\n                if mutation.type == 'AASite':\n                    # Do not handle this\n                    # Example:\n                    # MedscanEntity(name='D1', urn='urn:agi-aa:D1',\n                    # type='AASite', properties=None)\n                    return None\n                elif mutation.type == 'Mutation':\n                    # Convert mutation properties to an INDRA MutCondition\n                    r_old, pos, r_new = _parse_mut_string(mutation.name)\n                    if r_old is None:\n                        logger.warning('Could not parse mutation string: ' +\n                                       mutation.name)\n                        # Don't create an agent\n                        return None\n                    else:\n                        try:\n                            cond = MutCondition(pos, r_old, r_new)\n                            ag = Agent(normalize_medscan_name(agent_name),\n                                       db_refs=db_refs, mutations=[cond])\n                            return ag, bounds\n                        except BaseException:\n                            logger.warning('Could not parse mutation ' +\n                                           'string: ' + mutation.name)\n                            return None\n                elif mutation.type == 'MethSite':\n                    # Convert methylation site information to an INDRA\n                    # ModCondition\n                    res, pos = _parse_mod_string(mutation.name)\n                    if res is None:\n                        return None\n                    cond = ModCondition('methylation', res, pos)\n                    ag = Agent(normalize_medscan_name(agent_name),\n                               db_refs=db_refs, mods=[cond])\n                    return ag, bounds\n\n                    # Example:\n                    # MedscanEntity(name='R457',\n                    # urn='urn:agi-s-llid:R457-2185', type='MethSite',\n                    # properties=None)\n                elif mutation.type == 'PhosphoSite':\n                    # Convert phosphorylation site information to an INDRA\n                    # ModCondition\n                    res, pos = _parse_mod_string(mutation.name)\n                    if res is None:\n                        return None\n                    cond = ModCondition('phosphorylation', res, pos)\n                    ag = Agent(normalize_medscan_name(agent_name),\n                               db_refs=db_refs, mods=[cond])\n                    return ag, bounds\n\n                    # Example:\n                    # MedscanEntity(name='S455',\n                    # urn='urn:agi-s-llid:S455-47', type='PhosphoSite',\n                    # properties=None)\n                    pass\n                elif mutation.type == 'Lysine':\n                    # Ambiguous whether this is a methylation or\n                    # demethylation; skip\n\n                    # Example:\n                    # MedscanEntity(name='K150',\n                    # urn='urn:agi-s-llid:K150-5624', type='Lysine',\n                    # properties=None)\n                    return None\n                else:\n                    logger.warning('Processor currently cannot process ' +\n                                   'mutations of type ' + mutation.type)\n            else:\n                # Handle the more common case where we just ground the entity\n                # without mutation or modification information\n                db_refs, db_name = _urn_to_db_refs(entity.urn)\n                if db_refs is None:\n                    return None\n                db_refs['TEXT'] = entity.name\n\n                if db_name is None:\n                    agent_name = db_refs['TEXT']\n                else:\n                    agent_name = db_name\n\n                ag = Agent(normalize_medscan_name(agent_name),\n                           db_refs=db_refs)\n                return ag, bounds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a parser that is generic to reading scripts.", "response": "def get_parser(description, input_desc):\n    \"\"\"Get a parser that is generic to reading scripts.\n\n    Parameters\n    ----------\n    description : str\n        A description of the tool, usually about one line long.\n    input_desc: str\n        A string describing the nature of the input file used by the reading\n        tool.\n\n    Returns\n    -------\n    parser : argparse.ArgumentParser instance\n        An argument parser object, to which further arguments can be added.\n    \"\"\"\n    parser = ArgumentParser(description=description)\n    parser.add_argument(\n        dest='input_file',\n        help=input_desc\n        )\n    parser.add_argument(\n        '-r', '--readers',\n        choices=['reach', 'sparser', 'trips'],\n        help='List of readers to be used.',\n        nargs='+'\n        )\n    parser.add_argument(\n        '-n', '--num_procs',\n        dest='n_proc',\n        help='Select the number of processes to use.',\n        type=int,\n        default=1\n        )\n    parser.add_argument(\n        '-s', '--sample',\n        dest='n_samp',\n        help='Read a random sample of size N_SAMP of the inputs.',\n        type=int\n        )\n    parser.add_argument(\n        '-I', '--in_range',\n        dest='range_str',\n        help='Only read input lines in the range given as <start>:<end>.'\n        )\n    parser.add_argument(\n        '-v', '--verbose',\n        help='Include output from the readers.',\n        action='store_true'\n        )\n    parser.add_argument(\n        '-q', '--quiet',\n        help='Suppress most output. Overrides -v and -d options.',\n        action='store_true'\n        )\n    parser.add_argument(\n        '-d', '--debug',\n        help='Set the logging to debug level.',\n        action='store_true'\n        )\n    # parser.add_argument(\n    #     '-m', '--messy',\n    #     help='Do not clean up directories created while reading.',\n    #     action='store_true'\n    #     )\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a request to a NewsAPI endpoint and return the JSON response.", "response": "def send_request(endpoint, **kwargs):\n    \"\"\"Return the response to a query as JSON from the NewsAPI web service.\n\n    The basic API is limited to 100 results which is chosen unless explicitly\n    given as an argument. Beyond that, paging is supported through the \"page\"\n    argument, if needed.\n\n    Parameters\n    ----------\n    endpoint : str\n        Endpoint to query, e.g. \"everything\" or \"top-headlines\"\n\n    kwargs : dict\n        A list of keyword arguments passed as parameters with the query.\n        The basic ones are \"q\" which is the search query, \"from\" is a start\n        date formatted as for instance 2018-06-10 and \"to\" is an end date\n        with the same format.\n\n    Returns\n    -------\n    res_json : dict\n        The response from the web service as a JSON dict.\n    \"\"\"\n    if api_key is None:\n        logger.error('NewsAPI cannot be used without an API key')\n        return None\n    url = '%s/%s' % (newsapi_url, endpoint)\n    if 'apiKey' not in kwargs:\n        kwargs['apiKey'] = api_key\n    if 'pageSize' not in kwargs:\n        kwargs['pageSize'] = 100\n    res = requests.get(url, params=kwargs)\n    res.raise_for_status()\n    res_json = res.json() \n    return res_json"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_cx_file(file_name, require_grounding=True):\n    with open(file_name, 'rt') as fh:\n        json_list = json.load(fh)\n        return process_cx(json_list, require_grounding=require_grounding)", "response": "Process a CX file into Statements."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_ndex_network(network_id, username=None, password=None,\n                         require_grounding=True):\n    \"\"\"Process an NDEx network into Statements.\n\n    Parameters\n    ----------\n    network_id : str\n        NDEx network ID.\n    username : str\n        NDEx username.\n    password : str\n        NDEx password.\n    require_grounding: bool\n        Whether network nodes lacking grounding information should be included\n        among the extracted Statements (default is True).\n\n    Returns\n    -------\n    NdexCxProcessor\n        Processor containing Statements. Returns None if there if the HTTP\n        status code indicates an unsuccessful request.\n    \"\"\"\n    nd = ndex2.client.Ndex2(username=username, password=password)\n    res = nd.get_network_as_cx_stream(network_id)\n    if res.status_code != 200:\n        logger.error('Problem downloading network: status code %s' %\n                     res.status_code)\n        logger.error('Response: %s' % res.text)\n        return None\n    json_list = res.json()\n    summary = nd.get_network_summary(network_id)\n    return process_cx(json_list, summary=summary,\n                      require_grounding=require_grounding)", "response": "Process an NDEx network into a NdexCxProcessor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a CX JSON object into Statements.", "response": "def process_cx(cx_json, summary=None, require_grounding=True):\n    \"\"\"Process a CX JSON object into Statements.\n\n    Parameters\n    ----------\n    cx_json : list\n        CX JSON object.\n    summary : Optional[dict]\n        The network summary object which can be obtained via\n        get_network_summary through the web service. THis contains metadata\n        such as the owner and the creation time of the network.\n    require_grounding: bool\n        Whether network nodes lacking grounding information should be included\n        among the extracted Statements (default is True).\n\n    Returns\n    -------\n    NdexCxProcessor\n        Processor containing Statements.\n    \"\"\"\n    ncp = NdexCxProcessor(cx_json, summary=summary,\n                          require_grounding=require_grounding)\n    ncp.get_statements()\n    return ncp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the files in files with the readers in readers.", "response": "def read_files(files, readers, **kwargs):\n    \"\"\"Read the files in `files` with the reader objects in `readers`.\n\n    Parameters\n    ----------\n    files : list [str]\n        A list of file paths to be read by the readers. Supported files are\n        limited to text and nxml files.\n    readers : list [Reader instances]\n        A list of Reader objects to be used reading the files.\n    **kwargs :\n        Other keyword arguments are passed to the `read` method of the readers.\n\n    Returns\n    -------\n    output_list : list [ReadingData]\n        A list of ReadingData objects with the contents of the readings.\n    \"\"\"\n    reading_content = [Content.from_file(filepath) for filepath in files]\n    output_list = []\n    for reader in readers:\n        res_list = reader.read(reading_content, **kwargs)\n        if res_list is None:\n            logger.info(\"Nothing read by %s.\" % reader.name)\n        else:\n            logger.info(\"Successfully read %d content entries with %s.\"\n                        % (len(res_list), reader.name))\n            output_list += res_list\n    logger.info(\"Read %s text content entries in all.\" % len(output_list))\n    return output_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expand_families(self, stmts):\n        new_stmts = []\n        for stmt in stmts:\n            # Put together the lists of families, with their members. E.g.,\n            # for a statement involving RAF and MEK, should return a list of\n            # tuples like [(BRAF, RAF1, ARAF), (MAP2K1, MAP2K2)]\n            families_list = []\n            for ag in stmt.agent_list():\n                ag_children = self.get_children(ag)\n                # If the agent has no children, then we use the agent itself\n                if len(ag_children) == 0:\n                    families_list.append([ag])\n                # Otherwise, we add the tuple of namespaces/IDs for the children\n                else:\n                    families_list.append(ag_children)\n            # Now, put together new statements frmo the cross product of the\n            # expanded family members\n            for ag_combo in itertools.product(*families_list):\n                # Create new agents based on the namespaces/IDs, with\n                # appropriate name and db_refs entries\n                child_agents = []\n                for ag_entry in ag_combo:\n                    # If we got an agent, or None, that means there were no\n                    # children; so we use the original agent rather than\n                    # construct a new agent\n                    if ag_entry is None or isinstance(ag_entry, Agent):\n                        new_agent = ag_entry\n                    # Otherwise, create a new agent from the ns/ID\n                    elif isinstance(ag_entry, tuple):\n                        # FIXME FIXME FIXME\n                        # This doesn't reproduce agent state from the original\n                        # family-level statements!\n                        ag_ns, ag_id = ag_entry\n                        new_agent = _agent_from_ns_id(ag_ns, ag_id)\n                    else:\n                        raise Exception('Unrecognized agent entry type.')\n                    # Add agent to our list of child agents\n                    child_agents.append(new_agent)\n                # Create a copy of the statement\n                new_stmt = deepcopy(stmt)\n                # Replace the agents in the statement with the newly-created\n                # child agents\n                new_stmt.set_agent_list(child_agents)\n                # Add to list\n                new_stmts.append(new_stmt)\n        return new_stmts", "response": "Generate statements by expanding members of families and complexes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading an ontology formatted like Eidos from github.", "response": "def update_ontology(ont_url, rdf_path):\n    \"\"\"Load an ontology formatted like Eidos' from github.\"\"\"\n    yaml_root = load_yaml_from_url(ont_url)\n    G = rdf_graph_from_yaml(yaml_root)\n    save_hierarchy(G, rdf_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the YAML object into an RDF Graph object.", "response": "def rdf_graph_from_yaml(yaml_root):\n    \"\"\"Convert the YAML object into an RDF Graph object.\"\"\"\n    G = Graph()\n    for top_entry in yaml_root:\n        assert len(top_entry) == 1\n        node = list(top_entry.keys())[0]\n        build_relations(G, node, top_entry[node], None)\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a YAML object loaded from a YAML file URL.", "response": "def load_yaml_from_url(ont_url):\n    \"\"\"Return a YAML object loaded from a YAML file URL.\"\"\"\n    res = requests.get(ont_url)\n    if res.status_code != 200:\n        raise Exception('Could not load ontology from %s' % ont_url)\n    root = yaml.load(res.content)\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_preprocessed_file(self, infile, pmid, extra_annotations):\n        infile_base = os.path.basename(infile)\n        outfile = os.path.join(self.preprocessed_dir, infile_base)\n        shutil.copyfile(infile, outfile)\n\n        infile_key = os.path.splitext(infile_base)[0]\n\n        self.pmids[infile_key] = pmid\n        self.extra_annotations[infile_key] = extra_annotations", "response": "This method is essentially a mock function to register already preprocessed text files and get an IsiProcessor object that can be passed to the IsiProcessor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef preprocess_plain_text_string(self, text, pmid, extra_annotations):\n        output_file = '%s.txt' % self.next_file_id\n        output_file = os.path.join(self.preprocessed_dir, output_file)\n\n        # Tokenize sentence\n        sentences = nltk.sent_tokenize(text)\n\n        # Write sentences to text file\n        first_sentence = True\n        with codecs.open(output_file, 'w', encoding='utf-8') as f:\n            for sentence in sentences:\n                if not first_sentence:\n                    f.write('\\n')\n                f.write(sentence.rstrip())\n                first_sentence = False\n\n        # Store annotations\n        self.pmids[str(self.next_file_id)] = pmid\n        self.extra_annotations[str(self.next_file_id)] = extra_annotations\n\n        # Increment file id\n        self.next_file_id += 1", "response": "Preprocess a plain text string for use by ISI reader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef preprocess_plain_text_file(self, filename, pmid, extra_annotations):\n        with codecs.open(filename, 'r', encoding='utf-8') as f:\n            content = f.read()\n            self.preprocess_plain_text_string(content, pmid,\n                                              extra_annotations)", "response": "Preprocess a plain text file for use with ISI reder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef preprocess_nxml_file(self, filename, pmid, extra_annotations):\n        # Create a temporary directory\n        tmp_dir = tempfile.mkdtemp('indra_isi_nxml2txt_output')\n\n        # Run nxml2txt\n        if nxml2txt_path is None:\n            logger.error('NXML2TXT_PATH not specified in config file or ' + \n                         'environment variable')\n            return\n        if python2_path is None:\n            logger.error('PYTHON2_PATH not specified in config file or ' + \n                         'environment variable')\n            return\n        else:\n            txt_out = os.path.join(tmp_dir, 'out.txt')\n            so_out = os.path.join(tmp_dir, 'out.so')\n            command = [python2_path,\n                       os.path.join(nxml2txt_path, 'nxml2txt'),\n                       filename,\n                       txt_out,\n                       so_out]\n            ret = subprocess.call(command)\n            if ret != 0:\n                logger.warning('nxml2txt returned non-zero error code')\n\n            with open(txt_out, 'r') as f:\n                txt_content = f.read()\n\n        # Remote temporary directory\n        shutil.rmtree(tmp_dir)\n\n        # We need to remove some common LaTEX commands from the converted text\n        # or the reader will get confused\n        cmd1 = '[^ \\{\\}]+\\{[^\\{\\}]+\\}\\{[^\\{\\}]+\\}'\n        cmd2 = '[^ \\{\\}]+\\{[^\\{\\}]+\\}'\n        txt_content = re.sub(cmd1, '', txt_content)\n        txt_content = re.sub(cmd2, '', txt_content)\n\n        with open('tmp.txt', 'w') as f:\n            f.write(txt_content)\n\n        # Prepocess text extracted from nxml\n        self.preprocess_plain_text_string(txt_content, pmid, extra_annotations)", "response": "This function processes an NXML file for use with the ISI reader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef preprocess_abstract_list(self, abstract_list):\n        for abstract_struct in abstract_list:\n            abs_format = abstract_struct['format']\n            content_type = abstract_struct['text_type']\n            content_zipped = abstract_struct['content']\n            tcid = abstract_struct['tcid']\n            trid = abstract_struct['trid']\n\n            assert(abs_format == 'text')\n            assert(content_type == 'abstract')\n\n            pmid = None  # Don't worry about pmid for now\n            extra_annotations = {'tcid': tcid, 'trid': trid}\n\n            # Uncompress content\n            content = zlib.decompress(content_zipped,\n                                      zlib.MAX_WBITS+16).decode('utf-8')\n\n            self.preprocess_plain_text_string(content, pmid, extra_annotations)", "response": "Preprocess abstracts in database pickle dump format for ISI reader."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_geneways_files(input_folder=data_folder, get_evidence=True):\n    gp = GenewaysProcessor(input_folder, get_evidence)\n    return gp", "response": "Reads in Geneways data and returns a list of INDRA statements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post_update(self, post_id, tag_string=None, rating=None, source=None,\n                    parent_id=None, has_embedded_notes=None,\n                    is_rating_locked=None, is_note_locked=None,\n                    is_status_locked=None):\n        \"\"\"Update a specific post (Requires login).\n\n        Parameters:\n            post_id (int): The id number of the post to update.\n            tag_string (str): A space delimited list of tags.\n            rating (str): The rating for the post. Can be: safe, questionable,\n                          or explicit.\n            source (str): If this is a URL, Danbooru will download the file.\n            parent_id (int): The ID of the parent post.\n            has_embedded_notes (int): Can be 1, 0.\n            is_rating_locked (int): Can be: 0, 1 (Builder+ only).\n            is_note_locked (int): Can be: 0, 1 (Builder+ only).\n            is_status_locked (int): Can be: 0, 1 (Admin only).\n        \"\"\"\n        params = {\n            'post[tag_string]': tag_string,\n            'post[rating]': rating,\n            'ost[source]': source,\n            'post[parent_id]': parent_id,\n            'post[has_embedded_notes]': has_embedded_notes,\n            'post[is_rating_locked]': is_rating_locked,\n            'post[is_note_locked]': is_note_locked,\n            'post[is_status_locked]': is_status_locked\n            }\n        return self._get('posts/{0}.json'.format(post_id), params, 'PUT',\n                         auth=True)", "response": "Updates a specific post."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_revert(self, post_id, version_id):\n        return self._get('posts/{0}/revert.json'.format(post_id),\n                         {'version_id': version_id}, 'PUT', auth=True)", "response": "This function reverts a post to a previous version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to copy notes from another post.", "response": "def post_copy_notes(self, post_id, other_post_id):\n        \"\"\"Function to copy notes (requires login).\n\n        Parameters:\n            post_id (int):\n            other_post_id (int): The id of the post to copy notes to.\n        \"\"\"\n        return self._get('posts/{0}/copy_notes.json'.format(post_id),\n                         {'other_post_id': other_post_id}, 'PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmarks the post as translated.", "response": "def post_mark_translated(self, post_id, check_translation,\n                             partially_translated):\n        \"\"\"Mark post as translated (Requires login) (UNTESTED).\n\n        If you set check_translation and partially_translated to 1 post will\n        be tagged as 'translated_request'\n\n        Parameters:\n            post_id (int):\n            check_translation (int): Can be 0, 1.\n            partially_translated (int): Can be 0, 1\n        \"\"\"\n        param = {\n            'post[check_translation]': check_translation,\n            'post[partially_translated]': partially_translated\n            }\n        return self._get('posts/{0}/mark_as_translated.json'.format(post_id),\n                         param, method='PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nactions lets you vote for a post (Requires login). Danbooru: Post votes/create. Parameters: post_id (int): score (str): Can be: up, down.", "response": "def post_vote(self, post_id, score):\n        \"\"\"Action lets you vote for a post (Requires login).\n        Danbooru: Post votes/create.\n\n        Parameters:\n            post_id (int):\n            score (str): Can be: up, down.\n        \"\"\"\n        return self._get('posts/{0}/votes.json'.format(post_id),\n                         {'score': score}, 'POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post_unvote(self, post_id):\n        return self._get('posts/{0}/unvote.json'.format(post_id),\n                         method='PUT', auth=True)", "response": "This action lets you unvote a user from a post."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions to flag a post.", "response": "def post_flag_list(self, creator_id=None, creator_name=None, post_id=None,\n                       reason_matches=None, is_resolved=None, category=None):\n        \"\"\"Function to flag a post (Requires login).\n\n        Parameters:\n            creator_id (int): The user id of the flag's creator.\n            creator_name (str): The name of the flag's creator.\n            post_id (int): The post id if the flag.\n        \"\"\"\n        params = {\n            'search[creator_id]': creator_id,\n            'search[creator_name]': creator_name,\n            'search[post_id]': post_id,\n            }\n        return self._get('post_flags.json', params, auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to flag a post.", "response": "def post_flag_create(self, post_id, reason):\n        \"\"\"Function to flag a post.\n\n        Parameters:\n            post_id (int): The id of the flagged post.\n            reason (str): The reason of the flagging.\n        \"\"\"\n        params = {'post_flag[post_id]': post_id, 'post_flag[reason]': reason}\n        return self._get('post_flags.json', params, 'POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_appeals_list(self, creator_id=None, creator_name=None,\n                          post_id=None):\n        \"\"\"Function to return list of appeals (Requires login).\n\n        Parameters:\n            creator_id (int): The user id of the appeal's creator.\n            creator_name (str): The name of the appeal's creator.\n            post_id (int): The post id if the appeal.\n        \"\"\"\n        params = {\n            'creator_id': creator_id,\n            'creator_name': creator_name,\n            'post_id': post_id\n            }\n        return self._get('post_appeals.json', params, auth=True)", "response": "Function to return list of appeals. Requires login."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_appeals_create(self, post_id, reason):\n        params = {'post_appeal[post_id]': post_id,\n                  'post_appeal[reason]': reason}\n        return self._get('post_appeals.json', params, 'POST', auth=True)", "response": "Function to create appeals. Requires login."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post_versions_list(self, updater_name=None, updater_id=None,\n                           post_id=None, start_id=None):\n        \"\"\"Get list of post versions.\n\n        Parameters:\n            updater_name (str):\n            updater_id (int):\n            post_id (int):\n            start_id (int):\n        \"\"\"\n        params = {\n            'search[updater_name]': updater_name,\n            'search[updater_id]': updater_id,\n            'search[post_id]': post_id,\n            'search[start_id]': start_id\n            }\n        return self._get('post_versions.json', params)", "response": "Get list of post versions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nundoes post version (Requires login) (UNTESTED). Parameters: version_id (int):", "response": "def post_versions_undo(self, version_id):\n        \"\"\"Undo post version (Requires login) (UNTESTED).\n\n        Parameters:\n            version_id (int):\n        \"\"\"\n        return self._get('post_versions/{0}/undo.json'.format(version_id),\n                         method='PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload_list(self, uploader_id=None, uploader_name=None, source=None):\n        params = {\n            'search[uploader_id]': uploader_id,\n            'search[uploader_name]': uploader_name,\n            'search[source]': source\n            }\n        return self._get('uploads.json', params, auth=True)", "response": "Search and return an uploads list ( Requires login."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to create a new upload.", "response": "def upload_create(self, tags, rating, file_=None, source=None,\n                      parent_id=None):\n        \"\"\"Function to create a new upload (Requires login).\n\n        Parameters:\n            tags (str):\n            rating (str): Can be: `s`, `q`, or `e`. Alternatively, you can\n                          specify `rating:safe`, `rating:questionable`, or\n                          `rating:explicit` in the tag string.\n            file_ (file_path): The file data encoded as a multipart form.\n            source (str): The source URL.\n            parent_id (int): The parent post id.\n\n        Raises:\n            PybooruAPIError: When file_ or source are empty.\n        \"\"\"\n        if file_ or source is not None:\n            params = {\n                'upload[source]': source,\n                'upload[rating]': rating,\n                'upload[parent_id]': parent_id,\n                'upload[tag_string]': tags\n                }\n            file_ = {'upload[file]': open(file_, 'rb')}\n            return self._get('uploads.json', params, 'POST', auth=True,\n                             file_=file_)\n        else:\n            raise PybooruAPIError(\"'file_' or 'source' is required.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef comment_list(self, group_by, limit=None, page=None, body_matches=None,\n                     post_id=None, post_tags_match=None, creator_name=None,\n                     creator_id=None, is_deleted=None):\n        \"\"\"Return a list of comments.\n\n        Parameters:\n            limit (int): How many posts you want to retrieve.\n            page (int): The page number.\n            group_by: Can be 'comment', 'post'. Comment will return recent\n                      comments. Post will return posts that have been recently\n                      commented on.\n            body_matches (str): Body contains the given terms.\n            post_id (int):\n            post_tags_match (str): The comment's post's tags match the\n                                   given terms. Meta-tags not supported.\n            creator_name (str): The name of the creator (exact match).\n            creator_id (int): The user id of the creator.\n            is_deleted (bool): Can be: True, False.\n\n        Raises:\n            PybooruAPIError: When 'group_by' is invalid.\n        \"\"\"\n        params = {\n            'group_by': group_by,\n            'limit': limit,\n            'page': page,\n            'search[body_matches]': body_matches,\n            'search[post_id]': post_id,\n            'search[post_tags_match]': post_tags_match,\n            'search[creator_name]': creator_name,\n            'search[creator_id]': creator_id,\n            'search[is_deleted]': is_deleted\n            }\n        return self._get('comments.json', params)", "response": "Returns a list of comments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nactioning to lets you create a comment (Requires login). Parameters: post_id (int): body (str): do_not_bump_post (bool): Set to 1 if you do not want the post to be bumped to the top of the comment listing.", "response": "def comment_create(self, post_id, body, do_not_bump_post=None):\n        \"\"\"Action to lets you create a comment (Requires login).\n\n        Parameters:\n            post_id (int):\n            body (str):\n            do_not_bump_post (bool): Set to 1 if you do not want the post to be\n                                     bumped to the top of the comment listing.\n        \"\"\"\n        params = {\n            'comment[post_id]': post_id,\n            'comment[body]': body,\n            'comment[do_not_bump_post]': do_not_bump_post\n            }\n        return self._get('comments.json', params, 'POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef comment_update(self, comment_id, body):\n        params = {'comment[body]': body}\n        return self._get('comments/{0}.json'.format(comment_id), params, 'PUT',\n                         auth=True)", "response": "Function to update a comment. Requires login."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef comment_delete(self, comment_id):\n        return self._get('comments/{0}.json'.format(comment_id),\n                         method='DELETE', auth=True)", "response": "Removes a specific comment from the user. Requires login."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef comment_vote(self, comment_id, score):\n        params = {'score': score}\n        return self._get('comments/{0}/votes.json'.format(comment_id), params,\n                         method='POST', auth=True)", "response": "Lets you vote for a comment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nletting you unvote a specific comment.", "response": "def comment_unvote(self, comment_id):\n        \"\"\"Lets you unvote a specific comment (Requires login).\n\n        Parameters:\n            comment_id (int):\n        \"\"\"\n        return self._get('posts/{0}/unvote.json'.format(comment_id),\n                         method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a post from favorites. Requires login.", "response": "def favorite_remove(self, post_id):\n        \"\"\"Remove a post from favorites (Requires login).\n\n        Parameters:\n            post_id (int): Where post_id is the post id.\n        \"\"\"\n        return self._get('favorites/{0}.json'.format(post_id), method='DELETE',\n                         auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of Dmails.", "response": "def dmail_list(self, message_matches=None, to_name=None, to_id=None,\n                   from_name=None, from_id=None, read=None):\n        \"\"\"Return list of Dmails. You can only view dmails you own\n        (Requires login).\n\n        Parameters:\n            message_matches (str): The message body contains the given terms.\n            to_name (str): The recipient's name.\n            to_id (int): The recipient's user id.\n            from_name (str): The sender's name.\n            from_id (int): The sender's user id.\n            read (bool): Can be: true, false.\n        \"\"\"\n        params = {\n            'search[message_matches]': message_matches,\n            'search[to_name]': to_name,\n            'search[to_id]': to_id,\n            'search[from_name]': from_name,\n            'search[from_id]': from_id,\n            'search[read]': read\n            }\n        return self._get('dmails.json', params, auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new dmail.", "response": "def dmail_create(self, to_name, title, body):\n        \"\"\"Create a dmail (Requires login)\n\n        Parameters:\n            to_name (str): The recipient's name.\n            title (str): The title of the message.\n            body (str): The body of the message.\n        \"\"\"\n        params = {\n            'dmail[to_name]': to_name,\n            'dmail[title]': title,\n            'dmail[body]': body\n            }\n        return self._get('dmails.json', params, 'POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dmail_delete(self, dmail_id):\n        return self._get('dmails/{0}.json'.format(dmail_id), method='DELETE',\n                         auth=True)", "response": "Delete a dmail. You can only delete dmails you own (Requires login).\n\n        Parameters:\n            dmail_id (int): where dmail_id is the dmail id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an artist of a list of artists. Parameters: query (str): This field has multiple uses depending on what the query starts with: 'http:desired_url': Search for artist with this URL. 'name:desired_url': Search for artists with the given name as their base name. 'other:other_name': Search for artists with the given name in their other names. 'group:group_name': Search for artists belonging to the group with the given name. 'status:banned': Search for artists that are banned. else Search for the given name in the base name and the other names. artist_id (id): The artist id. creator_name (str): Exact creator name. creator_id (id): Artist creator id. is_active (bool): Can be: true, false is_banned (bool): Can be: true, false empty_only (True): Search for artists that have 0 posts. Can be: true order (str): Can be: name, updated_at.", "response": "def artist_list(self, query=None, artist_id=None, creator_name=None,\n                    creator_id=None, is_active=None, is_banned=None,\n                    empty_only=None, order=None):\n        \"\"\"Get an artist of a list of artists.\n\n        Parameters:\n            query (str):\n                This field has multiple uses depending on what the query starts\n                with:\n                'http:desired_url':\n                    Search for artist with this URL.\n                'name:desired_url':\n                    Search for artists with the given name as their base name.\n                'other:other_name':\n                    Search for artists with the given name in their other\n                    names.\n                'group:group_name':\n                    Search for artists belonging to the group with the given\n                    name.\n                'status:banned':\n                    Search for artists that are banned. else Search for the\n                    given name in the base name and the other names.\n            artist_id (id): The artist id.\n            creator_name (str): Exact creator name.\n            creator_id (id): Artist creator id.\n            is_active (bool): Can be: true, false\n            is_banned (bool): Can be: true, false\n            empty_only (True): Search for artists that have 0 posts. Can be:\n                               true\n            order (str): Can be: name, updated_at.\n        \"\"\"\n        params = {\n            'search[name]': query,\n            'search[id]': artist_id,\n            'search[creator_name]': creator_name,\n            'search[creator_id]': creator_id,\n            'search[is_active]': is_active,\n            'search[is_banned]': is_banned,\n            'search[empty_only]': empty_only,\n            'search[order]': order\n            }\n        return self._get('artists.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to create an artist.", "response": "def artist_create(self, name, other_names_comma=None, group_name=None,\n                      url_string=None, body=None):\n        \"\"\"Function to create an artist (Requires login) (UNTESTED).\n\n        Parameters:\n            name (str):\n            other_names_comma (str): List of alternative names for this\n                                     artist, comma delimited.\n            group_name (str): The name of the group this artist belongs to.\n            url_string (str): List of URLs associated with this artist,\n                              whitespace or newline delimited.\n            body (str): DText that will be used to create a wiki entry at the\n                        same time.\n        \"\"\"\n        params = {\n            'artist[name]': name,\n            'artist[other_names_comma]': other_names_comma,\n            'artist[group_name]': group_name,\n            'artist[url_string]': url_string,\n            'artist[body]': body,\n            }\n        return self.get('artists.json', params, method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef artist_update(self, artist_id, name=None, other_names_comma=None,\n                      group_name=None, url_string=None, body=None):\n        \"\"\"Function to update artists (Requires login) (UNTESTED).\n\n        Parameters:\n            artist_id (str):\n            name (str): Artist name.\n            other_names_comma (str): List of alternative names for this\n                                     artist, comma delimited.\n            group_name (str): The name of the group this artist belongs to.\n            url_string (str): List of URLs associated with this artist,\n                              whitespace or newline delimited.\n            body (str): DText that will be used to create/update a wiki entry\n                        at the same time.\n        \"\"\"\n        params = {\n            'artist[name]': name,\n            'artist[other_names_comma]': other_names_comma,\n            'artist[group_name]': group_name,\n            'artist[url_string]': url_string,\n            'artist[body]': body\n            }\n        return self .get('artists/{0}.json'.format(artist_id), params,\n                         method='PUT', auth=True)", "response": "Function to update artists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef artist_delete(self, artist_id):\n        return self._get('artists/{0}.json'.format(artist_id), method='DELETE',\n                         auth=True)", "response": "This action lets you delete an artist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef artist_undelete(self, artist_id):\n        return self._get('artists/{0}/undelete.json'.format(artist_id),\n                         method='POST', auth=True)", "response": "Lets you undelete artist (Requires login) (UNTESTED) (Only Builder+).\n\n        Parameters:\n            artist_id (int):"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef artist_revert(self, artist_id, version_id):\n        params = {'version_id': version_id}\n        return self._get('artists/{0}/revert.json'.format(artist_id), params,\n                         method='PUT', auth=True)", "response": "Revert an artist (Requires login) (UNTESTED).\n\n        Parameters:\n            artist_id (int): The artist id.\n            version_id (int): The artist version id to revert to."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting list of artist versions.", "response": "def artist_versions(self, name=None, updater_name=None, updater_id=None,\n                        artist_id=None, is_active=None, is_banned=None,\n                        order=None):\n        \"\"\"Get list of artist versions (Requires login).\n\n        Parameters:\n            name (str):\n            updater_name (str):\n            updater_id (int):\n            artist_id (int):\n            is_active (bool): Can be: True, False.\n            is_banned (bool): Can be: True, False.\n            order (str): Can be: name (Defaults to ID)\n        \"\"\"\n        params = {\n            'search[name]': name,\n            'search[updater_name]': updater_name,\n            'search[updater_id]': updater_id,\n            'search[artist_id]': artist_id,\n            'search[is_active]': is_active,\n            'search[is_banned]': is_banned,\n            'search[order]': order\n            }\n        return self._get('artist_versions.json', params, auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting artist commentary. Parameters: text_matches (str): post_id (int): post_tags_match (str): The commentary's post's tags match the giventerms. Meta-tags not supported. original_present (str): Can be: yes, no. translated_present (str): Can be: yes, no.", "response": "def artist_commentary_list(self, text_matches=None, post_id=None,\n                               post_tags_match=None, original_present=None,\n                               translated_present=None):\n        \"\"\"list artist commentary.\n\n        Parameters:\n            text_matches (str):\n            post_id (int):\n            post_tags_match (str): The commentary's post's tags match the\n                                   giventerms. Meta-tags not supported.\n            original_present (str): Can be: yes, no.\n            translated_present (str): Can be: yes, no.\n        \"\"\"\n        params = {\n            'search[text_matches]': text_matches,\n            'search[post_id]': post_id,\n            'search[post_tags_match]': post_tags_match,\n            'search[original_present]': original_present,\n            'search[translated_present]': translated_present\n            }\n        return self._get('artist_commentaries.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating or updates an artist commentary.", "response": "def artist_commentary_create_update(self, post_id, original_title,\n                                        original_description, translated_title,\n                                        translated_description):\n        \"\"\"Create or update artist commentary (Requires login) (UNTESTED).\n\n        Parameters:\n            post_id (int): Post id.\n            original_title (str): Original title.\n            original_description (str): Original description.\n            translated_title (str): Translated title.\n            translated_description (str): Translated description.\n        \"\"\"\n        params = {\n            'artist_commentary[post_id]': post_id,\n            'artist_commentary[original_title]': original_title,\n            'artist_commentary[original_description]': original_description,\n            'artist_commentary[translated_title]': translated_title,\n            'artist_commentary[translated_description]': translated_description\n            }\n        return self._get('artist_commentaries/create_or_update.json', params,\n                         method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrevert an artist commentary.", "response": "def artist_commentary_revert(self, id_, version_id):\n        \"\"\"Revert artist commentary (Requires login) (UNTESTED).\n\n        Parameters:\n            id_ (int): The artist commentary id.\n            version_id (int): The artist commentary version id to\n                              revert to.\n        \"\"\"\n        params = {'version_id': version_id}\n        return self._get('artist_commentaries/{0}/revert.json'.format(id_),\n                         params, method='PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef artist_commentary_versions(self, post_id, updater_id):\n        params = {'search[updater_id]': updater_id, 'search[post_id]': post_id}\n        return self._get('artist_commentary_versions.json', params)", "response": "Get a list of artist commentary versions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of notes.", "response": "def note_list(self, body_matches=None, post_id=None, post_tags_match=None,\n                  creator_name=None, creator_id=None, is_active=None):\n        \"\"\"Return list of notes.\n\n        Parameters:\n            body_matches (str): The note's body matches the given terms.\n            post_id (int): A specific post.\n            post_tags_match (str): The note's post's tags match the given terms.\n            creator_name (str): The creator's name. Exact match.\n            creator_id (int): The creator's user id.\n            is_active (bool): Can be: True, False.\n        \"\"\"\n        params = {\n            'search[body_matches]': body_matches,\n            'search[post_id]': post_id,\n            'search[post_tags_match]': post_tags_match,\n            'search[creator_name]': creator_name,\n            'search[creator_id]': creator_id,\n            'search[is_active]': is_active\n            }\n        return self._get('notes.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to create a note in a specific post.", "response": "def note_create(self, post_id, coor_x, coor_y, width, height, body):\n        \"\"\"Function to create a note (Requires login) (UNTESTED).\n\n        Parameters:\n            post_id (int):\n            coor_x (int): The x coordinates of the note in pixels,\n                          with respect to the top-left corner of the image.\n            coor_y (int): The y coordinates of the note in pixels,\n                          with respect to the top-left corner of the image.\n            width (int): The width of the note in pixels.\n            height (int): The height of the note in pixels.\n            body (str): The body of the note.\n        \"\"\"\n        params = {\n            'note[post_id]': post_id,\n            'note[x]': coor_x,\n            'note[y]': coor_y,\n            'note[width]': width,\n            'note[height]': height,\n            'note[body]': body\n            }\n        return self._get('notes.json', params, method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to update a note in the database.", "response": "def note_update(self, note_id, coor_x=None, coor_y=None, width=None,\n                    height=None, body=None):\n        \"\"\"Function to update a note (Requires login) (UNTESTED).\n\n        Parameters:\n            note_id (int): Where note_id is the note id.\n            coor_x (int): The x coordinates of the note in pixels,\n                          with respect to the top-left corner of the image.\n            coor_y (int): The y coordinates of the note in pixels,\n                          with respect to the top-left corner of the image.\n            width (int): The width of the note in pixels.\n            height (int): The height of the note in pixels.\n            body (str): The body of the note.\n        \"\"\"\n        params = {\n            'note[x]': coor_x,\n            'note[y]': coor_y,\n            'note[width]': width,\n            'note[height]': height,\n            'note[body]': body\n            }\n        return self._get('notes/{0}.jso'.format(note_id), params, method='PUT',\n                         auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef note_delete(self, note_id):\n        return self._get('notes/{0}.json'.format(note_id), method='DELETE',\n                         auth=True)", "response": "Delete a specific note."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to revert a specific note (Requires login) (UNTESTED). Parameters: note_id (int): Where note_id is the note id. version_id (int): The note version id to revert to.", "response": "def note_revert(self, note_id, version_id):\n        \"\"\"Function to revert a specific note (Requires login) (UNTESTED).\n\n        Parameters:\n            note_id (int): Where note_id is the note id.\n            version_id (int): The note version id to revert to.\n        \"\"\"\n        return self._get('notes/{0}/revert.json'.format(note_id),\n                         {'version_id': version_id}, method='PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef note_versions(self, updater_id=None, post_id=None, note_id=None):\n        params = {\n            'search[updater_id]': updater_id,\n            'search[post_id]': post_id,\n            'search[note_id]': note_id\n            }\n        return self._get('note_versions.json', params)", "response": "Get list of note versions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to get a list of users or a specific user.", "response": "def user_list(self, name=None, name_matches=None, min_level=None,\n                  max_level=None, level=None, user_id=None, order=None):\n        \"\"\"Function to get a list of users or a specific user.\n\n        Levels:\n            Users have a number attribute called level representing their role.\n            The current levels are:\n\n            Member 20, Gold 30, Platinum 31, Builder 32, Contributor 33,\n            Janitor 35, Moderator 40 and Admin 50.\n\n        Parameters:\n            name (str): Supports patterns.\n            name_matches (str): Same functionality as name.\n            min_level (int): Minimum level (see section on levels).\n            max_level (int): Maximum level (see section on levels).\n            level (int): Current level (see section on levels).\n            user_id (int): The user id.\n            order (str): Can be: 'name', 'post_upload_count', 'note_count',\n                         'post_update_count', 'date'.\n        \"\"\"\n        params = {\n            'search[name]': name,\n            'search[name_matches]': name_matches,\n            'search[min_level]': min_level,\n            'search[max_level]': max_level,\n            'search[level]': level,\n            'search[id]': user_id,\n            'search[order]': order\n            }\n        return self._get('users.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of pools.", "response": "def pool_list(self, name_matches=None, pool_ids=None, category=None,\n                  description_matches=None, creator_name=None, creator_id=None,\n                  is_deleted=None, is_active=None, order=None):\n        \"\"\"Get a list of pools.\n\n        Parameters:\n            name_matches (str):\n            pool_ids (str): Can search for multiple ID's at once, separated by\n                           commas.\n            description_matches (str):\n            creator_name (str):\n            creator_id (int):\n            is_active (bool): Can be: true, false.\n            is_deleted (bool): Can be: True, False.\n            order (str): Can be: name, created_at, post_count, date.\n            category (str): Can be: series, collection.\n        \"\"\"\n        params = {\n            'search[name_matches]': name_matches,\n            'search[id]': pool_ids,\n            'search[description_matches]': description_matches,\n            'search[creator_name]': creator_name,\n            'search[creator_id]': creator_id,\n            'search[is_active]': is_active,\n            'search[is_deleted]': is_deleted,\n            'search[order]': order,\n            'search[category]': category\n            }\n        return self._get('pools.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to create a pool", "response": "def pool_create(self, name, description, category):\n        \"\"\"Function to create a pool (Requires login) (UNTESTED).\n\n        Parameters:\n            name (str): Pool name.\n            description (str): Pool description.\n            category (str): Can be: series, collection.\n        \"\"\"\n        params = {\n            'pool[name]': name,\n            'pool[description]': description,\n            'pool[category]': category\n            }\n        return self._get('pools.json', params, method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pool_update(self, pool_id, name=None, description=None, post_ids=None,\n                    is_active=None, category=None):\n        \"\"\"Update a pool (Requires login) (UNTESTED).\n\n        Parameters:\n            pool_id (int): Where pool_id is the pool id.\n            name (str):\n            description (str):\n            post_ids (str): List of space delimited post ids.\n            is_active (int): Can be: 1, 0.\n            category (str): Can be: series, collection.\n        \"\"\"\n        params = {\n            'pool[name]': name,\n            'pool[description]': description,\n            'pool[post_ids]': post_ids,\n            'pool[is_active]': is_active,\n            'pool[category]': category\n            }\n        return self._get('pools/{0}.json'.format(pool_id), params,\n                         method='PUT', auth=True)", "response": "Update a pool (Requires login) (UNTESTED).\n\n        Parameters:\n            pool_id (int): Where pool_id is the pool id.\n            name (str):\n            description (str):\n            post_ids (str): List of space delimited post ids.\n            is_active (int): Can be: 1, 0.\n            category (str): Can be: series, collection."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a pool (Requires login) (UNTESTED) (Moderator+). Parameters: pool_id (int): Where pool_id is the pool id.", "response": "def pool_delete(self, pool_id):\n        \"\"\"Delete a pool (Requires login) (UNTESTED) (Moderator+).\n\n        Parameters:\n            pool_id (int): Where pool_id is the pool id.\n        \"\"\"\n        return self._get('pools/{0}.json'.format(pool_id), method='DELETE',\n                         auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pool_undelete(self, pool_id):\n        return self._get('pools/{0}/undelete.json'.format(pool_id),\n                         method='POST', auth=True)", "response": "Undelete a specific poool."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pool_revert(self, pool_id, version_id):\n        return self._get('pools/{0}/revert.json'.format(pool_id),\n                         {'version_id': version_id}, method='PUT', auth=True)", "response": "This function reverts a specific pool."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting list of pool versions.", "response": "def pool_versions(self, updater_id=None, updater_name=None, pool_id=None):\n        \"\"\"Get list of pool versions.\n\n        Parameters:\n            updater_id (int):\n            updater_name (str):\n            pool_id (int):\n        \"\"\"\n        params = {\n            'search[updater_id]': updater_id,\n            'search[updater_name]': updater_name,\n            'search[pool_id]': pool_id\n            }\n        return self._get('pool_versions.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a list of tags.", "response": "def tag_list(self, name_matches=None, name=None, category=None,\n                 hide_empty=None, has_wiki=None, has_artist=None, order=None):\n        \"\"\"Get a list of tags.\n\n        Parameters:\n            name_matches (str): Can be: part or full name.\n            name (str): Allows searching for multiple tags with exact given\n                        names, separated by commas. e.g.\n                        search[name]=touhou,original,k-on! would return the\n                        three listed tags.\n            category (str): Can be: 0, 1, 3, 4 (general, artist, copyright,\n                            character respectively).\n            hide_empty (str): Can be: yes, no. Excludes tags with 0 posts\n                              when \"yes\".\n            has_wiki (str): Can be: yes, no.\n            has_artist (str): Can be: yes, no.\n            order (str): Can be: name, date, count.\n        \"\"\"\n        params = {\n            'search[name_matches]': name_matches,\n            'search[name]': name,\n            'search[category]': category,\n            'search[hide_empty]': hide_empty,\n            'search[has_wiki]': has_wiki,\n            'search[has_artist]': has_artist,\n            'search[order]': order\n            }\n        return self._get('tags.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nletting you update a tag.", "response": "def tag_update(self, tag_id, category):\n        \"\"\"Lets you update a tag (Requires login) (UNTESTED).\n\n        Parameters:\n            tag_id (int):\n            category (str): Can be: 0, 1, 3, 4 (general, artist, copyright,\n                            character respectively).\n        \"\"\"\n        param = {'tag[category]': category}\n        return self._get('pools/{0}.json'.format(tag_id), param, method='PUT',\n                         auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tag_aliases(self, name_matches=None, antecedent_name=None,\n                    tag_id=None):\n        \"\"\"Get tags aliases.\n\n        Parameters:\n            name_matches (str): Match antecedent or consequent name.\n            antecedent_name (str): Match antecedent name (exact match).\n            tag_id (int): The tag alias id.\n        \"\"\"\n        params = {\n            'search[name_matches]': name_matches,\n            'search[antecedent_name]': antecedent_name,\n            'search[id]': tag_id\n            }\n        return self._get('tag_aliases.json', params)", "response": "Get tags aliases.\n\n        Parameters:\n            name_matches (str): Match antecedent or consequent name.\n            antecedent_name (str): Match antecedent name (exact match).\n            tag_id (int): The tag alias id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets tags implications. Parameters: name_matches (str): Match antecedent or consequent name. antecedent_name (str): Match antecedent name (exact match). tag_id (int): Tag implication id.", "response": "def tag_implications(self, name_matches=None, antecedent_name=None,\n                         tag_id=None):\n        \"\"\"Get tags implications.\n\n        Parameters:\n            name_matches (str): Match antecedent or consequent name.\n            antecedent_name (str): Match antecedent name (exact match).\n            tag_id (int): Tag implication id.\n        \"\"\"\n        params = {\n            'search[name_matches]': name_matches,\n            'search[antecedent_name]': antecedent_name,\n            'search[id]': tag_id\n            }\n        return self._get('tag_implications.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets related tags. Parameters: query (str): The tag to find the related tags for. category (str): If specified, show only tags of a specific category. Can be: General 0, Artist 1, Copyright 3 and Character 4.", "response": "def tag_related(self, query, category=None):\n        \"\"\"Get related tags.\n\n        Parameters:\n            query (str): The tag to find the related tags for.\n            category (str): If specified, show only tags of a specific\n                            category. Can be: General 0, Artist 1, Copyright\n                            3 and Character 4.\n        \"\"\"\n        params = {'query': query, 'category': category}\n        return self._get('related_tag.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to retrieves a list of every wiki page.", "response": "def wiki_list(self, title=None, creator_id=None, body_matches=None,\n                  other_names_match=None, creator_name=None, hide_deleted=None,\n                  other_names_present=None, order=None):\n        \"\"\"Function to retrieves a list of every wiki page.\n\n        Parameters:\n            title (str): Page title.\n            creator_id (int): Creator id.\n            body_matches (str): Page content.\n            other_names_match (str): Other names.\n            creator_name (str): Creator name.\n            hide_deleted (str): Can be: yes, no.\n            other_names_present (str): Can be: yes, no.\n            order (str): Can be: date, title.\n        \"\"\"\n        params = {\n            'search[title]': title,\n            'search[creator_id]': creator_id,\n            'search[body_matches]': body_matches,\n            'search[other_names_match]': other_names_match,\n            'search[creator_name]': creator_name,\n            'search[hide_deleted]': hide_deleted,\n            'search[other_names_present]': other_names_present,\n            'search[order]': order\n            }\n        return self._get('wiki_pages.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wiki_create(self, title, body, other_names=None):\n        params = {\n            'wiki_page[title]': title,\n            'wiki_page[body]': body,\n            'wiki_page[other_names]': other_names\n            }\n        return self._get('wiki_pages.json', params, method='POST', auth=True)", "response": "This action lets you create a wiki page."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wiki_update(self, page_id, title=None, body=None,\n                    other_names=None, is_locked=None, is_deleted=None):\n        \"\"\"Action to lets you update a wiki page (Requires login) (UNTESTED).\n\n        Parameters:\n            page_id (int): Whre page_id is the wiki page id.\n            title (str): Page title.\n            body (str): Page content.\n            other_names (str): Other names.\n            is_locked (int): Can be: 0, 1 (Builder+).\n            is_deleted (int): Can be: 0, 1 (Builder+).\n        \"\"\"\n        params = {\n            'wiki_page[title]': title,\n            'wiki_page[body]': body,\n            'wiki_page[other_names]': other_names\n            }\n        return self._get('wiki_pages/{0}.json'.format(page_id), params,\n                         method='PUT', auth=True)", "response": "This action lets you update a wiki page."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wiki_delete(self, page_id):\n        return self._get('wiki_pages/{0}.json'.format(page_id), auth=True,\n                         method='DELETE')", "response": "Delete a specific wiki page"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreverting wiki page to a previeous version.", "response": "def wiki_revert(self, wiki_page_id, version_id):\n        \"\"\"Revert page to a previeous version (Requires login) (UNTESTED).\n\n        Parameters:\n            wiki_page_id (int): Where page_id is the wiki page id.\n            version_id (int):\n        \"\"\"\n        return self._get('wiki_pages/{0}/revert.json'.format(wiki_page_id),\n                         {'version_id': version_id}, method='PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wiki_versions_list(self, page_id, updater_id):\n        params = {\n            'earch[updater_id]': updater_id,\n            'search[wiki_page_id]': page_id\n            }\n        return self._get('wiki_page_versions.json', params)", "response": "Return a list of wiki page versions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction to get forum topics.", "response": "def forum_topic_list(self, title_matches=None, title=None,\n                         category_id=None):\n        \"\"\"Function to get forum topics.\n\n        Parameters:\n            title_matches (str): Search body for the given terms.\n            title (str): Exact title match.\n            category_id (int): Can be: 0, 1, 2 (General, Tags, Bugs & Features\n                               respectively).\n        \"\"\"\n        params = {\n            'search[title_matches]': title_matches,\n            'search[title]': title,\n            'search[category_id]': category_id\n            }\n        return self._get('forum_topics.json', params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions to create topic.", "response": "def forum_topic_create(self, title, body, category=None):\n        \"\"\"Function to create topic (Requires login) (UNTESTED).\n\n        Parameters:\n            title (str): topic title.\n            body (str): Message of the initial post.\n            category (str): Can be: 0, 1, 2 (General, Tags, Bugs & Features\n                            respectively).\n        \"\"\"\n        params = {\n            'forum_topic[title]': title,\n            'forum_topic[original_post_attributes][body]': body,\n            'forum_topic[category_id]': category\n            }\n        return self._get('forum_topics.json', params, method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef forum_topic_update(self, topic_id, title=None, category=None):\n        params = {\n            'forum_topic[title]': title,\n            'forum_topic[category_id]': category\n            }\n        return self._get('forum_topics/{0}.json'.format(topic_id), params,\n                         method='PUT', auth=True)", "response": "Update a specific topic."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a topic (Login Requires) (Moderator+) (UNTESTED). Parameters: topic_id (int): Where topic_id is the topic id.", "response": "def forum_topic_delete(self, topic_id):\n        \"\"\"Delete a topic (Login Requires) (Moderator+) (UNTESTED).\n\n        Parameters:\n            topic_id (int): Where topic_id is the topic id.\n        \"\"\"\n        return self._get('forum_topics/{0}.json'.format(topic_id),\n                         method='DELETE', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef forum_topic_undelete(self, topic_id):\n        return self._get('forum_topics/{0}/undelete.json'.format(topic_id),\n                         method='POST', auth=True)", "response": "Un delete a topic (Login requries) (Moderator+) (UNTESTED).\n\n        Parameters:\n            topic_id (int): Where topic_id is the topic id."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forum_post_list(self, creator_id=None, creator_name=None,\n                        topic_id=None, topic_title_matches=None,\n                        topic_category_id=None, body_matches=None):\n        \"\"\"Return a list of forum posts.\n\n        Parameters:\n            creator_id (int):\n            creator_name (str):\n            topic_id (int):\n            topic_title_matches (str):\n            topic_category_id (int): Can be: 0, 1, 2 (General, Tags, Bugs &\n                                     Features respectively).\n            body_matches (str): Can be part of the post content.\n        \"\"\"\n        params = {\n            'search[creator_id]': creator_id,\n            'search[creator_name]': creator_name,\n            'search[topic_id]': topic_id,\n            'search[topic_title_matches]': topic_title_matches,\n            'search[topic_category_id]': topic_category_id,\n            'search[body_matches]': body_matches\n            }\n        return self._get('forum_posts.json', params)", "response": "Returns a list of forum posts."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a forum post.", "response": "def forum_post_create(self, topic_id, body):\n        \"\"\"Create a forum post (Requires login).\n\n        Parameters:\n            topic_id (int):\n            body (str): Post content.\n        \"\"\"\n        params = {'forum_post[topic_id]': topic_id, 'forum_post[body]': body}\n        return self._get('forum_posts.json', params, method='POST', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate a specific forum post.", "response": "def forum_post_update(self, topic_id, body):\n        \"\"\"Update a specific forum post (Requries login)(Moderator+)(UNTESTED).\n\n        Parameters:\n            post_id (int): Forum topic id.\n            body (str): Post content.\n        \"\"\"\n        params = {'forum_post[body]': body}\n        return self._get('forum_posts/{0}.json'.format(topic_id), params,\n                         method='PUT', auth=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forum_post_delete(self, post_id):\n        return self._get('forum_posts/{0}.json'.format(post_id),\n                         method='DELETE', auth=True)", "response": "Delete a specific forum post."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forum_post_undelete(self, post_id):\n        return self._get('forum_posts/{0}/undelete.json'.format(post_id),\n                         method='POST', auth=True)", "response": "Undelete a specific forum post."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef site_name(self, site_name):\n        if site_name in SITE_LIST:\n            self.__site_name = site_name\n            self.__site_url = SITE_LIST[site_name]['url']\n        else:\n            raise PybooruError(\n                \"The 'site_name' is not valid, specify a valid 'site_name'.\")", "response": "Function that sets and checks the site name and url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _request(self, url, api_call, request_args, method='GET'):\n        try:\n            if method != 'GET':\n                # Reset content-type for data encoded as a multipart form\n                self.client.headers.update({'content-type': None})\n\n            response = self.client.request(method, url, **request_args)\n\n            self.last_call.update({\n                'API': api_call,\n                'url': response.url,\n                'status_code': response.status_code,\n                'status': self._get_status(response.status_code),\n                'headers': response.headers\n                })\n\n            if response.status_code in (200, 201, 202, 204):\n                return response.json()\n            raise PybooruHTTPError(\"In _request\", response.status_code,\n                                   response.url)\n        except requests.exceptions.Timeout:\n            raise PybooruError(\"Timeout! url: {0}\".format(response.url))\n        except ValueError as e:\n            raise PybooruError(\"JSON Error: {0} in line {1} column {2}\".format(\n                e.msg, e.lineno, e.colno))", "response": "Function to request and return JSON data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get(self, api_call, params=None, method='GET', auth=False,\n             file_=None):\n        \"\"\"Function to preapre API call.\n\n        Parameters:\n            api_call (str): API function to be called.\n            params (str): API function parameters.\n            method (str): (Defauld: GET) HTTP method (GET, POST, PUT or\n                           DELETE)\n            file_ (file): File to upload (only uploads).\n\n        Raise:\n            PybooruError: When 'username' or 'api_key' are not set.\n        \"\"\"\n        url = \"{0}/{1}\".format(self.site_url, api_call)\n\n        if method == 'GET':\n            request_args = {'params': params}\n        else:\n            request_args = {'data': params, 'files': file_}\n\n        # Adds auth. Also adds auth if username and api_key are specified\n        # Members+ have less restrictions\n        if auth or (self.username and self.api_key):\n            if self.username and self.api_key:\n                request_args['auth'] = (self.username, self.api_key)\n            else:\n                raise PybooruError(\"'username' and 'api_key' attribute of \"\n                                   \"Danbooru are required.\")\n\n        # Do call\n        return self._request(url, api_call, request_args, method)", "response": "Internal method for getting a specific object from the API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post_create(self, tags, file_=None, rating=None, source=None,\n                    rating_locked=None, note_locked=None, parent_id=None,\n                    md5=None):\n        \"\"\"Function to create a new post (Requires login).\n\n        There are only two mandatory fields: you need to supply the\n        'tags', and you need to supply the 'file_', either through a\n        multipart form or through a source URL (Requires login) (UNTESTED).\n\n        Parameters:\n            tags (str): A space delimited list of tags.\n            file_ (str): The file data encoded as a multipart form. Path of\n                         content.\n            rating (str): The rating for the post. Can be: safe, questionable,\n                          or explicit.\n            source (str): If this is a URL, Moebooru will download the file.\n            rating_locked (bool): Set to True to prevent others from changing\n                                  the rating.\n            note_locked (bool): Set to True to prevent others from adding notes.\n            parent_id (int): The ID of the parent post.\n            md5 (str): Supply an MD5 if you want Moebooru to verify the file\n                       after uploading. If the MD5 doesn't match, the post is\n                       destroyed.\n\n        Raises:\n            PybooruAPIError: When file or source are empty.\n        \"\"\"\n        if file_ or source is not None:\n            params = {\n                'post[tags]': tags,\n                'post[source]': source,\n                'post[rating]': rating,\n                'post[is_rating_locked]': rating_locked,\n                'post[is_note_locked]': note_locked,\n                'post[parent_id]': parent_id,\n                'md5': md5}\n            file_ = {'post[file]': open(file_, 'rb')}\n            return self._get('post/create', params, 'POST', file_)\n        else:\n            raise PybooruAPIError(\"'file_' or 'source' is required.\")", "response": "Function to create a new post."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates a specific post.", "response": "def post_update(self, post_id, tags=None, file_=None, rating=None,\n                    source=None, is_rating_locked=None, is_note_locked=None,\n                    parent_id=None):\n        \"\"\"Update a specific post.\n\n        Only the 'post_id' parameter is required. Leave the other parameters\n        blank if you don't want to change them (Requires login).\n\n        Parameters:\n            post_id (int): The id number of the post to update.\n            tags (str): A space delimited list of tags. Specify previous tags.\n            file_ (str): The file data ENCODED as a multipart form.\n            rating (str): The rating for the post. Can be: safe, questionable,\n                          or explicit.\n            source (str): If this is a URL, Moebooru will download the file.\n            rating_locked (bool): Set to True to prevent others from changing\n                                  the rating.\n            note_locked (bool): Set to True to prevent others from adding\n                                notes.\n            parent_id (int): The ID of the parent post.\n        \"\"\"\n        params = {\n            'id': post_id,\n            'post[tags]': tags,\n            'post[rating]': rating,\n            'post[source]': source,\n            'post[is_rating_locked]': is_rating_locked,\n            'post[is_note_locked]': is_note_locked,\n            'post[parent_id]': parent_id\n            }\n        if file_ is not None:\n            file_ = {'post[file]': open(file_, 'rb')}\n            return self._get('post/update', params, 'PUT', file_)\n        else:\n            return self._get('post/update', params, 'PUT')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nactions lets you vote for a post (Requires login). Parameters: post_id (int): The post id. score (int): * 0: No voted or Remove vote. * 1: Good. * 2: Great. * 3: Favorite, add post to favorites. Raises: PybooruAPIError: When score is > 3.", "response": "def post_vote(self, post_id, score):\n        \"\"\"Action lets you vote for a post (Requires login).\n\n        Parameters:\n            post_id (int): The post id.\n            score (int):\n                * 0: No voted or Remove vote.\n                * 1: Good.\n                * 2: Great.\n                * 3: Favorite, add post to favorites.\n\n        Raises:\n            PybooruAPIError: When score is > 3.\n        \"\"\"\n        if score <= 3 and score >= 0:\n            params = {'id': post_id, 'score': score}\n            return self._get('post/vote', params, 'POST')\n        else:\n            raise PybooruAPIError(\"Value of 'score' only can be 0, 1, 2 or 3.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nactions to lets you update tag (Requires login) (UNTESTED). Parameters: name (str): The name of the tag to update. tag_type (int): * General: 0. * artist: 1. * copyright: 3. * character: 4. is_ambiguous (int): Whether or not this tag is ambiguous. Use 1 for True and 0 for False.", "response": "def tag_update(self, name=None, tag_type=None, is_ambiguous=None):\n        \"\"\"Action to lets you update tag (Requires login) (UNTESTED).\n\n        Parameters:\n            name (str): The name of the tag to update.\n            tag_type (int):\n                * General: 0.\n                * artist: 1.\n                * copyright: 3.\n                * character: 4.\n            is_ambiguous (int): Whether or not this tag is ambiguous. Use 1\n                                for True and 0 for False.\n        \"\"\"\n        params = {\n            'name': name,\n            'tag[tag_type]': tag_type,\n            'tag[is_ambiguous]': is_ambiguous\n            }\n        return self._get('tag/update', params, 'PUT')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions to create an artist.", "response": "def artist_create(self, name, urls=None, alias=None, group=None):\n        \"\"\"Function to create an artist (Requires login) (UNTESTED).\n\n        Parameters:\n            name (str): The artist's name.\n            urls (str): A list of URLs associated with the artist, whitespace\n                        delimited.\n            alias (str): The artist that this artist is an alias for. Simply\n                         enter the alias artist's name.\n            group (str): The group or cicle that this artist is a member of.\n                         Simply:param  enter the group's name.\n        \"\"\"\n        params = {\n            'artist[name]': name,\n            'artist[urls]': urls,\n            'artist[alias]': alias,\n            'artist[group]': group\n            }\n        return self._get('artist/create', params, method='POST')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to update artists.", "response": "def artist_update(self, artist_id, name=None, urls=None, alias=None,\n                      group=None):\n        \"\"\"Function to update artists (Requires Login) (UNTESTED).\n\n        Only the artist_id parameter is required. The other parameters are\n        optional.\n\n        Parameters:\n            artist_id (int): The id of thr artist to update (Type: INT).\n            name (str): The artist's name.\n            urls (str): A list of URLs associated with the artist, whitespace\n                        delimited.\n            alias (str): The artist that this artist is an alias for. Simply\n                         enter the alias artist's name.\n            group (str): The group or cicle that this artist is a member of.\n                         Simply enter the group's name.\n        \"\"\"\n        params = {\n            'id': artist_id,\n            'artist[name]': name,\n            'artist[urls]': urls,\n            'artist[alias]': alias,\n            'artist[group]': group\n            }\n        return self._get('artist/update', params, method='PUT')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef comment_create(self, post_id, comment_body, anonymous=None):\n        params = {\n            'comment[post_id]': post_id,\n            'comment[body]': comment_body,\n            'comment[anonymous]': anonymous\n            }\n        return self._get('comment/create', params, method='POST')", "response": "This action lets you create a comment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naction to lets you create a wiki page (Requires login) (UNTESTED). Parameters: title (str): The title of the wiki page. body (str): The body of the wiki page.", "response": "def wiki_create(self, title, body):\n        \"\"\"Action to lets you create a wiki page (Requires login) (UNTESTED).\n\n        Parameters:\n            title (str): The title of the wiki page.\n            body (str): The body of the wiki page.\n        \"\"\"\n        params = {'wiki_page[title]': title, 'wiki_page[body]': body}\n        return self._get('wiki/create', params, method='POST')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wiki_update(self, title, new_title=None, page_body=None):\n        params = {\n            'title': title,\n            'wiki_page[title]': new_title,\n            'wiki_page[body]': page_body\n            }\n        return self._get('wiki/update', params, method='PUT')", "response": "This action lets you update a wiki page."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef note_revert(self, note_id, version):\n        params = {'id': note_id, 'version': version}\n        return self._get('note/revert', params, method='PUT')", "response": "Function to revert a specific note."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pool_update(self, pool_id, name=None, is_public=None,\n                    description=None):\n        \"\"\"Function to update a pool (Requires login) (UNTESTED).\n\n        Parameters:\n            pool_id (int): The pool id number.\n            name (str): The name.\n            is_public (int): 1 or 0, whether or not the pool is public.\n            description (str): A description of the pool.\n        \"\"\"\n        params = {\n            'id': pool_id,\n            'pool[name]': name,\n            'pool[is_public]': is_public,\n            'pool[description]': description\n            }\n        return self._get('pool/update', params, method='PUT')", "response": "Function to update a pool."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pool_create(self, name, description, is_public):\n        params = {'pool[name]': name, 'pool[description]': description,\n                  'pool[is_public]': is_public}\n        return self._get('pool/create', params, method='POST')", "response": "Function to create a pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef site_name(self, site_name):\n        # Set base class property site_name\n        _Pybooru.site_name.fset(self, site_name)\n\n        if ('api_version' and 'hashed_string') in SITE_LIST[site_name]:\n            self.api_version = SITE_LIST[site_name]['api_version']\n            self.hash_string = SITE_LIST[site_name]['hashed_string']", "response": "Sets api_version and hash_string properties based on site_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild url for a base API call.", "response": "def _build_url(self, api_call):\n        \"\"\"Build request url.\n\n        Parameters:\n            api_call (str): Base API Call.\n\n        Returns:\n            Complete url (str).\n        \"\"\"\n        if self.api_version in ('1.13.0', '1.13.0+update.1', '1.13.0+update.2'):\n            if '/' not in api_call:\n                return \"{0}/{1}/index.json\".format(self.site_url, api_call)\n        return \"{0}/{1}.json\".format(self.site_url, api_call)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build_hash_string(self):\n        # Build AUTENTICATION hash_string\n        # Check if hash_string exists\n        if self.site_name in SITE_LIST or self.hash_string:\n            if self.username and self.password:\n                try:\n                    hash_string = self.hash_string.format(self.password)\n                except TypeError:\n                    raise PybooruError(\"Pybooru can't add 'password' \"\n                                       \"to 'hash_string'\")\n                # encrypt hashed_string to SHA1 and return hexdigest string\n                self.password_hash = hashlib.sha1(\n                    hash_string.encode('utf-8')).hexdigest()\n            else:\n                raise PybooruError(\"Specify the 'username' and 'password' \"\n                                   \"parameters of the Pybooru object, for \"\n                                   \"setting 'password_hash' attribute.\")\n        else:\n            raise PybooruError(\n                \"Specify the 'hash_string' parameter of the Pybooru\"\n                \" object, for the functions that requires login.\")", "response": "Function for building password hash string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get(self, api_call, params, method='GET', file_=None):\n        url = self._build_url(api_call)\n\n        if method == 'GET':\n            request_args = {'params': params}\n        else:\n            if self.password_hash is None:\n                self._build_hash_string()\n\n            # Set login\n            params['login'] = self.username\n            params['password_hash'] = self.password_hash\n            request_args = {'data': params, 'files': file_}\n\n        # Do call\n        return self._request(url, api_call, request_args, method)", "response": "Internal method for getting a specific object from the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the expressions for the dependent variables are autonomous.", "response": "def _is_autonomous(indep, exprs):\n    \"\"\" Whether the expressions for the dependent variables are autonomous.\n\n    Note that the system may still behave as an autonomous system on the interface\n    of :meth:`integrate` due to use of pre-/post-processors.\n    \"\"\"\n    if indep is None:\n        return True\n    for expr in exprs:\n        try:\n            in_there = indep in expr.free_symbols\n        except:\n            in_there = expr.has(indep)\n        if in_there:\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_callback(cls, rhs, ny=None, nparams=None, first_step_factory=None,\n                      roots_cb=None, indep_name=None, **kwargs):\n        \"\"\" Create an instance from a callback.\n\n        Parameters\n        ----------\n        rhs : callbable\n            Signature ``rhs(x, y[:], p[:], backend=math) -> f[:]``.\n        ny : int\n            Length of ``y`` in ``rhs``.\n        nparams : int\n            Length of ``p`` in ``rhs``.\n        first_step_factory : callabble\n            Signature ``step1st(x, y[:], p[:]) -> dx0``.\n        roots_cb : callable\n            Callback with signature ``roots(x, y[:], p[:], backend=math) -> r[:]``.\n        indep_name : str\n            Default 'x' if not already in ``names``, otherwise indep0, or indep1, or ...\n        dep_by_name : bool\n            Make ``y`` passed to ``rhs`` a dict (keys from :attr:`names`) and convert\n            its return value from dict to array.\n        par_by_name : bool\n            Make ``p`` passed to ``rhs`` a dict (keys from :attr:`param_names`).\n        \\*\\*kwargs :\n            Keyword arguments passed onto :class:`SymbolicSys`.\n\n        Examples\n        --------\n        >>> def decay(x, y, p, backend=None):\n        ...     rate = y['Po-210']*p[0]\n        ...     return {'Po-210': -rate, 'Pb-206': rate}\n        ...\n        >>> odesys = SymbolicSys.from_callback(decay, dep_by_name=True, names=('Po-210', 'Pb-206'), nparams=1)\n        >>> xout, yout, info = odesys.integrate([0, 138.4*24*3600], {'Po-210': 1.0, 'Pb-206': 0.0}, [5.798e-8])\n        >>> import numpy as np; np.allclose(yout[-1, :], [0.5, 0.5], rtol=1e-3, atol=1e-3)\n        True\n\n\n        Returns\n        -------\n        An instance of :class:`SymbolicSys`.\n        \"\"\"\n        ny, nparams = _get_ny_nparams_from_kw(ny, nparams, kwargs)\n        be = Backend(kwargs.pop('backend', None))\n        names = tuple(kwargs.pop('names', ''))\n        indep_name = indep_name or _get_indep_name(names)\n        try:\n            x = be.Symbol(indep_name, real=True)\n        except TypeError:\n            x = be.Symbol(indep_name)\n        y = be.real_symarray('y', ny)\n        p = be.real_symarray('p', nparams)\n        _y = dict(zip(names, y)) if kwargs.get('dep_by_name', False) else y\n        _p = dict(zip(kwargs['param_names'], p)) if kwargs.get('par_by_name', False) else p\n\n        try:\n            exprs = rhs(x, _y, _p, be)\n        except TypeError:\n            exprs = _ensure_4args(rhs)(x, _y, _p, be)\n\n        try:\n            if len(exprs) != ny:\n                raise ValueError(\"Callback returned unexpected (%d) number of expressions: %d\" % (ny, len(exprs)))\n        except TypeError:\n            raise ValueError(\"Callback did not return an array_like of expressions: %s\" % str(exprs))\n\n        cls._kwargs_roots_from_roots_cb(roots_cb, kwargs, x, _y, _p, be)\n\n        if first_step_factory is not None:\n            if 'first_step_exprs' in kwargs:\n                raise ValueError(\"Cannot override first_step_exprs.\")\n            try:\n                kwargs['first_step_expr'] = first_step_factory(x, _y, _p, be)\n            except TypeError:\n                kwargs['first_step_expr'] = _ensure_4args(first_step_factory)(x, _y, _p, be)\n        if kwargs.get('dep_by_name', False):\n            exprs = [exprs[k] for k in names]\n        return cls(zip(y, exprs), x, kwargs.pop('params', None) if len(p) == 0 else p,\n                   backend=be, names=names, **kwargs)", "response": "Create a new instance of a new object from a callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_other(cls, ori, **kwargs):\n        for k in cls._attrs_to_copy + ('params', 'roots', 'init_indep', 'init_dep'):\n            if k not in kwargs:\n                val = getattr(ori, k)\n                if val is not None:\n                    kwargs[k] = val\n        if 'lower_bounds' not in kwargs and getattr(ori, 'lower_bounds') is not None:\n            kwargs['lower_bounds'] = ori.lower_bounds\n        if 'upper_bounds' not in kwargs and getattr(ori, 'upper_bounds') is not None:\n            kwargs['upper_bounds'] = ori.upper_bounds\n\n        if len(ori.pre_processors) > 0:\n            if 'pre_processors' not in kwargs:\n                kwargs['pre_processors'] = []\n            kwargs['pre_processors'] = kwargs['pre_processors'] + ori.pre_processors\n\n        if len(ori.post_processors) > 0:\n            if 'post_processors' not in kwargs:\n                kwargs['post_processors'] = []\n            kwargs['post_processors'] = ori.post_processors + kwargs['post_processors']\n        if 'dep_exprs' not in kwargs:\n            kwargs['dep_exprs'] = zip(ori.dep, ori.exprs)\n        if 'indep' not in kwargs:\n            kwargs['indep'] = ori.indep\n\n        instance = cls(**kwargs)\n        for attr in ori._attrs_to_copy:\n            if attr not in cls._attrs_to_copy:\n                setattr(instance, attr, getattr(ori, attr))\n        return instance", "response": "Creates a new instance of the class with an existing one as a template."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_other_new_params(cls, ori, par_subs, new_pars, new_par_names=None,\n                              new_latex_par_names=None, **kwargs):\n        \"\"\" Creates a new instance with an existing one as a template (with new parameters)\n\n        Calls ``.from_other`` but first it replaces some parameters according to ``par_subs``\n        and (optionally) introduces new parameters given in ``new_pars``.\n\n        Parameters\n        ----------\n        ori : SymbolicSys instance\n        par_subs : dict\n            Dictionary with substitutions (mapping symbols to new expressions) for parameters.\n            Parameters appearing in this instance will be omitted in the new instance.\n        new_pars : iterable (optional)\n            Iterable of symbols for new parameters.\n        new_par_names : iterable of str\n            Names of the new parameters given in ``new_pars``.\n        new_latex_par_names : iterable of str\n            TeX formatted names of the new parameters given in ``new_pars``.\n        \\\\*\\\\*kwargs:\n            Keyword arguments passed to ``.from_other``.\n\n        Returns\n        -------\n        Intance of the class\n        extra : dict with keys:\n            - recalc_params : ``f(t, y, p1) -> p0``\n\n        \"\"\"\n        new_exprs = [expr.subs(par_subs) for expr in ori.exprs]\n        drop_idxs = [ori.params.index(par) for par in par_subs]\n        params = _skip(drop_idxs, ori.params, False) + list(new_pars)\n        back_substitute = _Callback(ori.indep, ori.dep, params, list(par_subs.values()),\n                                    Lambdify=ori.be.Lambdify)\n\n        def recalc_params(t, y, p):\n            rev = back_substitute(t, y, p)\n            return _reinsert(drop_idxs, np.repeat(np.atleast_2d(p), rev.shape[0], axis=0),\n                             rev)[..., :len(ori.params)]\n\n        return cls.from_other(\n            ori, dep_exprs=zip(ori.dep, new_exprs),\n            params=params,\n            param_names=_skip(drop_idxs, ori.param_names, False) + list(new_par_names or []),\n            latex_param_names=_skip(drop_idxs, ori.latex_param_names, False) + list(new_latex_par_names or []),\n            **kwargs\n        ), {'recalc_params': recalc_params}", "response": "Creates a new instance with a template with new parameters and replaces some parameters according to new_pars."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new instance of this class from a set of parameters and returns it.", "response": "def from_other_new_params_by_name(cls, ori, par_subs, new_par_names=(), **kwargs):\n        \"\"\" Creates a new instance with an existing one as a template (with new parameters)\n\n        Calls ``.from_other_new_params`` but first it creates the new instances from user provided\n        callbacks generating the expressions the parameter substitutions.\n\n        Parameters\n        ----------\n        ori : SymbolicSys instance\n        par_subs : dict mapping str to ``f(t, y{}, p{}) -> expr``\n            User provided callbacks for parameter names in ``ori``.\n        new_par_names : iterable of str\n        \\\\*\\\\*kwargs:\n            Keyword arguments passed to ``.from_other_new_params``.\n\n        \"\"\"\n        if not ori.dep_by_name:\n            warnings.warn('dep_by_name is not True')\n        if not ori.par_by_name:\n            warnings.warn('par_by_name is not True')\n        dep = dict(zip(ori.names, ori.dep))\n        new_pars = ori.be.real_symarray(\n            'p', len(ori.params) + len(new_par_names))[len(ori.params):]\n        par = dict(chain(zip(ori.param_names, ori.params), zip(new_par_names, new_pars)))\n        par_symb_subs = OrderedDict([(ori.params[ori.param_names.index(pk)], cb(\n            ori.indep, dep, par, backend=ori.be)) for pk, cb in par_subs.items()])\n        return cls.from_other_new_params(\n            ori, par_symb_subs, new_pars, new_par_names=new_par_names, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_jac(self):\n        if self._jac is True:\n            if self.sparse is True:\n                self._jac, self._colptrs, self._rowvals = self.be.sparse_jacobian_csc(self.exprs, self.dep)\n            elif self.band is not None:  # Banded\n                self._jac = self.be.banded_jacobian(self.exprs, self.dep, *self.band)\n            else:\n                f = self.be.Matrix(1, self.ny, self.exprs)\n                self._jac = f.jacobian(self.be.Matrix(1, self.ny, self.dep))\n        elif self._jac is False:\n            return False\n\n        return self._jac", "response": "Derives the jacobian from self. exprs and self. dep."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_jtimes(self):\n        if self._jtimes is False:\n            return False\n\n        if self._jtimes is True:\n            r = self.be.Dummy('r')\n            v = tuple(self.be.Dummy('v_{0}'.format(i)) for i in range(self.ny))\n            f = self.be.Matrix(1, self.ny, self.exprs)\n            f = f.subs([(x_i, x_i + r * v_i) for x_i, v_i in zip(self.dep, v)])\n            return v, self.be.flatten(f.diff(r).subs(r, 0))\n        else:\n            return tuple(zip(*self._jtimes))", "response": "Derive the jacobian - vector product from self. exprs and self. dep."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if Jacobian is singular else False.", "response": "def jacobian_singular(self):\n        \"\"\" Returns True if Jacobian is singular, else False. \"\"\"\n        cses, (jac_in_cses,) = self.be.cse(self.get_jac())\n        if jac_in_cses.nullspace():\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates 2nd derivatives of the class instance.", "response": "def get_dfdx(self):\n        \"\"\" Calculates 2nd derivatives of ``self.exprs`` \"\"\"\n        if self._dfdx is True:\n            if self.indep is None:\n                zero = 0*self.be.Dummy()**0\n                self._dfdx = self.be.Matrix(1, self.ny, [zero]*self.ny)\n            else:\n                self._dfdx = self.be.Matrix(1, self.ny, [expr.diff(self.indep) for expr in self.exprs])\n        elif self._dfdx is False:\n            return False\n        return self._dfdx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a callback for evaluating self. exprs.", "response": "def get_f_ty_callback(self):\n        \"\"\" Generates a callback for evaluating ``self.exprs``. \"\"\"\n        cb = self._callback_factory(self.exprs)\n        lb = self.lower_bounds\n        ub = self.upper_bounds\n        if lb is not None or ub is not None:\n            def _bounds_wrapper(t, y, p=(), be=None):\n                if lb is not None:\n                    if np.any(y < lb - 10*self._current_integration_kwargs['atol']):\n                        raise RecoverableError\n                    y = np.array(y)\n                    y[y < lb] = lb[y < lb]\n                if ub is not None:\n                    if np.any(y > ub + 10*self._current_integration_kwargs['atol']):\n                        raise RecoverableError\n                    y = np.array(y)\n                    y[y > ub] = ub[y > ub]\n                return cb(t, y, p, be)\n            return _bounds_wrapper\n        else:\n            return cb"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_j_ty_callback(self):\n        j_exprs = self.get_jac()\n        if j_exprs is False:\n            return None\n        cb = self._callback_factory(j_exprs)\n        if self.sparse:\n            from scipy.sparse import csc_matrix\n\n            def sparse_cb(x, y, p=()):\n                data = cb(x, y, p).flatten()\n                return csc_matrix((data, self._rowvals, self._colptrs))\n\n            return sparse_cb\n        else:\n            return cb", "response": "Generates a callback for evaluating the jacobian of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_dfdx_callback(self):\n        dfdx_exprs = self.get_dfdx()\n        if dfdx_exprs is False:\n            return None\n        return self._callback_factory(dfdx_exprs)", "response": "Generate a callback for evaluating derivative of self. exprs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a callback fro evaluating the jacobian - vector product.", "response": "def get_jtimes_callback(self):\n        \"\"\" Generate a callback fro evaluating the jacobian-vector product.\"\"\"\n        jtimes = self.get_jtimes()\n        if jtimes is False:\n            return None\n        v, jtimes_exprs = jtimes\n        return _Callback(self.indep, tuple(self.dep) + tuple(v), self.params,\n                         jtimes_exprs, Lambdify=self.be.Lambdify)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an instance of a new object from a callback.", "response": "def from_callback(cls, cb, ny=None, nparams=None, dep_transf_cbs=None,\n                      indep_transf_cbs=None, roots_cb=None, **kwargs):\n        \"\"\"\n        Create an instance from a callback.\n\n        Analogous to :func:`SymbolicSys.from_callback`.\n\n        Parameters\n        ----------\n        cb : callable\n            Signature ``rhs(x, y[:], p[:]) -> f[:]``\n        ny : int\n            length of y\n        nparams : int\n            length of p\n        dep_transf_cbs : iterable of pairs callables\n            callables should have the signature ``f(yi) -> expression`` in yi\n        indep_transf_cbs : pair of callbacks\n            callables should have the signature ``f(x) -> expression`` in x\n        roots_cb : callable\n            Callback with signature ``roots(x, y[:], p[:], backend=math) -> r[:]``.\n            Callback should return untransformed roots.\n        \\*\\*kwargs :\n            Keyword arguments passed onto :class:`TransformedSys`.\n\n        \"\"\"\n        ny, nparams = _get_ny_nparams_from_kw(ny, nparams, kwargs)\n        be = Backend(kwargs.pop('backend', None))\n        x, = be.real_symarray('x', 1)\n        y = be.real_symarray('y', ny)\n        p = be.real_symarray('p', nparams)\n        _y = dict(zip(kwargs['names'], y)) if kwargs.get('dep_by_name', False) else y\n        _p = dict(zip(kwargs['param_names'], p)) if kwargs.get('par_by_name', False) else p\n        exprs = _ensure_4args(cb)(x, _y, _p, be)\n        if dep_transf_cbs is not None:\n            dep_transf = [(fw(yi), bw(yi)) for (fw, bw), yi\n                          in zip(dep_transf_cbs, y)]\n        else:\n            dep_transf = None\n\n        if indep_transf_cbs is not None:\n            indep_transf = indep_transf_cbs[0](x), indep_transf_cbs[1](x)\n        else:\n            indep_transf = None\n        if kwargs.get('dep_by_name', False):\n            exprs = [exprs[k] for k in kwargs['names']]\n\n        cls._kwargs_roots_from_roots_cb(roots_cb, kwargs, x, _y, _p, be)\n\n        return cls(list(zip(y, exprs)), x, dep_transf,\n                   indep_transf, p, backend=be, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_callback(cls, cb, ny=None, nparams=None, dep_scaling=1, indep_scaling=1,\n                      **kwargs):\n        \"\"\"\n        Create an instance from a callback.\n\n        Analogous to :func:`SymbolicSys.from_callback`.\n\n        Parameters\n        ----------\n        cb : callable\n            Signature rhs(x, y[:], p[:]) -> f[:]\n        ny : int\n            length of y\n        nparams : int\n            length of p\n        dep_scaling : number (>0) or iterable of numbers\n            scaling of the dependent variables (default: 1)\n        indep_scaling: number (>0)\n            scaling of the independent variable (default: 1)\n        \\*\\*kwargs :\n            Keyword arguments passed onto :class:`ScaledSys`.\n\n        Examples\n        --------\n        >>> def f(x, y, p):\n        ...     return [p[0]*y[0]**2]\n        >>> odesys = ScaledSys.from_callback(f, 1, 1, dep_scaling=10)\n        >>> odesys.exprs\n        (p_0*y_0**2/10,)\n\n        \"\"\"\n        return TransformedSys.from_callback(\n            cb, ny, nparams,\n            dep_transf_cbs=repeat(cls._scale_fw_bw(dep_scaling)),\n            indep_transf_cbs=cls._scale_fw_bw(indep_scaling),\n            **kwargs\n        )", "response": "Create an instance of a TransformedSys from a callback."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct an ODE system from linear invariants.", "response": "def from_linear_invariants(cls, ori_sys, preferred=None, **kwargs):\n        \"\"\" Reformulates the ODE system in fewer variables.\n\n        Given linear invariant equations one can always reduce the number\n        of dependent variables in the system by the rank of the matrix describing\n        this linear system.\n\n        Parameters\n        ----------\n        ori_sys : :class:`SymbolicSys` instance\n        preferred : iterable of preferred dependent variables\n            Due to numerical rounding it is preferable to choose the variables\n            which are expected to be of the largest magnitude during integration.\n        \\*\\*kwargs :\n            Keyword arguments passed on to constructor.\n        \"\"\"\n        _be = ori_sys.be\n        A = _be.Matrix(ori_sys.linear_invariants)\n        rA, pivots = A.rref()\n        if len(pivots) < A.shape[0]:\n            # If the linear system contains rows which a linearly dependent these could be removed.\n            # The criterion for removal could be dictated by a user provided callback.\n            #\n            # An alternative would be to write the matrix in reduced row echelon form, however,\n            # this would cause the invariants to become linear combinations of each other and\n            # their intuitive meaning (original principles they were formulated from) will be lost.\n            # Hence that is not the default behaviour. However, the user may choose to rewrite the\n            # equations in reduced row echelon form if they choose to before calling this method.\n            raise NotImplementedError(\"Linear invariants contain linear dependencies.\")\n        per_row_cols = [(ri, [ci for ci in range(A.cols) if A[ri, ci] != 0]) for ri in range(A.rows)]\n        if preferred is None:\n            preferred = ori_sys.names[:A.rows] if ori_sys.dep_by_name else list(range(A.rows))\n        targets = [\n            ori_sys.names.index(dep) if ori_sys.dep_by_name else (\n                dep if isinstance(dep, int) else ori_sys.dep.index(dep))\n            for dep in preferred]\n        row_tgt = []\n        for ri, colids in sorted(per_row_cols, key=lambda k: len(k[1])):\n            for tgt in targets:\n                if tgt in colids:\n                    row_tgt.append((ri, tgt))\n                    targets.remove(tgt)\n                    break\n            if len(targets) == 0:\n                break\n        else:\n            raise ValueError(\"Could not find a solutions for: %s\" % targets)\n\n        def analytic_factory(x0, y0, p0, be):\n            return {\n                ori_sys.dep[tgt]: y0[ori_sys.dep[tgt] if ori_sys.dep_by_name else tgt] - sum(\n                    [A[ri, ci]*(ori_sys.dep[ci] - y0[ori_sys.dep[ci] if ori_sys.dep_by_name else ci]) for\n                     ci in range(A.cols) if ci != tgt])/A[ri, tgt] for ri, tgt in row_tgt\n            }\n\n        ori_li_nms = ori_sys.linear_invariant_names or ()\n        new_lin_invar = [[cell for ci, cell in enumerate(row) if ci not in list(zip(*row_tgt))[1]]\n                         for ri, row in enumerate(A.tolist()) if ri not in list(zip(*row_tgt))[0]]\n        new_lin_i_nms = [nam for ri, nam in enumerate(ori_li_nms) if ri not in list(zip(*row_tgt))[0]]\n        return cls(ori_sys, analytic_factory, linear_invariants=new_lin_invar,\n                   linear_invariant_names=new_lin_i_nms, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef integrate_auto_switch(odes, kw, x, y0, params=(), **kwargs):\n    x_arr = np.asarray(x)\n    if x_arr.shape[-1] > 2:\n        raise NotImplementedError(\"Only adaptive support return_on_error for now\")\n    multimode = False if x_arr.ndim < 2 else x_arr.shape[0]\n    nfo_keys = ('nfev', 'njev', 'time_cpu', 'time_wall')\n\n    next_autonomous = getattr(odes[0], 'autonomous_interface', False) == True  # noqa (np.True_)\n    if multimode:\n        tot_x = [np.array([0] if next_autonomous else [x[_][0]]) for _ in range(multimode)]\n        tot_y = [np.asarray([y0[_]]) for _ in range(multimode)]\n        tot_nfo = [defaultdict(int) for _ in range(multimode)]\n        glob_x = [_[0] for _ in x] if next_autonomous else [0.0]*multimode\n    else:\n        tot_x, tot_y, tot_nfo = np.array([0 if next_autonomous else x[0]]), np.asarray([y0]), defaultdict(int)\n        glob_x = x[0] if next_autonomous else 0.0\n\n    for oi in range(len(odes)):\n        if oi < len(odes) - 1:\n            next_autonomous = getattr(odes[oi+1], 'autonomous_interface', False) == True  # noqa (np.True_)\n        _int_kw = kwargs.copy()\n        for k, v in kw.items():\n            _int_kw[k] = v[oi]\n        res = odes[oi].integrate(x, y0, params, **_int_kw)\n\n        if multimode:\n            for idx in range(multimode):\n                tot_x[idx] = np.concatenate((tot_x[idx], res[idx].xout[1:] + glob_x[idx]))\n                tot_y[idx] = np.concatenate((tot_y[idx], res[idx].yout[1:, :]))\n                for k in nfo_keys:\n                    if k in res[idx].info:\n                        tot_nfo[idx][k] += res[idx].info[k]\n                tot_nfo[idx]['success'] = res[idx].info['success']\n        else:\n            tot_x = np.concatenate((tot_x, res.xout[1:] + glob_x))\n            tot_y = np.concatenate((tot_y, res.yout[1:, :]))\n            for k in nfo_keys:\n                if k in res.info:\n                    tot_nfo[k] += res.info[k]\n            tot_nfo['success'] = res.info['success']\n\n        if multimode:\n            if all([r.info['success'] for r in res]):\n                break\n        else:\n            if res.info['success']:\n                break\n        if oi < len(odes) - 1:\n            if multimode:\n                _x, y0 = [], []\n                for idx in range(multimode):\n                    _x.append(_new_x(res[idx].xout, x[idx], next_autonomous))\n                    y0.append(res[idx].yout[-1, :])\n                    if next_autonomous:\n                        glob_x[idx] += res[idx].xout[-1]\n                x = _x\n            else:\n                x = _new_x(res.xout, x, next_autonomous)\n                y0 = res.yout[-1, :]\n                if next_autonomous:\n                    glob_x += res.xout[-1]\n    if multimode:  # don't return defaultdict\n        tot_nfo = [dict(nsys=oi+1, **_nfo) for _nfo in tot_nfo]\n        return [Result(tot_x[idx], tot_y[idx], res[idx].params, tot_nfo[idx], odes[0])\n                for idx in range(len(res))]\n    else:\n        tot_nfo = dict(nsys=oi+1, **tot_nfo)\n        return Result(tot_x, tot_y, res.params, tot_nfo, odes[0])", "response": "Integrates the auto - switch between formulations of ODE system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chained_parameter_variation(subject, durations, y0, varied_params, default_params=None,\n                                integrate_kwargs=None, x0=None, npoints=1, numpy=None):\n    \"\"\" Integrate an ODE-system for a serie of durations with some parameters changed in-between\n\n    Parameters\n    ----------\n    subject : function or ODESys instance\n        If a function: should have the signature of :meth:`pyodesys.ODESys.integrate`\n        (and resturn a :class:`pyodesys.results.Result` object).\n        If a ODESys instance: the ``integrate`` method will be used.\n    durations : iterable of floats\n        Spans of the independent variable.\n    y0 : dict or array_like\n    varied_params : dict mapping parameter name (or index) to array_like\n        Each array_like need to be of same length as durations.\n    default_params : dict or array_like\n        Default values for the parameters of the ODE system.\n    integrate_kwargs : dict\n        Keyword arguments passed on to ``integrate``.\n    x0 : float-like\n        First value of independent variable. default: 0.\n    npoints : int\n        Number of points per sub-interval.\n\n    Examples\n    --------\n    >>> odesys = ODESys(lambda t, y, p: [-p[0]*y[0]])\n    >>> int_kw = dict(integrator='cvode', method='adams', atol=1e-12, rtol=1e-12)\n    >>> kwargs = dict(default_params=[0], integrate_kwargs=int_kw)\n    >>> res = chained_parameter_variation(odesys, [2, 3], [42], {0: [.7, .1]}, **kwargs)\n    >>> mask1 = res.xout <= 2\n    >>> import numpy as np\n    >>> np.allclose(res.yout[mask1, 0], 42*np.exp(-.7*res.xout[mask1]))\n    True\n    >>> mask2 = 2 <= res.xout\n    >>> np.allclose(res.yout[mask2, 0], res.yout[mask2, 0][0]*np.exp(-.1*(res.xout[mask2] - res.xout[mask2][0])))\n    True\n\n    \"\"\"\n    assert len(durations) > 0, 'need at least 1 duration (preferably many)'\n    assert npoints > 0, 'need at least 1 point per duration'\n    for k, v in varied_params.items():\n        if len(v) != len(durations):\n            raise ValueError(\"Mismathced lengths of durations and varied_params\")\n\n    if isinstance(subject, ODESys):\n        integrate = subject.integrate\n        numpy = numpy or subject.numpy\n    else:\n        integrate = subject\n        numpy = numpy or np\n\n    default_params = default_params or {}\n    integrate_kwargs = integrate_kwargs or {}\n\n    def _get_idx(cont, idx):\n        if isinstance(cont, dict):\n            return {k: (v[idx] if hasattr(v, '__len__') and getattr(v, 'ndim', 1) > 0 else v)\n                    for k, v in cont.items()}\n        else:\n            return cont[idx]\n\n    durations = numpy.cumsum(durations)\n    for idx_dur in range(len(durations)):\n        params = copy.copy(default_params)\n        for k, v in varied_params.items():\n            params[k] = v[idx_dur]\n        if idx_dur == 0:\n            if x0 is None:\n                x0 = durations[0]*0\n            out = integrate(numpy.linspace(x0, durations[0], npoints + 1), y0, params, **integrate_kwargs)\n        else:\n            if isinstance(out, Result):\n                out.extend_by_integration(durations[idx_dur], params, npoints=npoints, **integrate_kwargs)\n            else:\n                for idx_res, r in enumerate(out):\n                    r.extend_by_integration(durations[idx_dur], _get_idx(params, idx_res),\n                                            npoints=npoints, **integrate_kwargs)\n\n    return out", "response": "Integrate an ODE - system for a serie of durations y0 and varied_params."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pre_process(self, xout, y0, params=()):\n        for pre_processor in self.pre_processors:\n            xout, y0, params = pre_processor(xout, y0, params)\n        return [self.numpy.atleast_1d(arr) for arr in (xout, y0, params)]", "response": "Transforms input to internal values used internally."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_process(self, xout, yout, params):\n        for post_processor in self.post_processors:\n            xout, yout, params = post_processor(xout, yout, params)\n        return xout, yout, params", "response": "Transforms internal values to output used internally."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adaptive(self, y0, x0, xend, params=(), **kwargs):\n        return self.integrate((x0, xend), y0,\n                              params=params, **kwargs)", "response": "Integrate with integrator chosen output."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predefined(self, y0, xout, params=(), **kwargs):\n        xout, yout, info = self.integrate(xout, y0, params=params,\n                                          force_predefined=True, **kwargs)\n        return yout, info", "response": "Integrate with user chosen output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nintegrate the system of ordinary differential equations.", "response": "def integrate(self, x, y0, params=(), atol=1e-8, rtol=1e-8, **kwargs):\n        \"\"\" Integrate the system of ordinary differential equations.\n\n        Solves the initial value problem (IVP).\n\n        Parameters\n        ----------\n        x : array_like or pair (start and final time) or float\n            if float:\n                make it a pair: (0, x)\n            if pair or length-2 array:\n                initial and final value of the independent variable\n            if array_like:\n                values of independent variable report at\n        y0 : array_like\n            Initial values at x[0] for the dependent variables.\n        params : array_like (default: tuple())\n            Value of parameters passed to user-supplied callbacks.\n        integrator : str or None\n            Name of integrator, one of:\n                - 'scipy': :meth:`_integrate_scipy`\n                - 'gsl': :meth:`_integrate_gsl`\n                - 'odeint': :meth:`_integrate_odeint`\n                - 'cvode':  :meth:`_integrate_cvode`\n\n            See respective method for more information.\n            If ``None``: ``os.environ.get('PYODESYS_INTEGRATOR', 'scipy')``\n        atol : float\n            Absolute tolerance\n        rtol : float\n            Relative tolerance\n        with_jacobian : bool or None (default)\n            Whether to use the jacobian. When ``None`` the choice is\n            done automatically (only used when required). This matters\n            when jacobian is derived at runtime (high computational cost).\n        with_jtimes : bool (default: False)\n            Whether to use the jacobian-vector product. This is only supported\n            by ``cvode`` and only when ``linear_solver`` is one of: gmres',\n            'gmres_classic', 'bicgstab', 'tfqmr'. See the documentation\n            for ``pycvodes`` for more information.\n        force_predefined : bool (default: False)\n            override behaviour of ``len(x) == 2`` => :meth:`adaptive`\n        \\\\*\\\\*kwargs :\n            Additional keyword arguments for ``_integrate_$(integrator)``.\n\n        Returns\n        -------\n        Length 3 tuple: (x, yout, info)\n            x : array of values of the independent variable\n            yout : array of the dependent variable(s) for the different\n                values of x.\n            info : dict ('nfev' is guaranteed to be a key)\n        \"\"\"\n        arrs = self.to_arrays(x, y0, params)\n        _x, _y, _p = _arrs = self.pre_process(*arrs)\n        ndims = [a.ndim for a in _arrs]\n        if ndims == [1, 1, 1]:\n            twodim = False\n        elif ndims == [2, 2, 2]:\n            twodim = True\n        else:\n            raise ValueError(\"Pre-processor made ndims inconsistent?\")\n\n        if self.append_iv:\n            _p = self.numpy.concatenate((_p, _y), axis=-1)\n\n        if hasattr(self, 'ny'):\n            if _y.shape[-1] != self.ny:\n                raise ValueError(\"Incorrect shape of intern_y0\")\n        if isinstance(atol, dict):\n            kwargs['atol'] = [atol[k] for k in self.names]\n        else:\n            kwargs['atol'] = atol\n        kwargs['rtol'] = rtol\n\n        integrator = kwargs.pop('integrator', None)\n        if integrator is None:\n            integrator = os.environ.get('PYODESYS_INTEGRATOR', 'scipy')\n\n        args = tuple(map(self.numpy.atleast_2d, (_x, _y, _p)))\n\n        self._current_integration_kwargs = kwargs\n        if isinstance(integrator, str):\n            nfo = getattr(self, '_integrate_' + integrator)(*args, **kwargs)\n        else:\n            kwargs['with_jacobian'] = getattr(integrator, 'with_jacobian', None)\n            nfo = self._integrate(integrator.integrate_adaptive,\n                                  integrator.integrate_predefined,\n                                  *args, **kwargs)\n        if twodim:\n            _xout = [d['internal_xout'] for d in nfo]\n            _yout = [d['internal_yout'] for d in nfo]\n            _params = [d['internal_params'] for d in nfo]\n            res = [Result(*(self.post_process(_xout[i], _yout[i], _params[i]) + (nfo[i], self)))\n                   for i in range(len(nfo))]\n        else:\n            _xout = nfo[0]['internal_xout']\n            _yout = nfo[0]['internal_yout']\n\n            self._internal = _xout.copy(), _yout.copy(), _p.copy()\n            nfo = nfo[0]\n            res = Result(*(self.post_process(_xout, _yout, _p) + (nfo, self)))\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nintegrate a set of values from the Scipy integrator.", "response": "def _integrate_scipy(self, intern_xout, intern_y0, intern_p,\n                         atol=1e-8, rtol=1e-8, first_step=None, with_jacobian=None,\n                         force_predefined=False, name=None, **kwargs):\n        \"\"\" Do not use directly (use ``integrate('scipy', ...)``).\n\n        Uses `scipy.integrate.ode <http://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html>`_\n\n        Parameters\n        ----------\n        \\*args :\n            See :meth:`integrate`.\n        name : str (default: 'lsoda'/'dopri5' when jacobian is available/not)\n            What integrator wrapped in scipy.integrate.ode to use.\n        \\*\\*kwargs :\n            Keyword arguments passed onto `set_integrator(...) <\n        http://docs.scipy.org/doc/scipy/reference/generated/\n        scipy.integrate.ode.set_integrator.html#scipy.integrate.ode.set_integrator>`_\n\n        Returns\n        -------\n        See :meth:`integrate`.\n        \"\"\"\n        from scipy.integrate import ode\n        ny = intern_y0.shape[-1]\n        nx = intern_xout.shape[-1]\n        results = []\n        for _xout, _y0, _p in zip(intern_xout, intern_y0, intern_p):\n            if name is None:\n                if self.j_cb is None:\n                    name = 'dopri5'\n                else:\n                    name = 'lsoda'\n            if with_jacobian is None:\n                if name == 'lsoda':  # lsoda might call jacobian\n                    with_jacobian = True\n                elif name in ('dop853', 'dopri5'):\n                    with_jacobian = False  # explicit steppers\n                elif name == 'vode':\n                    with_jacobian = kwargs.get('method', 'adams') == 'bdf'\n\n            def rhs(t, y, p=()):\n                rhs.ncall += 1\n                return self.f_cb(t, y, p)\n            rhs.ncall = 0\n\n            if self.j_cb is not None:\n                def jac(t, y, p=()):\n                    jac.ncall += 1\n                    return self.j_cb(t, y, p)\n                jac.ncall = 0\n\n            r = ode(rhs, jac=jac if with_jacobian else None)\n            if 'lband' in kwargs or 'uband' in kwargs or 'band' in kwargs:\n                raise ValueError(\"lband and uband set locally (set `band` at initialization instead)\")\n            if self.band is not None:\n                kwargs['lband'], kwargs['uband'] = self.band\n            r.set_integrator(name, atol=atol, rtol=rtol, **kwargs)\n            if len(_p) > 0:\n                r.set_f_params(_p)\n                r.set_jac_params(_p)\n            r.set_initial_value(_y0, _xout[0])\n            if nx == 2 and not force_predefined:\n                mode = 'adaptive'\n                if name in ('vode', 'lsoda'):\n                    warnings.warn(\"'adaptive' mode with SciPy's integrator (vode/lsoda) may overshoot (itask=2)\")\n                    warnings.warn(\"'adaptive' mode with SciPy's integrator is unreliable, consider using e.g. cvode\")\n                    # vode itask 2 (may overshoot)\n                    ysteps = [_y0]\n                    xsteps = [_xout[0]]\n                    while r.t < _xout[1]:\n                        r.integrate(_xout[1], step=True)\n                        if not r.successful():\n                            raise RuntimeError(\"failed\")\n                        xsteps.append(r.t)\n                        ysteps.append(r.y)\n                else:\n                    xsteps, ysteps = [], []\n\n                    def solout(x, y):\n                        xsteps.append(x)\n                        ysteps.append(y)\n                    r.set_solout(solout)\n                    r.integrate(_xout[1])\n                    if not r.successful():\n                        raise RuntimeError(\"failed\")\n                _yout = np.array(ysteps)\n                _xout = np.array(xsteps)\n\n            else:  # predefined\n                mode = 'predefined'\n                _yout = np.empty((nx, ny))\n                _yout[0, :] = _y0\n                for idx in range(1, nx):\n                    r.integrate(_xout[idx])\n                    if not r.successful():\n                        raise RuntimeError(\"failed\")\n                    _yout[idx, :] = r.y\n            info = {\n                'internal_xout': _xout,\n                'internal_yout': _yout,\n                'internal_params': _p,\n                'success': r.successful(),\n                'nfev': rhs.ncall,\n                'n_steps': -1,  # don't know how to obtain this number\n                'name': name,\n                'mode': mode,\n                'atol': atol,\n                'rtol': rtol\n            }\n            if self.j_cb is not None:\n                info['njev'] = jac.ncall\n            results.append(info)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _integrate_gsl(self, *args, **kwargs):\n        import pygslodeiv2  # Python interface GSL's \"odeiv2\" integrators\n        kwargs['with_jacobian'] = kwargs.get(\n            'method', 'bsimp') in pygslodeiv2.requires_jac\n        return self._integrate(pygslodeiv2.integrate_adaptive,\n                               pygslodeiv2.integrate_predefined,\n                               *args, **kwargs)", "response": "Integrate the internal state of a set of items from GSL to ODE system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nintegrating the odeint system.", "response": "def _integrate_odeint(self, *args, **kwargs):\n        \"\"\" Do not use directly (use ``integrate(..., integrator='odeint')``).\n\n        Uses `Boost.Numeric.Odeint <http://www.odeint.com>`_\n        (via `pyodeint <https://pypi.python.org/pypi/pyodeint>`_) to integrate\n        the ODE system.\n        \"\"\"\n        import pyodeint  # Python interface to boost's odeint integrators\n        kwargs['with_jacobian'] = kwargs.get(\n            'method', 'rosenbrock4') in pyodeint.requires_jac\n        return self._integrate(pyodeint.integrate_adaptive,\n                               pyodeint.integrate_predefined,\n                               *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _integrate_cvode(self, *args, **kwargs):\n        import pycvodes  # Python interface to SUNDIALS's cvodes integrators\n        kwargs['with_jacobian'] = kwargs.get('method', 'bdf') in pycvodes.requires_jac\n        if 'lband' in kwargs or 'uband' in kwargs or 'band' in kwargs:\n            raise ValueError(\"lband and uband set locally (set at\"\n                             \" initialization instead)\")\n        if self.band is not None:\n            kwargs['lband'], kwargs['uband'] = self.band\n        kwargs['autonomous_exprs'] = self.autonomous_exprs\n\n        return self._integrate(pycvodes.integrate_adaptive,\n                               pycvodes.integrate_predefined,\n                               *args, **kwargs)", "response": "Integrate the CVode system using the SUNDIALS s cvodes integrators."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot a phase portrait from last integration.", "response": "def plot_phase_plane(self, indices=None, **kwargs):\n        \"\"\" Plots a phase portrait from last integration.\n\n        This method will be deprecated. Please use :meth:`Result.plot_phase_plane`.\n        See :func:`pyodesys.plotting.plot_phase_plane`\n        \"\"\"\n        return self._plot(plot_phase_plane, indices=indices, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stiffness(self, xyp=None, eigenvals_cb=None):\n        if eigenvals_cb is None:\n            if self.band is not None:\n                raise NotImplementedError\n            eigenvals_cb = self._jac_eigenvals_svd\n\n        if xyp is None:\n            x, y, intern_p = self._internal\n        else:\n            x, y, intern_p = self.pre_process(*xyp)\n\n        singular_values = []\n        for xval, yvals in zip(x, y):\n            singular_values.append(eigenvals_cb(xval, yvals, intern_p))\n\n        return (np.abs(singular_values).max(axis=-1) /\n                np.abs(singular_values).min(axis=-1))", "response": "Returns the stiffness ratio of the user may have requested an entry in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a dummy HttpRequest object that is as far as possible and returns it.", "response": "def build_dummy_request(newsitem):\n    \"\"\"\n    Construct a HttpRequest object that is, as far as possible,\n    representative of ones that would receive this page as a response. Used\n    for previewing / moderation and any other place where we want to\n    display a view of this page in the admin interface without going\n    through the regular page routing logic.\n    \"\"\"\n    url = newsitem.full_url\n    if url:\n        url_info = urlparse(url)\n        hostname = url_info.hostname\n        path = url_info.path\n        port = url_info.port or 80\n    else:\n        # Cannot determine a URL to this page - cobble one together based on\n        # whatever we find in ALLOWED_HOSTS\n        try:\n            hostname = settings.ALLOWED_HOSTS[0]\n        except IndexError:\n            hostname = 'localhost'\n        path = '/'\n        port = 80\n\n    request = WSGIRequest({\n        'REQUEST_METHOD': 'GET',\n        'PATH_INFO': path,\n        'SERVER_NAME': hostname,\n        'SERVER_PORT': port,\n        'HTTP_HOST': hostname,\n        'wsgi.input': StringIO(),\n    })\n\n    # Apply middleware to the request - see http://www.mellowmorning.com/2011/04/18/mock-django-request-for-testing/\n    handler = BaseHandler()\n    handler.load_middleware()\n    # call each middleware in turn and throw away any responses that they might return\n    if hasattr(handler, '_request_middleware'):\n        for middleware_method in handler._request_middleware:\n            middleware_method(request)\n    else:\n        handler.get_response(request)\n\n    return request"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_can_edit_news(user):\n    newsitem_models = [model.get_newsitem_model()\n                       for model in NEWSINDEX_MODEL_CLASSES]\n\n    if user.is_active and user.is_superuser:\n        # admin can edit news iff any news types exist\n        return bool(newsitem_models)\n\n    for NewsItem in newsitem_models:\n        for perm in format_perms(NewsItem, ['add', 'change', 'delete']):\n            if user.has_perm(perm):\n                return True\n\n    return False", "response": "Check if the user has permission to edit any of the registered NewsItem types."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the user has permission to edit a particular NewsItem type.", "response": "def user_can_edit_newsitem(user, NewsItem):\n    \"\"\"\n    Check if the user has permission to edit a particular NewsItem type.\n    \"\"\"\n    for perm in format_perms(NewsItem, ['add', 'change', 'delete']):\n        if user.has_perm(perm):\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_date_or_404(year, month, day):\n    try:\n        return datetime.date(int(year), int(month), int(day))\n    except ValueError:\n        raise Http404", "response": "Try to make a date from the given inputs raising Http404 on error"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef respond(self, request, view, newsitems, extra_context={}):\n        context = self.get_context(request, view=view)\n        context.update(self.paginate_newsitems(request, newsitems))\n        context.update(extra_context)\n        template = self.get_template(request, view=view)\n        return TemplateResponse(request, template, context)", "response": "A helper that takes some news items and returns an HttpResponse"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a point from latitude and longitude in WGS84.", "response": "def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):\n        \"\"\"Creates a point from lat/lon in WGS84\"\"\"\n        assert -180.0 <= longitude <= 180.0, 'Longitude needs to be a value between -180.0 and 180.0.'\n        assert -90.0 <= latitude <= 90.0, 'Latitude needs to be a value between -90.0 and 90.0.'\n        return cls(latitude=latitude, longitude=longitude)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a point from pixels X Y Z.", "response": "def from_pixel(cls, pixel_x=0, pixel_y=0, zoom=None):\n        \"\"\"Creates a point from pixels X Y Z (zoom) in pyramid\"\"\"\n        max_pixel = (2 ** zoom) * TILE_SIZE\n        assert 0 <= pixel_x <= max_pixel, 'Point X needs to be a value between 0 and (2^zoom) * 256.'\n        assert 0 <= pixel_y <= max_pixel, 'Point Y needs to be a value between 0 and (2^zoom) * 256.'\n        meter_x = pixel_x * resolution(zoom) - ORIGIN_SHIFT\n        meter_y = pixel_y * resolution(zoom) - ORIGIN_SHIFT\n        meter_x, meter_y = cls._sign_meters(meters=(meter_x, meter_y), pixels=(pixel_x, pixel_y), zoom=zoom)\n        return cls.from_meters(meter_x=meter_x, meter_y=meter_y)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_meters(cls, meter_x=0.0, meter_y=0.0):\n        assert -ORIGIN_SHIFT <= meter_x <= ORIGIN_SHIFT, \\\n            'Meter X needs to be a value between -{0} and {0}.'.format(ORIGIN_SHIFT)\n        assert -ORIGIN_SHIFT <= meter_y <= ORIGIN_SHIFT, \\\n            'Meter Y needs to be a value between -{0} and {0}.'.format(ORIGIN_SHIFT)\n        longitude = (meter_x / ORIGIN_SHIFT) * 180.0\n        latitude = (meter_y / ORIGIN_SHIFT) * 180.0\n        latitude = 180.0 / math.pi * (2 * math.atan(math.exp(latitude * math.pi / 180.0)) - math.pi / 2.0)\n        return cls(latitude=latitude, longitude=longitude)", "response": "Creates a point from X Y Z ( zoom ) meters in Spherical Mercator EPSG : 900913"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pixels(self, zoom=None):\n        meter_x, meter_y = self.meters\n        pixel_x = (meter_x + ORIGIN_SHIFT) / resolution(zoom=zoom)\n        pixel_y = (meter_y - ORIGIN_SHIFT) / resolution(zoom=zoom)\n        return abs(round(pixel_x)), abs(round(pixel_y))", "response": "Gets the pixels of the EPSG 4326 pyramid by a specific zoom"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the XY meters in Spherical Mercator EPSG : 900913 converted from lat / lon in WGS84", "response": "def meters(self):\n        \"\"\"Gets the XY meters in Spherical Mercator EPSG:900913, converted from lat/lon in WGS84\"\"\"\n        latitude, longitude = self.latitude_longitude\n        meter_x = longitude * ORIGIN_SHIFT / 180.0\n        meter_y = math.log(math.tan((90.0 + latitude) * math.pi / 360.0)) / (math.pi / 180.0)\n        meter_y = meter_y * ORIGIN_SHIFT / 180.0\n        return meter_x, meter_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a tile from a Microsoft QuadTree value.", "response": "def from_quad_tree(cls, quad_tree):\n        \"\"\"Creates a tile from a Microsoft QuadTree\"\"\"\n        assert bool(re.match('^[0-3]*$', quad_tree)), 'QuadTree value can only consists of the digits 0, 1, 2 and 3.'\n        zoom = len(str(quad_tree))\n        offset = int(math.pow(2, zoom)) - 1\n        google_x, google_y = [reduce(lambda result, bit: (result << 1) | bit, bits, 0)\n                              for bits in zip(*(reversed(divmod(digit, 2))\n                                                for digit in (int(c) for c in str(quad_tree))))]\n        return cls(tms_x=google_x, tms_y=(offset - google_y), zoom=zoom)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_tms(cls, tms_x, tms_y, zoom):\n        max_tile = (2 ** zoom) - 1\n        assert 0 <= tms_x <= max_tile, 'TMS X needs to be a value between 0 and (2^zoom) -1.'\n        assert 0 <= tms_y <= max_tile, 'TMS Y needs to be a value between 0 and (2^zoom) -1.'\n        return cls(tms_x=tms_x, tms_y=tms_y, zoom=zoom)", "response": "Creates a tile from a Tile Map Service X Y and zoom"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new tile from Google format X Y and zoom", "response": "def from_google(cls, google_x, google_y, zoom):\n        \"\"\"Creates a tile from Google format X Y and zoom\"\"\"\n        max_tile = (2 ** zoom) - 1\n        assert 0 <= google_x <= max_tile, 'Google X needs to be a value between 0 and (2^zoom) -1.'\n        assert 0 <= google_y <= max_tile, 'Google Y needs to be a value between 0 and (2^zoom) -1.'\n        return cls(tms_x=google_x, tms_y=(2 ** zoom - 1) - google_y, zoom=zoom)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a tile for a given point and zoom", "response": "def for_point(cls, point, zoom):\n        \"\"\"Creates a tile for given point\"\"\"\n        latitude, longitude = point.latitude_longitude\n        return cls.for_latitude_longitude(latitude=latitude, longitude=longitude, zoom=zoom)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef for_pixels(cls, pixel_x, pixel_y, zoom):\n        tms_x = int(math.ceil(pixel_x / float(TILE_SIZE)) - 1)\n        tms_y = int(math.ceil(pixel_y / float(TILE_SIZE)) - 1)\n        return cls(tms_x=tms_x, tms_y=(2 ** zoom - 1) - tms_y, zoom=zoom)", "response": "Creates a tile from pixels X Y Z"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a tile from X Y meters in Spherical Mercator EPSG : 900913", "response": "def for_meters(cls, meter_x, meter_y, zoom):\n        \"\"\"Creates a tile from X Y meters in Spherical Mercator EPSG:900913\"\"\"\n        point = Point.from_meters(meter_x=meter_x, meter_y=meter_y)\n        pixel_x, pixel_y = point.pixels(zoom=zoom)\n        return cls.for_pixels(pixel_x=pixel_x, pixel_y=pixel_y, zoom=zoom)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a tile from a latitude and longitude in WGS84", "response": "def for_latitude_longitude(cls, latitude, longitude, zoom):\n        \"\"\"Creates a tile from lat/lon in WGS84\"\"\"\n        point = Point.from_latitude_longitude(latitude=latitude, longitude=longitude)\n        pixel_x, pixel_y = point.pixels(zoom=zoom)\n        return cls.for_pixels(pixel_x=pixel_x, pixel_y=pixel_y, zoom=zoom)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the tile in the Microsoft QuadTree format converted from TMS", "response": "def quad_tree(self):\n        \"\"\"Gets the tile in the Microsoft QuadTree format, converted from TMS\"\"\"\n        value = ''\n        tms_x, tms_y = self.tms\n        tms_y = (2 ** self.zoom - 1) - tms_y\n        for i in range(self.zoom, 0, -1):\n            digit = 0\n            mask = 1 << (i - 1)\n            if (tms_x & mask) != 0:\n                digit += 1\n            if (tms_y & mask) != 0:\n                digit += 2\n            value += str(digit)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the tile in the Google format converted from TMS", "response": "def google(self):\n        \"\"\"Gets the tile in the Google format, converted from TMS\"\"\"\n        tms_x, tms_y = self.tms\n        return tms_x, (2 ** self.zoom - 1) - tms_y"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the bounds of a tile represented as the most west and south point and the most east and north point", "response": "def bounds(self):\n        \"\"\"Gets the bounds of a tile represented as the most west and south point and the most east and north point\"\"\"\n        google_x, google_y = self.google\n        pixel_x_west, pixel_y_north = google_x * TILE_SIZE, google_y * TILE_SIZE\n        pixel_x_east, pixel_y_south = (google_x + 1) * TILE_SIZE, (google_y + 1) * TILE_SIZE\n\n        point_min = Point.from_pixel(pixel_x=pixel_x_west, pixel_y=pixel_y_south, zoom=self.zoom)\n        point_max = Point.from_pixel(pixel_x=pixel_x_east, pixel_y=pixel_y_north, zoom=self.zoom)\n        return point_min, point_max"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X, C):\n        X, C = _check_fit_input(X, C)\n        self.nclasses = C.shape[1]\n        ncombs = int( self.nclasses * (self.nclasses - 1) / 2 )\n        self.classifiers = [ deepcopy(self.base_classifier) for c in range(ncombs) ]\n        self.classes_compared = [None for i in range(ncombs)]\n        if self.weigh_by_cost_diff:\n            V = C\n        else:\n            V = self._calculate_v(C)\n\n        V = np.asfortranarray(V)\n        Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")\\\n            (  delayed(self._fit)(i, j, V, X) for i in range(self.nclasses - 1) for j in range(i + 1, self.nclasses) )\n        self.classes_compared = np.array(self.classes_compared)\n        return self", "response": "Fit one classifier comparing each pair of classes\n        \n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate a 'goodness' distribution over labels Note ---- Predictions can be calculated either by counting which class wins the most pairwise comparisons (as in [1] and [2]), or - for classifiers with a 'predict_proba' method - by taking into account also the margins of the prediction difference for one class over the other for each comparison. If passing method = 'most-wins', this 'decision_function' will output the proportion of comparisons that each class won. If passing method = 'goodness', it sums the outputs from 'predict_proba' from each pairwise comparison and divides it by the number of comparisons. Using method = 'goodness' requires the base classifier to have a 'predict_proba' method. Parameters ---------- X : array (n_samples, n_features) Data for which to predict the cost of each label. method : str, either 'most-wins' or 'goodness': How to decide the best label (see Note) Returns ------- pred : array (n_samples, n_classes) A goodness score (more is better) for each label and observation. If passing method='most-wins', it counts the proportion of comparisons that each class won. If passing method='goodness', it sums the outputs from 'predict_proba' from each pairwise comparison and divides it by the number of comparisons. References ---------- [1] Beygelzimer, A., Dani, V., Hayes, T., Langford, J., & Zadrozny, B. (2005) Error limiting reductions between classification tasks. [2] Beygelzimer, A., Langford, J., & Zadrozny, B. (2008). Machine learning techniques-reductions between prediction quality metrics.", "response": "def decision_function(self, X, method='most-wins'):\n        \"\"\"\n        Calculate a 'goodness' distribution over labels\n        \n        Note\n        ----\n        Predictions can be calculated either by counting which class wins the most\n        pairwise comparisons (as in [1] and [2]), or - for classifiers with a 'predict_proba'\n        method - by taking into account also the margins of the prediction difference\n        for one class over the other for each comparison.\n        \n        If passing method = 'most-wins', this 'decision_function' will output the proportion\n        of comparisons that each class won. If passing method = 'goodness', it sums the\n        outputs from 'predict_proba' from each pairwise comparison and divides it by the\n        number of comparisons.\n        \n        Using method = 'goodness' requires the base classifier to have a 'predict_proba' method.\n        \n        Parameters\n        ----------\n        X : array (n_samples, n_features)\n            Data for which to predict the cost of each label.\n        method : str, either 'most-wins' or 'goodness':\n            How to decide the best label (see Note)\n        \n        Returns\n        -------\n        pred : array (n_samples, n_classes)\n            A goodness score (more is better) for each label and observation.\n            If passing method='most-wins', it counts the proportion of comparisons\n            that each class won.\n            If passing method='goodness', it sums the outputs from 'predict_proba' from\n            each pairwise comparison and divides it by the number of comparisons.\n            \n        References\n        ----------\n        [1] Beygelzimer, A., Dani, V., Hayes, T., Langford, J., & Zadrozny, B. (2005)\n            Error limiting reductions between classification tasks.\n        [2] Beygelzimer, A., Langford, J., & Zadrozny, B. (2008).\n            Machine learning techniques-reductions between prediction quality metrics.\n        \"\"\"\n        X = _check_2d_inp(X, reshape = True)\n        if method == 'most-wins':\n            return self._decision_function_winners(X)\n        elif method == 'goodness':\n            return self._decision_function_goodness(X)\n        else:\n            raise ValueError(\"method must be one of 'most-wins' or 'goodness'.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npredicts the less costly class for a given observation.", "response": "def predict(self, X, method = 'most-wins'):\n        \"\"\"\n        Predict the less costly class for a given observation\n        \n        Note\n        ----\n        Predictions can be calculated either by counting which class wins the most\n        pairwise comparisons (as in [1] and [2]), or - for classifiers with a 'predict_proba'\n        method - by taking into account also the margins of the prediction difference\n        for one class over the other for each comparison.\n        \n        Using method = 'goodness' requires the base classifier to have a 'predict_proba' method.\n        \n        Parameters\n        ----------\n        X : array (n_samples, n_features)\n            Data for which to predict minimum cost label.\n        method : str, either 'most-wins' or 'goodness':\n            How to decide the best label (see Note)\n        \n        Returns\n        -------\n        y_hat : array (n_samples,)\n            Label with expected minimum cost for each observation.\n            \n        References\n        ----------\n        [1] Beygelzimer, A., Dani, V., Hayes, T., Langford, J., & Zadrozny, B. (2005)\n            Error limiting reductions between classification tasks.\n        [2] Beygelzimer, A., Langford, J., & Zadrozny, B. (2008).\n            Machine learning techniques-reductions between prediction quality metrics.\n        \"\"\"\n        X = _check_2d_inp(X, reshape = True)\n        if method == 'most-wins':\n            return self._predict_winners(X)\n        elif method == 'goodness':\n            goodness = self._decision_function_goodness(X)\n            if (len(goodness.shape) == 1) or (goodness.shape[0] == 1):\n                return np.argmax(goodness)\n            else:\n                return np.argmax(goodness, axis=1)\n        else:\n            raise ValueError(\"method must be one of 'most-wins' or 'goodness'.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit a filter tree classifier to the given data.", "response": "def fit(self, X, C):\n        \"\"\"\n        Fit a filter tree classifier\n        \n        Note\n        ----\n        Shifting the order of the classes within the cost array will produce different\n        results, as it will build a different binary tree comparing different classes\n        at each node.\n        \n        Parameters\n        ----------\n        X : array (n_samples, n_features)\n            The data on which to fit a cost-sensitive classifier.\n        C : array (n_samples, n_classes)\n            The cost of predicting each label for each observation (more means worse).\n        \"\"\"\n        X,C = _check_fit_input(X,C)\n        C = np.asfortranarray(C)\n        nclasses=C.shape[1]\n        self.tree=_BinTree(nclasses)\n        self.classifiers=[deepcopy(self.base_classifier) for c in range(nclasses-1)]\n        classifier_queue=self.tree.is_at_bottom\n        next_round=list()\n        already_fitted=set()\n        labels_take=-np.ones((X.shape[0],len(self.classifiers)))\n        while True:\n            for c in classifier_queue:\n                if c in already_fitted or (c is None):\n                    continue\n                child1, child2 = self.tree.childs[c]\n                if (child1>0) and (child1 not in already_fitted):\n                    continue\n                if (child2>0) and (child2 not in already_fitted):\n                    continue\n                    \n                if child1<=0:\n                    class1=-np.repeat(child1,X.shape[0]).astype(\"int64\")\n                else:\n                    class1=labels_take[:, child1].astype(\"int64\")\n                if child2<=0:\n                    class2=-np.repeat(child2,X.shape[0]).astype(\"int64\")\n                else:\n                    class2=labels_take[:, child2].astype(\"int64\")\n\n\n                cost1=C[np.arange(X.shape[0]),np.clip(class1,a_min=0,a_max=None)]\n                cost2=C[np.arange(X.shape[0]),np.clip(class2,a_min=0,a_max=None)]\n                y=(cost1<cost2).astype('uint8')\n                w=np.abs(cost1-cost2)\n\n                valid_obs=w>0\n                if child1>0:\n                    valid_obs=valid_obs&(labels_take[:,child1]>=0)\n                if child2>0:\n                    valid_obs=valid_obs&(labels_take[:,child2]>=0)\n                \n                X_take=X[valid_obs,:]\n                y_take=y[valid_obs]\n                w_take=w[valid_obs]\n                w_take=_standardize_weights(w_take)\n                \n                self.classifiers[c].fit(X_take,y_take,sample_weight=w_take)\n                \n                labels_arr=np.c_[class1,class2].astype(\"int64\")\n                labels_take[valid_obs,c]=labels_arr[np.repeat(0,X_take.shape[0]),\\\n                                                    self.classifiers[c].predict(X_take).reshape(-1).astype('uint8')]\n                already_fitted.add(c)\n                next_round.append(self.tree.parents[c])\n                if c==0 or (len(classifier_queue)==0):\n                    break\n            classifier_queue=list(set(next_round))\n            next_round=list()\n            if (len(classifier_queue)==0):\n                break\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, X):\n        X = _check_2d_inp(X, reshape = True)\n        if X.shape[0] == 1:\n            return self._predict(X)\n        else:\n            shape_single = list(X.shape)\n            shape_single[0] = 1\n            pred = np.empty(X.shape[0], dtype = \"int64\")\n            Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._predict)(row, pred, shape_single, X) for row in range(X.shape[0]))\n            return pred", "response": "Predict the less costly class for a given observation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, X, y, sample_weight=None):\n        assert self.extra_rej_const >= 0\n        if sample_weight is None:\n            sample_weight = np.ones(y.shape[0])\n        else:\n            if isinstance(sample_weight, list):\n                sample_weight = np.array(sample_weight)\n            if len(sample_weight.shape):\n                sample_weight = sample_weight.reshape(-1)\n        assert sample_weight.shape[0] == X.shape[0]\n        assert sample_weight.min() > 0\n        \n        Z = sample_weight.max() + self.extra_rej_const\n        sample_weight = sample_weight / Z # sample weight is now acceptance prob\n        self.classifiers = [deepcopy(self.base_classifier) for c in range(self.n_samples)]\n        ### Note: don't parallelize random number generation, as it's not always thread-safe\n        take_all = np.random.random(size = (self.n_samples, X.shape[0]))\n        Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._fit)(c, take_all, X, y, sample_weight) for c in range(self.n_samples))\n        return self", "response": "Fit a binary classifier with sample weights to data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the decision function for the classifiers that voted for the positive class.", "response": "def decision_function(self, X, aggregation = 'raw'):\n        \"\"\"\n        Calculate how preferred is positive class according to classifiers\n        \n        Note\n        ----\n        If passing aggregation = 'raw', it will output the proportion of the classifiers\n        that voted for the positive class.\n        If passing aggregation = 'weighted', it will output the average predicted probability\n        for the positive class for each classifier.\n        \n        Calculating it with aggregation = 'weighted' requires the base classifier to have a\n        'predict_proba' method.\n        \n        Parameters\n        ----------\n        X : array (n_samples, n_features):\n            Observations for which to determine class likelihood.\n        aggregation : str, either 'raw' or 'weighted'\n            How to compute the 'goodness' of the positive class (see Note)\n            \n        Returns\n        -------\n        pred : array (n_samples,)\n            Score for the positive class (see Note)\n        \"\"\"\n        if aggregation == 'weighted':\n            if 'predict_proba' not in dir(self.classifiers[0]):\n                raise Exception(\"'aggregation='weighted'' is only available for classifiers with 'predict_proba' method.\")\n\n        preds = np.empty((X.shape[0], self.n_samples), dtype = \"float64\")\n        if aggregation == \"raw\":\n            Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._decision_function_raw)(c, preds, X) for c in range(self.nsamples))\n        elif aggregation == \"weighted\":\n            Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._decision_function_weighted)(c, preds, X) for c in range(self.nsamples))\n        else:\n            raise ValueError(\"'aggregation' must be one of 'raw' or 'weighted'.\")\n        return preds.mean(axis = 1).reshape(-1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X, C):\n        X, C = _check_fit_input(X, C)\n        C = np.asfortranarray(C)\n        self.nclasses = C.shape[1]\n        self.classifiers = [deepcopy(self.base_classifier) for i in range(self.nclasses)]\n        if not self.weight_simple_diff:\n            C = WeightedAllPairs._calculate_v(self, C)\n\n        Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._fit)(c, X, C) for c in range(self.nclasses))\n        return self", "response": "Fit one weighted classifier per class\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decision_function(self, X):\n        X = _check_2d_inp(X)\n        preds = np.empty((X.shape[0], self.nclasses))\n\n        available_methods = dir(self.classifiers[0])\n        if \"decision_function\" in available_methods:\n            Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._decision_function_decision_function)(c, preds, X) for c in range(self.nclasses))\n            apply_softmax = True\n        elif \"predict_proba\" in available_methods:\n            Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._decision_function_predict_proba)(c, preds, X) for c in range(self.nclasses))\n            apply_softmax = False\n        elif \"predict\" in available_methods:\n            Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self.decision_function_predict)(c, preds, X) for c in range(self.nclasses))\n            apply_softmax = False\n        else:\n            raise ValueError(\"'base_classifier' must have at least one of 'decision_function', 'predict_proba', 'Predict'.\")\n\n        if apply_softmax:\n            preds = np.exp(preds - preds.max(axis=1).reshape((-1, 1)))\n            preds = preds / preds.sum(axis=1).reshape((-1, 1))\n        return preds", "response": "Calculate a goodness distribution over labels X."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict(self, X):\n        X = _check_2d_inp(X)\n        return np.argmax(self.decision_function(X), axis=1)", "response": "Predict the less costly class for a given observation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfits one regressor per class", "response": "def fit(self, X, C):\n        \"\"\"\n        Fit one regressor per class\n        \n        Parameters\n        ----------\n        X : array (n_samples, n_features)\n            The data on which to fit a cost-sensitive classifier.\n        C : array (n_samples, n_classes)\n            The cost of predicting each label for each observation (more means worse).\n        \"\"\"\n        X, C = _check_fit_input(X, C)\n        C = np.asfortranarray(C)\n        self.nclasses = C.shape[1]\n        self.regressors = [deepcopy(self.base_regressor) for i in range(self.nclasses)]\n        Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._fit)(c, X, C) for c in range(self.nclasses))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the decision function for each observation of the class.", "response": "def decision_function(self, X, apply_softmax = True):\n        \"\"\"\n        Get cost estimates for each observation\n        \n        Note\n        ----\n        If called with apply_softmax = False, this will output the predicted\n        COST rather than the 'goodness' - meaning, more is worse.\n        \n        If called with apply_softmax = True, it will output one minus the softmax on the costs,\n        producing a distribution over the choices summing up to 1 where more is better.\n        \n        \n        Parameters\n        ----------\n        X : array (n_samples, n_features)\n            Data for which to predict the cost of each label.\n        apply_softmax : bool\n            Whether to apply a softmax transform to the costs (see Note).\n        \n        Returns\n        -------\n        pred : array (n_samples, n_classes)\n            Either predicted cost or a distribution of 'goodness' over the choices,\n            according to the apply_softmax argument.\n        \"\"\"\n        X = _check_2d_inp(X, reshape = True)\n        preds = np.empty((X.shape[0], self.nclasses), dtype = \"float64\")\n        Parallel(n_jobs=self.njobs, verbose=0, require=\"sharedmem\")(delayed(self._decision_function)(c, preds, X) for c in range(self.nclasses))\n        \n        if not apply_softmax:\n            return preds\n        else:\n            preds = np.exp(preds - preds.max(axis=1).reshape((-1, 1)))\n            preds = preds/ preds.sum(axis=1).reshape((-1, 1))\n            return 1 - preds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict(self, X):\n        X = _check_2d_inp(X)\n        return np.argmin(self.decision_function(X, False), axis=1)", "response": "Predict the less costly class for a given observation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread timeseries data from an ixmp object", "response": "def read_ix(ix, **kwargs):\n    \"\"\"Read timeseries data from an ixmp object\n\n    Parameters\n    ----------\n    ix: ixmp.TimeSeries or ixmp.Scenario\n        this option requires the ixmp package as a dependency\n    kwargs: arguments passed to ixmp.TimeSeries.timeseries()\n    \"\"\"\n    if not isinstance(ix, ixmp.TimeSeries):\n        error = 'not recognized as valid ixmp class: {}'.format(ix)\n        raise ValueError(error)\n\n    df = ix.timeseries(iamc=False, **kwargs)\n    df['model'] = ix.model\n    df['scenario'] = ix.scenario\n    return df, 'year', []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a pandas DataFrame to an ExcelWriter", "response": "def write_sheet(writer, name, df, index=False):\n    \"\"\"Write a pandas DataFrame to an ExcelWriter,\n    auto-formatting column width depending on maxwidth of data and colum header\n\n    Parameters\n    ----------\n    writer: pandas.ExcelWriter\n        an instance of a pandas ExcelWriter\n    name: string\n        name of the sheet to be written\n    df: pandas.DataFrame\n        a pandas DataFrame to be written to the sheet\n    index: boolean, default False\n        flag whether index should be written to the sheet\n    \"\"\"\n    if index:\n        df = df.reset_index()\n    df.to_excel(writer, name, index=False)\n    worksheet = writer.sheets[name]\n    for i, col in enumerate(df.columns):\n        if df.dtypes[col].name.startswith(('float', 'int')):\n            width = len(str(col)) + 2\n        else:\n            width = max([df[col].map(lambda x: len(str(x or 'None'))).max(),\n                         len(col)]) + 2\n        xls_col = '{c}:{c}'.format(c=NUMERIC_TO_STR[i])\n        worksheet.set_column(xls_col, width)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a file and return a pd. DataFrame", "response": "def read_pandas(fname, *args, **kwargs):\n    \"\"\"Read a file and return a pd.DataFrame\"\"\"\n    if not os.path.exists(fname):\n        raise ValueError('no data file `{}` found!'.format(fname))\n    if fname.endswith('csv'):\n        df = pd.read_csv(fname, *args, **kwargs)\n    else:\n        xl = pd.ExcelFile(fname)\n        if len(xl.sheet_names) > 1 and 'sheet_name' not in kwargs:\n            kwargs['sheet_name'] = 'data'\n        df = pd.read_excel(fname, *args, **kwargs)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading data from a file saved in the standard IAMC format or a table with year and value columns.", "response": "def read_file(fname, *args, **kwargs):\n    \"\"\"Read data from a file saved in the standard IAMC format\n    or a table with year/value columns\n    \"\"\"\n    if not isstr(fname):\n        raise ValueError('reading multiple files not supported, '\n                         'please use `pyam.IamDataFrame.append()`')\n    logger().info('Reading `{}`'.format(fname))\n    format_kwargs = {}\n    # extract kwargs that are intended for `format_data`\n    for c in [i for i in IAMC_IDX + ['year', 'time', 'value'] if i in kwargs]:\n        format_kwargs[c] = kwargs.pop(c)\n    return format_data(read_pandas(fname, *args, **kwargs), **format_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_data(df, **kwargs):\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n\n    # Check for R-style year columns, converting where necessary\n    def convert_r_columns(c):\n        try:\n            first = c[0]\n            second = c[1:]\n            if first == 'X':\n                try:\n                    #  bingo! was X2015 R-style, return the integer\n                    return int(second)\n                except:\n                    # nope, not an int, fall down to final return statement\n                    pass\n        except:\n            # not a string/iterable/etc, fall down to final return statement\n            pass\n        return c\n    df.columns = df.columns.map(convert_r_columns)\n\n    # if `value` is given but not `variable`,\n    # melt value columns and use column name as `variable`\n    if 'value' in kwargs and 'variable' not in kwargs:\n        value = kwargs.pop('value')\n        value = value if islistable(value) else [value]\n        _df = df.set_index(list(set(df.columns) - set(value)))\n        dfs = []\n        for v in value:\n            if v not in df.columns:\n                raise ValueError('column `{}` does not exist!'.format(v))\n            vdf = _df[v].to_frame().rename(columns={v: 'value'})\n            vdf['variable'] = v\n            dfs.append(vdf.reset_index())\n        df = pd.concat(dfs).reset_index(drop=True)\n\n    # otherwise, rename columns or concat to IAMC-style or do a fill-by-value\n    for col, value in kwargs.items():\n        if col in df:\n            raise ValueError('conflict of kwarg with column `{}` in dataframe!'\n                             .format(col))\n\n        if isstr(value) and value in df:\n            df.rename(columns={value: col}, inplace=True)\n        elif islistable(value) and all([c in df.columns for c in value]):\n            df[col] = df.apply(lambda x: concat_with_pipe(x, value), axis=1)\n            df.drop(value, axis=1, inplace=True)\n        elif isstr(value):\n            df[col] = value\n        else:\n            raise ValueError('invalid argument for casting `{}: {}`'\n                             .format(col, value))\n\n    # all lower case\n    str_cols = [c for c in df.columns if isstr(c)]\n    df.rename(columns={c: str(c).lower() for c in str_cols}, inplace=True)\n\n    if 'notes' in df.columns:  # this came from the database\n        logger().info('Ignoring notes column in dataframe')\n        df.drop(columns='notes', inplace=True)\n        col = df.columns[0]  # first column has database copyright notice\n        df = df[~df[col].str.contains('database', case=False)]\n        if 'scenario' in df.columns and 'model' not in df.columns:\n            # model and scenario are jammed together in RCP data\n            scen = df['scenario']\n            df['model'] = scen.apply(lambda s: s.split('-')[0].strip())\n            df['scenario'] = scen.apply(\n                lambda s: '-'.join(s.split('-')[1:]).strip())\n\n    # reset the index if meaningful entries are included there\n    if not list(df.index.names) == [None]:\n        df.reset_index(inplace=True)\n\n    # format columns to lower-case and check that all required columns exist\n    if not set(IAMC_IDX).issubset(set(df.columns)):\n        missing = list(set(IAMC_IDX) - set(df.columns))\n        raise ValueError(\"missing required columns `{}`!\".format(missing))\n\n    # check whether data in wide format (IAMC) or long format (`value` column)\n    if 'value' in df.columns:\n        # check if time column is given as `year` (int) or `time` (datetime)\n        cols = df.columns\n        if 'year' in cols:\n            time_col = 'year'\n        elif 'time' in cols:\n            time_col = 'time'\n        else:\n            msg = 'invalid time format, must have either `year` or `time`!'\n            raise ValueError(msg)\n        extra_cols = list(set(cols) - set(IAMC_IDX + [time_col, 'value']))\n    else:\n        # if in wide format, check if columns are years (int) or datetime\n        cols = set(df.columns) - set(IAMC_IDX)\n        year_cols, time_cols, extra_cols = [], [], []\n        for i in cols:\n            try:\n                int(i)  # this is a year\n                year_cols.append(i)\n            except (ValueError, TypeError):\n                try:\n                    dateutil.parser.parse(str(i))  # this is datetime\n                    time_cols.append(i)\n                except ValueError:\n                    extra_cols.append(i)  # some other string\n        if year_cols and not time_cols:\n            time_col = 'year'\n            melt_cols = year_cols\n        elif not year_cols and time_cols:\n            time_col = 'time'\n            melt_cols = time_cols\n        else:\n            msg = 'invalid column format, must be either years or `datetime`!'\n            raise ValueError(msg)\n        df = pd.melt(df, id_vars=IAMC_IDX + extra_cols, var_name=time_col,\n                     value_vars=sorted(melt_cols), value_name='value')\n\n    # cast value columns to numeric, drop NaN's, sort data\n    df['value'] = df['value'].astype('float64')\n    df.dropna(inplace=True)\n\n    # check for duplicates and return sorted data\n    idx_cols = IAMC_IDX + [time_col] + extra_cols\n    if any(df[idx_cols].duplicated()):\n        raise ValueError('duplicate rows in `data`!')\n\n    return sort_data(df, idx_cols), time_col, extra_cols", "response": "Convert a pd. Dataframe or pd. Series to the required format"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsort data rows and order columns", "response": "def sort_data(data, cols):\n    \"\"\"Sort `data` rows and order columns\"\"\"\n    return data.sort_values(cols)[cols + ['value']].reset_index(drop=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_depth(data, s='', level=None):\n    # remove wildcard as last character from string, escape regex characters\n    _s = re.compile('^' + _escape_regexp(s.rstrip('*')))\n    _p = re.compile('\\\\|')\n\n    # find depth\n    def _count_pipes(val):\n        return len(_p.findall(re.sub(_s, '', val))) if _s.match(val) else None\n\n    n_pipes = map(_count_pipes, data)\n\n    # if no level test is specified, return the depth as int\n    if level is None:\n        return list(n_pipes)\n\n    # if `level` is given, set function for finding depth level =, >=, <= |s\n    if not isstr(level):\n        test = lambda x: level == x if x is not None else False\n    elif level[-1] == '-':\n        level = int(level[:-1])\n        test = lambda x: level >= x if x is not None else False\n    elif level[-1] == '+':\n        level = int(level[:-1])\n        test = lambda x: level <= x if x is not None else False\n    else:\n        raise ValueError('Unknown level type: `{}`'.format(level))\n\n    return list(map(test, n_pipes))", "response": "Find the depth of a set of IAMC - style variables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmatching data to a list of model names variables regions and meta columns.", "response": "def pattern_match(data, values, level=None, regexp=False, has_nan=True):\n    \"\"\"\n    matching of model/scenario names, variables, regions, and meta columns to\n    pseudo-regex (if `regexp == False`) for filtering (str, int, bool)\n    \"\"\"\n    matches = np.array([False] * len(data))\n    if not isinstance(values, collections.Iterable) or isstr(values):\n        values = [values]\n\n    # issue (#40) with string-to-nan comparison, replace nan by empty string\n    _data = data.copy()\n    if has_nan:\n        _data.loc[[np.isnan(i) if not isstr(i) else False for i in _data]] = ''\n\n    for s in values:\n        if isstr(s):\n            pattern = re.compile(_escape_regexp(s) + '$' if not regexp else s)\n            subset = filter(pattern.match, _data)\n            depth = True if level is None else find_depth(_data, s, level)\n            matches |= (_data.isin(subset) & depth)\n        else:\n            matches |= data == s\n    return matches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nescaping characters with specific regexp use", "response": "def _escape_regexp(s):\n    \"\"\"escape characters with specific regexp use\"\"\"\n    return (\n        str(s)\n        .replace('|', '\\\\|')\n        .replace('.', '\\.')  # `.` has to be replaced before `*`\n        .replace('*', '.*')\n        .replace('+', '\\+')\n        .replace('(', '\\(')\n        .replace(')', '\\)')\n        .replace('$', '\\\\$')\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef years_match(data, years):\n    years = [years] if isinstance(years, int) else years\n    dt = datetime.datetime\n    if isinstance(years, dt) or isinstance(years[0], dt):\n        error_msg = \"`year` can only be filtered with ints or lists of ints\"\n        raise TypeError(error_msg)\n    return data.isin(years)", "response": "Match data with a list of years"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a boolean array indicating whether data is in the given hours.", "response": "def hour_match(data, hours):\n    \"\"\"\n    matching of days in time columns for data filtering\n    \"\"\"\n    hours = [hours] if isinstance(hours, int) else hours\n    return data.isin(hours)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if data is in datetimes in time columns.", "response": "def datetime_match(data, dts):\n    \"\"\"\n    matching of datetimes in time columns for data filtering\n    \"\"\"\n    dts = dts if islistable(dts) else [dts]\n    if any([not isinstance(i, datetime.datetime) for i in dts]):\n        error_msg = (\n            \"`time` can only be filtered by datetimes\"\n        )\n        raise TypeError(error_msg)\n    return data.isin(dts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats series or timeseries columns to int and checks validity.", "response": "def to_int(x, index=False):\n    \"\"\"Formatting series or timeseries columns to int and checking validity.\n    If `index=False`, the function works on the `pd.Series x`; else,\n    the function casts the index of `x` to int and returns x with a new index.\n    \"\"\"\n    _x = x.index if index else x\n    cols = list(map(int, _x))\n    error = _x[cols != _x]\n    if not error.empty:\n        raise ValueError('invalid values `{}`'.format(list(error)))\n    if index:\n        x.index = cols\n        return x\n    else:\n        return _x"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef concat_with_pipe(x, cols=None):\n    cols = cols or x.index\n    return '|'.join([x[i] for i in cols if x[i] not in [None, np.nan]])", "response": "Concatenate a pd. Series separated by | drop None or np. nan"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reduce_hierarchy(x, depth):\n    _x = x.split('|')\n    depth = len(_x) + depth - 1 if depth < 0 else depth\n    return '|'.join(_x[0:(depth + 1)])", "response": "Reduce the hierarchy by |"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naggregates df by specified column ( s ) return indexed pd. Series", "response": "def _aggregate(df, by):\n    \"\"\"Aggregate `df` by specified column(s), return indexed `pd.Series`\"\"\"\n    by = [by] if isstr(by) else by\n    cols = [c for c in list(df.columns) if c not in ['value'] + by]\n    return df.groupby(cols).sum()['value']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck all rows to be in or out of a certain range and returns a set of entries that match the given conditions.", "response": "def _check_rows(rows, check, in_range=True, return_test='any'):\n    \"\"\"Check all rows to be in/out of a certain range and provide testing on\n    return values based on provided conditions\n\n    Parameters\n    ----------\n    rows: pd.DataFrame\n        data rows\n    check: dict\n        dictionary with possible values of 'up', 'lo', and 'year'\n    in_range: bool, optional\n        check if values are inside or outside of provided range\n    return_test: str, optional\n        possible values:\n            - 'any': default, return scenarios where check passes for any entry\n            - 'all': test if all values match checks, if not, return empty set\n    \"\"\"\n    valid_checks = set(['up', 'lo', 'year'])\n    if not set(check.keys()).issubset(valid_checks):\n        msg = 'Unknown checking type: {}'\n        raise ValueError(msg.format(check.keys() - valid_checks))\n\n    where_idx = set(rows.index[rows['year'] == check['year']]) \\\n        if 'year' in check else set(rows.index)\n    rows = rows.loc[list(where_idx)]\n\n    up_op = rows['value'].__le__ if in_range else rows['value'].__gt__\n    lo_op = rows['value'].__ge__ if in_range else rows['value'].__lt__\n\n    check_idx = []\n    for (bd, op) in [('up', up_op), ('lo', lo_op)]:\n        if bd in check:\n            check_idx.append(set(rows.index[op(check[bd])]))\n\n    if return_test is 'any':\n        ret = where_idx & set.union(*check_idx)\n    elif return_test == 'all':\n        ret = where_idx if where_idx == set.intersection(*check_idx) else set()\n    else:\n        raise ValueError('Unknown return test: {}'.format(return_test))\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply criteria individually to every model or scenario instance", "response": "def _apply_criteria(df, criteria, **kwargs):\n    \"\"\"Apply criteria individually to every model/scenario instance\"\"\"\n    idxs = []\n    for var, check in criteria.items():\n        _df = df[df['variable'] == var]\n        for group in _df.groupby(META_IDX):\n            grp_idxs = _check_rows(group[-1], check, **kwargs)\n            idxs.append(grp_idxs)\n    df = df.loc[itertools.chain(*idxs)]\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an index from the columns of a dataframe", "response": "def _make_index(df, cols=META_IDX):\n    \"\"\"Create an index from the columns of a dataframe\"\"\"\n    return pd.MultiIndex.from_tuples(\n        pd.unique(list(zip(*[df[col] for col in cols]))), names=tuple(cols))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(df, criteria={}, exclude_on_fail=False, **kwargs):\n    fdf = df.filter(**kwargs)\n    if len(fdf.data) > 0:\n        vdf = fdf.validate(criteria=criteria, exclude_on_fail=exclude_on_fail)\n        df.meta['exclude'] |= fdf.meta['exclude']  # update if any excluded\n        return vdf", "response": "Validate scenarios using criteria on timeseries values"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether all scenarios have a required variable in the IamDataFrame", "response": "def require_variable(df, variable, unit=None, year=None, exclude_on_fail=False,\n                     **kwargs):\n    \"\"\"Check whether all scenarios have a required variable\n\n    Parameters\n    ----------\n    df: IamDataFrame instance\n    args: see `IamDataFrame.require_variable()` for details\n    kwargs: passed to `df.filter()`\n    \"\"\"\n    fdf = df.filter(**kwargs)\n    if len(fdf.data) > 0:\n        vdf = fdf.require_variable(variable=variable, unit=unit, year=year,\n                                   exclude_on_fail=exclude_on_fail)\n        df.meta['exclude'] |= fdf.meta['exclude']  # update if any excluded\n        return vdf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef categorize(df, name, value, criteria,\n               color=None, marker=None, linestyle=None, **kwargs):\n    \"\"\"Assign scenarios to a category according to specific criteria\n    or display the category assignment\n\n    Parameters\n    ----------\n    df: IamDataFrame instance\n    args: see `IamDataFrame.categorize()` for details\n    kwargs: passed to `df.filter()`\n    \"\"\"\n    fdf = df.filter(**kwargs)\n    fdf.categorize(name=name, value=value, criteria=criteria, color=color,\n                   marker=marker, linestyle=linestyle)\n\n    # update metadata\n    if name in df.meta:\n        df.meta[name].update(fdf.meta[name])\n    else:\n        df.meta[name] = fdf.meta[name]", "response": "Assign scenarios to a specific category according to specific criteria\n    or display the category assignment\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck whether the timeseries values match the aggregation of sub - categories.", "response": "def check_aggregate(df, variable, components=None, exclude_on_fail=False,\n                    multiplier=1, **kwargs):\n    \"\"\"Check whether the timeseries values match the aggregation\n    of sub-categories\n\n    Parameters\n    ----------\n    df: IamDataFrame instance\n    args: see IamDataFrame.check_aggregate() for details\n    kwargs: passed to `df.filter()`\n    \"\"\"\n    fdf = df.filter(**kwargs)\n    if len(fdf.data) > 0:\n        vdf = fdf.check_aggregate(variable=variable, components=components,\n                                  exclude_on_fail=exclude_on_fail,\n                                  multiplier=multiplier)\n        df.meta['exclude'] |= fdf.meta['exclude']  # update if any excluded\n        return vdf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_by_meta(data, df, join_meta=False, **kwargs):\n    if not set(META_IDX).issubset(data.index.names + list(data.columns)):\n        raise ValueError('missing required index dimensions or columns!')\n\n    meta = pd.DataFrame(df.meta[list(set(kwargs) - set(META_IDX))].copy())\n\n    # filter meta by columns\n    keep = np.array([True] * len(meta))\n    apply_filter = False\n    for col, values in kwargs.items():\n        if col in META_IDX and values is not None:\n            _col = meta.index.get_level_values(0 if col is 'model' else 1)\n            keep &= pattern_match(_col, values, has_nan=False)\n            apply_filter = True\n        elif values is not None:\n            keep &= pattern_match(meta[col], values)\n        apply_filter |= values is not None\n    meta = meta[keep]\n\n    # set the data index to META_IDX and apply filtered meta index\n    data = data.copy()\n    idx = list(data.index.names) if not data.index.names == [None] else None\n    data = data.reset_index().set_index(META_IDX)\n    meta = meta.loc[meta.index.intersection(data.index)]\n    meta.index.names = META_IDX\n    if apply_filter:\n        data = data.loc[meta.index]\n    data.index.names = META_IDX\n\n    # join meta (optional), reset index to format as input arg\n    data = data.join(meta) if join_meta else data\n    data = data.reset_index().set_index(idx or 'index')\n    if idx is None:\n        data.index.name = None\n\n    return data", "response": "Filter by and join meta columns from an IamDataFrame to a pd. DataFrame instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compare(left, right, left_label='left', right_label='right',\n            drop_close=True, **kwargs):\n    \"\"\"Compare the data in two IamDataFrames and return a pd.DataFrame\n\n    Parameters\n    ----------\n    left, right: IamDataFrames\n        the IamDataFrames to be compared\n    left_label, right_label: str, default `left`, `right`\n        column names of the returned dataframe\n    drop_close: bool, default True\n        remove all data where `left` and `right` are close\n    kwargs: passed to `np.isclose()`\n    \"\"\"\n    ret = pd.concat({right_label: right.data.set_index(right._LONG_IDX),\n                     left_label: left.data.set_index(left._LONG_IDX)}, axis=1)\n    ret.columns = ret.columns.droplevel(1)\n    if drop_close:\n        ret = ret[~np.isclose(ret[left_label], ret[right_label], **kwargs)]\n    return ret[[right_label, left_label]]", "response": "Compare the data in two IamDataFrames and return a pd. DataFrame containing the data in the same order as the IamDataFrames left and right."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconcatenate a series of pyam. IamDataFrame - like objects together", "response": "def concat(dfs):\n    \"\"\"Concatenate a series of `pyam.IamDataFrame`-like objects together\"\"\"\n    if isstr(dfs) or not hasattr(dfs, '__iter__'):\n        msg = 'Argument must be a non-string iterable (e.g., list or tuple)'\n        raise TypeError(msg)\n\n    _df = None\n    for df in dfs:\n        df = df if isinstance(df, IamDataFrame) else IamDataFrame(df)\n        if _df is None:\n            _df = copy.deepcopy(df)\n        else:\n            _df.append(df, inplace=True)\n    return _df"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of variables in the log.", "response": "def variables(self, include_units=False):\n        \"\"\"Get a list of variables\n\n        Parameters\n        ----------\n        include_units: boolean, default False\n            include the units\n        \"\"\"\n        if include_units:\n            return self.data[['variable', 'unit']].drop_duplicates()\\\n                .reset_index(drop=True).sort_values('variable')\n        else:\n            return pd.Series(self.data.variable.unique(), name='variable')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending any castable object to this IamDataFrame.", "response": "def append(self, other, ignore_meta_conflict=False, inplace=False,\n               **kwargs):\n        \"\"\"Append any castable object to this IamDataFrame.\n        Columns in `other.meta` that are not in `self.meta` are always merged,\n        duplicate region-variable-unit-year rows raise a ValueError.\n\n        Parameters\n        ----------\n        other: pyam.IamDataFrame, ixmp.TimeSeries, ixmp.Scenario,\n        pd.DataFrame or data file\n            An IamDataFrame, TimeSeries or Scenario (requires `ixmp`),\n            pandas.DataFrame or data file with IAMC-format data columns\n        ignore_meta_conflict : bool, default False\n            If False and `other` is an IamDataFrame, raise an error if\n            any meta columns present in `self` and `other` are not identical.\n        inplace : bool, default False\n            If True, do operation inplace and return None\n        kwargs are passed through to `IamDataFrame(other, **kwargs)`\n        \"\"\"\n        if not isinstance(other, IamDataFrame):\n            other = IamDataFrame(other, **kwargs)\n            ignore_meta_conflict = True\n\n        if self.time_col is not other.time_col:\n            raise ValueError('incompatible time format (years vs. datetime)!')\n\n        ret = copy.deepcopy(self) if not inplace else self\n\n        diff = other.meta.index.difference(ret.meta.index)\n        intersect = other.meta.index.intersection(ret.meta.index)\n\n        # merge other.meta columns not in self.meta for existing scenarios\n        if not intersect.empty:\n            # if not ignored, check that overlapping meta dataframes are equal\n            if not ignore_meta_conflict:\n                cols = [i for i in other.meta.columns if i in ret.meta.columns]\n                if not ret.meta.loc[intersect, cols].equals(\n                        other.meta.loc[intersect, cols]):\n                    conflict_idx = (\n                        pd.concat([ret.meta.loc[intersect, cols],\n                                   other.meta.loc[intersect, cols]]\n                                  ).drop_duplicates()\n                        .index.drop_duplicates()\n                    )\n                    msg = 'conflict in `meta` for scenarios {}'.format(\n                        [i for i in pd.DataFrame(index=conflict_idx).index])\n                    raise ValueError(msg)\n\n            cols = [i for i in other.meta.columns if i not in ret.meta.columns]\n            _meta = other.meta.loc[intersect, cols]\n            ret.meta = ret.meta.merge(_meta, how='outer',\n                                      left_index=True, right_index=True)\n\n        # join other.meta for new scenarios\n        if not diff.empty:\n            # sorting not supported by ` pd.append()`  prior to version 23\n            sort_kwarg = {} if int(pd.__version__.split('.')[1]) < 23 \\\n                else dict(sort=False)\n            ret.meta = ret.meta.append(other.meta.loc[diff, :], **sort_kwarg)\n\n        # append other.data (verify integrity for no duplicates)\n        _data = ret.data.set_index(ret._LONG_IDX).append(\n            other.data.set_index(other._LONG_IDX), verify_integrity=True)\n\n        # merge extra columns in `data` and set `LONG_IDX`\n        ret.extra_cols += [i for i in other.extra_cols\n                           if i not in ret.extra_cols]\n        ret._LONG_IDX = IAMC_IDX + [ret.time_col] + ret.extra_cols\n        ret.data = sort_data(_data.reset_index(), ret._LONG_IDX)\n\n        if not inplace:\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pivot_table(self, index, columns, values='value',\n                    aggfunc='count', fill_value=None, style=None):\n        \"\"\"Returns a pivot table\n\n        Parameters\n        ----------\n        index: str or list of strings\n            rows for Pivot table\n        columns: str or list of strings\n            columns for Pivot table\n        values: str, default 'value'\n            dataframe column to aggregate or count\n        aggfunc: str or function, default 'count'\n            function used for aggregation,\n            accepts 'count', 'mean', and 'sum'\n        fill_value: scalar, default None\n            value to replace missing values with\n        style: str, default None\n            output style for pivot table formatting\n            accepts 'highlight_not_max', 'heatmap'\n        \"\"\"\n        index = [index] if isstr(index) else index\n        columns = [columns] if isstr(columns) else columns\n\n        df = self.data\n\n        # allow 'aggfunc' to be passed as string for easier user interface\n        if isstr(aggfunc):\n            if aggfunc == 'count':\n                df = self.data.groupby(index + columns, as_index=False).count()\n                fill_value = 0\n            elif aggfunc == 'mean':\n                df = self.data.groupby(index + columns, as_index=False).mean()\\\n                    .round(2)\n                aggfunc = np.sum\n                fill_value = 0 if style == 'heatmap' else \"\"\n            elif aggfunc == 'sum':\n                aggfunc = np.sum\n                fill_value = 0 if style == 'heatmap' else \"\"\n\n        df = df.pivot_table(values=values, index=index, columns=columns,\n                            aggfunc=aggfunc, fill_value=fill_value)\n        return df", "response": "Returns a pivot table of the user interface with the specified values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef interpolate(self, year):\n        df = self.pivot_table(index=IAMC_IDX, columns=['year'],\n                              values='value', aggfunc=np.sum)\n        # drop year-rows where values are already defined\n        if year in df.columns:\n            df = df[np.isnan(df[year])]\n        fill_values = df.apply(fill_series,\n                               raw=False, axis=1, year=year)\n        fill_values = fill_values.dropna().reset_index()\n        fill_values = fill_values.rename(columns={0: \"value\"})\n        fill_values['year'] = year\n        self.data = self.data.append(fill_values, ignore_index=True)", "response": "Interpolate missing values in timeseries"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_pandas(self, with_metadata=False):\n        if with_metadata:\n            cols = self._discover_meta_cols(**with_metadata) \\\n                if isinstance(with_metadata, dict) else self.meta.columns\n            return (\n                self.data\n                .set_index(META_IDX)\n                .join(self.meta[cols])\n                .reset_index()\n            )\n        else:\n            return self.data.copy()", "response": "Return this as a pd. DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _discover_meta_cols(self, **kwargs):\n        cols = set(['exclude'])\n        for arg, value in kwargs.items():\n            if isstr(value) and value in self.meta.columns:\n                cols.add(value)\n        return list(cols)", "response": "Return the subset of kwargs values ( not keys!) matching\n        a meta column name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a pd. DataFrame with the data in wide format", "response": "def timeseries(self, iamc_index=False):\n        \"\"\"Returns a pd.DataFrame in wide format (years or timedate as columns)\n\n        Parameters\n        ----------\n        iamc_index: bool, default False\n            if True, use `['model', 'scenario', 'region', 'variable', 'unit']`;\n            else, use all `data` columns\n        \"\"\"\n        index = IAMC_IDX if iamc_index else IAMC_IDX + self.extra_cols\n        df = (\n            self.data\n            .pivot_table(index=index, columns=self.time_col)\n            .value  # column name\n            .rename_axis(None, axis=1)\n        )\n        if df.index.has_duplicates:\n            raise ValueError('timeseries object has duplicates in index ',\n                             'use `iamc_index=False`')\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding metadata columns as pd. Series list or value to metadata.", "response": "def set_meta(self, meta, name=None, index=None):\n        \"\"\"Add metadata columns as pd.Series, list or value (int/float/str)\n\n        Parameters\n        ----------\n        meta: pd.Series, list, int, float or str\n            column to be added to metadata\n            (by `['model', 'scenario']` index if possible)\n        name: str, optional\n            meta column name (defaults to meta pd.Series.name);\n            either a meta.name or the name kwarg must be defined\n        index: pyam.IamDataFrame, pd.DataFrame or pd.MultiIndex, optional\n            index to be used for setting meta column (`['model', 'scenario']`)\n        \"\"\"\n        # check that name is valid and doesn't conflict with data columns\n        if (name or (hasattr(meta, 'name') and meta.name)) in [None, False]:\n            raise ValueError('Must pass a name or use a named pd.Series')\n        name = name or meta.name\n        if name in self.data.columns:\n            raise ValueError('`{}` already exists in `data`!'.format(name))\n\n        # check if meta has a valid index and use it for further workflow\n        if hasattr(meta, 'index') and hasattr(meta.index, 'names') \\\n                and set(META_IDX).issubset(meta.index.names):\n            index = meta.index\n\n        # if no valid index is provided, add meta as new column `name` and exit\n        if index is None:\n            self.meta[name] = list(meta) if islistable(meta) else meta\n            return  # EXIT FUNCTION\n\n        # use meta.index if index arg is an IamDataFrame\n        if isinstance(index, IamDataFrame):\n            index = index.meta.index\n        # turn dataframe to index if index arg is a DataFrame\n        if isinstance(index, pd.DataFrame):\n            index = index.set_index(META_IDX).index\n        if not isinstance(index, pd.MultiIndex):\n            raise ValueError('index cannot be coerced to pd.MultiIndex')\n\n        # raise error if index is not unique\n        if index.duplicated().any():\n            raise ValueError(\"non-unique ['model', 'scenario'] index!\")\n\n        # create pd.Series from meta, index and name if provided\n        meta = pd.Series(data=meta, index=index, name=name)\n\n        # reduce index dimensions to model-scenario only\n        meta = (\n            meta\n            .reset_index()\n            .reindex(columns=META_IDX + [name])\n            .set_index(META_IDX)\n        )\n\n        # check if trying to add model-scenario index not existing in self\n        diff = meta.index.difference(self.meta.index)\n        if not diff.empty:\n            error = \"adding metadata for non-existing scenarios '{}'!\"\n            raise ValueError(error.format(diff))\n\n        self._new_meta_column(name)\n        self.meta[name] = meta[name].combine_first(self.meta[name])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef categorize(self, name, value, criteria,\n                   color=None, marker=None, linestyle=None):\n        \"\"\"Assign scenarios to a category according to specific criteria\n        or display the category assignment\n\n        Parameters\n        ----------\n        name: str\n            category column name\n        value: str\n            category identifier\n        criteria: dict\n            dictionary with variables mapped to applicable checks\n            ('up' and 'lo' for respective bounds, 'year' for years - optional)\n        color: str\n            assign a color to this category for plotting\n        marker: str\n            assign a marker to this category for plotting\n        linestyle: str\n            assign a linestyle to this category for plotting\n        \"\"\"\n        # add plotting run control\n        for kind, arg in [('color', color), ('marker', marker),\n                          ('linestyle', linestyle)]:\n            if arg:\n                run_control().update({kind: {name: {value: arg}}})\n\n        # find all data that matches categorization\n        rows = _apply_criteria(self.data, criteria,\n                               in_range=True, return_test='all')\n        idx = _meta_idx(rows)\n\n        if len(idx) == 0:\n            logger().info(\"No scenarios satisfy the criteria\")\n            return  # EXIT FUNCTION\n\n        # update metadata dataframe\n        self._new_meta_column(name)\n        self.meta.loc[idx, name] = value\n        msg = '{} scenario{} categorized as `{}: {}`'\n        logger().info(msg.format(len(idx), '' if len(idx) == 1 else 's',\n                                 name, value))", "response": "Assign scenarios to a category according to specific criteria or display the category assignment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _new_meta_column(self, name):\n        if name is None:\n            raise ValueError('cannot add a meta column `{}`'.format(name))\n        if name not in self.meta:\n            self.meta[name] = np.nan", "response": "Add a column to meta if it doesn t exist set to np. nan"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether all scenarios have a required variable.", "response": "def require_variable(self, variable, unit=None, year=None,\n                         exclude_on_fail=False):\n        \"\"\"Check whether all scenarios have a required variable\n\n        Parameters\n        ----------\n        variable: str\n            required variable\n        unit: str, default None\n            name of unit (optional)\n        year: int or list, default None\n            years (optional)\n        exclude_on_fail: bool, default False\n            flag scenarios missing the required variables as `exclude: True`\n        \"\"\"\n        criteria = {'variable': variable}\n        if unit:\n            criteria.update({'unit': unit})\n        if year:\n            criteria.update({'year': year})\n\n        keep = self._apply_filters(**criteria)\n        idx = self.meta.index.difference(_meta_idx(self.data[keep]))\n\n        n = len(idx)\n        if n == 0:\n            logger().info('All scenarios have the required variable `{}`'\n                          .format(variable))\n            return\n\n        msg = '{} scenario does not include required variable `{}`' if n == 1 \\\n            else '{} scenarios do not include required variable `{}`'\n\n        if exclude_on_fail:\n            self.meta.loc[idx, 'exclude'] = True\n            msg += ', marked as `exclude: True` in metadata'\n\n        logger().info(msg.format(n, variable))\n        return pd.DataFrame(index=idx).reset_index()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self, criteria={}, exclude_on_fail=False):\n        df = _apply_criteria(self.data, criteria, in_range=False)\n\n        if not df.empty:\n            msg = '{} of {} data points to not satisfy the criteria'\n            logger().info(msg.format(len(df), len(self.data)))\n\n            if exclude_on_fail and len(df) > 0:\n                self._exclude_on_fail(df)\n\n            return df", "response": "Validate scenarios using criteria on timeseries values\n          "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename(self, mapping=None, inplace=False, append=False,\n               check_duplicates=True, **kwargs):\n        \"\"\"Rename and aggregate column entries using `groupby.sum()` on values.\n        When renaming models or scenarios, the uniqueness of the index must be\n        maintained, and the function will raise an error otherwise.\n\n        Renaming is only applied to any data where a filter matches for all\n        columns given in `mapping`. Renaming can only be applied to the `model`\n        and `scenario` columns or to other data columns simultaneously.\n\n        Parameters\n        ----------\n        mapping: dict or kwargs\n            mapping of column name to rename-dictionary of that column\n            >> {<column_name>: {<current_name_1>: <target_name_1>,\n            >>                  <current_name_2>: <target_name_2>}}\n            or kwargs as `column_name={<current_name_1>: <target_name_1>, ...}`\n        inplace: bool, default False\n            if True, do operation inplace and return None\n        append: bool, default False\n            if True, append renamed timeseries to IamDataFrame\n        check_duplicates: bool, default True\n            check whether conflict between existing and renamed data exists.\n            If True, raise ValueError; if False, rename and merge\n            with `groupby().sum()`.\n        \"\"\"\n        # combine `mapping` arg and mapping kwargs, ensure no rename conflicts\n        mapping = mapping or {}\n        duplicate = set(mapping).intersection(kwargs)\n        if duplicate:\n            msg = 'conflicting rename args for columns `{}`'.format(duplicate)\n            raise ValueError(msg)\n        mapping.update(kwargs)\n\n        # determine columns that are not `model` or `scenario`\n        data_cols = set(self._LONG_IDX) - set(META_IDX)\n\n        # changing index and data columns can cause model-scenario mismatch\n        if any(i in mapping for i in META_IDX)\\\n                and any(i in mapping for i in data_cols):\n            msg = 'Renaming index and data cols simultaneously not supported!'\n            raise ValueError(msg)\n\n        # translate rename mapping to `filter()` arguments\n        filters = {col: _from.keys() for col, _from in mapping.items()}\n\n        # if append is True, downselect and append renamed data\n        if append:\n            df = self.filter(filters)\n            # note that `append(other, inplace=True)` returns None\n            return self.append(df.rename(mapping), inplace=inplace)\n\n        # if append is False, iterate over rename mapping and do groupby\n        ret = copy.deepcopy(self) if not inplace else self\n\n        # renaming is only applied where a filter matches for all given columns\n        rows = ret._apply_filters(**filters)\n        idx = ret.meta.index.isin(_make_index(ret.data[rows]))\n\n        # if `check_duplicates`, do the rename on a copy until after the check\n        _data = ret.data.copy() if check_duplicates else ret.data\n\n        # apply renaming changes\n        for col, _mapping in mapping.items():\n            if col in META_IDX:\n                _index = pd.DataFrame(index=ret.meta.index).reset_index()\n                _index.loc[idx, col] = _index.loc[idx, col].replace(_mapping)\n                if _index.duplicated().any():\n                    raise ValueError('Renaming to non-unique `{}` index!'\n                                     .format(col))\n                ret.meta.index = _index.set_index(META_IDX).index\n            elif col not in data_cols:\n                raise ValueError('Renaming by `{}` not supported!'.format(col))\n            _data.loc[rows, col] = _data.loc[rows, col].replace(_mapping)\n\n        # check if duplicates exist between the renamed and not-renamed data\n        if check_duplicates:\n            merged = (\n                _data.loc[rows, self._LONG_IDX].drop_duplicates().append(\n                    _data.loc[~rows, self._LONG_IDX].drop_duplicates())\n            )\n            if any(merged.duplicated()):\n                msg = 'Duplicated rows between original and renamed data!\\n{}'\n                conflict_rows = merged.loc[merged.duplicated(), self._LONG_IDX]\n                raise ValueError(msg.format(conflict_rows.drop_duplicates()))\n\n        # merge using `groupby().sum()`\n        ret.data = _data.groupby(ret._LONG_IDX).sum().reset_index()\n\n        if not inplace:\n            return ret", "response": "Rename the timeseries with the given mapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts units based on provided unit conversion factors", "response": "def convert_unit(self, conversion_mapping, inplace=False):\n        \"\"\"Converts units based on provided unit conversion factors\n\n        Parameters\n        ----------\n        conversion_mapping: dict\n            for each unit for which a conversion should be carried out,\n            provide current unit and target unit and conversion factor\n            {<current unit>: [<target unit>, <conversion factor>]}\n        inplace: bool, default False\n            if True, do operation inplace and return None\n        \"\"\"\n        ret = copy.deepcopy(self) if not inplace else self\n        for current_unit, (new_unit, factor) in conversion_mapping.items():\n            factor = pd.to_numeric(factor)\n            where = ret.data['unit'] == current_unit\n            ret.data.loc[where, 'value'] *= factor\n            ret.data.loc[where, 'unit'] = new_unit\n        if not inplace:\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes the data to a given value. Currently only supports normalizing to a specific time.", "response": "def normalize(self, inplace=False, **kwargs):\n        \"\"\"Normalize data to a given value. Currently only supports normalizing\n        to a specific time.\n\n        Parameters\n        ----------\n        inplace: bool, default False\n            if True, do operation inplace and return None\n        kwargs: the values on which to normalize (e.g., `year=2005`)\n        \"\"\"\n        if len(kwargs) > 1 or self.time_col not in kwargs:\n            raise ValueError('Only time(year)-based normalization supported')\n        ret = copy.deepcopy(self) if not inplace else self\n        df = ret.data\n        # change all below if supporting more in the future\n        cols = self.time_col\n        value = kwargs[self.time_col]\n        x = df.set_index(IAMC_IDX)\n        x['value'] /= x[x[cols] == value]['value']\n        ret.data = x.reset_index()\n        if not inplace:\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aggregate(self, variable, components=None, append=False):\n        # default components to all variables one level below `variable`\n        components = components or self._variable_components(variable)\n\n        if not len(components):\n            msg = 'cannot aggregate variable `{}` because it has no components'\n            logger().info(msg.format(variable))\n\n            return\n\n        rows = self._apply_filters(variable=components)\n        _data = _aggregate(self.data[rows], 'variable')\n\n        if append is True:\n            self.append(_data, variable=variable, inplace=True)\n        else:\n            return _data", "response": "Compute the aggregate of timeseries components or sub - categories of a variable"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether a timeseries matches the aggregation of its components.", "response": "def check_aggregate(self, variable, components=None, exclude_on_fail=False,\n                        multiplier=1, **kwargs):\n        \"\"\"Check whether a timeseries matches the aggregation of its components\n\n        Parameters\n        ----------\n        variable: str\n            variable to be checked for matching aggregation of sub-categories\n        components: list of str, default None\n            list of variables, defaults to all sub-categories of `variable`\n        exclude_on_fail: boolean, default False\n            flag scenarios failing validation as `exclude: True`\n        multiplier: number, default 1\n            factor when comparing variable and sum of components\n        kwargs: passed to `np.isclose()`\n        \"\"\"\n        # compute aggregate from components, return None if no components\n        df_components = self.aggregate(variable, components)\n        if df_components is None:\n            return\n\n        # filter and groupby data, use `pd.Series.align` for matching index\n        rows = self._apply_filters(variable=variable)\n        df_variable, df_components = (\n            _aggregate(self.data[rows], 'variable').align(df_components)\n        )\n\n        # use `np.isclose` for checking match\n        diff = df_variable[~np.isclose(df_variable, multiplier * df_components,\n                                       **kwargs)]\n\n        if len(diff):\n            msg = '`{}` - {} of {} rows are not aggregates of components'\n            logger().info(msg.format(variable, len(diff), len(df_variable)))\n\n            if exclude_on_fail:\n                self._exclude_on_fail(diff.index.droplevel([2, 3, 4]))\n\n            return IamDataFrame(diff, variable=variable).timeseries()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the aggregate of timeseries over a number of regions with variable components only defined at the level of the given variable in the specified region.", "response": "def aggregate_region(self, variable, region='World', subregions=None,\n                         components=None, append=False):\n        \"\"\"Compute the aggregate of timeseries over a number of regions\n        including variable components only defined at the `region` level\n\n        Parameters\n        ----------\n        variable: str\n            variable for which the aggregate should be computed\n        region: str, default 'World'\n            dimension\n        subregions: list of str\n            list of subregions, defaults to all regions other than `region`\n        components: list of str\n            list of variables, defaults to all sub-categories of `variable`\n            included in `region` but not in any of `subregions`\n        append: bool, default False\n            append the aggregate timeseries to `data` and return None,\n            else return aggregate timeseries\n        \"\"\"\n        # default subregions to all regions other than `region`\n        if subregions is None:\n            rows = self._apply_filters(variable=variable)\n            subregions = set(self.data[rows].region) - set([region])\n\n        if not len(subregions):\n            msg = 'cannot aggregate variable `{}` to `{}` because it does not'\\\n                  ' exist in any subregion'\n            logger().info(msg.format(variable, region))\n\n            return\n\n        # compute aggregate over all subregions\n        subregion_df = self.filter(region=subregions)\n        cols = ['region', 'variable']\n        _data = _aggregate(subregion_df.filter(variable=variable).data, cols)\n\n        # add components at the `region` level, defaults to all variables one\n        # level below `variable` that are only present in `region`\n        region_df = self.filter(region=region)\n        components = components or (\n            set(region_df._variable_components(variable)).difference(\n                subregion_df._variable_components(variable)))\n\n        if len(components):\n            rows = region_df._apply_filters(variable=components)\n            _data = _data.add(_aggregate(region_df.data[rows], cols),\n                              fill_value=0)\n\n        if append is True:\n            self.append(_data, region=region, variable=variable, inplace=True)\n        else:\n            return _data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether the timeseries data for a given variable matches the aggregation of subregions.", "response": "def check_aggregate_region(self, variable, region='World', subregions=None,\n                               components=None, exclude_on_fail=False,\n                               **kwargs):\n        \"\"\"Check whether the region timeseries data match the aggregation\n        of components\n\n        Parameters\n        ----------\n        variable: str\n            variable to be checked for matching aggregation of subregions\n        region: str, default 'World'\n            region to be checked for matching aggregation of subregions\n        subregions: list of str\n            list of subregions, defaults to all regions other than `region`\n        components: list of str, default None\n            list of variables, defaults to all sub-categories of `variable`\n            included in `region` but not in any of `subregions`\n        exclude_on_fail: boolean, default False\n            flag scenarios failing validation as `exclude: True`\n        kwargs: passed to `np.isclose()`\n        \"\"\"\n        # compute aggregate from subregions, return None if no subregions\n        df_subregions = self.aggregate_region(variable, region, subregions,\n                                              components)\n        if df_subregions is None:\n            return\n\n        # filter and groupby data, use `pd.Series.align` for matching index\n        rows = self._apply_filters(region=region, variable=variable)\n        df_region, df_subregions = (\n            _aggregate(self.data[rows], ['region', 'variable'])\n            .align(df_subregions)\n        )\n\n        # use `np.isclose` for checking match\n        diff = df_region[~np.isclose(df_region, df_subregions, **kwargs)]\n\n        if len(diff):\n            msg = (\n                '`{}` - {} of {} rows are not aggregates of subregions'\n            )\n            logger().info(msg.format(variable, len(diff), len(df_region)))\n\n            if exclude_on_fail:\n                self._exclude_on_fail(diff.index.droplevel([2, 3]))\n\n            col_args = dict(region=region, variable=variable)\n            return IamDataFrame(diff, **col_args).timeseries()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all components of a variable", "response": "def _variable_components(self, variable):\n        \"\"\"Get all components (sub-categories) of a variable\n\n        For `variable='foo'`, return `['foo|bar']`, but don't include\n        `'foo|bar|baz'`, which is a sub-sub-category\"\"\"\n        var_list = pd.Series(self.data.variable.unique())\n        return var_list[pattern_match(var_list, '{}|*'.format(variable), 0)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_internal_consistency(self, **kwargs):\n        inconsistent_vars = {}\n        for variable in self.variables():\n            diff_agg = self.check_aggregate(variable, **kwargs)\n            if diff_agg is not None:\n                inconsistent_vars[variable + \"-aggregate\"] = diff_agg\n\n            diff_regional = self.check_aggregate_region(variable, **kwargs)\n            if diff_regional is not None:\n                inconsistent_vars[variable + \"-regional\"] = diff_regional\n\n        return inconsistent_vars if inconsistent_vars else None", "response": "Check whether the database is internally consistent with the current database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _exclude_on_fail(self, df):\n        idx = df if isinstance(df, pd.MultiIndex) else _meta_idx(df)\n        self.meta.loc[idx, 'exclude'] = True\n        logger().info('{} non-valid scenario{} will be excluded'\n                      .format(len(idx), '' if len(idx) == 1 else 's'))", "response": "Assign a selection of scenarios as exclude : True in meta"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new IamDataFrame with only the entries that satisfy the filters.", "response": "def filter(self, filters=None, keep=True, inplace=False, **kwargs):\n        \"\"\"Return a filtered IamDataFrame (i.e., a subset of current data)\n\n        Parameters\n        ----------\n        keep: bool, default True\n            keep all scenarios satisfying the filters (if True) or the inverse\n        inplace: bool, default False\n            if True, do operation inplace and return None\n        filters by kwargs or dict (deprecated):\n            The following columns are available for filtering:\n             - metadata columns: filter by category assignment\n             - 'model', 'scenario', 'region', 'variable', 'unit':\n               string or list of strings, where `*` can be used as a wildcard\n             - 'level': the maximum \"depth\" of IAM variables (number of '|')\n               (excluding the strings given in the 'variable' argument)\n             - 'year': takes an integer, a list of integers or a range\n               note that the last year of a range is not included,\n               so `range(2010, 2015)` is interpreted as `[2010, ..., 2014]`\n             - arguments for filtering by `datetime.datetime`\n               ('month', 'hour', 'time')\n             - 'regexp=True' disables pseudo-regexp syntax in `pattern_match()`\n        \"\"\"\n        if filters is not None:\n            msg = '`filters` keyword argument in `filter()` is deprecated ' + \\\n                'and will be removed in the next release'\n            warnings.warn(msg)\n            kwargs.update(filters)\n\n        _keep = self._apply_filters(**kwargs)\n        _keep = _keep if keep else ~_keep\n        ret = copy.deepcopy(self) if not inplace else self\n        ret.data = ret.data[_keep]\n\n        idx = _make_index(ret.data)\n        if len(idx) == 0:\n            logger().warning('Filtered IamDataFrame is empty!')\n        ret.meta = ret.meta.loc[idx]\n        if not inplace:\n            return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _apply_filters(self, **filters):\n        regexp = filters.pop('regexp', False)\n        keep = np.array([True] * len(self.data))\n\n        # filter by columns and list of values\n        for col, values in filters.items():\n            # treat `_apply_filters(col=None)` as no filter applied\n            if values is None:\n                continue\n\n            if col in self.meta.columns:\n                matches = pattern_match(self.meta[col], values, regexp=regexp)\n                cat_idx = self.meta[matches].index\n                keep_col = (self.data[META_IDX].set_index(META_IDX)\n                                .index.isin(cat_idx))\n\n            elif col == 'variable':\n                level = filters['level'] if 'level' in filters else None\n                keep_col = pattern_match(self.data[col], values, level, regexp)\n\n            elif col == 'year':\n                _data = self.data[col] if self.time_col is not 'time' \\\n                    else self.data['time'].apply(lambda x: x.year)\n                keep_col = years_match(_data, values)\n\n            elif col == 'month' and self.time_col is 'time':\n                keep_col = month_match(self.data['time']\n                                           .apply(lambda x: x.month),\n                                       values)\n\n            elif col == 'day' and self.time_col is 'time':\n                if isinstance(values, str):\n                    wday = True\n                elif isinstance(values, list) and isinstance(values[0], str):\n                    wday = True\n                else:\n                    wday = False\n\n                if wday:\n                    days = self.data['time'].apply(lambda x: x.weekday())\n                else:  # ints or list of ints\n                    days = self.data['time'].apply(lambda x: x.day)\n\n                keep_col = day_match(days, values)\n\n            elif col == 'hour' and self.time_col is 'time':\n                keep_col = hour_match(self.data['time']\n                                          .apply(lambda x: x.hour),\n                                      values)\n\n            elif col == 'time' and self.time_col is 'time':\n                keep_col = datetime_match(self.data[col], values)\n\n            elif col == 'level':\n                if 'variable' not in filters.keys():\n                    keep_col = find_depth(self.data['variable'], level=values)\n                else:\n                    continue\n\n            elif col in self.data.columns:\n                keep_col = pattern_match(self.data[col], values, regexp=regexp)\n\n            else:\n                _raise_filter_error(col)\n\n            keep &= keep_col\n\n        return keep", "response": "Return a numpy array of boolean values that are not in the current data set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef col_apply(self, col, func, *args, **kwargs):\n        if col in self.data:\n            self.data[col] = self.data[col].apply(func, *args, **kwargs)\n        else:\n            self.meta[col] = self.meta[col].apply(func, *args, **kwargs)", "response": "Applies a function to a column in the log."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_file_format(self, iamc_index):\n        df = self.timeseries(iamc_index=iamc_index).reset_index()\n        df = df.rename(columns={c: str(c).title() for c in df.columns})\n        return df", "response": "Return a dataframe suitable for writing to a file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_csv(self, path, iamc_index=False, **kwargs):\n        self._to_file_format(iamc_index).to_csv(path, index=False, **kwargs)", "response": "Write the timeseries data to a csv file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_excel(self, excel_writer, sheet_name='data',\n                 iamc_index=False, **kwargs):\n        \"\"\"Write timeseries data to Excel format\n\n        Parameters\n        ----------\n        excel_writer: string or ExcelWriter object\n            file path or existing ExcelWriter\n        sheet_name: string, default 'data'\n            name of sheet which will contain `IamDataFrame.timeseries()` data\n        iamc_index: bool, default False\n            if True, use `['model', 'scenario', 'region', 'variable', 'unit']`;\n            else, use all `data` columns\n        \"\"\"\n        if not isinstance(excel_writer, pd.ExcelWriter):\n            close = True\n            excel_writer = pd.ExcelWriter(excel_writer)\n        self._to_file_format(iamc_index)\\\n            .to_excel(excel_writer, sheet_name=sheet_name, index=False,\n                      **kwargs)\n        if close:\n            excel_writer.close()", "response": "Write timeseries data to Excel format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexport metadata to Excel file", "response": "def export_metadata(self, path):\n        \"\"\"Export metadata to Excel\n\n        Parameters\n        ----------\n        path: string\n            path/filename for xlsx file of metadata export\n        \"\"\"\n        writer = pd.ExcelWriter(path)\n        write_sheet(writer, 'meta', self.meta, index=True)\n        writer.save()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload metadata exported from an Excel file.", "response": "def load_metadata(self, path, *args, **kwargs):\n        \"\"\"Load metadata exported from `pyam.IamDataFrame` instance\n\n        Parameters\n        ----------\n        path: string\n            xlsx file with metadata exported from `pyam.IamDataFrame` instance\n        \"\"\"\n        if not os.path.exists(path):\n            raise ValueError(\"no metadata file '\" + path + \"' found!\")\n\n        if path.endswith('csv'):\n            df = pd.read_csv(path, *args, **kwargs)\n        else:\n            xl = pd.ExcelFile(path)\n            if len(xl.sheet_names) > 1 and 'sheet_name' not in kwargs:\n                kwargs['sheet_name'] = 'meta'\n            df = pd.read_excel(path, *args, **kwargs)\n\n        req_cols = ['model', 'scenario', 'exclude']\n        if not set(req_cols).issubset(set(df.columns)):\n            e = 'File `{}` does not have required columns ({})!'\n            raise ValueError(e.format(path, req_cols))\n\n        # set index, filter to relevant scenarios from imported metadata file\n        df.set_index(META_IDX, inplace=True)\n        idx = self.meta.index.intersection(df.index)\n\n        n_invalid = len(df) - len(idx)\n        if n_invalid > 0:\n            msg = 'Ignoring {} scenario{} from imported metadata'\n            logger().info(msg.format(n_invalid, 's' if n_invalid > 1 else ''))\n\n        if idx.empty:\n            raise ValueError('No valid scenarios in imported metadata file!')\n\n        df = df.loc[idx]\n\n        # Merge in imported metadata\n        msg = 'Importing metadata for {} scenario{} (for total of {})'\n        logger().info(msg.format(len(df), 's' if len(df) > 1 else '',\n                                 len(self.meta)))\n\n        for col in df.columns:\n            self._new_meta_column(col)\n            self.meta[col] = df[col].combine_first(self.meta[col])\n        # set column `exclude` to bool\n        self.meta.exclude = self.meta.exclude.astype('bool')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef line_plot(self, x='year', y='value', **kwargs):\n        df = self.as_pandas(with_metadata=kwargs)\n\n        # pivot data if asked for explicit variable name\n        variables = df['variable'].unique()\n        if x in variables or y in variables:\n            keep_vars = set([x, y]) & set(variables)\n            df = df[df['variable'].isin(keep_vars)]\n            idx = list(set(df.columns) - set(['value']))\n            df = (df\n                  .reset_index()\n                  .set_index(idx)\n                  .value  # df -> series\n                  .unstack(level='variable')  # keep_vars are columns\n                  .rename_axis(None, axis=1)  # rm column index name\n                  .reset_index()\n                  .set_index(META_IDX)\n                  )\n            if x != 'year' and y != 'year':\n                df = df.drop('year', axis=1)  # years causes NaNs\n\n        ax, handles, labels = plotting.line_plot(\n            df.dropna(), x=x, y=y, **kwargs)\n        return ax", "response": "Plot timeseries lines of existing data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots timeseries stacks of existing data", "response": "def stack_plot(self, *args, **kwargs):\n        \"\"\"Plot timeseries stacks of existing data\n\n        see pyam.plotting.stack_plot() for all available options\n        \"\"\"\n        df = self.as_pandas(with_metadata=True)\n        ax = plotting.stack_plot(df, *args, **kwargs)\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scatter(self, x, y, **kwargs):\n        variables = self.data['variable'].unique()\n        xisvar = x in variables\n        yisvar = y in variables\n        if not xisvar and not yisvar:\n            cols = [x, y] + self._discover_meta_cols(**kwargs)\n            df = self.meta[cols].reset_index()\n        elif xisvar and yisvar:\n            # filter pivot both and rename\n            dfx = (\n                self\n                .filter(variable=x)\n                .as_pandas(with_metadata=kwargs)\n                .rename(columns={'value': x, 'unit': 'xunit'})\n                .set_index(YEAR_IDX)\n                .drop('variable', axis=1)\n            )\n            dfy = (\n                self\n                .filter(variable=y)\n                .as_pandas(with_metadata=kwargs)\n                .rename(columns={'value': y, 'unit': 'yunit'})\n                .set_index(YEAR_IDX)\n                .drop('variable', axis=1)\n            )\n            df = dfx.join(dfy, lsuffix='_left', rsuffix='').reset_index()\n        else:\n            # filter, merge with meta, and rename value column to match var\n            var = x if xisvar else y\n            df = (\n                self\n                .filter(variable=var)\n                .as_pandas(with_metadata=kwargs)\n                .rename(columns={'value': var})\n            )\n        ax = plotting.scatter(df.dropna(), x, y, **kwargs)\n        return ax", "response": "Plot a scatter chart using metadata columns"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_regions(self, map_col, agg=None, copy_col=None, fname=None,\n                    region_col=None, remove_duplicates=False, inplace=False):\n        \"\"\"Plot regional data for a single model, scenario, variable, and year\n\n        see pyam.plotting.region_plot() for all available options\n\n        Parameters\n        ----------\n        map_col: string\n            The column used to map new regions to. Common examples include\n            iso and 5_region.\n        agg: string, optional\n            Perform a data aggregation. Options include: sum.\n        copy_col: string, optional\n            Copy the existing region data into a new column for later use.\n        fname: string, optional\n            Use a non-default region mapping file\n        region_col: string, optional\n            Use a non-default column name for regions to map from.\n        remove_duplicates: bool, optional, default: False\n            If there are duplicates in the mapping from one regional level to\n            another, then remove these duplicates by counting the most common\n            mapped value.\n            This option is most useful when mapping from high resolution\n            (e.g., model regions) to low resolution (e.g., 5_region).\n        inplace : bool, default False\n            if True, do operation inplace and return None\n        \"\"\"\n        models = self.meta.index.get_level_values('model').unique()\n        fname = fname or run_control()['region_mapping']['default']\n        mapping = read_pandas(fname).rename(str.lower, axis='columns')\n        map_col = map_col.lower()\n\n        ret = copy.deepcopy(self) if not inplace else self\n        _df = ret.data\n        columns_orderd = _df.columns\n\n        # merge data\n        dfs = []\n        for model in models:\n            df = _df[_df['model'] == model]\n            _col = region_col or '{}.REGION'.format(model)\n            _map = mapping.rename(columns={_col.lower(): 'region'})\n            _map = _map[['region', map_col]].dropna().drop_duplicates()\n            _map = _map[_map['region'].isin(_df['region'])]\n            if remove_duplicates and _map['region'].duplicated().any():\n                # find duplicates\n                where_dup = _map['region'].duplicated(keep=False)\n                dups = _map[where_dup]\n                logger().warning(\"\"\"\n                Duplicate entries found for the following regions.\n                Mapping will occur only for the most common instance.\n                {}\"\"\".format(dups['region'].unique()))\n                # get non duplicates\n                _map = _map[~where_dup]\n                # order duplicates by the count frequency\n                dups = (dups\n                        .groupby(['region', map_col])\n                        .size()\n                        .reset_index(name='count')\n                        .sort_values(by='count', ascending=False)\n                        .drop('count', axis=1))\n                # take top occurance\n                dups = dups[~dups['region'].duplicated(keep='first')]\n                # combine them back\n                _map = pd.concat([_map, dups])\n            if copy_col is not None:\n                df[copy_col] = df['region']\n\n            df = (df\n                  .merge(_map, on='region')\n                  .drop('region', axis=1)\n                  .rename(columns={map_col: 'region'})\n                  )\n            dfs.append(df)\n        df = pd.concat(dfs)\n\n        # perform aggregations\n        if agg == 'sum':\n            df = df.groupby(self._LONG_IDX).sum().reset_index()\n\n        ret.data = (df\n                    .reindex(columns=columns_orderd)\n                    .sort_values(SORT_IDX)\n                    .reset_index(drop=True)\n                    )\n        if not inplace:\n            return ret", "response": "Plot regional data for a single model and variable and year."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef region_plot(self, **kwargs):\n        df = self.as_pandas(with_metadata=True)\n        ax = plotting.region_plot(df, **kwargs)\n        return ax", "response": "Plot regional data for a single model scenario variable and year"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd additional run control parameters to the store", "response": "def update(self, rc):\n        \"\"\"Add additional run control parameters\n\n        Parameters\n        ----------\n        rc : string, file, dictionary, optional\n            a path to a YAML file, a file handle for a YAML file, or a\n            dictionary describing run control configuration\n        \"\"\"\n        rc = self._load_yaml(rc)\n        self.store = _recursive_update(self.store, rc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill_series(x, year):\n    x = x.dropna()\n    if year in x.index and not np.isnan(x[year]):\n        return x[year]\n    else:\n        prev = [i for i in x.index if i < year]\n        nxt = [i for i in x.index if i > year]\n        if prev and nxt:\n            p = max(prev)\n            n = min(nxt)\n            return ((n - year) * x[p] + (year - p) * x[n]) / (n - p)\n        else:\n            return np.nan", "response": "Returns the value of a timeseries for a year\n    by linear interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cumulative(x, first_year, last_year):\n    # if the timeseries does not cover the range `[first_year, last_year]`,\n    # return nan to avoid erroneous aggregation\n    if min(x.index) > first_year:\n        logger().warning('the timeseries `{}` does not start by {}'.format(\n            x.name or x, first_year))\n        return np.nan\n    if max(x.index) < last_year:\n        logger().warning('the timeseries `{}` does not extend until {}'\n                         .format(x.name or x, last_year))\n        return np.nan\n\n    # make sure we're using integers\n    to_int(x, index=True)\n\n    x[first_year] = fill_series(x, first_year)\n    x[last_year] = fill_series(x, last_year)\n\n    years = [i for i in x.index if i >= first_year and i <= last_year\n             and ~np.isnan(x[i])]\n    years.sort()\n\n    # loop over years\n    if not np.isnan(x[first_year]) and not np.isnan(x[last_year]):\n        value = 0\n        for (i, yr) in enumerate(years[:-1]):\n            next_yr = years[i + 1]\n            # the summation is shifted to include the first year fully in sum,\n            # otherwise, would return a weighted average of `yr` and `next_yr`\n            value += ((next_yr - yr - 1) * x[next_yr] +\n                      (next_yr - yr + 1) * x[yr]) / 2\n\n        # the loop above does not include the last element in range\n        # (`last_year`), therefore added explicitly\n        value += x[last_year]\n\n        return value", "response": "Returns the cumulative sum of a timeseries over years."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cross_threshold(x, threshold=0, direction=['from above', 'from below']):\n    prev_yr, prev_val = None, None\n    years = []\n    direction = [direction] if isstr(direction) else list(direction)\n    if not set(direction).issubset(set(['from above', 'from below'])):\n        raise ValueError('invalid direction `{}`'.format(direction))\n\n    for yr, val in zip(x.index, x.values):\n        if np.isnan(val):  # ignore nans in the timeseries\n            continue\n        if prev_val is None:\n            prev_yr, prev_val = yr, val\n            continue\n        if not np.sign(prev_val - threshold) == np.sign(val - threshold):\n            if ('from above' in direction and prev_val > val) \\\n                    or ('from below' in direction and prev_val < val):\n                change = (val - prev_val) / (yr - prev_yr)\n                # add one because int() rounds down\n                cross_yr = prev_yr + int((threshold - prev_val) / change) + 1\n                years.append(cross_yr)\n        prev_yr, prev_val = yr, val\n    return years", "response": "Returns a list of the years in which a timeseries is crosses a given threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads an IIASA database and returns a DataFrame containing the data and metadata for the current IIASA record.", "response": "def read_iiasa(name, meta=False, **kwargs):\n    \"\"\"\n    Query an IIASA database. See Connection.query() for more documentation\n\n    Parameters\n    ----------\n    name : str\n        A valid IIASA database name, see pyam.iiasa.valid_connection_names()\n    meta : bool or list of strings\n        If not False, also include metadata indicators (or subset if provided).\n    kwargs :\n        Arguments for pyam.iiasa.Connection.query()\n    \"\"\"\n    conn = Connection(name)\n    # data\n    df = conn.query(**kwargs)\n    df = IamDataFrame(df)\n    # metadata\n    if meta:\n        mdf = conn.metadata()\n        # only data for models/scenarios in df\n        mdf = mdf[mdf.model.isin(df['model'].unique()) &\n                  mdf.scenario.isin(df['scenario'].unique())]\n        # get subset of data if meta is a list\n        if islistable(meta):\n            mdf = mdf[['model', 'scenario'] + meta]\n        mdf = mdf.set_index(['model', 'scenario'])\n        # we have to loop here because `set_meta()` can only take series\n        for col in mdf:\n            df.set_meta(mdf[col])\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scenario_list(self, default=True):\n        default = 'true' if default else 'false'\n        add_url = 'runs?getOnlyDefaultRuns={}'\n        url = self.base_url + add_url.format(default)\n        headers = {'Authorization': 'Bearer {}'.format(self.auth())}\n        r = requests.get(url, headers=headers)\n        return pd.read_json(r.content, orient='records')", "response": "Return a list of all scenarios in the connected data source."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all scenario metadata indicators available in the connected data source", "response": "def available_metadata(self):\n        \"\"\"\n        List all scenario metadata indicators available in the connected\n        data source\n        \"\"\"\n        url = self.base_url + 'metadata/types'\n        headers = {'Authorization': 'Bearer {}'.format(self.auth())}\n        r = requests.get(url, headers=headers)\n        return pd.read_json(r.content, orient='records')['name']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef metadata(self, default=True):\n        # at present this reads in all data for all scenarios, it could be sped\n        # up in the future to try to query a subset\n        default = 'true' if default else 'false'\n        add_url = 'runs?getOnlyDefaultRuns={}&includeMetadata=true'\n        url = self.base_url + add_url.format(default)\n        headers = {'Authorization': 'Bearer {}'.format(self.auth())}\n        r = requests.get(url, headers=headers)\n        df = pd.read_json(r.content, orient='records')\n\n        def extract(row):\n            return (\n                pd.concat([row[['model', 'scenario']],\n                           pd.Series(row.metadata)])\n                .to_frame()\n                .T\n                .set_index(['model', 'scenario'])\n            )\n\n        return pd.concat([extract(row) for idx, row in df.iterrows()],\n                         sort=False).reset_index()", "response": "Return the metadata of the scenarios in the connected data source."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef variables(self):\n        url = self.base_url + 'ts'\n        headers = {'Authorization': 'Bearer {}'.format(self.auth())}\n        r = requests.get(url, headers=headers)\n        df = pd.read_json(r.content, orient='records')\n        return pd.Series(df['variable'].unique(), name='variable')", "response": "All variables in the connected data source"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a row with describe columns to a concise string", "response": "def format_rows(row, center, fullrange=None, interquartile=None,\n                custom_format='{:.2f}'):\n    \"\"\"Format a row with `describe()` columns to a concise string\"\"\"\n    if (fullrange or 0) + (interquartile or 0) == 1:\n        legend = '{} ({})'.format(center, 'max, min' if fullrange is True\n                                  else 'interquartile range')\n        index = row.index.droplevel(2).drop_duplicates()\n        count_arg = dict(tuples=[('count', '')], names=[None, legend])\n    else:\n        msg = 'displaying multiple range formats simultaneously not supported'\n        raise NotImplementedError(msg)\n\n    ret = pd.Series(index=pd.MultiIndex.from_tuples(**count_arg).append(index))\n\n    row = row.sort_index()\n    center = '50%' if center == 'median' else center\n\n    # get maximum of `count` and write to first entry of return series\n    count = max([i for i in row.loc[(slice(None), slice(None), 'count')]\n                 if not np.isnan(i)])\n    ret.loc[('count', '')] = ('{:.0f}'.format(count)) if count > 1 else ''\n\n    # set upper and lower for the range\n    upper, lower = ('max', 'min') if fullrange is True else ('75%', '25%')\n\n    # format `describe()` columns to string output\n    for i in index:\n        x = row.loc[i]\n        _count = x['count']\n        if np.isnan(_count) or _count == 0:\n            s = ''\n        elif _count > 1:\n            s = '{f} ({f}, {f})'.format(f=custom_format)\\\n                .format(x[center], x[upper], x[lower])\n        elif _count == 1:\n            s = '{f}'.format(f=custom_format).format(x['50%'])\n        # add count of this section as `[]` if different from count_max\n        if 0 < _count < count:\n            s += ' [{:.0f}]'.format(_count)\n        ret.loc[i] = s\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new entry to the summary statistics.", "response": "def add(self, data, header, row=None, subheader=None):\n        \"\"\"Filter `data` by arguments of this SummaryStats instance,\n        then apply `pd.describe()` and format the statistics\n\n        Parameters\n        ----------\n        data : pd.DataFrame or pd.Series\n            data for which summary statistics should be computed\n        header : str\n            column name for descriptive statistics\n        row : str\n            row name for descriptive statistics\n            (required if `pyam.Statistics(rows=True)`)\n        subheader : str, optional\n            column name (level=1) if data is a unnamed `pd.Series`\n        \"\"\"\n        # verify validity of specifications\n        if self.rows is not None and row is None:\n            raise ValueError('row specification required')\n        if self.rows is None and row is not None:\n            raise ValueError('row arg illegal for this `Statistics` instance')\n        if isinstance(data, pd.Series):\n            if subheader is not None:\n                data.name = subheader\n            elif data.name is None:\n                msg = '`data` must be named `pd.Series` or provide `subheader`'\n                raise ValueError(msg)\n            data = pd.DataFrame(data)\n\n        if self.rows is not None and row not in self.rows:\n            self.rows.append(row)\n\n        _stats = None\n\n        # describe with groupby feature\n        if self.groupby is not None:\n            filter_args = dict(data=data, df=self.df, join_meta=True)\n            filter_args.update(self.groupby)\n            _stats = (\n                filter_by_meta(**filter_args).groupby(self.col)\n                .describe(percentiles=self.percentiles)\n            )\n            _stats = pd.concat([_stats], keys=[self.col], names=[''], axis=0)\n            if self.rows:\n                _stats['row'] = row\n                _stats.set_index('row', append=True, inplace=True)\n            _stats.index.names = [''] * 3 if self.rows else [''] * 2\n\n        # describe with filter feature\n        for (idx, _filter) in self.filters:\n            filter_args = dict(data=data, df=self.df)\n            filter_args.update(_filter)\n            _stats_f = (\n                filter_by_meta(**filter_args)\n                .describe(percentiles=self.percentiles)\n            )\n            _stats_f = pd.DataFrame(_stats_f.unstack()).T\n            if self.idx_depth == 1:\n                levels = [[idx]]\n            else:\n                levels = [[idx[0]], [idx[1]]]\n            lvls, lbls = (levels, [[0]] * self.idx_depth) if not self.rows \\\n                else (levels + [[row]], [[0]] * (self.idx_depth + 1))\n            _stats_f.index = pd.MultiIndex(levels=lvls, labels=lbls)\n            _stats = _stats_f if _stats is None else _stats.append(_stats_f)\n\n        # add header\n        _stats = pd.concat([_stats], keys=[header], names=[''], axis=1)\n        subheader = _stats.columns.get_level_values(1).unique()\n        self._add_to_header(header, subheader)\n\n        # set statistics\n        if self.stats is None:\n            self.stats = _stats\n        else:\n            self.stats = _stats.combine_first(self.stats)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reindex(self, copy=True):\n        ret = deepcopy(self) if copy else self\n\n        ret.stats = ret.stats.reindex(index=ret._idx, level=0)\n        if ret.idx_depth == 2:\n            ret.stats = ret.stats.reindex(index=ret._sub_idx, level=1)\n        if ret.rows is not None:\n            ret.stats = ret.stats.reindex(index=ret.rows, level=ret.idx_depth)\n\n        ret.stats = ret.stats.reindex(columns=ret._headers, level=0)\n        ret.stats = ret.stats.reindex(columns=ret._subheaders, level=1)\n        ret.stats = ret.stats.reindex(columns=ret._describe_cols, level=2)\n\n        if copy:\n            return ret", "response": "Reindex the summary statistics dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef summarize(self, center='mean', fullrange=None, interquartile=None,\n                  custom_format='{:.2f}'):\n        \"\"\"Format the compiled statistics to a concise string output\n\n        Parameter\n        ---------\n        center : str, default `mean`\n            what to return as 'center' of the summary: `mean`, `50%`, `median`\n        fullrange : bool, default None\n            return full range of data if True or `fullrange`, `interquartile`\n            and `format_spec` are None\n        interquartile : bool, default None\n            return interquartile range if True\n        custom_format : formatting specifications\n        \"\"\"\n        # call `reindex()` to reorder index and columns\n        self.reindex(copy=False)\n\n        center = 'median' if center == '50%' else center\n        if fullrange is None and interquartile is None:\n            fullrange = True\n        return self.stats.apply(format_rows, center=center,\n                                fullrange=fullrange,\n                                interquartile=interquartile,\n                                custom_format=custom_format,\n                                axis=1, raw=False)", "response": "Format the compiled statistics to a concise string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_default_props(**kwargs):\n    global _DEFAULT_PROPS\n    pcycle = plt.rcParams['axes.prop_cycle']\n    _DEFAULT_PROPS = {\n        'color': itertools.cycle(_get_standard_colors(**kwargs))\n        if len(kwargs) > 0 else itertools.cycle([x['color'] for x in pcycle]),\n        'marker': itertools.cycle(['o', 'x', '.', '+', '*']),\n        'linestyle': itertools.cycle(['-', '--', '-.', ':']),\n    }", "response": "Reset default properties to initial cycle point"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning current default properties", "response": "def default_props(reset=False, **kwargs):\n    \"\"\"Return current default properties\n\n    Parameters\n    ----------\n    reset : bool\n            if True, reset properties and return\n            default: False\n    \"\"\"\n    global _DEFAULT_PROPS\n    if _DEFAULT_PROPS is None or reset:\n        reset_default_props(**kwargs)\n    return _DEFAULT_PROPS"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassigns the style properties for a single object to a single object.", "response": "def assign_style_props(df, color=None, marker=None, linestyle=None,\n                       cmap=None):\n    \"\"\"Assign the style properties for a plot\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        data to be used for style properties\n    \"\"\"\n    if color is None and cmap is not None:\n        raise ValueError('`cmap` must be provided with the `color` argument')\n\n    # determine color, marker, and linestyle for each line\n    n = len(df[color].unique()) if color in df.columns else \\\n        len(df[list(set(df.columns) & set(IAMC_IDX))].drop_duplicates())\n    defaults = default_props(reset=True, num_colors=n, colormap=cmap)\n\n    props = {}\n    rc = run_control()\n\n    kinds = [('color', color), ('marker', marker), ('linestyle', linestyle)]\n\n    for kind, var in kinds:\n        rc_has_kind = kind in rc\n        if var in df.columns:\n            rc_has_var = rc_has_kind and var in rc[kind]\n            props_for_kind = {}\n\n            for val in df[var].unique():\n                if rc_has_var and val in rc[kind][var]:\n                    props_for_kind[val] = rc[kind][var][val]\n                    # cycle any way to keep defaults the same\n                    next(defaults[kind])\n                else:\n                    props_for_kind[val] = next(defaults[kind])\n            props[kind] = props_for_kind\n\n    # update for special properties only if they exist in props\n    if 'color' in props:\n        d = props['color']\n        values = list(d.values())\n        # find if any colors in our properties corresponds with special colors\n        # we know about\n        overlap_idx = np.in1d(values, list(PYAM_COLORS.keys()))\n        if overlap_idx.any():  # some exist in our special set\n            keys = np.array(list(d.keys()))[overlap_idx]\n            values = np.array(values)[overlap_idx]\n            # translate each from pyam name, like AR6-SSP2-45 to proper color\n            # designation\n            for k, v in zip(keys, values):\n                d[k] = PYAM_COLORS[v]\n            # replace props with updated dict without special colors\n            props['color'] = d\n    return props"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread a shapefile for use in regional plots.", "response": "def read_shapefile(fname, region_col=None, **kwargs):\n    \"\"\"Read a shapefile for use in regional plots. Shapefiles must have a\n    column denoted as \"region\".\n\n    Parameters\n    ----------\n    fname : string\n        path to shapefile to be read by geopandas\n    region_col : string, default None\n        if provided, rename a column in the shapefile to \"region\"\n    \"\"\"\n    gdf = gpd.read_file(fname, **kwargs)\n    if region_col is not None:\n        gdf = gdf.rename(columns={region_col: 'region'})\n    if 'region' not in gdf.columns:\n        raise IOError('Must provide a region column')\n    gdf['region'] = gdf['region'].str.upper()\n    return gdf"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef region_plot(df, column='value', ax=None, crs=None, gdf=None,\n                add_features=True, vmin=None, vmax=None, cmap=None,\n                cbar=True, legend=False, title=True):\n    \"\"\"Plot data on a map.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Data to plot as a long-form data frame\n    column : string, optional, default: 'value'\n        The column to use for plotting values\n    ax : matplotlib.Axes, optional\n    crs : cartopy.crs, optional\n        The crs to plot, PlateCarree is used by default.\n    gdf : geopandas.GeoDataFrame, optional\n        The geometries to plot. The gdf must have a \"region\" column.\n    add_features : bool, optional, default: True\n        If true, add land, ocean, coastline, and border features.\n    vmin : numeric, optional\n        The minimum value to plot.\n    vmax : numeric, optional\n        The maximum value to plot.\n    cmap : string, optional\n        The colormap to use.\n    cbar : bool or dictionary, optional, default: True\n        Add a colorbar. If a dictionary is provided, it will be used as keyword\n        arguments in creating the colorbar.\n    legend : bool or dictionary, optional, default: False\n        Add a legend. If a dictionary is provided, it will be used as keyword\n        arguments in creating the legend.\n    title : bool or string, optional\n        Display a default or custom title.\n    \"\"\"\n    for col in ['model', 'scenario', 'year', 'variable']:\n        if len(df[col].unique()) > 1:\n            msg = 'Can not plot multiple {}s in region_plot'\n            raise ValueError(msg.format(col))\n\n    crs = crs or cartopy.crs.PlateCarree()\n    if ax is None:\n        fig, ax = plt.subplots(subplot_kw=dict(projection=crs))\n    elif not isinstance(ax, cartopy.mpl.geoaxes.GeoAxesSubplot):\n        msg = 'Must provide a cartopy axes object, not: {}'\n        raise ValueError(msg.format(type(ax)))\n\n    gdf = gdf or read_shapefile(gpd.datasets.get_path('naturalearth_lowres'),\n                                region_col='iso_a3')\n    data = gdf.merge(df, on='region', how='inner').to_crs(crs.proj4_init)\n    if data.empty:  # help users with iso codes\n        df['region'] = df['region'].str.upper()\n        data = gdf.merge(df, on='region', how='inner').to_crs(crs.proj4_init)\n    if data.empty:\n        raise ValueError('No data to plot')\n\n    if add_features:\n        ax.add_feature(cartopy.feature.LAND)\n        ax.add_feature(cartopy.feature.OCEAN)\n        ax.add_feature(cartopy.feature.COASTLINE)\n        ax.add_feature(cartopy.feature.BORDERS)\n\n    vmin = vmin if vmin is not None else data['value'].min()\n    vmax = vmax if vmax is not None else data['value'].max()\n    norm = colors.Normalize(vmin=vmin, vmax=vmax)\n    cmap = plt.get_cmap(cmap)\n    scalar_map = cmx.ScalarMappable(norm=norm, cmap=cmap)\n    labels = []\n    handles = []\n    for _, row in data.iterrows():\n        label = row['label'] if 'label' in row else row['region']\n        color = scalar_map.to_rgba(row['value'])\n        ax.add_geometries(\n            [row['geometry']],\n            crs,\n            facecolor=color,\n            label=label,\n        )\n        if label not in labels:\n            labels.append(label)\n            handle = mpatches.Rectangle((0, 0), 5, 5, facecolor=color)\n            handles.append(handle)\n\n    if cbar:\n        scalar_map._A = []  # for some reason you have to clear this\n        if cbar is True:  # use some defaults\n            cbar = dict(\n                fraction=0.022,  # these are magic numbers\n                pad=0.02,       # that just seem to \"work\"\n            )\n        plt.colorbar(scalar_map, ax=ax, **cbar)\n\n    if legend is not False:\n        if legend is True:  # use some defaults\n            legend = dict(\n                bbox_to_anchor=(1.32, 0.5) if cbar else (1.2, 0.5),\n                loc='right',\n            )\n        _add_legend(ax, handles, labels, legend)\n\n    if title:\n        var = df['variable'].unique()[0]\n        unit = df['unit'].unique()[0]\n        year = df['year'].unique()[0]\n        default_title = '{} ({}) in {}'.format(var, unit, year)\n        title = default_title if title is True else title\n        ax.set_title(title)\n\n    return ax", "response": "Plots data on a region of the specified region."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots data as a bar chart.", "response": "def pie_plot(df, value='value', category='variable',\n             ax=None, legend=False, title=True, cmap=None,\n             **kwargs):\n    \"\"\"Plot data as a bar chart.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Data to plot as a long-form data frame\n    value : string, optional\n        The column to use for data values\n        default: value\n    category : string, optional\n        The column to use for labels\n        default: variable\n    ax : matplotlib.Axes, optional\n    legend : bool, optional\n        Include a legend\n        default: False\n    title : bool or string, optional\n        Display a default or custom title.\n    cmap : string, optional\n        A colormap to use.\n        default: None\n    kwargs : Additional arguments to pass to the pd.DataFrame.plot() function\n    \"\"\"\n    for col in set(SORT_IDX) - set([category]):\n        if len(df[col].unique()) > 1:\n            msg = 'Can not plot multiple {}s in pie_plot with value={},' +\\\n                ' category={}'\n            raise ValueError(msg.format(col, value, category))\n\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # get data, set negative values to explode\n    _df = df.groupby(category)[value].sum()\n    where = _df > 0\n    explode = tuple(0 if _ else 0.2 for _ in where)\n    _df = _df.abs()\n\n    # explicitly get colors\n    defaults = default_props(reset=True, num_colors=len(_df.index),\n                             colormap=cmap)['color']\n    rc = run_control()\n    color = []\n    for key, c in zip(_df.index, defaults):\n        if 'color' in rc and \\\n           category in rc['color'] and \\\n           key in rc['color'][category]:\n            c = rc['color'][category][key]\n        color.append(c)\n\n    # plot data\n    _df.plot(kind='pie', colors=color, ax=ax, explode=explode, **kwargs)\n\n    # add legend\n    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), labels=_df.index)\n    if not legend:\n        ax.legend_.remove()\n\n    # remove label\n    ax.set_ylabel('')\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a stack chart.", "response": "def stack_plot(df, x='year', y='value', stack='variable',\n               ax=None, legend=True, title=True, cmap=None, total=None,\n               **kwargs):\n    \"\"\"Plot data as a stack chart.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Data to plot as a long-form data frame\n    x : string, optional\n        The column to use for x-axis values\n        default: year\n    y : string, optional\n        The column to use for y-axis values\n        default: value\n    stack: string, optional\n        The column to use for stack groupings\n        default: variable\n    ax : matplotlib.Axes, optional\n    legend : bool, optional\n        Include a legend\n        default: False\n    title : bool or string, optional\n        Display a default or custom title.\n    cmap : string, optional\n        A colormap to use.\n        default: None\n    total : bool or dict, optional\n        If True, plot a total line with default pyam settings. If a dict, then\n        plot the total line using the dict key-value pairs as keyword arguments\n        to ax.plot(). If None, do not plot the total line.\n        default : None\n    kwargs : Additional arguments to pass to the pd.DataFrame.plot() function\n    \"\"\"\n    for col in set(SORT_IDX) - set([x, stack]):\n        if len(df[col].unique()) > 1:\n            msg = 'Can not plot multiple {}s in stack_plot with x={}, stack={}'\n            raise ValueError(msg.format(col, x, stack))\n\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # long form to one column per bar group\n    _df = reshape_bar_plot(df, x, y, stack)\n\n    # Line below is for interpolation. On datetimes I think you'd downcast to\n    # seconds first and then cast back to datetime at the end..?\n    _df.index = _df.index.astype(float)\n\n    time_original = _df.index.values\n    first_zero_times = pd.DataFrame(index=[\"first_zero_time\"])\n\n    both_positive_and_negative = _df.apply(\n        lambda x: (x >= 0).any() and (x < 0).any()\n    )\n    for col in _df.loc[:, both_positive_and_negative]:\n        values = _df[col].dropna().values\n        positive = (values >= 0)\n        negative = (values < 0)\n        pos_to_neg = positive[:-1] & negative[1:]\n        neg_to_pos = positive[1:] & negative[:-1]\n        crosses = np.argwhere(pos_to_neg | neg_to_pos)\n        for i, cross in enumerate(crosses):\n            cross = cross[0]  # get location\n            x_1 = time_original[cross]\n            x_2 = time_original[cross + 1]\n            y_1 = values[cross]\n            y_2 = values[cross + 1]\n\n            zero_time = x_1 - y_1 * (x_2 - x_1) / (y_2 - y_1)\n            if i == 0:\n                first_zero_times.loc[:, col] = zero_time\n            if zero_time not in _df.index.values:\n                _df.loc[zero_time, :] = np.nan\n\n    first_zero_times = first_zero_times.sort_values(\n        by=\"first_zero_time\",\n        axis=1,\n    )\n    _df = _df.reindex(sorted(_df.index)).interpolate(method=\"values\")\n\n    # Sort lines so that negative timeseries are on the right, positive\n    # timeseries are on the left and timeseries which go from positive to\n    # negative are ordered such that the timeseries which goes negative first\n    # is on the right (case of timeseries which go from negative to positive\n    # is an edge case we haven't thought about as it's unlikely to apply to\n    # us).\n    pos_cols = [c for c in _df if (_df[c] > 0).all()]\n    cross_cols = first_zero_times.columns[::-1].tolist()\n    neg_cols = [c for c in _df if (_df[c] < 0).all()]\n    col_order = pos_cols + cross_cols + neg_cols\n    _df = _df[col_order]\n\n    # explicitly get colors\n    defaults = default_props(reset=True, num_colors=len(_df.columns),\n                             colormap=cmap)['color']\n    rc = run_control()\n    colors = {}\n    for key in _df.columns:\n        c = next(defaults)\n        c_in_rc = 'color' in rc\n        if c_in_rc and stack in rc['color'] and key in rc['color'][stack]:\n            c = rc['color'][stack][key]\n        colors[key] = c\n\n    # plot stacks, starting from the top and working our way down to the bottom\n    negative_only_cumulative = _df.applymap(\n        lambda x: x if x < 0 else 0\n    ).cumsum(axis=1)\n    positive_only_cumulative = _df.applymap(lambda x: x if x >= 0 else 0)[\n        col_order[::-1]\n    ].cumsum(axis=1)[\n        col_order\n    ]\n    time = _df.index.values\n    upper = positive_only_cumulative.iloc[:, 0].values\n    for j, col in enumerate(_df):\n        noc_tr = negative_only_cumulative.iloc[:, j].values\n        try:\n            poc_nr = positive_only_cumulative.iloc[:, j + 1].values\n        except IndexError:\n            poc_nr = np.zeros_like(upper)\n        lower = poc_nr.copy()\n        if (noc_tr < 0).any():\n            lower[np.where(poc_nr == 0)] = noc_tr[np.where(poc_nr == 0)]\n\n        ax.fill_between(time, lower, upper, label=col,\n                        color=colors[col], **kwargs)\n        upper = lower.copy()\n\n    # add total\n    if (total is not None) and total:  # cover case where total=False\n        if isinstance(total, bool):  # can now assume total=True\n            total = {}\n        total.setdefault(\"label\", \"Total\")\n        total.setdefault(\"color\", \"black\")\n        total.setdefault(\"lw\", 4.0)\n        ax.plot(time, _df.sum(axis=1), **total)\n\n    # add legend\n    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n    if not legend:\n        ax.legend_.remove()\n\n    # add default labels if possible\n    ax.set_xlabel(x.capitalize())\n    units = df['unit'].unique()\n    if len(units) == 1:\n        ax.set_ylabel(units[0])\n\n    # build a default title if possible\n    _title = []\n    for var in ['model', 'scenario', 'region', 'variable']:\n        values = df[var].unique()\n        if len(values) == 1:\n            _title.append('{}: {}'.format(var, values[0]))\n    if title and _title:\n        title = ' '.join(_title) if title is True else title\n        ax.set_title(title)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot data as a bar chart.", "response": "def bar_plot(df, x='year', y='value', bars='variable',\n             ax=None, orient='v', legend=True, title=True, cmap=None,\n             **kwargs):\n    \"\"\"Plot data as a bar chart.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Data to plot as a long-form data frame\n    x : string, optional\n        The column to use for x-axis values\n        default: year\n    y : string, optional\n        The column to use for y-axis values\n        default: value\n    bars: string, optional\n        The column to use for bar groupings\n        default: variable\n    ax : matplotlib.Axes, optional\n    orient : string, optional\n        Vertical or horizontal orientation.\n        default: variable\n    legend : bool, optional\n        Include a legend\n        default: False\n    title : bool or string, optional\n        Display a default or custom title.\n    cmap : string, optional\n        A colormap to use.\n        default: None\n    kwargs : Additional arguments to pass to the pd.DataFrame.plot() function\n    \"\"\"\n    for col in set(SORT_IDX) - set([x, bars]):\n        if len(df[col].unique()) > 1:\n            msg = 'Can not plot multiple {}s in bar_plot with x={}, bars={}'\n            raise ValueError(msg.format(col, x, bars))\n\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # long form to one column per bar group\n    _df = reshape_bar_plot(df, x, y, bars)\n\n    # explicitly get colors\n    defaults = default_props(reset=True, num_colors=len(_df.columns),\n                             colormap=cmap)['color']\n    rc = run_control()\n    color = []\n    for key in _df.columns:\n        c = next(defaults)\n        if 'color' in rc and bars in rc['color'] and key in rc['color'][bars]:\n            c = rc['color'][bars][key]\n        color.append(c)\n\n    # plot data\n    kind = 'bar' if orient.startswith('v') else 'barh'\n    _df.plot(kind=kind, color=color, ax=ax, **kwargs)\n\n    # add legend\n    ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n    if not legend:\n        ax.legend_.remove()\n\n    # add default labels if possible\n    if orient == 'v':\n        ax.set_xlabel(x.capitalize())\n    else:\n        ax.set_ylabel(x.capitalize())\n    units = df['unit'].unique()\n    if len(units) == 1 and y == 'value':\n        if orient == 'v':\n            ax.set_ylabel(units[0])\n        else:\n            ax.set_xlabel(units[0])\n\n    # build a default title if possible\n    _title = []\n    for var in ['model', 'scenario', 'region', 'variable']:\n        values = df[var].unique()\n        if len(values) == 1:\n            _title.append('{}: {}'.format(var, values[0]))\n    if title and _title:\n        title = ' '.join(_title) if title is True else title\n        ax.set_title(title)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_net_values_to_bar_plot(axs, color='k'):\n    axs = axs if isinstance(axs, Iterable) else [axs]\n    for ax in axs:\n        box_args = _get_boxes(ax)\n        for x, args in box_args.items():\n            rect = mpatches.Rectangle(*args, color=color)\n            ax.add_patch(rect)", "response": "Adds net values next to an existing vertical stacked bar chart."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scatter(df, x, y, ax=None, legend=None, title=None,\n            color=None, marker='o', linestyle=None, cmap=None,\n            groupby=['model', 'scenario'], with_lines=False, **kwargs):\n    \"\"\"Plot data as a scatter chart.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Data to plot as a long-form data frame\n    x : str\n        column to be plotted on the x-axis\n    y : str\n        column to be plotted on the y-axis\n    ax : matplotlib.Axes, optional\n    legend : bool, optional\n        Include a legend (`None` displays legend only if less than 13 entries)\n        default: None\n    title : bool or string, optional\n        Display a custom title.\n    color : string, optional\n        A valid matplotlib color or column name. If a column name, common\n        values will be provided the same color.\n        default: None\n    marker : string\n        A valid matplotlib marker or column name. If a column name, common\n        values will be provided the same marker.\n        default: 'o'\n    linestyle : string, optional\n        A valid matplotlib linestyle or column name. If a column name, common\n        values will be provided the same linestyle.\n        default: None\n    cmap : string, optional\n        A colormap to use.\n        default: None\n    groupby : list-like, optional\n        Data grouping for plotting.\n        default: ['model', 'scenario']\n    with_lines : bool, optional\n        Make the scatter plot with lines connecting common data.\n        default: False\n    kwargs : Additional arguments to pass to the pd.DataFrame.plot() function\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # assign styling properties\n    props = assign_style_props(df, color=color, marker=marker,\n                               linestyle=linestyle, cmap=cmap)\n\n    # group data\n    groups = df.groupby(groupby)\n\n    # loop over grouped dataframe, plot data\n    legend_data = []\n    for name, group in groups:\n        pargs = {}\n        labels = []\n        for key, kind, var in [('c', 'color', color),\n                               ('marker', 'marker', marker),\n                               ('linestyle', 'linestyle', linestyle)]:\n            if kind in props:\n                label = group[var].values[0]\n                pargs[key] = props[kind][group[var].values[0]]\n                labels.append(repr(label).lstrip(\"u'\").strip(\"'\"))\n            else:\n                pargs[key] = var\n\n        if len(labels) > 0:\n            legend_data.append(' '.join(labels))\n        else:\n            legend_data.append(' '.join(name))\n        kwargs.update(pargs)\n        if with_lines:\n            ax.plot(group[x], group[y], **kwargs)\n        else:\n            kwargs.pop('linestyle')  # scatter() can't take a linestyle\n            ax.scatter(group[x], group[y], **kwargs)\n\n    # build legend handles and labels\n    handles, labels = ax.get_legend_handles_labels()\n    if legend_data != [''] * len(legend_data):\n        labels = sorted(list(set(tuple(legend_data))))\n        idxs = [legend_data.index(d) for d in labels]\n        handles = [handles[i] for i in idxs]\n    if legend is None and len(labels) < 13 or legend is not False:\n        _add_legend(ax, handles, labels, legend)\n\n    # add labels and title\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n    if title:\n        ax.set_title(title)\n\n    return ax", "response": "Plots data as a scatter chart."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef line_plot(df, x='year', y='value', ax=None, legend=None, title=True,\n              color=None, marker=None, linestyle=None, cmap=None,\n              fill_between=None, final_ranges=None,\n              rm_legend_label=[], **kwargs):\n    \"\"\"Plot data as lines with or without markers.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Data to plot as a long-form data frame\n    x : string, optional\n        The column to use for x-axis values\n        default: year\n    y : string, optional\n        The column to use for y-axis values\n        default: value\n    ax : matplotlib.Axes, optional\n    legend : bool or dictionary, optional\n        Add a legend. If a dictionary is provided, it will be used as keyword\n        arguments in creating the legend.\n        default: None (displays legend only if less than 13 entries)\n    title : bool or string, optional\n        Display a default or custom title.\n    color : string, optional\n        A valid matplotlib color or column name. If a column name, common\n        values will be provided the same color.\n        default: None\n    marker : string, optional\n        A valid matplotlib marker or column name. If a column name, common\n        values will be provided the same marker.\n        default: None\n    linestyle : string, optional\n        A valid matplotlib linestyle or column name. If a column name, common\n        values will be provided the same linestyle.\n        default: None\n    cmap : string, optional\n        A colormap to use.\n        default: None\n    fill_between : boolean or dict, optional\n        Fill lines between minima/maxima of the 'color' argument. This can only\n        be used if also providing a 'color' argument. If this is True, then\n        default arguments will be provided to `ax.fill_between()`. If this is a\n        dictionary, those arguments will be provided instead of defaults.\n        default: None\n    final_ranges : boolean or dict, optional\n        Add vertical line between minima/maxima of the 'color' argument in the\n        last period plotted.  This can only be used if also providing a 'color'\n        argument. If this is True, then default arguments will be provided to\n        `ax.axvline()`. If this is a dictionary, those arguments will be\n        provided instead of defaults.\n        default: None\n    rm_legend_label : string, list, optional\n        Remove the color, marker, or linestyle label in the legend.\n        default: []\n    kwargs : Additional arguments to pass to the pd.DataFrame.plot() function\n    \"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # assign styling properties\n    props = assign_style_props(df, color=color, marker=marker,\n                               linestyle=linestyle, cmap=cmap)\n\n    if fill_between and 'color' not in props:\n        raise ValueError('Must use `color` kwarg if using `fill_between`')\n    if final_ranges and 'color' not in props:\n        raise ValueError('Must use `color` kwarg if using `final_ranges`')\n\n    # reshape data for use in line_plot\n    df = reshape_line_plot(df, x, y)  # long form to one column per line\n\n    # determine index of column name in reshaped dataframe\n    prop_idx = {}\n    for kind, var in [('color', color), ('marker', marker),\n                      ('linestyle', linestyle)]:\n        if var is not None and var in df.columns.names:\n            prop_idx[kind] = df.columns.names.index(var)\n\n    # plot data, keeping track of which legend labels to apply\n    no_label = [rm_legend_label] if isstr(rm_legend_label) else rm_legend_label\n\n    for col, data in df.iteritems():\n        pargs = {}\n        labels = []\n        # build plotting args and line legend labels\n        for key, kind, var in [('c', 'color', color),\n                               ('marker', 'marker', marker),\n                               ('linestyle', 'linestyle', linestyle)]:\n            if kind in props:\n                label = col[prop_idx[kind]]\n                pargs[key] = props[kind][label]\n                if kind not in no_label:\n                    labels.append(repr(label).lstrip(\"u'\").strip(\"'\"))\n            else:\n                pargs[key] = var\n        kwargs.update(pargs)\n        data = data.dropna()\n        data.plot(ax=ax, **kwargs)\n        if labels:\n            ax.lines[-1].set_label(' '.join(labels))\n\n    if fill_between:\n        _kwargs = {'alpha': 0.25} if fill_between in [True, None] \\\n            else fill_between\n        data = df.T\n        columns = data.columns\n        # get outer boundary mins and maxes\n        allmins = data.groupby(color).min()\n        intermins = (\n            data.dropna(axis=1).groupby(color).min()  # nonan data\n            .reindex(columns=columns)  # refill with nans\n            .T.interpolate(method='index').T  # interpolate\n        )\n        mins = pd.concat([allmins, intermins]).min(level=0)\n        allmaxs = data.groupby(color).max()\n        intermaxs = (\n            data.dropna(axis=1).groupby(color).max()  # nonan data\n            .reindex(columns=columns)  # refill with nans\n            .T.interpolate(method='index').T  # interpolate\n        )\n        maxs = pd.concat([allmaxs, intermaxs]).max(level=0)\n        # do the fill\n        for idx in mins.index:\n            ymin = mins.loc[idx]\n            ymax = maxs.loc[idx]\n            ax.fill_between(ymin.index, ymin, ymax,\n                            facecolor=props['color'][idx], **_kwargs)\n\n    # add bars to the end of the plot showing range\n    if final_ranges:\n        # have to explicitly draw it to get the tick labels (these change once\n        # you add the vlines)\n        plt.gcf().canvas.draw()\n        _kwargs = {'linewidth': 2} if final_ranges in [True, None] \\\n            else final_ranges\n        first = df.index[0]\n        final = df.index[-1]\n        mins = df.T.groupby(color).min()[final]\n        maxs = df.T.groupby(color).max()[final]\n        ymin, ymax = ax.get_ylim()\n        ydiff = ymax - ymin\n        xmin, xmax = ax.get_xlim()\n        xdiff = xmax - xmin\n        xticks = ax.get_xticks()\n        xlabels = ax.get_xticklabels()\n        # 1.5% increase seems to be ok per extra line\n        extra_space = 0.015\n        for i, idx in enumerate(mins.index):\n            xpos = final + xdiff * extra_space * (i + 1)\n            _ymin = (mins[idx] - ymin) / ydiff\n            _ymax = (maxs[idx] - ymin) / ydiff\n            ax.axvline(xpos, ymin=_ymin, ymax=_ymax,\n                       color=props['color'][idx], **_kwargs)\n        # for equal spacing between xmin and first datapoint and xmax and last\n        # line\n        ax.set_xlim(xmin, xpos + first - xmin)\n        ax.set_xticks(xticks)\n        ax.set_xticklabels(xlabels)\n\n    # build unique legend handles and labels\n    handles, labels = ax.get_legend_handles_labels()\n    handles, labels = np.array(handles), np.array(labels)\n    _, idx = np.unique(labels, return_index=True)\n    handles, labels = handles[idx], labels[idx]\n    if legend is not False:\n        _add_legend(ax, handles, labels, legend)\n\n    # add default labels if possible\n    ax.set_xlabel(x.title())\n    units = df.columns.get_level_values('unit').unique()\n    units_for_ylabel = len(units) == 1 and x == 'year' and y == 'value'\n    ylabel = units[0] if units_for_ylabel else y.title()\n    ax.set_ylabel(ylabel)\n\n    # build a default title if possible\n    if title:\n        default_title = []\n        for var in ['model', 'scenario', 'region', 'variable']:\n            if var in df.columns.names:\n                values = df.columns.get_level_values(var).unique()\n                if len(values) == 1:\n                    default_title.append('{}: {}'.format(var, values[0]))\n        title = ' '.join(default_title) if title is True else title\n        ax.set_title(title)\n\n    return ax, handles, labels", "response": "Plot data as lines with or without markers."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a panel label to the figure or axes by default in the top - left corner of the figure.", "response": "def set_panel_label(label, ax=None, x=0.05, y=0.9):\n    \"\"\"Add a panel label to the figure/axes, by default in the top-left corner\n\n    Parameters\n    ----------\n    label : str\n        text to be added as panel label\n    ax : matplotlib.Axes, optional\n        panel to which to add the panel label\n    x : number, default 0.05\n        relative location of label to x-axis\n    y : number, default 0.9\n        relative location of label to y-axis\n    \"\"\"\n    def _lim_loc(lim, loc):\n        return lim[0] + (lim[1] - lim[0]) * loc\n\n    if ax is not None:\n        ax.text(_lim_loc(ax.get_xlim(), x), _lim_loc(ax.get_ylim(), y), label)\n    else:\n        plt.text(_lim_loc(plt.xlim(), x), _lim_loc(plt.ylim(), y), label)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nodes(self):\n        if not hasattr(self, '_nodes'):\n            base_url = \"{}/{}\".format(NodeBalancerConfig.api_endpoint, NodeBalancerNode.derived_url_path)\n            result = self._client._get_objects(base_url, NodeBalancerNode, model=self, parent_id=(self.id, self.nodebalancer_id))\n\n            self._set('_nodes', result)\n\n        return self._nodes", "response": "Get all nodes of this nodebalancer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_ssl_data(self, cert_file, key_file):\n        # we're disabling warnings here because these attributes are defined dynamically\n        # through linode.objects.Base, and pylint isn't privy\n        if os.path.isfile(os.path.expanduser(cert_file)):\n            with open(os.path.expanduser(cert_file)) as f:\n                self.ssl_cert = f.read() # pylint: disable=attribute-defined-outside-init\n\n        if os.path.isfile(os.path.expanduser(key_file)):\n            with open(os.path.expanduser(key_file)) as f:\n                self.ssl_key = f.read()", "response": "This method loads a cert and key from files and sets them on self. ssl_cert and self. ssl_key attributes on self. ssl_cert and self. ssl_key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_and_validate_keys(authorized_keys):\n    if not authorized_keys:\n        return None\n\n    if not isinstance(authorized_keys, list):\n        authorized_keys = [authorized_keys]\n\n    ret = []\n\n    for k in authorized_keys:\n        accepted_types = ('ssh-dss', 'ssh-rsa', 'ecdsa-sha2-nistp', 'ssh-ed25519')\n        if any([ t for t in accepted_types if k.startswith(t) ]):\n            # this looks like a key, cool\n            ret.append(k)\n        else:\n            # it doesn't appear to be a key.. is it a path to the key?\n            k = os.path.expanduser(k)\n            if os.path.isfile(k):\n                with open(k) as f:\n                    ret.append(f.read().rstrip())\n            else:\n                raise ValueError(\"authorized_keys must either be paths \"\n                                 \"to the key files or a list of raw \"\n                                 \"public key of one of these types: {}\".format(accepted_types))\n    return ret", "response": "Load the authorized_keys from the file system and validate them."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attach(self, to_linode, config=None):\n        result = self._client.post('{}/attach'.format(Volume.api_endpoint), model=self,\n                data={\n                    \"linode_id\": to_linode.id if issubclass(type(to_linode), Base) else to_linode,\n                    \"config\": None if not config else config.id if issubclass(type(config), Base) else config,\n        })\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when attaching volume!', json=result)\n\n        self._populate(result)\n        return True", "response": "Attaches this Volume to Linode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detach(self):\n        self._client.post('{}/detach'.format(Volume.api_endpoint), model=self)\n\n        return True", "response": "Detaches this Volume if it is attached to any other Volume."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clone(self, label):\n        result = self._client.post('{}/clone'.format(Volume.api_endpoint),\n                model=self, data={'label': label})\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response cloning volume!')\n\n        return Volume(self._client, result['id'], result)", "response": "Clones this volume to a new volume in the same region with the given label."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_raw_objects(self):\n        if not hasattr(self, '_raw_objects'):\n            result = self._client.get(type(self).api_endpoint, model=self)\n\n            # I want to cache this to avoid making duplicate requests, but I don't\n            # want it in the __init__\n            self._raw_objects = result # pylint: disable=attribute-defined-outside-init\n\n        return self._raw_objects", "response": "Helper function to populate the first page of raw objects for this tag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef objects(self):\n        data = self._get_raw_objects()\n\n        return PaginatedList.make_paginated_list(data, self._client, TaggedObjectProxy,\n                                                 page_url=type(self).api_endpoint.format(**vars(self)))", "response": "Returns a list of objects with this Tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_instance(cls, id, client, parent_id=None, json=None):\n        make_cls = CLASS_MAP.get(id) # in this case, ID is coming in as the type\n\n        if make_cls is None:\n            # we don't recognize this entity type - do nothing?\n            return None\n\n        # discard the envelope\n        real_json = json['data']\n        real_id = real_json['id']\n\n        # make the real object type\n        return Base.make(real_id, client, make_cls, parent_id=None, json=real_json)", "response": "Overrides Base s make_instance to allow dynamic creation of objects based on the defined type in the response json."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resize(self, new_size):\n        self._client.post('{}/resize'.format(Disk.api_endpoint), model=self, data={\"size\": new_size})\n\n        return True", "response": "Resizes this disk.  The Linode Instance this disk belongs to must have\n        sufficient space available to accommodate the new size, and must be\n        offline.\n\n        **NOTE** If resizing a disk down, the filesystem on the disk must still\n        fit on the new disk size.  You may need to resize the filesystem on the\n        disk first before performing this action.\n\n        :param new_size: The intended new size of the disk, in MB\n        :type new_size: int\n\n        :returns: True if the resize was initiated successfully.\n        :rtype: bool"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate the object with the contents of the JSON object.", "response": "def _populate(self, json):\n        \"\"\"\n        Map devices more nicely while populating.\n        \"\"\"\n        from .volume import Volume\n\n        DerivedBase._populate(self, json)\n\n        devices = {}\n        for device_index, device in json['devices'].items():\n            if not device:\n                devices[device_index] = None\n                continue\n\n            dev = None\n            if 'disk_id' in device and device['disk_id']: # this is a disk\n                dev = Disk.make_instance(device['disk_id'], self._client,\n                        parent_id=self.linode_id)\n            else:\n                dev = Volume.make_instance(device['volume_id'], self._client,\n                        parent_id=self.linode_id)\n            devices[device_index] = dev\n\n        self._set('devices', MappedObject(**devices))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of IP related objects for this instance.", "response": "def ips(self):\n        \"\"\"\n        The ips related collection is not normalized like the others, so we have to\n        make an ad-hoc object to return for its response\n        \"\"\"\n        if not hasattr(self, '_ips'):\n            result = self._client.get(\"{}/ips\".format(Instance.api_endpoint), model=self)\n\n            if not \"ipv4\" in result:\n                raise UnexpectedResponseError('Unexpected response loading IPs', json=result)\n\n            v4pub = []\n            for c in result['ipv4']['public']:\n                i = IPAddress(self._client, c['address'], c)\n                v4pub.append(i)\n\n            v4pri = []\n            for c in result['ipv4']['private']:\n                i = IPAddress(self._client, c['address'], c)\n                v4pri.append(i)\n\n            shared_ips = []\n            for c in result['ipv4']['shared']:\n                i = IPAddress(self._client, c['address'], c)\n                shared_ips.append(i)\n\n            slaac = IPAddress(self._client, result['ipv6']['slaac']['address'],\n                              result['ipv6']['slaac'])\n            link_local = IPAddress(self._client, result['ipv6']['link_local']['address'],\n                                   result['ipv6']['link_local'])\n\n            pools = []\n            for p in result['ipv6']['global']:\n                pools.append(IPv6Pool(self._client, p['range']))\n\n            ips = MappedObject(**{\n                \"ipv4\": {\n                    \"public\": v4pub,\n                    \"private\": v4pri,\n                    \"shared\": shared_ips,\n                },\n                \"ipv6\": {\n                    \"slaac\": slaac,\n                    \"link_local\": link_local,\n                    \"pools\": pools,\n                },\n            })\n\n            self._set('_ips', ips)\n\n        return self._ips"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef available_backups(self):\n        if not hasattr(self, '_avail_backups'):\n            result = self._client.get(\"{}/backups\".format(Instance.api_endpoint), model=self)\n\n            if not 'automatic' in result:\n                raise UnexpectedResponseError('Unexpected response loading available backups!', json=result)\n\n            automatic = []\n            for a in result['automatic']:\n                cur = Backup(self._client, a['id'], self.id, a)\n                automatic.append(cur)\n\n            snap = None\n            if result['snapshot']['current']:\n                snap = Backup(self._client, result['snapshot']['current']['id'], self.id,\n                        result['snapshot']['current'])\n\n            psnap = None\n            if result['snapshot']['in_progress']:\n                psnap = Backup(self._client, result['snapshot']['in_progress']['id'], self.id,\n                        result['snapshot']['in_progress'])\n\n            self._set('_avail_backups', MappedObject(**{\n                \"automatic\": automatic,\n                \"snapshot\": {\n                    \"current\": snap,\n                    \"in_progress\": psnap,\n                }\n            }))\n\n        return self._avail_backups", "response": "Returns a list of all available backups for this instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invalidate(self):\n        if hasattr(self, '_avail_backups'):\n            del self._avail_backups\n        if hasattr(self, '_ips'):\n            del self._ips\n\n        Base.invalidate(self)", "response": "Clear out the cached properties of this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef config_create(self, kernel=None, label=None, devices=[], disks=[],\n            volumes=[], **kwargs):\n        \"\"\"\n        Creates a Linode Config with the given attributes.\n\n        :param kernel: The kernel to boot with.\n        :param label: The config label\n        :param disks: The list of disks, starting at sda, to map to this config.\n        :param volumes: The volumes, starting after the last disk, to map to this\n            config\n        :param devices: A list of devices to assign to this config, in device\n            index order.  Values must be of type Disk or Volume. If this is\n            given, you may not include disks or volumes.\n        :param **kwargs: Any other arguments accepted by the api.\n\n        :returns: A new Linode Config\n        \"\"\"\n        from .volume import Volume\n\n        hypervisor_prefix = 'sd' if self.hypervisor == 'kvm' else 'xvd'\n        device_names = [hypervisor_prefix + string.ascii_lowercase[i] for i in range(0, 8)]\n        device_map = {device_names[i]: None for i in range(0, len(device_names))}\n\n        if devices and (disks or volumes):\n            raise ValueError('You may not call config_create with \"devices\" and '\n                    'either of \"disks\" or \"volumes\" specified!')\n\n        if not devices:\n            if not isinstance(disks, list):\n                disks = [disks]\n            if not isinstance(volumes, list):\n                volumes = [volumes]\n\n            devices = []\n\n            for d in disks:\n                if d is None:\n                    devices.append(None)\n                elif isinstance(d, Disk):\n                    devices.append(d)\n                else:\n                    devices.append(Disk(self._client, int(d), self.id))\n\n            for v in volumes:\n                if v is None:\n                    devices.append(None)\n                elif isinstance(v, Volume):\n                    devices.append(v)\n                else:\n                    devices.append(Volume(self._client, int(v)))\n\n        if not devices:\n            raise ValueError('Must include at least one disk or volume!')\n\n        for i, d in enumerate(devices):\n            if d is None:\n                pass\n            elif isinstance(d, Disk):\n                device_map[device_names[i]] = {'disk_id': d.id }\n            elif isinstance(d, Volume):\n                device_map[device_names[i]] = {'volume_id': d.id }\n            else:\n                raise TypeError('Disk or Volume expected!')\n\n        params = {\n            'kernel': kernel.id if issubclass(type(kernel), Base) else kernel,\n            'label': label if label else \"{}_config_{}\".format(self.label, len(self.configs)),\n            'devices': device_map,\n        }\n        params.update(kwargs)\n\n        result = self._client.post(\"{}/configs\".format(Instance.api_endpoint), model=self, data=params)\n        self.invalidate()\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response creating config!', json=result)\n\n        c = Config(self._client, result['id'], self.id, result)\n        return c", "response": "Creates a new Linode Config with the given attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enable_backups(self):\n        self._client.post(\"{}/backups/enable\".format(Instance.api_endpoint), model=self)\n        self.invalidate()\n        return True", "response": "Enable Backups for this Instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ip_allocate(self, public=False):\n        result = self._client.post(\n            \"{}/ips\".format(Instance.api_endpoint),\n            model=self,\n            data={\n                \"type\": \"ipv4\",\n                \"public\": public,\n            })\n\n        if not 'address' in result:\n            raise UnexpectedResponseError('Unexpected response allocating IP!',\n                                          json=result)\n\n        i = IPAddress(self._client, result['address'], result)\n        return i", "response": "Allocates a new IP for this Instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rebuild(self, image, root_pass=None, authorized_keys=None, **kwargs):\n        ret_pass = None\n        if not root_pass:\n            ret_pass = Instance.generate_root_password()\n            root_pass = ret_pass\n\n        authorized_keys = load_and_validate_keys(authorized_keys)\n\n        params = {\n             'image': image.id if issubclass(type(image), Base) else image,\n             'root_pass': root_pass,\n             'authorized_keys': authorized_keys,\n         }\n        params.update(kwargs)\n\n        result = self._client.post('{}/rebuild'.format(Instance.api_endpoint), model=self, data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response issuing rebuild!', json=result)\n\n        # update ourself with the newly-returned information\n        self._populate(result)\n\n        if not ret_pass:\n            return True\n        else:\n            return ret_pass", "response": "Rebuilding an Instance deletes all existing Disks and Configs and deploys\n        a new :any:`Image` to it.  This can be used to reset an existing\n        Instance or to install an Image on an empty Instance.\n\n        :param image: The Image to deploy to this Instance\n        :type image: str or Image\n        :param root_pass: The root password for the newly rebuilt Instance.  If\n                          omitted, a password will be generated and returned.\n        :type root_pass: str\n        :param authorized_keys: The ssh public keys to install in the linode's\n                                /root/.ssh/authorized_keys file.  Each entry may\n                                be a single key, or a path to a file containing\n                                the key.\n        :type authorized_keys: list or str\n\n        :returns: The newly generated password, if one was not provided\n                  (otherwise True)\n        :rtype: str or bool"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmutate this Instance to the latest generation type", "response": "def mutate(self):\n        \"\"\"\n        Upgrades this Instance to the latest generation type\n        \"\"\"\n        self._client.post('{}/mutate'.format(Instance.api_endpoint), model=self)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitiating a pending migration for this LinodeInstance", "response": "def initiate_migration(self):\n        \"\"\"\n        Initiates a pending migration that is already scheduled for this Linode\n        Instance\n        \"\"\"\n        self._client.post('{}/migrate'.format(Instance.api_endpoint), model=self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclone this instance into a new linode in the given region or into a new linode in the given region.", "response": "def clone(self, to_linode=None, region=None, service=None, configs=[], disks=[],\n            label=None, group=None, with_backups=None):\n        \"\"\" Clones this linode into a new linode or into a new linode in the given region \"\"\"\n        if to_linode and region:\n            raise ValueError('You may only specify one of \"to_linode\" and \"region\"')\n\n        if region and not service:\n            raise ValueError('Specifying a region requires a \"service\" as well')\n\n        if not isinstance(configs, list) and not isinstance(configs, PaginatedList):\n            configs = [configs]\n        if not isinstance(disks, list) and not isinstance(disks, PaginatedList):\n            disks = [disks]\n\n        cids = [ c.id if issubclass(type(c), Base) else c for c in configs ]\n        dids = [ d.id if issubclass(type(d), Base) else d for d in disks ]\n\n        params = {\n            \"linode_id\": to_linode.id if issubclass(type(to_linode), Base) else to_linode,\n            \"region\": region.id if issubclass(type(region), Base) else region,\n            \"type\": service.id if issubclass(type(service), Base) else service,\n            \"configs\": cids if cids else None,\n            \"disks\": dids if dids else None,\n            \"label\": label,\n            \"group\": group,\n            \"with_backups\": with_backups,\n        }\n\n        result = self._client.post('{}/clone'.format(Instance.api_endpoint), model=self, data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response cloning Instance!', json=result)\n\n        l = Instance(self._client, result['id'], result)\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the JSON stats for this Instance", "response": "def stats(self):\n        \"\"\"\n        Returns the JSON stats for this Instance\n        \"\"\"\n        # TODO - this would be nicer if we formatted the stats\n        return self._client.get('{}/stats'.format(Instance.api_endpoint), model=self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the stats for the given datetime", "response": "def stats_for(self, dt):\n        \"\"\"\n        Returns stats for the month containing the given datetime\n        \"\"\"\n        # TODO - this would be nicer if we formatted the stats\n        if not isinstance(dt, datetime):\n            raise TypeError('stats_for requires a datetime object!')\n        return self._client.get('{}/stats/'.format(dt.strftime('%Y/%m')))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _populate(self, json):\n        Base._populate(self, json)\n\n        mapped_udfs = []\n        for udf in self.user_defined_fields:\n            t = UserDefinedFieldType.text\n            choices = None\n            if hasattr(udf, 'oneof'):\n                t = UserDefinedFieldType.select_one\n                choices = udf.oneof.split(',')\n            elif hasattr(udf, 'manyof'):\n                t = UserDefinedFieldType.select_many\n                choices = udf.manyof.split(',')\n\n            mapped_udfs.append(UserDefinedField(udf.name,\n                    udf.label if hasattr(udf, 'label') else None,\n                    udf.example if hasattr(udf, 'example') else None,\n                    t, choices=choices))\n\n        self._set('user_defined_fields', mapped_udfs)\n        ndist = [ Image(self._client, d) for d in self.images ]\n        self._set('images', ndist)", "response": "Override the populate method to map user_defined_fields to user_defined_fields to fancy values\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the object with the contents of the JSON object.", "response": "def _populate(self, json):\n        \"\"\"\n        Allows population of \"from_date\" from the returned \"from\" attribute which\n        is a reserved word in python.  Also populates \"to_date\" to be complete.\n        \"\"\"\n        super(InvoiceItem, self)._populate(json)\n\n        self.from_date = datetime.strptime(json['from'], DATE_FORMAT)\n        self.to_date = datetime.strptime(json['to'], DATE_FORMAT)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset_secret(self):\n        result = self._client.post(\"{}/reset_secret\".format(OAuthClient.api_endpoint), model=self)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when resetting secret!', json=result)\n\n        self._populate(result)\n        return self.secret", "response": "Resets the client secret for this client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef thumbnail(self, dump_to=None):\n        headers = {\n            \"Authorization\": \"token {}\".format(self._client.token)\n        }\n\n        result = requests.get('{}/{}/thumbnail'.format(self._client.base_url,\n                OAuthClient.api_endpoint.format(id=self.id)),\n                headers=headers)\n\n        if not result.status_code == 200:\n            raise ApiError('No thumbnail found for OAuthClient {}'.format(self.id))\n\n        if dump_to:\n            with open(dump_to, 'wb+') as f:\n                f.write(result.content)\n        return result.content", "response": "This returns binary data that represents a 128x128 image."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the thumbnail for this OAuth Client.", "response": "def set_thumbnail(self, thumbnail):\n        \"\"\"\n        Sets the thumbnail for this OAuth Client.  If thumbnail is bytes,\n        uploads it as a png.  Otherwise, assumes thumbnail is a path to the\n        thumbnail and reads it in as bytes before uploading.\n        \"\"\"\n        headers = {\n            \"Authorization\": \"token {}\".format(self._client.token),\n            \"Content-type\": \"image/png\",\n        }\n\n        # TODO this check needs to be smarter - python2 doesn't do it right\n        if not isinstance(thumbnail, bytes):\n            with open(thumbnail, 'rb') as f:\n                thumbnail = f.read()\n\n        result = requests.put('{}/{}/thumbnail'.format(self._client.base_url,\n                OAuthClient.api_endpoint.format(id=self.id)),\n                headers=headers, data=thumbnail)\n\n        if not result.status_code == 200:\n            errors = []\n            j = result.json()\n            if 'errors' in j:\n                errors = [ e['reason'] for e in j['errors'] ]\n            raise ApiError('{}: {}'.format(result.status_code, errors), json=j)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grants(self):\n        from linode_api4.objects.account import UserGrants\n        if not hasattr(self, '_grants'):\n            resp = self._client.get(UserGrants.api_endpoint.format(username=self.username))\n\n            grants = UserGrants(self._client, self.username, resp)\n            self._set('_grants', grants)\n\n        return self._grants", "response": "Retrieves the grants for this user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef entity(self):\n        # there are no grants for derived types, so this shouldn't happen\n        if not issubclass(self.cls, Base) or issubclass(self.cls, DerivedBase):\n            raise ValueError(\"Cannot get entity for non-base-class {}\".format(self.cls))\n        return self.cls(self._client, self.id)", "response": "Returns the object this grant is for."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_list(json_arr, client, cls, parent_id=None):\n        result = []\n\n        for obj in json_arr:\n            id_val = None\n\n            if 'id' in obj:\n                id_val = obj['id']\n            elif hasattr(cls, 'id_attribute') and getattr(cls, 'id_attribute') in obj:\n                id_val = obj[getattr(cls, 'id_attribute')]\n            else:\n                continue\n            o = cls.make_instance(id_val, client, parent_id=parent_id, json=obj)\n            result.append(o)\n\n        return result", "response": "Makes a list of Populated objects of the given class type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_paginated_list(json, client, cls, parent_id=None, page_url=None,\n            filters=None):\n        \"\"\"\n        Returns a PaginatedList populated with the first page of data provided,\n        and the ability to load additional pages.  This should not be called\n        outside of the :any:`LinodeClient` class.\n\n        :param json: The JSON list to use as the first page\n        :param client: A LinodeClient to use to load additional pages\n        :param parent_id: The parent ID for derived objects\n        :param page_url: The URL to use when loading more pages\n        :param cls: The class to instantiate for objects\n        :param filters: The filters used when making the call that generated\n                        this list.  If not provided, this will fail when\n                        loading additional pages.\n\n        :returns: An instance of PaginatedList that will represent the entire\n                  collection whose first page is json\n        \"\"\"\n        l = PaginatedList.make_list(json[\"data\"], client, cls, parent_id=parent_id)\n        p = PaginatedList(client, page_url, page=l, max_pages=json['pages'],\n                total_items=json['results'], parent_id=parent_id, filters=filters)\n        return p", "response": "Creates a PaginatedList populated with the first page of data provided and the ability to load additional pages."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends this object s mutable values to the server in a PUT request", "response": "def save(self):\n        \"\"\"\n        Send this object's mutable values to the server in a PUT request\n        \"\"\"\n        resp = self._client.put(type(self).api_endpoint, model=self,\n            data=self._serialize())\n\n        if 'error' in resp:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a DELETE request to the object s API endpoint. Returns True if successful False otherwise.", "response": "def delete(self):\n        \"\"\"\n        Sends a DELETE request for this object\n        \"\"\"\n        resp = self._client.delete(type(self).api_endpoint, model=self)\n\n        if 'error' in resp:\n            return False\n        self.invalidate()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef invalidate(self):\n        for key in [k for k in type(self).properties.keys()\n                if not type(self).properties[k].identifier]:\n            self._set(key, None)\n\n        self._set('_populated', False)", "response": "Invalidates all non - identifier Properties this object has locally and removes all non - identifier Properties from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _api_get(self):\n        json = self._client.get(type(self).api_endpoint, model=self)\n        self._populate(json)", "response": "A helper method to get this object from the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _populate(self, json):\n        if not json:\n            return\n\n        # hide the raw JSON away in case someone needs it\n        self._set('_raw_json', json)\n\n        for key in json:\n            if key in (k for k in type(self).properties.keys()\n                    if not type(self).properties[k].identifier):\n                if type(self).properties[key].relationship \\\n                    and not json[key] is None:\n                    if isinstance(json[key], list):\n                        objs = []\n                        for d in json[key]:\n                            if not 'id' in d:\n                                continue\n                            new_class = type(self).properties[key].relationship\n                            obj = new_class.make_instance(d['id'],\n                                    getattr(self,'_client'))\n                            if obj:\n                                obj._populate(d)\n                            objs.append(obj)\n                        self._set(key, objs)\n                    else:\n                        if isinstance(json[key], dict):\n                            related_id = json[key]['id']\n                        else:\n                            related_id = json[key]\n                        new_class = type(self).properties[key].relationship\n                        obj = new_class.make_instance(related_id, getattr(self,'_client'))\n                        if obj and isinstance(json[key], dict):\n                            obj._populate(json[key])\n                        self._set(key, obj)\n                elif  type(self).properties[key].slug_relationship \\\n                        and not json[key] is None:\n                    # create an object of the expected type with the given slug\n                    self._set(key, type(self).properties[key].slug_relationship(self._client, json[key]))\n                elif type(json[key]) is dict:\n                    self._set(key, MappedObject(**json[key]))\n                elif type(json[key]) is list:\n                    # we're going to use MappedObject's behavior with lists to\n                    # expand these, then grab the resulting value to set\n                    mapping = MappedObject(_list=json[key])\n                    self._set(key, mapping._list) # pylint: disable=no-member\n                elif type(self).properties[key].is_datetime:\n                    try:\n                        t = time.strptime(json[key], DATE_FORMAT)\n                        self._set(key, datetime.fromtimestamp(time.mktime(t)))\n                    except:\n                        #TODO - handle this better (or log it?)\n                        self._set(key, json[key])\n                else:\n                    self._set(key, json[key])\n\n        self._set('_populated', True)\n        self._set('_last_updated', datetime.now())", "response": "A helper method that assigns values based on the properties dict and the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make(id, client, cls, parent_id=None, json=None):\n        from .dbase import DerivedBase\n\n        if issubclass(cls, DerivedBase):\n            return cls(client, id, parent_id, json)\n        else:\n            return cls(client, id, json)", "response": "Creates an api object based on an id and class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake an instance of the class cls with the given id and client and json populated with json.", "response": "def make_instance(cls, id, client, parent_id=None, json=None):\n        \"\"\"\n        Makes an instance of the class this is called on and returns it.\n\n        The intended usage is:\n          instance = Linode.make_instance(123, client, json=response)\n\n        :param cls: The class this was called on.\n        :param id: The id of the instance to create\n        :param client: The client to use for this instance\n        :param parent_id: The parent id for derived classes\n        :param json: The JSON to populate the instance with\n\n        :returns: A new instance of this type, populated with json\n        \"\"\"\n        return Base.make(id, client, cls, parent_id=parent_id, json=json)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef or_(a, b):\n    if not isinstance(a, Filter) or not isinstance(b, Filter):\n        raise TypeError\n    return a.__or__(b)", "response": "Combines two filters<Filter > with an or operation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to(self, linode):\n        from .linode import Instance\n        if not isinstance(linode, Instance):\n            raise ValueError(\"IP Address can only be assigned to a Linode!\")\n        return { \"address\": self.address, \"linode_id\": linode.id }", "response": "Returns a dictionary that can be used to create an IP Assign request."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stackscripts(self, *filters, **kwargs):\n        # python2 can't handle *args and a single keyword argument, so this is a workaround\n        if 'mine_only' in kwargs:\n            if kwargs['mine_only']:\n                new_filter = Filter({\"mine\":True})\n                if filters:\n                    filters = [ f for f in filters ]\n                    filters[0] = filters[0] & new_filter\n                else:\n                    filters = [new_filter]\n\n            del kwargs['mine_only']\n\n        if kwargs:\n            raise TypeError(\"stackscripts() got unexpected keyword argument '{}'\".format(kwargs.popitem()[0]))\n\n        return self.client._get_and_filter(StackScript, *filters)", "response": "Returns a list of all StackScripts that match the given filters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Linode Instance.", "response": "def instance_create(self, ltype, region, image=None,\n            authorized_keys=None, **kwargs):\n        \"\"\"\n        Creates a new Linode Instance. This function has several modes of operation:\n\n        **Create an Instance from an Image**\n\n        To create an Instance from an :any:`Image`, call `instance_create` with\n        a :any:`Type`, a :any:`Region`, and an :any:`Image`.  All three of\n        these fields may be provided as either the ID or the appropriate object.\n        In this mode, a root password will be generated and returned with the\n        new Instance object.  For example::\n\n           new_linode, password = client.linode.instance_create(\n               \"g6-standard-2\",\n               \"us-east\",\n               image=\"linode/debian9\")\n\n           ltype = client.linode.types().first()\n           region = client.regions().first()\n           image = client.images().first()\n\n           another_linode, password = client.linode.instance_create(\n               ltype,\n               region,\n               image=image)\n\n        **Create an Instance from StackScript**\n\n        When creating an Instance from a :any:`StackScript`, an :any:`Image` that\n        the StackScript support must be provided..  You must also provide any\n        required StackScript data for the script's User Defined Fields..  For\n        example, if deploying `StackScript 10079`_ (which deploys a new Instance\n        with a user created from keys on `github`_::\n\n           stackscript = StackScript(client, 10079)\n\n           new_linode, password = client.linode.instance_create(\n              \"g6-standard-2\",\n              \"us-east\",\n              image=\"linode/debian9\",\n              stackscript=stackscript,\n              stackscript_data={\"gh_username\": \"example\"})\n\n        In the above example, \"gh_username\" is the name of a User Defined Field\n        in the chosen StackScript.  For more information on StackScripts, see\n        the `StackScript guide`_.\n\n        .. _`StackScript 10079`: https://www.linode.com/stackscripts/view/10079\n        .. _`github`: https://github.com\n        .. _`StackScript guide`: https://www.linode.com/docs/platform/stackscripts/\n\n        **Create an Instance from a Backup**\n\n        To create a new Instance by restoring a :any:`Backup` to it, provide a\n        :any:`Type`, a :any:`Region`, and the :any:`Backup` to restore.  You\n        may provide either IDs or objects for all of these fields::\n\n           existing_linode = Instance(client, 123)\n           snapshot = existing_linode.available_backups.snapshot.current\n\n           new_linode = client.linode.instance_create(\n               \"g6-standard-2\",\n               \"us-east\",\n               backup=snapshot)\n\n        **Create an empty Instance**\n\n        If you want to create an empty Instance that you will configure manually,\n        simply call `instance_create` with a :any:`Type` and a :any:`Region`::\n\n           empty_linode = client.linode.instance_create(\"g6-standard-2\", \"us-east\")\n\n        When created this way, the Instance will not be booted and cannot boot\n        successfully until disks and configs are created, or it is otherwise\n        configured.\n\n        :param ltype: The Instance Type we are creating\n        :type ltype: str or Type\n        :param region: The Region in which we are creating the Instance\n        :type region: str or Region\n        :param image: The Image to deploy to this Instance. If this is provided\n                      and no root_pass is given, a password will be generated\n                      and returned along with the new Instance.\n        :type image: str or Image\n        :param stackscript: The StackScript to deploy to the new Instance.  If\n                            provided, \"image\" is required and must be compatible\n                            with the chosen StackScript.\n        :type stackscript: int or StackScript\n        :param stackscript_data: Values for the User Defined Fields defined in\n                                 the chosen StackScript.  Does nothing if\n                                 StackScript is not provided.\n        :type stackscript_data: dict\n        :param backup: The Backup to restore to the new Instance.  May not be\n                       provided if \"image\" is given.\n        :type backup: int of Backup\n        :param authorized_keys: The ssh public keys to install in the linode's\n                                /root/.ssh/authorized_keys file.  Each entry may\n                                be a single key, or a path to a file containing\n                                the key.\n        :type authorized_keys: list or str\n        :param label: The display label for the new Instance\n        :type label: str\n        :param group: The display group for the new Instance\n        :type group: str\n        :param booted: Whether the new Instance should be booted.  This will\n                       default to True if the Instance is deployed from an Image\n                       or Backup.\n        :type booted: bool\n\n        :returns: A new Instance object, or a tuple containing the new Instance and\n                  the generated password.\n        :rtype: Instance or tuple(Instance, str)\n        :raises ApiError: If contacting the API fails\n        :raises UnexpectedResponseError: If the API resposne is somehow malformed.\n                                         This usually indicates that you are using\n                                         an outdated library.\n        \"\"\"\n        ret_pass = None\n        if image and not 'root_pass' in kwargs:\n            ret_pass = Instance.generate_root_password()\n            kwargs['root_pass'] = ret_pass\n\n        authorized_keys = load_and_validate_keys(authorized_keys)\n\n        if \"stackscript\" in kwargs:\n            # translate stackscripts\n            kwargs[\"stackscript_id\"] = (kwargs[\"stackscript\"].id if issubclass(type(kwargs[\"stackscript\"]), Base)\n                                        else kwargs[\"stackscript\"])\n            del kwargs[\"stackscript\"]\n\n        if \"backup\" in kwargs:\n            # translate backups\n            kwargs[\"backup_id\"] = (kwargs[\"backup\"].id if issubclass(type(kwargs[\"backup\"]), Base)\n                                   else kwargs[\"backup\"])\n            del kwargs[\"backup\"]\n\n        params = {\n             'type': ltype.id if issubclass(type(ltype), Base) else ltype,\n             'region': region.id if issubclass(type(region), Base) else region,\n             'image': (image.id if issubclass(type(image), Base) else image) if image else None,\n             'authorized_keys': authorized_keys,\n         }\n        params.update(kwargs)\n\n        result = self.client.post('/linode/instances', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating linode!', json=result)\n\n        l = Instance(self.client, result['id'], result)\n        if not ret_pass:\n            return l\n        return l, ret_pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new StackScript on your account.", "response": "def stackscript_create(self, label, script, images, desc=None, public=False, **kwargs):\n        \"\"\"\n        Creates a new :any:`StackScript` on your account.\n\n        :param label: The label for this StackScript.\n        :type label: str\n        :param script: The script to run when an :any:`Instance` is deployed with\n                       this StackScript.  Must begin with a shebang (#!).\n        :type script: str\n        :param images: A list of :any:`Images<Image>` that this StackScript\n                       supports.  Instances will not be deployed from this\n                       StackScript unless deployed from one of these Images.\n        :type images: list of Image\n        :param desc: A description for this StackScript.\n        :type desc: str\n        :param public: Whether this StackScript is public.  Defaults to False.\n                       Once a StackScript is made public, it may not be set\n                       back to private.\n        :type public: bool\n\n        :returns: The new StackScript\n        :rtype: StackScript\n        \"\"\"\n        image_list = None\n        if type(images) is list or type(images) is PaginatedList:\n            image_list = [d.id if issubclass(type(d), Base) else d for d in images ]\n        elif type(images) is Image:\n            image_list = [images.id]\n        elif type(images) is str:\n            image_list = [images]\n        else:\n            raise ValueError('images must be a list of Images or a single Image')\n\n        script_body = script\n        if not script.startswith(\"#!\"):\n            # it doesn't look like a stackscript body, let's see if it's a file\n            import os\n            if os.path.isfile(script):\n                with open(script) as f:\n                    script_body = f.read()\n            else:\n                raise ValueError(\"script must be the script text or a path to a file\")\n\n        params = {\n            \"label\": label,\n            \"images\": image_list,\n            \"is_public\": public,\n            \"script\": script_body,\n            \"description\": desc if desc else '',\n        }\n        params.update(kwargs)\n\n        result = self.client.post('/linode/stackscripts', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating StackScript!', json=result)\n\n        s = StackScript(self.client, result['id'], result)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating and returns a new Personal Access Token", "response": "def token_create(self, label=None, expiry=None, scopes=None, **kwargs):\n        \"\"\"\n        Creates and returns a new Personal Access Token\n        \"\"\"\n        if label:\n            kwargs['label'] = label\n        if expiry:\n            if isinstance(expiry, datetime):\n                expiry = datetime.strftime(expiry, \"%Y-%m-%dT%H:%M:%S\")\n            kwargs['expiry'] = expiry\n        if scopes:\n            kwargs['scopes'] = scopes\n\n        result = self.client.post('/profile/tokens', data=kwargs)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating Personal Access '\n                    'Token!', json=result)\n\n        token = PersonalAccessToken(self.client, result['id'], result)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload a new SSH Public Key to your profile.", "response": "def ssh_key_upload(self, key, label):\n        \"\"\"\n        Uploads a new SSH Public Key to your profile  This key can be used in\n        later Linode deployments.\n\n        :param key: The ssh key, or a path to the ssh key.  If a path is provided,\n                    the file at the path must exist and be readable or an exception\n                    will be thrown.\n        :type key: str\n        :param label: The name to give this key.  This is purely aesthetic.\n        :type label: str\n\n        :returns: The newly uploaded SSH Key\n        :rtype: SSHKey\n        :raises ValueError: If the key provided does not appear to be valid, and\n                            does not appear to be a path to a valid key.\n        \"\"\"\n        if not key.startswith(SSH_KEY_TYPES):\n            # this might be a file path - look for it\n            path = os.path.expanduser(key)\n            if os.path.isfile(path):\n                with open(path) as f:\n                    key = f.read().strip()\n            if not key.startswith(SSH_KEY_TYPES):\n                raise ValueError('Invalid SSH Public Key')\n\n        params = {\n            'ssh_key': key,\n            'label': label,\n        }\n\n        result = self.client.post('/profile/sshkeys', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when uploading SSH Key!',\n                                          json=result)\n\n        ssh_key = SSHKey(self.client, result['id'], result)\n        return ssh_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new LongviewClient optionally with a given label.", "response": "def client_create(self, label=None):\n        \"\"\"\n        Creates a new LongviewClient, optionally with a given label.\n\n        :param label: The label for the new client.  If None, a default label based\n            on the new client's ID will be used.\n\n        :returns: A new LongviewClient\n\n        :raises ApiError: If a non-200 status code is returned\n        :raises UnexpectedResponseError: If the returned data from the api does\n            not look as expected.\n        \"\"\"\n        result = self.client.post('/longview/clients', data={\n            \"label\": label\n        })\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating Longivew '\n                'Client!', json=result)\n\n        c = LongviewClient(self.client, result['id'], result)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmarking the event as the last event we have seen.", "response": "def events_mark_seen(self, event):\n        \"\"\"\n        Marks event as the last event we have seen.  If event is an int, it is treated\n        as an event_id, otherwise it should be an event object whose id will be used.\n        \"\"\"\n        last_seen = event if isinstance(event, int) else event.id\n        self.client.post('{}/seen'.format(Event.api_endpoint), model=Event(self.client, last_seen))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef settings(self):\n        result = self.client.get('/account/settings')\n\n        if not 'managed' in result:\n            raise UnexpectedResponseError('Unexpected response when getting account settings!',\n                    json=result)\n\n        s = AccountSettings(self.client, result['managed'], result)\n        return s", "response": "Returns the account settings for this acocunt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new OAuth Client and return it.", "response": "def oauth_client_create(self, name, redirect_uri, **kwargs):\n        \"\"\"\n        Make a new OAuth Client and return it\n        \"\"\"\n        params = {\n            \"label\": name,\n            \"redirect_uri\": redirect_uri,\n        }\n        params.update(kwargs)\n\n        result = self.client.post('/account/oauth-clients', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating OAuth Client!',\n                    json=result)\n\n        c = OAuthClient(self.client, result['id'], result)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a MappedObject containing the account s transfer pool data.", "response": "def transfer(self):\n        \"\"\"\n        Returns a MappedObject containing the account's transfer pool data\n        \"\"\"\n        result = self.client.get('/account/transfer')\n\n        if not 'used' in result:\n            raise UnexpectedResponseError('Unexpected response when getting Transfer Pool!')\n\n        return MappedObject(**result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef user_create(self, email, username, restricted=True):\n        params = {\n            \"email\": email,\n            \"username\": username,\n            \"restricted\": restricted,\n        }\n        result = self.client.post('/account/users', data=params)\n\n        if not 'email' and 'restricted' and 'username' in result:\n            raise UnexpectedResponseError('Unexpected response when creating user!', json=result)\n\n        u = User(self.client, result['username'], result)\n        return u", "response": "Creates a new user on your account."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ips_assign(self, region, *assignments):\n        for a in assignments:\n            if not 'address' in a or not 'linode_id' in a:\n                raise ValueError(\"Invalid assignment: {}\".format(a))\n        if isinstance(region, Region):\n            region = region.id\n\n        self.client.post('/networking/ipv4/assign', data={\n            \"region\": region,\n            \"assignments\": [ a for a in assignments ],\n        })", "response": "This function assigns IP addresses to instances in a single region."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ip_allocate(self, linode, public=True):\n        result = self.client.post('/networking/ipv4/', data={\n            \"linode_id\": linode.id if isinstance(linode, Base) else linode,\n            \"type\": \"ipv4\",\n            \"public\": public,\n        })\n\n        if not 'address' in result:\n            raise UnexpectedResponseError('Unexpected response when adding IPv4 address!',\n                    json=result)\n\n        ip = IPAddress(self.client, result['address'], result)\n        return ip", "response": "Allocates an IP to a Linode instance you own."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shared_ips(self, linode, *ips):\n        if not isinstance(linode, Instance):\n            # make this an object\n            linode = Instance(self.client, linode)\n\n        params = []\n        for ip in ips:\n            if isinstance(ip, str):\n                params.append(ip)\n            elif isinstance(ip, IPAddress):\n                params.append(ip.address)\n            else:\n                params.append(str(ip)) # and hope that works\n\n        params = {\n            \"ips\": params\n        }\n\n        self.client.post('{}/networking/ipv4/share'.format(Instance.api_endpoint),\n                         model=linode, data=params)\n\n        linode.invalidate()", "response": "This method is used to bring up the given list of IPAddresses with the provided Instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs and immediately loads the object.", "response": "def load(self, target_type, target_id, target_parent_id=None):\n        \"\"\"\n        Constructs and immediately loads the object, circumventing the\n        lazy-loading scheme by immediately making an API request.  Does not\n        load related objects.\n\n        For example, if you wanted to load an :any:`Instance` object with ID 123,\n        you could do this::\n\n           loaded_linode = client.load(Instance, 123)\n\n        Similarly, if you instead wanted to load a :any:`NodeBalancerConfig`,\n        you could do so like this::\n\n           loaded_nodebalancer_config = client.load(NodeBalancerConfig, 456, 432)\n\n        :param target_type: The type of object to create.\n        :type target_type: type\n        :param target_id: The ID of the object to create.\n        :type target_id: int or str\n        :param target_parent_id: The parent ID of the object to create, if\n                                 applicable.\n        :type target_parent_id: int, str, or None\n\n        :returns: The resulting object, fully loaded.\n        :rtype: target_type\n        :raise ApiError: if the requested object could not be loaded.\n        \"\"\"\n        result = target_type.make_instance(target_id, self, parent_id=target_parent_id)\n        result._api_get()\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _api_call(self, endpoint, model=None, method=None, data=None, filters=None):\n        if not self.token:\n            raise RuntimeError(\"You do not have an API token!\")\n\n        if not method:\n            raise ValueError(\"Method is required for API calls!\")\n\n        if model:\n            endpoint = endpoint.format(**vars(model))\n        url = '{}{}'.format(self.base_url, endpoint)\n        headers = {\n            'Authorization': \"Bearer {}\".format(self.token),\n            'Content-Type': 'application/json',\n            'User-Agent': self._user_agent,\n        }\n\n        if filters:\n            headers['X-Filter'] = json.dumps(filters)\n\n        body = None\n        if data is not None:\n            body = json.dumps(data)\n\n        response = method(url, headers=headers, data=body)\n\n        warning = response.headers.get('Warning', None)\n        if warning:\n            logger.warning('Received warning from server: {}'.format(warning))\n\n        if 399 < response.status_code < 600:\n            j = None\n            error_msg = '{}: '.format(response.status_code)\n            try:\n                j = response.json()\n                if 'errors' in j.keys():\n                    for e in j['errors']:\n                        error_msg += '{}; '.format(e['reason']) \\\n                                if 'reason' in e.keys() else ''\n            except:\n                pass\n            raise ApiError(error_msg, status=response.status_code, json=j)\n\n        if response.status_code != 204:\n            j = response.json()\n        else:\n            j = None # handle no response body\n\n        return j", "response": "Makes a call to the linode API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new Image from a disk.", "response": "def image_create(self, disk, label=None, description=None):\n        \"\"\"\n        Creates a new Image from a disk you own.\n\n        :param disk: The Disk to imagize.\n        :type disk: Disk or int\n        :param label: The label for the resulting Image (defaults to the disk's\n                      label.\n        :type label: str\n        :param description: The description for the new Image.\n        :type description: str\n\n        :returns: The new Image.\n        :rtype: Image\n        \"\"\"\n        params = {\n            \"disk_id\": disk.id if issubclass(type(disk), Base) else disk,\n        }\n\n        if label is not None:\n            params[\"label\"] = label\n\n        if description is not None:\n            params[\"description\"] = description\n\n        result = self.post('/images', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating an '\n                                          'Image from disk {}'.format(disk))\n\n        return Image(self, result['id'], result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nodebalancer_create(self, region, **kwargs):\n        params = {\n            \"region\": region.id if isinstance(region, Base) else region,\n        }\n        params.update(kwargs)\n\n        result = self.post('/nodebalancers', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating Nodebalaner!', json=result)\n\n        n = NodeBalancer(self, result['id'], result)\n        return n", "response": "Creates a new NodeBalancer in the given Region."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef domain_create(self, domain, master=True, **kwargs):\n        params = {\n            'domain': domain,\n            'type': 'master' if master else 'slave',\n        }\n        params.update(kwargs)\n\n        result = self.post('/domains', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating Domain!', json=result)\n\n        d = Domain(self, result['id'], result)\n        return d", "response": "Registers a new Domain on Linode s nameservers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new Tag and optionally applies it to the given entities. :param label: The label for the new Tag :type label: str :param entities: A list of objects to apply this Tag to upon creation. May only be taggable types (Linode Instances, Domains, NodeBalancers, or Volumes). These are applied *in addition to* any IDs specified with ``instances``, ``domains``, ``nodebalancers``, or ``volumes``, and is a convenience for sending multiple entity types without sorting them yourself. :type entities: list of Instance, Domain, NodeBalancer, and/or Volume :param instances: A list of Linode Instances to apply this Tag to upon creation :type instances: list of Instance or list of int :param domains: A list of Domains to apply this Tag to upon creation :type domains: list of Domain or list of int :param nodebalancers: A list of NodeBalancers to apply this Tag to upon creation :type nodebalancers: list of NodeBalancer or list of int :param volumes: A list of Volumes to apply this Tag to upon creation :type volumes: list of Volumes or list of int :returns: The new Tag :rtype: Tag", "response": "def tag_create(self, label, instances=None, domains=None, nodebalancers=None,\n                   volumes=None, entities=[]):\n        \"\"\"\n        Creates a new Tag and optionally applies it to the given entities.\n\n        :param label: The label for the new Tag\n        :type label: str\n        :param entities: A list of objects to apply this Tag to upon creation.\n                         May only be taggable types (Linode Instances, Domains,\n                         NodeBalancers, or Volumes).  These are applied *in addition\n                         to* any IDs specified with ``instances``, ``domains``,\n                         ``nodebalancers``, or ``volumes``, and is a convenience\n                         for sending multiple entity types without sorting them\n                         yourself.\n        :type entities: list of Instance, Domain, NodeBalancer, and/or Volume\n        :param instances: A list of Linode Instances to apply this Tag to upon\n                        creation\n        :type instances: list of Instance or list of int\n        :param domains: A list of Domains to apply this Tag to upon\n                        creation\n        :type domains: list of Domain or list of int\n        :param nodebalancers: A list of NodeBalancers to apply this Tag to upon\n                        creation\n        :type nodebalancers: list of NodeBalancer or list of int\n        :param volumes: A list of Volumes to apply this Tag to upon\n                        creation\n        :type volumes: list of Volumes or list of int\n\n        :returns: The new Tag\n        :rtype: Tag\n        \"\"\"\n        linode_ids, nodebalancer_ids, domain_ids, volume_ids = [], [], [], []\n\n        # filter input into lists of ids\n        sorter = zip((linode_ids, nodebalancer_ids, domain_ids, volume_ids),\n                     (instances, nodebalancers, domains, volumes))\n\n        for id_list, input_list in sorter:\n            # if we got something, we need to find its ID\n            if input_list is not None:\n                for cur in input_list:\n                    if isinstance(cur, int):\n                        id_list.append(cur)\n                    else:\n                        id_list.append(cur.id)\n\n        # filter entities into id lists too\n        type_map = {\n            Instance: linode_ids,\n            NodeBalancer: nodebalancer_ids,\n            Domain: domain_ids,\n            Volume: volume_ids,\n        }\n\n        for e in entities:\n            if type(e) in type_map:\n                type_map[type(e)].append(e.id)\n            else:\n                raise ValueError('Unsupported entity type {}'.format(type(e)))\n\n        # finally, omit all id lists that are empty\n        params = {\n            'label': label,\n            'linodes': linode_ids or None,\n            'nodebalancers': nodebalancer_ids or None,\n            'domains': domain_ids or None,\n            'volumes': volume_ids or None,\n        }\n\n        result = self.post('/tags', data=params)\n\n        if not 'label' in result:\n            raise UnexpectedResponseError('Unexpected response when creating Tag!', json=result)\n\n        t = Tag(self, result['label'], result)\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Block Storage Volume in the given Region or in the given Linode.", "response": "def volume_create(self, label, region=None, linode=None, size=20, **kwargs):\n        \"\"\"\n        Creates a new Block Storage Volume, either in the given Region or\n        attached to the given Instance.\n\n        :param label: The label for the new Volume.\n        :type label: str\n        :param region: The Region to create this Volume in.  Not required if\n                       `linode` is provided.\n        :type region: Region or str\n        :param linode: The Instance to attach this Volume to.  If not given, the\n                       new Volume will not be attached to anything.\n        :type linode: Instance or int\n        :param size: The size, in GB, of the new Volume.  Defaults to 20.\n        :type size: int\n\n        :returns: The new Volume.\n        :rtype: Volume\n        \"\"\"\n        if not (region or linode):\n            raise ValueError('region or linode required!')\n\n        params = {\n            \"label\": label,\n            \"size\": size,\n            \"region\": region.id if issubclass(type(region), Base) else region,\n            \"linode_id\": linode.id if issubclass(type(linode), Base) else linode,\n        }\n        params.update(kwargs)\n\n        result = self.post('/volumes', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating volume!', json=result)\n\n        v = Volume(self, result['id'], result)\n        return v"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_login_url(self, scopes=None, redirect_uri=None):\n        url = self.base_url + \"/oauth/authorize\"\n        split = list(urlparse(url))\n        params = {\n            \"client_id\": self.client_id,\n            \"response_type\": \"code\", # needed for all logins\n        }\n        if scopes:\n            params[\"scopes\"] = OAuthScopes.serialize(scopes)\n        if redirect_uri:\n            params[\"redirect_uri\"] = redirect_uri\n        split[4] = urlencode(params)\n        return urlunparse(split)", "response": "Generates a url that can be used to send users to for this login attempt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinishing an OAuth exchange with the Linode Login Server.", "response": "def finish_oauth(self, code):\n        \"\"\"\n        Given an OAuth Exchange Code, completes the OAuth exchange with the\n        authentication server.  This should be called once the user has already\n        been directed to the login_uri, and has been sent back after successfully\n        authenticating.  For example, in `Flask`_, this might be implemented as\n        a route like this::\n\n           @app.route(\"/oauth-redirect\")\n           def oauth_redirect():\n               exchange_code = request.args.get(\"code\")\n               login_client = LinodeLoginClient(client_id, client_secret)\n\n               token, scopes = login_client.finish_oauth(exchange_code)\n\n               # store the user's OAuth token in their session for later use\n               # and mark that they are logged in.\n\n               return redirect(\"/\")\n\n        .. _Flask: http://flask.pocoo.org\n\n        :param code: The OAuth Exchange Code returned from the authentication\n                     server in the query string.\n        :type code: str\n\n        :returns: The new OAuth token, and a list of scopes the token has, when\n                  the token expires, and a refresh token that can generate a new\n                  valid token when this one is expired.\n        :rtype: tuple(str, list)\n\n        :raise ApiError: If the OAuth exchange fails.\n        \"\"\"\n        r = requests.post(self._login_uri(\"/oauth/token\"), data={\n                \"code\": code,\n                \"client_id\": self.client_id,\n                \"client_secret\": self.client_secret\n            })\n\n        if r.status_code != 200:\n            raise ApiError(\"OAuth token exchange failed\", status=r.status_code, json=r.json())\n\n        token = r.json()[\"access_token\"]\n        scopes = OAuthScopes.parse(r.json()[\"scopes\"])\n        expiry = datetime.now() + timedelta(seconds=r.json()['expires_in'])\n        refresh_token = r.json()['refresh_token']\n\n        return token, scopes, expiry, refresh_token"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a token, makes a request to the authentication server to expire it immediately. This is considered a responsible way to log out a user. If you simply remove the session your application has for the user without expiring their token, the user is not _really_ logged out. :param token: The OAuth token you wish to expire :type token: str :returns: If the expiration attempt succeeded. :rtype: bool :raises ApiError: If the expiration attempt failed.", "response": "def expire_token(self, token):\n        \"\"\"\n        Given a token, makes a request to the authentication server to expire\n        it immediately.  This is considered a responsible way to log out a\n        user.  If you simply remove the session your application has for the\n        user without expiring their token, the user is not _really_ logged out.\n\n        :param token: The OAuth token you wish to expire\n        :type token: str\n\n        :returns: If the expiration attempt succeeded.\n        :rtype: bool\n\n        :raises ApiError: If the expiration attempt failed.\n        \"\"\"\n        r = requests.post(self._login_uri(\"/oauth/token/expire\"),\n            data={\n                \"client_id\": self.client_id,\n                \"client_secret\": self.client_secret,\n                \"token\": token,\n            })\n\n        if r.status_code != 200:\n            raise ApiError(\"Failed to expire token!\", r)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of grants for the current user", "response": "def grants(self):\n        \"\"\"\n        Returns grants for the current user\n        \"\"\"\n        from linode_api4.objects.account import UserGrants\n        resp = self._client.get('/profile/grants') # use special endpoint for restricted users\n\n        grants = None\n        if resp is not None:\n            # if resp is None, we're unrestricted and do not have grants\n            grants = UserGrants(self._client, self.username, resp)\n\n        return grants"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new entry to this user s IP whitelist.", "response": "def add_whitelist_entry(self, address, netmask, note=None):\n        \"\"\"\n        Adds a new entry to this user's IP whitelist, if enabled\n        \"\"\"\n        result = self._client.post(\"{}/whitelist\".format(Profile.api_endpoint),\n                data={\n                    \"address\": address,\n                    \"netmask\": netmask,\n                    \"note\": note,\n        })\n\n        if not 'id' in result:\n            raise UnexpectedResponseError(\"Unexpected response creating whitelist entry!\")\n\n        return WhitelistEntry(result['id'], self._client, json=result)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncontrol whether the given User may log in. This is a policy setting, independent of end-user authentication. This default behavior is to allow login by active users, and reject login by inactive users. If the given user cannot log in, this method should raise a ``forms.ValidationError``. If the given user may log in, this method should return None.", "response": "def confirm_login_allowed(self, user):\n        \"\"\"\n        Controls whether the given User may log in. This is a policy setting,\n        independent of end-user authentication. This default behavior is to\n        allow login by active users, and reject login by inactive users.\n\n        If the given user cannot log in, this method should raise a\n        ``forms.ValidationError``.\n\n        If the given user may log in, this method should return None.\n        \"\"\"\n        if not user.is_active:\n            raise forms.ValidationError(\n                self.error_messages['inactive'],\n                code='inactive',\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the broken chains in the given samples.", "response": "def broken_chains(samples, chains):\n    \"\"\"Find the broken chains.\n\n    Args:\n        samples (array_like):\n            Samples as a nS x nV array_like object where nS is the number of samples and nV is the\n            number of variables. The values should all be 0/1 or -1/+1.\n\n        chains (list[array_like]):\n            List of chains of length nC where nC is the number of chains.\n            Each chain should be an array_like collection of column indices in samples.\n\n    Returns:\n        :obj:`numpy.ndarray`: A nS x nC boolean array. If i, j is True, then chain j in sample i is\n        broken.\n\n    Examples:\n        >>> samples = np.array([[-1, +1, -1, +1], [-1, -1, +1, +1]], dtype=np.int8)\n        >>> chains = [[0, 1], [2, 3]]\n        >>> dwave.embedding.broken_chains(samples, chains)\n        array([[True, True],\n               [ False,  False]])\n\n        >>> samples = np.array([[-1, +1, -1, +1], [-1, -1, +1, +1]], dtype=np.int8)\n        >>> chains = [[0, 2], [1, 3]]\n        >>> dwave.embedding.broken_chains(samples, chains)\n        array([[False, False],\n               [ True,  True]])\n\n    \"\"\"\n    samples = np.asarray(samples)\n    if samples.ndim != 2:\n        raise ValueError(\"expected samples to be a numpy 2D array\")\n\n    num_samples, num_variables = samples.shape\n    num_chains = len(chains)\n\n    broken = np.zeros((num_samples, num_chains), dtype=bool, order='F')\n\n    for cidx, chain in enumerate(chains):\n        if isinstance(chain, set):\n            chain = list(chain)\n        chain = np.asarray(chain)\n\n        if chain.ndim > 1:\n            raise ValueError(\"chains should be 1D array_like objects\")\n\n        # chains of length 1, or 0 cannot be broken\n        if len(chain) <= 1:\n            continue\n\n        all_ = (samples[:, chain] == 1).all(axis=1)\n        any_ = (samples[:, chain] == 1).any(axis=1)\n        broken[:, cidx] = np.bitwise_xor(all_, any_)\n\n    return broken"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef discard(samples, chains):\n    samples = np.asarray(samples)\n    if samples.ndim != 2:\n        raise ValueError(\"expected samples to be a numpy 2D array\")\n\n    num_samples, num_variables = samples.shape\n    num_chains = len(chains)\n\n    broken = broken_chains(samples, chains)\n\n    unbroken_idxs, = np.where(~broken.any(axis=1))\n\n    chain_variables = np.fromiter((np.asarray(tuple(chain))[0] if isinstance(chain, set) else np.asarray(chain)[0]\n                                   for chain in chains),\n                                  count=num_chains, dtype=int)\n\n    return samples[np.ix_(unbroken_idxs, chain_variables)], unbroken_idxs", "response": "Discards broken source entries in the given samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse the most common element in broken chains.", "response": "def majority_vote(samples, chains):\n    \"\"\"Use the most common element in broken chains.\n\n    Args:\n        samples (array_like):\n            Samples as a nS x nV array_like object where nS is the number of samples and nV is the\n            number of variables. The values should all be 0/1 or -1/+1.\n\n        chains (list[array_like]):\n            List of chains of length nC where nC is the number of chains.\n            Each chain should be an array_like collection of column indices in samples.\n\n    Returns:\n        tuple: A 2-tuple containing:\n\n            :obj:`numpy.ndarray`: A nS x nC array of unembedded samples. The array has dtype 'int8'.\n            Where there is a chain break, the value is chosen to match the most common value in the\n            chain. For broken chains without a majority, the value is chosen arbitrarily.\n\n            :obj:`numpy.ndarray`: Equivalent to :code:`np.arange(nS)` because all samples are kept\n            and no samples are added.\n\n    Examples:\n        This example unembeds samples from a target graph that chains nodes 0 and 1 to\n        represent one source node and nodes 2, 3, and 4 to represent another.\n        Both samples have one broken chain, with different majority values.\n\n        >>> import dimod\n        >>> import numpy as np\n        ...\n        >>> chains = [(0, 1), (2, 3, 4)]\n        >>> samples = np.array([[1, 1, 0, 0, 1], [1, 1, 1, 0, 1]], dtype=np.int8)\n        >>> unembedded, idx = dwave.embedding.majority_vote(samples, chains)\n        >>> unembedded\n        array([[1, 0],\n               [1, 1]], dtype=int8)\n        >>> idx\n        array([0, 1])\n\n    \"\"\"\n    samples = np.asarray(samples)\n    if samples.ndim != 2:\n        raise ValueError(\"expected samples to be a numpy 2D array\")\n\n    num_samples, num_variables = samples.shape\n    num_chains = len(chains)\n\n    unembedded = np.empty((num_samples, num_chains), dtype='int8', order='F')\n\n    # determine if spin or binary. If samples are all 1, then either method works, so we use spin\n    # because it is faster\n    if samples.all():  # spin-valued\n        for cidx, chain in enumerate(chains):\n            # we just need the sign for spin. We don't use np.sign because in that can return 0\n            # and fixing the 0s is slow.\n            unembedded[:, cidx] = 2*(samples[:, chain].sum(axis=1) >= 0) - 1\n    else:  # binary-valued\n        for cidx, chain in enumerate(chains):\n            mid = len(chain) / 2\n            unembedded[:, cidx] = (samples[:, chain].sum(axis=1) >= mid)\n\n    return unembedded, np.arange(num_samples)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the sample values of chains by weighed random choice. Args: samples (array_like): Samples as a nS x nV array_like object where nS is the number of samples and nV is the number of variables. The values should all be 0/1 or -1/+1. chains (list[array_like]): List of chains of length nC where nC is the number of chains. Each chain should be an array_like collection of column indices in samples. Returns: tuple: A 2-tuple containing: :obj:`numpy.ndarray`: A nS x nC array of unembedded samples. The array has dtype 'int8'. Where there is a chain break, the value is chosen randomly, weighted by frequency of the chain's value. :obj:`numpy.ndarray`: Equivalent to :code:`np.arange(nS)` because all samples are kept and no samples are added. Examples: This example unembeds samples from a target graph that chains nodes 0 and 1 to represent one source node and nodes 2, 3, and 4 to represent another. The sample has broken chains for both source nodes. >>> import dimod >>> import numpy as np ... >>> chains = [(0, 1), (2, 3, 4)] >>> samples = np.array([[1, 0, 1, 0, 1]], dtype=np.int8) >>> unembedded, idx = dwave.embedding.weighted_random(samples, chains) # doctest: +SKIP >>> unembedded # doctest: +SKIP array([[1, 1]], dtype=int8) >>> idx # doctest: +SKIP array([0, 1])", "response": "def weighted_random(samples, chains):\n    \"\"\"Determine the sample values of chains by weighed random choice.\n\n    Args:\n        samples (array_like):\n            Samples as a nS x nV array_like object where nS is the number of samples and nV is the\n            number of variables. The values should all be 0/1 or -1/+1.\n\n        chains (list[array_like]):\n            List of chains of length nC where nC is the number of chains.\n            Each chain should be an array_like collection of column indices in samples.\n\n    Returns:\n        tuple: A 2-tuple containing:\n\n            :obj:`numpy.ndarray`: A nS x nC array of unembedded samples. The array has dtype 'int8'.\n            Where there is a chain break, the value is chosen randomly, weighted by frequency of the\n            chain's value.\n\n            :obj:`numpy.ndarray`: Equivalent to :code:`np.arange(nS)` because all samples are kept\n            and no samples are added.\n\n    Examples:\n        This example unembeds samples from a target graph that chains nodes 0 and 1 to\n        represent one source node and nodes 2, 3, and 4 to represent another.\n        The sample has broken chains for both source nodes.\n\n        >>> import dimod\n        >>> import numpy as np\n        ...\n        >>> chains = [(0, 1), (2, 3, 4)]\n        >>> samples = np.array([[1, 0, 1, 0, 1]], dtype=np.int8)\n        >>> unembedded, idx = dwave.embedding.weighted_random(samples, chains)  # doctest: +SKIP\n        >>> unembedded  # doctest: +SKIP\n        array([[1, 1]], dtype=int8)\n        >>> idx  # doctest: +SKIP\n        array([0, 1])\n\n    \"\"\"\n    samples = np.asarray(samples)\n    if samples.ndim != 2:\n        raise ValueError(\"expected samples to be a numpy 2D array\")\n\n    # it sufficies to choose a random index from each chain and use that to construct the matrix\n    idx = [np.random.choice(chain) for chain in chains]\n\n    num_samples, num_variables = samples.shape\n    return samples[:, idx], np.arange(num_samples)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample from the specified Ising model.", "response": "def sample_ising(self, h, J, **kwargs):\n        \"\"\"Sample from the specified Ising model.\n\n        Args:\n            h (list/dict):\n                Linear biases of the Ising model. If a list, the list's indices are\n                used as variable labels.\n\n            J (dict[(int, int): float]):\n                Quadratic biases of the Ising model.\n\n            **kwargs:\n                Optional keyword arguments for the sampling method, specified per solver in\n                :attr:`.DWaveSampler.parameters`\n\n        Returns:\n            :class:`dimod.SampleSet`: A `dimod` :obj:`~dimod.SampleSet` object.\n\n        Examples:\n            This example submits a two-variable Ising problem mapped directly to qubits\n            0 and 1 on a D-Wave system selected by the user's default\n            :std:doc:`D-Wave Cloud Client configuration file <cloud-client:intro>`.\n\n            >>> from dwave.system.samplers import DWaveSampler\n            >>> sampler = DWaveSampler()\n            >>> response = sampler.sample_ising({0: -1, 1: 1}, {})\n            >>> for sample in response.samples():    # doctest: +SKIP\n            ...    print(sample)\n            ...\n            {0: 1, 1: -1}\n\n        See `Ocean Glossary <https://docs.ocean.dwavesys.com/en/latest/glossary.html>`_\n        for explanations of technical terms in descriptions of Ocean tools.\n\n        \"\"\"\n        if isinstance(h, list):\n            h = dict(enumerate(h))\n\n        variables = set(h).union(*J)\n        try:\n            active_variables = sorted(variables)\n        except TypeError:\n            active_variables = list(variables)\n        num_variables = len(active_variables)\n\n        future = self.solver.sample_ising(h, J, **kwargs)\n\n        return dimod.SampleSet.from_future(future, _result_to_response_hook(active_variables, dimod.SPIN))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsampling from the specified QUBO model.", "response": "def sample_qubo(self, Q, **kwargs):\n        \"\"\"Sample from the specified QUBO.\n\n        Args:\n            Q (dict):\n                Coefficients of a quadratic unconstrained binary optimization (QUBO) model.\n\n            **kwargs:\n                Optional keyword arguments for the sampling method, specified per solver in\n                :attr:`.DWaveSampler.parameters`\n\n        Returns:\n            :class:`dimod.SampleSet`: A `dimod` :obj:`~dimod.SampleSet` object.\n\n        Examples:\n            This example submits a two-variable Ising problem mapped directly to qubits\n            0 and 4 on a D-Wave system selected by the user's default\n            :std:doc:`D-Wave Cloud Client configuration file <cloud-client:intro>`.\n\n            >>> from dwave.system.samplers import DWaveSampler\n            >>> sampler = DWaveSampler()\n            >>> Q = {(0, 0): -1, (4, 4): -1, (0, 4): 2}\n            >>> response = sampler.sample_qubo(Q)\n            >>> for sample in response.samples():    # doctest: +SKIP\n            ...    print(sample)\n            ...\n            {0: 0, 4: 1}\n\n        See `Ocean Glossary <https://docs.ocean.dwavesys.com/en/latest/glossary.html>`_\n        for explanations of technical terms in descriptions of Ocean tools.\n\n        \"\"\"\n        variables = set().union(*Q)\n        try:\n            active_variables = sorted(variables)\n        except TypeError:\n            active_variables = list(variables)\n        num_variables = len(active_variables)\n\n        future = self.solver.sample_qubo(Q, **kwargs)\n\n        return dimod.SampleSet.from_future(future, _result_to_response_hook(active_variables, dimod.BINARY))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nraising an exception if the specified schedule is invalid for the sampler.", "response": "def validate_anneal_schedule(self, anneal_schedule):\n        \"\"\"Raise an exception if the specified schedule is invalid for the sampler.\n\n        Args:\n            anneal_schedule (list):\n                An anneal schedule variation is defined by a series of pairs of floating-point\n                numbers identifying points in the schedule at which to change slope. The first\n                element in the pair is time t in microseconds; the second, normalized persistent\n                current s in the range [0,1]. The resulting schedule is the piecewise-linear curve\n                that connects the provided points.\n\n        Raises:\n            ValueError: If the schedule violates any of the conditions listed below.\n\n            RuntimeError: If the sampler does not accept the `anneal_schedule` parameter or\n                if it does not have `annealing_time_range` or `max_anneal_schedule_points`\n                properties.\n\n        An anneal schedule must satisfy the following conditions:\n\n            * Time t must increase for all points in the schedule.\n            * For forward annealing, the first point must be (0,0) and the anneal fraction s must\n              increase monotonically.\n            * For reverse annealing, the anneal fraction s must start and end at s=1.\n            * In the final point, anneal fraction s must equal 1 and time t must not exceed the\n              maximum  value in the `annealing_time_range` property.\n            * The number of points must be >=2.\n            * The upper bound is system-dependent; check the `max_anneal_schedule_points` property.\n              For reverse annealing, the maximum number of points allowed is one more than the\n              number given by this property.\n\n        Examples:\n            This example sets a quench schedule on a D-Wave system selected by the user's default\n            :std:doc:`D-Wave Cloud Client configuration file <cloud-client:intro>`.\n\n            >>> from dwave.system.samplers import DWaveSampler\n            >>> sampler = DWaveSampler()\n            >>> quench_schedule=[[0.0, 0.0], [12.0, 0.6], [12.8, 1.0]]\n            >>> DWaveSampler().validate_anneal_schedule(quench_schedule)    # doctest: +SKIP\n            >>>\n\n        \"\"\"\n        if 'anneal_schedule' not in self.parameters:\n            raise RuntimeError(\"anneal_schedule is not an accepted parameter for this sampler\")\n\n        properties = self.properties\n\n        try:\n            min_anneal_time, max_anneal_time = properties['annealing_time_range']\n            max_anneal_schedule_points = properties['max_anneal_schedule_points']\n        except KeyError:\n            raise RuntimeError(\"annealing_time_range and max_anneal_schedule_points are not properties of this solver\")\n\n        # The number of points must be >= 2.\n        # The upper bound is system-dependent; check the max_anneal_schedule_points property\n        if not isinstance(anneal_schedule, list):\n            raise TypeError(\"anneal_schedule should be a list\")\n        elif len(anneal_schedule) < 2 or len(anneal_schedule) > max_anneal_schedule_points:\n            msg = (\"anneal_schedule must contain between 2 and {} points (contains {})\"\n                   ).format(max_anneal_schedule_points, len(anneal_schedule))\n            raise ValueError(msg)\n\n        try:\n            t_list, s_list = zip(*anneal_schedule)\n        except ValueError:\n            raise ValueError(\"anneal_schedule should be a list of 2-tuples\")\n\n        # Time t must increase for all points in the schedule.\n        if not all(tail_t < lead_t for tail_t, lead_t in zip(t_list, t_list[1:])):\n            raise ValueError(\"Time t must increase for all points in the schedule\")\n\n        # max t cannot exceed max_anneal_time\n        if t_list[-1] > max_anneal_time:\n            raise ValueError(\"schedule cannot be longer than the maximum anneal time of {}\".format(max_anneal_time))\n\n        start_s, end_s = s_list[0], s_list[-1]\n        if end_s != 1:\n            raise ValueError(\"In the final point, anneal fraction s must equal 1.\")\n        if start_s == 1:\n            # reverse annealing\n            pass\n        elif start_s == 0:\n            # forward annealing, s must monotonically increase.\n            if not all(tail_s <= lead_s for tail_s, lead_s in zip(s_list, s_list[1:])):\n                raise ValueError(\"For forward anneals, anneal fraction s must monotonically increase\")\n        else:\n            msg = (\"In the first point, anneal fraction s must equal 0 for forward annealing or \"\n                   \"1 for reverse annealing\")\n            raise ValueError(msg)\n\n        # finally check the slope abs(slope) < 1/min_anneal_time\n        max_slope = 1.0 / min_anneal_time\n        for (t0, s0), (t1, s1) in zip(anneal_schedule, anneal_schedule[1:]):\n            if abs((s0 - s1) / (t0 - t1)) > max_slope:\n                raise ValueError(\"the maximum slope cannot exceed {}\".format(max_slope))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef target_to_source(target_adjacency, embedding):\n    # the nodes in the source adjacency are just the keys of the embedding\n    source_adjacency = {v: set() for v in embedding}\n\n    # we need the mapping from each node in the target to its source node\n    reverse_embedding = {}\n    for v, chain in iteritems(embedding):\n        for u in chain:\n            if u in reverse_embedding:\n                raise ValueError(\"target node {} assigned to more than one source node\".format(u))\n            reverse_embedding[u] = v\n\n    # v is node in target, n node in source\n    for v, n in iteritems(reverse_embedding):\n        neighbors = target_adjacency[v]\n\n        # u is node in target\n        for u in neighbors:\n\n            # some nodes might not be assigned to chains\n            if u not in reverse_embedding:\n                continue\n\n            # m is node in source\n            m = reverse_embedding[u]\n\n            if m == n:\n                continue\n\n            source_adjacency[n].add(m)\n            source_adjacency[m].add(n)\n\n    return source_adjacency", "response": "Derive the source adjacency from an embedding and target adjacency."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chain_to_quadratic(chain, target_adjacency, chain_strength):\n    quadratic = {}  # we will be adding the edges that make the chain here\n\n    # do a breadth first search\n    seen = set()\n    try:\n        next_level = {next(iter(chain))}\n    except StopIteration:\n        raise ValueError(\"chain must have at least one variable\")\n    while next_level:\n        this_level = next_level\n        next_level = set()\n        for v in this_level:\n            if v not in seen:\n                seen.add(v)\n\n                for u in target_adjacency[v]:\n                    if u not in chain:\n                        continue\n                    next_level.add(u)\n                    if u != v and (u, v) not in quadratic:\n                        quadratic[(v, u)] = -chain_strength\n\n    if len(chain) != len(seen):\n        raise ValueError('{} is not a connected chain'.format(chain))\n\n    return quadratic", "response": "Determine the quadratic biases that induce the given chain."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chain_break_frequency(samples_like, embedding):\n    if isinstance(samples_like, dimod.SampleSet):\n        labels = samples_like.variables\n        samples = samples_like.record.sample\n        num_occurrences = samples_like.record.num_occurrences\n    else:\n        samples, labels = dimod.as_samples(samples_like)\n        num_occurrences = np.ones(samples.shape[0])\n\n    if not all(v == idx for idx, v in enumerate(labels)):\n        labels_to_idx = {v: idx for idx, v in enumerate(labels)}\n        embedding = {v: {labels_to_idx[u] for u in chain} for v, chain in embedding.items()}\n\n    if not embedding:\n        return {}\n\n    variables, chains = zip(*embedding.items())\n\n    broken = broken_chains(samples, chains)\n\n    return {v: float(np.average(broken[:, cidx], weights=num_occurrences))\n            for cidx, v in enumerate(variables)}", "response": "Determine the frequency of chain breaks in the given samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an iterator of edges to an adjacency dict.", "response": "def edgelist_to_adjacency(edgelist):\n    \"\"\"Converts an iterator of edges to an adjacency dict.\n\n    Args:\n        edgelist (iterable):\n            An iterator over 2-tuples where each 2-tuple is an edge.\n\n    Returns:\n        dict: The adjacency dict. A dict of the form {v: Nv, ...} where v is a node in a graph and\n        Nv is the neighbors of v as an set.\n\n    \"\"\"\n    adjacency = dict()\n    for u, v in edgelist:\n        if u in adjacency:\n            adjacency[u].add(v)\n        else:\n            adjacency[u] = {v}\n        if v in adjacency:\n            adjacency[v].add(u)\n        else:\n            adjacency[v] = {u}\n    return adjacency"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsamples from the specified binary quadratic model.", "response": "def sample(self, bqm, **kwargs):\n        \"\"\"Sample from the specified binary quadratic model.\n\n        Args:\n            bqm (:obj:`dimod.BinaryQuadraticModel`):\n                Binary quadratic model to be sampled from.\n\n            **kwargs:\n                Optional keyword arguments for the sampling method, specified per solver.\n\n        Returns:\n            :class:`dimod.SampleSet`\n\n        Examples:\n            This example submits a simple Ising problem of just two variables on a\n            D-Wave system selected by the user's default\n            :std:doc:`D-Wave Cloud Client configuration file <cloud-client:intro>`.\n            Because the problem fits in a single :term:`Chimera` unit cell, it is tiled\n            across the solver's entire Chimera graph, resulting in multiple samples\n            (the exact number depends on the working Chimera graph of the D-Wave system).\n\n            >>> from dwave.system.samplers import DWaveSampler\n            >>> from dwave.system.composites import EmbeddingComposite\n            >>> from dwave.system.composites import EmbeddingComposite, TilingComposite\n            ...\n            >>> sampler = EmbeddingComposite(TilingComposite(DWaveSampler(), 1, 1, 4))\n            >>> response = sampler.sample_ising({},{('a', 'b'): 1})\n            >>> len(response)    # doctest: +SKIP\n            246\n\n        See `Ocean Glossary <https://docs.ocean.dwavesys.com/en/latest/glossary.html>`_\n        for explanations of technical terms in descriptions of Ocean tools.\n\n        \"\"\"\n\n        # apply the embeddings to the given problem to tile it across the child sampler\n        embedded_bqm = dimod.BinaryQuadraticModel.empty(bqm.vartype)\n        __, __, target_adjacency = self.child.structure\n        for embedding in self.embeddings:\n            embedded_bqm.update(dwave.embedding.embed_bqm(bqm, embedding, target_adjacency))\n\n        # solve the problem on the child system\n        tiled_response = self.child.sample(embedded_bqm, **kwargs)\n\n        responses = []\n\n        for embedding in self.embeddings:\n            embedding = {v: chain for v, chain in embedding.items() if v in bqm.variables}\n\n            responses.append(dwave.embedding.unembed_sampleset(tiled_response, embedding, bqm))\n\n        return dimod.concatenate(responses)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a sqlite3 connection object to a sqlite database.", "response": "def cache_connect(database=None):\n    \"\"\"Returns a connection object to a sqlite database.\n\n    Args:\n        database (str, optional): The path to the database the user wishes\n            to connect to. If not specified, a default is chosen using\n            :func:`.cache_file`. If the special database name ':memory:'\n            is given, then a temporary database is created in memory.\n\n    Returns:\n        :class:`sqlite3.Connection`\n\n    \"\"\"\n    if database is None:\n        database = cache_file()\n\n    if os.path.isfile(database):\n        # just connect to the database as-is\n        conn = sqlite3.connect(database)\n    else:\n        # we need to populate the database\n        conn = sqlite3.connect(database)\n        conn.executescript(schema)\n\n    with conn as cur:\n        # turn on foreign keys, allows deletes to cascade.\n        cur.execute(\"PRAGMA foreign_keys = ON;\")\n\n    conn.row_factory = sqlite3.Row\n\n    return conn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef insert_chain(cur, chain, encoded_data=None):\n    if encoded_data is None:\n        encoded_data = {}\n\n    if 'nodes' not in encoded_data:\n        encoded_data['nodes'] = json.dumps(sorted(chain), separators=(',', ':'))\n    if 'chain_length' not in encoded_data:\n        encoded_data['chain_length'] = len(chain)\n\n    insert = \"INSERT OR IGNORE INTO chain(chain_length, nodes) VALUES (:chain_length, :nodes);\"\n\n    cur.execute(insert, encoded_data)", "response": "Insert a chain into the cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_chain(cur):\n    select = \"SELECT nodes FROM chain\"\n    for nodes, in cur.execute(select):\n        yield json.loads(nodes)", "response": "Iterate over all of the chain nodes in the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_system(cur, system_name, encoded_data=None):\n    if encoded_data is None:\n        encoded_data = {}\n\n    if 'system_name' not in encoded_data:\n        encoded_data['system_name'] = system_name\n\n    insert = \"INSERT OR IGNORE INTO system(system_name) VALUES (:system_name);\"\n    cur.execute(insert, encoded_data)", "response": "Inserts a system name into the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts a flux bias offset into the cache.", "response": "def insert_flux_bias(cur, chain, system, flux_bias, chain_strength, encoded_data=None):\n    \"\"\"Insert a flux bias offset into the cache.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n        chain (iterable):\n            A collection of nodes. Chains in embedding act as one node.\n\n        system (str):\n            The unique name of a system.\n\n        flux_bias (float):\n            The flux bias offset associated with the given chain.\n\n        chain_strength (float):\n            The magnitude of the negative quadratic bias that induces the given chain in an Ising\n            problem.\n\n        encoded_data (dict, optional):\n            If a dictionary is provided, it will be populated with the serialized data. This is\n            useful for preventing encoding the same information many times.\n\n    \"\"\"\n    if encoded_data is None:\n        encoded_data = {}\n\n    insert_chain(cur, chain, encoded_data)\n    insert_system(cur, system, encoded_data)\n\n    if 'flux_bias' not in encoded_data:\n        encoded_data['flux_bias'] = _encode_real(flux_bias)\n    if 'chain_strength' not in encoded_data:\n        encoded_data['chain_strength'] = _encode_real(chain_strength)\n    if 'insert_time' not in encoded_data:\n        encoded_data['insert_time'] = datetime.datetime.now()\n\n    insert = \\\n        \"\"\"\n        INSERT OR REPLACE INTO flux_bias(chain_id, system_id, insert_time, flux_bias, chain_strength)\n        SELECT\n            chain.id,\n            system.id,\n            :insert_time,\n            :flux_bias,\n            :chain_strength\n        FROM chain, system\n        WHERE\n            chain.chain_length = :chain_length AND\n            chain.nodes = :nodes AND\n            system.system_name = :system_name;\n        \"\"\"\n\n    cur.execute(insert, encoded_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\niterate over all flux biases in the cache.", "response": "def iter_flux_bias(cur):\n    \"\"\"Iterate over all flux biases in the cache.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n    Yields:\n        tuple: A 4-tuple:\n\n            list: The chain.\n\n            str: The system name.\n\n            float: The flux bias associated with the chain.\n\n            float: The chain strength associated with the chain.\n\n    \"\"\"\n    select = \\\n        \"\"\"\n        SELECT nodes, system_name, flux_bias, chain_strength FROM flux_bias_view;\n        \"\"\"\n\n    for nodes, system, flux_bias, chain_strength in cur.execute(select):\n        yield json.loads(nodes), system, _decode_real(flux_bias), _decode_real(chain_strength)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the flux biases for all of the given chains system and chain strength.", "response": "def get_flux_biases_from_cache(cur, chains, system_name, chain_strength, max_age=3600):\n    \"\"\"Determine the flux biases for all of the the given chains, system and chain strength.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n        chains (iterable):\n            An iterable of chains. Each chain is a collection of nodes. Chains in embedding act as\n            one node.\n\n        system_name (str):\n            The unique name of a system.\n\n        chain_strength (float):\n            The magnitude of the negative quadratic bias that induces the given chain in an Ising\n            problem.\n\n        max_age (int, optional, default=3600):\n            The maximum age (in seconds) for the flux_bias offsets.\n\n    Returns:\n        dict: A dict where the keys are the nodes in the chains and the values are the flux biases.\n\n    \"\"\"\n\n    select = \\\n        \"\"\"\n        SELECT\n            flux_bias\n        FROM flux_bias_view WHERE\n            chain_length = :chain_length AND\n            nodes = :nodes AND\n            chain_strength = :chain_strength AND\n            system_name = :system_name AND\n            insert_time >= :time_limit;\n        \"\"\"\n\n    encoded_data = {'chain_strength': _encode_real(chain_strength),\n                    'system_name': system_name,\n                    'time_limit': datetime.datetime.now() + datetime.timedelta(seconds=-max_age)}\n\n    flux_biases = {}\n    for chain in chains:\n        encoded_data['chain_length'] = len(chain)\n        encoded_data['nodes'] = json.dumps(sorted(chain), separators=(',', ':'))\n\n        row = cur.execute(select, encoded_data).fetchone()\n        if row is None:\n            raise MissingFluxBias\n        flux_bias = _decode_real(*row)\n\n        if flux_bias == 0:\n            continue\n\n        flux_biases.update({v: flux_bias for v in chain})\n\n    return flux_biases"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert_graph(cur, nodelist, edgelist, encoded_data=None):\n    if encoded_data is None:\n        encoded_data = {}\n\n    if 'num_nodes' not in encoded_data:\n        encoded_data['num_nodes'] = len(nodelist)\n    if 'num_edges' not in encoded_data:\n        encoded_data['num_edges'] = len(edgelist)\n    if 'edges' not in encoded_data:\n        encoded_data['edges'] = json.dumps(edgelist, separators=(',', ':'))\n\n    insert = \\\n        \"\"\"\n        INSERT OR IGNORE INTO graph(num_nodes, num_edges, edges)\n        VALUES (:num_nodes, :num_edges, :edges);\n        \"\"\"\n\n    cur.execute(insert, encoded_data)", "response": "Insert a graph into the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate over all graphs in the cache.", "response": "def iter_graph(cur):\n    \"\"\"Iterate over all graphs in the cache.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`): An sqlite3 cursor. This function\n            is meant to be run within a :obj:`with` statement.\n\n    Yields:\n        tuple: A 2-tuple containing:\n\n            list: The nodelist for a graph in the cache.\n\n            list: the edgelist for a graph in the cache.\n\n    Examples:\n        >>> nodelist = [0, 1, 2]\n        >>> edgelist = [(0, 1), (1, 2)]\n        >>> with pmc.cache_connect(':memory:') as cur:\n        ...     pmc.insert_graph(cur, nodelist, edgelist)\n        ...     list(pmc.iter_graph(cur))\n        [([0, 1, 2], [[0, 1], [1, 2]])]\n\n    \"\"\"\n    select = \"\"\"SELECT num_nodes, num_edges, edges from graph;\"\"\"\n    for num_nodes, num_edges, edges in cur.execute(select):\n        yield list(range(num_nodes)), json.loads(edges)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_embedding(cur, source_nodelist, source_edgelist, target_nodelist, target_edgelist,\n                     embedding, embedding_tag):\n    \"\"\"Insert an embedding into the cache.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n        source_nodelist (list):\n            The nodes in the source graph. Should be integer valued.\n\n        source_edgelist (list):\n            The edges in the source graph.\n\n        target_nodelist (list):\n            The nodes in the target graph. Should be integer valued.\n\n        target_edgelist (list):\n            The edges in the target graph.\n\n        embedding (dict):\n            The mapping from the source graph to the target graph.\n            Should be of the form {v: {s, ...}, ...} where v is a variable in the\n            source model and s is a variable in the target model.\n\n        embedding_tag (str):\n            A string tag to associate with the embedding.\n\n    \"\"\"\n    encoded_data = {}\n\n    # first we need to encode the graphs and create the embedding id\n\n    source_data = {}\n    insert_graph(cur, source_nodelist, source_edgelist, source_data)\n    encoded_data['source_edges'] = source_data['edges']\n    encoded_data['source_num_nodes'] = source_data['num_nodes']\n    encoded_data['source_num_edges'] = source_data['num_edges']\n\n    target_data = {}\n    insert_graph(cur, target_nodelist, target_edgelist, target_data)\n    encoded_data['target_edges'] = target_data['edges']\n    encoded_data['target_num_nodes'] = target_data['num_nodes']\n    encoded_data['target_num_edges'] = target_data['num_edges']\n\n    encoded_data['tag'] = embedding_tag\n\n    insert_embedding = \\\n        \"\"\"\n        INSERT OR REPLACE INTO embedding(\n            source_id,\n            target_id,\n            tag)\n        SELECT\n            source_graph.id,\n            target_graph.id,\n            :tag\n        FROM\n            graph 'source_graph',\n            graph 'target_graph'\n        WHERE\n            source_graph.edges = :source_edges AND\n            source_graph.num_nodes = :source_num_nodes AND\n            source_graph.num_edges = :source_num_edges AND\n            target_graph.edges = :target_edges AND\n            target_graph.num_nodes = :target_num_nodes AND\n            target_graph.num_edges = :target_num_edges\n        \"\"\"\n\n    cur.execute(insert_embedding, encoded_data)\n\n    # now each chain needs to be inserted\n\n    insert_embedding_component = \\\n        \"\"\"\n        INSERT OR REPLACE INTO embedding_component(\n            source_node,\n            chain_id,\n            embedding_id)\n        SELECT\n            :source_node,\n            chain.id,\n            embedding.id\n        FROM\n            graph 'source_graph',\n            graph 'target_graph',\n            chain,\n            embedding\n        WHERE\n            source_graph.edges = :source_edges AND\n            source_graph.num_nodes = :source_num_nodes AND\n            target_graph.edges = :target_edges AND\n            target_graph.num_nodes = :target_num_nodes AND\n            embedding.source_id = source_graph.id AND\n            embedding.target_id = target_graph.id AND\n            embedding.tag = :tag AND\n            chain.nodes = :nodes AND\n            chain.chain_length = :chain_length\n        \"\"\"\n\n    for v, chain in iteritems(embedding):\n        chain_data = {'source_node': v}\n        insert_chain(cur, chain, chain_data)\n\n        encoded_data.update(chain_data)\n\n        cur.execute(insert_embedding_component, encoded_data)", "response": "Inserts an embedding into the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects an embedding from the given tag and target graph.", "response": "def select_embedding_from_tag(cur, embedding_tag, target_nodelist, target_edgelist):\n    \"\"\"Select an embedding from the given tag and target graph.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n        source_nodelist (list):\n            The nodes in the source graph. Should be integer valued.\n\n        source_edgelist (list):\n            The edges in the source graph.\n\n        target_nodelist (list):\n            The nodes in the target graph. Should be integer valued.\n\n        target_edgelist (list):\n            The edges in the target graph.\n\n    Returns:\n        dict: The mapping from the source graph to the target graph.\n        In the form {v: {s, ...}, ...} where v is a variable in the\n        source model and s is a variable in the target model.\n\n    \"\"\"\n    encoded_data = {'num_nodes': len(target_nodelist),\n                    'num_edges': len(target_edgelist),\n                    'edges': json.dumps(target_edgelist, separators=(',', ':')),\n                    'tag': embedding_tag}\n\n    select = \\\n        \"\"\"\n        SELECT\n            source_node,\n            chain\n        FROM\n            embedding_component_view\n        WHERE\n            embedding_tag = :tag AND\n            target_edges = :edges AND\n            target_num_nodes = :num_nodes AND\n            target_num_edges = :num_edges\n        \"\"\"\n\n    embedding = {v: json.loads(chain) for v, chain in cur.execute(select, encoded_data)}\n    return embedding"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_embedding_from_source(cur, source_nodelist, source_edgelist,\n                                 target_nodelist, target_edgelist):\n    \"\"\"Select an embedding from the source graph and target graph.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n        target_nodelist (list):\n            The nodes in the target graph. Should be integer valued.\n\n        target_edgelist (list):\n            The edges in the target graph.\n\n        embedding_tag (str):\n            A string tag to associate with the embedding.\n\n    Returns:\n        dict: The mapping from the source graph to the target graph.\n        In the form {v: {s, ...}, ...} where v is a variable in the\n        source model and s is a variable in the target model.\n\n    \"\"\"\n    encoded_data = {'target_num_nodes': len(target_nodelist),\n                    'target_num_edges': len(target_edgelist),\n                    'target_edges': json.dumps(target_edgelist, separators=(',', ':')),\n                    'source_num_nodes': len(source_nodelist),\n                    'source_num_edges': len(source_edgelist),\n                    'source_edges': json.dumps(source_edgelist, separators=(',', ':'))}\n\n    select = \\\n        \"\"\"\n        SELECT\n            source_node,\n            chain\n        FROM\n            embedding_component_view\n        WHERE\n            source_num_edges = :source_num_edges AND\n            source_edges = :source_edges AND\n            source_num_nodes = :source_num_nodes AND\n\n            target_num_edges = :target_num_edges AND\n            target_edges = :target_edges AND\n            target_num_nodes = :target_num_nodes\n        \"\"\"\n\n    embedding = {v: json.loads(chain) for v, chain in cur.execute(select, encoded_data)}\n    return embedding", "response": "Select an embedding from the source graph and target graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_clique_embedding(k, m=None, target_graph=None):\n    # Organize parameter values\n    if target_graph is None:\n        if m is None:\n            raise TypeError(\"m and target_graph cannot both be None.\")\n        target_graph = pegasus_graph(m)\n\n    m = target_graph.graph['rows']     # We only support square Pegasus graphs\n    _, nodes = k\n\n    # Deal with differences in ints vs coordinate target_graphs\n    if target_graph.graph['labels'] == 'nice':\n        fwd_converter = get_nice_to_pegasus_fn(m = m)\n        back_converter = get_pegasus_to_nice_fn(m = m)\n        pegasus_coords = [fwd_converter(*p) for p in target_graph.nodes]\n        back_translate = lambda embedding: {key: [back_converter(*p) for p in chain]\n                                      for key, chain in embedding.items()}\n    elif target_graph.graph['labels'] == 'int':\n        # Convert nodes in terms of Pegasus coordinates\n        coord_converter = pegasus_coordinates(m)\n        pegasus_coords = map(coord_converter.tuple, target_graph.nodes)\n\n        # A function to convert our final coordinate embedding to an ints embedding\n        back_translate = lambda embedding: {key: list(coord_converter.ints(chain))\n                                      for key, chain in embedding.items()}\n    else:\n        pegasus_coords = target_graph.nodes\n        back_translate = lambda embedding: embedding\n\n    # Break each Pegasus qubits into six Chimera fragments\n    # Note: By breaking the graph in this way, you end up with a K2,2 Chimera graph\n    fragment_tuple = get_tuple_fragmentation_fn(target_graph)\n    fragments = fragment_tuple(pegasus_coords)\n\n    # Create a K2,2 Chimera graph\n    # Note: 6 * m because Pegasus qubits split into six pieces, so the number of rows and columns\n    #   get multiplied by six\n    chim_m = 6 * m\n    chim_graph = chimera_graph(chim_m, t=2, coordinates=True)\n\n    # Determine valid fragment couplers in a K2,2 Chimera graph\n    edges = chim_graph.subgraph(fragments).edges()\n\n    # Find clique embedding in K2,2 Chimera graph\n    embedding_processor = processor(edges, M=chim_m, N=chim_m, L=2, linear=False)\n    chimera_clique_embedding = embedding_processor.tightestNativeClique(len(nodes))\n\n    # Convert chimera fragment embedding in terms of Pegasus coordinates\n    defragment_tuple = get_tuple_defragmentation_fn(target_graph)\n    pegasus_clique_embedding = map(defragment_tuple, chimera_clique_embedding)\n    pegasus_clique_embedding = dict(zip(nodes, pegasus_clique_embedding))\n    pegasus_clique_embedding = back_translate(pegasus_clique_embedding)\n\n    if len(pegasus_clique_embedding) != len(nodes):\n        raise ValueError(\"No clique embedding found\")\n\n    return pegasus_clique_embedding", "response": "Find an embedding of a k - sized clique on a Pegasus graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndraws a Chimera Graph representation of a Binary Quadratic Model.", "response": "def draw_chimera_bqm(bqm, width=None, height=None):\n    \"\"\"Draws a Chimera Graph representation of a Binary Quadratic Model.\n\n    If cell width and height not provided assumes square cell dimensions.\n    Throws an error if drawing onto a Chimera graph of the given dimensions fails.\n\n    Args:\n        bqm (:obj:`dimod.BinaryQuadraticModel`):\n            Should be equivalent to a Chimera graph or a subgraph of a Chimera graph produced by dnx.chimera_graph.\n            The nodes and edges should have integer variables as in the dnx.chimera_graph.\n        width (int, optional):\n            An integer representing the number of cells of the Chimera graph will be in width.\n        height (int, optional):\n            An integer representing the number of cells of the Chimera graph will be in height.\n\n    Examples:\n        >>> from dwave.embedding.drawing import draw_chimera_bqm\n        >>> from dimod import BinaryQuadraticModel\n        >>> Q={(0, 0): 2, (1, 1): 1, (2, 2): 0, (3, 3): -1, (4, 4): -2, (5, 5): -2, (6, 6): -2, (7, 7): -2,\n        ... (0, 4): 2, (0, 4): -1, (1, 7): 1, (1, 5): 0, (2, 5): -2, (2, 6): -2, (3, 4): -2, (3, 7): -2}\n        >>> draw_chimera_bqm(BinaryQuadraticModel.from_qubo(Q), width=1, height=1)\n\n    \"\"\"\n\n    linear = bqm.linear.keys()\n    quadratic = bqm.quadratic.keys()\n\n    if width is None and height is None:\n        # Create a graph large enough to fit the input networkx graph.\n        graph_size = ceil(sqrt((max(linear) + 1) / 8.0))\n        width = graph_size\n        height = graph_size\n\n    if not width or not height:\n        raise Exception(\"Both dimensions must be defined, not just one.\")\n\n    # A background image of the same size is created to show the complete graph.\n    G0 = chimera_graph(height, width, 4)\n    G = chimera_graph(height, width, 4)\n\n\n    # Check if input graph is chimera graph shaped, by making sure that no edges are invalid.\n    # Invalid edges can also appear if the size of the chimera graph is incompatible with the input graph in cell dimensions.\n    non_chimera_nodes = []\n    non_chimera_edges = []\n    for node in linear:\n        if not node in G.nodes:\n            non_chimera_nodes.append(node)\n    for edge in quadratic:\n        if not edge in G.edges:\n            non_chimera_edges.append(edge)\n\n    linear_set = set(linear)\n    g_node_set = set(G.nodes)\n\n    quadratic_set = set(map(frozenset, quadratic))\n    g_edge_set = set(map(frozenset, G.edges))\n\n    non_chimera_nodes = linear_set - g_node_set\n    non_chimera_edges = quadratic_set - g_edge_set\n\n    if non_chimera_nodes or non_chimera_edges:\n        raise Exception(\"Input graph is not a chimera graph: Nodes: %s Edges: %s\" % (non_chimera_nodes, non_chimera_edges))\n\n\n    # Get lists of nodes and edges to remove from the complete graph to turn the complete graph into your graph.\n    remove_nodes = list(g_node_set - linear_set)\n    remove_edges = list(g_edge_set - quadratic_set)\n\n    # Remove the nodes and edges from the graph.\n    for edge in remove_edges:\n        G.remove_edge(*edge)\n    for node in remove_nodes:\n        G.remove_node(node)\n\n    node_size = 100\n    # Draw the complete chimera graph as the background.\n    draw_chimera(G0, node_size=node_size*0.5, node_color='black', edge_color='black')\n    # Draw your graph over the complete graph to show the connectivity.\n    draw_chimera(G, node_size=node_size, linear_biases=bqm.linear, quadratic_biases=bqm.quadratic,\n                     width=3)\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef embed_bqm(source_bqm, embedding, target_adjacency, chain_strength=1.0,\n              smear_vartype=None):\n    \"\"\"Embed a binary quadratic model onto a target graph.\n\n    Args:\n        source_bqm (:obj:`.BinaryQuadraticModel`):\n            Binary quadratic model to embed.\n\n        embedding (dict):\n            Mapping from source graph to target graph as a dict of form {s: {t, ...}, ...},\n            where s is a source-model variable and t is a target-model variable.\n\n        target_adjacency (dict/:class:`networkx.Graph`):\n            Adjacency of the target graph as a dict of form {t: Nt, ...},\n            where t is a variable in the target graph and Nt is its set of neighbours.\n\n        chain_strength (float, optional):\n            Magnitude of the quadratic bias (in SPIN-space) applied between variables to create chains. Note\n            that the energy penalty of chain breaks is 2 * `chain_strength`.\n\n        smear_vartype (:class:`.Vartype`, optional, default=None):\n            When a single variable is embedded, it's linear bias is 'smeared' evenly over the\n            chain. This parameter determines whether the variable is smeared in SPIN or BINARY\n            space. By default the embedding is done according to the given source_bqm.\n\n    Returns:\n        :obj:`.BinaryQuadraticModel`: Target binary quadratic model.\n\n    Examples:\n        This example embeds a fully connected :math:`K_3` graph onto a square target graph.\n        Embedding is accomplished by an edge contraction operation on the target graph:\n        target-nodes 2 and 3 are chained to represent source-node c.\n\n        >>> import dimod\n        >>> import networkx as nx\n        >>> # Binary quadratic model for a triangular source graph\n        >>> bqm = dimod.BinaryQuadraticModel.from_ising({}, {('a', 'b'): 1, ('b', 'c'): 1, ('a', 'c'): 1})\n        >>> # Target graph is a graph\n        >>> target = nx.cycle_graph(4)\n        >>> # Embedding from source to target graphs\n        >>> embedding = {'a': {0}, 'b': {1}, 'c': {2, 3}}\n        >>> # Embed the BQM\n        >>> target_bqm = dimod.embed_bqm(bqm, embedding, target)\n        >>> target_bqm.quadratic[(0, 1)] == bqm.quadratic[('a', 'b')]\n        True\n        >>> target_bqm.quadratic   # doctest: +SKIP\n        {(0, 1): 1.0, (0, 3): 1.0, (1, 2): 1.0, (2, 3): -1.0}\n\n        This example embeds a fully connected :math:`K_3` graph onto the target graph\n        of a dimod reference structured sampler, `StructureComposite`, using the dimod reference\n        `ExactSolver` sampler with a square graph specified. Target-nodes 2 and 3\n        are chained to represent source-node c.\n\n        >>> import dimod\n        >>> # Binary quadratic model for a triangular source graph\n        >>> bqm = dimod.BinaryQuadraticModel.from_ising({}, {('a', 'b'): 1, ('b', 'c'): 1, ('a', 'c'): 1})\n        >>> # Structured dimod sampler with a structure defined by a square graph\n        >>> sampler = dimod.StructureComposite(dimod.ExactSolver(), [0, 1, 2, 3], [(0, 1), (1, 2), (2, 3), (0, 3)])\n        >>> # Embedding from source to target graph\n        >>> embedding = {'a': {0}, 'b': {1}, 'c': {2, 3}}\n        >>> # Embed the BQM\n        >>> target_bqm = dimod.embed_bqm(bqm, embedding, sampler.adjacency)\n        >>> # Sample\n        >>> samples = sampler.sample(target_bqm)\n        >>> samples.record.sample   # doctest: +SKIP\n        array([[-1, -1, -1, -1],\n               [ 1, -1, -1, -1],\n               [ 1,  1, -1, -1],\n               [-1,  1, -1, -1],\n               [-1,  1,  1, -1],\n        >>> # Snipped above samples for brevity\n\n    \"\"\"\n    if smear_vartype is dimod.SPIN and source_bqm.vartype is dimod.BINARY:\n        return embed_bqm(source_bqm.spin, embedding, target_adjacency,\n                         chain_strength=chain_strength, smear_vartype=None).binary\n    elif smear_vartype is dimod.BINARY and source_bqm.vartype is dimod.SPIN:\n        return embed_bqm(source_bqm.binary, embedding, target_adjacency,\n                         chain_strength=chain_strength, smear_vartype=None).spin\n\n    # create a new empty binary quadratic model with the same class as source_bqm\n    target_bqm = source_bqm.empty(source_bqm.vartype)\n\n    # add the offset\n    target_bqm.add_offset(source_bqm.offset)\n\n    # start with the linear biases, spreading the source bias equally over the target variables in\n    # the chain\n    for v, bias in iteritems(source_bqm.linear):\n\n        if v in embedding:\n            chain = embedding[v]\n        else:\n            raise MissingChainError(v)\n\n        if any(u not in target_adjacency for u in chain):\n            raise InvalidNodeError(v, next(u not in target_adjacency for u in chain))\n\n        b = bias / len(chain)\n\n        target_bqm.add_variables_from({u: b for u in chain})\n\n    # next up the quadratic biases, spread the quadratic biases evenly over the available\n    # interactions\n    for (u, v), bias in iteritems(source_bqm.quadratic):\n        available_interactions = {(s, t) for s in embedding[u] for t in embedding[v] if s in target_adjacency[t]}\n\n        if not available_interactions:\n            raise MissingEdgeError(u, v)\n\n        b = bias / len(available_interactions)\n\n        target_bqm.add_interactions_from((u, v, b) for u, v in available_interactions)\n\n    for chain in itervalues(embedding):\n\n        # in the case where the chain has length 1, there are no chain quadratic biases, but we\n        # none-the-less want the chain variables to appear in the target_bqm\n        if len(chain) == 1:\n            v, = chain\n            target_bqm.add_variable(v, 0.0)\n            continue\n\n        quadratic_chain_biases = chain_to_quadratic(chain, target_adjacency, chain_strength)\n        target_bqm.add_interactions_from(quadratic_chain_biases, vartype=dimod.SPIN)  # these are spin\n\n        # add the energy for satisfied chains to the offset\n        energy_diff = -sum(itervalues(quadratic_chain_biases))\n        target_bqm.add_offset(energy_diff)\n\n    return target_bqm", "response": "Embed a binary quadratic model onto a target graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nembedding an Ising problem onto a target graph.", "response": "def embed_ising(source_h, source_J, embedding, target_adjacency, chain_strength=1.0):\n    \"\"\"Embed an Ising problem onto a target graph.\n\n    Args:\n        source_h (dict[variable, bias]/list[bias]):\n            Linear biases of the Ising problem. If a list, the list's indices are used as\n            variable labels.\n\n        source_J (dict[(variable, variable), bias]):\n            Quadratic biases of the Ising problem.\n\n        embedding (dict):\n            Mapping from source graph to target graph as a dict of form {s: {t, ...}, ...},\n            where s is a source-model variable and t is a target-model variable.\n\n        target_adjacency (dict/:class:`networkx.Graph`):\n            Adjacency of the target graph as a dict of form {t: Nt, ...},\n            where t is a target-graph variable and Nt is its set of neighbours.\n\n        chain_strength (float, optional):\n            Magnitude of the quadratic bias (in SPIN-space) applied between variables to form a chain. Note\n            that the energy penalty of chain breaks is 2 * `chain_strength`.\n\n    Returns:\n        tuple: A 2-tuple:\n\n            dict[variable, bias]: Linear biases of the target Ising problem.\n\n            dict[(variable, variable), bias]: Quadratic biases of the target Ising problem.\n\n    Examples:\n        This example embeds a fully connected :math:`K_3` graph onto a square target graph.\n        Embedding is accomplished by an edge contraction operation on the target graph: target-nodes\n        2 and 3 are chained to represent source-node c.\n\n        >>> import dimod\n        >>> import networkx as nx\n        >>> # Ising problem for a triangular source graph\n        >>> h = {}\n        >>> J = {('a', 'b'): 1, ('b', 'c'): 1, ('a', 'c'): 1}\n        >>> # Target graph is a square graph\n        >>> target = nx.cycle_graph(4)\n        >>> # Embedding from source to target graph\n        >>> embedding = {'a': {0}, 'b': {1}, 'c': {2, 3}}\n        >>> # Embed the Ising problem\n        >>> target_h, target_J = dimod.embed_ising(h, J, embedding, target)\n        >>> target_J[(0, 1)] == J[('a', 'b')]\n        True\n        >>> target_J        # doctest: +SKIP\n        {(0, 1): 1.0, (0, 3): 1.0, (1, 2): 1.0, (2, 3): -1.0}\n\n        This example embeds a fully connected :math:`K_3` graph onto the target graph\n        of a dimod reference structured sampler, `StructureComposite`, using the dimod reference\n        `ExactSolver` sampler with a square graph specified. Target-nodes 2 and 3 are chained to\n        represent source-node c.\n\n        >>> import dimod\n        >>> # Ising problem for a triangular source graph\n        >>> h = {}\n        >>> J = {('a', 'b'): 1, ('b', 'c'): 1, ('a', 'c'): 1}\n        >>> # Structured dimod sampler with a structure defined by a square graph\n        >>> sampler = dimod.StructureComposite(dimod.ExactSolver(), [0, 1, 2, 3], [(0, 1), (1, 2), (2, 3), (0, 3)])\n        >>> # Embedding from source to target graph\n        >>> embedding = {'a': {0}, 'b': {1}, 'c': {2, 3}}\n        >>> # Embed the Ising problem\n        >>> target_h, target_J = dimod.embed_ising(h, J, embedding, sampler.adjacency)\n        >>> # Sample\n        >>> samples = sampler.sample_ising(target_h, target_J)\n        >>> for sample in samples.samples(n=3, sorted_by='energy'):   # doctest: +SKIP\n        ...     print(sample)\n        ...\n        {0: 1, 1: -1, 2: -1, 3: -1}\n        {0: 1, 1: 1, 2: -1, 3: -1}\n        {0: -1, 1: 1, 2: -1, 3: -1}\n\n    \"\"\"\n    source_bqm = dimod.BinaryQuadraticModel.from_ising(source_h, source_J)\n    target_bqm = embed_bqm(source_bqm, embedding, target_adjacency, chain_strength=chain_strength)\n    target_h, target_J, __ = target_bqm.to_ising()\n    return target_h, target_J"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nembedding a QUBO onto a target graph.", "response": "def embed_qubo(source_Q, embedding, target_adjacency, chain_strength=1.0):\n    \"\"\"Embed a QUBO onto a target graph.\n\n    Args:\n        source_Q (dict[(variable, variable), bias]):\n            Coefficients of a quadratic unconstrained binary optimization (QUBO) model.\n\n        embedding (dict):\n            Mapping from source graph to target graph as a dict of form {s: {t, ...}, ...},\n            where s is a source-model variable and t is a target-model variable.\n\n        target_adjacency (dict/:class:`networkx.Graph`):\n            Adjacency of the target graph as a dict of form {t: Nt, ...},\n            where t is a target-graph variable and Nt is its set of neighbours.\n\n        chain_strength (float, optional):\n            Magnitude of the quadratic bias (in SPIN-space) applied between variables to form a chain. Note\n            that the energy penalty of chain breaks is 2 * `chain_strength`.\n\n    Returns:\n        dict[(variable, variable), bias]: Quadratic biases of the target QUBO.\n\n    Examples:\n        This example embeds a square source graph onto fully connected :math:`K_5` graph.\n        Embedding is accomplished by an edge deletion operation on the target graph: target-node\n        0 is not used.\n\n        >>> import dimod\n        >>> import networkx as nx\n        >>> # QUBO problem for a square graph\n        >>> Q = {(1, 1): -4.0, (1, 2): 4.0, (2, 2): -4.0, (2, 3): 4.0,\n        ...      (3, 3): -4.0, (3, 4): 4.0, (4, 1): 4.0, (4, 4): -4.0}\n        >>> # Target graph is a fully connected k5 graph\n        >>> K_5 = nx.complete_graph(5)\n        >>> 0 in K_5\n        True\n        >>> # Embedding from source to target graph\n        >>> embedding = {1: {4}, 2: {3}, 3: {1}, 4: {2}}\n        >>> # Embed the QUBO\n        >>> target_Q = dimod.embed_qubo(Q, embedding, K_5)\n        >>> (0, 0) in target_Q\n        False\n        >>> target_Q     # doctest: +SKIP\n        {(1, 1): -4.0,\n         (1, 2): 4.0,\n         (2, 2): -4.0,\n         (2, 4): 4.0,\n         (3, 1): 4.0,\n         (3, 3): -4.0,\n         (4, 3): 4.0,\n         (4, 4): -4.0}\n\n        This example embeds a square graph onto the target graph of a dimod reference structured\n        sampler, `StructureComposite`, using the dimod reference `ExactSolver` sampler with a\n        fully connected :math:`K_5` graph specified.\n\n        >>> import dimod\n        >>> import networkx as nx\n        >>> # QUBO problem for a square graph\n        >>> Q = {(1, 1): -4.0, (1, 2): 4.0, (2, 2): -4.0, (2, 3): 4.0,\n        ...      (3, 3): -4.0, (3, 4): 4.0, (4, 1): 4.0, (4, 4): -4.0}\n        >>> # Structured dimod sampler with a structure defined by a K5 graph\n        >>> sampler = dimod.StructureComposite(dimod.ExactSolver(), list(K_5.nodes), list(K_5.edges))\n        >>> sampler.adjacency      # doctest: +SKIP\n        {0: {1, 2, 3, 4},\n         1: {0, 2, 3, 4},\n         2: {0, 1, 3, 4},\n         3: {0, 1, 2, 4},\n         4: {0, 1, 2, 3}}\n        >>> # Embedding from source to target graph\n        >>> embedding = {0: [4], 1: [3], 2: [1], 3: [2], 4: [0]}\n        >>> # Embed the QUBO\n        >>> target_Q = dimod.embed_qubo(Q, embedding, sampler.adjacency)\n        >>> # Sample\n        >>> samples = sampler.sample_qubo(target_Q)\n        >>> for datum in samples.data():   # doctest: +SKIP\n        ...     print(datum)\n        ...\n        Sample(sample={1: 0, 2: 1, 3: 1, 4: 0}, energy=-8.0)\n        Sample(sample={1: 1, 2: 0, 3: 0, 4: 1}, energy=-8.0)\n        Sample(sample={1: 1, 2: 0, 3: 0, 4: 0}, energy=-4.0)\n        Sample(sample={1: 1, 2: 1, 3: 0, 4: 0}, energy=-4.0)\n        Sample(sample={1: 0, 2: 1, 3: 0, 4: 0}, energy=-4.0)\n        Sample(sample={1: 1, 2: 1, 3: 1, 4: 0}, energy=-4.0)\n        >>> # Snipped above samples for brevity\n\n    \"\"\"\n    source_bqm = dimod.BinaryQuadraticModel.from_qubo(source_Q)\n    target_bqm = embed_bqm(source_bqm, embedding, target_adjacency, chain_strength=chain_strength)\n    target_Q, __ = target_bqm.to_qubo()\n    return target_Q"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _adjacency_to_edges(adjacency):\n    edges = set()\n    for u in adjacency:\n        for v in adjacency[u]:\n            try:\n                edge = (u, v) if u <= v else (v, u)\n            except TypeError:\n                # Py3 does not allow sorting of unlike types\n                if (v, u) in edges:\n                    continue\n                edge = (u, v)\n\n            edges.add(edge)\n    return edges", "response": "determine from an adjacency the list of edges"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nembeds a single state by spreading it s values over the chains in the embedding", "response": "def _embed_state(embedding, state):\n    \"\"\"Embed a single state/sample by spreading it's values over the chains in the embedding\"\"\"\n    return {u: state[v] for v, chain in embedding.items() for u in chain}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parameters(self):\n\n        param = self.child.parameters.copy()\n        param['chain_strength'] = []\n        param['chain_break_fraction'] = []\n        return param", "response": "Return the parameters of the current sampler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(self, bqm, chain_strength=1.0, chain_break_fraction=True, **parameters):\n\n        # solve the problem on the child system\n        child = self.child\n\n        # apply the embedding to the given problem to map it to the child sampler\n        __, target_edgelist, target_adjacency = child.structure\n\n        # add self-loops to edgelist to handle singleton variables\n        source_edgelist = list(bqm.quadratic) + [(v, v) for v in bqm.linear]\n\n        # get the embedding\n        embedding = minorminer.find_embedding(source_edgelist, target_edgelist)\n\n        if bqm and not embedding:\n            raise ValueError(\"no embedding found\")\n\n        bqm_embedded = embed_bqm(bqm, embedding, target_adjacency,\n                                 chain_strength=chain_strength,\n                                 smear_vartype=dimod.SPIN)\n\n        if 'initial_state' in parameters:\n            parameters['initial_state'] = _embed_state(embedding, parameters['initial_state'])\n\n        response = child.sample(bqm_embedded, **parameters)\n\n        return unembed_sampleset(response, embedding, source_bqm=bqm,\n                                 chain_break_fraction=chain_break_fraction)", "response": "Sample from the provided binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(self, bqm, chain_strength=1.0, chain_break_fraction=True, **parameters):\n\n        # solve the problem on the child system\n        child = self.child\n\n        # apply the embedding to the given problem to map it to the child sampler\n        __, __, target_adjacency = child.structure\n\n        # get the embedding\n        embedding = self.embedding\n\n        bqm_embedded = embed_bqm(bqm, embedding, target_adjacency,\n                                 chain_strength=chain_strength,\n                                 smear_vartype=dimod.SPIN)\n\n        if 'initial_state' in parameters:\n            parameters['initial_state'] = _embed_state(embedding, parameters['initial_state'])\n\n        response = child.sample(bqm_embedded, **parameters)\n\n        return unembed_sampleset(response, embedding, source_bqm=bqm,\n                                 chain_break_fraction=chain_break_fraction)", "response": "Sample from the provided binary quadratic model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample(self, bqm, chain_strength=1.0, chain_break_fraction=True, **parameters):\n        if self.embedding is None:\n            # Find embedding\n            child = self.child   # Solve the problem on the child system\n            __, target_edgelist, target_adjacency = child.structure\n            source_edgelist = list(bqm.quadratic) + [(v, v) for v in bqm.linear]  # Add self-loops for single variables\n            embedding = minorminer.find_embedding(source_edgelist, target_edgelist)\n\n            # Initialize properties that need embedding\n            super(LazyFixedEmbeddingComposite, self)._set_graph_related_init(embedding=embedding)\n\n        return super(LazyFixedEmbeddingComposite, self).sample(bqm, chain_strength=chain_strength,\n                                                               chain_break_fraction=chain_break_fraction, **parameters)", "response": "Sample the binary quadratic model from the child sampler."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _accumulate_random(count, found, oldthing, newthing):\n    if randint(1, count + found) <= found:\n        return count + found, newthing\n    else:\n        return count + found, oldthing", "response": "This function performs on - line random selection of the object tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bulk_to_linear(M, N, L, qubits):\n    \"Converts a list of chimera coordinates to linear indices.\"\n    return [2 * L * N * x + 2 * L * y + L * u + k for x, y, u, k in qubits]", "response": "Converts a list of chimera coordinates to linear indices."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_linear(M, N, L, q):\n    \"Converts a qubit in chimera coordinates to its linear index.\"\n    (x, y, u, k) = q\n    return 2 * L * N * x + 2 * L * y + L * u + k", "response": "Converts a qubit in chimera coordinates to its linear index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bulk_to_chimera(M, N, L, qubits):\n    \"Converts a list of linear indices to chimera coordinates.\"\n    return [(q // N // L // 2, (q // L // 2) % N, (q // L) % 2, q % L) for q in qubits]", "response": "Converts a list of linear indices to chimera coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a qubit s linear index to chimera coordinates.", "response": "def _to_chimera(M, N, L, q):\n    \"Converts a qubit's linear index to chimera coordinates.\"\n    return (q // N // L // 2, (q // L // 2) % N, (q // L) % 2, q % L)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _chimera_neighbors(M, N, L, q):\n    \"Returns a list of neighbors of (x,y,u,k) in a perfect :math:`C_{M,N,L}`\"\n    (x, y, u, k) = q\n    n = [(x, y, 1 - u, l) for l in range(L)]\n    if u == 0:\n        if x:\n            n.append((x - 1, y, u, k))\n        if x < M - 1:\n            n.append((x + 1, y, u, k))\n    else:\n        if y:\n            n.append((x, y - 1, u, k))\n        if y < N - 1:\n            n.append((x, y + 1, u, k))\n    return n", "response": "Returns a list of neighbors of ( x y u k ) in a perfect : math : C_{M N L }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef random_processor(M, N, L, qubit_yield, num_evil=0):\n    # replacement for lambda in edge filter below that works with bot h\n    def edge_filter(pq):\n        # we have to unpack the (p,q) edge\n        p, q = pq\n        return q in qubits and p < q\n\n    qubits = [(x, y, u, k) for x in range(M) for y in range(N) for u in [0, 1] for k in range(L)]\n    nqubits = len(qubits)\n    qubits = set(sample(qubits, int(nqubits * qubit_yield)))\n    edges = ((p, q) for p in qubits for q in _chimera_neighbors(M, N, L, p))\n    edges = list(filter(edge_filter, edges))\n    possibly_evil_edges = [(p, q) for p, q in edges if p[:2] == q[:2]]\n    num_evil = min(num_evil, len(possibly_evil_edges))\n    evil_edges = sample(possibly_evil_edges, num_evil)\n    return processor(set(edges) - set(evil_edges), M=M, N=N, L=L, linear=False)", "response": "A utility function that generates a random chimera random set of missing some\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the number of unbroken paths of qubits at the given location.", "response": "def vline_score(self, x, ymin, ymax):\n        \"\"\"Returns the number of unbroken paths of qubits\n\n        >>> [(x,y,1,k) for y in range(ymin,ymax+1)]\n\n        for :math:`k = 0,1,\\cdots,L-1`.  This is precomputed for speed.\n        \"\"\"\n        return self._vline_score[x, ymin, ymax]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the number of unbroken paths of qubits at y", "response": "def hline_score(self, y, xmin, xmax):\n        \"\"\"Returns the number of unbroken paths of qubits\n\n        >>> [(x,y,0,k) for x in range(xmin,xmax+1)]\n\n        for :math:`k = 0,1,\\cdots,L-1`.  This is precomputed for speed.\n        \"\"\"\n        return self._hline_score[y, xmin, xmax]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the vline score for each entry in the cache.", "response": "def _compute_vline_scores(self):\n        \"\"\"Does the hard work to prepare ``vline_score``.\n        \"\"\"\n        M, N, L = self.M, self.N, self.L\n        vline_score = {}\n        for x in range(M):\n            laststart = [0 if (x, 0, 1, k) in self else None for k in range(L)]\n            for y in range(N):\n                block = [0] * (y + 1)\n                for k in range(L):\n                    if (x, y, 1, k) not in self:\n                        laststart[k] = None\n                    elif laststart[k] is None:\n                        laststart[k] = y\n                        block[y] += 1\n                    elif y and (x, y, 1, k) not in self[x, y - 1, 1, k]:\n                        laststart[k] = y\n                    else:\n                        for y1 in range(laststart[k], y + 1):\n                            block[y1] += 1\n                for y1 in range(y + 1):\n                    vline_score[x, y1, y] = block[y1]\n        self._vline_score = vline_score"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_hline_scores(self):\n        M, N, L = self.M, self.N, self.L\n        hline_score = {}\n        for y in range(N):\n            laststart = [0 if (0, y, 0, k) in self else None for k in range(L)]\n            for x in range(M):\n                block = [0] * (x + 1)\n                for k in range(L):\n                    if (x, y, 0, k) not in self:\n                        laststart[k] = None\n                    elif laststart[k] is None:\n                        laststart[k] = x\n                        block[x] += 1\n                    elif x and (x, y, 0, k) not in self[x - 1, y, 0, k]:\n                        laststart[k] = x\n                    else:\n                        for x1 in range(laststart[k], x + 1):\n                            block[x1] += 1\n                for x1 in range(x + 1):\n                    hline_score[y, x1, x] = block[x1]\n        self._hline_score = hline_score", "response": "Compute the hard work to prepare the hline score."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the biclique sizes for every rectangle contained in this ArcGIS instance.", "response": "def _compute_biclique_sizes(self, recompute=False):\n        \"\"\"Calls ``self.biclique_size(...)`` for every rectangle contained in this\n        processor, to fill the biclique size cache.\n\n        INPUTS:\n            recompute: if ``True``, then we dump the existing cache and compute\n                all biclique sizes from scratch.  (default: ``False``)\n        \"\"\"\n        if recompute or not self._biclique_size_computed:\n            self._biclique_size = {}\n            self._biclique_size_to_length = defaultdict(dict)\n            self._biclique_length_to_size = defaultdict(dict)\n        else:\n            return\n        M, N = self.M, self.N\n        for xmax in range(M):\n            for xmin in range(xmax + 1):\n                for ymax in range(N):\n                    for ymin in range(ymax + 1):\n                        ab = self.biclique_size(xmin, xmax, ymin, ymax)\n                        wh = xmax - xmin + 1, ymax - ymin + 1\n                        self._biclique_size_to_length[ab][\n                            wh] = (xmin, xmax, ymin, ymax)\n                        self._biclique_length_to_size[wh][\n                            ab] = (xmin, xmax, ymin, ymax)\n\n        self._biclique_size_computed = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the size parameters hscore and vscore of the complete bipartite graph.", "response": "def biclique_size(self, xmin, xmax, ymin, ymax):\n        \"\"\"Returns the size parameters ``(m,n)`` of the complete bipartite graph\n        :math:`K_{m,n}` comprised of ``m`` unbroken chains of horizontally-aligned qubits\n        and ``n`` unbroken chains of vertically-aligned qubits (known as line\n        bundles)\n\n        INPUTS:\n            xmin,xmax,ymin,ymax: integers defining the bounds of a rectangle\n            where we look for unbroken chains.  These ranges include both\n            endpoints.\n\n        OUTPUTS:\n            m,n: integers corresponding to the number of horizontal and\n                vertical line bundles contained in this rectangle.\n        \"\"\"\n        try:\n            return self._biclique_size[xmin, xmax, ymin, ymax]\n        except KeyError:\n            hscore = self.hline_score(ymin, xmin, xmax)\n            vscore = self.vline_score(xmin, ymin, ymax)\n            if ymin < ymax:\n                hscore += self.biclique_size(xmin, xmax, ymin + 1, ymax)[0]\n            if xmin < xmax:\n                vscore += self.biclique_size(xmin + 1, xmax, ymin, ymax)[1]\n            self._biclique_size[xmin, xmax, ymin, ymax] = hscore, vscore\n            return hscore, vscore"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a maximum - sized complete bipartite graph contained in the rectangle defined by xmin xmax ymin ymax.", "response": "def biclique(self, xmin, xmax, ymin, ymax):\n        \"\"\"Compute a maximum-sized complete bipartite graph contained in the\n        rectangle defined by ``xmin, xmax, ymin, ymax`` where each chain of\n        qubits is either a vertical line or a horizontal line.\n\n        INPUTS:\n            xmin,xmax,ymin,ymax: integers defining the bounds of a rectangle\n            where we look for unbroken chains.  These ranges include both\n            endpoints.\n\n        OUTPUT:\n            (A_side, B_side): a tuple of two lists containing lists of qubits.\n                the lists found in ``A_side`` and ``B_side`` are chains of qubits.\n                These lists of qubits are arranged so that\n\n                >>> [zip(chain,chain[1:]) for chain in A_side]\n\n                and\n\n                >>> [zip(chain,chain[1:]) for chain in B_side]\n\n                are lists of valid couplers.\n        \"\"\"\n\n        Aside = sum((self.maximum_hline_bundle(y, xmin, xmax)\n                     for y in range(ymin, ymax + 1)), [])\n        Bside = sum((self.maximum_vline_bundle(x, ymin, ymax)\n                     for x in range(xmin, xmax + 1)), [])\n        return Aside, Bside"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _contains_line(self, line):\n        return all(v in self for v in line) and all(u in self[v] for u, v in zip(line, line[1::]))", "response": "Test if a chain of qubits is completely contained in self."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute a maximum set of vertical lines in the unit cells x0 y0 y1.", "response": "def maximum_vline_bundle(self, x0, y0, y1):\n        \"\"\"Compute a maximum set of vertical lines in the unit cells ``(x0,y)``\n        for :math:`y0 \\leq y \\leq y1`.\n\n        INPUTS:\n            y0,x0,x1: int\n\n        OUTPUT:\n            list of lists of qubits\n        \"\"\"\n\n        y_range = range(y1, y0 - 1, -1) if y0 < y1 else range(y1, y0 + 1)\n        vlines = [[(x0, y, 1, k) for y in y_range] for k in range(self.L)]\n        return list(filter(self._contains_line, vlines))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing a maximum set of horizontal lines in the unit cells y0 x0 x1.", "response": "def maximum_hline_bundle(self, y0, x0, x1):\n        \"\"\"Compute a maximum set of horizontal lines in the unit cells ``(x,y0)``\n        for :math:`x0 \\leq x \\leq x1`.\n\n        INPUTS:\n            y0,x0,x1: int\n\n        OUTPUT:\n            list of lists of qubits\n        \"\"\"\n        x_range = range(x0, x1 + 1) if x0 < x1 else range(x0, x1 - 1, -1)\n        hlines = [[(x, y0, 0, k) for x in x_range] for k in range(self.L)]\n        return list(filter(self._contains_line, hlines))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef maximum_ell_bundle(self, ell):\n        (x0, x1, y0, y1) = ell\n        hlines = self.maximum_hline_bundle(y0, x0, x1)\n        vlines = self.maximum_vline_bundle(x0, y0, y1)\n\n        if self.random_bundles:\n            shuffle(hlines)\n            shuffle(vlines)\n\n        return [v + h for h, v in zip(hlines, vlines)]", "response": "Return a maximum ell bundle in the rectangle bounded by ell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _combine_clique_scores(self, rscore, hbar, vbar):\n        (y0, xmin, xmax) = hbar\n        (x0, ymin, ymax) = vbar\n        if rscore is None:\n            rscore = 0\n        hscore = self.hline_score(y0, xmin, xmax)\n        vscore = self.vline_score(x0, ymin, ymax)\n        if vscore < hscore:\n            score = rscore + vscore\n        else:\n            score = rscore + hscore\n        return score", "response": "Computes the score of a partial native clique embedding given the score hbar and vbar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maxCliqueWithRectangle(self, R, maxCWR):\n\n        (xmin, xmax, ymin, ymax) = R\n        best = nothing = 0, None, None, 1\n        bestscore = None\n        count = 0\n        N = self.N\n\n        Xlist = (xmin, xmax, xmin + 1, xmax), (xmax, xmin, xmin, xmax - 1)\n        Ylist = (ymin - 1, ymax, ymin - 1,\n                 ymax), (ymax + 1, ymin, ymin, ymax + 1)\n        XY = [(X, Y) for X in Xlist for Y in Ylist if 0 <= Y[2] <= Y[3] < N]\n\n        bests = []\n        for X, Y in XY:\n            x0, x1, nxmin, nxmax = X\n            y0, y1, nymin, nymax = Y\n            r = nxmin, nxmax, nymin, nymax\n            try:\n                rscore, rell, rparent, nr = maxCWR[r]\n            except:\n                rscore, nr = None, 1\n            score = self._combine_clique_scores(\n                rscore, (y0, xmin, xmax), (x0, nymin, nymax))\n            if bestscore is None or score > bestscore:\n                bestscore = score\n                count = 0\n            if score == bestscore:\n                count, best = _accumulate_random(\n                    count, nr, best, (score, (x0, x1, y0, y1), r, nr))\n\n        return bestscore, best", "response": "This function calculates the maximum of the native cliques with the given rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nativeCliqueEmbed(self, width):\n        maxCWR = {}\n\n        M, N = self.M, self.N\n        maxscore = None\n        count = 0\n        key = None\n        for w in range(width + 2):\n            h = width - w - 2\n            for ymin in range(N - h):\n                ymax = ymin + h\n                for xmin in range(M - w):\n                    xmax = xmin + w\n                    R = (xmin, xmax, ymin, ymax)\n                    score, best = self.maxCliqueWithRectangle(R, maxCWR)\n                    maxCWR[R] = best\n                    if maxscore is None or (score is not None and maxscore < score):\n                        maxscore = score\n                        key = None  # this gets overwritten immediately\n                        count = 0  # this gets overwritten immediately\n                    if maxscore == score:\n                        count, key = _accumulate_random(count, best[3], key, R)\n\n        clique = []\n        while key in maxCWR:\n            score, ell, key, num = maxCWR[key]\n            if ell is not None:\n                meb = self.maximum_ell_bundle(ell)\n                clique.extend(meb)\n        return maxscore, clique", "response": "Compute a maximum - sized native clique embedding in an induced Chimera with all chainlengths width + 1."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef largestNativeClique(self, max_chain_length=None):\n        bigclique = []\n        bestscore = None\n        if max_chain_length is None:\n            wmax = min(self.M, self.N)\n        else:\n            wmax = max_chain_length - 1\n\n        for w in range(wmax + 1):\n            score, clique = self.nativeCliqueEmbed(w)\n            if bestscore is None or score > bestscore:\n                bigclique = clique\n                bestscore = score\n        return bestscore, bigclique", "response": "Returns the largest native clique embedding we can find on the working\n            processor with the shortest chainlength possible."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a native embedding for the complete bipartite graph with the largest possible clique.", "response": "def largestNativeBiClique(self, chain_imbalance=0, max_chain_length=None):\n        \"\"\"Returns a native embedding for the complete bipartite graph :math:`K_{n,m}`\n        for :math:`n \\leq m`; where :math:`n` is as large as possible and :math:`m` is as large as\n        possible subject to :math:`n`.  The native embedding of a complete bipartite\n        graph is a set of horizontally-aligned qubits connected in lines\n        together with an equal-sized set of vertically-aligned qubits\n        connected in lines.\n\n        INPUTS:\n            chain_imbalance: how big of a difference to allow between the\n            chain lengths on the two sides of the bipartition. If ``None``,\n            then we allow an arbitrary imbalance.  (default: ``0``)\n\n            max_chain_length: longest chain length to consider or ``None`` if chain\n            lengths are allowed to be unbounded.  (default: ``None``)\n\n        OUTPUT:\n            score (tuple): the score for the returned clique (just ``(n,m)`` in the class\n            :class:`eden_processor`; may differ in subclasses)\n\n            embedding (tuple): a tuple of two lists containing lists of qubits.\n            If ``embedding = (A_side, B_side)``, the lists found in ``A_side`` and\n            ``B_side`` are chains of qubits.\n            These lists of qubits are arranged so that\n\n            >>> [zip(chain,chain[1:]) for chain in A_side]\n\n            and\n\n            >>> [zip(chain,chain[1:]) for chain in B_side]\n\n            are lists of valid couplers.\n\n        \"\"\"\n        self._compute_biclique_sizes()\n        Len2Siz = self._biclique_length_to_size\n        Siz2Len = self._biclique_size_to_length\n        overkill = self.M + self.N\n        if max_chain_length is None:\n            max_chain_length = overkill\n        if chain_imbalance is None:\n            chain_imbalance = overkill\n\n        def acceptable_chains(t):\n            a, b = t\n            return a <= max_chain_length and b <= max_chain_length and abs(a - b) <= chain_imbalance\n\n        def sortedpair(k):\n            return min(k), max(k)\n\n        feasible_sizes = {mn for mn, S in Siz2Len.items()\n                          if any(map(acceptable_chains, S))}\n        m, n = max(feasible_sizes, key=sortedpair)\n        best_r = None\n        best_ab = overkill, overkill\n        for mn in set(((m, n), (n, m))) & feasible_sizes:\n            for ab, r in Siz2Len[mn].items():\n                ab = max(ab), min(ab)\n                if acceptable_chains(ab) and ab < best_ab:\n                    best_ab = ab\n                    best_r = r\n\n        bestsize = sortedpair(self.biclique_size(*best_r))\n        bestbiclique = self.biclique(*best_r)\n        return bestsize, (bestbiclique[0], bestbiclique[1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the minimum edge covers of the set of evil edges.", "response": "def _compute_all_deletions(self):\n        \"\"\"Returns all minimal edge covers of the set of evil edges.\n        \"\"\"\n        minimum_evil = []\n        for disabled_qubits in map(set, product(*self._evil)):\n            newmin = []\n            for s in minimum_evil:\n                if s < disabled_qubits:\n                    break\n                elif disabled_qubits < s:\n                    continue\n                newmin.append(s)\n            else:\n                minimum_evil = newmin + [disabled_qubits]\n        return minimum_evil"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _subprocessor(self, disabled_qubits):\n        edgelist = [(p, q) for p, q in self._edgelist if\n                    p not in disabled_qubits and\n                    q not in disabled_qubits]\n        return eden_processor(edgelist, self.M, self.N, self.L, random_bundles=self._random_bundles)", "response": "Create a subprocessor by deleting a set of qubits."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing all possible deletion of a sets.", "response": "def _compute_deletions(self):\n        \"\"\"If there are fewer than self._proc_limit possible deletion\n        sets, compute all subprocessors obtained by deleting a\n        minimal subset of qubits.\n        \"\"\"\n        M, N, L, edgelist = self.M, self.N, self.L, self._edgelist\n        if 2**len(self._evil) <= self._proc_limit:\n            deletions = self._compute_all_deletions()\n            self._processors = [self._subprocessor(d) for d in deletions]\n        else:\n            self._processors = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a random subprocessor where there is a coupler between every pair of working qubits on opposite sides of the same cell and every pair of working qubits on opposite sides of the same cell. Returns a new instance of eden_processor.", "response": "def _random_subprocessor(self):\n        \"\"\"Creates a random subprocessor where there is a coupler between\n        every pair of working qubits on opposite sides of the same cell.\n        This is guaranteed to be minimal in that adding a qubit back in\n        will reintroduce a bad coupler, but not to have minimum size.\n\n        OUTPUT:\n            an :class:`eden_processor` instance\n        \"\"\"\n        deletion = set()\n        for e in self._evil:\n            if e[0] in deletion or e[1] in deletion:\n                continue\n            deletion.add(choice(e))\n        return self._subprocessor(deletion)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nproduces an iterator of eden_processor instances.", "response": "def _random_subprocessors(self):\n        \"\"\"Produces an iterator of subprocessors.  If there are fewer than\n        self._proc_limit subprocessors to consider (by knocking out a\n        minimal subset of working qubits incident to broken couplers),\n        we work exhaustively.  Otherwise, we generate a random set of\n        ``self._proc_limit`` subprocessors.\n\n        If the total number of possibilities is rather small, then we\n        deliberately pick a random minimum subset to avoid coincidences.\n        Otherwise, we give up on minimum, satisfy ourselves with minimal,\n        and randomly generate subprocessors with :func:`self._random_subprocessor`.\n\n        OUTPUT:\n            an iterator of eden_processor instances.\n        \"\"\"\n        if self._processors is not None:\n            return (p for p in self._processors)\n        elif 2**len(self._evil) <= 8 * self._proc_limit:\n            deletions = self._compute_all_deletions()\n            if len(deletions) > self._proc_limit:\n                deletions = sample(deletions, self._proc_limit)\n            return (self._subprocessor(d) for d in deletions)\n        else:\n            return (self._random_subprocessor() for i in range(self._proc_limit))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping a function to a list of processors and return the output that satisfies a transitive objective function.", "response": "def _map_to_processors(self, f, objective):\n        \"\"\"Map a function to a list of processors, and return the output that\n        best satisfies a transitive objective function.  The list of\n        processors will differ according to the number of evil qubits and\n        :func:`_proc_limit`, see details in :func:`self._random_subprocessors`.\n\n        INPUT:\n            f (callable): the function to call on each processor\n\n            objective (callable): a function where objective(x,y) is True if x is\n                better than y, and False otherwise.  Assumes transitivity!\n\n        OUTPUT:\n            best: the object returned by f that maximizes the objective.\n        \"\"\"\n\n        P = self._random_subprocessors()\n        best = f(next(P))\n        for p in P:\n            x = f(p)\n            if objective(best, x):\n                best = x\n        return best[1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _objective_bestscore(self, old, new):\n        (oldscore, oldthing) = old\n        (newscore, newthing) = new\n        if oldscore is None:\n            return True\n        if newscore is None:\n            return False\n        return oldscore < newscore", "response": "An objective function that returns True if new has a better score than old and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the largest native clique embedding we can find on the processor.", "response": "def largestNativeClique(self, max_chain_length=None):\n        \"\"\"Returns the largest native clique embedding we can find on the\n        processor, with the shortest chainlength possible (for that clique\n        size).  If possible, returns a uniform choice among all largest\n        cliques.\n\n        INPUTS:\n            max_chain_length (int): longest chain length to consider or ``None`` if chain\n            lengths are allowed to be unbounded.  (default: ``None``)\n\n        OUTPUT:\n            clique (list): a list containing lists of qubits, each associated to a\n            chain.  These lists of qubits are carefully arranged so that\n\n            >>> [zip(chain,chain[1:]) for chain in clique]\n\n            is a list of valid couplers.\n\n        Note: this fails to return a uniform choice if there are broken\n        intra-cell couplers between working qubits. (the choice is\n        uniform on a particular subprocessor)\n        \"\"\"\n        def f(x):\n            return x.largestNativeClique(max_chain_length=max_chain_length)\n        objective = self._objective_bestscore\n        return self._translate(self._map_to_processors(f, objective))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes a maximum - sized native clique embedding in an induced subgraph of chimera with chainsize width + 1.", "response": "def nativeCliqueEmbed(self, width):\n        \"\"\"Compute a maximum-sized native clique embedding in an induced\n        subgraph of chimera with chainsize ``width+1``.  If possible,\n        returns a uniform choice among all largest cliques.\n\n        INPUTS:\n            width: width of the squares to search, also `chainlength-1`\n\n        OUTPUT:\n            clique: a list containing lists of qubits, each associated\n            to a chain.  These lists of qubits are carefully\n            arranged so that\n\n            >>> [zip(chain,chain[1:]) for chain in clique]\n\n            is a list of valid couplers.\n\n        Note: this fails to return a uniform choice if there are broken\n        intra-cell couplers between working qubits. (the choice is\n        uniform on a particular subprocessor)\n        \"\"\"\n        def f(x):\n            return x.nativeCliqueEmbed(width)\n        objective = self._objective_bestscore\n        return self._translate(self._map_to_processors(f, objective))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a native embedding for the complete bipartite graph with the largest valid couplers.", "response": "def largestNativeBiClique(self, chain_imbalance=0, max_chain_length=None):\n        \"\"\"Returns a native embedding for the complete bipartite graph :math:`K_{n,m}`\n        for `n <= m`; where `n` is as large as possible and `m` is as large as\n        possible subject to `n`.  The native embedding of a complete bipartite\n        graph is a set of horizontally-aligned qubits connected in lines\n        together with an equal-sized set of vertically-aligned qubits\n        connected in lines.\n\n        INPUTS:\n            chain_imbalance: how big of a difference to allow between the\n            chain lengths on the two sides of the bipartition. If ``None``,\n            then we allow an arbitrary imbalance.  (default: ``0``)\n\n            max_chain_length: longest chain length to consider or None if chain\n            lengths are allowed to be unbounded.  (default: ``None``)\n\n        OUTPUT:\n            embedding (tuple): a tuple of two lists containing lists of qubits.\n            If ``embedding = (A_side, B_side)``, the lists found in ``A_side`` and\n            ``B_side`` are chains of qubits. These lists of qubits are arranged so that\n\n            >>> [zip(chain,chain[1:]) for chain in A_side]\n\n            and\n\n            >>> [zip(chain,chain[1:]) for chain in B_side]\n\n            are lists of valid couplers.\n        \"\"\"\n        def f(x):\n            return x.largestNativeBiClique(chain_imbalance=chain_imbalance,\n                                           max_chain_length=max_chain_length)\n        objective = self._objective_bestscore\n        emb = self._map_to_processors(f, objective)\n        return self._translate_partitioned(emb)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _translate(self, embedding):\n        \"Translates an embedding back to linear coordinates if necessary.\"\n        if embedding is None:\n            return None\n        if not self._linear:\n            return embedding\n        return [_bulk_to_linear(self.M, self.N, self.L, chain) for chain in embedding]", "response": "Translates an embedding back to linear coordinates if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_chain_strength(sampler, chain_strength):\n    properties = sampler.properties\n\n    if 'extended_j_range' in properties:\n        max_chain_strength = - min(properties['extended_j_range'])\n    elif 'j_range' in properties:\n        max_chain_strength = - min(properties['j_range'])\n    else:\n        raise ValueError(\"input sampler should have 'j_range' and/or 'extended_j_range' property.\")\n\n    if chain_strength is None:\n        chain_strength = max_chain_strength\n    elif chain_strength > max_chain_strength:\n        raise ValueError(\"Provided chain strength exceedds the allowed range.\")\n\n    return chain_strength", "response": "Validate the provided chain strength."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sample(self, bqm, apply_flux_bias_offsets=True, **kwargs):\n        child = self.child\n\n        if apply_flux_bias_offsets:\n            if self.flux_biases is not None:\n                kwargs[FLUX_BIAS_KWARG] = self.flux_biases\n\n        return child.sample(bqm, **kwargs)", "response": "Sample from the given Ising model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_flux_biases(sampler, embedding, chain_strength, num_reads=1000, max_age=3600):\n\n    if not isinstance(sampler, dimod.Sampler):\n        raise TypeError(\"input sampler should be DWaveSampler\")\n\n    # try to read the chip_id, otherwise get the name\n    system_name = sampler.properties.get('chip_id', str(sampler.__class__))\n\n    try:\n        with cache_connect() as cur:\n            fbo = get_flux_biases_from_cache(cur, embedding.values(), system_name,\n                                             chain_strength=chain_strength,\n                                             max_age=max_age)\n        return fbo\n    except MissingFluxBias:\n        pass\n\n    # if dwave-drivers is not available, then we can't calculate the biases\n    try:\n        import dwave.drivers as drivers\n    except ImportError:\n        msg = (\"dwave-drivers not found, cannot calculate flux biases. dwave-drivers can be \"\n               \"installed with \"\n               \"'pip install dwave-drivers --extra-index-url https://pypi.dwavesys.com/simple'. \"\n               \"See documentation for dwave-drivers license.\")\n        raise RuntimeError(msg)\n\n    fbo = drivers.oneshot_flux_bias(sampler, embedding.values(), num_reads=num_reads,\n                                    chain_strength=chain_strength)\n\n    # store them in the cache\n    with cache_connect() as cur:\n        for chain in embedding.values():\n            v = next(iter(chain))\n            flux_bias = fbo.get(v, 0.0)\n            insert_flux_bias(cur, chain, system_name, flux_bias, chain_strength)\n\n    return fbo", "response": "Get the flux bias offsets for a sampler and embedding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_clique_embedding(k, m, n=None, t=None, target_edges=None):\n    import random\n\n    _, nodes = k\n\n    m, n, t, target_edges = _chimera_input(m, n, t, target_edges)\n\n    # Special cases to return optimal embeddings for small k.  The general clique embedder uses chains of length\n    # at least 2, whereas cliques of size 1 and 2 can be embedded with single-qubit chains.\n\n    if len(nodes) == 1:\n        # If k == 1 we simply return a single chain consisting of a randomly sampled qubit.\n\n        qubits = set().union(*target_edges)\n        qubit = random.choice(tuple(qubits))\n        embedding = [[qubit]]\n\n    elif len(nodes) == 2:\n        # If k == 2 we simply return two one-qubit chains that are the endpoints of a randomly sampled coupler.\n\n        if not isinstance(target_edges, list):\n            edges = list(target_edges)\n        edge = edges[random.randrange(len(edges))]\n        embedding = [[edge[0]], [edge[1]]]\n\n    else:\n        # General case for k > 2.\n\n        embedding = processor(target_edges, M=m, N=n, L=t).tightestNativeClique(len(nodes))\n\n    if not embedding:\n        raise ValueError(\"cannot find a K{} embedding for given Chimera lattice\".format(k))\n\n    return dict(zip(nodes, embedding))", "response": "Given a target Chimera graph size and a clique in a Chimera graph returns an embedding for a clique in a Chimera graph."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a target Chimera graph size and a biclique and a target Chimera graph size and a target Chimera graph size and a target Chimera graph size and a target Chimera graph size and a target Chimera graph size and a target set of edges returns an embedding for that biclique.", "response": "def find_biclique_embedding(a, b, m, n=None, t=None, target_edges=None):\n    \"\"\"Find an embedding for a biclique in a Chimera graph.\n\n    Given a target :term:`Chimera` graph size, and a biclique (a bipartite graph where every\n    vertex in a set in connected to all vertices in the other set), attempts to find an embedding.\n\n    Args:\n        a (int/iterable):\n            Left shore of the biclique to embed. If a is an integer, generates an embedding\n            for a biclique with the left shore of size a labelled [0,a-1].\n            If a is an iterable, generates an embedding for a biclique with the left shore of size\n            len(a), where iterable a is the variable labels.\n\n        b (int/iterable):\n            Right shore of the biclique to embed.If b is an integer, generates an embedding\n            for a biclique with the right shore of size b labelled [0,b-1].\n            If b is an iterable, generates an embedding for a biclique with the right shore of\n            size len(b), where iterable b provides the variable labels.\n\n        m (int):\n            Number of rows in the Chimera lattice.\n\n        n (int, optional, default=m):\n            Number of columns in the Chimera lattice.\n\n        t (int, optional, default 4):\n            Size of the shore within each Chimera tile.\n\n        target_edges (iterable[edge]):\n            A list of edges in the target Chimera graph. Nodes are labelled as\n            returned by :func:`~dwave_networkx.generators.chimera_graph`.\n\n    Returns:\n        tuple: A 2-tuple containing:\n\n            dict: An embedding mapping the left shore of the biclique to the Chimera lattice.\n\n            dict: An embedding mapping the right shore of the biclique to the Chimera lattice\n\n    Examples:\n        This example finds an embedding for an alphanumerically labeled biclique in a single\n        Chimera unit cell.\n\n        >>> from dwave.embedding.chimera import find_biclique_embedding\n        ...\n        >>> left, right = find_biclique_embedding(['a', 'b', 'c'], ['d', 'e'], 1, 1)\n        >>> print(left, right)  # doctest: +SKIP\n        {'a': [4], 'b': [5], 'c': [6]} {'d': [0], 'e': [1]}\n\n    \"\"\"\n    _, anodes = a\n    _, bnodes = b\n\n    m, n, t, target_edges = _chimera_input(m, n, t, target_edges)\n    embedding = processor(target_edges, M=m, N=n, L=t).tightestNativeBiClique(len(anodes), len(bnodes))\n\n    if not embedding:\n        raise ValueError(\"cannot find a K{},{} embedding for given Chimera lattice\".format(a, b))\n\n    left, right = embedding\n    return dict(zip(anodes, left)), dict(zip(bnodes, right))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_grid_embedding(dim, m, n=None, t=4):\n\n    m, n, t, target_edges = _chimera_input(m, n, t, None)\n    indexer = dnx.generators.chimera.chimera_coordinates(m, n, t)\n\n    dim = list(dim)\n    num_dim = len(dim)\n    if num_dim == 1:\n        def _key(row, col, aisle): return row\n        dim.extend([1, 1])\n    elif num_dim == 2:\n        def _key(row, col, aisle): return row, col\n        dim.append(1)\n    elif num_dim == 3:\n        def _key(row, col, aisle): return row, col, aisle\n    else:\n        raise ValueError(\"find_grid_embedding supports between one and three dimensions\")\n\n    rows, cols, aisles = dim\n    if rows > m or cols > n or aisles > t:\n        msg = (\"the largest grid that find_grid_embedding can fit in a ({}, {}, {}) Chimera-lattice \"\n               \"is {}x{}x{}; given grid is {}x{}x{}\").format(m, n, t, m, n, t, rows, cols, aisles)\n        raise ValueError(msg)\n\n    return {_key(row, col, aisle): [indexer.int((row, col, 0, aisle)), indexer.int((row, col, 1, aisle))]\n            for row in range(dim[0]) for col in range(dim[1]) for aisle in range(dim[2])}", "response": "Finds an embedding for a grid in a Chimera graph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cache_file(app_name=APPNAME, app_author=APPAUTHOR, filename=DATABASENAME):\n    user_data_dir = homebase.user_data_dir(app_name=app_name, app_author=app_author, create=True)\n    return os.path.join(user_data_dir, filename)", "response": "Returns the full path to the data cache file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _restore_isolated(sampleset, bqm, isolated):\n\n    samples = sampleset.record.sample\n    variables = sampleset.variables\n\n    new_samples = np.empty((len(sampleset), len(isolated)), dtype=samples.dtype)\n\n    # we don't let the isolated variables interact with each other for now because\n    # it will slow this down substantially\n    for col, v in enumerate(isolated):\n        try:\n            neighbours, biases = zip(*((u, bias) for u, bias in bqm.adj[v].items()\n                                       if u in variables))  # ignore other isolates\n        except ValueError:\n            # happens when only neighbors are other isolated variables\n            new_samples[:, col] = bqm.linear[v] <= 0\n            continue\n\n        idxs = [variables.index[u] for u in neighbours]\n\n        # figure out which value for v would minimize the energy\n        # v(h_v + \\sum_u J_uv * u)\n        new_samples[:, col] = samples[:, idxs].dot(biases) < -bqm.linear[v]\n\n    if bqm.vartype is dimod.SPIN:\n        new_samples = 2*new_samples - 1\n\n    return np.concatenate((samples, new_samples), axis=1), list(variables) + isolated", "response": "Return samples - like by adding isolated variables into sampleset in a\n    way that minimizes the energy relative to other non - isolated variables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _restore_isolated_higherorder(sampleset, poly, isolated):\n\n    samples = sampleset.record.sample\n    variables = sampleset.variables\n\n    new_samples = np.empty((len(sampleset), len(isolated)), dtype=samples.dtype)\n\n    # we don't let the isolated variables interact with eachother for now because\n    # it will slow this down substantially\n    isolated_energies = {v: 0 for v in isolated}\n    for term, bias in poly.items():\n\n        isolated_components = term.intersection(isolated)\n\n        if not isolated_components:\n            continue\n\n        en = bias  # energy contribution of the term\n        for v in term:\n            if v in isolated_energies:\n                continue\n            en *= samples[:, sampleset.variables.index(v)]\n\n        for v in isolated_components:\n            isolated_energies[v] += en\n\n    # now put those energies into new_samples\n    for col, v in enumerate(isolated):\n        new_samples[:, col] = isolated_energies[v] < 0\n\n    if poly.vartype is dimod.SPIN:\n        new_samples = 2*new_samples - 1\n\n    return np.concatenate((samples, new_samples), axis=1), list(variables) + isolated", "response": "Return samples - like by adding isolated variables into sampleset in a\n    way that minimizes the energy relative to the other non - isolated variables."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample_poly(self, poly, **kwargs):\n        child = self.child\n        cutoff = self._cutoff\n        cutoff_vartype = self._cutoff_vartype\n        comp = self._comparison\n\n        if cutoff_vartype is dimod.SPIN:\n            original = poly.to_spin(copy=False)\n        else:\n            original = poly.to_binary(copy=False)\n\n        # remove all of the terms of order >= 2 that have a bias less than cutoff\n        new = type(poly)(((term, bias) for term, bias in original.items()\n                          if len(term) > 1 and not comp(abs(bias), cutoff)),\n                         cutoff_vartype)\n\n        # also include the linear biases for the variables in new\n        for v in new.variables:\n            term = v,\n            if term in original:\n                new[term] = original[term]\n\n        # everything else is isolated\n        isolated = list(original.variables.difference(new.variables))\n\n        if isolated and len(new) == 0:\n            # in this case all variables are isolated, so we just put one back\n            # to serve as the basis\n            term = isolated.pop(),\n            new[term] = original[term]\n\n        # get the samples from the child sampler and put them into the original vartype\n        sampleset = child.sample_poly(new, **kwargs).change_vartype(poly.vartype, inplace=True)\n\n        # we now need to add the isolated back in, in a way that minimizes\n        # the energy. There are lots of ways to do this but for now we'll just\n        # do one\n        if isolated:\n            samples, variables = _restore_isolated_higherorder(sampleset, poly, isolated)\n        else:\n            samples = sampleset.record.sample\n            variables = sampleset.variables\n\n        vectors = sampleset.data_vectors\n        vectors.pop('energy')  # we're going to recalculate the energy anyway\n\n        return dimod.SampleSet.from_samples_bqm((samples, variables), poly, **vectors)", "response": "This method is used to sample from the provided binary polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef diagnose_embedding(emb, source, target):\n\n    if not hasattr(source, 'edges'):\n        source = nx.Graph(source)\n    if not hasattr(target, 'edges'):\n        target = nx.Graph(target)\n\n    label = {}\n    embedded = set()\n    for x in source:\n        try:\n            embx = emb[x]\n            missing_chain = len(embx) == 0\n        except KeyError:\n            missing_chain = True\n        if missing_chain:\n            yield MissingChainError, x\n            continue\n        all_present = True\n        for q in embx:\n            if label.get(q, x) != x:\n                yield ChainOverlapError, q, x, label[q]\n            elif q not in target:\n                all_present = False\n                yield InvalidNodeError, x, q\n            else:\n                label[q] = x\n        if all_present:\n            embedded.add(x)\n            if not nx.is_connected(target.subgraph(embx)):\n                yield DisconnectedChainError, x\n\n    yielded = nx.Graph()\n    for p, q in target.subgraph(label).edges():\n        yielded.add_edge(label[p], label[q])\n    for x, y in source.edges():\n        if x == y:\n            continue\n        if x in embedded and y in embedded and not yielded.has_edge(x, y):\n            yield MissingEdgeError, x, y", "response": "A detailed diagnostic for minor embeddings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef verify_embedding(emb, source, target, ignore_errors=()):\n\n    for error in diagnose_embedding(emb, source, target):\n        eclass = error[0]\n        if eclass not in ignore_errors:\n            raise eclass(*error[1:])\n    return True", "response": "Verify that an embedding of source and target nodes is correct."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve_object(self, object_arg_name, resolver):\n        def decorator(func_or_class):\n            if isinstance(func_or_class, type):\n                # Handle Resource classes decoration\n                # pylint: disable=protected-access\n                func_or_class._apply_decorator_to_methods(decorator)\n                return func_or_class\n\n            @wraps(func_or_class)\n            def wrapper(*args, **kwargs):\n                kwargs[object_arg_name] = resolver(kwargs)\n                return func_or_class(*args, **kwargs)\n            return wrapper\n        return decorator", "response": "A decorator to resolve object instance from arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef model(self, name=None, model=None, mask=None, **kwargs):\n        if isinstance(model, (flask_marshmallow.Schema, flask_marshmallow.base_fields.FieldABC)):\n            if not name:\n                name = model.__class__.__name__\n            api_model = Model(name, model, mask=mask)\n            api_model.__apidoc__ = kwargs\n            return self.add_model(name, api_model)\n        return super(Namespace, self).model(name=name, model=model, **kwargs)", "response": "Register a new resource with the specified name and model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _apply_decorator_to_methods(cls, decorator):\n        for method in cls.methods:\n            method_name = method.lower()\n            decorated_method_func = decorator(getattr(cls, method_name))\n            setattr(cls, method_name, decorated_method_func)", "response": "This helper applies a given decorator to all methods on the current\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef options(self, *args, **kwargs):\n        # This is a generic implementation of OPTIONS method for resources.\n        # This method checks every permissions provided as decorators for other\n        # methods to provide information about what methods `current_user` can\n        # use.\n        method_funcs = [getattr(self, m.lower()) for m in self.methods]\n        allowed_methods = []\n        request_oauth_backup = getattr(flask.request, 'oauth', None)\n        for method_func in method_funcs:\n            if getattr(method_func, '_access_restriction_decorators', None):\n                if not hasattr(method_func, '_cached_fake_method_func'):\n                    fake_method_func = lambda *args, **kwargs: True\n                    # `__name__` is used in `login_required` decorator, so it\n                    # is required to fake this also\n                    fake_method_func.__name__ = 'options'\n\n                    # Decorate the fake method with the registered access\n                    # restriction decorators\n                    for decorator in method_func._access_restriction_decorators:\n                        fake_method_func = decorator(fake_method_func)\n\n                    # Cache the `fake_method_func` to avoid redoing this over\n                    # and over again\n                    method_func.__dict__['_cached_fake_method_func'] = fake_method_func\n                else:\n                    fake_method_func = method_func._cached_fake_method_func\n\n                flask.request.oauth = None\n                try:\n                    fake_method_func(self, *args, **kwargs)\n                except HTTPException:\n                    # This method is not allowed, so skip it\n                    continue\n\n            allowed_methods.append(method_func.__name__.upper())\n        flask.request.oauth = request_oauth_backup\n\n        return flask.Response(\n            status=HTTPStatus.NO_CONTENT,\n            headers={'Allow': \", \".join(allowed_methods)}\n        )", "response": "This method is used to fake the OPTIONS method for all methods in this endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_patch_structure(self, data):\n        if data['op'] not in self.NO_VALUE_OPERATIONS and 'value' not in data:\n            raise ValidationError('value is required')\n\n        if 'path' not in data:\n            raise ValidationError('Path is required and must always begin with /')\n        else:\n            data['field_name'] = data['path'][1:]", "response": "Validate the PATCH structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef perform_patch(cls, operations, obj, state=None):\n        if state is None:\n            state = {}\n        for operation in operations:\n            if not cls._process_patch_operation(operation, obj=obj, state=state):\n                log.info(\n                    \"%s patching has been stopped because of unknown operation %s\",\n                    obj.__class__.__name__,\n                    operation\n                )\n                raise ValidationError(\n                    \"Failed to update %s details. Operation %s could not succeed.\" % (\n                        obj.__class__.__name__,\n                        operation\n                    )\n                )\n        return True", "response": "Perform all necessary operations by calling class methods with their corresponding names."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_patch_operation(cls, operation, obj, state):\n        field_operaion = operation['op']\n\n        if field_operaion == cls.OP_REPLACE:\n            return cls.replace(obj, operation['field_name'], operation['value'], state=state)\n\n        elif field_operaion == cls.OP_TEST:\n            return cls.test(obj, operation['field_name'], operation['value'], state=state)\n\n        elif field_operaion == cls.OP_ADD:\n            return cls.add(obj, operation['field_name'], operation['value'], state=state)\n\n        elif field_operaion == cls.OP_MOVE:\n            return cls.move(obj, operation['field_name'], operation['value'], state=state)\n\n        elif field_operaion == cls.OP_COPY:\n            return cls.copy(obj, operation['field_name'], operation['value'], state=state)\n\n        elif field_operaion == cls.OP_REMOVE:\n            return cls.remove(obj, operation['field_name'], state=state)\n\n        return False", "response": "Process an inter - operation in RFC 6902 format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef replace(cls, obj, field, value, state):\n        if not hasattr(obj, field):\n            raise ValidationError(\"Field '%s' does not exist, so it cannot be patched\" % field)\n        setattr(obj, field, value)\n        return True", "response": "This method replaces the value of a field in an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_identities(self, item):\n\n        # All identities are in the post stream\n        # The first post is the question. Next replies\n\n        posts = item['data']['post_stream']['posts']\n\n        for post in posts:\n            user = self.get_sh_identity(post)\n            yield user", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __related_categories(self, category_id):\n        related = []\n        for cat in self.categories_tree:\n            if category_id in self.categories_tree[cat]:\n                related.append(self.categories[cat])\n        return related", "response": "Get all related categories to a given one"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing the category tree", "response": "def __show_categories_tree(self):\n        \"\"\" Show the category tree: list of categories and its subcategories \"\"\"\n        for cat in self.categories_tree:\n            print(\"%s (%i)\" % (self.categories[cat], cat))\n            for subcat in self.categories_tree[cat]:\n                print(\"-> %s (%i)\" % (self.categories[subcat], subcat))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch_track_items(upstream_file_url, data_source):\n\n    track_uris = []\n    req = requests_ses.get(upstream_file_url)\n    try:\n        req.raise_for_status()\n    except requests.exceptions.HTTPError as ex:\n        logger.warning(\"Can't get gerrit reviews from %s\", upstream_file_url)\n        logger.warning(ex)\n        return track_uris\n\n    logger.debug(\"Found reviews to be tracked in %s\", upstream_file_url)\n\n    lines = iter(req.text.split(\"\\n\"))\n    for line in lines:\n        if 'url: ' in line:\n            dso = next(lines).split('system: ')[1].strip('\\n')\n            if dso == data_source:\n                track_uris.append(line.split('url: ')[1].strip('\\n'))\n    return track_uris", "response": "Fetch the list of track_uris from the upstream file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a projects file from the items origin data", "response": "def _create_projects_file(project_name, data_source, items):\n    \"\"\" Create a projects file from the items origin data \"\"\"\n\n    repositories = []\n    for item in items:\n        if item['origin'] not in repositories:\n            repositories.append(item['origin'])\n    projects = {\n        project_name: {\n            data_source: repositories\n        }\n    }\n\n    projects_file, projects_file_path = tempfile.mkstemp(prefix='track_items_')\n\n    with open(projects_file_path, \"w\") as pfile:\n        json.dump(projects, pfile, indent=True)\n\n    return projects_file_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __convert_booleans(self, eitem):\n\n        for field in eitem.keys():\n            if isinstance(eitem[field], bool):\n                if eitem[field]:\n                    eitem[field] = 1\n                else:\n                    eitem[field] = 0\n        return eitem", "response": "Convert True or False to 1 / 0 for better kibana processing"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef enrich_items(self, ocean_backend, events=False):\n\n        max_items = self.elastic.max_items_bulk\n        current = 0\n        total = 0\n        bulk_json = \"\"\n\n        items = ocean_backend.fetch()\n        images_items = {}\n\n        url = self.elastic.index_url + '/items/_bulk'\n\n        logger.debug(\"Adding items to %s (in %i packs)\", self.elastic.anonymize_url(url), max_items)\n\n        for item in items:\n            if current >= max_items:\n                total += self.elastic.safe_put_bulk(url, bulk_json)\n                json_size = sys.getsizeof(bulk_json) / (1024 * 1024)\n                logger.debug(\"Added %i items to %s (%0.2f MB)\", total, self.elastic.anonymize_url(url), json_size)\n                bulk_json = \"\"\n                current = 0\n\n            rich_item = self.get_rich_item(item)\n            data_json = json.dumps(rich_item)\n            bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % \\\n                (item[self.get_field_unique_id()])\n            bulk_json += data_json + \"\\n\"  # Bulk document\n            current += 1\n\n            if rich_item['id'] not in images_items:\n                # Let's transform the rich_event in a rich_image\n                rich_item['is_docker_image'] = 1\n                rich_item['is_event'] = 0\n                images_items[rich_item['id']] = rich_item\n            else:\n                image_date = images_items[rich_item['id']]['last_updated']\n                if image_date <= rich_item['last_updated']:\n                    # This event is newer for the image\n                    rich_item['is_docker_image'] = 1\n                    rich_item['is_event'] = 0\n                    images_items[rich_item['id']] = rich_item\n\n        if current > 0:\n            total += self.elastic.safe_put_bulk(url, bulk_json)\n\n        if total == 0:\n            # No items enriched, nothing to upload to ES\n            return total\n\n        # Time to upload the images enriched items. The id is uuid+\"_image\"\n        # Normally we are enriching events for a unique image so all images\n        # data can be upload in one query\n        for image in images_items:\n            data = images_items[image]\n            data_json = json.dumps(data)\n            bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % \\\n                (data['id'] + \"_image\")\n            bulk_json += data_json + \"\\n\"  # Bulk document\n\n        total += self.elastic.safe_put_bulk(url, bulk_json)\n        return total", "response": "This method adds items to the index_url and adds them to the index_url."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_params_parser():\n\n    parser = argparse.ArgumentParser()\n\n    ElasticOcean.add_params(parser)\n\n    parser.add_argument('-g', '--debug', dest='debug', action='store_true')\n    parser.add_argument('-t', '--token', dest='token', help=\"GitHub token\")\n    parser.add_argument('-o', '--org', dest='org', help='GitHub Organization to be analyzed')\n    parser.add_argument('-c', '--contact', dest='contact', help='Contact (mail) to notify events.')\n    parser.add_argument('--twitter', dest='twitter', help='Twitter account to notify.')\n    parser.add_argument('-w', '--web-dir', default='/var/www/cauldron/dashboards', dest='web_dir',\n                        help='Redirect HTML project pages for accessing Kibana dashboards.')\n    parser.add_argument('-k', '--kibana-url', default='https://dashboard.cauldron.io', dest='kibana_url',\n                        help='Kibana URL.')\n    parser.add_argument('-u', '--graas-url', default='https://cauldron.io', dest='graas_url',\n                        help='GraaS service URL.')\n    parser.add_argument('-n', '--nrepos', dest='nrepos', type=int, default=NREPOS,\n                        help='Number of GitHub repositories from the Organization to be analyzed (default:10)')\n\n    return parser", "response": "Parse command line arguments and return a parser."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_owner_repos_url(owner, token):\n    url_org = GITHUB_API_URL + \"/orgs/\" + owner + \"/repos\"\n    url_user = GITHUB_API_URL + \"/users/\" + owner + \"/repos\"\n\n    url_owner = url_org  # Use org by default\n\n    try:\n        r = requests.get(url_org,\n                         params=get_payload(),\n                         headers=get_headers(token))\n        r.raise_for_status()\n\n    except requests.exceptions.HTTPError as e:\n        if r.status_code == 403:\n            rate_limit_reset_ts = datetime.fromtimestamp(int(r.headers['X-RateLimit-Reset']))\n            seconds_to_reset = (rate_limit_reset_ts - datetime.utcnow()).seconds + 1\n            logging.info(\"GitHub rate limit exhausted. Waiting %i secs for rate limit reset.\" % (seconds_to_reset))\n            sleep(seconds_to_reset)\n        else:\n            # owner is not an org, try with a user\n            url_owner = url_user\n    return url_owner", "response": "Get the URL of the owner repos."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of repositores from GitHub.", "response": "def get_repositores(owner_url, token, nrepos):\n    \"\"\" owner could be an org or and user \"\"\"\n    all_repos = []\n\n    url = owner_url\n\n    while True:\n        logging.debug(\"Getting repos from: %s\" % (url))\n        try:\n            r = requests.get(url,\n                             params=get_payload(),\n                             headers=get_headers(token))\n\n            r.raise_for_status()\n            all_repos += r.json()\n\n            logging.debug(\"Rate limit: %s\" % (r.headers['X-RateLimit-Remaining']))\n\n            if 'next' not in r.links:\n                break\n\n            url = r.links['next']['url']  # Loving requests :)\n        except requests.exceptions.ConnectionError:\n            logging.error(\"Can not connect to GitHub\")\n            break\n\n    # Remove forks\n    nrepos_recent = [repo for repo in all_repos if not repo['fork']]\n    # Sort by updated_at and limit to nrepos\n    nrepos_sorted = sorted(nrepos_recent, key=lambda repo: parser.parse(repo['updated_at']), reverse=True)\n    nrepos_sorted = nrepos_sorted[0:nrepos]\n    # First the small repositories to feedback the user quickly\n    nrepos_sorted = sorted(nrepos_sorted, key=lambda repo: repo['size'])\n    for repo in nrepos_sorted:\n        logging.debug(\"%s %i %s\" % (repo['updated_at'], repo['size'], repo['name']))\n    return nrepos_sorted"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_redirect_web_page(web_dir, org_name, kibana_url):\n    html_redirect = \"\"\"\n    <html>\n        <head>\n    \"\"\"\n    html_redirect += \"\"\"<meta http-equiv=\"refresh\" content=\"0; URL=%s/app/kibana\"\"\"\\\n                     % kibana_url\n    html_redirect += \"\"\"#/dashboard/Overview?_g=(filters:!(('$state':\"\"\"\n    html_redirect += \"\"\"(store:globalState),meta:(alias:!n,disabled:!f,index:\"\"\"\n    html_redirect += \"\"\"github_git_enrich,key:project,negate:!f,value:%s),\"\"\"\\\n                     % org_name\n    html_redirect += \"\"\"query:(match:(project:(query:%s,type:phrase))))),\"\"\"\\\n                     % org_name\n    html_redirect += \"\"\"refreshInterval:(display:Off,pause:!f,value:0),\"\"\"\n    html_redirect += \"\"\"time:(from:now-2y,mode:quick,to:now))\" />\n        </head>\n    </html>\n    \"\"\"\n    try:\n        with open(path.join(web_dir, org_name), \"w\") as f:\n            f.write(html_redirect)\n    except FileNotFoundError as ex:\n        logging.error(\"Wrong web dir for redirect pages: %s\" % (web_dir))\n        logging.error(ex)", "response": "Create a page that redirects to the Kibana dashboard filtered for this org"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends an email to the user who has a contact with the details to access the Kibana dashboard", "response": "def notify_contact(mail, owner, graas_url, repos, first_repo=False):\n    \"\"\" Send an email to the contact with the details to access\n        the Kibana dashboard \"\"\"\n\n    footer = \"\"\"\n--\nBitergia Cauldron Team\nhttp://bitergia.com\n    \"\"\"\n\n    twitter_txt = \"Check Cauldron.io dashboard for %s at %s/dashboards/%s\" % (owner, graas_url, owner)\n    twitter_url = \"https://twitter.com/intent/tweet?text=\" + quote_plus(twitter_txt)\n    twitter_url += \"&via=bitergia\"\n\n    if first_repo:\n        logging.info(\"Sending first email to %s\" % (mail))\n        subject = \"First repository for %s already in the Cauldron\" % (owner)\n    else:\n        logging.info(\"Sending last email to %s\" % (mail))\n        subject = \"Your Cauldron %s dashboard is ready!\" % (owner)\n\n    if first_repo:\n        # body = \"%s/dashboards/%s\\n\\n\" % (graas_url, owner)\n        # body += \"First repository analized: %s\\n\" % (repos[0]['html_url'])\n        body = \"\"\"\nFirst repository has been analyzed and it's already in the Cauldron. Be patient, we have just started, step by step.\n\nWe will notify you when everything is ready.\n\nMeanwhile, check latest dashboards in %s\n\nThanks,\n%s\n    \"\"\" % (graas_url, footer)\n    else:\n        body = \"\"\"\nCheck it at: %s/dashboards/%s\n\nPlay with it, and send us feedback:\nhttps://github.com/Bitergia/cauldron.io/issues/new\n\nShare it on Twitter:\n%s\n\nThank you very much,\n%s\n    \"\"\" % (graas_url, owner, twitter_url, footer)\n\n    msg = MIMEText(body)\n    msg['Subject'] = subject\n    msg['From'] = 'info@bitergia.com'\n    msg['To'] = mail\n\n    try:\n        s = smtplib.SMTP('localhost')\n        s.send_message(msg)\n        s.quit()\n    except ConnectionRefusedError:\n        logging.error(\"Can not notify user. Can not connect to email server.\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef publish_twitter(twitter_contact, owner):\n    dashboard_url = CAULDRON_DASH_URL + \"/%s\" % (owner)\n    tweet = \"@%s your http://cauldron.io dashboard for #%s at GitHub is ready: %s. Check it out! #oscon\" \\\n        % (twitter_contact, owner, dashboard_url)\n    status = quote_plus(tweet)\n    oauth = get_oauth()\n    r = requests.post(url=\"https://api.twitter.com/1.1/statuses/update.json?status=\" + status, auth=oauth)", "response": "Publish in twitter the dashboard"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the identities from an item", "response": "def get_identities(self, item):\n        \"\"\"Return the identities from an item\"\"\"\n\n        user = self.get_sh_identity(item, self.get_field_author())\n        yield user"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        for rol in self.roles:\n            if rol in item['data']:\n                yield self.get_sh_identity(item[\"data\"][rol])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_identities(self, item):\n\n        item = item['data']\n        for identity in self.issue_roles:\n            if item[identity]:\n                user = self.get_sh_identity(item[identity])\n                if user:\n                    yield user", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_perceval_params_from_url(cls, urls):\n        params = []\n\n        dparam = cls.get_arthur_params_from_url(urls)\n        params.append(dparam[\"url\"])\n\n        return params", "response": "Get the perceval params given the URLs for the data source"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        item = item['data']\n\n        for identity in ['creator']:\n            # Todo: questions has also involved and solved_by\n            if identity in item and item[identity]:\n                user = self.get_sh_identity(item[identity])\n                yield user\n            if 'answers_data' in item:\n                for answer in item['answers_data']:\n                    user = self.get_sh_identity(answer[identity])\n                    yield user"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef kafka_kip(enrich):\n\n    \"\"\" Kafka Improvement Proposals process study \"\"\"\n\n    def extract_vote_and_binding(body):\n        \"\"\" Extracts the vote and binding for a KIP process included in message body \"\"\"\n\n        vote = 0\n        binding = 0  # by default the votes are binding for +1\n        nlines = 0\n\n        for line in body.split(\"\\n\"):\n            if nlines > MAX_LINES_FOR_VOTE:\n                # The vote must be in the first MAX_LINES_VOTE\n                break\n            if line.startswith(\">\"):\n                # This line is from a previous email\n                continue\n            elif \"+1\" in line and \"-1\" in line:\n                # Report summary probably\n                continue\n            elif \"to -1\" in line or \"is -1\" in line or \"= -1\" in line or \"-1 or\" in line:\n                continue\n            elif line.startswith(\"+1\") or \" +1 \" in line or line.endswith(\"+1\") \\\n                    or \" +1.\" in line or \" +1,\" in line:\n                vote = 1\n                binding = 1  # by default the votes are binding for +1\n                if 'non-binding' in line.lower():\n                    binding = 0\n                elif 'binding' in line.lower():\n                    binding = 1\n                break\n            elif line.startswith(\"-1\") or line.endswith(\" -1\") or \" -1 \" in line \\\n                    or \" -1.\" in line or \" -1,\" in line:\n                vote = -1\n                if 'non-binding' in line.lower():\n                    binding = 0\n                elif 'binding' in line.lower():\n                    binding = 1\n                break\n            nlines += 1\n\n        return (vote, binding)\n\n    def extract_kip(subject):\n        \"\"\" Extracts a KIP number from an email subject \"\"\"\n\n        kip = None\n\n        if not subject:\n            return kip\n\n        if 'KIP' not in subject:\n            return kip\n\n        kip_tokens = subject.split('KIP')\n        if len(kip_tokens) > 2:\n            # [KIP-DISCUSSION] KIP-7 Security\n            for token in kip_tokens:\n                kip = extract_kip(\"KIP\" + token)\n                if kip:\n                    break\n            # logger.debug(\"Several KIPs in %s. Found: %i\", subject, kip)\n            return kip\n\n        str_with_kip = kip_tokens[1]\n\n        if not str_with_kip:\n            # Sample use case subject: Create a space template for KIP\n            return kip\n\n        if str_with_kip[0] == '-':\n            try:\n                # KIP-120: Control\n                str_kip = str_with_kip[1:].split(\":\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # KIP-8 Add\n                str_kip = str_with_kip[1:].split(\" \")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # KIP-11- Authorization\n                str_kip = str_with_kip[1:].split(\"-\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # Bound fetch response size (KIP-74)\n                str_kip = str_with_kip[1:].split(\")\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # KIP-31&\n                str_kip = str_with_kip[1:].split(\"&\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # KIP-31/\n                str_kip = str_with_kip[1:].split(\"/\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # Re: Copycat (KIP-26. PR-99) - plan on moving forward\n                str_kip = str_with_kip[1:].split(\".\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n        elif str_with_kip[0] == ' ':\n            try:\n                # KIP 20 Enable\n                str_kip = str_with_kip[1:].split(\" \")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # Re: [DISCUSS] KIP 88: DescribeGroups Protocol Update\n                str_kip = str_with_kip[1:].split(\":\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n            try:\n                # [jira] [Updated] (KAFKA-5092) KIP 141- ProducerRecordBuilder\n                str_kip = str_with_kip[1:].split(\"-\")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n        elif str_with_kip[0] == ':':\n            try:\n                # Re: [VOTE] KIP:71 Enable log compaction and deletion to co-exist\n                str_kip = str_with_kip[1:].split(\" \")[0]\n                kip = int(str_kip)\n                return kip\n            except ValueError:\n                pass\n\n        if not kip:\n            # logger.debug(\"Can not extract KIP from %s\", subject)\n            pass\n\n        return kip\n\n    def lazy_result(votes):\n        \"\"\" Compute the result of a votation using lazy consensus\n            which requires 3 binding +1 votes and no binding vetoes.\n        \"\"\"\n        yes = 0\n        yes_binding = 0\n        veto = 0\n        veto_binding = 0\n\n        result = -1\n\n        for (vote, binding) in votes:\n            if vote == 1:\n                if binding:\n                    yes_binding += 1\n                else:\n                    yes += 1\n            if vote == -1:\n                if binding:\n                    veto_binding += 1\n                else:\n                    veto += 1\n\n        if veto_binding == 0 and yes_binding >= 3:\n            result = 1\n\n        return result\n\n    def add_kip_final_status_field(enrich):\n        \"\"\" Add kip final status field \"\"\"\n\n        total = 0\n\n        for eitem in enrich.fetch():\n            if \"kip\" not in eitem:\n                # It is not a KIP message\n                continue\n\n            if eitem['kip'] in enrich.kips_final_status:\n                eitem.update({\"kip_final_status\":\n                              enrich.kips_final_status[eitem['kip']]})\n            else:\n                logger.warning(\"No final status for kip: %i\", eitem['kip'])\n                eitem.update({\"kip_final_status\": None})\n            yield eitem\n            total += 1\n\n        logger.info(\"Total eitems with kafka final status kip field %i\", total)\n\n    def add_kip_time_status_fields(enrich):\n        \"\"\" Add kip fields with final status and times \"\"\"\n\n        total = 0\n        max_inactive_days = 90  # days\n\n        enrich.kips_final_status = {}  # final status for each kip\n\n        for eitem in enrich.fetch():\n            # kip_status: adopted (closed), discussion (open), voting (open),\n            #             inactive (open), discarded (closed)\n            # kip_start_end: discuss_start, discuss_end, voting_start, voting_end\n            kip_fields = {\n                \"kip_status\": None,\n                \"kip_discuss_time_days\": None,\n                \"kip_discuss_inactive_days\": None,\n                \"kip_voting_time_days\": None,\n                \"kip_voting_inactive_days\": None,\n                \"kip_is_first_discuss\": 0,\n                \"kip_is_first_vote\": 0,\n                \"kip_is_last_discuss\": 0,\n                \"kip_is_last_vote\": 0,\n                \"kip_result\": None,\n                \"kip_start_end\": None\n            }\n\n            if \"kip\" not in eitem:\n                # It is not a KIP message\n                continue\n            kip = eitem[\"kip\"]\n            kip_date = parser.parse(eitem[\"email_date\"])\n\n            if eitem['kip_is_discuss']:\n                kip_fields[\"kip_discuss_time_days\"] = \\\n                    get_time_diff_days(enrich.kips_dates[kip]['kip_min_discuss'],\n                                       enrich.kips_dates[kip]['kip_max_discuss'])\n\n                # Detect first and last discuss messages\n                if kip_date == enrich.kips_dates[kip]['kip_min_discuss']:\n                    kip_fields['kip_is_first_discuss'] = 1\n                    kip_fields['kip_start_end'] = 'discuss_start'\n                elif kip_date == enrich.kips_dates[kip]['kip_max_discuss']:\n                    kip_fields['kip_is_last_discuss'] = 1\n                    kip_fields['kip_start_end'] = 'discuss_end'\n\n                # Detect discussion status\n                if \"kip_min_vote\" not in enrich.kips_dates[kip]:\n                    kip_fields['kip_status'] = 'discussion'\n                max_discuss_date = enrich.kips_dates[kip]['kip_max_discuss']\n                kip_fields['kip_discuss_inactive_days'] = \\\n                    get_time_diff_days(max_discuss_date.replace(tzinfo=None),\n                                       datetime.utcnow())\n\n            if eitem['kip_is_vote']:\n                kip_fields[\"kip_voting_time_days\"] = \\\n                    get_time_diff_days(enrich.kips_dates[kip]['kip_min_vote'],\n                                       enrich.kips_dates[kip]['kip_max_vote'])\n\n                # Detect first and last discuss messages\n                if kip_date == enrich.kips_dates[kip]['kip_min_vote']:\n                    kip_fields['kip_is_first_vote'] = 1\n                    kip_fields['kip_start_end'] = 'voting_start'\n                elif kip_date == enrich.kips_dates[kip]['kip_max_vote']:\n                    kip_fields['kip_is_last_vote'] = 1\n                    kip_fields['kip_start_end'] = 'voting_end'\n\n                # Detect discussion status\n                kip_fields['kip_status'] = 'voting'\n                max_vote_date = enrich.kips_dates[kip]['kip_max_vote']\n                kip_fields['kip_voting_inactive_days'] = \\\n                    get_time_diff_days(max_vote_date.replace(tzinfo=None),\n                                       datetime.utcnow())\n\n                # Now check if there is a result from enrich.kips_scores\n                kip_fields['kip_result'] = lazy_result(enrich.kips_scores[kip])\n\n                if kip_fields['kip_result'] == 1:\n                    kip_fields['kip_status'] = 'adopted'\n                elif kip_fields['kip_result'] == -1:\n                    kip_fields['kip_status'] = 'discarded'\n\n            # And now change the status inactive\n            if kip_fields['kip_status'] not in ['adopted', 'discarded']:\n                inactive_days = kip_fields['kip_discuss_inactive_days']\n\n                if inactive_days and inactive_days > max_inactive_days:\n                    kip_fields['kip_status'] = 'inactive'\n\n                inactive_days = kip_fields['kip_voting_inactive_days']\n                if inactive_days and inactive_days > max_inactive_days:\n                    kip_fields['kip_status'] = 'inactive'\n\n            # The final status is in the kip_is_last_discuss or kip_is_last_vote\n            # It will be filled in the next enrichment round\n            eitem.update(kip_fields)\n            if eitem['kip'] not in enrich.kips_final_status:\n                enrich.kips_final_status[kip] = None\n            if eitem['kip_is_last_discuss'] and not enrich.kips_final_status[kip]:\n                enrich.kips_final_status[kip] = kip_fields['kip_status']\n            if eitem['kip_is_last_vote']:\n                enrich.kips_final_status[kip] = kip_fields['kip_status']\n\n            yield eitem\n            total += 1\n\n        logger.info(\"Total eitems with kafka extra kip fields %i\", total)\n\n    def add_kip_fields(enrich):\n        \"\"\" Add extra fields needed for kip analysis\"\"\"\n\n        total = 0\n\n        enrich.kips_dates = {\n            0: {\n                \"kip_min_discuss\": None,\n                \"kip_max_discuss\": None,\n                \"kip_min_vote\": None,\n                \"kip_max_vote\": None,\n            }\n        }\n\n        enrich.kips_scores = {}\n\n        # First iteration\n        for eitem in enrich.fetch():\n            kip_fields = {\n                \"kip_is_vote\": 0,\n                \"kip_is_discuss\": 0,\n                \"kip_vote\": 0,\n                \"kip_binding\": 0,\n                \"kip\": 0,\n                \"kip_type\": \"general\"\n            }\n\n            kip = extract_kip(eitem['Subject'])\n            if not kip:\n                # It is not a KIP message\n                continue\n            if kip not in enrich.kips_dates:\n                enrich.kips_dates[kip] = {}\n            if kip not in enrich.kips_scores:\n                enrich.kips_scores[kip] = []\n\n            kip_date = parser.parse(eitem[\"email_date\"])\n\n            # Analyze the subject to fill the kip fields\n            if '[discuss]' in eitem['Subject'].lower() or \\\n               '[kip-discussion]'in eitem['Subject'].lower() or \\\n               '[discussion]'in eitem['Subject'].lower():\n                kip_fields['kip_is_discuss'] = 1\n                kip_fields['kip_type'] = \"discuss\"\n                kip_fields['kip'] = kip\n                # Update kip discuss dates\n                if \"kip_min_discuss\" not in enrich.kips_dates[kip]:\n                    enrich.kips_dates[kip].update({\n                        \"kip_min_discuss\": kip_date,\n                        \"kip_max_discuss\": kip_date\n                    })\n                else:\n                    if enrich.kips_dates[kip][\"kip_min_discuss\"] >= kip_date:\n                        enrich.kips_dates[kip][\"kip_min_discuss\"] = kip_date\n                    if enrich.kips_dates[kip][\"kip_max_discuss\"] <= kip_date:\n                        enrich.kips_dates[kip][\"kip_max_discuss\"] = kip_date\n\n            if '[vote]' in eitem['Subject'].lower():\n                kip_fields['kip_is_vote'] = 1\n                kip_fields['kip_type'] = \"vote\"\n                kip_fields['kip'] = kip\n                if 'body_extract' in eitem:\n                    (vote, binding) = extract_vote_and_binding(eitem['body_extract'])\n                    enrich.kips_scores[kip] += [(vote, binding)]\n                    kip_fields['kip_vote'] = vote\n                    kip_fields['kip_binding'] = binding\n                else:\n                    logger.debug(\"Message %s without body\", eitem['Subject'])\n                # Update kip discuss dates\n                if \"kip_min_vote\" not in enrich.kips_dates[kip]:\n                    enrich.kips_dates[kip].update({\n                        \"kip_min_vote\": kip_date,\n                        \"kip_max_vote\": kip_date\n                    })\n                else:\n                    if enrich.kips_dates[kip][\"kip_min_vote\"] >= kip_date:\n                        enrich.kips_dates[kip][\"kip_min_vote\"] = kip_date\n                    if enrich.kips_dates[kip][\"kip_max_vote\"] <= kip_date:\n                        enrich.kips_dates[kip][\"kip_max_vote\"] = kip_date\n\n            eitem.update(kip_fields)\n            yield eitem\n            total += 1\n\n        logger.info(\"Total eitems with kafka kip fields %i\", total)\n\n    logger.debug(\"Doing kafka_kip study from %s\", enrich.elastic.anonymize_url(enrich.elastic.index_url))\n\n    # First iteration with the basic fields\n    eitems = add_kip_fields(enrich)\n    enrich.elastic.bulk_upload(eitems, enrich.get_field_unique_id())\n\n    # Second iteration with the final time and status fields\n    eitems = add_kip_time_status_fields(enrich)\n    enrich.elastic.bulk_upload(eitems, enrich.get_field_unique_id())\n\n    # Third iteration to compute the end status field for all KIPs\n    eitems = add_kip_final_status_field(enrich)\n    enrich.elastic.bulk_upload(eitems, enrich.get_field_unique_id())", "response": "Kafka Improvement Proposals process study"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an identity to the sorting hat", "response": "def add_identity(cls, db, identity, backend):\n        \"\"\" Load and identity list from backend in Sorting Hat \"\"\"\n        uuid = None\n\n        try:\n            uuid = api.add_identity(db, backend, identity['email'],\n                                    identity['name'], identity['username'])\n\n            logger.debug(\"New sortinghat identity %s %s,%s,%s \",\n                         uuid, identity['username'], identity['name'], identity['email'])\n\n            profile = {\"name\": identity['name'] if identity['name'] else identity['username'],\n                       \"email\": identity['email']}\n\n            api.edit_profile(db, uuid, **profile)\n\n        except AlreadyExistsError as ex:\n            uuid = ex.eid\n        except InvalidValueError as ex:\n            logger.warning(\"Trying to add a None identity. Ignoring it.\")\n        except UnicodeEncodeError as ex:\n            logger.warning(\"UnicodeEncodeError. Ignoring it. %s %s %s\",\n                           identity['email'], identity['name'],\n                           identity['username'])\n        except Exception as ex:\n            logger.warning(\"Unknown exception adding identity. Ignoring it. %s %s %s\",\n                           identity['email'], identity['name'],\n                           identity['username'], exc_info=True)\n\n        if 'company' in identity and identity['company'] is not None:\n            try:\n                api.add_organization(db, identity['company'])\n                api.add_enrollment(db, uuid, identity['company'],\n                                   datetime(1900, 1, 1),\n                                   datetime(2100, 1, 1))\n            except AlreadyExistsError:\n                pass\n\n        return uuid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding identities to Sorting Hat", "response": "def add_identities(cls, db, identities, backend):\n        \"\"\" Load identities list from backend in Sorting Hat \"\"\"\n\n        logger.info(\"Adding the identities to SortingHat\")\n\n        total = 0\n\n        for identity in identities:\n            try:\n                cls.add_identity(db, identity, backend)\n                total += 1\n            except Exception as e:\n                logger.error(\"Unexcepted error when adding identities: %s\" % e)\n                continue\n\n        logger.info(\"Total identities added to SH: %i\", total)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete an identity from SortingHat.", "response": "def remove_identity(cls, sh_db, ident_id):\n        \"\"\"Delete an identity from SortingHat.\n\n        :param sh_db: SortingHat database\n        :param ident_id: identity identifier\n        \"\"\"\n        success = False\n        try:\n            api.delete_identity(sh_db, ident_id)\n            logger.debug(\"Identity %s deleted\", ident_id)\n            success = True\n        except Exception as e:\n            logger.debug(\"Identity not deleted due to %s\", str(e))\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a unique identity from SortingHat.", "response": "def remove_unique_identity(cls, sh_db, uuid):\n        \"\"\"Delete a unique identity from SortingHat.\n\n        :param sh_db: SortingHat database\n        :param uuid: Unique identity identifier\n        \"\"\"\n        success = False\n        try:\n            api.delete_unique_identity(sh_db, uuid)\n            logger.debug(\"Unique identity %s deleted\", uuid)\n            success = True\n        except Exception as e:\n            logger.debug(\"Unique identity not deleted due to %s\", str(e))\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unique_identities(cls, sh_db):\n        try:\n            for unique_identity in api.unique_identities(sh_db):\n                yield unique_identity\n        except Exception as e:\n            logger.debug(\"Unique identities not returned from SortingHat due to %s\", str(e))", "response": "List the unique identities available in SortingHat."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        data = item['data']\n        if 'assigned_to' in data:\n            user = self.get_sh_identity(data, 'assigned_to')\n            yield user\n        author = self.get_sh_identity(data, 'author')\n        yield author"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        # question\n        user = self.get_sh_identity(item, self.get_field_author())\n        yield user\n\n        # answers\n        if 'answers' in item['data']:\n            for answer in item['data']['answers']:\n                # avoid \"answered_by\" : \"This post is a wiki\" corner case\n                if type(answer['answered_by']) is dict:\n                    user = self.get_sh_identity(answer['answered_by'])\n                    yield user\n                if 'comments' in answer:\n                    for comment in answer['comments']:\n                        commenter = self.get_sh_identity(comment)\n                        yield commenter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_identities(self, item):\n\n        user = self.get_sh_identity(item, self.get_field_author())\n        yield user\n\n        # Get the identities from the releases\n        for release in item['data']['releases']:\n            user = self.get_sh_identity(release['module'], self.get_field_author())\n            yield user", "response": "Get the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the enriched events related to a module", "response": "def get_rich_events(self, item):\n        \"\"\"\n        Get the enriched events related to a module\n        \"\"\"\n        module = item['data']\n        if not item['data']['releases']:\n            return []\n\n        for release in item['data']['releases']:\n            event = self.get_rich_item(item)\n            # Update specific fields for this release\n            event[\"uuid\"] += \"_\" + release['slug']\n            event[\"author_url\"] = 'https://forge.puppet.com/' + release['module']['owner']['username']\n            event[\"gravatar_id\"] = release['module']['owner']['gravatar_id']\n            event[\"downloads\"] = release['downloads']\n            event[\"slug\"] = release['slug']\n            event[\"version\"] = release['version']\n            event[\"uri\"] = release['uri']\n            event[\"validation_score\"] = release['validation_score']\n            event[\"homepage_url\"] = None\n            if 'project_page' in release['metadata']:\n                event[\"homepage_url\"] = release['metadata']['project_page']\n            event[\"issues_url\"] = None\n            if \"issues_url\" in release['metadata']:\n                event[\"issues_url\"] = release['metadata']['issues_url']\n            event[\"tags\"] = release['tags']\n            event[\"license\"] = release['metadata']['license']\n            event[\"source_url\"] = release['metadata']['source']\n            event[\"summary\"] = release['metadata']['summary']\n\n            event[\"metadata__updated_on\"] = parser.parse(release['updated_at']).isoformat()\n\n            if self.sortinghat:\n                release[\"metadata__updated_on\"] = event[\"metadata__updated_on\"]  # Needed in get_item_sh logic\n                event.update(self.get_item_sh(release))\n\n            if self.prjs_map:\n                event.update(self.get_item_project(event))\n\n            event.update(self.get_grimoire_fields(release[\"created_at\"], \"release\"))\n\n            yield event"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to the MySQL database.", "response": "def _connect(self):\n        \"\"\"Connect to the MySQL database.\n        \"\"\"\n\n        try:\n            db = pymysql.connect(user=self.user, passwd=self.passwd,\n                                 host=self.host, port=self.port,\n                                 db=self.shdb, use_unicode=True)\n            return db, db.cursor()\n        except Exception:\n            logger.error(\"Database connection error\")\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self, query):\n\n        # sql = query.format(scm_db = self.scmdb,\n        #                   sh_db = self.shdb,\n        #                   prj_db = self.prjdb)\n\n        results = int(self.cursor.execute(query))\n        if results > 0:\n            result1 = self.cursor.fetchall()\n            return result1\n        else:\n            return []", "response": "Execute an SQL query with the corresponding database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfeeds Ocean with backend data collected from arthur redis queue", "response": "def feed_arthur():\n    \"\"\" Feed Ocean with backend data collected from arthur redis queue\"\"\"\n\n    logger.info(\"Collecting items from redis queue\")\n\n    db_url = 'redis://localhost/8'\n\n    conn = redis.StrictRedis.from_url(db_url)\n    logger.debug(\"Redis connection stablished with %s.\", db_url)\n\n    # Get and remove queued items in an atomic transaction\n    pipe = conn.pipeline()\n    pipe.lrange(Q_STORAGE_ITEMS, 0, -1)\n    pipe.ltrim(Q_STORAGE_ITEMS, 1, 0)\n    items = pipe.execute()[0]\n\n    for item in items:\n        arthur_item = pickle.loads(item)\n        if arthur_item['tag'] not in arthur_items:\n            arthur_items[arthur_item['tag']] = []\n        arthur_items[arthur_item['tag']].append(arthur_item)\n\n    for tag in arthur_items:\n        logger.debug(\"Items for %s: %i\", tag, len(arthur_items[tag]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfeeding items from arthur redis queue with backend data collected from arthur redis queue", "response": "def feed_backend_arthur(backend_name, backend_params):\n    \"\"\" Feed Ocean with backend data collected from arthur redis queue\"\"\"\n\n    # Always get pending items from arthur for all data sources\n    feed_arthur()\n\n    logger.debug(\"Items available for %s\", arthur_items.keys())\n\n    # Get only the items for the backend\n    if not get_connector_from_name(backend_name):\n        raise RuntimeError(\"Unknown backend %s\" % backend_name)\n    connector = get_connector_from_name(backend_name)\n    klass = connector[3]  # BackendCmd for the connector\n\n    backend_cmd = init_backend(klass(*backend_params))\n\n    tag = backend_cmd.backend.tag\n    logger.debug(\"Getting items for %s.\", tag)\n\n    if tag in arthur_items:\n        logger.debug(\"Found items for %s.\", tag)\n        for item in arthur_items[tag]:\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef feed_backend(url, clean, fetch_archive, backend_name, backend_params,\n                 es_index=None, es_index_enrich=None, project=None, arthur=False,\n                 es_aliases=None, projects_json_repo=None):\n    \"\"\" Feed Ocean with backend data \"\"\"\n\n    backend = None\n    repo = {'backend_name': backend_name, 'backend_params': backend_params}  # repository data to be stored in conf\n\n    if es_index:\n        clean = False  # don't remove index, it could be shared\n\n    if not get_connector_from_name(backend_name):\n        raise RuntimeError(\"Unknown backend %s\" % backend_name)\n    connector = get_connector_from_name(backend_name)\n    klass = connector[3]  # BackendCmd for the connector\n\n    try:\n        logger.info(\"Feeding Ocean from %s (%s)\", backend_name, es_index)\n\n        if not es_index:\n            logger.error(\"Raw index not defined for %s\", backend_name)\n\n        repo['repo_update_start'] = datetime.now().isoformat()\n\n        # perceval backends fetch params\n        offset = None\n        from_date = None\n        category = None\n        latest_items = None\n        filter_classified = None\n\n        backend_cmd = klass(*backend_params)\n\n        parsed_args = vars(backend_cmd.parsed_args)\n        init_args = find_signature_parameters(backend_cmd.BACKEND,\n                                              parsed_args)\n\n        if backend_cmd.archive_manager and fetch_archive:\n            archive = Archive(parsed_args['archive_path'])\n        else:\n            archive = backend_cmd.archive_manager.create_archive() if backend_cmd.archive_manager else None\n\n        init_args['archive'] = archive\n        backend_cmd.backend = backend_cmd.BACKEND(**init_args)\n        backend = backend_cmd.backend\n\n        ocean_backend = connector[1](backend, fetch_archive=fetch_archive, project=project)\n        elastic_ocean = get_elastic(url, es_index, clean, ocean_backend, es_aliases)\n        ocean_backend.set_elastic(elastic_ocean)\n        ocean_backend.set_projects_json_repo(projects_json_repo)\n\n        if fetch_archive:\n            signature = inspect.signature(backend.fetch_from_archive)\n        else:\n            signature = inspect.signature(backend.fetch)\n\n        if 'from_date' in signature.parameters:\n            try:\n                # Support perceval pre and post BackendCommand refactoring\n                from_date = backend_cmd.from_date\n            except AttributeError:\n                from_date = backend_cmd.parsed_args.from_date\n\n        if 'offset' in signature.parameters:\n            try:\n                offset = backend_cmd.offset\n            except AttributeError:\n                offset = backend_cmd.parsed_args.offset\n\n        if 'category' in signature.parameters:\n            try:\n                category = backend_cmd.category\n            except AttributeError:\n                try:\n                    category = backend_cmd.parsed_args.category\n                except AttributeError:\n                    pass\n\n        if 'filter_classified' in signature.parameters:\n            try:\n                filter_classified = backend_cmd.parsed_args.filter_classified\n            except AttributeError:\n                pass\n\n        if 'latest_items' in signature.parameters:\n            try:\n                latest_items = backend_cmd.latest_items\n            except AttributeError:\n                latest_items = backend_cmd.parsed_args.latest_items\n\n        # fetch params support\n        if arthur:\n            # If using arthur just provide the items generator to be used\n            # to collect the items and upload to Elasticsearch\n            aitems = feed_backend_arthur(backend_name, backend_params)\n            ocean_backend.feed(arthur_items=aitems)\n        else:\n            params = {}\n            if latest_items:\n                params['latest_items'] = latest_items\n            if category:\n                params['category'] = category\n            if filter_classified:\n                params['filter_classified'] = filter_classified\n            if from_date and (from_date.replace(tzinfo=None) != parser.parse(\"1970-01-01\")):\n                params['from_date'] = from_date\n            if offset:\n                params['from_offset'] = offset\n\n            ocean_backend.feed(**params)\n\n    except Exception as ex:\n        if backend:\n            logger.error(\"Error feeding ocean from %s (%s): %s\", backend_name, backend.origin, ex, exc_info=True)\n        else:\n            logger.error(\"Error feeding ocean %s\", ex, exc_info=True)\n\n    logger.info(\"Done %s \", backend_name)", "response": "Feed Ocean with backend data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_items_from_uuid(uuid, enrich_backend, ocean_backend):\n\n    # logger.debug(\"Getting items for merged uuid %s \"  % (uuid))\n\n    uuid_fields = enrich_backend.get_fields_uuid()\n\n    terms = \"\"  # all terms with uuids in the enriched item\n\n    for field in uuid_fields:\n        terms += \"\"\"\n         {\"term\": {\n           \"%s\": {\n              \"value\": \"%s\"\n           }\n         }}\n         \"\"\" % (field, uuid)\n        terms += \",\"\n\n    terms = terms[:-1]  # remove last , for last item\n\n    query = \"\"\"\n    {\"query\": { \"bool\": { \"should\": [%s] }}}\n    \"\"\" % (terms)\n\n    url_search = enrich_backend.elastic.index_url + \"/_search\"\n    url_search += \"?size=1000\"  # TODO get all items\n\n    r = requests_ses.post(url_search, data=query)\n\n    eitems = r.json()['hits']['hits']\n\n    if len(eitems) == 0:\n        # logger.warning(\"No enriched items found for uuid: %s \" % (uuid))\n        return []\n\n    items_ids = []\n\n    for eitem in eitems:\n        item_id = enrich_backend.get_item_id(eitem)\n        # For one item several eitems could be generated\n        if item_id not in items_ids:\n            items_ids.append(item_id)\n\n    # Time to get the items\n    logger.debug(\"Items to be renriched for merged uuids: %s\" % (\",\".join(items_ids)))\n\n    url_mget = ocean_backend.elastic.index_url + \"/_mget\"\n\n    items_ids_query = \"\"\n\n    for item_id in items_ids:\n        items_ids_query += '{\"_id\" : \"%s\"}' % (item_id)\n        items_ids_query += \",\"\n    items_ids_query = items_ids_query[:-1]  # remove last , for last item\n\n    query = '{\"docs\" : [%s]}' % (items_ids_query)\n    r = requests_ses.post(url_mget, data=query)\n\n    res_items = r.json()['docs']\n\n    items = []\n    for res_item in res_items:\n        if res_item['found']:\n            items.append(res_item[\"_source\"])\n\n    return items", "response": "Get all items that include a given uuid"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrefreshes the items in the enriched index with fresh data from the SortingHat database.", "response": "def refresh_identities(enrich_backend, author_field=None, author_values=None):\n    \"\"\"Refresh identities in enriched index.\n\n    Retrieve items from the enriched index corresponding to enrich_backend,\n    and update their identities information, with fresh data from the\n    SortingHat database.\n\n    Instead of the whole index, only items matching the filter_author\n    filter are fitered, if that parameters is not None.\n\n    :param enrich_backend: enriched backend to update\n    :param  author_field: field to match items authored by a user\n    :param  author_values: values of the authored field to match items\n    \"\"\"\n\n    def update_items(new_filter_author):\n\n        for eitem in enrich_backend.fetch(new_filter_author):\n            roles = None\n            try:\n                roles = enrich_backend.roles\n            except AttributeError:\n                pass\n            new_identities = enrich_backend.get_item_sh_from_id(eitem, roles)\n            eitem.update(new_identities)\n            yield eitem\n\n    logger.debug(\"Refreshing identities fields from %s\",\n                 enrich_backend.elastic.anonymize_url(enrich_backend.elastic.index_url))\n\n    total = 0\n\n    max_ids = enrich_backend.elastic.max_items_clause\n    logger.debug('Refreshing identities')\n\n    if author_field is None:\n        # No filter, update all items\n        for item in update_items(None):\n            yield item\n            total += 1\n    else:\n        to_refresh = []\n        for author_value in author_values:\n            to_refresh.append(author_value)\n\n            if len(to_refresh) > max_ids:\n                filter_author = {\"name\": author_field,\n                                 \"value\": to_refresh}\n\n                for item in update_items(filter_author):\n                    yield item\n                    total += 1\n\n                to_refresh = []\n\n        if len(to_refresh) > 0:\n            filter_author = {\"name\": author_field,\n                             \"value\": to_refresh}\n\n            for item in update_items(filter_author):\n                yield item\n                total += 1\n\n    logger.info(\"Total eitems refreshed for identities fields %i\", total)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the ocean backend configured to start from the last enriched date", "response": "def get_ocean_backend(backend_cmd, enrich_backend, no_incremental,\n                      filter_raw=None, filter_raw_should=None):\n    \"\"\" Get the ocean backend configured to start from the last enriched date \"\"\"\n\n    if no_incremental:\n        last_enrich = None\n    else:\n        last_enrich = get_last_enrich(backend_cmd, enrich_backend, filter_raw=filter_raw)\n\n    logger.debug(\"Last enrichment: %s\", last_enrich)\n\n    backend = None\n\n    connector = get_connectors()[enrich_backend.get_connector_name()]\n\n    if backend_cmd:\n        backend_cmd = init_backend(backend_cmd)\n        backend = backend_cmd.backend\n\n        signature = inspect.signature(backend.fetch)\n        if 'from_date' in signature.parameters:\n            ocean_backend = connector[1](backend, from_date=last_enrich)\n        elif 'offset' in signature.parameters:\n            ocean_backend = connector[1](backend, offset=last_enrich)\n        else:\n            if last_enrich:\n                ocean_backend = connector[1](backend, from_date=last_enrich)\n            else:\n                ocean_backend = connector[1](backend)\n    else:\n        # We can have params for non perceval backends also\n        params = enrich_backend.backend_params\n        if params:\n            try:\n                date_pos = params.index('--from-date')\n                last_enrich = parser.parse(params[date_pos + 1])\n            except ValueError:\n                pass\n        if last_enrich:\n            ocean_backend = connector[1](backend, from_date=last_enrich)\n        else:\n            ocean_backend = connector[1](backend)\n\n    if filter_raw:\n        ocean_backend.set_filter_raw(filter_raw)\n    if filter_raw_should:\n        ocean_backend.set_filter_raw_should(filter_raw_should)\n\n    return ocean_backend"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_studies(ocean_backend, enrich_backend, studies_args, retention_time=None):\n    for study in enrich_backend.studies:\n        selected_studies = [(s['name'], s['params']) for s in studies_args if s['type'] == study.__name__]\n\n        for (name, params) in selected_studies:\n            logger.info(\"Starting study: %s, params %s\", name, str(params))\n            try:\n                study(ocean_backend, enrich_backend, **params)\n            except Exception as e:\n                logger.error(\"Problem executing study %s, %s\", name, str(e))\n                raise e\n\n            # identify studies which creates other indexes. If the study is onion,\n            # it can be ignored since the index is recreated every week\n            if name.startswith('enrich_onion'):\n                continue\n\n            index_params = [p for p in params if 'out_index' in p]\n\n            for ip in index_params:\n                index_name = params[ip]\n                elastic = get_elastic(enrich_backend.elastic_url, index_name)\n\n                elastic.delete_items(retention_time)", "response": "Executes studies related to a given enrich backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenriches a backend with the given parameters", "response": "def enrich_backend(url, clean, backend_name, backend_params, cfg_section_name,\n                   ocean_index=None,\n                   ocean_index_enrich=None,\n                   db_projects_map=None, json_projects_map=None,\n                   db_sortinghat=None,\n                   no_incremental=False, only_identities=False,\n                   github_token=None, studies=False, only_studies=False,\n                   url_enrich=None, events_enrich=False,\n                   db_user=None, db_password=None, db_host=None,\n                   do_refresh_projects=False, do_refresh_identities=False,\n                   author_id=None, author_uuid=None, filter_raw=None,\n                   filters_raw_prefix=None, jenkins_rename_file=None,\n                   unaffiliated_group=None, pair_programming=False,\n                   node_regex=False, studies_args=None, es_enrich_aliases=None,\n                   last_enrich_date=None, projects_json_repo=None):\n    \"\"\" Enrich Ocean index \"\"\"\n\n    backend = None\n    enrich_index = None\n\n    if ocean_index or ocean_index_enrich:\n        clean = False  # don't remove index, it could be shared\n\n    if do_refresh_projects or do_refresh_identities:\n        clean = False  # refresh works over the existing enriched items\n\n    if not get_connector_from_name(backend_name):\n        raise RuntimeError(\"Unknown backend %s\" % backend_name)\n    connector = get_connector_from_name(backend_name)\n    klass = connector[3]  # BackendCmd for the connector\n\n    try:\n        backend = None\n        backend_cmd = None\n        if klass:\n            # Data is retrieved from Perceval\n            backend_cmd = init_backend(klass(*backend_params))\n            backend = backend_cmd.backend\n\n        if ocean_index_enrich:\n            enrich_index = ocean_index_enrich\n        else:\n            if not ocean_index:\n                ocean_index = backend_name + \"_\" + backend.origin\n            enrich_index = ocean_index + \"_enrich\"\n        if events_enrich:\n            enrich_index += \"_events\"\n\n        enrich_backend = connector[2](db_sortinghat, db_projects_map, json_projects_map,\n                                      db_user, db_password, db_host)\n        enrich_backend.set_params(backend_params)\n        # store the cfg section name in the enrich backend to recover the corresponding project name in projects.json\n        enrich_backend.set_cfg_section_name(cfg_section_name)\n        enrich_backend.set_from_date(last_enrich_date)\n        if url_enrich:\n            elastic_enrich = get_elastic(url_enrich, enrich_index, clean, enrich_backend, es_enrich_aliases)\n        else:\n            elastic_enrich = get_elastic(url, enrich_index, clean, enrich_backend, es_enrich_aliases)\n        enrich_backend.set_elastic(elastic_enrich)\n        if github_token and backend_name == \"git\":\n            enrich_backend.set_github_token(github_token)\n        if jenkins_rename_file and backend_name == \"jenkins\":\n            enrich_backend.set_jenkins_rename_file(jenkins_rename_file)\n        if unaffiliated_group:\n            enrich_backend.unaffiliated_group = unaffiliated_group\n        if pair_programming:\n            enrich_backend.pair_programming = pair_programming\n        if node_regex:\n            enrich_backend.node_regex = node_regex\n\n        # The filter raw is needed to be able to assign the project value to an enriched item\n        # see line 544, grimoire_elk/enriched/enrich.py (fltr = eitem['origin'] + ' --filter-raw=' + self.filter_raw)\n        if filter_raw:\n            enrich_backend.set_filter_raw(filter_raw)\n        elif filters_raw_prefix:\n            enrich_backend.set_filter_raw_should(filters_raw_prefix)\n\n        enrich_backend.set_projects_json_repo(projects_json_repo)\n        ocean_backend = get_ocean_backend(backend_cmd, enrich_backend,\n                                          no_incremental, filter_raw,\n                                          filters_raw_prefix)\n\n        if only_studies:\n            logger.info(\"Running only studies (no SH and no enrichment)\")\n            do_studies(ocean_backend, enrich_backend, studies_args)\n        elif do_refresh_projects:\n            logger.info(\"Refreshing project field in %s\",\n                        enrich_backend.elastic.anonymize_url(enrich_backend.elastic.index_url))\n            field_id = enrich_backend.get_field_unique_id()\n            eitems = refresh_projects(enrich_backend)\n            enrich_backend.elastic.bulk_upload(eitems, field_id)\n        elif do_refresh_identities:\n\n            author_attr = None\n            author_values = None\n            if author_id:\n                author_attr = 'author_id'\n                author_values = [author_id]\n            elif author_uuid:\n                author_attr = 'author_uuid'\n                author_values = [author_uuid]\n\n            logger.info(\"Refreshing identities fields in %s\",\n                        enrich_backend.elastic.anonymize_url(enrich_backend.elastic.index_url))\n\n            field_id = enrich_backend.get_field_unique_id()\n            eitems = refresh_identities(enrich_backend, author_attr, author_values)\n            enrich_backend.elastic.bulk_upload(eitems, field_id)\n        else:\n            clean = False  # Don't remove ocean index when enrich\n            elastic_ocean = get_elastic(url, ocean_index, clean, ocean_backend)\n            ocean_backend.set_elastic(elastic_ocean)\n\n            logger.info(\"Adding enrichment data to %s\",\n                        enrich_backend.elastic.anonymize_url(enrich_backend.elastic.index_url))\n\n            if db_sortinghat and enrich_backend.has_identities():\n                # FIXME: This step won't be done from enrich in the future\n                total_ids = load_identities(ocean_backend, enrich_backend)\n                logger.info(\"Total identities loaded %i \", total_ids)\n\n            if only_identities:\n                logger.info(\"Only SH identities added. Enrich not done!\")\n\n            else:\n                # Enrichment for the new items once SH update is finished\n                if not events_enrich:\n                    enrich_count = enrich_items(ocean_backend, enrich_backend)\n                    if enrich_count is not None:\n                        logger.info(\"Total items enriched %i \", enrich_count)\n                else:\n                    enrich_count = enrich_items(ocean_backend, enrich_backend, events=True)\n                    if enrich_count is not None:\n                        logger.info(\"Total events enriched %i \", enrich_count)\n                if studies:\n                    do_studies(ocean_backend, enrich_backend, studies_args)\n\n    except Exception as ex:\n        if backend:\n            logger.error(\"Error enriching ocean from %s (%s): %s\",\n                         backend_name, backend.origin, ex, exc_info=True)\n        else:\n            logger.error(\"Error enriching ocean %s\", ex, exc_info=True)\n\n    logger.info(\"Done %s \", backend_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes all unique identities which appear in the IDENTITIES_INDEX but not in the active_data_sources list.", "response": "def delete_orphan_unique_identities(es, sortinghat_db, current_data_source, active_data_sources):\n    \"\"\"Delete all unique identities which appear in SortingHat, but not in the IDENTITIES_INDEX.\n\n    :param es: ElasticSearchDSL object\n    :param sortinghat_db: instance of the SortingHat database\n    :param current_data_source: current data source\n    :param active_data_sources: list of active data sources\n    \"\"\"\n    def get_uuids_in_index(target_uuids):\n        \"\"\"Find a set of uuids in IDENTITIES_INDEX and return them if exist.\n\n        :param target_uuids: target uuids\n        \"\"\"\n        page = es.search(\n            index=IDENTITIES_INDEX,\n            scroll=\"360m\",\n            size=SIZE_SCROLL_IDENTITIES_INDEX,\n            body={\n                \"query\": {\n                    \"bool\": {\n                        \"filter\": [\n                            {\n                                \"terms\": {\n                                    \"sh_uuid\": target_uuids\n                                }\n                            }\n                        ]\n                    }\n                }\n            }\n        )\n\n        hits = []\n        if page['hits']['total'] != 0:\n            hits = page['hits']['hits']\n\n        return hits\n\n    def delete_unique_identities(target_uuids):\n        \"\"\"Delete a list of uuids from SortingHat.\n\n        :param target_uuids: uuids to be deleted\n        \"\"\"\n        count = 0\n\n        for uuid in target_uuids:\n            success = SortingHat.remove_unique_identity(sortinghat_db, uuid)\n            count = count + 1 if success else count\n\n        return count\n\n    def delete_identities(unique_ident, data_sources):\n        \"\"\"Remove the identities in non active data sources.\n\n        :param unique_ident: unique identity object\n        :param data_sources: target data sources\n        \"\"\"\n        count = 0\n        for ident in unique_ident.identities:\n            if ident.source not in data_sources:\n                success = SortingHat.remove_identity(sortinghat_db, ident.id)\n                count = count + 1 if success else count\n\n        return count\n\n    def has_identities_in_data_sources(unique_ident, data_sources):\n        \"\"\"Check if a unique identity has identities in a set of data sources.\n\n        :param unique_ident: unique identity object\n        :param data_sources: target data sources\n        \"\"\"\n        in_active = False\n        for ident in unique_ident.identities:\n            if ident.source in data_sources:\n                in_active = True\n                break\n\n        return in_active\n\n    deleted_unique_identities = 0\n    deleted_identities = 0\n    uuids_to_process = []\n\n    # Collect all unique identities\n    for unique_identity in SortingHat.unique_identities(sortinghat_db):\n\n        # Remove a unique identity if all its identities are in non active data source\n        if not has_identities_in_data_sources(unique_identity, active_data_sources):\n            deleted_unique_identities += delete_unique_identities([unique_identity.uuid])\n            continue\n\n        # Remove the identities of non active data source for a given unique identity\n        deleted_identities += delete_identities(unique_identity, active_data_sources)\n\n        # Process only the unique identities that include the current data source, since\n        # it may be that unique identities in other data source have not been\n        # added yet to IDENTITIES_INDEX\n        if not has_identities_in_data_sources(unique_identity, [current_data_source]):\n            continue\n\n        # Add the uuid to the list to check its existence in the IDENTITIES_INDEX\n        uuids_to_process.append(unique_identity.uuid)\n\n        # Process the uuids in block of SIZE_SCROLL_IDENTITIES_INDEX\n        if len(uuids_to_process) != SIZE_SCROLL_IDENTITIES_INDEX:\n            continue\n\n        # Find which uuids to be processed exist in IDENTITIES_INDEX\n        results = get_uuids_in_index(uuids_to_process)\n        uuids_found = [item['_source']['sh_uuid'] for item in results]\n\n        # Find the uuids which exist in SortingHat but not in IDENTITIES_INDEX\n        orphan_uuids = set(uuids_to_process) - set(uuids_found)\n        # Delete the orphan uuids from SortingHat\n        deleted_unique_identities += delete_unique_identities(orphan_uuids)\n        # Reset the list\n        uuids_to_process = []\n\n    # Check that no uuids have been left to process\n    if uuids_to_process:\n        # Find which uuids to be processed exist in IDENTITIES_INDEX\n        results = get_uuids_in_index(uuids_to_process)\n        uuids_found = [item['_source']['sh_uuid'] for item in results]\n\n        # Find the uuids which exist in SortingHat but not in IDENTITIES_INDEX\n        orphan_uuids = set(uuids_to_process) - set(uuids_found)\n\n        # Delete the orphan uuids from SortingHat\n        deleted_unique_identities += delete_unique_identities(orphan_uuids)\n\n    logger.debug(\"[identities retention] Total orphan unique identities deleted from SH: %i\",\n                 deleted_unique_identities)\n    logger.debug(\"[identities retention] Total identities in non-active data sources deleted from SH: %i\",\n                 deleted_identities)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting the unique identities not seen before before_date and delete them from SortingHat.", "response": "def delete_inactive_unique_identities(es, sortinghat_db, before_date):\n    \"\"\"Select the unique identities not seen before `before_date` and\n    delete them from SortingHat.\n\n    :param es: ElasticSearchDSL object\n    :param sortinghat_db: instance of the SortingHat database\n    :param before_date: datetime str to filter the identities\n    \"\"\"\n    page = es.search(\n        index=IDENTITIES_INDEX,\n        scroll=\"360m\",\n        size=SIZE_SCROLL_IDENTITIES_INDEX,\n        body={\n            \"query\": {\n                \"range\": {\n                    \"last_seen\": {\n                        \"lte\": before_date\n                    }\n                }\n            }\n        }\n    )\n\n    sid = page['_scroll_id']\n    scroll_size = page['hits']['total']\n\n    if scroll_size == 0:\n        logging.warning(\"[identities retention] No inactive identities found in %s after %s!\",\n                        IDENTITIES_INDEX, before_date)\n        return\n\n    count = 0\n\n    while scroll_size > 0:\n        for item in page['hits']['hits']:\n            to_delete = item['_source']['sh_uuid']\n            success = SortingHat.remove_unique_identity(sortinghat_db, to_delete)\n            # increment the number of deleted identities only if the corresponding command was successful\n            count = count + 1 if success else count\n\n        page = es.scroll(scroll_id=sid, scroll='60m')\n        sid = page['_scroll_id']\n        scroll_size = len(page['hits']['hits'])\n\n    logger.debug(\"[identities retention] Total inactive identities deleted from SH: %i\", count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retain_identities(retention_time, es_enrichment_url, sortinghat_db, data_source, active_data_sources):\n    before_date = get_diff_current_date(minutes=retention_time)\n    before_date_str = before_date.isoformat()\n\n    es = Elasticsearch([es_enrichment_url], timeout=120, max_retries=20, retry_on_timeout=True, verify_certs=False)\n\n    # delete the unique identities which have not been seen after `before_date`\n    delete_inactive_unique_identities(es, sortinghat_db, before_date_str)\n    # delete the unique identities for a given data source which are not in the IDENTITIES_INDEX\n    delete_orphan_unique_identities(es, sortinghat_db, data_source, active_data_sources)", "response": "Retain unique identities from SortingHat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the backend within the backend_cmd", "response": "def init_backend(backend_cmd):\n    \"\"\"Init backend within the backend_cmd\"\"\"\n\n    try:\n        backend_cmd.backend\n    except AttributeError:\n        parsed_args = vars(backend_cmd.parsed_args)\n        init_args = find_signature_parameters(backend_cmd.BACKEND,\n                                              parsed_args)\n        backend_cmd.backend = backend_cmd.BACKEND(**init_args)\n\n    return backend_cmd"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef populate_identities_index(es_enrichment_url, enrich_index):\n    class Mapping(BaseMapping):\n\n        @staticmethod\n        def get_elastic_mappings(es_major):\n            \"\"\"Get Elasticsearch mapping.\n\n            :param es_major: major version of Elasticsearch, as string\n            :returns:        dictionary with a key, 'items', with the mapping\n            \"\"\"\n\n            mapping = \"\"\"\n            {\n                \"properties\": {\n                   \"sh_uuid\": {\n                       \"type\": \"keyword\"\n                   },\n                   \"last_seen\": {\n                       \"type\": \"date\"\n                   }\n                }\n            }\n            \"\"\"\n\n            return {\"items\": mapping}\n\n    # identities index\n    mapping_identities_index = Mapping()\n    elastic_identities = get_elastic(es_enrichment_url, IDENTITIES_INDEX, mapping=mapping_identities_index)\n\n    # enriched index\n    elastic_enrich = get_elastic(es_enrichment_url, enrich_index)\n    # collect mapping attributes in enriched index\n    attributes = elastic_enrich.all_properties()\n    # select attributes coming from SortingHat (*_uuid except git_uuid)\n    sh_uuid_attributes = [attr for attr in attributes if attr.endswith('_uuid') and not attr.startswith('git_')]\n\n    enriched_items = ElasticItems(None)\n    enriched_items.elastic = elastic_enrich\n\n    logger.debug(\"[identities-index] Start adding identities to %s\", IDENTITIES_INDEX)\n\n    identities = []\n    for eitem in enriched_items.fetch(ignore_incremental=True):\n        for sh_uuid_attr in sh_uuid_attributes:\n\n            if sh_uuid_attr not in eitem:\n                continue\n\n            if not eitem[sh_uuid_attr]:\n                continue\n\n            identity = {\n                'sh_uuid': eitem[sh_uuid_attr],\n                'last_seen': datetime_utcnow().isoformat()\n            }\n\n            identities.append(identity)\n\n            if len(identities) == elastic_enrich.max_items_bulk:\n                elastic_identities.bulk_upload(identities, 'sh_uuid')\n                identities = []\n\n    if len(identities) > 0:\n        elastic_identities.bulk_upload(identities, 'sh_uuid')\n\n    logger.debug(\"[identities-index] End adding identities to %s\", IDENTITIES_INDEX)", "response": "Populate the identities index with the identities from the given elastic search."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a valid elastic index generated from unique_id", "response": "def safe_index(cls, unique_id):\n        \"\"\" Return a valid elastic index generated from unique_id \"\"\"\n        index = unique_id\n        if unique_id:\n            index = unique_id.replace(\"/\", \"_\").lower()\n        return index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_instance(url, insecure):\n\n        res = grimoire_con(insecure).get(url)\n        if res.status_code != 200:\n            logger.error(\"Didn't get 200 OK from url %s\", url)\n            raise ElasticConnectException\n        else:\n            try:\n                version_str = res.json()['version']['number']\n                version_major = version_str.split('.')[0]\n                return version_major\n            except Exception:\n                logger.error(\"Could not read proper welcome message from url %s\",\n                             ElasticSearch.anonymize_url(url))\n                logger.error(\"Message read: %s\", res.text)\n                raise ElasticConnectException", "response": "Checks if there is an instance of ElasticSearch in url."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_put_bulk(self, url, bulk_json):\n\n        headers = {\"Content-Type\": \"application/x-ndjson\"}\n\n        try:\n            res = self.requests.put(url + '?refresh=true', data=bulk_json, headers=headers)\n            res.raise_for_status()\n        except UnicodeEncodeError:\n            # Related to body.encode('iso-8859-1'). mbox data\n            logger.error(\"Encondig error ... converting bulk to iso-8859-1\")\n            bulk_json = bulk_json.encode('iso-8859-1', 'ignore')\n            res = self.requests.put(url, data=bulk_json, headers=headers)\n            res.raise_for_status()\n\n        result = res.json()\n        failed_items = []\n        if result['errors']:\n            # Due to multiple errors that may be thrown when inserting bulk data, only the first error is returned\n            failed_items = [item['index'] for item in result['items'] if 'error' in item['index']]\n            error = str(failed_items[0]['error'])\n\n            logger.error(\"Failed to insert data to ES: %s, %s\", error, self.anonymize_url(url))\n\n        inserted_items = len(result['items']) - len(failed_items)\n\n        # The exception is currently not thrown to avoid stopping ocean uploading processes\n        try:\n            if failed_items:\n                raise ELKError(cause=error)\n        except ELKError:\n            pass\n\n        logger.debug(\"%i items uploaded to ES (%s)\", inserted_items, self.anonymize_url(url))\n        return inserted_items", "response": "Bulk PUT controlling unicode issues"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist all aliases used in ES", "response": "def all_es_aliases(self):\n        \"\"\"List all aliases used in ES\"\"\"\n\n        r = self.requests.get(self.url + \"/_aliases\", headers=HEADER_JSON, verify=False)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as ex:\n            logger.warning(\"Something went wrong when retrieving aliases on %s.\",\n                           self.anonymize_url(self.index_url))\n            logger.warning(ex)\n            return\n\n        aliases = []\n        for index in r.json().keys():\n            aliases.extend(list(r.json()[index]['aliases'].keys()))\n\n        aliases = list(set(aliases))\n        return aliases"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_aliases(self):\n\n        # check alias doesn't exist\n        r = self.requests.get(self.index_url + \"/_alias\", headers=HEADER_JSON, verify=False)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as ex:\n            logger.warning(\"Something went wrong when retrieving aliases on %s.\",\n                           self.anonymize_url(self.index_url))\n            logger.warning(ex)\n            return\n\n        aliases = r.json()[self.index]['aliases']\n        return aliases", "response": "List aliases linked to the index"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an alias to the elastic obj SetException", "response": "def add_alias(self, alias):\n        \"\"\"Add an alias to the index set in the elastic obj\n\n        :param alias: alias to add\n\n        :returns: None\n        \"\"\"\n        aliases = self.list_aliases()\n        if alias in aliases:\n            logger.debug(\"Alias %s already exists on %s.\", alias, self.anonymize_url(self.index_url))\n            return\n\n        # add alias\n        alias_data = \"\"\"\n        {\n            \"actions\": [\n                {\n                    \"add\": {\n                        \"index\": \"%s\",\n                        \"alias\": \"%s\"\n                    }\n                }\n            ]\n        }\n        \"\"\" % (self.index, alias)\n\n        r = self.requests.post(self.url + \"/_aliases\", headers=HEADER_JSON, verify=False, data=alias_data)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as ex:\n            logger.warning(\"Something went wrong when adding an alias on %s. Alias not set.\",\n                           self.anonymize_url(self.index_url))\n            logger.warning(ex)\n            return\n\n        logger.info(\"Alias %s created on %s.\", alias, self.anonymize_url(self.index_url))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bulk_upload(self, items, field_id):\n\n        current = 0\n        new_items = 0  # total items added with bulk\n        bulk_json = \"\"\n\n        if not items:\n            return new_items\n\n        url = self.index_url + '/items/_bulk'\n\n        logger.debug(\"Adding items to %s (in %i packs)\", self.anonymize_url(url), self.max_items_bulk)\n        task_init = time()\n\n        for item in items:\n            if current >= self.max_items_bulk:\n                task_init = time()\n                new_items += self.safe_put_bulk(url, bulk_json)\n                current = 0\n                json_size = sys.getsizeof(bulk_json) / (1024 * 1024)\n                logger.debug(\"bulk packet sent (%.2f sec, %i total, %.2f MB)\"\n                             % (time() - task_init, new_items, json_size))\n                bulk_json = \"\"\n            data_json = json.dumps(item)\n            bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % (item[field_id])\n            bulk_json += data_json + \"\\n\"  # Bulk document\n            current += 1\n\n        if current > 0:\n            new_items += self.safe_put_bulk(url, bulk_json)\n            json_size = sys.getsizeof(bulk_json) / (1024 * 1024)\n            logger.debug(\"bulk packet sent (%.2f sec prev, %i total, %.2f MB)\"\n                         % (time() - task_init, new_items, json_size))\n\n        return new_items", "response": "Upload in controlled packs items to ES using bulk API"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the last date of the item in the record", "response": "def get_last_date(self, field, filters_=[]):\n        '''\n            :field: field with the data\n            :filters_: additional filters to find the date\n        '''\n\n        last_date = self.get_last_item_field(field, filters_=filters_)\n\n        return last_date"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_last_offset(self, field, filters_=[]):\n        '''\n            :field: field with the data\n            :filters_: additional filters to find the date\n        '''\n\n        offset = self.get_last_item_field(field, filters_=filters_, offset=True)\n\n        return offset", "response": "returns the offset of the last item in the record"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_last_item_field(self, field, filters_=[], offset=False):\n        '''\n            :field: field with the data\n            :filters_: additional filters to find the date\n            :offset: Return offset field insted of date field\n        '''\n\n        last_value = None\n\n        url = self.index_url\n        url += \"/_search\"\n\n        if filters_ is None:\n            filters_ = []\n\n        terms = []\n        for filter_ in filters_:\n            if not filter_:\n                continue\n            term = '''{\"term\" : { \"%s\" : \"%s\"}}''' % (filter_['name'], filter_['value'])\n            terms.append(term)\n\n        data_query = '''\"query\": {\"bool\": {\"filter\": [%s]}},''' % (','.join(terms))\n\n        data_agg = '''\n            \"aggs\": {\n                \"1\": {\n                  \"max\": {\n                    \"field\": \"%s\"\n                  }\n                }\n            }\n        ''' % field\n\n        data_json = '''\n        { \"size\": 0, %s  %s\n        } ''' % (data_query, data_agg)\n\n        logger.debug(\"%s %s\", self.anonymize_url(url), data_json)\n\n        headers = {\"Content-Type\": \"application/json\"}\n\n        res = self.requests.post(url, data=data_json, headers=headers)\n        res.raise_for_status()\n        res_json = res.json()\n\n        if 'aggregations' in res_json:\n            last_value = res_json[\"aggregations\"][\"1\"][\"value\"]\n\n            if offset:\n                if last_value is not None:\n                    last_value = int(last_value)\n            else:\n                if \"value_as_string\" in res_json[\"aggregations\"][\"1\"]:\n                    last_value = res_json[\"aggregations\"][\"1\"][\"value_as_string\"]\n                    last_value = parser.parse(last_value)\n                else:\n                    last_value = res_json[\"aggregations\"][\"1\"][\"value\"]\n                    if last_value:\n                        try:\n                            last_value = unixtime_to_datetime(last_value)\n                        except ValueError:\n                            # last_value is in microsecs\n                            last_value = unixtime_to_datetime(last_value / 1000)\n        return last_value", "response": "Get the last value of a field in the item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete items from the index before a given date.", "response": "def delete_items(self, retention_time, time_field=\"metadata__updated_on\"):\n        \"\"\"Delete documents updated before a given date\n\n        :param retention_time: maximum number of minutes wrt the current date to retain the data\n        :param time_field: time field to delete the data\n        \"\"\"\n        if retention_time is None:\n            logger.debug(\"[items retention] Retention policy disabled, no items will be deleted.\")\n            return\n\n        if retention_time <= 0:\n            logger.debug(\"[items retention] Minutes to retain must be greater than 0.\")\n            return\n\n        before_date = get_diff_current_date(minutes=retention_time)\n        before_date_str = before_date.isoformat()\n\n        es_query = '''\n                    {\n                      \"query\": {\n                        \"range\": {\n                            \"%s\": {\n                                \"lte\": \"%s\"\n                            }\n                        }\n                      }\n                    }\n                    ''' % (time_field, before_date_str)\n\n        r = self.requests.post(self.index_url + \"/_delete_by_query?refresh\",\n                               data=es_query, headers=HEADER_JSON, verify=False)\n        try:\n            r.raise_for_status()\n            r_json = r.json()\n            logger.debug(\"[items retention] %s items deleted from %s before %s.\",\n                         r_json['deleted'], self.anonymize_url(self.index_url), before_date)\n        except requests.exceptions.HTTPError as ex:\n            logger.error(\"[items retention] Error deleted items from %s.\", self.anonymize_url(self.index_url))\n            logger.error(ex)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget all properties of a given index", "response": "def all_properties(self):\n        \"\"\"Get all properties of a given index\"\"\"\n\n        properties = {}\n        r = self.requests.get(self.index_url + \"/_mapping\", headers=HEADER_JSON, verify=False)\n        try:\n            r.raise_for_status()\n            r_json = r.json()\n\n            if 'items' not in r_json[self.index]['mappings']:\n                return properties\n\n            if 'properties' not in r_json[self.index]['mappings']['items']:\n                return properties\n\n            properties = r_json[self.index]['mappings']['items']['properties']\n        except requests.exceptions.HTTPError as ex:\n            logger.error(\"Error all attributes for %s.\", self.anonymize_url(self.index_url))\n            logger.error(ex)\n            return\n\n        return properties"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the kibiter major number version from the given url", "response": "def get_kibiter_version(url):\n    \"\"\"\n        Return kibiter major number version\n\n        The url must point to the Elasticsearch used by Kibiter\n    \"\"\"\n\n    config_url = '.kibana/config/_search'\n    # Avoid having // in the URL because ES will fail\n    if url[-1] != '/':\n        url += \"/\"\n    url += config_url\n    r = requests.get(url)\n    r.raise_for_status()\n\n    if len(r.json()['hits']['hits']) == 0:\n        logger.error(\"Can not get the Kibiter version\")\n        return None\n\n    version = r.json()['hits']['hits'][0]['_id']\n    # 5.4.0-SNAPSHOT\n    major_version = version.split(\".\", 1)[0]\n    return major_version"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_params_parser():\n\n    parser = argparse.ArgumentParser(usage=ARTHUR_USAGE_MSG,\n                                     description=ARTHUR_DESC_MSG,\n                                     epilog=ARTHUR_EPILOG_MSG,\n                                     formatter_class=argparse.RawDescriptionHelpFormatter,\n                                     add_help=False)\n\n    ElasticOcean.add_params(parser)\n\n    parser.add_argument('-h', '--help', action='help',\n                        help=argparse.SUPPRESS)\n    parser.add_argument('-g', '--debug', dest='debug',\n                        action='store_true',\n                        help=argparse.SUPPRESS)\n    parser.add_argument(\"--no_incremental\", action='store_true',\n                        help=\"don't use last state for data source\")\n    parser.add_argument(\"--fetch_cache\", action='store_true',\n                        help=\"Use cache for item retrieval\")\n    parser.add_argument(\"--enrich\", action='store_true',\n                        help=\"Enrich items after retrieving\")\n    parser.add_argument(\"--enrich_only\", action='store_true',\n                        help=\"Only enrich items (DEPRECATED, use --only-enrich)\")\n    parser.add_argument(\"--only-enrich\", dest='enrich_only', action='store_true',\n                        help=\"Only enrich items\")\n    parser.add_argument(\"--filter-raw\", dest='filter_raw',\n                        help=\"Filter raw items. Format: field:value\")\n    parser.add_argument(\"--filters-raw-prefix\", nargs='*',\n                        help=\"Filter raw items with prefix filter. Format: field:value field:value ...\")\n    parser.add_argument(\"--events-enrich\", dest='events_enrich', action='store_true',\n                        help=\"Enrich events in items\")\n    parser.add_argument('--index', help=\"Ocean index name\")\n    parser.add_argument('--index-enrich', dest=\"index_enrich\", help=\"Ocean enriched index name\")\n    parser.add_argument('--db-user', help=\"User for db connection (default to root)\",\n                        default=\"root\")\n    parser.add_argument('--db-password', help=\"Password for db connection (default empty)\",\n                        default=\"\")\n    parser.add_argument('--db-host', help=\"Host for db connection (default to mariadb)\",\n                        default=\"mariadb\")\n    parser.add_argument('--db-projects-map', help=\"Projects Mapping DB\")\n    parser.add_argument('--json-projects-map', help=\"Projects Mapping JSON file\")\n    parser.add_argument('--project', help=\"Project for the repository (origin)\")\n    parser.add_argument('--refresh-projects', action='store_true', help=\"Refresh projects in enriched items\")\n    parser.add_argument('--db-sortinghat', help=\"SortingHat DB\")\n    parser.add_argument('--only-identities', action='store_true', help=\"Only add identities to SortingHat DB\")\n    parser.add_argument('--refresh-identities', action='store_true', help=\"Refresh identities in enriched items\")\n    parser.add_argument('--author_id', nargs='*', help=\"Field author_ids to be refreshed\")\n    parser.add_argument('--author_uuid', nargs='*', help=\"Field author_uuids to be refreshed\")\n    parser.add_argument('--github-token', help=\"If provided, github usernames will be retrieved in git enrich.\")\n    parser.add_argument('--jenkins-rename-file', help=\"CSV mapping file with nodes renamed schema.\")\n    parser.add_argument('--studies', action='store_true', help=\"Execute studies after enrichment.\")\n    parser.add_argument('--only-studies', action='store_true', help=\"Execute only studies.\")\n    parser.add_argument('--bulk-size', default=1000, type=int,\n                        help=\"Number of items per bulk request to Elasticsearch.\")\n    parser.add_argument('--scroll-size', default=100, type=int,\n                        help=\"Number of items to get from Elasticsearch when scrolling.\")\n    parser.add_argument('--arthur', action='store_true', help=\"Read items from arthur redis queue\")\n    parser.add_argument('--pair-programming', action='store_true', help=\"Do pair programming in git enrich\")\n    parser.add_argument('--studies-list', nargs='*', help=\"List of studies to be executed\")\n    parser.add_argument('backend', help=argparse.SUPPRESS)\n    parser.add_argument('backend_args', nargs=argparse.REMAINDER,\n                        help=argparse.SUPPRESS)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    return parser", "response": "Parse command line arguments and return a parser object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget params definition from ElasticOcean and from all backends", "response": "def get_params():\n    \"\"\" Get params definition from ElasticOcean and from all the backends \"\"\"\n\n    parser = get_params_parser()\n    args = parser.parse_args()\n\n    if not args.enrich_only and not args.only_identities and not args.only_studies:\n        if not args.index:\n            # Check that the raw index name is defined\n            print(\"[error] --index <name> param is required when collecting items from raw\")\n            sys.exit(1)\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_time_diff_days(start_txt, end_txt):\n    ''' Number of days between two days  '''\n\n    if start_txt is None or end_txt is None:\n        return None\n\n    start = parser.parse(start_txt)\n    end = parser.parse(end_txt)\n\n    seconds_day = float(60 * 60 * 24)\n    diff_days = \\\n        (end - start).total_seconds() / seconds_day\n    diff_days = float('%.2f' % diff_days)\n\n    return diff_days", "response": "Returns the number of days between two days"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\"Return the identities from an item\"\"\"\n\n        item = item['data']\n        if 'owner' in item:\n            owner = self.get_sh_identity(item['owner'])\n            yield owner\n        if 'user' in item:\n            user = self.get_sh_identity(item['user'])\n            yield user\n        if 'mentor' in item:\n            mentor = self.get_sh_identity(item['mentor'])\n            yield mentor"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding sorting hat enrichment fields", "response": "def get_item_sh(self, item, roles=None, date_field=None):\n        \"\"\"Add sorting hat enrichment fields\"\"\"\n\n        eitem_sh = {}\n        created = str_to_datetime(date_field)\n\n        for rol in roles:\n            identity = self.get_sh_identity(item, rol)\n            eitem_sh.update(self.get_item_sh_fields(identity, created, rol=rol))\n\n            if not eitem_sh[rol + '_org_name']:\n                eitem_sh[rol + '_org_name'] = SH_UNKNOWN_VALUE\n\n            if not eitem_sh[rol + '_name']:\n                eitem_sh[rol + '_name'] = SH_UNKNOWN_VALUE\n\n            if not eitem_sh[rol + '_user_name']:\n                eitem_sh[rol + '_user_name'] = SH_UNKNOWN_VALUE\n\n            # Add the author field common in all data sources\n            if rol == self.get_field_author():\n                identity = self.get_sh_identity(item, rol)\n                eitem_sh.update(self.get_item_sh_fields(identity, created, rol=\"author\"))\n\n                if not eitem_sh['author_org_name']:\n                    eitem_sh['author_org_name'] = SH_UNKNOWN_VALUE\n\n                if not eitem_sh['author_name']:\n                    eitem_sh['author_name'] = SH_UNKNOWN_VALUE\n\n                if not eitem_sh['author_user_name']:\n                    eitem_sh['author_user_name'] = SH_UNKNOWN_VALUE\n\n        return eitem_sh"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_identities(self, item):\n\n        item = item['data']\n\n        for field in [\"assignee\", \"reporter\", \"creator\"]:\n            if field not in item[\"fields\"]:\n                continue\n            if item[\"fields\"][field]:\n                user = self.get_sh_identity(item[\"fields\"][field])\n                yield user\n\n        comments = item.get('comments_data', [])\n        for comment in comments:\n            if 'author' in comment and comment['author']:\n                user = self.get_sh_identity(comment['author'])\n                yield user\n            if 'updateAuthor' in comment and comment['updateAuthor']:\n                user = self.get_sh_identity(comment['updateAuthor'])\n                yield user", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enrich_fields(cls, fields, eitem):\n\n        for field in fields:\n            if field.startswith('customfield_'):\n                if type(fields[field]) is dict:\n                    if 'name' in fields[field]:\n                        if fields[field]['name'] == \"Story Points\":\n                            eitem['story_points'] = fields[field]['value']\n                        elif fields[field]['name'] == \"Sprint\":\n                            value = fields[field]['value']\n                            if value:\n                                sprint = value[0].partition(\",name=\")[2].split(',')[0]\n                                sprint_start = value[0].partition(\",startDate=\")[2].split(',')[0]\n                                sprint_end = value[0].partition(\",endDate=\")[2].split(',')[0]\n                                sprint_complete = value[0].partition(\",completeDate=\")[2].split(',')[0]\n                                eitem['sprint'] = sprint\n                                eitem['sprint_start'] = cls.fix_value_null(sprint_start)\n                                eitem['sprint_end'] = cls.fix_value_null(sprint_end)\n                                eitem['sprint_complete'] = cls.fix_value_null(sprint_complete)", "response": "Enrich the fields property of an issue with new properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_identities(self, item):\n        identities = []\n\n        if 'data' not in item:\n            return identities\n        if 'revisions' not in item['data']:\n            return identities\n\n        revisions = item['data']['revisions']\n\n        for revision in revisions:\n            user = self.get_sh_identity(revision)\n            yield user", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_review_sh(self, revision, item):\n\n        identity = self.get_sh_identity(revision)\n        update = parser.parse(item[self.get_field_date()])\n        erevision = self.get_item_sh_fields(identity, update)\n\n        return erevision", "response": "Add sorting hat enrichment fields for the author of the revision"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\"Return the identities from an item\"\"\"\n\n        category = item['category']\n        item = item['data']\n\n        if category == \"issue\":\n            identity_types = ['user', 'assignee']\n        elif category == \"pull_request\":\n            identity_types = ['user', 'merged_by']\n        else:\n            identity_types = []\n\n        for identity in identity_types:\n            if item[identity]:\n                # In user_data we have the full user data\n                user = self.get_sh_identity(item[identity + \"_data\"])\n                if user:\n                    yield user"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets cache data for items of _type using key_ as the cache dict key", "response": "def get_github_cache(self, kind, key_):\n        \"\"\" Get cache data for items of _type using key_ as the cache dict key \"\"\"\n\n        cache = {}\n        res_size = 100  # best size?\n        from_ = 0\n\n        index_github = \"github/\" + kind\n\n        url = self.elastic.url + \"/\" + index_github\n        url += \"/_search\" + \"?\" + \"size=%i\" % res_size\n        r = self.requests.get(url)\n        type_items = r.json()\n\n        if 'hits' not in type_items:\n            logger.info(\"No github %s data in ES\" % (kind))\n\n        else:\n            while len(type_items['hits']['hits']) > 0:\n                for hit in type_items['hits']['hits']:\n                    item = hit['_source']\n                    cache[item[key_]] = item\n                from_ += res_size\n                r = self.requests.get(url + \"&from=%i\" % from_)\n                type_items = r.json()\n                if 'hits' not in type_items:\n                    break\n\n        return cache"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_time_to_merge_request_response(self, item):\n        review_dates = [str_to_datetime(review['created_at']) for review in item['review_comments_data']\n                        if item['user']['login'] != review['user']['login']]\n        if review_dates:\n            return min(review_dates)\n        return None", "response": "Get the first date at which a review was made on the PR by someone who created the PR"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_num_commenters(self, item):\n\n        commenters = [comment['user']['login'] for comment in item['comments_data']]\n        return len(set(commenters))", "response": "Get the number of unique people who commented on the issue"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        field = self.get_field_author()\n        yield self.get_sh_identity(item, field)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_identities(self, item):\n\n        message = item['data']['message']\n        identity = self.get_sh_identity(message['from'])\n\n        yield identity", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses command line arguments", "response": "def get_params():\n    \"\"\"Parse command line arguments\"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-g', '--debug', dest='debug', action='store_true')\n    parser.add_argument('-t', '--token', dest='token', help=\"GitHub token\")\n    parser.add_argument('-o', '--owner', dest='owner', help='GitHub owner (user or org) to be analyzed')\n    parser.add_argument('-n', '--nrepos', dest='nrepos', type=int, default=NREPOS,\n                        help='Number of GitHub repositories from the Organization to be analyzed (default:10)')\n\n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the arthur params given a URL for the data source", "response": "def get_arthur_params_from_url(cls, url):\n        \"\"\" Get the arthur params given a URL for the data source \"\"\"\n        params = {}\n\n        owner = url.split('/')[-2]\n        repository = url.split('/')[-1]\n        # params.append('--owner')\n        params['owner'] = owner\n        # params.append('--repository')\n        params['repository'] = repository\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_identities(self, item):\n        identities = []\n\n        field = self.get_field_author()\n        identities.append(self.get_sh_identity(item, field))\n\n        return identities", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_rich_events(self, item):\n        if \"version_downloads_data\" not in item['data']:\n            return []\n\n        # To get values from the task\n        eitem = self.get_rich_item(item)\n\n        for sample in item['data'][\"version_downloads_data\"][\"version_downloads\"]:\n            event = deepcopy(eitem)\n            event['download_sample_id'] = sample['id']\n            event['sample_date'] = sample['date']\n            sample_date = parser.parse(event['sample_date'])\n            event['sample_version'] = sample['version']\n            event['sample_downloads'] = sample['downloads']\n            event.update(self.get_grimoire_fields(sample_date.isoformat(), \"downloads_event\"))\n\n            yield event", "response": "Get the rich events from the item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_params_parser_create_dash():\n\n    parser = argparse.ArgumentParser(usage=\"usage: e2k.py [options]\",\n                                     description=\"Create a Kibana dashboard from a template\")\n\n    ElasticOcean.add_params(parser)\n\n    parser.add_argument(\"-d\", \"--dashboard\", help=\"dashboard to be used as template\")\n    parser.add_argument(\"-i\", \"--index\", help=\"enriched index to be used as data source\")\n    parser.add_argument(\"--kibana\", dest=\"kibana_index\", default=\".kibana\",\n                        help=\"Kibana index name (.kibana default)\")\n    parser.add_argument('-g', '--debug', dest='debug', action='store_true')\n\n    return parser", "response": "Parse command line arguments for create a Kibana dashboard from a template"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        item = item['data']\n        user = self.get_sh_identity(item)\n\n        yield user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting project mapping enrichment field.", "response": "def get_item_project(self, eitem):\n        \"\"\" Get project mapping enrichment field.\n\n        Twitter mappings is pretty special so it needs a special\n        implementacion.\n        \"\"\"\n\n        project = None\n        eitem_project = {}\n        ds_name = self.get_connector_name()  # data source name in projects map\n\n        if ds_name not in self.prjs_map:\n            return eitem_project\n\n        for tag in eitem['hashtags_analyzed']:\n            # lcanas: hashtag provided in projects.json file should not be case sensitive T6876\n            tags2project = CaseInsensitiveDict(self.prjs_map[ds_name])\n            if tag in tags2project:\n                project = tags2project[tag]\n                break\n\n        if project is None:\n            project = DEFAULT_PROJECT\n\n        eitem_project = {\"project\": project}\n\n        eitem_project.update(self.add_project_levels(project))\n\n        return eitem_project"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the nodes renaming file for Jenkins.", "response": "def set_jenkins_rename_file(self, nodes_rename_file):\n        \"\"\" File with nodes renaming mapping:\n\n        Node,Comment\n        arm-build1,remove\n        arm-build2,keep\n        ericsson-build3,merge into ericsson-build1\n        ....\n\n        Once set in the next enrichment the rename will be done\n        \"\"\"\n        self.nodes_rename_file = nodes_rename_file\n        self.__load_node_renames()\n        logger.info(\"Jenkis node rename file active: %s\", nodes_rename_file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_fields_from_job_name(self, job_name):\n\n        extra_fields = {\n            'category': None,\n            'installer': None,\n            'scenario': None,\n            'testproject': None,\n            'pod': None,\n            'loop': None,\n            'branch': None\n        }\n\n        try:\n            components = job_name.split('-')\n\n            if len(components) < 2:\n                return extra_fields\n\n            kind = components[1]\n            if kind == 'os':\n                extra_fields['category'] = 'parent/main'\n                extra_fields['installer'] = components[0]\n                extra_fields['scenario'] = '-'.join(components[2:-3])\n            elif kind == 'deploy':\n                extra_fields['category'] = 'deploy'\n                extra_fields['installer'] = components[0]\n            else:\n                extra_fields['category'] = 'test'\n                extra_fields['testproject'] = components[0]\n                extra_fields['installer'] = components[1]\n\n            extra_fields['pod'] = components[-3]\n            extra_fields['loop'] = components[-2]\n            extra_fields['branch'] = components[-1]\n        except IndexError as ex:\n            # Just DEBUG level because it is just for OPNFV\n            logger.debug('Problems parsing job name %s', job_name)\n            logger.debug(ex)\n\n        return extra_fields", "response": "Analyze a Jenkins job name and produce a dictionary of categorization information."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts node name using a regular expression.", "response": "def extract_builton(self, built_on, regex):\n        \"\"\"Extracts node name using a regular expression. Node name is expected to\n        be group 1.\n        \"\"\"\n        pattern = re.compile(regex, re.M | re.I)\n        match = pattern.search(built_on)\n        if match and len(match.groups()) >= 1:\n            node_name = match.group(1)\n        else:\n            msg = \"Node name not extracted, using builtOn as it is: \" + regex + \":\" + built_on\n            logger.warning(msg)\n            node_name = built_on\n\n        return node_name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        data = item['data']\n        identity = self.get_sh_identity(data)\n\n        if identity['username']:\n            self.add_sh_github_identity(identity['username'])\n\n        yield identity"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef onion_study(in_conn, out_conn, data_source):\n    onion = OnionStudy(in_connector=in_conn, out_connector=out_conn, data_source=data_source)\n    ndocs = onion.analyze()\n    return ndocs", "response": "Build and index for onion from a given Git index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread author commits by quarter org and project.", "response": "def read_block(self, size=None, from_date=None):\n        \"\"\"Read author commits by Quarter, Org and Project.\n\n        :param from_date: not used here. Incremental mode not supported yet.\n        :param size: not used here.\n        :return: DataFrame with commit count per author, split by quarter, org and project.\n        \"\"\"\n\n        # Get quarters corresponding to All items (Incremental mode NOT SUPPORTED)\n        quarters = self.__quarters()\n\n        for quarter in quarters:\n\n            logger.info(self.__log_prefix + \" Quarter: \" + str(quarter))\n\n            date_range = {self._timeframe_field: {'gte': quarter.start_time, 'lte': quarter.end_time}}\n\n            orgs = self.__list_uniques(date_range, self.AUTHOR_ORG)\n            projects = self.__list_uniques(date_range, self.PROJECT)\n\n            # Get global data\n            s = self.__build_search(date_range)\n            response = s.execute()\n\n            for timing in response.aggregations[self.TIMEFRAME].buckets:\n                yield self.__build_dataframe(timing).copy()\n\n            # Get global data by Org\n            for org_name in orgs:\n\n                logger.info(self.__log_prefix + \" Quarter: \" + str(quarter) + \"  Org: \" + org_name)\n\n                s = self.__build_search(date_range, org_name=org_name)\n                response = s.execute()\n\n                for timing in response.aggregations[self.TIMEFRAME].buckets:\n                    yield self.__build_dataframe(timing, org_name=org_name).copy()\n\n            # Get project specific data\n            for project in projects:\n\n                logger.info(self.__log_prefix + \" Quarter: \" + str(quarter) + \"  Project: \" + project)\n\n                # Global project\n                s = self.__build_search(date_range, project_name=project)\n                response = s.execute()\n\n                for timing in response.aggregations[self.TIMEFRAME].buckets:\n                    yield self.__build_dataframe(timing, project_name=project).copy()\n\n                # Split by Org\n                for org_name in orgs:\n\n                    logger.info(self.__log_prefix + \" Quarter: \" + str(quarter) + \"  Project: \" + project + \"  Org: \" + org_name)\n\n                    s = self.__build_search(date_range, project_name=project, org_name=org_name)\n                    response = s.execute()\n\n                    for timing in response.aggregations[self.TIMEFRAME].buckets:\n                        yield self.__build_dataframe(timing, project_name=project, org_name=org_name).copy()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, items):\n        if self._read_only:\n            raise IOError(\"Cannot write, Connector created as Read Only\")\n\n        if len(items) == 0:\n            logger.info(self.__log_prefix + \" Nothing to write\")\n            return\n\n        # Uploading info to the new ES\n        rows = items.to_dict(\"index\")\n        docs = []\n        for row_index in rows.keys():\n            row = rows[row_index]\n            item_id = row[self.AUTHOR_ORG] + '_' + row[self.PROJECT] + '_' \\\n                + row[self.TIMEFRAME] + '_' + row[self.AUTHOR_UUID]\n            item_id = item_id.replace(' ', '').lower()\n\n            doc = {\n                \"_index\": self._es_index,\n                \"_type\": \"item\",\n                \"_id\": item_id,\n                \"_source\": row\n            }\n            docs.append(doc)\n\n        # TODO uncomment following lines for incremental version\n        # # Delete old data if exists to ensure refreshing in case of deleted commits\n        # timeframe = docs[0]['_source']['timeframe']\n        # org = docs[0]['_source']['author_org_name']\n        # project = docs[0]['_source']['project']\n        # s = Search(using=self._es_conn, index=self._es_index)\n        # s = s.filter('term', project=project)\n        # s = s.filter('term', author_org_name=org)\n        # s = s.filter('term', timeframe=timeframe)\n        # response = s.execute()\n        #\n        # if response.hits.total > 0:\n        #     response = s.delete()\n        #     logger.info(\"[Onion] Deleted \" + str(response.deleted) + \" items for refreshing: \" + timeframe + \" \"\n        #                 + org + \" \" + project)\n\n        # TODO exception and error handling\n        helpers.bulk(self._es_conn, docs)\n        logger.info(self.__log_prefix + \" Written: \" + str(len(docs)))", "response": "Write items into ElasticSearch."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of pandas. Period objects corresponding to the available items from a given index date.", "response": "def __quarters(self, from_date=None):\n        \"\"\"Get a set of quarters with available items from a given index date.\n\n        :param from_date:\n        :return: list of `pandas.Period` corresponding to quarters\n        \"\"\"\n        s = Search(using=self._es_conn, index=self._es_index)\n        if from_date:\n            # Work around to solve conversion problem of '__' to '.' in field name\n            q = Q('range')\n            q.__setattr__(self._sort_on_field, {'gte': from_date})\n            s = s.filter(q)\n\n        # from:to parameters (=> from: 0, size: 0)\n        s = s[0:0]\n\n        s.aggs.bucket(self.TIMEFRAME, 'date_histogram', field=self._timeframe_field,\n                      interval='quarter', min_doc_count=1)\n        response = s.execute()\n\n        quarters = []\n        for quarter in response.aggregations[self.TIMEFRAME].buckets:\n            period = pandas.Period(quarter.key_as_string, 'Q')\n            quarters.append(period)\n\n        return quarters"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __list_uniques(self, date_range, field_name):\n        # Get project list\n        s = Search(using=self._es_conn, index=self._es_index)\n        s = s.filter('range', **date_range)\n        # from:to parameters (=> from: 0, size: 0)\n        s = s[0:0]\n        s.aggs.bucket('uniques', 'terms', field=field_name, size=1000)\n        response = s.execute()\n        uniques_list = []\n        for item in response.aggregations.uniques.buckets:\n            uniques_list.append(item.key)\n\n        return uniques_list", "response": "Retrieve a list of unique values in a given field within a given date range."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __build_dataframe(self, timing, project_name=None, org_name=None):\n        date_list = []\n        uuid_list = []\n        name_list = []\n        contribs_list = []\n        latest_ts_list = []\n        logger.debug(self.__log_prefix + \" timing: \" + timing.key_as_string)\n\n        for author in timing[self.AUTHOR_UUID].buckets:\n            latest_ts_list.append(timing[self.LATEST_TS].value_as_string)\n            date_list.append(timing.key_as_string)\n            uuid_list.append(author.key)\n            if author[self.AUTHOR_NAME] and author[self.AUTHOR_NAME].buckets \\\n                    and len(author[self.AUTHOR_NAME].buckets) > 0:\n                name_list.append(author[self.AUTHOR_NAME].buckets[0].key)\n            else:\n                name_list.append(\"Unknown\")\n            contribs_list.append(author[self.CONTRIBUTIONS].value)\n\n        df = pandas.DataFrame()\n        df[self.TIMEFRAME] = date_list\n        df[self.AUTHOR_UUID] = uuid_list\n        df[self.AUTHOR_NAME] = name_list\n        df[self.CONTRIBUTIONS] = contribs_list\n        df[self.TIMESTAMP] = latest_ts_list\n\n        if not project_name:\n            project_name = \"_Global_\"\n        df[self.PROJECT] = project_name\n\n        if not org_name:\n            org_name = \"_Global_\"\n        df[self.AUTHOR_ORG] = org_name\n\n        return df", "response": "Build a DataFrame from a time bucket."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses a DataFrame to compute Onion.", "response": "def process(self, items_block):\n        \"\"\"Process a DataFrame to compute Onion.\n\n        :param items_block: items to be processed. Expects to find a pandas DataFrame.\n        \"\"\"\n\n        logger.info(self.__log_prefix + \" Authors to process: \" + str(len(items_block)))\n\n        onion_enrich = Onion(items_block)\n        df_onion = onion_enrich.enrich(member_column=ESOnionConnector.AUTHOR_UUID,\n                                       events_column=ESOnionConnector.CONTRIBUTIONS)\n\n        # Get and store Quarter as String\n        df_onion['quarter'] = df_onion[ESOnionConnector.TIMEFRAME].map(lambda x: str(pandas.Period(x, 'Q')))\n\n        # Add metadata: enriched on timestamp\n        df_onion['metadata__enriched_on'] = datetime.utcnow().isoformat()\n        df_onion['data_source'] = self.data_source\n        df_onion['grimoire_creation_date'] = df_onion[ESOnionConnector.TIMEFRAME]\n\n        logger.info(self.__log_prefix + \" Final new events: \" + str(len(df_onion)))\n\n        return self.ProcessResults(processed=len(df_onion), out_items=df_onion)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the projects list from database", "response": "def get_projects(self):\n        \"\"\" Get the projects list from database \"\"\"\n\n        repos_list = []\n\n        gerrit_projects_db = self.projects_db\n\n        db = Database(user=\"root\", passwd=\"\", host=\"localhost\", port=3306,\n                      scrdb=None, shdb=gerrit_projects_db, prjdb=None)\n\n        sql = \"\"\"\n            SELECT DISTINCT(repository_name)\n            FROM project_repositories\n            WHERE data_source='scr'\n        \"\"\"\n\n        repos_list_raw = db.execute(sql)\n\n        # Convert from review.openstack.org_openstack/rpm-packaging-tools to\n        # openstack_rpm-packaging-tools\n        for repo in repos_list_raw:\n            # repo_name = repo[0].replace(\"review.openstack.org_\",\"\")\n            repo_name = repo[0].replace(self.repository + \"_\", \"\")\n            repos_list.append(repo_name)\n\n        return repos_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metadata(func):\n    @functools.wraps(func)\n    def decorator(self, *args, **kwargs):\n        eitem = func(self, *args, **kwargs)\n        metadata = {\n            'metadata__gelk_version': self.gelk_version,\n            'metadata__gelk_backend_name': self.__class__.__name__,\n            'metadata__enriched_on': datetime_utcnow().isoformat()\n        }\n        eitem.update(metadata)\n        return eitem\n    return decorator", "response": "Decorator that adds metadata to an item such as\n    the gelk revision used."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __convert_json_to_projects_map(self, json):\n        ds_repo_to_prj = {}\n\n        for project in json:\n            for ds in json[project]:\n                if ds == \"meta\":\n                    continue  # not a real data source\n                if ds not in ds_repo_to_prj:\n                    if ds not in ds_repo_to_prj:\n                        ds_repo_to_prj[ds] = {}\n                for repo in json[project][ds]:\n                    if repo in ds_repo_to_prj[ds]:\n                        if project == ds_repo_to_prj[ds][repo]:\n                            logger.debug(\"Duplicated repo: %s %s %s\", ds, repo, project)\n                        else:\n                            if len(project.split(\".\")) > len(ds_repo_to_prj[ds][repo].split(\".\")):\n                                logger.debug(\"Changed repo project because we found a leaf: %s leaf vs %s (%s, %s)\",\n                                             project, ds_repo_to_prj[ds][repo], repo, ds)\n                                ds_repo_to_prj[ds][repo] = project\n                    else:\n                        ds_repo_to_prj[ds][repo] = project\n        return ds_repo_to_prj", "response": "Convert the JSON format to the projects map"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef enrich_items(self, ocean_backend, events=False):\n\n        max_items = self.elastic.max_items_bulk\n        current = 0\n        total = 0\n        bulk_json = \"\"\n\n        items = ocean_backend.fetch()\n\n        url = self.elastic.index_url + '/items/_bulk'\n\n        logger.debug(\"Adding items to %s (in %i packs)\", self.elastic.anonymize_url(url), max_items)\n\n        if events:\n            logger.debug(\"Adding events items\")\n\n        for item in items:\n            if current >= max_items:\n                try:\n                    total += self.elastic.safe_put_bulk(url, bulk_json)\n                    json_size = sys.getsizeof(bulk_json) / (1024 * 1024)\n                    logger.debug(\"Added %i items to %s (%0.2f MB)\", total, self.elastic.anonymize_url(url), json_size)\n                except UnicodeEncodeError:\n                    # Why is requests encoding the POST data as ascii?\n                    logger.error(\"Unicode error in enriched items\")\n                    logger.debug(bulk_json)\n                    safe_json = str(bulk_json.encode('ascii', 'ignore'), 'ascii')\n                    total += self.elastic.safe_put_bulk(url, safe_json)\n                bulk_json = \"\"\n                current = 0\n\n            if not events:\n                rich_item = self.get_rich_item(item)\n                data_json = json.dumps(rich_item)\n                bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % \\\n                    (item[self.get_field_unique_id()])\n                bulk_json += data_json + \"\\n\"  # Bulk document\n                current += 1\n            else:\n                rich_events = self.get_rich_events(item)\n                for rich_event in rich_events:\n                    data_json = json.dumps(rich_event)\n                    bulk_json += '{\"index\" : {\"_id\" : \"%s_%s\" } }\\n' % \\\n                        (item[self.get_field_unique_id()],\n                         rich_event[self.get_field_event_unique_id()])\n                    bulk_json += data_json + \"\\n\"  # Bulk document\n                    current += 1\n\n        if current > 0:\n            total += self.elastic.safe_put_bulk(url, bulk_json)\n\n        return total", "response": "Enrich the items from the Ocean backend and add them to the Elasticsearch index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_grimoire_fields(self, creation_date, item_name):\n\n        grimoire_date = None\n        try:\n            grimoire_date = str_to_datetime(creation_date).isoformat()\n        except Exception as ex:\n            pass\n\n        name = \"is_\" + self.get_connector_name() + \"_\" + item_name\n\n        return {\n            \"grimoire_creation_date\": grimoire_date,\n            name: 1\n        }", "response": "Return common grimoire fields for all data sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_project_levels(cls, project):\n\n        eitem_path = ''\n        eitem_project_levels = {}\n\n        if project is not None:\n            subprojects = project.split('.')\n            for i in range(0, len(subprojects)):\n                if i > 0:\n                    eitem_path += \".\"\n                eitem_path += subprojects[i]\n                eitem_project_levels['project_' + str(i + 1)] = eitem_path\n\n        return eitem_project_levels", "response": "Add project sub levels extra items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_item_project(self, eitem):\n        # get the data source name relying on the cfg section name, if null use the connector name\n        ds_name = self.cfg_section_name if self.cfg_section_name else self.get_connector_name()\n\n        try:\n            # retrieve the project which includes the repo url in the projects.json,\n            # the variable `projects_json_repo` is passed from mordred to ELK when\n            # iterating over the repos in the projects.json, (see: param\n            # `projects_json_repo` in the functions elk.feed_backend and\n            # elk.enrich_backend)\n            if self.projects_json_repo:\n                project = self.prjs_map[ds_name][self.projects_json_repo]\n            # if `projects_json_repo`, which shouldn't never happen, use the\n            # method `get_project_repository` (defined in each enricher)\n            else:\n                repository = self.get_project_repository(eitem)\n                project = self.prjs_map[ds_name][repository]\n        # With the introduction of `projects_json_repo` the code in the\n        # except should be unreachable, and could be removed\n        except KeyError:\n            # logger.warning(\"Project not found for repository %s (data source: %s)\", repository, ds_name)\n            project = None\n\n            if self.filter_raw:\n                fltr = eitem['origin'] + ' --filter-raw=' + self.filter_raw\n                if ds_name in self.prjs_map and fltr in self.prjs_map[ds_name]:\n                    project = self.prjs_map[ds_name][fltr]\n\n            if project == UNKNOWN_PROJECT:\n                return None\n            if project:\n                return project\n\n            # Try to use always the origin in any case\n            if 'origin' in eitem:\n                if ds_name in self.prjs_map and eitem['origin'] in self.prjs_map[ds_name]:\n                    project = self.prjs_map[ds_name][eitem['origin']]\n                elif ds_name in self.prjs_map:\n                    # Try to find origin as part of the keys\n                    for ds_repo in self.prjs_map[ds_name]:\n                        ds_repo = str(ds_repo)  # discourse has category_id ints\n                        if eitem['origin'] in ds_repo:\n                            project = self.prjs_map[ds_name][ds_repo]\n                            break\n\n        if project == UNKNOWN_PROJECT:\n            project = None\n\n        return project", "response": "Find the project for a given enriched item."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the project name related to the eitem", "response": "def get_item_project(self, eitem):\n        \"\"\"\n        Get the project name related to the eitem\n        :param eitem: enriched item for which to find the project\n        :return: a dictionary with the project data\n        \"\"\"\n        eitem_project = {}\n        project = self.find_item_project(eitem)\n\n        if project is None:\n            project = DEFAULT_PROJECT\n\n        eitem_project = {\"project\": project}\n        # Time to add the project levels: eclipse.platform.releng.aggregator\n        eitem_project.update(self.add_project_levels(project))\n\n        # And now time to add the metadata\n        eitem_project.update(self.get_item_metadata(eitem))\n\n        return eitem_project"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the metadata for the item", "response": "def get_item_metadata(self, eitem):\n        \"\"\"\n        In the projects.json file, inside each project, there is a field called \"meta\" which has a\n        dictionary with fields to be added to the enriched items for this project.\n\n        This fields must be added with the prefix cm_ (custom metadata).\n\n        This method fetch the metadata fields for the project in which the eitem is included.\n\n        :param eitem: enriched item to search metadata for\n        :return: a dictionary with the metadata fields\n        \"\"\"\n\n        eitem_metadata = {}\n\n        # Get the project entry for the item, which includes the metadata\n        project = self.find_item_project(eitem)\n\n        if project and 'meta' in self.json_projects[project]:\n            meta_fields = self.json_projects[project]['meta']\n            if isinstance(meta_fields, dict):\n                eitem_metadata = {CUSTOM_META_PREFIX + \"_\" + field: value for field, value in meta_fields.items()}\n\n        return eitem_metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the domain from a SH identity", "response": "def get_domain(self, identity):\n        \"\"\" Get the domain from a SH identity \"\"\"\n        domain = None\n        if identity['email']:\n            try:\n                domain = identity['email'].split(\"@\")[1]\n            except IndexError:\n                # logger.warning(\"Bad email format: %s\" % (identity['email']))\n                pass\n        return domain"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the enrollment for the uuid when the item was done", "response": "def get_enrollment(self, uuid, item_date):\n        \"\"\" Get the enrollment for the uuid when the item was done \"\"\"\n        # item_date must be offset-naive (utc)\n        if item_date and item_date.tzinfo:\n            item_date = (item_date - item_date.utcoffset()).replace(tzinfo=None)\n\n        enrollments = self.get_enrollments(uuid)\n        enroll = self.unaffiliated_group\n        if enrollments:\n            for enrollment in enrollments:\n                if not item_date:\n                    enroll = enrollment.organization.name\n                    break\n                elif item_date >= enrollment.start and item_date <= enrollment.end:\n                    enroll = enrollment.organization.name\n                    break\n        return enroll"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a SH identity with all fields to empty_field", "response": "def __get_item_sh_fields_empty(self, rol, undefined=False):\n        \"\"\" Return a SH identity with all fields to empty_field \"\"\"\n        # If empty_field is None, the fields do not appear in index patterns\n        empty_field = '' if not undefined else '-- UNDEFINED --'\n        return {\n            rol + \"_id\": empty_field,\n            rol + \"_uuid\": empty_field,\n            rol + \"_name\": empty_field,\n            rol + \"_user_name\": empty_field,\n            rol + \"_domain\": empty_field,\n            rol + \"_gender\": empty_field,\n            rol + \"_gender_acc\": None,\n            rol + \"_org_name\": empty_field,\n            rol + \"_bot\": False\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the standard fields for a specific item in a given order", "response": "def get_item_sh_fields(self, identity=None, item_date=None, sh_id=None,\n                           rol='author'):\n        \"\"\" Get standard SH fields from a SH identity \"\"\"\n        eitem_sh = self.__get_item_sh_fields_empty(rol)\n\n        if identity:\n            # Use the identity to get the SortingHat identity\n            sh_ids = self.get_sh_ids(identity, self.get_connector_name())\n            eitem_sh[rol + \"_id\"] = sh_ids.get('id', '')\n            eitem_sh[rol + \"_uuid\"] = sh_ids.get('uuid', '')\n            eitem_sh[rol + \"_name\"] = identity.get('name', '')\n            eitem_sh[rol + \"_user_name\"] = identity.get('username', '')\n            eitem_sh[rol + \"_domain\"] = self.get_identity_domain(identity)\n        elif sh_id:\n            # Use the SortingHat id to get the identity\n            eitem_sh[rol + \"_id\"] = sh_id\n            eitem_sh[rol + \"_uuid\"] = self.get_uuid_from_id(sh_id)\n        else:\n            # No data to get a SH identity. Return an empty one.\n            return eitem_sh\n\n        # If the identity does not exists return and empty identity\n        if rol + \"_uuid\" not in eitem_sh or not eitem_sh[rol + \"_uuid\"]:\n            return self.__get_item_sh_fields_empty(rol, undefined=True)\n\n        # Get the SH profile to use first this data\n        profile = self.get_profile_sh(eitem_sh[rol + \"_uuid\"])\n\n        if profile:\n            # If name not in profile, keep its old value (should be empty or identity's name field value)\n            eitem_sh[rol + \"_name\"] = profile.get('name', eitem_sh[rol + \"_name\"])\n\n            email = profile.get('email', None)\n            if email:\n                eitem_sh[rol + \"_domain\"] = self.get_email_domain(email)\n\n            eitem_sh[rol + \"_gender\"] = profile.get('gender', self.unknown_gender)\n            eitem_sh[rol + \"_gender_acc\"] = profile.get('gender_acc', 0)\n\n        elif not profile and sh_id:\n            logger.warning(\"Can't find SH identity profile: %s\", sh_id)\n\n        # Ensure we always write gender fields\n        if not eitem_sh.get(rol + \"_gender\"):\n            eitem_sh[rol + \"_gender\"] = self.unknown_gender\n            eitem_sh[rol + \"_gender_acc\"] = 0\n\n        eitem_sh[rol + \"_org_name\"] = self.get_enrollment(eitem_sh[rol + \"_uuid\"], item_date)\n        eitem_sh[rol + \"_bot\"] = self.is_bot(eitem_sh[rol + '_uuid'])\n        return eitem_sh"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the enriched item dictionary.", "response": "def get_item_sh(self, item, roles=None, date_field=None):\n        \"\"\"\n        Add sorting hat enrichment fields for different roles\n\n        If there are no roles, just add the author fields.\n\n        \"\"\"\n\n        eitem_sh = {}  # Item enriched\n\n        author_field = self.get_field_author()\n\n        if not roles:\n            roles = [author_field]\n\n        if not date_field:\n            item_date = str_to_datetime(item[self.get_field_date()])\n        else:\n            item_date = str_to_datetime(item[date_field])\n\n        users_data = self.get_users_data(item)\n\n        for rol in roles:\n            if rol in users_data:\n                identity = self.get_sh_identity(item, rol)\n                eitem_sh.update(self.get_item_sh_fields(identity, item_date, rol=rol))\n\n                if not eitem_sh[rol + '_org_name']:\n                    eitem_sh[rol + '_org_name'] = SH_UNKNOWN_VALUE\n\n                if not eitem_sh[rol + '_name']:\n                    eitem_sh[rol + '_name'] = SH_UNKNOWN_VALUE\n\n                if not eitem_sh[rol + '_user_name']:\n                    eitem_sh[rol + '_user_name'] = SH_UNKNOWN_VALUE\n\n        # Add the author field common in all data sources\n        rol_author = 'author'\n        if author_field in users_data and author_field != rol_author:\n            identity = self.get_sh_identity(item, author_field)\n            eitem_sh.update(self.get_item_sh_fields(identity, item_date, rol=rol_author))\n\n            if not eitem_sh['author_org_name']:\n                eitem_sh['author_org_name'] = SH_UNKNOWN_VALUE\n\n            if not eitem_sh['author_name']:\n                eitem_sh['author_name'] = SH_UNKNOWN_VALUE\n\n            if not eitem_sh['author_user_name']:\n                eitem_sh['author_user_name'] = SH_UNKNOWN_VALUE\n\n        return eitem_sh"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_sh_ids(self, identity, backend_name):\n        # Convert the dict to tuple so it is hashable\n        identity_tuple = tuple(identity.items())\n        sh_ids = self.__get_sh_ids_cache(identity_tuple, backend_name)\n        return sh_ids", "response": "Return the Sorting Hat id and uuid for an identity"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_author_min_max_date(min_date, max_date, target_author, author_field=\"author_uuid\"):\n\n        es_query = '''\n        {\n          \"script\": {\n            \"source\":\n            \"ctx._source.demography_min_date = params.min_date;ctx._source.demography_max_date = params.max_date;\",\n            \"lang\": \"painless\",\n            \"params\": {\n                \"min_date\": \"%s\",\n                \"max_date\": \"%s\"\n            }\n          },\n          \"query\": {\n            \"term\": {\n              \"%s\": \"%s\"\n            }\n          }\n        }\n        ''' % (min_date, max_date, author_field, target_author)\n\n        return es_query", "response": "Get the query to update demography_min_date and demography_max_date of a given author."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse command line arguments", "response": "def get_params_parser():\n    \"\"\"Parse command line arguments\"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"-e\", \"--elastic_url\", default=\"http://127.0.0.1:9200\",\n                        help=\"Host with elastic search (default: http://127.0.0.1:9200)\")\n    parser.add_argument('-g', '--debug', dest='debug', action='store_true')\n    parser.add_argument('-t', '--token', dest='token', help=\"GitHub token\")\n    parser.add_argument('-o', '--org', dest='org', nargs='*', help='GitHub Organization/s to be analyzed')\n    parser.add_argument('-l', '--list', dest='list', action='store_true', help='Just list the repositories')\n    parser.add_argument('-n', '--nrepos', dest='nrepos', type=int, default=NREPOS,\n                        help='Number of GitHub repositories from the Organization to be analyzed (default:0, no limit)')\n    parser.add_argument('--db-projects-map', help=\"Database to include the projects Mapping DB\")\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_repository_filter_raw(self, term=False):\n        perceval_backend_name = self.get_connector_name()\n        filter_ = get_repository_filter(self.perceval_backend, perceval_backend_name, term)\n        return filter_", "response": "Returns the filter to be used in queries in a repository items"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters to be used when getting items from Ocean index", "response": "def set_filter_raw(self, filter_raw):\n        \"\"\"Filter to be used when getting items from Ocean index\"\"\"\n\n        self.filter_raw = filter_raw\n\n        self.filter_raw_dict = []\n        splitted = re.compile(FILTER_SEPARATOR).split(filter_raw)\n        for fltr_raw in splitted:\n            fltr = self.__process_filter(fltr_raw)\n\n            self.filter_raw_dict.append(fltr)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_filter_raw_should(self, filter_raw_should):\n\n        self.filter_raw_should = filter_raw_should\n\n        self.filter_raw_should_dict = []\n        splitted = re.compile(FILTER_SEPARATOR).split(filter_raw_should)\n        for fltr_raw in splitted:\n            fltr = self.__process_filter(fltr_raw)\n\n            self.filter_raw_should_dict.append(fltr)", "response": "Set the filter should to be used when getting items from Ocean index"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the items from raw or enriched index.", "response": "def fetch(self, _filter=None, ignore_incremental=False):\n        \"\"\" Fetch the items from raw or enriched index. An optional _filter\n        could be provided to filter the data collected \"\"\"\n\n        logger.debug(\"Creating a elastic items generator.\")\n\n        scroll_id = None\n        page = self.get_elastic_items(scroll_id, _filter=_filter, ignore_incremental=ignore_incremental)\n\n        if not page:\n            return []\n\n        scroll_id = page[\"_scroll_id\"]\n        scroll_size = page['hits']['total']\n\n        if scroll_size == 0:\n            logger.warning(\"No results found from %s\", self.elastic.anonymize_url(self.elastic.index_url))\n            return\n\n        while scroll_size > 0:\n\n            logger.debug(\"Fetching from %s: %d received\", self.elastic.anonymize_url(self.elastic.index_url),\n                         len(page['hits']['hits']))\n            for item in page['hits']['hits']:\n                eitem = item['_source']\n                yield eitem\n\n            page = self.get_elastic_items(scroll_id, _filter=_filter, ignore_incremental=ignore_incremental)\n\n            if not page:\n                break\n\n            scroll_size = len(page['hits']['hits'])\n\n        logger.debug(\"Fetching from %s: done receiving\", self.elastic.anonymize_url(self.elastic.index_url))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the items from the index related to the backend applying and_filter optional _scroll_id optional ignore_incremental", "response": "def get_elastic_items(self, elastic_scroll_id=None, _filter=None, ignore_incremental=False):\n        \"\"\" Get the items from the index related to the backend applying and\n        optional _filter if provided\"\"\"\n\n        headers = {\"Content-Type\": \"application/json\"}\n\n        if not self.elastic:\n            return None\n        url = self.elastic.index_url\n        # 1 minute to process the results of size items\n        # In gerrit enrich with 500 items per page we need >1 min\n        # In Mozilla ES in Amazon we need 10m\n        max_process_items_pack_time = \"10m\"  # 10 minutes\n        url += \"/_search?scroll=%s&size=%i\" % (max_process_items_pack_time,\n                                               self.scroll_size)\n\n        if elastic_scroll_id:\n            \"\"\" Just continue with the scrolling \"\"\"\n            url = self.elastic.url\n            url += \"/_search/scroll\"\n            scroll_data = {\n                \"scroll\": max_process_items_pack_time,\n                \"scroll_id\": elastic_scroll_id\n            }\n            query_data = json.dumps(scroll_data)\n        else:\n            # If using a perceval backends always filter by repository\n            # to support multi repository indexes\n            filters_dict = self.get_repository_filter_raw(term=True)\n            if filters_dict:\n                filters = json.dumps(filters_dict)\n            else:\n                filters = ''\n\n            if self.filter_raw:\n                for fltr in self.filter_raw_dict:\n                    filters += '''\n                        , {\"term\":\n                            { \"%s\":\"%s\"  }\n                        }\n                    ''' % (fltr['name'], fltr['value'])\n\n            if _filter:\n                filter_str = '''\n                    , {\"terms\":\n                        { \"%s\": %s }\n                    }\n                ''' % (_filter['name'], _filter['value'])\n                # List to string conversion uses ' that are not allowed in JSON\n                filter_str = filter_str.replace(\"'\", \"\\\"\")\n                filters += filter_str\n\n            # The code below performs the incremental enrichment based on the last value of `metadata__timestamp`\n            # in the enriched index, which is calculated in the TaskEnrich before enriching the single repos that\n            # belong to a given data source. The old implementation of the incremental enrichment, which consisted in\n            # collecting the last value of `metadata__timestamp` in the enriched index for each repo, didn't work\n            # for global data source (which are collected globally and only partially enriched).\n            if self.from_date and not ignore_incremental:\n                date_field = self.get_incremental_date()\n                from_date = self.from_date.isoformat()\n\n                filters += '''\n                    , {\"range\":\n                        {\"%s\": {\"gte\": \"%s\"}}\n                    }\n                ''' % (date_field, from_date)\n            elif self.offset and not ignore_incremental:\n                filters += '''\n                    , {\"range\":\n                        {\"offset\": {\"gte\": %i}}\n                    }\n                ''' % self.offset\n\n            # Order the raw items from the old ones to the new so if the\n            # enrich process fails, it could be resume incrementally\n            order_query = ''\n            order_field = None\n            if self.perceval_backend:\n                order_field = self.get_incremental_date()\n            if order_field is not None:\n                order_query = ', \"sort\": { \"%s\": { \"order\": \"asc\" }} ' % order_field\n\n            filters_should = ''\n            if self.filter_raw_should:\n                for fltr in self.filter_raw_should_dict:\n                    filters_should += '''\n                        {\"prefix\":\n                            { \"%s\":\"%s\"  }\n                        },''' % (fltr['name'], fltr['value'])\n\n                filters_should = filters_should.rstrip(',')\n                query_should = '{\"bool\": {\"should\": [%s]}}' % filters_should\n                filters += \", \" + query_should\n\n            # Fix the filters string if it starts with \",\" (empty first filter)\n            if filters.lstrip().startswith(','):\n                filters = filters.lstrip()[1:]\n\n            query = \"\"\"\n            {\n                \"query\": {\n                    \"bool\": {\n                        \"filter\": [%s]\n                    }\n                } %s\n            }\n            \"\"\" % (filters, order_query)\n\n            logger.debug(\"Raw query to %s\\n%s\", self.elastic.anonymize_url(url),\n                         json.dumps(json.loads(query), indent=4))\n            query_data = query\n\n        rjson = None\n        try:\n            res = self.requests.post(url, data=query_data, headers=headers)\n            res.raise_for_status()\n            rjson = res.json()\n        except Exception:\n            # The index could not exists yet or it could be empty\n            logger.warning(\"No results found from %s\", self.elastic.anonymize_url(url))\n\n        return rjson"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the unique identifier field for a given index", "response": "def find_uuid(es_url, index):\n    \"\"\" Find the unique identifier field for a given index \"\"\"\n\n    uid_field = None\n\n    # Get the first item to detect the data source and raw/enriched type\n    res = requests.get('%s/%s/_search?size=1' % (es_url, index))\n    first_item = res.json()['hits']['hits'][0]['_source']\n    fields = first_item.keys()\n\n    if 'uuid' in fields:\n        uid_field = 'uuid'\n    else:\n        # Non perceval backend\n        uuid_value = res.json()['hits']['hits'][0]['_id']\n        logging.debug(\"Finding unique id for %s with value %s\", index, uuid_value)\n        for field in fields:\n            if first_item[field] == uuid_value:\n                logging.debug(\"Found unique id for %s: %s\", index, field)\n                uid_field = field\n                break\n\n    if not uid_field:\n        logging.error(\"Can not find uid field for %s. Can not copy the index.\", index)\n        logging.error(\"Try to copy it directly with elasticdump or similar.\")\n        sys.exit(1)\n\n    return uid_field"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the mapping given an index", "response": "def find_mapping(es_url, index):\n    \"\"\" Find the mapping given an index \"\"\"\n\n    mapping = None\n\n    backend = find_perceval_backend(es_url, index)\n\n    if backend:\n        mapping = backend.get_elastic_mappings()\n\n    if mapping:\n        logging.debug(\"MAPPING FOUND:\\n%s\", json.dumps(json.loads(mapping['items']), indent=True))\n    return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the items from the index", "response": "def get_elastic_items(elastic, elastic_scroll_id=None, limit=None):\n    \"\"\" Get the items from the index \"\"\"\n\n    scroll_size = limit\n    if not limit:\n        scroll_size = DEFAULT_LIMIT\n\n    if not elastic:\n        return None\n\n    url = elastic.index_url\n    max_process_items_pack_time = \"5m\"  # 10 minutes\n    url += \"/_search?scroll=%s&size=%i\" % (max_process_items_pack_time,\n                                           scroll_size)\n\n    if elastic_scroll_id:\n        # Just continue with the scrolling\n        url = elastic.url\n        url += \"/_search/scroll\"\n        scroll_data = {\n            \"scroll\": max_process_items_pack_time,\n            \"scroll_id\": elastic_scroll_id\n        }\n        res = requests.post(url, data=json.dumps(scroll_data))\n    else:\n        query = \"\"\"\n        {\n            \"query\": {\n                \"bool\": {\n                    \"must\": []\n                }\n            }\n        }\n        \"\"\"\n\n        logging.debug(\"%s\\n%s\", url, json.dumps(json.loads(query), indent=4))\n        res = requests.post(url, data=query)\n\n    rjson = None\n    try:\n        rjson = res.json()\n    except Exception:\n        logging.error(\"No JSON found in %s\", res.text)\n        logging.error(\"No results found from %s\", url)\n\n    return rjson"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the items from the index using search after scrolling", "response": "def get_elastic_items_search(elastic, search_after=None, size=None):\n    \"\"\" Get the items from the index using search after scrolling \"\"\"\n\n    if not size:\n        size = DEFAULT_LIMIT\n\n    url = elastic.index_url + \"/_search\"\n\n    search_after_query = ''\n\n    if search_after:\n        logging.debug(\"Search after: %s\", search_after)\n        # timestamp uuid\n        search_after_query = ', \"search_after\": [%i, \"%s\"] ' % (search_after[0], search_after[1])\n\n    query = \"\"\"\n    {\n        \"size\": %i,\n        \"query\": {\n            \"bool\": {\n                \"must\": []\n            }\n        },\n        \"sort\": [\n            {\"metadata__timestamp\": \"asc\"},\n            {\"uuid\": \"asc\"}\n        ] %s\n\n    }\n    \"\"\" % (size, search_after_query)\n\n    # logging.debug(\"%s\\n%s\", url, json.dumps(json.loads(query), indent=4))\n    res = requests.post(url, data=query)\n\n    rjson = None\n    try:\n        rjson = res.json()\n    except Exception:\n        logging.error(\"No JSON found in %s\", res.text)\n        logging.error(\"No results found from %s\", url)\n\n    return rjson"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch(elastic, backend, limit=None, search_after_value=None, scroll=True):\n\n    logging.debug(\"Creating a elastic items generator.\")\n\n    elastic_scroll_id = None\n    search_after = search_after_value\n\n    while True:\n        if scroll:\n            rjson = get_elastic_items(elastic, elastic_scroll_id, limit)\n        else:\n            rjson = get_elastic_items_search(elastic, search_after, limit)\n\n        if rjson and \"_scroll_id\" in rjson:\n            elastic_scroll_id = rjson[\"_scroll_id\"]\n\n        if rjson and \"hits\" in rjson:\n            if not rjson[\"hits\"][\"hits\"]:\n                break\n            for hit in rjson[\"hits\"][\"hits\"]:\n                item = hit['_source']\n                if 'sort' in hit:\n                    search_after = hit['sort']\n                try:\n                    backend._fix_item(item)\n                except Exception:\n                    pass\n                yield item\n        else:\n            logging.error(\"No results found from %s\", elastic.index_url)\n            break\n\n    return", "response": "Fetch the items from raw or enriched index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_items(elastic_url, in_index, out_index, elastic_url_out=None,\n                 search_after=False, search_after_value=None, limit=None,\n                 copy=False):\n    \"\"\" Export items from in_index to out_index using the correct mapping \"\"\"\n\n    if not limit:\n        limit = DEFAULT_LIMIT\n\n    if search_after_value:\n        search_after_value_timestamp = int(search_after_value[0])\n        search_after_value_uuid = search_after_value[1]\n        search_after_value = [search_after_value_timestamp, search_after_value_uuid]\n\n    logging.info(\"Exporting items from %s/%s to %s\", elastic_url, in_index, out_index)\n\n    count_res = requests.get('%s/%s/_count' % (elastic_url, in_index))\n    try:\n        count_res.raise_for_status()\n    except requests.exceptions.HTTPError:\n        if count_res.status_code == 404:\n            logging.error(\"The index does not exists: %s\", in_index)\n        else:\n            logging.error(count_res.text)\n        sys.exit(1)\n\n    logging.info(\"Total items to copy: %i\", count_res.json()['count'])\n\n    # Time to upload the items with the correct mapping\n    elastic_in = ElasticSearch(elastic_url, in_index)\n    if not copy:\n        # Create the correct mapping for the data sources detected from in_index\n        ds_mapping = find_mapping(elastic_url, in_index)\n    else:\n        logging.debug('Using the input index mapping')\n        ds_mapping = extract_mapping(elastic_url, in_index)\n\n    if not elastic_url_out:\n        elastic_out = ElasticSearch(elastic_url, out_index, mappings=ds_mapping)\n    else:\n        elastic_out = ElasticSearch(elastic_url_out, out_index, mappings=ds_mapping)\n\n    # Time to just copy from in_index to our_index\n    uid_field = find_uuid(elastic_url, in_index)\n    backend = find_perceval_backend(elastic_url, in_index)\n    if search_after:\n        total = elastic_out.bulk_upload(fetch(elastic_in, backend, limit,\n                                              search_after_value, scroll=False), uid_field)\n    else:\n        total = elastic_out.bulk_upload(fetch(elastic_in, backend, limit), uid_field)\n\n    logging.info(\"Total items copied: %i\", total)", "response": "Export items from in_index to out_index using the correct mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_identities(self, item):\n\n        item = item['data']\n\n        # Changeset owner\n        user = item['owner']\n        identity = self.get_sh_identity(user)\n        yield identity\n\n        # Patchset uploader and author\n        if 'patchSets' in item:\n            for patchset in item['patchSets']:\n                user = patchset['uploader']\n                identity = self.get_sh_identity(user)\n                yield identity\n                if 'author' in patchset:\n                    user = patchset['author']\n                    identity = self.get_sh_identity(user)\n                    yield identity\n                if 'approvals' in patchset:\n                    # Approvals by\n                    for approval in patchset['approvals']:\n                        user = approval['by']\n                        identity = self.get_sh_identity(user)\n                        yield identity\n\n        # Comments reviewers\n        if 'comments' in item:\n            for comment in item['comments']:\n                user = comment['reviewer']\n                identity = self.get_sh_identity(user)\n                yield identity", "response": "Return the identities from an item"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting dates so ES detect them", "response": "def _fix_review_dates(self, item):\n        \"\"\"Convert dates so ES detect them\"\"\"\n\n        for date_field in ['timestamp', 'createdOn', 'lastUpdated']:\n            if date_field in item.keys():\n                date_ts = item[date_field]\n                item[date_field] = unixtime_to_datetime(date_ts).isoformat()\n\n        if 'patchSets' in item.keys():\n            for patch in item['patchSets']:\n                pdate_ts = patch['createdOn']\n                patch['createdOn'] = unixtime_to_datetime(pdate_ts).isoformat()\n\n                if 'approvals' in patch:\n                    for approval in patch['approvals']:\n                        adate_ts = approval['grantedOn']\n                        approval['grantedOn'] = unixtime_to_datetime(adate_ts).isoformat()\n\n        if 'comments' in item.keys():\n            for comment in item['comments']:\n                cdate_ts = comment['timestamp']\n                comment['timestamp'] = unixtime_to_datetime(cdate_ts).isoformat()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Sorting Hat identity using Bugzilla user data", "response": "def get_sh_identity(self, item, identity_field=None):\n        \"\"\" Return a Sorting Hat identity using bugzilla user data \"\"\"\n\n        def fill_list_identity(identity, user_list_data):\n            \"\"\" Fill identity with user data in first item in list \"\"\"\n            identity['username'] = user_list_data[0]['__text__']\n            if '@' in identity['username']:\n                identity['email'] = identity['username']\n            if 'name' in user_list_data[0]:\n                identity['name'] = user_list_data[0]['name']\n            return identity\n\n        identity = {}\n        for field in ['name', 'email', 'username']:\n            # Basic fields in Sorting Hat\n            identity[field] = None\n\n        user = item  # by default a specific user dict is used\n        if 'data' in item and type(item) == dict:\n            user = item['data'][identity_field]\n\n        identity = fill_list_identity(identity, user)\n\n        return identity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\"Return the identities from an item\"\"\"\n\n        for rol in self.roles:\n            if rol in item['data']:\n                user = self.get_sh_identity(item[\"data\"][rol])\n                yield user\n\n        if 'activity' in item[\"data\"]:\n            for event in item[\"data\"]['activity']:\n                event_user = [{\"__text__\": event['Who']}]\n                user = self.get_sh_identity(event_user)\n                yield user\n\n        if 'long_desc' in item[\"data\"]:\n            for comment in item[\"data\"]['long_desc']:\n                user = self.get_sh_identity(comment['who'])\n                yield user"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating an enriched index by processing input items in blocks.", "response": "def analyze(self):\n        \"\"\"Populate an enriched index by processing input items in blocks.\n\n        :return: total number of out_items written.\n        \"\"\"\n        from_date = self._out.latest_date()\n        if from_date:\n            logger.info(\"Reading items since \" + from_date)\n        else:\n            logger.info(\"Reading items since the beginning of times\")\n\n        cont = 0\n        total_processed = 0\n        total_written = 0\n\n        for item_block in self._in.read_block(size=self._block_size, from_date=from_date):\n            cont = cont + len(item_block)\n\n            process_results = self.process(item_block)\n            total_processed += process_results.processed\n\n            if len(process_results.out_items) > 0:\n                self._out.write(process_results.out_items)\n                total_written += len(process_results.out_items)\n            else:\n                logger.info(\"No new items to be written this time.\")\n\n            logger.info(\n                \"Items read/to be written/total read/total processed/total written: \"\n                \"{0}/{1}/{2}/{3}/{4}\".format(str(len(item_block)),\n                                             str(len(process_results.out_items)),\n                                             str(cont),\n                                             str(total_processed),\n                                             str(total_written)))\n\n        logger.info(\"SUMMARY: Items total read/total processed/total written: \"\n                    \"{0}/{1}/{2}\".format(str(cont),\n                                         str(total_processed),\n                                         str(total_written)))\n\n        logger.info(\"This is the end.\")\n\n        return total_written"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading items and return them one by one.", "response": "def read_item(self, from_date=None):\n        \"\"\"Read items and return them one by one.\n\n        :param from_date: start date for incremental reading.\n        :return: next single item when any available.\n        :raises ValueError: `metadata__timestamp` field not found in index\n        :raises NotFoundError: index not found in ElasticSearch\n        \"\"\"\n        search_query = self._build_search_query(from_date)\n        for hit in helpers.scan(self._es_conn,\n                                search_query,\n                                scroll='300m',\n                                index=self._es_index,\n                                preserve_order=True):\n            yield hit"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads items and return them in blocks.", "response": "def read_block(self, size, from_date=None):\n        \"\"\"Read items and return them in blocks.\n\n        :param from_date: start date for incremental reading.\n        :param size: block size.\n        :return: next block of items when any available.\n        :raises ValueError: `metadata__timestamp` field not found in index\n        :raises NotFoundError: index not found in ElasticSearch\n        \"\"\"\n        search_query = self._build_search_query(from_date)\n        hits_block = []\n        for hit in helpers.scan(self._es_conn,\n                                search_query,\n                                scroll='300m',\n                                index=self._es_index,\n                                preserve_order=True):\n\n            hits_block.append(hit)\n\n            if len(hits_block) % size == 0:\n                yield hits_block\n\n                # Reset hits block\n                hits_block = []\n\n        if len(hits_block) > 0:\n            yield hits_block"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, items):\n        if self._read_only:\n            raise IOError(\"Cannot write, Connector created as Read Only\")\n\n        # Uploading info to the new ES\n        docs = []\n        for item in items:\n            doc = {\n                \"_index\": self._es_index,\n                \"_type\": \"item\",\n                \"_id\": item[\"_id\"],\n                \"_source\": item[\"_source\"]\n            }\n            docs.append(doc)\n        # TODO exception and error handling\n        helpers.bulk(self._es_conn, docs)\n        logger.info(self.__log_prefix + \" Written: \" + str(len(docs)))", "response": "Uploads items to ElasticSearch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_index(self, mappings_file, delete=True):\n\n        if self._read_only:\n            raise IOError(\"Cannot write, Connector created as Read Only\")\n\n        if delete:\n            logger.info(self.__log_prefix + \" Deleting index \" + self._es_index)\n            self._es_conn.indices.delete(self._es_index, ignore=[400, 404])\n\n        # Read Mapping\n        with open(mappings_file) as f:\n            mapping = f.read()\n\n        self._es_conn.indices.create(self._es_index, body=mapping)", "response": "Create a new index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an alias pointing to the index configured in this connection", "response": "def create_alias(self, alias_name):\n        \"\"\"Creates an alias pointing to the index configured in this connection\"\"\"\n\n        return self._es_conn.indices.put_alias(index=self._es_index, name=alias_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef exists_alias(self, alias_name, index_name=None):\n\n        return self._es_conn.indices.exists_alias(index=index_name, name=alias_name)", "response": "Check whether or not the given alias exists"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding an ElasticSearch search query to retrieve items for read methods.", "response": "def _build_search_query(self, from_date):\n        \"\"\"Build an ElasticSearch search query to retrieve items for read methods.\n\n        :param from_date: date to start retrieving items from.\n        :return: JSON query in dict format\n        \"\"\"\n\n        sort = [{self._sort_on_field: {\"order\": \"asc\"}}]\n\n        filters = []\n        if self._repo:\n            filters.append({\"term\": {\"origin\": self._repo}})\n\n        if from_date:\n            filters.append({\"range\": {self._sort_on_field: {\"gte\": from_date}}})\n\n        if filters:\n            query = {\"bool\": {\"filter\": filters}}\n        else:\n            query = {\"match_all\": {}}\n\n        search_query = {\n            \"query\": query,\n            \"sort\": sort\n        }\n\n        return search_query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self, items_block):\n\n        out_items = []\n\n        for hit in items_block:\n            if __name__ == '__main__':\n                hit['_source']['metadata__enriched_on'] = datetime.datetime_utcnow().isoformat()\n            out_items.append(hit)\n\n        return self.ProcessResults(processed=0, out_items=out_items)", "response": "Process the items in the items_block and update their metadata__enriched_on field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the arthur params given a URL for the data source", "response": "def get_arthur_params_from_url(cls, url):\n        \"\"\" Get the arthur params given a URL for the data source \"\"\"\n        params = {}\n\n        args = cls.get_perceval_params_from_url(url)\n        parser = GitLabCommand.setup_cmd_parser()\n\n        parsed_args = parser.parse(*args)\n\n        params['owner'] = parsed_args.owner\n        params['repository'] = parsed_args.repository\n        # include only blacklist ids information\n        params['blacklist_ids'] = parsed_args.blacklist_ids\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_perceval_params_from_url(cls, url):\n        params = []\n\n        tokens = url.split(' ')\n        repo = tokens[0]\n\n        owner = repo.split('/')[-2]\n        repository = repo.split('/')[-1]\n\n        params.append(owner)\n        params.append(repository)\n\n        if len(tokens) > 1:\n            params.extend(tokens[1:])\n\n        return params", "response": "Get the perceval params given a URL for the data source"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the identities from an item", "response": "def get_identities(self, item):\n        ''' Return the identities from an item '''\n\n        item = item['data']\n\n        # Creators\n        if 'event_hosts' in item:\n            user = self.get_sh_identity(item['event_hosts'][0])\n            yield user\n\n        # rsvps\n        rsvps = item.get('rsvps', [])\n\n        for rsvp in rsvps:\n            user = self.get_sh_identity(rsvp['member'])\n            yield user\n\n        # Comments\n        for comment in item['comments']:\n            user = self.get_sh_identity(comment['member'])\n            yield user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_item_sh(self, item):\n\n        sh_fields = {}\n\n        # Not shared common get_item_sh because it is pretty specific\n        if 'member' in item:\n            # comment and rsvp\n            identity = self.get_sh_identity(item['member'])\n        elif 'event_hosts' in item:\n            # meetup event\n            identity = self.get_sh_identity(item['event_hosts'][0])\n        else:\n            return sh_fields\n\n        created = unixtime_to_datetime(item['created'] / 1000)\n        sh_fields = self.get_item_sh_fields(identity, created)\n\n        return sh_fields", "response": "Add sorting hat enrichment fields"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding shared params in all backends", "response": "def add_params(cls, cmdline_parser):\n        \"\"\" Shared params in all backends \"\"\"\n\n        parser = cmdline_parser\n\n        parser.add_argument(\"-e\", \"--elastic_url\", default=\"http://127.0.0.1:9200\",\n                            help=\"Host with elastic search (default: http://127.0.0.1:9200)\")\n        parser.add_argument(\"--elastic_url-enrich\",\n                            help=\"Host with elastic search and enriched indexes\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_p2o_params_from_url(cls, url):\n\n        # if the url doesn't contain a filter separator, return it\n        if PRJ_JSON_FILTER_SEPARATOR not in url:\n            return {\"url\": url}\n\n        # otherwise, add the url to the params\n        params = {'url': url.split(' ', 1)[0]}\n        # tokenize the filter and add them to the param dict\n        tokens = url.split(PRJ_JSON_FILTER_SEPARATOR)[1:]\n\n        if len(tokens) > 1:\n            cause = \"Too many filters defined for %s, only the first one is considered\" % url\n            logger.warning(cause)\n\n        token = tokens[0]\n        filter_tokens = token.split(PRJ_JSON_FILTER_OP_ASSIGNMENT)\n\n        if len(filter_tokens) != 2:\n            cause = \"Too many tokens after splitting for %s in %s\" % (token, url)\n            logger.error(cause)\n            raise ELKError(cause=cause)\n\n        fltr_name = filter_tokens[0].strip()\n        fltr_value = filter_tokens[1].strip()\n\n        params['filter-' + fltr_name] = fltr_value\n\n        return params", "response": "Get the p2o params given a URL for the data source"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_update_date(self, item):\n        updated = unixtime_to_datetime(item['updated_on'])\n        timestamp = unixtime_to_datetime(item['timestamp'])\n        item['metadata__updated_on'] = updated.isoformat()\n        # Also add timestamp used in incremental enrichment\n        item['metadata__timestamp'] = timestamp.isoformat()", "response": "Add the updated_on and timestamp to the item metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfeeding data in Elastic from Perceval or Arthur", "response": "def feed(self, from_date=None, from_offset=None, category=None,\n             latest_items=None, arthur_items=None, filter_classified=None):\n        \"\"\" Feed data in Elastic from Perceval or Arthur \"\"\"\n\n        if self.fetch_archive:\n            items = self.perceval_backend.fetch_from_archive()\n            self.feed_items(items)\n            return\n        elif arthur_items:\n            items = arthur_items\n            self.feed_items(items)\n            return\n\n        if from_date and from_offset:\n            raise RuntimeError(\"Can't not feed using from_date and from_offset.\")\n\n        # We need to filter by repository to support several repositories\n        # in the same raw index\n        filters_ = [get_repository_filter(self.perceval_backend,\n                    self.get_connector_name())]\n\n        # Check if backend supports from_date\n        signature = inspect.signature(self.perceval_backend.fetch)\n\n        last_update = None\n        if 'from_date' in signature.parameters:\n            if from_date:\n                last_update = from_date\n            else:\n                self.last_update = self.get_last_update_from_es(filters_=filters_)\n                last_update = self.last_update\n\n            logger.info(\"Incremental from: %s\", last_update)\n\n        offset = None\n        if 'offset' in signature.parameters:\n            if from_offset:\n                offset = from_offset\n            else:\n                offset = self.elastic.get_last_offset(\"offset\", filters_=filters_)\n\n            if offset is not None:\n                logger.info(\"Incremental from: %i offset\", offset)\n            else:\n                logger.info(\"Not incremental\")\n\n        params = {}\n        # category and filter_classified params are shared\n        # by all Perceval backends\n        if category is not None:\n            params['category'] = category\n        if filter_classified is not None:\n            params['filter_classified'] = filter_classified\n\n        # latest items, from_date and offset cannot be used together,\n        # thus, the params dictionary is filled with the param available\n        # and Perceval is executed\n        if latest_items:\n            params['latest_items'] = latest_items\n            items = self.perceval_backend.fetch(**params)\n        elif last_update:\n            last_update = last_update.replace(tzinfo=None)\n            params['from_date'] = last_update\n            items = self.perceval_backend.fetch(**params)\n        elif offset is not None:\n            params['offset'] = offset\n            items = self.perceval_backend.fetch(**params)\n        else:\n            items = self.perceval_backend.fetch(**params)\n\n        self.feed_items(items)\n        self.update_items()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends items JSON to ES", "response": "def _items_to_es(self, json_items):\n        \"\"\" Append items JSON to ES (data source state) \"\"\"\n\n        if len(json_items) == 0:\n            return\n\n        logger.info(\"Adding items to Ocean for %s (%i items)\" %\n                    (self, len(json_items)))\n\n        field_id = self.get_field_unique_id()\n\n        inserted = self.elastic.bulk_upload(json_items, field_id)\n\n        if len(json_items) != inserted:\n            missing = len(json_items) - inserted\n            info = json_items[0]\n\n            name = info['backend_name']\n            version = info['backend_version']\n            origin = info['origin']\n\n            logger.warning(\"%s/%s missing JSON items for backend %s [ver. %s], origin %s\",\n                           str(missing),\n                           str(len(json_items)),\n                           name, version, origin)\n\n        return inserted"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the identities from an item.", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item.\n            If the repo is in GitHub, get the usernames from GitHub. \"\"\"\n\n        def add_sh_github_identity(user, user_field, rol):\n            \"\"\" Add a new github identity to SH if it does not exists \"\"\"\n            github_repo = None\n            if GITHUB in item['origin']:\n                github_repo = item['origin'].replace(GITHUB, '')\n                github_repo = re.sub('.git$', '', github_repo)\n            if not github_repo:\n                return\n\n            # Try to get the identity from SH\n            user_data = item['data'][user_field]\n            sh_identity = SortingHat.get_github_commit_username(self.sh_db, user, SH_GIT_COMMIT)\n            if not sh_identity:\n                # Get the usename from GitHub\n                gh_username = self.get_github_login(user_data, rol, commit_hash, github_repo)\n                # Create a new SH identity with name, email from git and username from github\n                logger.debug(\"Adding new identity %s to SH %s: %s\", gh_username, SH_GIT_COMMIT, user)\n                user = self.get_sh_identity(user_data)\n                user['username'] = gh_username\n                SortingHat.add_identity(self.sh_db, user, SH_GIT_COMMIT)\n            else:\n                if user_data not in self.github_logins:\n                    self.github_logins[user_data] = sh_identity['username']\n                    logger.debug(\"GitHub-commit exists. username:%s user:%s\",\n                                 sh_identity['username'], user_data)\n\n        commit_hash = item['data']['commit']\n\n        if item['data']['Author']:\n            # Check multi authors commits\n            m = self.AUTHOR_P2P_REGEX.match(item['data'][\"Author\"])\n            n = self.AUTHOR_P2P_NEW_REGEX.match(item['data'][\"Author\"])\n            if (m or n) and self.pair_programming:\n                authors = self.__get_authors(item['data'][\"Author\"])\n                for author in authors:\n                    user = self.get_sh_identity(author)\n                    yield user\n            else:\n                user = self.get_sh_identity(item['data'][\"Author\"])\n                yield user\n                if self.github_token:\n                    add_sh_github_identity(user, 'Author', 'author')\n        if item['data']['Commit']:\n            m = self.AUTHOR_P2P_REGEX.match(item['data'][\"Commit\"])\n            n = self.AUTHOR_P2P_NEW_REGEX.match(item['data'][\"Author\"])\n            if (m or n) and self.pair_programming:\n                committers = self.__get_authors(item['data']['Commit'])\n                for committer in committers:\n                    user = self.get_sh_identity(committer)\n                    yield user\n            else:\n                user = self.get_sh_identity(item['data']['Commit'])\n                yield user\n                if self.github_token:\n                    add_sh_github_identity(user, 'Commit', 'committer')\n        if 'Signed-off-by' in item['data'] and self.pair_programming:\n            signers = item['data'][\"Signed-off-by\"]\n            for signer in signers:\n                user = self.get_sh_identity(signer)\n                yield user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_github_login(self, user, rol, commit_hash, repo):\n        login = None\n        try:\n            login = self.github_logins[user]\n        except KeyError:\n            # Get the login from github API\n            GITHUB_API_URL = \"https://api.github.com\"\n            commit_url = GITHUB_API_URL + \"/repos/%s/commits/%s\" % (repo, commit_hash)\n            headers = {'Authorization': 'token ' + self.github_token}\n\n            r = self.requests.get(commit_url, headers=headers)\n\n            try:\n                r.raise_for_status()\n            except requests.exceptions.ConnectionError as ex:\n                # Connection error\n                logger.error(\"Can't get github login for %s in %s because a connection error \", repo, commit_hash)\n                return login\n\n            self.rate_limit = int(r.headers['X-RateLimit-Remaining'])\n            self.rate_limit_reset_ts = int(r.headers['X-RateLimit-Reset'])\n            logger.debug(\"Rate limit pending: %s\", self.rate_limit)\n            if self.rate_limit <= self.min_rate_to_sleep:\n                seconds_to_reset = self.rate_limit_reset_ts - int(time.time()) + 1\n                if seconds_to_reset < 0:\n                    seconds_to_reset = 0\n                cause = \"GitHub rate limit exhausted.\"\n                logger.info(\"%s Waiting %i secs for rate limit reset.\", cause, seconds_to_reset)\n                time.sleep(seconds_to_reset)\n                # Retry once we have rate limit\n                r = self.requests.get(commit_url, headers=headers)\n\n            try:\n                r.raise_for_status()\n            except requests.exceptions.HTTPError as ex:\n                # commit not found probably or rate limit exhausted\n                logger.error(\"Can't find commit %s %s\", commit_url, ex)\n                return login\n\n            commit_json = r.json()\n            author_login = None\n            if 'author' in commit_json and commit_json['author']:\n                author_login = commit_json['author']['login']\n            else:\n                self.github_logins_author_not_found += 1\n\n            user_login = None\n            if 'committer' in commit_json and commit_json['committer']:\n                user_login = commit_json['committer']['login']\n            else:\n                self.github_logins_committer_not_found += 1\n\n            if rol == \"author\":\n                login = author_login\n            elif rol == \"committer\":\n                login = user_login\n            else:\n                logger.error(\"Wrong rol: %s\" % (rol))\n                raise RuntimeError\n\n            self.github_logins[user] = login\n            logger.debug(\"%s is %s in github (not found %i authors %i committers )\", user, login,\n                         self.github_logins_author_not_found,\n                         self.github_logins_committer_not_found)\n\n        return login", "response": "Get the login for a github user and commit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __fix_field_date(self, item, attribute):\n\n        field_date = str_to_datetime(item[attribute])\n\n        try:\n            _ = int(field_date.strftime(\"%z\")[0:3])\n        except ValueError:\n            logger.warning(\"%s in commit %s has a wrong format\", attribute, item['commit'])\n            item[attribute] = field_date.replace(tzinfo=None).isoformat()", "response": "Fix possible errors in the field date"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding items to the index.", "response": "def enrich_items(self, ocean_backend, events=False):\n        \"\"\" Implementation supporting signed-off and multiauthor/committer commits.\"\"\"\n\n        headers = {\"Content-Type\": \"application/json\"}\n\n        max_items = self.elastic.max_items_bulk\n        current = 0\n        total = 0\n        bulk_json = \"\"\n\n        total_signed_off = 0\n        total_multi_author = 0\n\n        url = self.elastic.index_url + '/items/_bulk'\n\n        logger.debug(\"Adding items to %s (in %i packs)\", self.elastic.anonymize_url(url), max_items)\n\n        items = ocean_backend.fetch()\n\n        for item in items:\n            if self.pair_programming:\n                # First we need to add the authors field to all commits\n                # Check multi author\n                m = self.AUTHOR_P2P_REGEX.match(item['data']['Author'])\n                n = self.AUTHOR_P2P_NEW_REGEX.match(item['data']['Author'])\n                if m or n:\n                    logger.debug(\"Multiauthor detected. Creating one commit \"\n                                 \"per author: %s\", item['data']['Author'])\n                    item['data']['authors'] = self.__get_authors(item['data']['Author'])\n                    item['data']['Author'] = item['data']['authors'][0]\n                m = self.AUTHOR_P2P_REGEX.match(item['data']['Commit'])\n                n = self.AUTHOR_P2P_NEW_REGEX.match(item['data']['Author'])\n                if m or n:\n                    logger.debug(\"Multicommitter detected: using just the first committer\")\n                    item['data']['committers'] = self.__get_authors(item['data']['Commit'])\n                    item['data']['Commit'] = item['data']['committers'][0]\n                # Add the authors list using the original Author and the Signed-off list\n                if 'Signed-off-by' in item['data']:\n                    authors_all = item['data']['Signed-off-by'] + [item['data']['Author']]\n                    item['data']['authors_signed_off'] = list(set(authors_all))\n\n            if current >= max_items:\n                try:\n                    total += self.elastic.safe_put_bulk(url, bulk_json)\n                    json_size = sys.getsizeof(bulk_json) / (1024 * 1024)\n                    logger.debug(\"Added %i items to %s (%0.2f MB)\", total, self.elastic.anonymize_url(url), json_size)\n                except UnicodeEncodeError:\n                    # Why is requests encoding the POST data as ascii?\n                    logger.error(\"Unicode error in enriched items\")\n                    logger.debug(bulk_json)\n                    safe_json = str(bulk_json.encode('ascii', 'ignore'), 'ascii')\n                    total += self.elastic.safe_put_bulk(url, safe_json)\n                bulk_json = \"\"\n                current = 0\n\n            rich_item = self.get_rich_item(item)\n            data_json = json.dumps(rich_item)\n            unique_field = self.get_field_unique_id()\n            bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % (rich_item[unique_field])\n            bulk_json += data_json + \"\\n\"  # Bulk document\n            current += 1\n\n            if self.pair_programming:\n                # Multi author support\n                if 'authors' in item['data']:\n                    # First author already added in the above commit\n                    authors = item['data']['authors']\n                    for i in range(1, len(authors)):\n                        # logger.debug('Adding a new commit for %s', authors[i])\n                        item['data']['Author'] = authors[i]\n                        item['data']['is_git_commit_multi_author'] = 1\n                        rich_item = self.get_rich_item(item)\n                        item['data']['is_git_commit_multi_author'] = 1\n                        data_json = json.dumps(rich_item)\n                        commit_id = item[\"uuid\"] + \"_\" + str(i - 1)\n                        rich_item['git_uuid'] = commit_id\n                        bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % rich_item['git_uuid']\n                        bulk_json += data_json + \"\\n\"  # Bulk document\n                        current += 1\n                        total_multi_author += 1\n\n                if rich_item['Signed-off-by_number'] > 0:\n                    nsg = 0\n                    # Remove duplicates and the already added Author if exists\n                    authors = list(set(item['data']['Signed-off-by']))\n                    if item['data']['Author'] in authors:\n                        authors.remove(item['data']['Author'])\n                    for author in authors:\n                        # logger.debug('Adding a new commit for %s', author)\n                        # Change the Author in the original commit and generate\n                        # a new enriched item with it\n                        item['data']['Author'] = author\n                        item['data']['is_git_commit_signed_off'] = 1\n                        rich_item = self.get_rich_item(item)\n                        commit_id = item[\"uuid\"] + \"_\" + str(nsg)\n                        rich_item['git_uuid'] = commit_id\n                        data_json = json.dumps(rich_item)\n                        bulk_json += '{\"index\" : {\"_id\" : \"%s\" } }\\n' % rich_item['git_uuid']\n                        bulk_json += data_json + \"\\n\"  # Bulk document\n                        current += 1\n                        total_signed_off += 1\n                        nsg += 1\n\n        if current > 0:\n            total += self.elastic.safe_put_bulk(url, bulk_json)\n\n        if total == 0:\n            # No items enriched, nothing to upload to ES\n            return total\n\n        if self.pair_programming:\n            logger.info(\"Signed-off commits generated: %i\", total_signed_off)\n            logger.info(\"Multi author commits generated: %i\", total_multi_author)\n\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_items(self, ocean_backend, enrich_backend):\n\n        fltr = {\n            'name': 'origin',\n            'value': [self.perceval_backend.origin]\n        }\n\n        logger.debug(\"[update-items] Checking commits for %s.\", self.perceval_backend.origin)\n\n        git_repo = GitRepository(self.perceval_backend.uri, self.perceval_backend.gitpath)\n\n        try:\n            current_hashes = set([commit for commit in git_repo.rev_list()])\n        except Exception as e:\n            logger.error(\"Skip updating branch info for repo %s, git rev-list command failed: %s\", git_repo.uri, e)\n            return\n\n        raw_hashes = set([item['data']['commit']\n                          for item in ocean_backend.fetch(ignore_incremental=True, _filter=fltr)])\n\n        hashes_to_delete = list(raw_hashes.difference(current_hashes))\n\n        to_process = []\n        for _hash in hashes_to_delete:\n            to_process.append(_hash)\n\n            if len(to_process) != MAX_BULK_UPDATE_SIZE:\n                continue\n\n            # delete documents from the raw index\n            self.remove_commits(to_process, ocean_backend.elastic.index_url,\n                                'data.commit', self.perceval_backend.origin)\n            # delete documents from the enriched index\n            self.remove_commits(to_process, enrich_backend.elastic.index_url,\n                                'hash', self.perceval_backend.origin)\n\n            to_process = []\n\n        if to_process:\n            # delete documents from the raw index\n            self.remove_commits(to_process, ocean_backend.elastic.index_url,\n                                'data.commit', self.perceval_backend.origin)\n            # delete documents from the enriched index\n            self.remove_commits(to_process, enrich_backend.elastic.index_url,\n                                'hash', self.perceval_backend.origin)\n\n        logger.debug(\"[update-items] %s commits deleted from %s with origin %s.\",\n                     len(hashes_to_delete), ocean_backend.elastic.anonymize_url(ocean_backend.elastic.index_url),\n                     self.perceval_backend.origin)\n        logger.debug(\"[update-items] %s commits deleted from %s with origin %s.\",\n                     len(hashes_to_delete), enrich_backend.elastic.anonymize_url(enrich_backend.elastic.index_url),\n                     self.perceval_backend.origin)\n\n        # update branch info\n        self.delete_commit_branches(enrich_backend)\n        self.add_commit_branches(git_repo, enrich_backend)", "response": "Retrieve the commits not present in the original repository and delete them from the raw and enriched indexes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_commit_branches(self, enrich_backend):\n        fltr = \"\"\"\n            \"filter\": [\n                {\n                    \"term\": {\n                        \"origin\": \"%s\"\n                    }\n                }\n            ]\n        \"\"\" % self.perceval_backend.origin\n\n        # reset references in enrich index\n        es_query = \"\"\"\n            {\n              \"script\": {\n                \"source\": \"ctx._source.branches = new HashSet();\",\n                \"lang\": \"painless\"\n              },\n              \"query\": {\n                \"bool\": {\n                    %s\n                }\n              }\n            }\n            \"\"\" % fltr\n\n        index = enrich_backend.elastic.index_url\n        r = self.requests.post(index + \"/_update_by_query?refresh\", data=es_query, headers=HEADER_JSON, verify=False)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError:\n            logger.error(\"Error while deleting branches on %s\",\n                         self.elastic.anonymize_url(index))\n            logger.error(r.text)\n            return\n\n        logger.debug(\"Delete branches %s, index %s\", r.text, self.elastic.anonymize_url(index))", "response": "Delete the information about branches from the documents representing\n        commits in the enriched index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the information about branches to the documents representing commits in the enriched index.", "response": "def add_commit_branches(self, git_repo, enrich_backend):\n        \"\"\"Add the information about branches to the documents representing commits in\n        the enriched index. Branches are obtained using the command `git ls-remote`,\n        then for each branch, the list of commits is retrieved via the command `git rev-list branch-name` and\n        used to update the corresponding items in the enriched index.\n\n        :param git_repo: GitRepository object\n        :param enrich_backend: the enrich backend\n        \"\"\"\n        to_process = []\n        for hash, refname in git_repo._discover_refs(remote=True):\n\n            if not refname.startswith('refs/heads/'):\n                continue\n\n            commit_count = 0\n            branch_name = refname.replace('refs/heads/', '')\n\n            try:\n                commits = git_repo.rev_list([branch_name])\n\n                for commit in commits:\n                    to_process.append(commit)\n                    commit_count += 1\n\n                    if commit_count == MAX_BULK_UPDATE_SIZE:\n                        self.__process_commits_in_branch(enrich_backend, branch_name, to_process)\n\n                        # reset the counter\n                        to_process = []\n                        commit_count = 0\n\n                if commit_count:\n                    self.__process_commits_in_branch(enrich_backend, branch_name, to_process)\n\n            except Exception as e:\n                logger.error(\"Skip adding branch info for repo %s due to %s\", git_repo.uri, e)\n                return"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_commits(self, items, index, attribute, origin):\n        es_query = '''\n            {\n              \"query\": {\n                \"bool\": {\n                    \"must\": {\n                        \"term\": {\n                            \"origin\": \"%s\"\n                        }\n                    },\n                    \"filter\": {\n                        \"terms\": {\n                            \"%s\": [%s]\n                        }\n                    }\n                }\n              }\n            }\n            ''' % (origin, attribute, \",\".join(['\"%s\"' % i for i in items]))\n\n        r = self.requests.post(index + \"/_delete_by_query?refresh\", data=es_query, headers=HEADER_JSON, verify=False)\n        try:\n            r.raise_for_status()\n        except requests.exceptions.HTTPError as ex:\n            logger.error(\"Error updating deleted commits for %s.\", self.elastic.anonymize_url(index))\n            logger.error(r.text)\n            return", "response": "Delete documents that correspond to commits deleted in the Git repository."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the general mappings applied to all data sources in ES < = 6", "response": "def find_general_mappings(es_major_version):\n    \"\"\"\n    Find the general mappings applied to all data sources\n    :param es_major_version: string with the major version for Elasticsearch\n    :return: a dict with the mappings (raw and enriched)\n    \"\"\"\n\n    if es_major_version not in ES_SUPPORTED:\n        print(\"Elasticsearch version not supported %s (supported %s)\" % (es_major_version, ES_SUPPORTED))\n        sys.exit(1)\n\n    # By default all strings are not analyzed in ES < 6\n    if es_major_version == '5':\n        # Before version 6, strings were strings\n        not_analyze_strings = \"\"\"\n        {\n          \"dynamic_templates\": [\n            { \"notanalyzed\": {\n                  \"match\": \"*\",\n                  \"match_mapping_type\": \"string\",\n                  \"mapping\": {\n                      \"type\":        \"string\",\n                      \"index\":       \"not_analyzed\"\n                  }\n               }\n            }\n          ]\n        } \"\"\"\n    else:\n        # After version 6, strings are keywords (not analyzed)\n        not_analyze_strings = \"\"\"\n        {\n          \"dynamic_templates\": [\n            { \"notanalyzed\": {\n                  \"match\": \"*\",\n                  \"match_mapping_type\": \"string\",\n                  \"mapping\": {\n                      \"type\":        \"keyword\"\n                  }\n               }\n            }\n          ]\n        } \"\"\"\n\n    return json.loads(not_analyze_strings)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the mapping given a perceval data source", "response": "def find_ds_mapping(data_source, es_major_version):\n    \"\"\"\n    Find the mapping given a perceval data source\n\n    :param data_source: name of the perceval data source\n    :param es_major_version: string with the major version for Elasticsearch\n    :return: a dict with the mappings (raw and enriched)\n    \"\"\"\n    mappings = {\"raw\": None,\n                \"enriched\": None}\n\n    # Backend connectors\n    connectors = get_connectors()\n\n    try:\n        raw_klass = connectors[data_source][1]\n        enrich_klass = connectors[data_source][2]\n    except KeyError:\n        print(\"Data source not found\", data_source)\n        sys.exit(1)\n\n    # Mapping for raw index\n    backend = raw_klass(None)\n    if backend:\n        mapping = json.loads(backend.mapping.get_elastic_mappings(es_major_version)['items'])\n        mappings['raw'] = [mapping, find_general_mappings(es_major_version)]\n\n    # Mapping for enriched index\n    backend = enrich_klass(None)\n    if backend:\n        mapping = json.loads(backend.mapping.get_elastic_mappings(es_major_version)['items'])\n        mappings['enriched'] = [mapping, find_general_mappings(es_major_version)]\n\n    return mappings"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild and index for areas of code.", "response": "def areas_of_code(git_enrich, in_conn, out_conn, block_size=100):\n    \"\"\"Build and index for areas of code from a given Perceval RAW index.\n\n    :param block_size: size of items block.\n    :param git_enrich: GitEnrich object to deal with SortingHat affiliations.\n    :param in_conn: ESPandasConnector to read from.\n    :param out_conn: ESPandasConnector to write to.\n    :return: number of documents written in ElasticSearch enriched index.\n    \"\"\"\n    aoc = AreasOfCode(in_connector=in_conn, out_connector=out_conn, block_size=block_size,\n                      git_enrich=git_enrich)\n    ndocs = aoc.analyze()\n    return ndocs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_hashcode(uuid, filepath, file_event):\n\n        content = ':'.join([uuid, filepath, file_event])\n        hashcode = hashlib.sha1(content.encode('utf-8'))\n        return hashcode.hexdigest()", "response": "Generate a SHA1 hash code based on the given arguments."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting items into ElasticSearch.", "response": "def write(self, items):\n        \"\"\"Write items into ElasticSearch.\n\n        :param items: Pandas DataFrame\n        \"\"\"\n\n        if self._read_only:\n            raise IOError(\"Cannot write, Connector created as Read Only\")\n\n        # Uploading info to the new ES\n        rows = items.to_dict(\"index\")\n        docs = []\n        for row_index in rows.keys():\n            row = rows[row_index]\n            item_id = self.make_hashcode(row[Events.PERCEVAL_UUID], row[Git.FILE_PATH], row[Git.FILE_EVENT])\n            row['uuid'] = item_id\n            doc = {\n                \"_index\": self._es_index,\n                \"_type\": \"items\",\n                \"_id\": item_id,\n                \"_source\": row\n            }\n            docs.append(doc)\n        # TODO exception and error handling\n        chunk_size = 2000\n        chunks = [docs[i:i + chunk_size] for i in range(0, len(docs), chunk_size)]\n        for chunk in chunks:\n            helpers.bulk(self._es_conn, chunk)\n        logger.info(self.__log_prefix + \"Written: \" + str(len(docs)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess items to add file related information.", "response": "def process(self, items_block):\n        \"\"\"Process items to add file related information.\n\n        Eventize items creating one new item per each file found in the commit (excluding\n        files with no actions performed on them). For each event, file path, file name,\n        path parts, file type and file extension are added as fields.\n\n        :param items_block: items to be processed. Expects to find ElasticSearch hits _source part only.\n        \"\"\"\n\n        logger.info(self.__log_prefix + \" New commits: \" + str(len(items_block)))\n\n        # Create events from commits\n        git_events = Git(items_block, self._git_enrich)\n        events_df = git_events.eventize(2)\n\n        logger.info(self.__log_prefix + \" New events: \" + str(len(events_df)))\n\n        if len(events_df) > 0:\n            # Filter information\n            data_filtered = FilterRows(events_df)\n            events_df = data_filtered.filter_([\"filepath\"], \"-\")\n\n            logger.info(self.__log_prefix + \" New events filtered: \" + str(len(events_df)))\n\n            events_df['message'] = events_df['message'].str.slice(stop=AreasOfCode.MESSAGE_MAX_SIZE)\n            logger.info(self.__log_prefix + \" Remove message content\")\n\n            # Add filetype info\n            enriched_filetype = FileType(events_df)\n            events_df = enriched_filetype.enrich('filepath')\n\n            logger.info(self.__log_prefix + \" New Filetype events: \" + str(len(events_df)))\n\n            # Split filepath info\n            enriched_filepath = FilePath(events_df)\n            events_df = enriched_filepath.enrich('filepath')\n\n            logger.info(self.__log_prefix + \" New Filepath events: \" + str(len(events_df)))\n\n            # Deal with surrogates\n            convert = ToUTF8(events_df)\n            events_df = convert.enrich([\"owner\"])\n\n        logger.info(self.__log_prefix + \" Final new events: \" + str(len(events_df)))\n\n        return self.ProcessResults(processed=len(events_df), out_items=events_df)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the perceval params given a URL for the data source", "response": "def get_perceval_params_from_url(cls, url):\n        \"\"\" Get the perceval params given a URL for the data source \"\"\"\n        params = []\n\n        dparam = cls.get_arthur_params_from_url(url)\n        params.append(dparam['url'])\n        params.append(dparam['channel'])\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_repository_filter(perceval_backend, perceval_backend_name,\n                          term=False):\n    \"\"\" Get the filter needed for get the items in a repository \"\"\"\n    from .github import GITHUB\n\n    filter_ = {}\n\n    if not perceval_backend:\n        return filter_\n\n    field = 'origin'\n    value = perceval_backend.origin\n\n    if perceval_backend_name in [\"meetup\", \"nntp\", \"stackexchange\", \"jira\"]:\n        # Until tag is supported in all raw and enriched indexes\n        # we should use origin. But stackexchange and meetup won't work with origin\n        # because the tag must be included in the filter.\n        # For nntp we have a common group server as origin, so we need to use also the tag.\n        # And in jira we can filter by product, and the origin is the same jira server.\n        field = 'tag'\n        value = perceval_backend.tag\n\n    if perceval_backend:\n        if not term:\n            filter_ = {\"name\": field,\n                       \"value\": value}\n        else:\n            filter_ = '''\n                {\"term\":\n                    { \"%s\" : \"%s\"  }\n                }\n            ''' % (field, value)\n            # Filters are always a dict\n            filter_ = json.loads(filter_)\n\n    if value in ['', GITHUB + '/', 'https://meetup.com/']:\n        # Support for getting all items from a multiorigin index\n        # In GitHub we receive GITHUB + '/', the site url without org and repo\n        # In Meetup we receive https://meetup.com/ as the tag\n        filter_ = {}\n\n    return filter_", "response": "Get the filter needed for get the items in a repository"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the number of days between two dates in UTC format", "response": "def get_time_diff_days(start, end):\n    ''' Number of days between two dates in UTC format  '''\n\n    if start is None or end is None:\n        return None\n\n    if type(start) is not datetime.datetime:\n        start = parser.parse(start).replace(tzinfo=None)\n    if type(end) is not datetime.datetime:\n        end = parser.parse(end).replace(tzinfo=None)\n\n    seconds_day = float(60 * 60 * 24)\n    diff_days = (end - start).total_seconds() / seconds_day\n    diff_days = float('%.2f' % diff_days)\n\n    return diff_days"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a unixtime timestamp to a datetime object.", "response": "def unixtime_to_datetime(ut):\n    \"\"\"Convert a unixtime timestamp to a datetime object.\n    The function converts a timestamp in Unix format to a\n    datetime object. UTC timezone will also be set.\n    :param ut: Unix timestamp to convert\n    :returns: a datetime object\n    :raises InvalidDateError: when the given timestamp cannot be\n        converted into a valid date\n    \"\"\"\n\n    dt = datetime.datetime.utcfromtimestamp(ut)\n    dt = dt.replace(tzinfo=tz.tzutc())\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the arthur params given a URL for the data source", "response": "def get_arthur_params_from_url(cls, url):\n        # In the url the org and the repository are included\n\n        params = url.split()\n        \"\"\" Get the arthur params given a URL for the data source \"\"\"\n        params = {\"owner\": params[0], \"repository\": params[1]}\n\n        return params"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the identities from an item", "response": "def get_identities(self, item):\n        \"\"\" Return the identities from an item \"\"\"\n\n        if 'authorData' in item['data']['fields']:\n            user = self.get_sh_identity(item['data']['fields']['authorData'])\n            yield user\n\n        if 'ownerData' in item['data']['fields']:\n            user = self.get_sh_identity(item['data']['fields']['ownerData'])\n            yield user"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_rich_events(self, item):\n        # To get values from the task\n        eitem = self.get_rich_item(item)\n\n        # Fields that don't change never\n        task_fields_nochange = ['author_userName', 'creation_date', 'url', 'id', 'bug_id']\n\n        # Follow changes in this fields\n        task_fields_change = ['priority_value', 'status', 'assigned_to_userName', 'tags_custom_analyzed']\n        task_change = {}\n        for f in task_fields_change:\n            task_change[f] = None\n        task_change['status'] = TASK_OPEN_STATUS\n        task_change['tags_custom_analyzed'] = eitem['tags_custom_analyzed']\n\n        # Events are in transactions field (changes in fields)\n        transactions = item['data']['transactions']\n\n        if not transactions:\n            return []\n\n        for t in transactions:\n            event = {}\n            # Needed for incremental updates from the item\n            event['metadata__updated_on'] = item['metadata__updated_on']\n            event['origin'] = item['origin']\n            # Real event data\n            event['transactionID'] = t['transactionID']\n            event['type'] = t['transactionType']\n            event['username'] = None\n            if 'authorData' in t and 'userName' in t['authorData']:\n                event['event_author_name'] = t['authorData']['userName']\n            event['update_date'] = unixtime_to_datetime(float(t['dateCreated'])).isoformat()\n            event['oldValue'] = ''\n            event['newValue'] = ''\n            if event['type'] == 'core:edge':\n                for val in t['oldValue']:\n                    if val in self.phab_ids_names:\n                        val = self.phab_ids_names[val]\n                    event['oldValue'] += \",\" + val\n                event['oldValue'] = event['oldValue'][1:]  # remove first comma\n                for val in t['newValue']:\n                    if val in self.phab_ids_names:\n                        val = self.phab_ids_names[val]\n                    event['newValue'] += \",\" + val\n                event['newValue'] = event['newValue'][1:]  # remove first comma\n            elif event['type'] in ['status', 'description', 'priority', 'reassign', 'title', 'space', 'core:create', 'parent']:\n                # Convert to str so the field is always a string\n                event['oldValue'] = str(t['oldValue'])\n                if event['oldValue'] in self.phab_ids_names:\n                    event['oldValue'] = self.phab_ids_names[event['oldValue']]\n                event['newValue'] = str(t['newValue'])\n                if event['newValue'] in self.phab_ids_names:\n                    event['newValue'] = self.phab_ids_names[event['newValue']]\n            elif event['type'] == 'core:comment':\n                event['newValue'] = t['comments']\n            elif event['type'] == 'core:subscribers':\n                event['newValue'] = \",\".join(t['newValue'])\n            else:\n                # logger.debug(\"Event type %s old to new value not supported\", t['transactionType'])\n                pass\n\n            for f in task_fields_nochange:\n                # The field name must be the same than in task for filtering\n                event[f] = eitem[f]\n\n            # To track history of some fields\n            if event['type'] in ['status']:\n                task_change['status'] = event['newValue']\n            elif event['type'] == 'priority':\n                task_change['priority'] = event['newValue']\n            elif event['type'] == 'core:edge':\n                task_change['tags_custom_analyzed'] = [event['newValue']]\n            if event['type'] in ['reassign']:\n                # Try to get the userName and not the user id\n                if event['newValue'] in self.phab_ids_names:\n                    task_change['assigned_to_userName'] = self.phab_ids_names[event['newValue']]\n                    event['newValue'] = task_change['assigned_to_userName']\n                else:\n                    task_change['assigned_to_userName'] = event['newValue']\n                if event['oldValue'] in self.phab_ids_names:\n                    # Try to get the userName and not the user id\n                    event['oldValue'] = self.phab_ids_names[event['oldValue']]\n\n            for f in task_change:\n                event[f] = task_change[f]\n\n            yield event", "response": "Get the rich events from the item"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfills the internal list of phab ids and names.", "response": "def __fill_phab_ids(self, item):\n        \"\"\" Get mappings between phab ids and names \"\"\"\n        for p in item['projects']:\n            if p and 'name' in p and 'phid' in p:\n                self.phab_ids_names[p['phid']] = p['name']\n        if 'authorData' not in item['fields'] or not item['fields']['authorData']:\n            return\n        self.phab_ids_names[item['fields']['authorData']['phid']] = item['fields']['authorData']['userName']\n        if 'ownerData' in item['fields'] and item['fields']['ownerData']:\n            self.phab_ids_names[item['fields']['ownerData']['phid']] = item['fields']['ownerData']['userName']\n        if 'priority' in item['fields']:\n            val = item['fields']['priority']['value']\n            self.phab_ids_names[str(val)] = item['fields']['priority']['name']\n        for t in item['transactions']:\n            if 'authorData' in t and t['authorData'] and 'userName' in t['authorData']:\n                self.phab_ids_names[t['authorData']['phid']] = t['authorData']['userName']\n            elif t['authorData'] and 'name' in t['authorData']:\n                # Herald\n                self.phab_ids_names[t['authorData']['phid']] = t['authorData']['name']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the starting time for the cron job.", "response": "def starting_at(self, datetime_or_str):\n        \"\"\"\n        Set the starting time for the cron job.  If not specified, the starting time will always\n        be the beginning of the interval that is current when the cron is started.\n\n        :param datetime_or_str: a datetime object or a string that dateutil.parser can understand\n        :return: self\n        \"\"\"\n        if isinstance(datetime_or_str, str):\n            self._starting_at = parse(datetime_or_str)\n        elif isinstance(datetime_or_str, datetime.datetime):\n            self._starting_at = datetime_or_str\n        else:\n            raise ValueError('.starting_at() method can only take strings or datetime objects')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the _every_kwargs attribute to be used when the job runs.", "response": "def every(self, **kwargs):\n        \"\"\"\n        Specify the interval at which you want the job run.  Takes exactly one keyword argument.\n        That argument must be one named one of [second, minute, hour, day, week, month, year] or\n        their plural equivalents.\n\n        :param kwargs: Exactly one keyword argument\n        :return: self\n        \"\"\"\n        if len(kwargs) != 1:\n            raise ValueError('.every() method must be called with exactly one keyword argument')\n\n        self._every_kwargs = self._clean_kwargs(kwargs)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, func, *func_args, **func__kwargs):\n        self._func = func\n        self._func_args = func_args\n        self._func_kwargs = func__kwargs\n        return self", "response": "Run the function at the scheduled times\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_target(self):\n        if None in [self._func, self._func_kwargs, self._func_kwargs, self._every_kwargs]:\n            raise ValueError('You must call the .every() and .run() methods on every tab.')\n        return self._loop", "response": "Returns a callable with no arguments designed\n        to be the target of a Subprocess\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap a target with queues replacing stdout and stderr with the given name.", "response": "def wrapped_target(target, q_stdout, q_stderr, q_error, robust, name, *args, **kwargs):  # pragma: no cover\n    \"\"\"\n    Wraps a target with queues replacing stdout and stderr\n    \"\"\"\n    import sys\n    sys.stdout = IOQueue(q_stdout)\n    sys.stderr = IOQueue(q_stderr)\n\n    try:\n        target(*args, **kwargs)\n    except:\n        if not robust:\n            s = 'Error in tab\\n' + traceback.format_exc()\n            logger = daiquiri.getLogger(name)\n            logger.error(s)\n        else:\n            raise\n\n\n\n        if not robust:\n            q_error.put(name)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loop(self, max_seconds=None):\n        loop_started =  datetime.datetime.now()\n\n        self._is_running = True\n        while self._is_running:\n            self.process_error_queue(self.q_error)\n\n            if max_seconds is not None:\n                if (datetime.datetime.now() - loop_started).total_seconds() > max_seconds:\n                    break\n            for subprocess in self._subprocesses:\n                if not subprocess.is_alive():\n                    subprocess.start()\n\n            self.process_io_queue(self.q_stdout, sys.stdout)\n            self.process_io_queue(self.q_stderr, sys.stderr)", "response": "Main loop for the process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_serializer(meas, schema, rm_none, extra_tags, placeholder):  # noqa: C901\n    _validate_schema(schema, placeholder)\n    tags = []\n    fields = []\n    ts = None\n    meas = meas\n    for k, t in schema.items():\n        if t is MEASUREMENT:\n            meas = f\"{{i.{k}}}\"\n        elif t is TIMEINT:\n            ts = f\"{{i.{k}}}\"\n        elif t is TIMESTR:\n            if pd:\n                ts = f\"{{pd.Timestamp(i.{k} or 0).value}}\"\n            else:\n                ts = f\"{{dt_to_int(str_to_dt(i.{k}))}}\"\n        elif t is TIMEDT:\n            if pd:\n                ts = f\"{{pd.Timestamp(i.{k} or 0).value}}\"\n            else:\n                ts = f\"{{dt_to_int(i.{k})}}\"\n        elif t is TAG:\n            tags.append(f\"{k}={{str(i.{k}).translate(tag_escape)}}\")\n        elif t is TAGENUM:\n            tags.append(f\"{k}={{getattr(i.{k}, 'name', i.{k} or None)}}\")\n        elif t in (FLOAT, BOOL):\n            fields.append(f\"{k}={{i.{k}}}\")\n        elif t is INT:\n            fields.append(f\"{k}={{i.{k}}}i\")\n        elif t is STR:\n            fields.append(f\"{k}=\\\\\\\"{{str(i.{k}).translate(str_escape)}}\\\\\\\"\")\n        elif t is ENUM:\n            fields.append(f\"{k}=\\\\\\\"{{getattr(i.{k}, 'name', i.{k} or None)}}\\\\\\\"\")\n        else:\n            raise SchemaError(f\"Invalid attribute type {k!r}: {t!r}\")\n    extra_tags = extra_tags or {}\n    for k, v in extra_tags.items():\n        tags.append(f\"{k}={v}\")\n    if placeholder:\n        fields.insert(0, f\"_=true\")\n\n    sep = ',' if tags else ''\n    ts = f' {ts}' if ts else ''\n    fmt = f\"{meas}{sep}{','.join(tags)} {','.join(fields)}{ts}\"\n    if rm_none:\n        # Has substantial runtime impact. Best avoided if performance is critical.\n        # First field can't be removed.\n        pat = r',\\w+=\"?None\"?i?'\n        f = eval('lambda i: re.sub(r\\'{}\\', \"\", f\"{}\").encode()'.format(pat, fmt))\n    else:\n        f = eval('lambda i: f\"{}\".encode()'.format(fmt))\n    f.__doc__ = \"Returns InfluxDB line protocol representation of user-defined class\"\n    f._args = dict(meas=meas, schema=schema, rm_none=rm_none,\n                   extra_tags=extra_tags, placeholder=placeholder)\n    return f", "response": "Factory of line protocol parsers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lineprotocol(\n        cls=None,\n        *,\n        schema: Optional[Mapping[str, type]] = None,\n        rm_none: bool = False,\n        extra_tags: Optional[Mapping[str, str]] = None,\n        placeholder: bool = False\n):\n    \"\"\"Adds ``to_lineprotocol`` method to arbitrary user-defined classes\n\n    :param cls: Class to monkey-patch\n    :param schema: Schema dictionary (attr/type pairs).\n    :param rm_none: Whether apply a regex to remove ``None`` values.\n        If ``False``, passing ``None`` values to boolean, integer or float or time fields\n        will result in write errors. Setting to ``True`` is \"safer\" but impacts performance.\n    :param extra_tags: Hard coded tags to be added to every point generated.\n    :param placeholder: If no field attributes are present, add a placeholder attribute (``_``)\n        which is always equal to ``True``. This is a workaround for creating field-less points\n        (which is not supported natively by InfluxDB)\n    \"\"\"\n\n    def _lineprotocol(cls):\n        _schema = schema or getattr(cls, '__annotations__', {})\n        f = _make_serializer(cls.__name__, _schema, rm_none, extra_tags, placeholder)\n        cls.to_lineprotocol = f\n        return cls\n\n    return _lineprotocol(cls) if cls else _lineprotocol", "response": "Returns a new class that can be used to generate a line - protocol for the given class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef serialize(point: Mapping, measurement=None, **extra_tags) -> bytes:\n    tags = _serialize_tags(point, extra_tags)\n    return (\n        f'{_serialize_measurement(point, measurement)}'\n        f'{\",\" if tags else \"\"}{tags} '\n        f'{_serialize_fields(point)} '\n        f'{_serialize_timestamp(point)}'\n    ).encode()", "response": "Converts dictionary - like data into a single line protocol line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _serialize_fields(point):\n    output = []\n    for k, v in point['fields'].items():\n        k = escape(k, key_escape)\n        if isinstance(v, bool):\n            output.append(f'{k}={v}')\n        elif isinstance(v, int):\n            output.append(f'{k}={v}i')\n        elif isinstance(v, str):\n            output.append(f'{k}=\"{v.translate(str_escape)}\"')\n        elif v is None:\n            # Empty values\n            continue\n        else:\n            # Floats\n            output.append(f'{k}={v}')\n    return ','.join(output)", "response": "Serialize the fields of a single object into a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize(data, measurement=None, tag_columns=None, **extra_tags):\n    if isinstance(data, bytes):\n        return data\n    elif isinstance(data, str):\n        return data.encode('utf-8')\n    elif hasattr(data, 'to_lineprotocol'):\n        return data.to_lineprotocol()\n    elif pd is not None and isinstance(data, pd.DataFrame):\n        return dataframe.serialize(data, measurement, tag_columns, **extra_tags)\n    elif isinstance(data, dict):\n        return mapping.serialize(data, measurement, **extra_tags)\n    elif hasattr(data, '__iter__'):\n        return b'\\n'.join([serialize(i, measurement, tag_columns, **extra_tags) for i in data])\n    else:\n        raise ValueError('Invalid input', data)", "response": "Converts input data into line protocol format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iterpoints(resp: dict, parser: Optional[Callable] = None) -> Iterator[Any]:\n    for statement in resp['results']:\n        if 'series' not in statement:\n            continue\n        for series in statement['series']:\n            if parser is None:\n                return (x for x in series['values'])\n            elif 'meta' in inspect.signature(parser).parameters:\n                meta = {k: series[k] for k in series if k != 'values'}\n                meta['statement_id'] = statement['statement_id']\n                return (parser(*x, meta=meta) for x in series['values'])\n            else:\n                return (parser(*x) for x in series['values'])\n    return iter([])", "response": "Iterates a response JSON containing data points."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a dictionary of DataFrames from a response object", "response": "def parse(resp) -> DataFrameType:\n    \"\"\"Makes a dictionary of DataFrames from a response object\"\"\"\n    statements = []\n    for statement in resp['results']:\n        series = {}\n        for s in statement.get('series', []):\n            series[_get_name(s)] = _drop_zero_index(_serializer(s))\n        statements.append(series)\n\n    if len(statements) == 1:\n        series: dict = statements[0]\n        if len(series) == 1:\n            return list(series.values())[0]  # DataFrame\n        else:\n            return series  # dict\n    return statements"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(df, measurement, tag_columns=None, **extra_tags) -> bytes:\n    # Pre-processing\n    if measurement is None:\n        raise ValueError(\"Missing 'measurement'\")\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError('DataFrame index is not DatetimeIndex')\n    tag_columns = set(tag_columns or [])\n    isnull = df.isnull().any(axis=1)\n\n    # Make parser function\n    tags = []\n    fields = []\n    for k, v in extra_tags.items():\n        tags.append(f\"{k}={escape(v, key_escape)}\")\n    for i, (k, v) in enumerate(df.dtypes.items()):\n        k = k.translate(key_escape)\n        if k in tag_columns:\n            tags.append(f\"{k}={{p[{i+1}]}}\")\n        elif issubclass(v.type, np.integer):\n            fields.append(f\"{k}={{p[{i+1}]}}i\")\n        elif issubclass(v.type, (np.float, np.bool_)):\n            fields.append(f\"{k}={{p[{i+1}]}}\")\n        else:\n            # String escaping is skipped for performance reasons\n            # Strings containing double-quotes can cause strange write errors\n            # and should be sanitized by the user.\n            # e.g., df[k] = df[k].astype('str').str.translate(str_escape)\n            fields.append(f\"{k}=\\\"{{p[{i+1}]}}\\\"\")\n    fmt = (f'{measurement}', f'{\",\" if tags else \"\"}', ','.join(tags),\n           ' ', ','.join(fields), ' {p[0].value}')\n    f = eval(\"lambda p: f'{}'\".format(''.join(fmt)))\n\n    # Map/concat\n    if isnull.any():\n        lp = map(f, _itertuples(df[~isnull]))\n        rep = _replace(df)\n        lp_nan = (reduce(lambda a, b: re.sub(*b, a), rep, f(p))\n                  for p in _itertuples(df[isnull]))\n        return '\\n'.join(chain(lp, lp_nan)).encode('utf-8')\n    else:\n        return '\\n'.join(map(f, _itertuples(df))).encode('utf-8')", "response": "Converts a Pandas DataFrame into line protocol format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an aiohttp. ClientSession object with the given kwargs.", "response": "async def create_session(self, **kwargs):\n        \"\"\"Creates an :class:`aiohttp.ClientSession`\n\n        Override this or call it with ``kwargs`` to use other :mod:`aiohttp`\n        functionality not covered by :class:`~.InfluxDBClient.__init__`\n        \"\"\"\n        self.opts.update(kwargs)\n        self._session = aiohttp.ClientSession(**self.opts, loop=self._loop)\n        if self.redis_opts:\n            if aioredis:\n                self._redis = await aioredis.create_redis(**self.redis_opts,\n                                                          loop=self._loop)\n            else:\n                warnings.warn(no_redis_warning)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def ping(self) -> dict:\n        if not self._session:\n            await self.create_session()\n        async with self._session.get(self.url.format(endpoint='ping')) as resp:\n            logger.debug(f'{resp.status}: {resp.reason}')\n            return dict(resp.headers.items())", "response": "Pings InfluxDB\n         Returns a dictionary containing the headers of the response from the InfluxDB ping endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def write(\n        self,\n        data: Union[PointType, Iterable[PointType]],\n        measurement: Optional[str] = None,\n        db: Optional[str] = None,\n        precision: Optional[str] = None,\n        rp: Optional[str] = None,\n        tag_columns: Optional[Iterable] = None,\n        **extra_tags,\n    ) -> bool:\n        \"\"\"Writes data to InfluxDB.\n        Input can be:\n\n        1. A mapping (e.g. ``dict``) containing the keys:\n           ``measurement``, ``time``, ``tags``, ``fields``\n        2. A Pandas :class:`~pandas.DataFrame` with a :class:`~pandas.DatetimeIndex`\n        3. A user defined class decorated w/\n            :func:`~aioinflux.serialization.usertype.lineprotocol`\n        4. A string (``str`` or ``bytes``) properly formatted in InfluxDB's line protocol\n        5. An iterable of one of the above\n\n        Input data in formats 1-3 are parsed to the line protocol before being\n        written to InfluxDB.\n        See the `InfluxDB docs <https://docs.influxdata.com/influxdb/latest/\n        write_protocols/line_protocol_reference/>`_ for more details.\n\n        :param data: Input data (see description above).\n        :param measurement: Measurement name. Mandatory when when writing DataFrames only.\n            When writing dictionary-like data, this field is treated as the default value\n            for points that do not contain a `measurement` field.\n        :param db: Database to be written to. Defaults to `self.db`.\n        :param precision: Sets the precision for the supplied Unix time values.\n            Ignored if input timestamp data is of non-integer type.\n            Valid values: ``{'ns', 'u', '\u00b5', 'ms', 's', 'm', 'h'}``\n        :param rp: Sets the target retention policy for the write.\n            If unspecified, data is written to the default retention policy.\n        :param tag_columns: Columns to be treated as tags\n            (used when writing DataFrames only)\n        :param extra_tags: Additional tags to be added to all points passed.\n        :return: Returns ``True`` if insert is successful.\n            Raises :py:class:`ValueError` otherwise.\n        \"\"\"\n        if not self._session:\n            await self.create_session()\n        if precision is not None:\n            # FIXME: Implement. Related issue: aioinflux/pull/13\n            raise NotImplementedError(\"'precision' parameter is not supported yet\")\n        data = serialization.serialize(data, measurement, tag_columns, **extra_tags)\n        params = {'db': db or self.db}\n        if rp:\n            params['rp'] = rp\n        url = self.url.format(endpoint='write')\n        async with self._session.post(url, params=params, data=data) as resp:\n            if resp.status == 204:\n                return True\n            raise InfluxDBWriteError(resp)", "response": "Writes data to InfluxDB."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def query(\n        self,\n        q: AnyStr,\n        *,\n        epoch: str = 'ns',\n        chunked: bool = False,\n        chunk_size: Optional[int] = None,\n        db: Optional[str] = None,\n        use_cache: bool = False,\n    ) -> Union[AsyncGenerator[ResultType, None], ResultType]:\n        \"\"\"Sends a query to InfluxDB.\n        Please refer to the InfluxDB documentation for all the possible queries:\n        https://docs.influxdata.com/influxdb/latest/query_language/\n\n        :param q: Raw query string\n        :param db: Database to be queried. Defaults to `self.db`.\n        :param epoch: Precision level of response timestamps.\n            Valid values: ``{'ns', 'u', '\u00b5', 'ms', 's', 'm', 'h'}``.\n        :param chunked: If ``True``, makes InfluxDB return results in streamed batches\n            rather than as a single response.\n            Returns an AsyncGenerator which yields responses\n            in the same format as non-chunked queries.\n        :param chunk_size: Max number of points for each chunk. By default, InfluxDB chunks\n            responses by series or by every 10,000 points, whichever occurs first.\n        :param use_cache:\n        :return: Response in the format specified by the combination of\n           :attr:`.InfluxDBClient.output` and ``chunked``\n        \"\"\"\n\n        async def _chunked_generator(url, data):\n            async with self._session.post(url, data=data) as resp:\n                logger.debug(f'{resp.status} (CHUNKED): {q}')\n                # Hack to avoid aiohttp raising ValueError('Line is too long')\n                # The number 16 is arbitrary (may be too large/small).\n                resp.content._high_water *= 16\n                async for chunk in resp.content:\n                    chunk = json.loads(chunk)\n                    self._check_error(chunk)\n                    yield chunk\n\n        if not self._session:\n            await self.create_session()\n\n        # InfluxDB documentation is wrong regarding `/query` parameters\n        # See https://github.com/influxdata/docs.influxdata.com/issues/1807\n        if not isinstance(chunked, bool):\n            raise ValueError(\"'chunked' must be a boolean\")\n        data = dict(q=q, db=db or self.db, chunked=str(chunked).lower(), epoch=epoch)\n        if chunked and chunk_size:\n            data['chunk_size'] = chunk_size\n\n        url = self.url.format(endpoint='query')\n        if chunked:\n            if use_cache:\n                raise ValueError(\"Can't use cache w/ chunked queries\")\n            if self.mode != 'async':\n                raise ValueError(\"Can't use 'chunked' with non-async mode\")\n            if self.output == 'json':\n                return _chunked_generator(url, data)\n            raise ValueError(f\"Chunked queries are not support with {self.output!r} output\")\n\n        key = f'aioinflux:{q}'\n        if use_cache and self._redis and await self._redis.exists(key):\n            logger.debug(f'Cache HIT: {q}')\n            data = lz4.decompress(await self._redis.get(key))\n        else:\n            async with self._session.post(url, data=data) as resp:\n                data = await resp.read()\n                if use_cache and self._redis:\n                    logger.debug(f'Cache MISS ({resp.status}): {q}')\n                    if resp.status == 200:\n                        await self._redis.set(key, lz4.compress(data))\n                        await self._redis.expire(key, self.cache_expiry)\n                else:\n                    logger.debug(f'{resp.status}: {q}')\n\n        data = json.loads(data)\n        self._check_error(data)\n        if self.output == 'json':\n            return data\n        elif self.output == 'dataframe':\n            return serialization.dataframe.parse(data)\n        else:\n            raise ValueError('Invalid output format')", "response": "Sends a query to InfluxDB."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck for JSON error messages and raises Python exception", "response": "def _check_error(response):\n        \"\"\"Checks for JSON error messages and raises Python exception\"\"\"\n        if 'error' in response:\n            raise InfluxDBError(response['error'])\n        elif 'results' in response:\n            for statement in response['results']:\n                if 'error' in statement:\n                    msg = '{d[error]} (statement {d[statement_id]})'\n                    raise InfluxDBError(msg.format(d=statement))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_magic_packet(macaddress):\n    if len(macaddress) == 12:\n        pass\n    elif len(macaddress) == 17:\n        sep = macaddress[2]\n        macaddress = macaddress.replace(sep, '')\n    else:\n        raise ValueError('Incorrect MAC address format')\n\n    # Pad the synchronization stream\n    data = b'FFFFFFFFFFFF' + (macaddress * 16).encode()\n    send_data = b''\n\n    # Split up the hex values in pack\n    for i in range(0, len(data), 2):\n        send_data += struct.pack(b'B', int(data[i: i + 2], 16))\n    return send_data", "response": "Create a magic packet from a MAC address."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_magic_packet(*macs, **kwargs):\n    packets = []\n    ip = kwargs.pop('ip_address', BROADCAST_IP)\n    port = kwargs.pop('port', DEFAULT_PORT)\n    for k in kwargs:\n        raise TypeError('send_magic_packet() got an unexpected keyword '\n                        'argument {!r}'.format(k))\n\n    for mac in macs:\n        packet = create_magic_packet(mac)\n        packets.append(packet)\n\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n    sock.connect((ip, port))\n    for packet in packets:\n        sock.send(packet)\n    sock.close()", "response": "Send a magic packet to one or more mac addresses."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun wake on lan as a CLI application.", "response": "def main(argv=None):\n    \"\"\"\n    Run wake on lan as a CLI application.\n\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description='Wake one or more computers using the wake on lan'\n                    ' protocol.')\n    parser.add_argument(\n        'macs',\n        metavar='mac address',\n        nargs='+',\n        help='The mac addresses or of the computers you are trying to wake.')\n    parser.add_argument(\n        '-i',\n        metavar='ip',\n        default=BROADCAST_IP,\n        help='The ip address of the host to send the magic packet to.'\n             ' (default {})'.format(BROADCAST_IP))\n    parser.add_argument(\n        '-p',\n        metavar='port',\n        type=int,\n        default=DEFAULT_PORT,\n        help='The port of the host to send the magic packet to (default 9)')\n    args = parser.parse_args(argv)\n    send_magic_packet(*args.macs, ip_address=args.i, port=args.p)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncompiling MJML template after render django template.", "response": "def mjml(parser, token):\n    \"\"\"\n    Compile MJML template after render django template.\n\n    Usage:\n        {% mjml %}\n            .. MJML template code ..\n        {% endmjml %}\n    \"\"\"\n    nodelist = parser.parse(('endmjml',))\n    parser.delete_first_token()\n    tokens = token.split_contents()\n    if len(tokens) != 1:\n        raise template.TemplateSyntaxError(\"'%r' tag doesn't receive any arguments.\" % tokens[0])\n    return MJMLRenderNode(nodelist)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_models_dict(annotated_models):\n    logger = getLogger(__name__)\n    logger.debug(\"Parsing models {0}\".format(annotated_models)\n    )\n    parsed_models = {}\n    for family_annotation in annotated_models:\n        family_id = family_annotation.split(':')[0]\n        logger.debug(\"Parsing family {0}\".format(family_id))\n        models = family_annotation.split(':')[1].split('|')\n        parsed_models[family_id] = models\n        logger.debug(\"Adding models {0}\".format(models))\n    \n    return parsed_models", "response": "Builds a dictionary with family_id as key and a list of genetic models as value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting the variants in the given dictionary into individual variants.", "response": "def split_variants(variant_dict, header_parser, allele_symbol='0'):\n    \"\"\"\n    Checks if there are multiple alternative alleles and splitts the \n    variant.\n    If there are multiple alternatives the info fields, vep annotations \n    and genotype calls will be splitted in the correct way\n    \n    Args:\n        variant_dict: a dictionary with the variant information\n    \n    Yields:\n        variant: A variant dictionary with the splitted information for each\n                alternative\n    \"\"\"\n    logger = getLogger(__name__)\n    logger.info(\"Allele symbol {0}\".format(allele_symbol))\n    alternatives = variant_dict['ALT'].split(',')\n    reference = variant_dict['REF']\n    number_of_values = 1\n    # Go through each of the alternative alleles:\n    for alternative_number, alternative in enumerate(alternatives):\n        variant = {}\n        info_dict = OrderedDict()\n        # This is a dict on the form {ALT:[{vep_info_dict}]}\n        vep_dict = {}\n        genotype_dict = {}\n        variant['CHROM'] = variant_dict['CHROM']\n        variant['POS'] = variant_dict['POS']\n        try:\n            # There will not allways be one rsID for each alternative\n            variant['ID'] = variant_dict['ID'].split(';')[alternative_number]\n        # If only one id is present for multiple alleles they all get the same ID\n        except IndexError:\n            variant['ID'] = variant_dict['ID']\n        \n        variant['REF'] = variant_dict['REF']\n        variant['ALT'] = alternative\n        variant['QUAL'] = variant_dict['QUAL']\n        variant['FILTER'] = variant_dict['FILTER']\n        \n\n        if 'FORMAT' in variant_dict:\n            gt_format = variant_dict['FORMAT']\n            variant['FORMAT'] = gt_format\n\n        for info in variant_dict['info_dict']:\n            if info and info != '.':\n                # Check if the info field have one entry per allele:\n                number_of_values = header_parser.extra_info[info]['Number']\n                \n                if info == 'CSQ':\n                    vep_dict[alternative] = variant_dict['vep_info'][alternative]\n                    if vep_dict[alternative]:\n                        info_dict['CSQ'] = [\n                            build_vep_string(\n                                vep_dict[alternative], \n                                header_parser.vep_columns\n                            )\n                        ]\n                # If there is one value per allele we need to split it in\n                # the proper way\n                elif number_of_values == 'A':\n                    try:\n                        # When we split the alleles we only want to annotate with the correct number\n                        info_dict[info] = [variant_dict['info_dict'][info][alternative_number]]\n                    except IndexError:\n                        # If there is only one annotation we choose that one\n                        info_dict[info] = [variant_dict['info_dict'][info][0]]\n                # Choose the right vep info from the old variant\n                elif number_of_values == 'R':\n                    reference_value = variant_dict['info_dict'][info][0]\n                    new_info = [reference_value]\n                    try:\n                        # When we split the alleles we only want to annotate with the correct number\n                        allele_value = variant_dict['info_dict'][info][alternative_number + 1]\n                        new_info.append(allele_value)\n                        info_dict[info] = new_info\n                    except IndexError:\n                        # If annotation is missing we keep the original annotation\n                        info_dict[info] = variant_dict['info_dict'][info]\n                    \n                else:\n                    info_dict[info] = variant_dict['info_dict'][info]\n                \n            else:\n                info_dict[info] = []\n        \n        variant['INFO'] = build_info_string(info_dict)\n        \n        for individual in variant_dict['genotypes']:\n            new_genotype = split_genotype(\n                            variant_dict[individual], \n                            variant['FORMAT'], \n                            alternative_number, \n                            allele_symbol\n                        )\n            \n            variant[individual] = new_genotype\n            genotype_dict[individual] = Genotype(**dict(zip(gt_format.split(':'), variant[individual].split(':'))))\n            \n        variant['info_dict'] = info_dict\n        variant['vep_info'] = vep_dict\n        variant['genotypes'] = genotype_dict\n        variant['variant_id'] = '_'.join([variant['CHROM'],\n                                    variant['POS'],\n                                    variant['REF'],\n                                    alternative])\n        yield variant"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a list with annotated rank scores for each family and returns a dictionary with family id as key and score as value.", "response": "def build_rank_score_dict(rank_scores):\n    \"\"\"\n    Take a list with annotated rank scores for each family and returns a \n    dictionary with family_id as key and a list of genetic models as value.\n    \n    Args:\n        rank_scores    : A list on the form ['1:12','2:20']\n    \n    Returns:\n        scores       : A dictionary with family id:s as key and scores as value\n                                {\n                                    '1':'12',\n                                    '2':'20'\n                                }\n    \n    \"\"\"\n    logger = getLogger(__name__)\n    logger.debug(\"Checking rank scores: {0}\".format(rank_scores))\n    scores = {}\n    for family in rank_scores:\n        entry = family.split(':')\n        try:\n            family_id = entry[0]\n            logger.debug(\"Extracting rank score for family:{0}\".format(family_id))\n            score = entry[1]\n            logger.debug(\"Score:{0}\".format(score))\n        except Exception:\n            raise SyntaxError(\"Malformed rank score input\")\n            \n        scores[family_id] = score\n    \n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the info field corresponds to the metadata specification.", "response": "def check_info_annotation(annotation, info, extra_info, alternatives, individuals=[]):\n    \"\"\"\n    Check if the info annotation corresponds to the metadata specification\n    \n    Arguments:\n        annotation (list): The annotation from the vcf file\n        info (str): Name of the info field\n        extra_info (dict): The metadata specification\n        alternatives (list): A list with the alternative variants\n        individuals (list): a list with the individuals\n    \n    Returns:\n        bool: If the annotation is correct or not\n    \"\"\"\n    \n    number = extra_info['Number']\n    if is_number(number):\n        number_of_entrys = float(number)\n        if number_of_entrys != 0:\n            if len(annotation) != number_of_entrys:\n                raise SyntaxError(\"Info field {0} has the wrong \"\\\n                \"number of entrys according to the vcf header.\"\\\n                \" Vcf header specifies {1} should have {2} entry(s)\".format(\n                    '='.join([info, ','.join(annotation)]), \n                    info,\n                    number\n                ))\n    elif number == 'A':\n        if len(annotation) != len(alternatives):\n            raise SyntaxError(\"Info field {0} has the wrong \"\\\n            \"number of entrys according to the vcf header.\"\\\n            \"Vcf header specifies {1} should have {2} entry(s)\".format(\n                    '='.join([info, ','.join(annotation)]), \n                    info,\n                    number\n            ))\n    elif number == 'R':\n        if len(annotation) != (len(alternatives) + 1):\n            raise SyntaxError(\"Info field {0} has the wrong \"\\\n            \"number of entrys according to the vcf header.\"\\\n            \"Vcf header specifies {1} should have {2} entry(s)\".format(\n                    '='.join([info, ','.join(annotation)]), \n                    info,\n                    number\n            ))\n    elif number == 'G':\n        if len(annotation) != len(individuals):\n            raise SyntaxError(\"Info field {0} has the wrong \"\\\n            \"number of entrys according to the vcf header.\"\\\n            \"Vcf header specifies {1} should have {2} entry(s)\".format(\n                    '='.join([info, ','.join(annotation)]), \n                    info,\n                    number\n            ))\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_variant(line, header_parser, check_info=False):\n    logger = getLogger(__name__)\n\n    individuals = []\n\n    vcf_header = header_parser.header\n\n    individuals = header_parser.individuals\n\n    variant_line = line.rstrip().split('\\t')\n\n    logger.debug(\"Checking if variant line is malformed\")\n    if len(vcf_header) != len(variant_line):\n        raise SyntaxError(\"One of the variant lines is malformed: {0}\".format(\n            line\n        ))\n\n    variant = dict(zip(vcf_header, variant_line))\n    \n    \n    # A dictionary with the vep information\n    variant['vep_info'] = {}\n    # A dictionary with the genetic models (family ids as keys)\n    variant['genetic_models'] = {}\n    # A dictionary with genotype objects (individual ids as keys)\n    variant['genotypes'] = {}\n    # A dictionary with the compounds (family ids as keys)\n    variant['compound_variants'] = {}\n    # A dictionary with the rank scores (family ids as keys)\n    variant['rank_scores'] = {}\n    \n    variant['individual_scores'] = {}\n    \n    alternatives = variant['ALT'].split(',')\n    \n    info_dict = build_info_dict(variant.get('INFO', ''))\n    \n    #For testing\n    \n    # Check that the entry is on the proper format_\n    if check_info:\n        for info in info_dict:\n            annotation = info_dict[info]\n            extra_info = header_parser.extra_info.get(info, None)\n            \n            if not extra_info:\n                raise SyntaxError(\"The INFO field {0} is not specified in vcf\"\\\n                \" header. {1}\".format(info, line))\n            try:\n                check_info_annotation(annotation, info, extra_info, alternatives, individuals)\n            except SyntaxError as e:\n                logger.critical(e)\n                logger.info(\"Line:{0}\".format(line))\n                raise e\n    \n    variant['info_dict'] = info_dict\n    \n    #################### Some fields require special parsing ###########################\n    \n    ##### VEP ANNOTATIONS #####\n    if 'CSQ' in info_dict:\n        vep_columns = header_parser.vep_columns\n        variant['vep_info'] = build_vep_annotation(\n                    info_dict['CSQ'], \n                    variant['REF'], \n                    alternatives,\n                    vep_columns\n                    )\n    \n    ##### GENMOD ANNOTATIONS #####\n    \n    if 'GeneticModels' in info_dict:\n        variant['genetic_models'] = build_models_dict(\n                        info_dict['GeneticModels'])\n\n    if 'Compounds' in info_dict:\n        variant['compound_variants'] = build_compounds_dict(\n                         info_dict['Compounds'])\n\n    if 'RankScore' in info_dict:\n        variant['rank_scores'] = build_rank_score_dict(\n                            info_dict['RankScore'])\n    \n    if 'IndividualRankScore' in info_dict:\n        variant['individual_scores'] = build_rank_score_dict(\n                                    info_dict['IndividualRankScore'])\n    \n    ##### GENOTYPE ANNOTATIONS #####\n    \n    gt_format = variant.get('FORMAT', '').split(':')\n    \n    genotype_dict = {}\n    for individual in individuals:\n        gt_info = variant[individual].split(':')\n        gt_call = dict(zip(gt_format, gt_info))\n        \n        #Create a genotype object for this individual\n        genotype_dict[individual] = Genotype(**gt_call)\n    \n    variant['genotypes'] = genotype_dict\n    \n    variant['variant_id'] = '_'.join(\n                                [\n                                    variant['CHROM'],\n                                    variant['POS'],\n                                    variant['REF'],\n                                    alternatives[0]\n                                ]\n                            )\n    \n    return variant", "response": "Takes a single line of vcf and returns a dictionary with the variant information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a new string based on the information in the info_dict.", "response": "def build_info_string(info):\n    \"\"\"\n    Build a new vcf INFO string based on the information in the info_dict.\n    \n    The info is a dictionary with vcf info keys as keys and lists of vcf values\n    as values. If there is no value False is value in info\n    \n    Args:\n        info (dict): A dictionary with information from the vcf file\n    \n    Returns:\n        String: A string that is on the proper vcf format for the INFO column\n    \n    \"\"\"\n    info_list = []\n    \n    for annotation in info:\n        \n        if info[annotation]:\n            info_list.append('='.join([annotation, ','.join(info[annotation])]))\n        else:\n            info_list.append(annotation)\n    \n    return ';'.join(info_list)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_info_dict(vcf_info):\n    logger = logging.getLogger(__name__)\n    logger.debug(\"Building info dict\")\n    info_dict = OrderedDict()\n    \n    for info in vcf_info.split(';'):\n        info = info.split('=')\n        if len(info) > 1:\n            # If the INFO entry is like key=value, we store the value as a list\n            info[1] = '='.join(info[1:])\n            info_dict[info[0]] = info[1].split(',')\n        else:\n            info_dict[info[0]] = []\n    \n    return info_dict", "response": "Builds a dictionary from the info of a single line of a vcf file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a genotype call and makes a new one that is working for the new variant", "response": "def split_genotype(genotype, gt_format, alternative_number, allele_symbol = '0'):\n    \"\"\"\n    Take a genotype call and make a new one that is working for the new\n    splitted variant\n    \n    Arguments:\n        genotype (str): The original genotype call\n        gt_format (str): The format of the gt call\n        alternative_number (int): What genotype call should we return\n        allele_symbol (str): How should the unobserved allele be represented\n                             when genotype is splitted\n    \n    Returns:\n        new_genotype (str): A string that represents the new genotype\n    \"\"\"\n    logger = getLogger(__name__)\n    \n    logger.info(\"Allele symbol {0}\".format(allele_symbol))\n\n    splitted_genotype = genotype.split(':')\n    logger.debug(\"Parsing genotype {0}\".format(splitted_genotype))\n    splitted_gt_format = gt_format.split(':')\n    logger.debug(\"Parsing gt format {0}\".format(splitted_gt_format))\n    new_genotype = []\n    phased = False\n    for number, genotype_info in enumerate(splitted_genotype):\n        gt_info = splitted_gt_format[number]\n        if gt_info == 'GT':\n            if '/' in genotype_info:\n                gt = genotype_info.split('/')\n            else:\n                gt = genotype_info.split('|')\n                phased = True\n            ref_allele = '.'\n            alt_allele = '.'\n            try:\n                # Check the ref Allele\n                if len(gt) == 2 and gt[0] != '.' and gt[1] != '.':\n                    ref_allele = allele_symbol\n                    alt_allele = allele_symbol\n                    if gt[0] == gt[1]:\n                        # In this case we have a homozygous call:\n                        if int(gt[0]) == alternative_number + 1:\n                            ref_allele = '1'\n                            alt_allele = '1'\n                    else:\n                        if (int(gt[0]) == alternative_number + 1 or \n                            int(gt[1]) == alternative_number + 1):\n                            alt_allele = '1'                        \n                else:\n                # We now know that at least one of the alleles are uncalled\n                    if gt[0] != '.':\n                        if int(gt[0]) == alternative_number + 1:\n                            ref_allele = '1'\n                        else:\n                            ref_allele = '0'\n                    elif len(gt) == 2 and gt[1] != '.':\n                        if int(gt[1]) == alternative_number + 1:\n                            alt_allele = '1'\n                        else:\n                            alt_allele = '0'\n            except (ValueError, KeyError):\n                pass\n            \n            if len(gt) == 2:\n                if phased:\n                    new_genotype.append('|'.join([ref_allele,alt_allele]))\n                else:\n                    new_genotype.append('/'.join([ref_allele,alt_allele]))\n            else:\n                new_genotype.append(ref_allele)\n        \n        elif gt_info == 'AD':\n            ad = []\n            # The reference depth will allways be the original depth now\n            ad.append(genotype_info.split(',')[0])\n            try:\n                ad.append(genotype_info.split(',')[alternative_number+1])\n            except IndexError:\n                ad.append('0')\n            new_genotype.append(','.join(ad))\n        elif gt_info == 'DP':\n            new_genotype.append(genotype_info)\n        elif gt_info == 'PL':\n            new_genotype.append(genotype_info)\n        else:\n            # There are several cases that we do not know how to handle yet so we just add the information\n            new_genotype.append(genotype_info)\n            \n    return ':'.join(new_genotype)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list with the header lines if proper format", "response": "def print_header(self):\n        \"\"\"Returns a list with the header lines if proper format\"\"\"\n        lines_to_print = []\n        lines_to_print.append('##fileformat='+self.fileformat)\n        if self.filedate:\n            lines_to_print.append('##fileformat='+self.fileformat)\n            \n        for filt in self.filter_dict:\n            lines_to_print.append(self.filter_dict[filt])\n        for form in self.format_dict:\n            lines_to_print.append(self.format_dict[form])\n        for info in self.info_dict:\n            lines_to_print.append(self.info_dict[info])\n        for contig in self.contig_dict:\n            lines_to_print.append(self.contig_dict[contig])\n        for alt in self.alt_dict:\n            lines_to_print.append(self.alt_dict[alt])\n        for other in self.other_dict:\n            lines_to_print.append(self.other_dict[other])\n        lines_to_print.append('#'+ '\\t'.join(self.header))\n        return lines_to_print"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_info(self, info_id, number, entry_type, description):\n        info_line = '##INFO=<ID={0},Number={1},Type={2},Description=\"{3}\">'.format(\n            info_id, number, entry_type, description\n        )\n        self.logger.info(\"Adding info line to vcf: {0}\".format(info_line))\n        self.parse_meta_data(info_line)\n        return", "response": "Add an info line to the header."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_version_tracking(self, info_id, version, date, command_line=''):\n        other_line = '##Software=<ID={0},Version={1},Date=\"{2}\",CommandLineOptions=\"{3}\">'.format(\n            info_id, version, date, command_line) \n        self.other_dict[info_id] = other_line\n        return", "response": "Adds a line with information about which software that was run and when \n        was run to the header."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a dictionary that contains the information about the compounds.", "response": "def build_compounds_dict(compounds):\n    \"\"\"\n    Take a list with annotated compound variants for each family and \n    returns a dictionary with family_id as key and a list of dictionarys\n    that holds the information about the compounds.\n    \n    Args:\n        compounds    : A list that can be either on the form \n                        [\n                            '1:1_23_A_C|1_24_T_A',\n                            '2:1_24_T_A'\n                        ]\n                        or if the compounds are scored:\n                        [\n                            '1:1_23_A_C>24|1_24_T_A>19',\n                            '2:1_24_T_A>17'\n                        ]\n    \n    Returns:\n        parsed_compounds : A dictionary on the form\n                                {\n                                    1:[\n                                        {\n                                            'variant_id':'1_23_A_C',\n                                            'compound_score':24\n                                        },\n                                        {\n                                            'variant_id':'1_24_T_A',\n                                            'compound_score:'19\n                                        },\n                                    ],\n                                    2:[\n                                        {'variant_id':'1_24_T_A',\n                                         'compound_score':17\n                                        }\n                                    ]\n                                }\n    \n    \"\"\"\n    logger = getLogger(__name__)\n    logger.debug(\"Parsing compounds: {0}\".format(compounds))\n    \n    parsed_compounds = {}\n    for family_info in compounds:\n        logger.debug(\"Parsing entry {0}\".format(family_info))\n        splitted_family_info = family_info.split(':')\n        family_id = splitted_family_info[0]\n        logger.debug(\"Found family {0}\".format(family_id))\n        parsed_compounds[family_id] = []\n        compound_list = splitted_family_info[1].split('|')\n        for compound in compound_list:\n            compound_id = compound.split('>')[0]\n            try:\n                compound_score = compound.split('>')[1]\n            except IndexError:\n                compound_score = None\n            parsed_compounds[family_id].append(\n                {\n                    'variant_id': compound_id,\n                    'compound_score': compound_score\n                }\n            )\n    \n    return parsed_compounds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntooling for parsing vcf files. Prints the vcf file to output. If --split/-s is used all multiallelic calls will be splitted and printed as single variant calls. For more information, please see github.com/moonso/vcf_parser.", "response": "def cli(variant_file, vep, split, outfile, verbose, silent, check_info,\n        allele_symbol, logfile, loglevel):\n    \"\"\"\n    Tool for parsing vcf files.\n    \n    Prints the vcf file to output. \n    If --split/-s is used all multiallelic calls will be splitted and printed \n    as single variant calls.\n    For more information, please see github.com/moonso/vcf_parser.\n    \"\"\"\n    from vcf_parser import logger, init_log\n\n    if not loglevel:\n        if verbose:\n            loglevel = 'INFO'\n\n    init_log(logger, logfile, loglevel)\n    nr_of_variants = 0\n    start = datetime.now()\n    \n    # with open(variant_file, 'r', encoding=\"utf-8\") as f:\n    #     for line in f:\n    #         if not line.startswith('#'):\n    #             nr_of_variants += 1\n    \n    if variant_file == '-':\n        logger.info(\"Start parsing variants from stdin\")\n        my_parser = VCFParser(\n            fsock=sys.stdin, \n            split_variants=split,\n            check_info=check_info, \n            allele_symbol=allele_symbol\n        )\n    else:\n        logger.info(\"Start parsing variants from file {0}\".format(variant_file))\n        my_parser = VCFParser(\n            infile = variant_file,\n            split_variants=split, \n            check_info=check_info, \n            allele_symbol=allele_symbol\n        )\n\n    if outfile:\n        f = open(outfile, 'w', encoding='utf-8')\n        logger.info(\"Printing vcf to file {0}\".format(outfile))\n    \n    if not silent:\n        logger.info(\"Printing vcf to stdout\")\n    else:\n        logger.info(\"Skip printing since silent is active\")\n    \n    for line in my_parser.metadata.print_header():\n        if outfile:\n            f.write(line+'\\n')\n        else:\n            if not silent:\n                print(line)\n    try:\n        for variant in my_parser:\n            variant_line = '\\t'.join([variant[head] for head in my_parser.header])\n            if outfile:\n                f.write(variant_line + '\\n')\n            else:\n                if not silent:\n                    print(variant_line)\n            nr_of_variants += 1\n    except SyntaxError as e:\n        print(e)\n\n    logger.info('Number of variants: {0}'.format(nr_of_variants))\n    logger.info('Time to parse file: {0}'.format(str(datetime.now() - start)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a vcf file.", "response": "def cli(variant_file, vep, split):\n    \"\"\"Parses a vcf file.\\n\n        \\n\n        Usage:\\n\n            parser infile.vcf\\n\n        If pipe:\\n\n            parser - \n    \"\"\"\n    from datetime import datetime\n    from pprint import pprint as pp\n    if variant_file == '-':\n        my_parser = VCFParser(fsock=sys.stdin, split_variants=split)\n    else:\n        my_parser = VCFParser(infile = variant_file, split_variants=split)\n    start = datetime.now()\n    nr_of_variants = 0\n    for line in my_parser.metadata.print_header():\n        print(line)\n    for variant in my_parser:\n        pp(variant)\n        nr_of_variants += 1\n    print('Number of variants: %s' % nr_of_variants)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_variant(self, chrom, pos, rs_id, ref, alt, qual, filt, info, form=None, genotypes=[]):\n        variant_info = [chrom, pos, rs_id, ref, alt, qual, filt, info]\n        if form:\n            variant_info.append(form)\n        for individual in genotypes:\n            variant_info.append(individual)\n        \n        variant_line = '\\t'.join(variant_info)\n        variant = format_variant(\n            line = variant_line, \n            header_parser = self.metadata, \n            check_info = self.check_info\n        )\n        \n        if not (self.split_variants and len(variant['ALT'].split(',')) > 1):\n            self.variants.append(variant)\n            \n        # If multiple alternative and split_variants we must split the variant                 \n        else:\n            for splitted_variant in split_variants(\n                                                    variant_dict=variant, \n                                                    header_parser=self.metadata, \n                                                    allele_symbol=self.allele_symbol):\n                self.variants.append(splitted_variant)", "response": "This function adds a variant to the parser. It takes the relevant parameters \n        and make a vcf variant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_vep_string(vep_info, vep_columns):\n    logger = getLogger(__name__)\n    logger.debug(\"Building vep string from {0}\".format(vep_info))\n    logger.debug(\"Found vep headers {0}\".format(vep_columns))\n    vep_strings = []\n    for vep_annotation in vep_info:\n        try:\n            vep_info_list = [\n                vep_annotation[vep_key] for vep_key in vep_columns\n            ]\n        except KeyError:\n            raise SyntaxError(\"Vep entry does not correspond to vep headers\")\n        \n        vep_strings.append('|'.join(vep_info_list))\n    return ','.join(vep_strings)", "response": "Build a vep string from a list of vep annotations and a new list of vep strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_vep_annotation(csq_info, reference, alternatives, vep_columns):\n    logger = getLogger(__name__)\n\n    # The keys in the vep dict are the vcf formatted alternatives, values are the\n    # dictionaries with vep annotations\n    vep_dict = {}\n\n    # If we have several alternatives we need to check what types of \n    # alternatives we have\n    vep_to_vcf = {}\n    number_of_deletions = 0\n    for alternative in alternatives:\n        if len(alternative) < len(reference):\n            number_of_deletions += 1\n\n    logger.debug(\"Number of deletions found: {0}\".format(number_of_deletions))\n    for alternative in alternatives:\n        # We store the annotations with keys from the vcf alternatives\n        vep_dict[alternative] = []\n\n        # If substitutuion reference and alternative have the same length\n        if len(alternative) == len(reference):\n             vep_to_vcf[alternative] = alternative\n        # If deletion alternative is shorter that the reference\n        else:\n            # If there is a deletion then the alternative will be '-' in vep entry\n            if len(alternative) == 1:\n                vep_to_vcf['-'] = alternative\n            else:\n                vep_to_vcf[alternative[1:]] = alternative\n\n    for vep_annotation in csq_info:\n        logger.debug(\"Parsing vep annotation: {0}\".format(vep_annotation))\n        splitted_vep = vep_annotation.split('|')\n        \n        if len(splitted_vep) != len(vep_columns):\n            raise SyntaxError(\"Csq info for variant does not match csq info in \"\\\n                            \"header. {0}, {1}\".format(\n            '|'.join(splitted_vep), '|'.join(vep_columns)))\n        \n        # Build the vep dict:\n        vep_info = dict(zip(vep_columns, splitted_vep))\n        \n        # If no allele is found we can not determine what allele\n        if vep_info.get('Allele', None):\n            vep_allele = vep_info['Allele']\n            try:\n                vcf_allele = vep_to_vcf[vep_allele]\n            except KeyError as e:\n                vcf_allele = vep_allele\n    \n            if vcf_allele in vep_dict:\n                vep_dict[vcf_allele].append(vep_info)\n            else:\n                vep_dict[vcf_allele] = [vep_info]\n        else:\n            logger.warning(\"No allele found in vep annotation! Skipping annotation\")\n            \n\n\n    return vep_dict", "response": "Builds a vep annotation dictionary from the vcf line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_login(self, email=None, password=None):\n        email = six.moves.input(\"Email: \") if email is None else email\n        password = getpass.getpass() if password is None else password\n\n        login_data = {\n            \"method\": \"user.login\",\n            \"params\": {\"email\": email,\n                       \"pass\": password}\n        }\n        # If the user/password match, the server respond will contain a\n        #  session cookie that you can use to authenticate future requests.\n        r = self.session.post(\n            self.base_api_urls[\"logic\"],\n            data=json.dumps(login_data),\n        )\n        if r.json()[\"result\"] not in [\"OK\"]:\n            raise AuthenticationError(\"Could not authenticate.\\n{}\"\n                                      .format(r.json()))", "response": "Login with email and password and get back a session cookie"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nauthenticate with a Share Your Class URL using a demo user.", "response": "def demo_login(self, auth=None, url=None):\n        \"\"\"Authenticate with a \"Share Your Class\" URL using a demo user.\n\n        You may provide either the entire ``url`` or simply the ``auth``\n        parameter.\n\n        :param url: Example - \"https://piazza.com/demo_login?nid=hbj11a1gcvl1s6&auth=06c111b\"\n        :param auth: Example - \"06c111b\"\n        \"\"\"\n        assert all([\n            auth or url,  # Must provide at least one\n            not (auth and url)  # Cannot provide more than one\n        ])\n        if url is None:\n            url = \"https://piazza.com/demo_login\"\n            params = dict(nid=self._nid, auth=auth)\n            res = self.session.get(url, params=params)\n        else:\n            res = self.session.get(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets data from post cid in network nid", "response": "def content_get(self, cid, nid=None):\n        \"\"\"Get data from post `cid` in network `nid`\n\n        :type  nid: str\n        :param nid: This is the ID of the network (or class) from which\n            to query posts. This is optional and only to override the existing\n            `network_id` entered when created the class\n        :type  cid: str|int\n        :param cid: This is the post ID which we grab\n        :returns: Python object containing returned data\n        \"\"\"\n        r = self.request(\n            method=\"content.get\",\n            data={\"cid\": cid},\n            nid=nid\n        )\n        return self._handle_error(r, \"Could not get post {}.\".format(cid))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a post or followup.", "response": "def content_create(self, params):\n        \"\"\"Create a post or followup.\n\n        :type  params: dict\n        :param params: A dict of options to pass to the endpoint. Depends on\n            the specific type of content being created.\n        :returns: Python object containing returned data\n        \"\"\"\n        r = self.request(\n            method=\"content.create\",\n            data=params\n        )\n        return self._handle_error(\n            r,\n            \"Could not create object {}.\".format(repr(params))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenrolls students in a network nid.", "response": "def add_students(self, student_emails, nid=None):\n        \"\"\"Enroll students in a network `nid`.\n\n        Piazza will email these students with instructions to\n        activate their account.\n\n        :type  student_emails: list of str\n        :param student_emails: A listing of email addresses to enroll\n            in the network (or class). This can be a list of length one.\n        :type  nid: str\n        :param nid: This is the ID of the network to add students\n            to. This is optional and only to override the existing\n            `network_id` entered when created the class\n        :returns: Python object containing returned data, a list\n            of dicts of user data of all of the users in the network\n            including the ones that were just added.\n        \"\"\"\n        r = self.request(\n            method=\"network.update\",\n            data={\n                \"from\": \"ClassSettingsPage\",\n                \"add_students\": student_emails\n            },\n            nid=nid,\n            nid_key=\"id\"\n        )\n        return self._handle_error(r, \"Could not add users.\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_users(self, nid=None):\n        r = self.request(\n            method=\"network.get_all_users\",\n            nid=nid\n        )\n        return self._handle_error(r, \"Could not get users.\")", "response": "Get a listing of data for each user in a network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_users(self, user_ids, nid=None):\n        r = self.request(\n            method=\"network.get_users\",\n            data={\"ids\": user_ids},\n            nid=nid\n        )\n        return self._handle_error(r, \"Could not get users.\")", "response": "Get a listing of data for specific users in the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_users(self, user_ids, nid=None):\n        r = self.request(\n            method=\"network.update\",\n            data={\"remove_users\": user_ids},\n            nid=nid,\n            nid_key=\"id\"\n        )\n        return self._handle_error(r, \"Could not remove users.\")", "response": "Remove users from a network."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the feed from the network", "response": "def get_my_feed(self, limit=150, offset=20, sort=\"updated\", nid=None):\n        \"\"\"Get my feed\n\n        :type limit: int\n        :param limit: Number of posts from feed to get, starting from ``offset``\n        :type offset: int\n        :param offset: Offset starting from bottom of feed\n        :type sort: str\n        :param sort: How to sort feed that will be retrieved; only current\n            known value is \"updated\"\n        :type  nid: str\n        :param nid: This is the ID of the network to get the feed\n            from. This is optional and only to override the existing\n            `network_id` entered when created the class\n        \"\"\"\n        r = self.request(\n            method=\"network.get_my_feed\",\n            nid=nid,\n            data=dict(\n                limit=limit,\n                offset=offset,\n                sort=sort\n            )\n        )\n        return self._handle_error(r, \"Could not retrieve your feed.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef filter_feed(self, updated=False, following=False, folder=False,\n                    filter_folder=\"\", sort=\"updated\", nid=None):\n        \"\"\"Get filtered feed\n\n        Only one filter type (updated, following, folder) is possible.\n\n        :type  nid: str\n        :param nid: This is the ID of the network to get the feed\n            from. This is optional and only to override the existing\n            `network_id` entered when created the class\n        :type sort: str\n        :param sort: How to sort feed that will be retrieved; only current\n            known value is \"updated\"\n        :type updated: bool\n        :param updated: Set to filter through only posts which have been updated\n            since you last read them\n        :type following: bool\n        :param following: Set to filter through only posts which you are\n            following\n        :type folder: bool\n        :param folder: Set to filter through only posts which are in the\n            provided ``filter_folder``\n        :type filter_folder: str\n        :param filter_folder: Name of folder to show posts from; required\n            only if ``folder`` is set\n        \"\"\"\n        assert sum([updated, following, folder]) == 1\n        if folder:\n            assert filter_folder\n\n        if updated:\n            filter_type = dict(updated=1)\n        elif following:\n            filter_type = dict(following=1)\n        else:\n            filter_type = dict(folder=1, filter_folder=filter_folder)\n\n        r = self.request(\n            nid=nid,\n            method=\"network.filter_feed\",\n            data=dict(\n                sort=sort,\n                **filter_type\n            )\n        )\n        return self._handle_error(r, \"Could not retrieve filtered feed.\")", "response": "Get filtered feed from the network"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self, query, nid=None):\n        r = self.request(\n            method=\"network.search\",\n            nid=nid,\n            data=dict(query=query)\n        )\n        return self._handle_error(r, \"Search with query '{}' failed.\"\n                                  .format(query))", "response": "Search for posts with query"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget statistics for the class.", "response": "def get_stats(self, nid=None):\n        \"\"\"Get statistics for class\n\n        :type  nid: str\n        :param nid: This is the ID of the network to get stats\n            from. This is optional and only to override the existing\n            `network_id` entered when created the class\n        \"\"\"\n        r = self.request(\n            api_type=\"main\",\n            method=\"network.get_stats\",\n            nid=nid,\n        )\n        return self._handle_error(r, \"Could not retrieve stats for class.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a request to an arbitrary Piazza API endpoint.", "response": "def request(self, method, data=None, nid=None, nid_key='nid',\n                api_type=\"logic\", return_response=False):\n        \"\"\"Get data from arbitrary Piazza API endpoint `method` in network `nid`\n\n        :type  method: str\n        :param method: An internal Piazza API method name like `content.get`\n            or `network.get_users`\n        :type  data: dict\n        :param data: Key-value data to pass to Piazza in the request\n        :type  nid: str\n        :param nid: This is the ID of the network to which the request\n            should be made. This is optional and only to override the\n            existing `network_id` entered when creating the class\n        :type  nid_key: str\n        :param nid_key: Name expected by Piazza for `nid` when making request.\n            (Usually and by default \"nid\", but sometimes \"id\" is expected)\n        :returns: Python object containing returned data\n        :type return_response: bool\n        :param return_response: If set, returns whole :class:`requests.Response`\n            object rather than just the response body\n        \"\"\"\n        self._check_authenticated()\n\n        nid = nid if nid else self._nid\n        if data is None:\n            data = {}\n\n        headers = {}\n        if \"session_id\" in self.session.cookies:\n            headers[\"CSRF-Token\"] = self.session.cookies[\"session_id\"]\n\n        # Adding a nonce to the request\n        endpoint = self.base_api_urls[api_type]\n        if api_type == \"logic\":\n            endpoint += \"?method={}&aid={}\".format(\n                method,\n                _piazza_nonce()\n            )\n\n        response = self.session.post(\n            endpoint,\n            data=json.dumps({\n                \"method\": method,\n                \"params\": dict({nid_key: nid}, **data)\n            }),\n            headers=headers\n        )\n        return response if return_response else response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck result for error and raise an exception if it has", "response": "def _handle_error(self, result, err_msg):\n        \"\"\"Check result for error\n\n        :type result: dict\n        :param result: response body\n        :type err_msg: str\n        :param err_msg: The message given to the :class:`RequestError` instance\n            raised\n        :returns: Actual result from result\n        :raises RequestError: If result has error\n        \"\"\"\n        if result.get(u'error'):\n            raise RequestError(\"{}\\nResponse: {}\".format(\n                err_msg,\n                json.dumps(result, indent=2)\n            ))\n        else:\n            return result.get(u'result')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef user_login(self, email=None, password=None):\n        self._rpc_api = PiazzaRPC()\n        self._rpc_api.user_login(email=email, password=password)", "response": "Login with email and password and get back a session cookie"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef demo_login(self, auth=None, url=None):\n        self._rpc_api = PiazzaRPC()\n        self._rpc_api.demo_login(auth=auth, url=url)", "response": "Authenticate with a Share Your Class URL using a demo user."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef network(self, network_id):\n        self._ensure_authenticated()\n        return Network(network_id, self._rpc_api.session)", "response": "Returns a Network instance for the given network_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets list of currently authenticated user s classes. This is a subset of the user s status.", "response": "def get_user_classes(self):\n        \"\"\"Get list of the current user's classes. This is a subset of the\n        information returned by the call to ``get_user_status``.\n\n        :returns: Classes of currently authenticated user\n        :rtype: list\n        \"\"\"\n        # Previously getting classes from profile (such a list is incomplete)\n        # raw_classes = self.get_user_profile().get('all_classes').values()\n\n        # Get classes from the user status (includes all classes)\n        status = self.get_user_status()\n        uid = status['id']\n        raw_classes = status.get('networks', [])\n\n        classes = []\n        for rawc in raw_classes:\n            c = {k: rawc[k] for k in ['name', 'term']}\n            c['num'] = rawc.get('course_number', '')\n            c['nid'] = rawc['id']\n            c['is_ta'] = uid in rawc['prof_hash']\n            classes.append(c)\n\n        return classes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new nonce to be used with the Piazza API.", "response": "def nonce():\n    \"\"\"\n    Returns a new nonce to be used with the Piazza API.\n    \"\"\"\n    nonce_part1 = _int2base(int(_time()*1000), 36) \n    nonce_part2 = _int2base(round(_random()*1679616), 36)\n    return \"{}{}\".format(nonce_part1, nonce_part2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _int2base(x, base):\n    \n    if base > len(_exradix_digits):\n        raise ValueError(\n            \"Base is too large: The defined digit set only allows for \"\n            \"bases smaller than \" + len(_exradix_digits) + \".\"\n        )\n\n    if x > 0:\n        sign = 1\n    elif x == 0:\n        return _exradix_digits[0]\n    else:\n        sign = -1\n\n    x *= sign\n    digits = []\n\n    while x:\n        digits.append(\n            _exradix_digits[int(x % base)])\n        x = int(x / base)\n\n    if sign < 0:\n        digits.append('-')\n\n    digits.reverse()\n\n    return ''.join(digits)", "response": "Convert an integer from base 10 to some arbitrary numerical base and return a string representing the number in the new base."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all posts visible to the current user.", "response": "def iter_all_posts(self, limit=None):\n        \"\"\"Get all posts visible to the current user\n\n        This grabs you current feed and ids of all posts from it; each post\n        is then individually fetched. This method does not go against\n        a bulk endpoint; it retrieves each post individually, so a\n        caution to the user when using this.\n\n        :type limit: int|None\n        :param limit: If given, will limit the number of posts to fetch\n            before the generator is exhausted and raises StopIteration.\n            No special consideration is given to `0`; provide `None` to\n            retrieve all posts.\n        :returns: An iterator which yields all posts which the current user\n            can view\n        :rtype: generator\n        \"\"\"\n        feed = self.get_feed(limit=999999, offset=0)\n        cids = [post['id'] for post in feed[\"feed\"]]\n        if limit is not None:\n            cids = cids[:limit]\n        for cid in cids:\n            yield self.get_post(cid)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_post(self, post_type, post_folders, post_subject, post_content, is_announcement=0, bypass_email=0, anonymous=False):\n        params = {\n            \"anonymous\": \"yes\" if anonymous else \"no\",\n            \"subject\": post_subject,\n            \"content\": post_content,\n            \"folders\": post_folders,\n            \"type\": post_type,\n            \"config\": {\n                \"bypass_email\": bypass_email,\n                \"is_announcement\": is_announcement\n            }\n        }\n\n        return self._rpc.content_create(params)", "response": "Creates a post in the specified folders subject and content."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_followup(self, post, content, anonymous=False):\n        try:\n            cid = post[\"id\"]\n        except KeyError:\n            cid = post\n\n        params = {\n            \"cid\": cid,\n            \"type\": \"followup\",\n\n            # For followups, the content is actually put into the subject.\n            \"subject\": content,\n            \"content\": \"\",\n\n            \"anonymous\": \"yes\" if anonymous else \"no\",\n        }\n        return self._rpc.content_create(params)", "response": "Create a follow - up on a post."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instructor s answer to a post.", "response": "def create_instructor_answer(self, post, content, revision, anonymous=False):\n        \"\"\"Create an instructor's answer to a post `post`.\n\n        It seems like if the post has `<p>` tags, then it's treated as HTML,\n        but is treated as text otherwise. You'll want to provide `content`\n        accordingly.\n\n        :type  post: dict|str|int\n        :param post: Either the post dict returned by another API method, or\n            the `cid` field of that post.\n        :type  content: str\n        :param content: The content of the answer.\n        :type  revision: int\n        :param revision: The number of revisions the answer has gone through.\n            The first responder should out 0, the first editor 1, etc.\n        :type  anonymous: bool\n        :param anonymous: Whether or not to post anonymously.\n        :rtype: dict\n        :returns: Dictionary with information about the created answer.\n        \"\"\"\n        try:\n            cid = post[\"id\"]\n        except KeyError:\n            cid = post\n\n        params = {\n            \"cid\": cid,\n            \"type\": \"i_answer\",\n            \"content\": content,\n            \"revision\": revision,\n            \"anonymous\": \"yes\" if anonymous else \"no\",\n        }\n        return self._rpc.content_instructor_answer(params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mark_as_duplicate(self, duplicated_cid, master_cid, msg=''):\n        content_id_from = self.get_post(duplicated_cid)[\"id\"]\n        content_id_to = self.get_post(master_cid)[\"id\"]\n        params = {\n            \"cid_dupe\": content_id_from,\n            \"cid_to\": content_id_to,\n            \"msg\": msg\n        }\n        return self._rpc.content_mark_duplicate(params)", "response": "Mark the post at duplicated_cid as a duplicate of master_cid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_post(self, post):\n        try:\n            cid = post[\"id\"]\n        except KeyError:\n            cid = post\n\n        params = {\n            \"cid\": cid,\n            \"resolved\": \"true\"\n        }\n\n        return self._rpc.content_mark_resolved(params)", "response": "Mark post as resolved."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pin_post(self, post):\n        try:\n            cid = post['id']\n        except KeyError:\n            cid = post\n\n        params = {\n            \"cid\": cid,\n        }\n\n        return self._rpc.content_pin(params)", "response": "Pin post\n\n        :type  post: dict|str|int\n        :param post: Either the post dict returned by another API method, or\n            the `cid` field of that post.\n        :returns: True if it is successful. False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a post by cid.", "response": "def delete_post(self, post):\n        \"\"\" Deletes post by cid\n\n        :type  post: dict|str|int\n        :param post: Either the post dict returned by another API method, the post ID, or\n            the `cid` field of that post.\n        :rtype: dict\n        :returns: Dictionary with information about the post cid.\n        \"\"\"\n\n        try:\n            cid = post['id']\n        except KeyError:\n            cid = post\n        except TypeError:\n            post = self.get_post(post)\n            cid = post['id']\n\n        params = {\n            \"cid\": cid,\n        }\n\n        return self._rpc.content_delete(params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_feed(self, limit=100, offset=0):\n        return self._rpc.get_my_feed(limit=limit, offset=offset)", "response": "Get your network s feed for this network."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget your feed containing only posts filtered by feed_filter", "response": "def get_filtered_feed(self, feed_filter):\n        \"\"\"Get your feed containing only posts filtered by ``feed_filter``\n\n        :type feed_filter: FeedFilter\n        :param feed_filter: Must be an instance of either: UnreadFilter,\n            FollowingFilter, or FolderFilter\n        :rtype: dict\n        \"\"\"\n        assert isinstance(feed_filter, (UnreadFilter, FollowingFilter,\n                                        FolderFilter))\n        return self._rpc.filter_feed(**feed_filter.to_kwargs())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all the datasets.", "response": "def get_all_datasets(self):\n        \"\"\"\n        Make sure the datasets are present. If not, downloads and extracts them.\n        Attempts the download five times because the file hosting is unreliable.\n        :return: True if successful, false otherwise\n        \"\"\"\n        success = True\n\n        for dataset in tqdm(self.datasets):\n            individual_success = self.get_dataset(dataset)\n            if not individual_success:\n                success = False\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_dataset(self, dataset):\n        # If the dataset is present, no need to download anything.\n        success = True\n        dataset_path = self.base_dataset_path + dataset\n        if not isdir(dataset_path):\n\n            # Try 5 times to download. The download page is unreliable, so we need a few tries.\n            was_error = False\n            for iteration in range(5):\n\n                # Guard against trying again if successful\n                if iteration == 0 or was_error is True:\n                    zip_path = dataset_path + \".zip\"\n\n                    # Download zip files if they're not there\n                    if not isfile(zip_path):\n                        try:\n                            with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset) as pbar:\n                                urlretrieve(self.datasets[dataset][\"url\"], zip_path, pbar.hook)\n                        except Exception as ex:\n                            print(\"Error downloading %s: %s\" % (dataset, ex))\n                            was_error = True\n\n                    # Unzip the data files\n                    if not isdir(dataset_path):\n                        try:\n                            with zipfile.ZipFile(zip_path) as zip_archive:\n                                zip_archive.extractall(path=dataset_path)\n                                zip_archive.close()\n                        except Exception as ex:\n                            print(\"Error unzipping %s: %s\" % (zip_path, ex))\n                            # Usually the error is caused by a bad zip file.\n                            # Delete it so the program will try to download it again.\n                            try:\n                                remove(zip_path)\n                            except FileNotFoundError:\n                                pass\n                            was_error = True\n\n            if was_error:\n                print(\"\\nThis recognizer is trained by the CASIA handwriting database.\")\n                print(\"If the download doesn't work, you can get the files at %s\" % self.datasets[dataset][\"url\"])\n                print(\"If you have download problems, \"\n                      \"wget may be effective at downloading because of download resuming.\")\n                success = False\n\n        return success", "response": "Download and unzip the data files for the specified dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate directories of all the images and labels of all the data.", "response": "def get_raw(self, verbose=True):\n        \"\"\"\n        Used to create easily introspectable image directories of all the data.\n        :return:\n        \"\"\"\n        assert self.get_all_datasets() is True, \"Datasets aren't properly downloaded, \" \\\n                                                \"rerun to try again or download datasets manually.\"\n\n        for dataset in self.datasets:\n            # Create a folder to hold the dataset\n            prefix_path = self.base_dataset_path + \"raw/\" + dataset\n            if not isdir(prefix_path):\n                makedirs(prefix_path)\n\n            label_count = Counter()\n\n            for image, label in tqdm(self.load_dataset(dataset, verbose=verbose)):\n                #assert type(image) == \"PIL.Image.Image\", \"image is not the correct type. \"\n\n                # Make sure there's a folder for the class label.\n                label_path = prefix_path + \"/\" + label\n                if not isdir(label_path):\n                    makedirs(label_path)\n\n                label_count[label] = label_count[label] + 1\n\n                image.save(label_path + \"/%s_%s.jpg\" % (label, label_count[label]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a directory of gnt files.", "response": "def load_dataset(self, dataset, verbose=True):\n        \"\"\"\n        Load a directory of gnt files. Yields the image and label in tuples.\n        :param dataset: The directory to load.\n        :return:  Yields (Pillow.Image.Image, label) pairs.\n        \"\"\"\n        assert self.get_dataset(dataset) is True, \"Datasets aren't properly downloaded, \" \\\n                                                  \"rerun to try again or download datasets manually.\"\n\n        if verbose:\n            print(\"Loading %s\" % dataset)\n\n        dataset_path = self.base_dataset_path + dataset\n        for path in tqdm(glob.glob(dataset_path + \"/*.gnt\")):\n            for image, label in self.load_gnt_file(path):\n                yield image, label"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_gnt_file(filename):\n\n        # Thanks to nhatch for the code to read the GNT file, available at https://github.com/nhatch/casia\n        with open(filename, \"rb\") as f:\n            while True:\n                packed_length = f.read(4)\n                if packed_length == b'':\n                    break\n\n                length = struct.unpack(\"<I\", packed_length)[0]\n                raw_label = struct.unpack(\">cc\", f.read(2))\n                width = struct.unpack(\"<H\", f.read(2))[0]\n                height = struct.unpack(\"<H\", f.read(2))[0]\n                photo_bytes = struct.unpack(\"{}B\".format(height * width), f.read(height * width))\n\n                # Comes out as a tuple of chars. Need to be combined. Encoded as gb2312, gotta convert to unicode.\n                label = decode(raw_label[0] + raw_label[1], encoding=\"gb2312\")\n                # Create an array of bytes for the image, match it to the proper dimensions, and turn it into an image.\n                image = toimage(np.array(photo_bytes).reshape(height, width))\n\n                yield image, label", "response": "Load characters and images from a given GNT file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecorating and register a middleware function to use as the decorator", "response": "def middleware(self, *args, **kwargs):\n        \"\"\"Decorate and register middleware\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The middleware function to use as the decorator\n        :rtype: fn\n        \"\"\"\n        kwargs.setdefault('priority', 5)\n        kwargs.setdefault('relative', None)\n        kwargs.setdefault('attach_to', None)\n        kwargs.setdefault('with_context', False)\n        if len(args) == 1 and callable(args[0]):\n            middle_f = args[0]\n            self._middlewares.append(\n                FutureMiddleware(middle_f, args=tuple(), kwargs=kwargs))\n            return middle_f\n\n        def wrapper(middleware_f):\n            self._middlewares.append(\n                FutureMiddleware(middleware_f, args=args, kwargs=kwargs))\n            return middleware_f\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndecorates and register an exception handler", "response": "def exception(self, *args, **kwargs):\n        \"\"\"Decorate and register an exception handler\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the decorator\n        :rtype: fn\n        \"\"\"\n        if len(args) == 1 and callable(args[0]):\n            if isinstance(args[0], type) and issubclass(args[0], Exception):\n                pass\n            else:  # pragma: no cover\n                raise RuntimeError(\"Cannot use the @exception decorator \"\n                                   \"without arguments\")\n\n        def wrapper(handler_f):\n            self._exceptions.append(FutureException(handler_f,\n                                                    exceptions=args,\n                                                    kwargs=kwargs))\n            return handler_f\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a listener function that will be called when a specific event occurs.", "response": "def listener(self, event, *args, **kwargs):\n        \"\"\"Create a listener from a decorated function.\n        :param event: Event to listen to.\n        :type event: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The function to use as the listener\n        :rtype: fn\n        \"\"\"\n        if len(args) == 1 and callable(args[0]):  # pragma: no cover\n            raise RuntimeError(\"Cannot use the @listener decorator without \"\n                               \"arguments\")\n\n        def wrapper(listener_f):\n            if len(kwargs) > 0:\n                listener_f = (listener_f, kwargs)\n            self._listeners[event].append(listener_f)\n            return listener_f\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef route(self, uri, *args, **kwargs):\n        if len(args) == 0 and callable(uri):  # pragma: no cover\n            raise RuntimeError(\"Cannot use the @route decorator without \"\n                               \"arguments.\")\n        kwargs.setdefault('methods', frozenset({'GET'}))\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', False)\n        kwargs.setdefault('stream', False)\n        kwargs.setdefault('name', None)\n\n        def wrapper(handler_f):\n            self._routes.append(FutureRoute(handler_f, uri, args, kwargs))\n            return handler_f\n        return wrapper", "response": "Decorator to create a plugin route from a function."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a websocket route from a decorated function :param uri: endpoint at which the socket endpoint will be accessible. :type uri: str :param args: captures all of the positional arguments passed in :type args: tuple(Any) :param kwargs: captures the keyword arguments passed in :type kwargs: dict(Any) :return: The exception function to use as the decorator :rtype: fn", "response": "def websocket(self, uri, *args, **kwargs):\n        \"\"\"Create a websocket route from a decorated function\n        :param uri: endpoint at which the socket endpoint will be accessible.\n        :type uri: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the decorator\n        :rtype: fn\n        \"\"\"\n\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', None)\n        kwargs.setdefault('subprotocols', None)\n        kwargs.setdefault('name', None)\n\n        def wrapper(handler_f):\n            self._ws.append(FutureWebsocket(handler_f, uri, args, kwargs))\n            return handler_f\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a websocket route from a decorated function", "response": "def static(self, uri, file_or_directory, *args, **kwargs):\n        \"\"\"Create a websocket route from a decorated function\n        :param uri: endpoint at which the socket endpoint will be accessible.\n        :type uri: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the decorator\n        :rtype: fn\n        \"\"\"\n\n        kwargs.setdefault('pattern', r'/?.+')\n        kwargs.setdefault('use_modified_since', True)\n        kwargs.setdefault('use_content_range', False)\n        kwargs.setdefault('stream_large_files', False)\n        kwargs.setdefault('name', 'static')\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', None)\n\n        self._static.append(FutureStatic(uri, file_or_directory, args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef first_plugin_context(self):\n        # Note, because registrations are stored in a set, its not _really_\n        # the first one, but whichever one it sees first in the set.\n        first_spf_reg = next(iter(self.registrations))\n        return self.get_context_from_spf(first_spf_reg)", "response": "Returns the context associated with the first app this plugin was\n         registered on"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def route_wrapper(self, route, request, context, request_args,\n                            request_kw, *decorator_args, with_context=None,\n                            **decorator_kw):\n        \"\"\"This is the function that is called when a route is decorated with\n           your plugin decorator. Context will normally be None, but the user\n           can pass use_context=True so the route will get the plugin\n           context\n        \"\"\"\n        # by default, do nothing, just run the wrapped function\n        if with_context:\n            resp = route(request, context, *request_args, **request_kw)\n        else:\n            resp = route(request, *request_args, **request_kw)\n        if isawaitable(resp):\n            resp = await resp\n        return resp", "response": "This is the function that is called when a route is decorated with\n          ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces the value of a key in this ContextDict.", "response": "def replace(self, key, value):\n        \"\"\"\n        If this ContextDict doesn't already have this key, it sets\n        the value on a parent ContextDict if that parent has the key,\n        otherwise sets the value on this ContextDict.\n        :param key:\n        :param value:\n        :return: Nothing\n        :rtype: None\n        \"\"\"\n        if key in self._inner().keys():\n            return self.__setitem__(key, value)\n        parents_searched = [self]\n        parent = self._parent_context\n        while parent:\n            try:\n                if key in parent.keys():\n                    return parent.__setitem__(key, value)\n            except (KeyError, AttributeError):\n                pass\n            parents_searched.append(parent)\n            # noinspection PyProtectedMember\n            next_parent = parent._parent_context\n            if next_parent in parents_searched:\n                raise RuntimeError(\"Recursive ContextDict found!\")\n            parent = next_parent\n        return self.__setitem__(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, E=None, **F):\n        if E is not None:\n            if hasattr(E, 'keys'):\n                for K in E:\n                    self.replace(K, E[K])\n            elif hasattr(E, 'items'):\n                for K, V in E.items():\n                    self.replace(K, V)\n            else:\n                for K, V in E:\n                    self.replace(K, V)\n        for K in F:\n            self.replace(K, F[K])", "response": "Update the ContextDict from dict E and F."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndecorate and register middleware that will be called by the middleware functions in order to get the correct context.", "response": "def middleware(self, *args, **kwargs):\n        \"\"\"Decorate and register middleware\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The middleware function to use as the decorator\n        :rtype: fn\n        \"\"\"\n        kwargs.setdefault('priority', 5)\n        kwargs.setdefault('relative', None)\n        kwargs.setdefault('attach_to', None)\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n        plugin = self.plugin\n        reg = self.reg\n\n        if len(args) == 1 and callable(args[0]):\n            middle_f = args[0]\n            return plugin._add_new_middleware(reg, middle_f, **kwargs)\n\n        def wrapper(middle_f):\n            nonlocal plugin, reg\n            nonlocal args, kwargs\n            return plugin._add_new_middleware(reg, middle_f, *args, **kwargs)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a plugin route from a decorated function. :param uri: endpoint at which the route will be accessible. :type uri: str :param args: captures all of the positional arguments passed in :type args: tuple(Any) :param kwargs: captures the keyword arguments passed in :type kwargs: dict(Any) :return: The exception function to use as the decorator :rtype: fn", "response": "def route(self, uri, *args, **kwargs):\n        \"\"\"Create a plugin route from a decorated function.\n        :param uri: endpoint at which the route will be accessible.\n        :type uri: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the decorator\n        :rtype: fn\n        \"\"\"\n        if len(args) == 0 and callable(uri):\n            raise RuntimeError(\"Cannot use the @route decorator without \"\n                               \"arguments.\")\n        kwargs.setdefault('methods', frozenset({'GET'}))\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', False)\n        kwargs.setdefault('stream', False)\n        kwargs.setdefault('name', None)\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n        plugin = self.plugin\n        reg = self.reg\n\n        def wrapper(handler_f):\n            nonlocal plugin, reg\n            nonlocal uri, args, kwargs\n            return plugin._add_new_route(reg, uri, handler_f, *args, **kwargs)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef listener(self, event, *args, **kwargs):\n        if len(args) == 1 and callable(args[0]):\n            raise RuntimeError(\"Cannot use the @listener decorator without \"\n                               \"arguments\")\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n        plugin = self.plugin\n        reg = self.reg\n\n        def wrapper(listener_f):\n            nonlocal plugin, reg\n            nonlocal event, args, kwargs\n            return plugin._add_new_listener(reg, event, listener_f, *args,\n                                            **kwargs)\n        return wrapper", "response": "Create a listener from a decorated function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a websocket route from a decorated function :param uri: endpoint at which the socket endpoint will be accessible. :type uri: str :param args: captures all of the positional arguments passed in :type args: tuple(Any) :param kwargs: captures the keyword arguments passed in :type kwargs: dict(Any) :return: The exception function to use as the decorator :rtype: fn", "response": "def websocket(self, uri, *args, **kwargs):\n        \"\"\"Create a websocket route from a decorated function\n        :param uri: endpoint at which the socket endpoint will be accessible.\n        :type uri: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the decorator\n        :rtype: fn\n        \"\"\"\n\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', None)\n        kwargs.setdefault('subprotocols', None)\n        kwargs.setdefault('name', None)\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n        plugin = self.plugin\n        reg = self.reg\n\n        def wrapper(handler_f):\n            nonlocal plugin, reg\n            nonlocal uri, args, kwargs\n            return plugin._add_new_ws_route(reg, uri, handler_f,\n                                            *args, **kwargs)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef middleware(self, *args, **kwargs):\n        kwargs.setdefault('priority', 5)\n        kwargs.setdefault('relative', None)\n        kwargs.setdefault('attach_to', None)\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n        if len(args) == 1 and callable(args[0]):\n            middle_f = args[0]\n            return super(Contextualize, self).middleware(middle_f, **kwargs)\n\n        def wrapper(middle_f):\n            nonlocal self, args, kwargs\n            return super(Contextualize, self).middleware(\n                *args, **kwargs)(middle_f)\n        return wrapper", "response": "Decorate and register middleware\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a plugin route from a decorated function. :param uri: endpoint at which the route will be accessible. :type uri: str :param args: captures all of the positional arguments passed in :type args: tuple(Any) :param kwargs: captures the keyword arguments passed in :type kwargs: dict(Any) :return: The exception function to use as the decorator :rtype: fn", "response": "def route(self, uri, *args, **kwargs):\n        \"\"\"Create a plugin route from a decorated function.\n        :param uri: endpoint at which the route will be accessible.\n        :type uri: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the decorator\n        :rtype: fn\n        \"\"\"\n        if len(args) == 0 and callable(uri):\n            raise RuntimeError(\"Cannot use the @route decorator without \"\n                               \"arguments.\")\n        kwargs.setdefault('methods', frozenset({'GET'}))\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', False)\n        kwargs.setdefault('stream', False)\n        kwargs.setdefault('name', None)\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n\n        def wrapper(handler_f):\n            nonlocal self, uri, args, kwargs\n            return super(Contextualize, self).route(\n                uri, *args, **kwargs)(handler_f)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a listener from a decorated function. :param event: Event to listen to. :type event: str :param args: captures all of the positional arguments passed in :type args: tuple(Any) :param kwargs: captures the keyword arguments passed in :type kwargs: dict(Any) :return: The exception function to use as the listener :rtype: fn", "response": "def listener(self, event, *args, **kwargs):\n        \"\"\"Create a listener from a decorated function.\n        :param event: Event to listen to.\n        :type event: str\n        :param args: captures all of the positional arguments passed in\n        :type args: tuple(Any)\n        :param kwargs: captures the keyword arguments passed in\n        :type kwargs: dict(Any)\n        :return: The exception function to use as the listener\n        :rtype: fn\n        \"\"\"\n        if len(args) == 1 and callable(args[0]):\n            raise RuntimeError(\"Cannot use the @listener decorator without \"\n                               \"arguments\")\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n\n        def wrapper(listener_f):\n            nonlocal self, event, args, kwargs\n            return super(Contextualize, self).listener(\n                event, *args, **kwargs)(listener_f)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef websocket(self, uri, *args, **kwargs):\n\n        kwargs.setdefault('host', None)\n        kwargs.setdefault('strict_slashes', None)\n        kwargs.setdefault('subprotocols', None)\n        kwargs.setdefault('name', None)\n        kwargs['with_context'] = True  # This is the whole point of this plugin\n\n        def wrapper(handler_f):\n            nonlocal self, uri, args, kwargs\n            return super(Contextualize, self).websocket(\n                uri, *args, **kwargs)(handler_f)\n        return wrapper", "response": "Decorator to create a websocket route from a decorated function"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_peercred(sock):\n    buf = sock.getsockopt(_PEERCRED_LEVEL, _PEERCRED_OPTION, struct.calcsize('3i'))\n    return struct.unpack('3i', buf)", "response": "Gets the pid uid gid for the client on the given connected socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_credentials(client):\n    pid, uid, gid = get_peercred(client)\n\n    euid = os.geteuid()\n    client_name = \"PID:%s UID:%s GID:%s\" % (pid, uid, gid)\n    if uid not in (0, euid):\n        raise SuspiciousClient(\"Can't accept client with %s. It doesn't match the current EUID:%s or ROOT.\" % (\n            client_name, euid\n        ))\n\n    _LOG(\"Accepted connection on fd:%s from %s\" % (client.fileno(), client_name))\n    return pid, uid, gid", "response": "Checks credentials for given socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles the connection to the ACS.", "response": "def handle_connection_exec(client):\n    \"\"\"\n    Alternate connection handler. No output redirection.\n    \"\"\"\n    class ExitExecLoop(Exception):\n        pass\n\n    def exit():\n        raise ExitExecLoop()\n\n    client.settimeout(None)\n    fh = os.fdopen(client.detach() if hasattr(client, 'detach') else client.fileno())\n\n    with closing(client):\n        with closing(fh):\n            try:\n                payload = fh.readline()\n                while payload:\n                    _LOG(\"Running: %r.\" % payload)\n                    eval(compile(payload, '<manhole>', 'exec'), {'exit': exit}, _MANHOLE.locals)\n                    payload = fh.readline()\n            except ExitExecLoop:\n                _LOG(\"Exiting exec loop.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a REPL connection.", "response": "def handle_connection_repl(client):\n    \"\"\"\n    Handles connection.\n    \"\"\"\n    client.settimeout(None)\n    # # disable this till we have evidence that it's needed\n    # client.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 0)\n    # # Note: setting SO_RCVBUF on UDS has no effect, see: http://man7.org/linux/man-pages/man7/unix.7.html\n\n    backup = []\n    old_interval = getinterval()\n    patches = [('r', ('stdin', '__stdin__')), ('w', ('stdout', '__stdout__'))]\n    if _MANHOLE.redirect_stderr:\n        patches.append(('w', ('stderr', '__stderr__')))\n    try:\n        client_fd = client.fileno()\n        for mode, names in patches:\n            for name in names:\n                backup.append((name, getattr(sys, name)))\n                setattr(sys, name, _ORIGINAL_FDOPEN(client_fd, mode, 1 if PY3 else 0))\n        try:\n            handle_repl(_MANHOLE.locals)\n        except Exception as exc:\n            _LOG(\"REPL failed with %r.\" % exc)\n        _LOG(\"DONE.\")\n    finally:\n        try:\n            # Change the switch/check interval to something ridiculous. We don't want to have other thread try\n            # to write to the redirected sys.__std*/sys.std* - it would fail horribly.\n            setinterval(2147483647)\n            try:\n                client.close()  # close before it's too late. it may already be dead\n            except IOError:\n                pass\n            junk = []  # keep the old file objects alive for a bit\n            for name, fh in backup:\n                junk.append(getattr(sys, name))\n                setattr(sys, name, fh)\n            del backup\n            for fh in junk:\n                try:\n                    if hasattr(fh, 'detach'):\n                        fh.detach()\n                    else:\n                        fh.close()\n                except IOError:\n                    pass\n                del fh\n            del junk\n        finally:\n            setinterval(old_interval)\n            _LOG(\"Cleaned up.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_repl(locals):\n    dump_stacktraces()\n    namespace = {\n        'dump_stacktraces': dump_stacktraces,\n        'sys': sys,\n        'os': os,\n        'socket': socket,\n        'traceback': traceback,\n    }\n    if locals:\n        namespace.update(locals)\n    ManholeConsole(namespace).interact()", "response": "Handles REPL - style output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninstalling the manhole. Args: verbose (bool): Set it to ``False`` to squelch the logging. verbose_destination (file descriptor or handle): Destination for verbose messages. Default is unbuffered stderr (stderr ``2`` file descriptor). patch_fork (bool): Set it to ``False`` if you don't want your ``os.fork`` and ``os.forkpy`` monkeypatched activate_on (int or signal name): set to ``\"USR1\"``, ``\"USR2\"`` or some other signal name, or a number if you want the Manhole thread to start when this signal is sent. This is desireable in case you don't want the thread active all the time. oneshot_on (int or signal name): Set to ``\"USR1\"``, ``\"USR2\"`` or some other signal name, or a number if you want the Manhole to listen for connection in the signal handler. This is desireable in case you don't want threads at all. thread (bool): Start the always-on ManholeThread. Default: ``True``. Automatically switched to ``False`` if ``oneshort_on`` or ``activate_on`` are used. sigmask (list of ints or signal names): Will set the signal mask to the given list (using ``signalfd.sigprocmask``). No action is done if ``signalfd`` is not importable. **NOTE**: This is done so that the Manhole thread doesn't *steal* any signals; Normally that is fine because Python will force all the signal handling to be run in the main thread but signalfd doesn't. socket_path (str): Use a specific path for the unix domain socket (instead of ``/tmp/manhole-<pid>``). This disables ``patch_fork`` as children cannot reuse the same path. reinstall_delay (float): Delay the unix domain socket creation *reinstall_delay* seconds. This alleviates cleanup failures when using fork+exec patterns. locals (dict): Names to add to manhole interactive shell locals. daemon_connection (bool): The connection thread is daemonic (dies on app exit). Default: ``False``. redirect_stderr (bool): Redirect output from stderr to manhole console. Default: ``True``. connection_handler (function): Connection handler to use. Use ``\"exec\"`` for simple implementation without output redirection or your own function. (warning: this is for advanced users). Default: ``\"repl\"``.", "response": "def install(verbose=True,\n            verbose_destination=sys.__stderr__.fileno() if hasattr(sys.__stderr__, 'fileno') else sys.__stderr__,\n            strict=True,\n            **kwargs):\n    \"\"\"\n    Installs the manhole.\n\n    Args:\n        verbose (bool): Set it to ``False`` to squelch the logging.\n        verbose_destination (file descriptor or handle): Destination for verbose messages. Default is unbuffered stderr\n            (stderr ``2`` file descriptor).\n        patch_fork (bool): Set it to ``False`` if you don't want your ``os.fork`` and ``os.forkpy`` monkeypatched\n        activate_on (int or signal name): set to ``\"USR1\"``, ``\"USR2\"`` or some other signal name, or a number if you\n            want the Manhole thread to start when this signal is sent. This is desireable in case you don't want the\n            thread active all the time.\n        oneshot_on (int or signal name): Set to ``\"USR1\"``, ``\"USR2\"`` or some other signal name, or a number if you\n            want the Manhole to listen for connection in the signal handler. This is desireable in case you don't want\n            threads at all.\n        thread (bool): Start the always-on ManholeThread. Default: ``True``. Automatically switched to ``False`` if\n            ``oneshort_on`` or ``activate_on`` are used.\n        sigmask (list of ints or signal names): Will set the signal mask to the given list (using\n            ``signalfd.sigprocmask``). No action is done if ``signalfd`` is not importable.\n            **NOTE**: This is done so that the Manhole thread doesn't *steal* any signals; Normally that is fine because\n            Python will force all the signal handling to be run in the main thread but signalfd doesn't.\n        socket_path (str): Use a specific path for the unix domain socket (instead of ``/tmp/manhole-<pid>``). This\n            disables ``patch_fork`` as children cannot reuse the same path.\n        reinstall_delay (float): Delay the unix domain socket creation *reinstall_delay* seconds. This\n            alleviates cleanup failures when using fork+exec patterns.\n        locals (dict): Names to add to manhole interactive shell locals.\n        daemon_connection (bool): The connection thread is daemonic (dies on app exit). Default: ``False``.\n        redirect_stderr (bool): Redirect output from stderr to manhole console. Default: ``True``.\n        connection_handler (function): Connection handler to use. Use ``\"exec\"`` for simple implementation without\n            output redirection or your own function. (warning: this is for advanced users). Default: ``\"repl\"``.\n    \"\"\"\n    # pylint: disable=W0603\n    global _MANHOLE\n\n    with _LOCK:\n        if _MANHOLE is None:\n            _MANHOLE = Manhole()\n        else:\n            if strict:\n                raise AlreadyInstalled(\"Manhole already installed!\")\n            else:\n                _LOG.release()\n                _MANHOLE.release()  # Threads might be started here\n\n    _LOG.configure(verbose, verbose_destination)\n    _MANHOLE.configure(**kwargs)  # Threads might be started here\n    return _MANHOLE"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dump_stacktraces():\n    lines = []\n    for thread_id, stack in sys._current_frames().items():  # pylint: disable=W0212\n        lines.append(\"\\n######### ProcessID=%s, ThreadID=%s #########\" % (\n            os.getpid(), thread_id\n        ))\n        for filename, lineno, name, line in traceback.extract_stack(stack):\n            lines.append('File: \"%s\", line %d, in %s' % (filename, lineno, name))\n            if line:\n                lines.append(\"  %s\" % (line.strip()))\n    lines.append(\"#############################################\\n\\n\")\n\n    print('\\n'.join(lines), file=sys.stderr if _MANHOLE.redirect_stderr else sys.stdout)", "response": "Dumps thread ids and tracebacks to stdout."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clone(self, **kwargs):\n        return ManholeThread(\n            self.get_socket, self.sigmask, self.start_timeout,\n            connection_handler=self.connection_handler,\n            daemon_connection=self.daemon_connection,\n            **kwargs\n        )", "response": "Make a fresh thread with the same options. This is usually used on dead threads."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the manhole loop.", "response": "def run(self):\n        \"\"\"\n        Runs the manhole loop. Only accepts one connection at a time because:\n\n        * This thread is a daemon thread (exits when main thread exists).\n        * The connection need exclusive access to stdin, stderr and stdout so it can redirect inputs and outputs.\n        \"\"\"\n        self.serious.set()\n        if signalfd and self.sigmask:\n            signalfd.sigprocmask(signalfd.SIG_BLOCK, self.sigmask)\n        pthread_setname_np(self.ident, self.psname)\n\n        if self.bind_delay:\n            _LOG(\"Delaying UDS binding %s seconds ...\" % self.bind_delay)\n            _ORIGINAL_SLEEP(self.bind_delay)\n\n        sock = self.get_socket()\n        while self.should_run:\n            _LOG(\"Waiting for new connection (in pid:%s) ...\" % os.getpid())\n            try:\n                client = ManholeConnectionThread(sock.accept()[0], self.connection_handler, self.daemon_connection)\n                client.start()\n                client.join()\n            except socket.timeout:\n                continue\n            except (InterruptedError, socket.error) as e:\n                if e.errno != errno.EINTR:\n                    raise\n                continue\n            finally:\n                client = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reinstall(self):\n        with _LOCK:\n            if not (self.thread.is_alive() and self.thread in _ORIGINAL__ACTIVE):\n                self.thread = self.thread.clone(bind_delay=self.reinstall_delay)\n                if self.should_restart:\n                    self.thread.start()", "response": "Reinstalls the manhole. Checks if the thread is running."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfork a child process.", "response": "def patched_fork(self):\n        \"\"\"Fork a child process.\"\"\"\n        pid = self.original_os_fork()\n        if not pid:\n            _LOG('Fork detected. Reinstalling Manhole.')\n            self.reinstall()\n        return pid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef patched_forkpty(self):\n        pid, master_fd = self.original_os_forkpty()\n        if not pid:\n            _LOG('Fork detected. Reinstalling Manhole.')\n            self.reinstall()\n        return pid, master_fd", "response": "Fork a new process with a new pseudo - terminal as controlling tty."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(  # noqa: C901\n            self, alert_condition_nrql_id, policy_id, name=None, threshold_type=None, query=None,\n            since_value=None, terms=None, expected_groups=None, value_function=None,\n            runbook_url=None, ignore_overlap=None, enabled=True):\n        \"\"\"\n        Updates any of the optional parameters of the alert condition nrql\n\n        :type alert_condition_nrql_id: int\n        :param alert_condition_nrql_id: Alerts condition NRQL id to update\n\n        :type policy_id: int\n        :param policy_id: Alert policy id where target alert condition belongs to\n\n        :type condition_scope: str\n        :param condition_scope: The scope of the condition, can be instance or application\n\n        :type name: str\n        :param name: The name of the alert\n\n        :type threshold_type: str\n        :param threshold_type: The tthreshold_typeype of the condition, can be static or outlier\n\n        :type query: str\n        :param query: nrql query for the alerts\n\n        :type since_value: str\n        :param since_value: since value for the alert\n\n        :type terms: list[hash]\n        :param terms: list of hashes containing threshold config for the alert\n\n        :type expected_groups: int\n        :param expected_groups: expected groups setting for outlier alerts\n\n        :type value_function: str\n        :param type: value function for static alerts\n\n        :type runbook_url: str\n        :param runbook_url: The url of the runbook\n\n        :type ignore_overlap: bool\n        :param ignore_overlap: Whether to ignore overlaps for outlier alerts\n\n        :type enabled: bool\n        :param enabled: Whether to enable that alert condition\n\n        :rtype: dict\n        :return: The JSON response of the API\n\n        :raises: This will raise a\n            :class:`NewRelicAPIServerException<newrelic_api.exceptions.NoEntityException>`\n            if target alert condition is not included in target policy\n\n        :raises: This will raise a\n            :class:`ConfigurationException<newrelic_api.exceptions.ConfigurationException>`\n            if metric is set as user_defined but user_defined config is not passed\n        ::\n        {\n            \"nrql_condition\": {\n                \"name\": \"string\",\n                \"runbook_url\": \"string\",\n                \"enabled\": \"boolean\",\n                \"expected_groups\": \"integer\",\n                \"ignore_overlap\": \"boolean\",\n                \"value_function\": \"string\",\n                \"terms\": [\n                    {\n                        \"duration\": \"string\",\n                        \"operator\": \"string\",\n                        \"priority\": \"string\",\n                        \"threshold\": \"string\",\n                        \"time_function\": \"string\"\n                    }\n                ],\n                \"nrql\": {\n                    \"query\": \"string\",\n                    \"since_value\": \"string\"\n                }\n            }\n        }\n        \"\"\"\n\n        conditions_nrql_dict = self.list(policy_id)\n        target_condition_nrql = None\n        for condition in conditions_nrql_dict['nrql_conditions']:\n            if int(condition['id']) == alert_condition_nrql_id:\n                target_condition_nrql = condition\n                break\n\n        if target_condition_nrql is None:\n            raise NoEntityException(\n                'Target alert condition nrql is not included in that policy.'\n                'policy_id: {}, alert_condition_nrql_id {}'.format(\n                    policy_id,\n                    alert_condition_nrql_id\n                )\n            )\n\n        data = {\n            'nrql_condition': {\n                'type': threshold_type or target_condition_nrql['type'],\n                'enabled': target_condition_nrql['enabled'],\n                'name': name or target_condition_nrql['name'],\n                'terms': terms or target_condition_nrql['terms'],\n                'nrql': {\n                    'query': query or target_condition_nrql['nrql']['query'],\n                    'since_value': since_value or target_condition_nrql['nrql']['since_value'],\n                }\n            }\n        }\n\n        if enabled is not None:\n            data['nrql_condition']['enabled'] = str(enabled).lower()\n\n        if runbook_url is not None:\n            data['nrql_condition']['runbook_url'] = runbook_url\n        elif 'runbook_url' in target_condition_nrql:\n            data['nrql_condition']['runbook_url'] = target_condition_nrql['runbook_url']\n\n        if expected_groups is not None:\n            data['nrql_condition']['expected_groups'] = expected_groups\n        elif 'expected_groups' in target_condition_nrql:\n            data['nrql_condition']['expected_groups'] = target_condition_nrql['expected_groups']\n\n        if ignore_overlap is not None:\n            data['nrql_condition']['ignore_overlap'] = ignore_overlap\n        elif 'ignore_overlap' in target_condition_nrql:\n            data['nrql_condition']['ignore_overlap'] = target_condition_nrql['ignore_overlap']\n\n        if value_function is not None:\n            data['nrql_condition']['value_function'] = value_function\n        elif 'value_function' in target_condition_nrql:\n            data['nrql_condition']['value_function'] = target_condition_nrql['value_function']\n\n        if data['nrql_condition']['type'] == 'static':\n            if 'value_function' not in data['nrql_condition']:\n                raise ConfigurationException(\n                    'Alert is set as static but no value_function config specified'\n                )\n            data['nrql_condition'].pop('expected_groups', None)\n            data['nrql_condition'].pop('ignore_overlap', None)\n\n        elif data['nrql_condition']['type'] == 'outlier':\n            if 'expected_groups' not in data['nrql_condition']:\n                raise ConfigurationException(\n                    'Alert is set as outlier but expected_groups config is not specified'\n                )\n            if 'ignore_overlap' not in data['nrql_condition']:\n                raise ConfigurationException(\n                    'Alert is set as outlier but ignore_overlap config is not  specified'\n                )\n            data['nrql_condition'].pop('value_function', None)\n\n        return self._put(\n            url='{0}alerts_nrql_conditions/{1}.json'.format(self.URL, alert_condition_nrql_id),\n            headers=self.headers,\n            data=data\n        )", "response": "Updates the values of the alert condition nrql_id policy_id and name of the target alert condition with optional parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(\n            self, policy_id, name, threshold_type, query, since_value, terms,\n            expected_groups=None, value_function=None, runbook_url=None,\n            ignore_overlap=None, enabled=True):\n        \"\"\"\n        Creates an alert condition nrql\n\n        :type policy_id: int\n        :param policy_id: Alert policy id where target alert condition nrql belongs to\n\n        :type name: str\n        :param name: The name of the alert\n\n        :type threshold_type: str\n        :param type: The threshold_type of the condition, can be static or outlier\n\n        :type query: str\n        :param query: nrql query for the alerts\n\n        :type since_value: str\n        :param since_value: since value for the alert\n\n        :type terms: list[hash]\n        :param terms: list of hashes containing threshold config for the alert\n\n        :type expected_groups: int\n        :param expected_groups: expected groups setting for outlier alerts\n\n        :type value_function: str\n        :param type: value function for static alerts\n\n        :type runbook_url: str\n        :param runbook_url: The url of the runbook\n\n        :type ignore_overlap: bool\n        :param ignore_overlap: Whether to ignore overlaps for outlier alerts\n\n        :type enabled: bool\n        :param enabled: Whether to enable that alert condition\n\n        :rtype: dict\n        :return: The JSON response of the API\n\n        :raises: This will raise a\n            :class:`NewRelicAPIServerException<newrelic_api.exceptions.NoEntityException>`\n            if target alert condition is not included in target policy\n\n        :raises: This will raise a\n            :class:`ConfigurationException<newrelic_api.exceptions.ConfigurationException>`\n            if metric is set as user_defined but user_defined config is not passed\n        ::\n        {\n            \"nrql_condition\": {\n                \"name\": \"string\",\n                \"runbook_url\": \"string\",\n                \"enabled\": \"boolean\",\n                \"expected_groups\": \"integer\",\n                \"ignore_overlap\": \"boolean\",\n                \"value_function\": \"string\",\n                \"terms\": [\n                    {\n                        \"duration\": \"string\",\n                        \"operator\": \"string\",\n                        \"priority\": \"string\",\n                        \"threshold\": \"string\",\n                        \"time_function\": \"string\"\n                    }\n                ],\n                \"nrql\": {\n                    \"query\": \"string\",\n                    \"since_value\": \"string\"\n                }\n            }\n        }\n        \"\"\"\n\n        data = {\n            'nrql_condition': {\n                'type': threshold_type,\n                'name': name,\n                'enabled': enabled,\n                'terms': terms,\n                'nrql': {\n                    'query': query,\n                    'since_value': since_value\n                }\n            }\n        }\n\n        if runbook_url is not None:\n            data['nrql_condition']['runbook_url'] = runbook_url\n\n        if expected_groups is not None:\n            data['nrql_condition']['expected_groups'] = expected_groups\n\n        if ignore_overlap is not None:\n            data['nrql_condition']['ignore_overlap'] = ignore_overlap\n\n        if value_function is not None:\n            data['nrql_condition']['value_function'] = value_function\n\n        if data['nrql_condition']['type'] == 'static':\n            if 'value_function' not in data['nrql_condition']:\n                raise ConfigurationException(\n                    'Alert is set as static but no value_function config specified'\n                )\n            data['nrql_condition'].pop('expected_groups', None)\n            data['nrql_condition'].pop('ignore_overlap', None)\n\n        elif data['nrql_condition']['type'] == 'outlier':\n            if 'expected_groups' not in data['nrql_condition']:\n                raise ConfigurationException(\n                    'Alert is set as outlier but expected_groups config is not specified'\n                )\n            if 'ignore_overlap' not in data['nrql_condition']:\n                raise ConfigurationException(\n                    'Alert is set as outlier but ignore_overlap config is not  specified'\n                )\n            data['nrql_condition'].pop('value_function', None)\n\n        return self._post(\n            url='{0}alerts_nrql_conditions/policies/{1}.json'.format(self.URL, policy_id),\n            headers=self.headers,\n            data=data\n        )", "response": "Creates an alert condition for the given policy and nrql query"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, alert_condition_nrql_id):\n\n        return self._delete(\n            url='{0}alerts_nrql_conditions/{1}.json'.format(self.URL, alert_condition_nrql_id),\n            headers=self.headers\n        )", "response": "This API endpoint allows you to delete an alert condition nrql."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, filter_name=None, filter_ids=None, filter_labels=None, page=None):\n        label_param = ''\n\n        if filter_labels:\n            label_param = ';'.join(['{}:{}'.format(label, value) for label, value in filter_labels.items()])\n\n        filters = [\n            'filter[name]={0}'.format(filter_name) if filter_name else None,\n            'filter[ids]={0}'.format(','.join([str(app_id) for app_id in filter_ids])) if filter_ids else None,\n            'filter[labels]={0}'.format(label_param) if filter_labels else None,\n            'page={0}'.format(page) if page else None\n        ]\n\n        return self._get(\n            url='{0}servers.json'.format(self.URL),\n            headers=self.headers,\n            params=self.build_param_string(filters)\n        )", "response": "This API endpoint returns a paginated list of the Servers\n            associated with your New Relic account."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, id, name=None):\n        nr_data = self.show(id)['server']\n\n        data = {\n            'server': {\n                'name': name or nr_data['name'],\n            }\n        }\n\n        return self._put(\n            url='{0}servers/{1}.json'.format(self.URL, id),\n            headers=self.headers,\n            data=data\n        )", "response": "Updates any of the optional parameters of the server with the specified ID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef metric_names(self, id, name=None, page=None):\n        params = [\n            'name={0}'.format(name) if name else None,\n            'page={0}'.format(page) if page else None\n        ]\n\n        return self._get(\n            url='{0}servers/{1}/metrics.json'.format(self.URL, id),\n            headers=self.headers,\n            params=self.build_param_string(params)\n        )", "response": "Return a list of known metrics and their value names for the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, id, name, incident_preference):\n\n        data = {\n            \"policy\": {\n                \"name\": name,\n                \"incident_preference\": incident_preference\n            }\n        }\n\n        return self._put(\n            url='{0}alerts_policies/{1}.json'.format(self.URL, id),\n            headers=self.headers,\n            data=data\n        )", "response": "This API endpoint allows you to update an alert policy"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, id):\n\n        return self._delete(\n            url='{0}alerts_policies/{1}.json'.format(self.URL, id),\n            headers=self.headers\n        )", "response": "This API endpoint allows you to delete an alert policy by its id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dissociate_from_notification_channel(self, id, channel_id):\n\n        return self._delete(\n            url='{0}alerts_policy_channels.json?policy_id={1}&channel_id={2}'.format(\n                self.URL,\n                id,\n                channel_id\n            ),\n            headers=self.headers\n        )", "response": "This endpoint allows you to dissociate an alert policy from an alert channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(\n            self, alert_condition_id, policy_id,\n            type=None,\n            condition_scope=None,\n            name=None,\n            entities=None,\n            metric=None,\n            runbook_url=None,\n            terms=None,\n            user_defined=None,\n            enabled=None):\n        \"\"\"\n        Updates any of the optional parameters of the alert condition\n\n        :type alert_condition_id: int\n        :param alert_condition_id: Alerts condition id to update\n\n        :type policy_id: int\n        :param policy_id: Alert policy id where target alert condition belongs to\n\n        :type type: str\n        :param type: The type of the condition, can be apm_app_metric,\n            apm_kt_metric, servers_metric, browser_metric, mobile_metric\n\n        :type condition_scope: str\n        :param condition_scope: The scope of the condition, can be instance or application\n\n        :type name: str\n        :param name: The name of the server\n\n        :type entities: list[str]\n        :param name: entity ids to which the alert condition is applied\n\n        :type : str\n        :param metric: The target metric\n\n        :type : str\n        :param runbook_url: The url of the runbook\n\n        :type terms: list[hash]\n        :param terms: list of hashes containing threshold config for the alert\n\n        :type user_defined: hash\n        :param user_defined: hash containing threshold user_defined for the alert\n            required if metric is set to user_defined\n\n        :type enabled: bool\n        :param enabled: Whether to enable that alert condition\n\n        :rtype: dict\n        :return: The JSON response of the API\n\n        :raises: This will raise a\n            :class:`NewRelicAPIServerException<newrelic_api.exceptions.NoEntityException>`\n            if target alert condition is not included in target policy\n\n        :raises: This will raise a\n            :class:`ConfigurationException<newrelic_api.exceptions.ConfigurationException>`\n            if metric is set as user_defined but user_defined config is not passed\n\n        ::\n\n            {\n                \"condition\": {\n                    \"id\": \"integer\",\n                    \"type\": \"string\",\n                    \"condition_scope\":  \"string\",\n                    \"name\": \"string\",\n                    \"enabled\": \"boolean\",\n                    \"entities\": [\n                        \"integer\"\n                    ],\n                    \"metric\": \"string\",\n                    \"runbook_url\": \"string\",\n                    \"terms\": [\n                        {\n                            \"duration\": \"string\",\n                            \"operator\": \"string\",\n                            \"priority\": \"string\",\n                            \"threshold\": \"string\",\n                            \"time_function\": \"string\"\n                        }\n                    ],\n                    \"user_defined\": {\n                        \"metric\": \"string\",\n                        \"value_function\": \"string\"\n                    }\n                }\n            }\n\n        \"\"\"\n        conditions_dict = self.list(policy_id)\n        target_condition = None\n        for condition in conditions_dict['conditions']:\n            if int(condition['id']) == alert_condition_id:\n                target_condition = condition\n                break\n\n        if target_condition is None:\n            raise NoEntityException(\n                'Target alert condition is not included in that policy.'\n                'policy_id: {}, alert_condition_id {}'.format(policy_id, alert_condition_id)\n            )\n\n        data = {\n            'condition': {\n                'type': type or target_condition['type'],\n                'name': name or target_condition['name'],\n                'entities': entities or target_condition['entities'],\n                'condition_scope': condition_scope or target_condition['condition_scope'],\n                'terms': terms or target_condition['terms'],\n                'metric': metric or target_condition['metric'],\n                'runbook_url': runbook_url or target_condition['runbook_url'],\n            }\n        }\n\n        if enabled is not None:\n            data['condition']['enabled'] = str(enabled).lower()\n\n        if data['condition']['metric'] == 'user_defined':\n            if user_defined:\n                data['condition']['user_defined'] = user_defined\n            elif 'user_defined' in target_condition:\n                data['condition']['user_defined'] = target_condition['user_defined']\n            else:\n                raise ConfigurationException(\n                    'Metric is set as user_defined but no user_defined config specified'\n                )\n\n        return self._put(\n            url='{0}alerts_conditions/{1}.json'.format(self.URL, alert_condition_id),\n            headers=self.headers,\n            data=data\n        )", "response": "Updates the alert condition with optional parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(\n            self, policy_id,\n            type,\n            condition_scope,\n            name,\n            entities,\n            metric,\n            terms,\n            runbook_url=None,\n            user_defined=None,\n            enabled=True):\n        \"\"\"\n        Creates an alert condition\n\n        :type policy_id: int\n        :param policy_id: Alert policy id where target alert condition belongs to\n\n        :type type: str\n        :param type: The type of the condition, can be apm_app_metric,\n            apm_kt_metric, servers_metric, browser_metric, mobile_metric\n\n        :type condition_scope: str\n        :param condition_scope: The scope of the condition, can be instance or application\n\n        :type name: str\n        :param name: The name of the server\n\n        :type entities: list[str]\n        :param name: entity ids to which the alert condition is applied\n\n        :type : str\n        :param metric: The target metric\n\n        :type : str\n        :param runbook_url: The url of the runbook\n\n        :type terms: list[hash]\n        :param terms: list of hashes containing threshold config for the alert\n\n        :type user_defined: hash\n        :param user_defined: hash containing threshold user_defined for the alert\n            required if metric is set to user_defined\n\n        :type enabled: bool\n        :param enabled: Whether to enable that alert condition\n\n        :rtype: dict\n        :return: The JSON response of the API\n\n        ::\n\n            {\n                \"condition\": {\n                    \"id\": \"integer\",\n                    \"type\": \"string\",\n                    \"condition_scope\":  \"string\",\n                    \"name\": \"string\",\n                    \"enabled\": \"boolean\",\n                    \"entities\": [\n                        \"integer\"\n                    ],\n                    \"metric\": \"string\",\n                    \"runbook_url\": \"string\",\n                    \"terms\": [\n                        {\n                            \"duration\": \"string\",\n                            \"operator\": \"string\",\n                            \"priority\": \"string\",\n                            \"threshold\": \"string\",\n                            \"time_function\": \"string\"\n                        }\n                    ],\n                    \"user_defined\": {\n                        \"metric\": \"string\",\n                        \"value_function\": \"string\"\n                    }\n                }\n            }\n\n        \"\"\"\n\n        data = {\n            'condition': {\n                'type': type,\n                'name': name,\n                'enabled': enabled,\n                'entities': entities,\n                'condition_scope': condition_scope,\n                'terms': terms,\n                'metric': metric,\n                'runbook_url': runbook_url,\n            }\n        }\n\n        if metric == 'user_defined':\n            if user_defined:\n                data['condition']['user_defined'] = user_defined\n            else:\n                raise ConfigurationException(\n                    'Metric is set as user_defined but no user_defined config specified'\n                )\n\n        return self._post(\n            url='{0}alerts_conditions/policies/{1}.json'.format(self.URL, policy_id),\n            headers=self.headers,\n            data=data\n        )", "response": "Creates an alert condition in the specified alert level"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, alert_condition_id):\n\n        return self._delete(\n            url='{0}alerts_conditions/{1}.json'.format(self.URL, alert_condition_id),\n            headers=self.headers\n        )", "response": "This API endpoint allows you to delete an alert condition."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show(self, id):\n        return self._get(\n            url='{root}key_transactions/{id}.json'.format(\n                root=self.URL,\n                id=id\n            ),\n            headers=self.headers,\n        )", "response": "This API endpoint returns a single Key transaction identified its ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show(self, alert_condition_infra_id):\n        return self._get(\n            url='{0}alerts/conditions/{1}'.format(self.URL, alert_condition_infra_id),\n            headers=self.headers,\n        )", "response": "This API endpoint returns the JSON response of the API that contains the information for the alert condition for the given infrastucture."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, alert_condition_infra_id, policy_id,\n               name, condition_type, alert_condition_configuration, enabled=True):\n        \"\"\"\n        This API endpoint allows you to update an alert condition for infrastucture\n\n        :type alert_condition_infra_id: int\n        :param alert_condition_infra_id: Alert Condition Infra ID\n\n        :type policy_id: int\n        :param policy_id: Alert policy id\n\n        :type name: str\n        :param name: The name of the alert condition\n\n        :type condition_type: str\n        :param condition_type: The type of the alert condition can be\n            infra_process_running, infra_metric or infra_host_not_reporting\n\n        :type alert_condition_configuration: hash\n        :param alert_condition_configuration: hash containing config for the alert\n\n        :type enabled: bool\n        :param enabled: Whether to enable that alert condition\n\n        :rtype: dict\n        :return: The JSON response of the API\n\n        ::\n\n            {\n                \"data\": {\n                    \"id\": \"integer\",\n                    \"policy_id\": \"integer\",\n                    \"type\": \"string\",\n                    \"name\": \"string\",\n                    \"enabled\": \"boolean\",\n                    \"where_clause\": \"string\",\n                    \"comparison\": \"string\",\n                    \"filter\": \"hash\",\n                    \"critical_threshold\": \"hash\",\n                    \"event_type\": \"string\",\n                    \"process_where_clause\": \"string\",\n                    \"created_at_epoch_millis\": \"time\",\n                    \"updated_at_epoch_millis\": \"time\"\n                }\n            }\n\n        \"\"\"\n\n        data = {\n            \"data\": alert_condition_configuration\n        }\n\n        data['data']['type'] = condition_type\n        data['data']['policy_id'] = policy_id\n        data['data']['name'] = name\n        data['data']['enabled'] = enabled\n\n        return self._put(\n            url='{0}alerts/conditions/{1}'.format(self.URL, alert_condition_infra_id),\n            headers=self.headers,\n            data=data\n        )", "response": "This API endpoint allows you to update an alert condition for infrastucture."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, name, category, applications=None, servers=None):\n\n        data = {\n            \"label\": {\n                \"category\": category,\n                \"name\": name,\n                \"links\": {\n                    \"applications\": applications or [],\n                    \"servers\": servers or []\n                }\n            }\n        }\n\n        return self._put(\n            url='{0}labels.json'.format(self.URL),\n            headers=self.headers,\n            data=data\n        )", "response": "This API endpoint will create a new label with the provided name and category."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, key):\n        return self._delete(\n            url='{url}labels/labels/{key}.json'.format(\n                url=self.URL,\n                key=key),\n            headers=self.headers,\n        )", "response": "This endpoint will delete the label with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list(self, filter_guid=None, filter_ids=None, detailed=None, page=None):\n        filters = [\n            'filter[guid]={0}'.format(filter_guid) if filter_guid else None,\n            'filter[ids]={0}'.format(','.join([str(app_id) for app_id in filter_ids])) if filter_ids else None,\n            'detailed={0}'.format(detailed) if detailed is not None else None,\n            'page={0}'.format(page) if page else None\n        ]\n\n        return self._get(\n            url='{0}plugins.json'.format(self.URL),\n            headers=self.headers,\n            params=self.build_param_string(filters)\n        )", "response": "This API endpoint returns a paginated list of the plugins associated with the keytonic account."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list(\n            self, application_id, filter_hostname=None, filter_ids=None,\n            page=None):\n        \"\"\"\n        This API endpoint returns a paginated list of instances associated with the\n        given application.\n\n        Application instances can be filtered by hostname, or the list of\n        application instance IDs.\n\n        :type application_id: int\n        :param application_id: Application ID\n\n        :type filter_hostname: str\n        :param filter_hostname: Filter by server hostname\n\n        :type filter_ids: list of ints\n        :param filter_ids: Filter by application instance ids\n\n        :type page: int\n        :param page: Pagination index\n\n        :rtype: dict\n        :return: The JSON response of the API, with an additional 'pages' key\n            if there are paginated results\n\n        ::\n\n            {\n                \"application_instances\": [\n                    {\n                        \"id\": \"integer\",\n                        \"application_name\": \"string\",\n                        \"host\": \"string\",\n                        \"port\": \"integer\",\n                        \"language\": \"integer\",\n                        \"health_status\": \"string\",\n                        \"application_summary\": {\n                            \"response_time\": \"float\",\n                            \"throughput\": \"float\",\n                            \"error_rate\": \"float\",\n                            \"apdex_score\": \"float\"\n                        },\n                        \"end_user_summary\": {\n                            \"response_time\": \"float\",\n                            \"throughput\": \"float\",\n                            \"apdex_score\": \"float\"\n                        },\n                        \"links\": {\n                            \"application\": \"integer\",\n                            \"application_host\": \"integer\",\n                            \"server\": \"integer\"\n                        }\n                    }\n                ],\n                \"pages\": {\n                    \"last\": {\n                        \"url\": \"https://api.newrelic.com/v2/applications/{application_id}/instances.json?page=2\",\n                        \"rel\": \"last\"\n                    },\n                    \"next\": {\n                        \"url\": \"https://api.newrelic.com/v2/applications/{application_id}/instances.json?page=2\",\n                        \"rel\": \"next\"\n                    }\n                }\n            }\n\n        \"\"\"\n        filters = [\n            'filter[hostname]={0}'.format(filter_hostname) if filter_hostname else None,\n            'filter[ids]={0}'.format(','.join([str(app_id) for app_id in filter_ids])) if filter_ids else None,\n            'page={0}'.format(page) if page else None\n        ]\n        return self._get(\n            url='{root}applications/{application_id}/instances.json'.format(\n                root=self.URL,\n                application_id=application_id\n            ),\n            headers=self.headers,\n            params=self.build_param_string(filters)\n        )", "response": "This API endpoint returns a paginated list of instances associated with the given application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef metric_names(self, application_id, host_id, name=None, page=None):\n        params = [\n            'name={0}'.format(name) if name else None,\n            'page={0}'.format(page) if page else None\n        ]\n        return self._get(\n            url='{root}applications/{application_id}/hosts/{host_id}/metrics.json'.format(\n                root=self.URL,\n                application_id=application_id,\n                host_id=host_id\n            ),\n            headers=self.headers,\n            params=self.build_param_string(params)\n        )", "response": "Return a list of known metrics and their value names for the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef metric_data(\n            self, id, names, values=None, from_dt=None, to_dt=None,\n            summarize=False):\n        \"\"\"\n        This API endpoint returns a list of values for each of the requested\n        metrics. The list of available metrics can be returned using the Metric\n        Name API endpoint.\n\n        Metric data can be filtered by a number of parameters, including\n        multiple names and values, and by time range. Metric names and values\n        will be matched intelligently in the background.\n\n        You can also retrieve a summarized data point across the entire time\n        range selected by using the summarize parameter.\n\n        **Note** All times sent and received are formatted in UTC. The default\n        time range is the last 30 minutes.\n\n        :type id: int\n        :param id: Component ID\n\n        :type names: list of str\n        :param names: Retrieve specific metrics by name\n\n        :type values: list of str\n        :param values: Retrieve specific metric values\n\n        :type from_dt: datetime\n        :param from_dt: Retrieve metrics after this time\n\n        :type to_dt: datetime\n        :param to_dt: Retrieve metrics before this time\n\n        :type summarize: bool\n        :param summarize: Summarize the data\n\n        :rtype: dict\n        :return: The JSON response of the API\n\n        ::\n\n            {\n                \"metric_data\": {\n                    \"from\": \"time\",\n                    \"to\": \"time\",\n                    \"metrics\": [\n                        {\n                            \"name\": \"string\",\n                            \"timeslices\": [\n                                {\n                                    \"from\": \"time\",\n                                    \"to\": \"time\",\n                                    \"values\": \"hash\"\n                                }\n                            ]\n                        }\n                    ]\n                }\n            }\n\n        \"\"\"\n        params = [\n            'from={0}'.format(from_dt) if from_dt else None,\n            'to={0}'.format(to_dt) if to_dt else None,\n            'summarize=true' if summarize else None\n        ]\n\n        params += ['names[]={0}'.format(name) for name in names]\n        if values:\n            params += ['values[]={0}'.format(value) for value in values]\n\n        return self._get(\n            url='{0}components/{1}/metrics/data.json'.format(self.URL, id),\n            headers=self.headers,\n            params=self.build_param_string(params)\n        )", "response": "This API endpoint returns a list of values for each of the requested metrics."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _put(self, *args, **kwargs):\n        if 'data' in kwargs:\n            kwargs['data'] = json.dumps(kwargs['data'])\n        response = requests.put(*args, **kwargs)\n        if not response.ok:\n            raise NewRelicAPIServerException('{}: {}'.format(response.status_code, response.text))\n\n        return response.json()", "response": "A wrapper for the put method that will json encode your data and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _post(self, *args, **kwargs):\n        if 'data' in kwargs:\n            kwargs['data'] = json.dumps(kwargs['data'])\n        response = requests.post(*args, **kwargs)\n        if not response.ok:\n            raise NewRelicAPIServerException('{}: {}'.format(response.status_code, response.text))\n\n        return response.json()", "response": "A wrapper for the requests. post method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _delete(self, *args, **kwargs):\n        response = requests.delete(*args, **kwargs)\n        if not response.ok:\n            raise NewRelicAPIServerException('{}: {}'.format(response.status_code, response.text))\n\n        if response.text:\n            return response.json()\n\n        return {}", "response": "A wrapper for deleting things from the New Relic s cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self, dashboard_data):\n        return self._post(\n            url='{0}dashboards.json'.format(self.URL),\n            headers=self.headers,\n            data=dashboard_data,\n        )", "response": "This API endpoint creates a dashboard and all defined widgets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding odd numbers subtract odd ones.", "response": "def addevensubodd(operator, operand):\n    \"\"\"Add even numbers, subtract odd ones. See http://1w6.org/w6 \"\"\"\n    try:\n        for i, x in enumerate(operand):\n            if x % 2:\n                operand[i] = -x\n        return operand\n    except TypeError:\n        if operand % 2:\n            return -operand\n        return operand"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun roll from a command line interface", "response": "def main(argv=None):\n    \"\"\"Run roll() from a command line interface\"\"\"\n    args = docopt.docopt(__doc__, argv=argv, version=__version__)\n    verbose = bool(args['--verbose'])\n\n    f_roll = dice.roll\n    kwargs = {}\n\n    if args['--min']:\n        f_roll = dice.roll_min\n    elif args['--max']:\n        f_roll = dice.roll_max\n\n    if args['--max-dice']:\n        try:\n            kwargs['max_dice'] = int(args['--max-dice'])\n        except ValueError:\n            print(\"Invalid value for --max-dice: '%s'\" % args['--max-dice'])\n            exit(1)\n\n    expr = ' '.join(args['<expression>'])\n\n    try:\n        roll, kwargs = f_roll(expr, raw=True, return_kwargs=True, **kwargs)\n\n        if verbose:\n            print('Result: ', end='')\n\n        print(str(roll.evaluate_cached(**kwargs)))\n\n        if verbose:\n            print('Breakdown:')\n            print(dice.utilities.verbose_print(roll, **kwargs))\n    except DiceBaseException as e:\n        print('Whoops! Something went wrong:')\n        print(e.pretty_print())\n        exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nevaluates elements and coerces objects to a class if needed", "response": "def evaluate_object(obj, cls=None, cache=False, **kwargs):\n        \"\"\"Evaluates elements, and coerces objects to a class if needed\"\"\"\n        old_obj = obj\n        if isinstance(obj, Element):\n            if cache:\n                obj = obj.evaluate_cached(**kwargs)\n            else:\n                obj = obj.evaluate(cache=cache, **kwargs)\n\n        if cls is not None and type(obj) != cls:\n            obj = cls(obj)\n\n        for attr in ('string', 'location', 'tokens'):\n            if hasattr(old_obj, attr):\n                setattr(obj, attr, getattr(old_obj, attr))\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef evaluate_cached(self, **kwargs):\n        if not hasattr(self, 'result'):\n            self.result = self.evaluate(cache=True, **kwargs)\n\n        return self.result", "response": "Wraps evaluate(), caching results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the files given by _edgeList_ and _nodeList_ and creates a networkx graph for the files produced by the metaknowledge.", "response": "def readGraph(edgeList, nodeList = None, directed = False, idKey = 'ID', eSource = 'From', eDest = 'To'):\n    \"\"\"Reads the files given by _edgeList_ and _nodeList_ and creates a networkx graph for the files.\n\n    This is designed only for the files produced by metaknowledge and is meant to be the reverse of [writeGraph()](#metaknowledge.graphHelpers.writeGraph), if this does not produce the desired results the networkx builtin [networkx.read_edgelist()](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.readwrite.edgelist.read_edgelist.html) could be tried as it is aimed at a more general usage.\n\n    The read edge list format assumes the column named _eSource_ (default `'From'`) is the source node, then the column _eDest_ (default `'To'`) givens the destination and all other columns are attributes of the edges, e.g. weight.\n\n    The read node list format assumes the column _idKey_ (default `'ID'`) is the ID of the node for the edge list and the resulting network. All other columns are considered attributes of the node, e.g. count.\n\n    **Note**: If the names of the columns do not match those given to **readGraph()** a `KeyError` exception will be raised.\n\n    **Note**: If nodes appear in the edgelist but not the nodeList they will be created silently with no attributes.\n\n    # Parameters\n\n    _edgeList_ : `str`\n\n    > a string giving the path to the edge list file\n\n    _nodeList_ : `optional [str]`\n\n    > default `None`, a string giving the path to the node list file\n\n    _directed_ : `optional [bool]`\n\n    > default `False`, if `True` the produced network is directed from _eSource_ to _eDest_\n\n    _idKey_ : `optional [str]`\n\n    > default `'ID'`, the name of the ID column in the node list\n\n    _eSource_ : `optional [str]`\n\n    > default `'From'`, the name of the source column in the edge list\n\n    _eDest_ : `optional [str]`\n\n    > default `'To'`, the name of the destination column in the edge list\n\n    # Returns\n\n    `networkx Graph`\n\n    > the graph described by the input files\n    \"\"\"\n    progArgs = (0, \"Starting to reading graphs\")\n    if metaknowledge.VERBOSE_MODE:\n        progKwargs = {'dummy' : False}\n    else:\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        if directed:\n            grph = nx.DiGraph()\n        else:\n            grph = nx.Graph()\n        if nodeList:\n            PBar.updateVal(0, \"Reading \" + nodeList)\n            f = open(os.path.expanduser(os.path.abspath(nodeList)))\n            nFile = csv.DictReader(f)\n            for line in nFile:\n                vals = line\n                ndID = vals[idKey]\n                del vals[idKey]\n                if len(vals) > 0:\n                    grph.add_node(ndID, **vals)\n                else:\n                    grph.add_node(ndID)\n            f.close()\n        PBar.updateVal(.25, \"Reading \" + edgeList)\n        f = open(os.path.expanduser(os.path.abspath(edgeList)))\n        eFile = csv.DictReader(f)\n        for line in eFile:\n            vals = line\n            eFrom = vals[eSource]\n            eTo = vals[eDest]\n            del vals[eSource]\n            del vals[eDest]\n            if len(vals) > 0:\n                grph.add_edge(eFrom, eTo, **vals)\n            else:\n                grph.add_edge(eFrom, eTo)\n        PBar.finish(\"{} nodes and {} edges found\".format(len(grph.nodes()), len(grph.edges())))\n        f.close()\n        return grph"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a networkx graph to a file.", "response": "def writeGraph(grph, name, edgeInfo = True, typing = False, suffix = 'csv', overwrite = True, allSameAttribute = False):\n    \"\"\"Writes both the edge list and the node attribute list of _grph_ to files starting with _name_.\n\n    The output files start with _name_, the file type (edgeList, nodeAttributes) then if typing is True the type of graph (directed or undirected) then the suffix, the default is as follows:\n\n    >> name_fileType.suffix\n\n    Both files are csv's with comma delimiters and double quote quoting characters. The edge list has two columns for the source and destination of the edge, `'From'` and `'To'` respectively, then, if _edgeInfo_ is `True`, for each attribute of the node another column is created. The node list has one column call \"ID\" with the node ids used by networkx and all other columns are the node attributes.\n\n    To read back these files use [readGraph()](#metaknowledge.graphHelpers.readGraph) and to write only one type of lsit use [writeEdgeList()](#metaknowledge.graphHelpers.writeEdgeList) or [writeNodeAttributeFile()](#metaknowledge.graphHelpers.writeNodeAttributeFile).\n\n    **Warning**: this function will overwrite files, if they are in the way of the output, to prevent this set _overwrite_ to `False`\n\n    **Note**: If any nodes or edges are missing an attribute a `KeyError` will be raised.\n\n    # Parameters\n\n    _grph_ : `networkx Graph`\n\n    > A networkx graph of the network to be written.\n\n    _name_ : `str`\n\n    > The start of the file name to be written, can include a path.\n\n    _edgeInfo_ : `optional [bool]`\n\n    > Default `True`, if `True` the the attributes of each edge are written to the edge list.\n\n    _typing_ : `optional [bool]`\n\n    > Default `False`, if `True` the directed ness of the graph will be added to the file names.\n\n    _suffix_ : `optional [str]`\n\n    > Default `\"csv\"`, the suffix of the file.\n\n    _overwrite_ : `optional [bool]`\n\n    > Default `True`, if `True` files will be overwritten silently, otherwise an `OSError` exception will be raised.\n    \"\"\"\n    progArgs = (0, \"Writing the graph to files starting with: {}\".format(name))\n    if metaknowledge.VERBOSE_MODE:\n        progKwargs = {'dummy' : False}\n    else:\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        if typing:\n            if isinstance(grph, nx.classes.digraph.DiGraph) or isinstance(grph, nx.classes.multidigraph.MultiDiGraph):\n                grphType = \"_directed\"\n            else:\n                grphType = \"_undirected\"\n        else:\n            grphType = ''\n        nameCompts = os.path.split(os.path.expanduser(os.path.normpath(name)))\n        if nameCompts[0] == '' and nameCompts[1] == '':\n            edgeListName = \"edgeList\"+ grphType + '.' + suffix\n            nodesAtrName = \"nodeAttributes\"+ grphType + '.' + suffix\n        elif nameCompts[0] == '':\n            edgeListName = nameCompts[1] + \"_edgeList\"+ grphType + '.' + suffix\n            nodesAtrName = nameCompts[1] + \"_nodeAttributes\"+ grphType + '.' + suffix\n        elif nameCompts[1] == '':\n            edgeListName = os.path.join(nameCompts[0], \"edgeList\"+ grphType + '.' + suffix)\n            nodesAtrName = os.path.join(nameCompts[0], \"nodeAttributes\"+ grphType + '.' + suffix)\n        else:\n            edgeListName = os.path.join(nameCompts[0], nameCompts[1] + \"_edgeList\"+ grphType + '.' + suffix)\n            nodesAtrName = os.path.join(nameCompts[0], nameCompts[1] + \"_nodeAttributes\"+ grphType + '.' + suffix)\n        if not overwrite:\n            if os.path.isfile(edgeListName):\n                raise OSError(edgeListName+ \" already exists\")\n            if os.path.isfile(nodesAtrName):\n                raise OSError(nodesAtrName + \" already exists\")\n        writeEdgeList(grph, edgeListName, extraInfo = edgeInfo, allSameAttribute = allSameAttribute, _progBar = PBar)\n        writeNodeAttributeFile(grph, nodesAtrName, allSameAttribute = allSameAttribute, _progBar = PBar)\n        PBar.finish(\"{} nodes and {} edges written to file\".format(len(grph.nodes()), len(grph.edges())))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites an edge list of a graph to a file.", "response": "def writeEdgeList(grph, name, extraInfo = True, allSameAttribute = False, _progBar = None):\n    \"\"\"Writes an edge list of _grph_ at the destination _name_.\n\n    The edge list has two columns for the source and destination of the edge, `'From'` and `'To'` respectively, then, if _edgeInfo_ is `True`, for each attribute of the node another column is created.\n\n    **Note**: If any edges are missing an attribute it will be left blank by default, enable _allSameAttribute_ to cause a `KeyError` to be raised.\n\n    # Parameters\n\n    _grph_ : `networkx Graph`\n\n    > The graph to be written to _name_\n\n    _name_ : `str`\n\n    > The name of the file to be written\n\n    _edgeInfo_ : `optional [bool]`\n\n    > Default `True`, if `True` the attributes of each edge will be written\n\n    _allSameAttribute_ : `optional [bool]`\n\n    > Default `False`, if `True` all the edges must have the same attributes or an exception will be raised. If `False` the missing attributes will be left blank.\n    \"\"\"\n    count = 0\n    eMax = len(grph.edges())\n    if metaknowledge.VERBOSE_MODE or isinstance(_progBar, _ProgressBar):\n        if isinstance(_progBar, _ProgressBar):\n            PBar = _progBar\n            PBar.updateVal(0, \"Writing edge list {}\".format(name))\n        else:\n            PBar = _ProgressBar(0, \"Writing edge list {}\".format(name))\n    else:\n        PBar = _ProgressBar(0, \"Writing edge list {}\".format(name), dummy = True)\n    if len(grph.edges(data = True)) < 1:\n        outFile = open(os.path.expanduser(os.path.abspath(name)), 'w')\n        outFile.write('\"From\",\"To\"\\n')\n        outFile.close()\n        PBar.updateVal(1, \"Done edge list '{}', 0 edges written.\".format(name))\n    else:\n        if extraInfo:\n            csvHeader = []\n            if allSameAttribute:\n                csvHeader = ['From'] +  ['To'] + list(grph.edges(data = True).__next__()[2].keys())\n            else:\n                extraAttribs = set()\n                for eTuple in grph.edges(data = True):\n                    count += 1\n                    if count % 1000 == 0:\n                        PBar.updateVal(count / eMax * .10, \"Checking over edge: '{}' to '{}'\".format(eTuple[0], eTuple[1]))\n                    s = set(eTuple[2].keys()) - extraAttribs\n                    if len(s) > 0:\n                        for i in s:\n                            extraAttribs.add(i)\n                csvHeader = ['From', 'To'] + list(extraAttribs)\n        else:\n            csvHeader = ['From'] +  ['To']\n        count = 0\n        PBar.updateVal(.01, \"Opening file {}\".format(name))\n        f = open(os.path.expanduser(os.path.abspath(name)), 'w', newline = '')\n        outFile = csv.DictWriter(f, csvHeader, delimiter = ',', quotechar = '\"', quoting=csv.QUOTE_NONNUMERIC)\n        outFile.writeheader()\n        if extraInfo:\n            for e in grph.edges(data = True):\n                count += 1\n                if count % 1000 == 0:\n                    PBar.updateVal(count / eMax * .90 + .10, \"Writing edge: '{}' to '{}'\".format(e[0], e[1]))\n                eDict = e[2].copy()\n                eDict['From'] = e[0]\n                eDict['To'] = e[1]\n                try:\n                    outFile.writerow(eDict)\n                except UnicodeEncodeError:\n                    #Because Windows\n                    newDict = {k.encode('ASCII', errors='ignore').decode('ASCII', errors='ignore') if isinstance(k, str) else k: v.encode('ASCII', errors='ignore').decode('ASCII', errors='ignore') if isinstance(v, str) else v for k, v in eDict.items()}\n                    outFile.writerow(newDict)\n                except ValueError:\n                    raise ValueError(\"Some edges in The graph do not have the same attributes\")\n        else:\n            for e in grph.edges():\n                count += 1\n                if count % 1000 == 0:\n                    PBar.updateVal(count / eMax * .90 + .10, \"Writing edge: '{}' to '{}'\".format(e[0], e[1]))\n                eDict['From'] = e[0]\n                eDict['To'] = e[1]\n                try:\n                    outFile.writerow(eDict)\n                except UnicodeEncodeError:\n                    #Because Windows\n                    newDict = {k.encode('ASCII', errors='ignore').decode('ASCII', errors='ignore') if isinstance(k, str) else k: v.encode('ASCII', errors='ignore').decode('ASCII', errors='ignore') if isinstance(v, str) else v for k, v in eDict.items()}\n                    outFile.writerow(newDict)\n        PBar.updateVal(1, \"Closing {}\".format(name))\n        f.close()\n        if not isinstance(_progBar, _ProgressBar):\n            PBar.finish(\"Done edge list {}, {} edges written.\".format(name, count))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a node attribute list to a file given by the path name.", "response": "def writeNodeAttributeFile(grph, name, allSameAttribute = False, _progBar = None):\n    \"\"\"Writes a node attribute list of _grph_ to the file given by the path _name_.\n\n    The node list has one column call `'ID'` with the node ids used by networkx and all other columns are the node attributes.\n\n    **Note**: If any nodes are missing an attribute it will be left blank by default, enable _allSameAttribute_ to cause a `KeyError` to be raised.\n\n    # Parameters\n\n    _grph_ : `networkx Graph`\n\n    > The graph to be written to _name_\n\n    _name_ : `str`\n\n    > The name of the file to be written\n\n    _allSameAttribute_ : `optional [bool]`\n\n    > Default `False`, if `True` all the nodes must have the same attributes or an exception will be raised. If `False` the missing attributes will be left blank.\n    \"\"\"\n    count = 0\n    nMax = len(grph.nodes())\n    if metaknowledge.VERBOSE_MODE or isinstance(_progBar, _ProgressBar):\n        if isinstance(_progBar, _ProgressBar):\n            PBar = _progBar\n            PBar.updateVal(0, \"Writing node list {}\".format(name))\n        else:\n            PBar = _ProgressBar(0, \"Writing node list {}\".format(name))\n    else:\n        PBar = _ProgressBar(0, \"Writing node list {}\".format(name), dummy = True)\n    if len(grph.nodes(data = True)) < 1:\n        outFile = open(os.path.expanduser(os.path.abspath(name)), 'w')\n        outFile.write('ID\\n')\n        outFile.close()\n        PBar.updateVal(1, \"Done node attribute list: {}, 0 nodes written.\".format(name))\n    else:\n        csvHeader = []\n        if allSameAttribute:\n            csvHeader = ['ID'] + list(grph.nodes(data = True).__next__()[1].keys())\n        else:\n            extraAttribs = set()\n            for n, attribs in grph.nodes(data = True):\n                count += 1\n                if count % 100 == 0:\n                    PBar.updateVal(count / nMax * .10, \"Checking over node: '{}'\".format(n))\n                s = set(attribs.keys()) - extraAttribs\n                if len(s) > 0:\n                    for i in s:\n                        extraAttribs.add(i)\n            csvHeader = ['ID'] + list(extraAttribs)\n        count = 0\n        PBar.updateVal(.10, \"Opening '{}'\".format(name))\n        f = open(name, 'w', newline = '')\n        outFile = csv.DictWriter(f, csvHeader, delimiter = ',', quotechar = '\"', quoting = csv.QUOTE_NONNUMERIC)\n        outFile.writeheader()\n        for n in grph.nodes(data = True):\n            count += 1\n            if count % 100 == 0:\n                PBar.updateVal(count / nMax * .90 + .10, \"Writing node: '{}'\".format(n[0]))\n            nDict = n[1].copy()\n            nDict['ID'] = n[0]\n            try:\n                outFile.writerow(nDict)\n            except UnicodeEncodeError:\n                #Because Windows\n                newDict = {k.encode('ASCII', errors='ignore').decode('ASCII', errors='ignore') if isinstance(k, str) else k: v.encode('ASCII', errors='ignore').decode('ASCII', errors='ignore') if isinstance(v, str) else v for k, v in nDict.items()}\n                outFile.writerow(newDict)\n            except ValueError:\n                raise ValueError(\"Some nodes in the graph do not have the same attributes\")\n        PBar.updateVal(1, \"Closing {}\".format(name))\n        f.close()\n        if not isinstance(_progBar, _ProgressBar):\n            PBar.finish(\"Done node attribute list: {}, {} nodes written.\".format(name, count))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite an edge list designed for reading by the _R_ package [tnet](https://toreopsahl.com/tnet/). The _networkx_ graph provided must be a pure two-mode network, the modes must be 2 different values for the node attribute accessed by _modeNameString_ and all edges must be between different node types. Each node will be given an integer id, stored in the attribute given by _nodeIndexString_, these ids are then written to the file as the endpoints of the edges. Unless _sourceMode_ is given which mode is the source (first column) and which the target (second column) is random. **Note** the _grph_ will be modified by this function, the ids of the nodes will be written to the graph at the attribute _nodeIndexString_. # Parameters _grph_ : `network Graph` > The graph that will be written to _name_ _name_ : `str` > The path of the file to write _modeNameString_ : `str` > The name of the attribute _grph_'s modes are stored in _weighted_ : `optional bool` > Default `False`, if `True` then the attribute _weightString_ will be written to the weight column _sourceMode_ : `optional str` > Default `None`, if given the name of the mode used for the source (first column) in the output file _timeString_ : `optional str` > Default `None`, if present the attribute _timeString_ of an edge will be written to the time column surrounded by double quotes (\"). **Note** The format used by tnet for dates is very strict it uses the ISO format, down to the second and without time zones. _nodeIndexString_ : `optional str` > Default `'tnet-ID'`, the name of the attribute to save the id for each node _weightString_ : `optional str` > Default `'weight'`, the name of the weight attribute", "response": "def writeTnetFile(grph, name, modeNameString, weighted = False, sourceMode = None, timeString = None, nodeIndexString = 'tnet-ID', weightString = 'weight'):\n    \"\"\"Writes an edge list designed for reading by the _R_ package [tnet](https://toreopsahl.com/tnet/).\n\n    The _networkx_ graph provided must be a pure two-mode network, the modes must be 2 different values for the node attribute accessed by _modeNameString_ and all edges must be between different node types. Each node will be given an integer id, stored in the attribute given by _nodeIndexString_, these ids are then written to the file as the endpoints of the edges. Unless _sourceMode_ is given which mode is the source (first column) and which the target (second column) is random.\n\n    **Note** the _grph_ will be modified by this function, the ids of the nodes will be written to the graph at the attribute _nodeIndexString_.\n\n    # Parameters\n\n    _grph_ : `network Graph`\n\n    > The graph that will be written to _name_\n\n    _name_ : `str`\n\n    > The path of the file to write\n\n    _modeNameString_ : `str`\n\n    > The name of the attribute _grph_'s modes are stored in\n\n    _weighted_ : `optional bool`\n\n    > Default `False`, if `True` then the attribute _weightString_ will be written to the weight column\n\n    _sourceMode_ : `optional str`\n\n    > Default `None`, if given the name of the mode used for the source (first column) in the output file\n\n    _timeString_ : `optional str`\n\n    > Default `None`, if present the attribute _timeString_ of an edge will be written to the time column surrounded by double quotes (\").\n\n    **Note** The format used by tnet for dates is very strict it uses the ISO format, down to the second and without time zones.\n\n    _nodeIndexString_ : `optional str`\n\n    > Default `'tnet-ID'`, the name of the attribute to save the id for each node\n\n    _weightString_ : `optional str`\n\n    > Default `'weight'`, the name of the weight attribute\n    \"\"\"\n    count = 0\n    eMax = len(grph.edges())\n    progArgs = (0, \"Writing tnet edge list {}\".format(name))\n    if metaknowledge.VERBOSE_MODE:\n        progKwargs = {'dummy' : False}\n    else:\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        if sourceMode is not None:\n            modes = [sourceMode]\n        else:\n            modes = []\n        mode1Set = set()\n        PBar.updateVal(.1, \"Indexing nodes for tnet\")\n        for nodeIndex, node in enumerate(grph.nodes(data = True), start = 1):\n            try:\n                nMode = node[1][modeNameString]\n            except KeyError:\n                #too many modes so will fail\n                modes = [1,2,3]\n                nMode = 4\n            if nMode not in modes:\n                if len(modes) < 2:\n                    modes.append(nMode)\n                else:\n                    raise RCValueError(\"Too many modes of '{}' found in the network or one of the nodes was missing its mode. There must be exactly 2 modes.\".format(modeNameString))\n            if nMode == modes[0]:\n                mode1Set.add(node[0])\n            node[1][nodeIndexString] = nodeIndex\n        if len(modes) != 2:\n            raise RCValueError(\"Too few modes of '{}' found in the network. There must be exactly 2 modes.\".format(modeNameString))\n        with open(name, 'w', encoding = 'utf-8') as f:\n            edgesCaller = {'data' : True}\n            if timeString is not None:\n                edgesCaller['keys'] = True\n            for *nodes, eDict in grph.edges(**edgesCaller):\n                if timeString is not None:\n                    n1, n2, keyVal = nodes\n                else:\n                    n1, n2 = nodes\n                count += 1\n                if count % 1000 == 1:\n                    PBar.updateVal(count/ eMax * .9 + .1, \"writing edge: '{}'-'{}'\".format(n1, n2))\n                if n1 in mode1Set:\n                    if n2 in mode1Set:\n                        raise RCValueError(\"The nodes '{}' and '{}' have an edge and the same type. The network must be purely 2-mode.\".format(n1, n2))\n                elif n2 in mode1Set:\n                    n1, n2 = n2, n1\n                else:\n                    raise RCValueError(\"The nodes '{}' and '{}' have an edge and the same type. The network must be purely 2-mode.\".format(n1, n2))\n                if timeString is not None:\n                    eTimeString = '\"{}\" '.format(keyVal)\n                else:\n                    eTimeString = ''\n                if weighted:\n                    f.write(\"{}{} {} {}\\n\".format(eTimeString, grph.node[n1][nodeIndexString], grph.node[n2][nodeIndexString], eDict[weightString]))\n                else:\n                    f.write(\"{}{} {}\\n\".format(eTimeString, grph.node[n1][nodeIndexString], grph.node[n2][nodeIndexString]))\n        PBar.finish(\"Done writing tnet file '{}'\".format(name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getWeight(grph, nd1, nd2, weightString = \"weight\", returnType = int):\n    if not weightString:\n        return returnType(1)\n    else:\n        return returnType(grph.edges[nd1, nd2][weightString])", "response": "Returns the weight of a given edge with or without weight"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getNodeDegrees(grph, weightString = \"weight\", strictMode = False,  returnType = int, edgeType = 'bi'):\n    ndsDict = {}\n    for nd in grph.nodes():\n        ndsDict[nd] = returnType(0)\n    for e in grph.edges(data = True):\n        if weightString:\n            try:\n                edgVal = returnType(e[2][weightString])\n            except KeyError:\n                if strictMode:\n                    raise KeyError(\"The edge from \" + str(e[0]) + \" to \" + str(e[1]) + \" does not have the attribute: '\" + str(weightString) + \"'\")\n                else:\n                    edgVal = returnType(1)\n        else:\n            edgVal = returnType(1)\n        if edgeType == 'bi':\n            ndsDict[e[0]] += edgVal\n            ndsDict[e[1]] += edgVal\n        elif edgeType == 'in':\n            ndsDict[e[1]] += edgVal\n        elif edgeType == 'out':\n            ndsDict[e[0]] += edgVal\n        else:\n            raise ValueError(\"edgeType must be 'bi', 'in', or 'out'\")\n    return ndsDict", "response": "Returns a dictionary of nodes to their degrees."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a copy of the graph G with edges dropped between minWeight and maxWeight.", "response": "def dropEdges(grph, minWeight = - float('inf'), maxWeight = float('inf'), parameterName = 'weight', ignoreUnweighted = False, dropSelfLoops = False):\n    \"\"\"Modifies _grph_ by dropping edges whose weight is not within the inclusive bounds of _minWeight_ and _maxWeight_, i.e after running _grph_ will only have edges whose weights meet the following inequality: _minWeight_ <= edge's weight <= _maxWeight_. A `Keyerror` will be raised if the graph is unweighted unless _ignoreUnweighted_ is `True`, the weight is determined by examining the attribute _parameterName_.\n\n    **Note**: none of the default options will result in _grph_ being modified so only specify the relevant ones, e.g. `dropEdges(G, dropSelfLoops = True)` will remove only the self loops from `G`.\n\n    # Parameters\n\n    _grph_ : `networkx Graph`\n\n    > The graph to be modified.\n\n    _minWeight_ : `optional [int or double]`\n\n    > default `-inf`, the minimum weight for an edge to be kept in the graph.\n\n    _maxWeight_ : `optional [int or double]`\n\n    > default `inf`, the maximum weight for an edge to be kept in the graph.\n\n    _parameterName_ : `optional [str]`\n\n    > default `'weight'`, key to weight field in the edge's attribute dictionary, the default is the same as networkx and metaknowledge so is likely to be correct\n\n    _ignoreUnweighted_ : `optional [bool]`\n\n    > default `False`, if `True` unweighted edges will kept\n\n    _dropSelfLoops_ : `optional [bool]`\n\n    > default `False`, if `True` self loops will be removed regardless of their weight\n    \"\"\"\n    count = 0\n    total = len(grph.edges())\n    if metaknowledge.VERBOSE_MODE:\n        progArgs = (0, \"Dropping edges\")\n        progKwargs = {}\n    else:\n        progArgs = (0, \"Dropping edges\")\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        if dropSelfLoops:\n            slps = list(grph.selfloop_edges())\n\n            PBar.updateVal(0, \"Dropping self {} loops\".format(len(slps)))\n            for e in slps:\n                grph.remove_edge(e[0], e[1])\n        edgesToDrop = []\n        if minWeight != - float('inf') or maxWeight != float('inf'):\n            for e in grph.edges(data = True):\n                try:\n                    val = e[2][parameterName]\n                except KeyError:\n                    if not ignoreUnweighted:\n                        raise KeyError(\"One or more Edges do not have weight or \" + str(parameterName), \" is not the name of the weight\")\n                    else:\n                        pass\n                else:\n\n                    count += 1\n                    if count % 100000 == 0:\n                        PBar.updateVal(count/ total, str(count) + \" edges analysed and \" + str(total -len(grph.edges())) + \" edges dropped\")\n                    if val > maxWeight or  val < minWeight:\n                        edgesToDrop.append((e[0], e[1]))\n        grph.remove_edges_from(edgesToDrop)\n        PBar.finish(str(total - len(grph.edges())) + \" edges out of \" + str(total) + \" dropped, \" + str(len(grph.edges())) + \" returned\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dropNodesByDegree(grph, minDegree = -float('inf'), maxDegree = float('inf'), useWeight = True, parameterName = 'weight', includeUnweighted = True):\n    count = 0\n    total = len(grph.nodes())\n    if metaknowledge.VERBOSE_MODE:\n        progArgs = (0, \"Dropping nodes by degree\")\n        progKwargs = {}\n    else:\n        progArgs = (0, \"Dropping nodes by degree\")\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        badNodes = []\n        for n in grph.nodes():\n            if PBar:\n                count += 1\n                if count % 10000 == 0:\n                    PBar.updateVal(count/ total, str(count) + \" nodes analysed and \" + str(len(badNodes)) + \" nodes dropped\")\n            val = 0\n            if useWeight:\n                for e in grph.edges(n, data = True):\n                    try:\n                        val += e[2][parameterName]\n                    except KeyError:\n                        if not includeUnweighted:\n                            raise KeyError(\"One or more Edges do not have weight or \" + str(parameterName), \" is not the name of the weight\")\n                        else:\n                            val += 1\n            else:\n                val = len(grph.edges(n))\n            if val < minDegree or val > maxDegree:\n                badNodes.append(n)\n        if PBar:\n            PBar.updateVal(1, \"Cleaning up graph\")\n        grph.remove_nodes_from(badNodes)\n        if PBar:\n            PBar.finish(\"{} nodes out of {} dropped, {} returned\".format(len(badNodes), total, total - len(badNodes)))", "response": "Returns a copy of the graph with nodes with a degree less than minDegree and less than maxDegree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dropNodesByCount(grph, minCount = -float('inf'), maxCount = float('inf'), parameterName = 'count', ignoreMissing = False):\n    count = 0\n    total = len(grph.nodes())\n    if metaknowledge.VERBOSE_MODE:\n        progArgs = (0, \"Dropping nodes by count\")\n        progKwargs = {}\n    else:\n        progArgs = (0, \"Dropping nodes by count\")\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        badNodes = []\n        for n in grph.nodes(data = True):\n            if PBar:\n                count += 1\n                if count % 10000 == 0:\n                    PBar.updateVal(count/ total, str(count) + \"nodes analysed and {} nodes dropped\".format(len(badNodes)))\n            try:\n                val = n[1][parameterName]\n            except KeyError:\n                if not ignoreMissing:\n                    raise KeyError(\"One or more nodes do not have counts or \" + str(parameterName), \" is not the name of the count parameter\")\n                else:\n                    pass\n            else:\n                if val < minCount or val > maxCount:\n                    badNodes.append(n[0])\n        if PBar:\n            PBar.updateVal(1, \"Cleaning up graph\")\n        grph.remove_nodes_from(badNodes)\n        if PBar:\n            PBar.finish(\"{} nodes out of {} dropped, {} returned\".format(len(badNodes), total, total - len(badNodes)))", "response": "Returns a copy of the _grph_ with the nodes that do not have a count that is within the specified range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mergeGraphs(targetGraph, addedGraph, incrementedNodeVal = 'count', incrementedEdgeVal = 'weight'):\n\n    for addedNode, attribs in addedGraph.nodes(data = True):\n        if incrementedNodeVal:\n            try:\n                targetGraph.node[addedNode][incrementedNodeVal] += attribs[incrementedNodeVal]\n            except KeyError:\n                targetGraph.add_node(addedNode, **attribs)\n        else:\n            if not targetGraph.has_node(addedNode):\n                targetGraph.add_node(addedNode, **attribs)\n    for edgeNode1, edgeNode2, attribs in addedGraph.edges(data = True):\n        if incrementedEdgeVal:\n            try:\n                targetGraph.edges[edgeNode1, edgeNode2][incrementedEdgeVal] += attribs[incrementedEdgeVal]\n            except KeyError:\n                targetGraph.add_edge(edgeNode1, edgeNode2, **attribs)\n        else:\n            if not targetGraph.Graph.has_edge(edgeNode1, edgeNode2):\n                targetGraph.add_edge(edgeNode1, edgeNode2, **attribs)", "response": "A quick way of merging graphs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef graphStats(G, stats = ('nodes', 'edges', 'isolates', 'loops', 'density', 'transitivity'), makeString = True, sentenceString = False):\n\n    for sts in stats:\n        if sts not in ['nodes', 'edges', 'isolates', 'loops', 'density', 'transitivity']:\n            raise RuntimeError('\"{}\" is not a valid stat.'.format(sts))\n    if makeString:\n        stsData = []\n    else:\n        stsData = {}\n    if 'nodes' in stats:\n        if makeString:\n            if sentenceString:\n                stsData.append(\"{:G} nodes\".format(len(G.nodes())))\n            else:\n                stsData.append(\"Nodes: {:G}\".format(len(G.nodes())))\n        else:\n            stsData['nodes'] = len(G.nodes())\n    if 'edges' in stats:\n        if makeString:\n            if sentenceString:\n                stsData.append(\"{:G} edges\".format(len(G.edges())))\n            else:\n                stsData.append(\"Edges: {:G}\".format(len(G.edges())))\n        else:\n            stsData['edges'] = len(G.edges())\n    if 'isolates' in stats:\n        if makeString:\n            if sentenceString:\n                stsData.append(\"{:G} isolates\".format(len(list(nx.isolates(G)))))\n            else:\n                stsData.append(\"Isolates: {:G}\".format(len(list(nx.isolates(G)))))\n        else:\n            stsData['isolates'] = len(list(nx.isolates(G)))\n    if 'loops' in stats:\n        if makeString:\n            if sentenceString:\n                stsData.append(\"{:G} self loops\".format(len(list(G.selfloop_edges()))))\n            else:\n                stsData.append(\"Self loops: {:G}\".format(len(list(G.selfloop_edges()))))\n        else:\n            stsData['loops'] = len(list(G.selfloop_edges()))\n    if 'density' in stats:\n        if makeString:\n            if sentenceString:\n                stsData.append(\"a density of {:G}\".format(nx.density(G)))\n            else:\n                stsData.append(\"Density: {:G}\".format(nx.density(G)))\n        else:\n            stsData['density'] = nx.density(G)\n    if 'transitivity' in stats:\n        if makeString:\n            if sentenceString:\n                stsData.append(\"a transitivity of {:G}\".format(nx.transitivity(G)))\n            else:\n                stsData.append(\"Transitivity: {:G}\".format(nx.transitivity(G)))\n        else:\n            stsData['transitivity'] = nx.transitivity(G)\n    if makeString:\n        if sentenceString:\n            retString = \"The graph has \"\n            if len(stsData) < 1:\n                return retString\n            elif len(stsData) == 1:\n                return retString + stsData[0]\n            else:\n                return retString + ', '.join(stsData[:-1]) + ' and ' + stsData[-1]\n        else:\n            return '\\n'.join(stsData)\n    else:\n        retLst = []\n        for sts in stats:\n            retLst.append(stsData[sts])\n        return tuple(retLst)", "response": "Returns a string or list containing statistics about the graph G."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef AD(val):\n    retDict = {}\n    for v in val:\n        split = v.split(' : ')\n        retDict[split[0]] = [s for s in' : '.join(split[1:]).replace('\\n', '').split(';') if s != '']\n    return retDict", "response": "Affiliation of the AD list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AUID(val):\n    retDict = {}\n    for v in val:\n        split = v.split(' : ')\n        retDict[split[0]] = ' : '.join(split[1:])\n    return retDict", "response": "AuthorIdentifier\n    one line only just need to undo the parser s effects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isInteractive():\n    if sys.stdout.isatty() and os.name != 'nt':\n        #Hopefully everything but ms supports '\\r'\n        try:\n            import threading\n        except ImportError:\n            return False\n        else:\n            return True\n    else:\n        return False", "response": "A basic check of if the program is running in interactive mode."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recordParser(paper):\n    tagList = []\n    doneReading = False\n    l = (0, '')\n    for l in paper:\n        if len(l[1]) < 3:\n            #Line too short\n            raise BadWOSRecord(\"Missing field on line {} : {}\".format(l[0], l[1]))\n        elif 'ER' in l[1][:2]:\n            #Reached the end of the record\n            doneReading = True\n            break\n        elif l[1][2] != ' ':\n            #Field tag longer than 2 or offset in some way\n            raise BadWOSFile(\"Field tag not formed correctly on line \" + str(l[0]) + \" : \" + l[1])\n        elif '   ' in l[1][:3]: #the string is three spaces in row\n            #No new tag append line to current tag (last tag in tagList)\n            tagList[-1][1].append(l[1][3:-1])\n        else:\n            #New tag create new entry at the end of tagList\n            tagList.append((l[1][:2], [l[1][3:-1]]))\n    if not doneReading:\n        raise BadWOSRecord(\"End of file reached before ER: {}\".format(l[1]))\n    else:\n        retdict = collections.OrderedDict(tagList)\n        if len(retdict) == len(tagList):\n            return retdict\n        else:\n            dupSet = set()\n            for tupl in tagList:\n                if tupl[0] in retdict:\n                    dupSet.add(tupl[0])\n            raise BadWOSRecord(\"Duplicate tags (\" + ', '.join(dupSet) + \") in record\")", "response": "This function is used to parse the WOS record file. It returns a generator that yields a list of dictionaries where each dictionary is a list of strings each string is a line of the record."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef writeRecord(self, infile):\n        if self.bad:\n            raise BadWOSRecord(\"This record cannot be converted to a file as the input was malformed.\\nThe original line number (if any) is: {} and the original file is: '{}'\".format(self._sourceLine, self._sourceFile))\n        else:\n            for tag in self._fieldDict.keys():\n                for i, value in enumerate(self._fieldDict[tag]):\n                    if i == 0:\n                        infile.write(tag + ' ')\n                    else:\n                        infile.write('   ')\n                    infile.write(value + '\\n')\n            infile.write(\"ER\\n\")", "response": "Writes to file the original contents of the Record."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getInstitutions(self, tags = None, seperator = \";\", _getTag = False):\n        if tags is None:\n            tags = []\n        elif isinstance(tags, str):\n            tags = [tags]\n        for k in self.keys():\n            if 'institution' in k.lower() and k not in tags:\n                tags.append(k)\n        return super().getInvestigators(tags = tags, seperator = seperator, _getTag = _getTag)", "response": "Returns a list with the names of the institution. The optional arguments are ignored."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef medlineRecordParser(record):\n    tagDict = collections.OrderedDict()\n    tag = 'PMID'\n    mostRecentAuthor = None\n    for lineNum, line in record:\n        tmptag = line[:4].rstrip()\n        contents = line[6:-1]\n        if tmptag.isalpha() and line[4] == '-':\n            tag = tmptag\n            if tag == 'AU':\n                mostRecentAuthor = contents\n            if tag in authorBasedTags:\n                contents = \"{} : {}\".format(mostRecentAuthor, contents)\n            try:\n                tagDict[tag].append(contents)\n            except KeyError:\n                tagDict[tag] = [contents]\n        elif line[:6] == '      ':\n            tagDict[tag][-1] += '\\n' + line[6:-1]\n        elif line == '\\n':\n            break\n        else:\n            raise BadPubmedRecord(\"Tag not formed correctly on line {}: '{}'\".format(lineNum, line))\n    return tagDict", "response": "The parser for the medline record parser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a single record to a file.", "response": "def writeRecord(self, f):\n        \"\"\"This is nearly identical to the original the FAU tag is the only tag not writen in the same place, doing so would require changing the parser and lots of extra logic.\n        \"\"\"\n        if self.bad:\n            raise BadPubmedRecord(\"This record cannot be converted to a file as the input was malformed.\\nThe original line number (if any) is: {} and the original file is: '{}'\".format(self._sourceLine, self._sourceFile))\n        else:\n            authTags = {}\n            for tag in authorBasedTags:\n                for val in self._fieldDict.get(tag, []):\n                    split = val.split(' : ')\n                    try:\n                        authTags[split[0]].append(\"{0}{1}- {2}\\n\".format(tag, ' ' * (4 - len(tag)),' : '.join(split[1:]).replace('\\n', '\\n      ')))\n                    except KeyError:\n                        authTags[split[0]] = [\"{0}{1}- {2}\\n\".format(tag, ' ' * (4 - len(tag)),' : '.join(split[1:]).replace('\\n', '\\n      '))]\n            for tag, value in self._fieldDict.items():\n                if tag in authorBasedTags:\n                    continue\n                else:\n                    for v in value:\n                        f.write(\"{0}{1}- {2}\\n\".format(tag, ' ' * (4 - len(tag)), v.replace('\\n', '\\n      ')))\n                        if tag == 'AU':\n                            for authVal in authTags.get(v,[]):\n                                f.write(authVal)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a 3D plot of the density of nodes on a 2D plane.", "response": "def graphDensityContourPlot(G, iters = 50, layout = None, layoutScaleFactor = 1, overlay = False, nodeSize = 10, axisSamples = 100, blurringFactor = .1, contours = 15, graphType = 'coloured'):\n    \"\"\"Creates a 3D plot giving the density of nodes on a 2D plane, as a surface in 3D.\n\n    Most of the options are for tweaking the final appearance. _layout_ and _layoutScaleFactor_ allow a pre-layout graph to be provided. If a layout is not provided the [networkx.spring_layout()](https://networkx.github.io/documentation/latest/reference/generated/networkx.drawing.layout.spring_layout.html) is used after _iters_ iterations. Then, once the graph has been laid out a grid of _axisSamples_ cells by _axisSamples_ cells is overlaid and the number of nodes in each cell is determined, a gaussian blur is then applied with a sigma of _blurringFactor_. This then forms a surface in 3 dimensions, which is then plotted.\n\n    If you find the resultant image looks too banded raise the the _contours_ number to ~50.\n\n    # Parameters\n\n    _G_ : `networkx Graph`\n\n    > The graph to be plotted\n\n    _iters_ : `optional [int]`\n\n    > Default `50`, the number of iterations for the spring layout if _layout_ is not provided.\n\n    _layout_ : `optional [networkx layout dictionary]`\n\n    > Default `None`, if provided will be used as a layout of the graph, the maximum distance from the origin along any axis must also given as _layoutScaleFactor_, which is by default `1`.\n\n    _layoutScaleFactor_ : `optional [double]`\n\n    > Default `1`, The maximum distance from the origin allowed along any axis given by _layout_, i.e. the layout must fit in a square centered at the origin with side lengths 2 * _layoutScaleFactor_\n\n    _overlay_ : `optional [bool]`\n\n    > Default `False`, if `True` the 2D graph will be plotted on the X-Y plane at Z = 0.\n\n    _nodeSize_ : `optional [double]`\n\n    > Default `10`, the size of the nodes dawn in the overlay\n\n    _axisSamples_ : `optional [int]`\n\n    > Default 100, the number of cells used along each axis for sampling. A larger number will mean a lower average density.\n\n    _blurringFactor_ : `optional [double]`\n\n    > Default `0.1`, the sigma value used for smoothing the surface density. The higher this number the smoother the surface.\n\n    _contours_ : `optional [int]`\n\n    > Default 15, the number of different heights drawn. If this number is low the resultant image will look very banded. It is recommended this be raised above `50` if you want your images to look good, **Warning** this will make them much slower to generate and interact with.\n\n    _graphType_ : `optional [str]`\n\n    > Default `'coloured'`, if `'coloured'` the image will have a destiny based colourization applied, the only other option is `'solid'` which removes the colourization.\n\n    \"\"\"\n    from mpl_toolkits.mplot3d import Axes3D\n\n    if not isinstance(G, nx.classes.digraph.DiGraph) and not isinstance(G, nx.classes.graph.Graph):\n        raise TypeError(\"{} is not a valid input.\".format(type(G)))\n    if layout is None:\n        layout = nx.spring_layout(G, scale = axisSamples - 1, iterations = iters)\n        grid = np.zeros( [axisSamples, axisSamples],dtype=np.float32)\n        for v in layout.values():\n            x, y = tuple(int(x) for x in v.round(0))\n            grid[y][x] += 1\n    elif isinstance(layout, dict):\n        layout = layout.copy()\n        grid = np.zeros([axisSamples, axisSamples],dtype=np.float32)\n        multFactor = (axisSamples - 1) / layoutScaleFactor\n        for k in layout.keys():\n            tmpPos = layout[k] * multFactor\n            layout[k] = tmpPos\n            x, y = tuple(int(x) for x in tmpPos.round(0))\n            grid[y][x] += 1\n    else:\n        raise TypeError(\"{} is not a valid input.\".format(type(layout)))\n    fig = plt.figure()\n    #axis = fig.add_subplot(111)\n    axis = fig.gca(projection='3d')\n    if overlay:\n        nx.draw_networkx(G, pos = layout, ax = axis, node_size = nodeSize, with_labels = False, edgelist = [])\n    grid = ndi.gaussian_filter(grid, (blurringFactor * axisSamples, blurringFactor * axisSamples))\n    X = Y = np.arange(0, axisSamples, 1)\n    X, Y = np.meshgrid(X, Y)\n    if graphType == \"solid\":\n        CS = axis.plot_surface(X,Y, grid)\n    else:\n        CS = axis.contourf(X, Y, grid, contours)\n    axis.set_xlabel('X')\n    axis.set_ylabel('Y')\n    axis.set_zlabel('Node Density')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getMonth(s):\n    monthOrSeason = s.split('-')[0].upper()\n    if monthOrSeason in monthDict:\n        return monthDict[monthOrSeason]\n    else:\n        monthOrSeason = s.split('-')[1].upper()\n        if monthOrSeason.isdigit():\n            return monthOrSeason\n        else:\n            return monthDict[monthOrSeason]\n\n    raise ValueError(\"Month format not recognized: \" + s)", "response": "Returns the month of the given string"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes dict that maps from key to value and back", "response": "def makeBiDirectional(d):\n    \"\"\"\n    Helper for generating tagNameConverter\n    Makes dict that maps from key to value and back\n    \"\"\"\n    dTmp = d.copy()\n    for k in d:\n        dTmp[d[k]] = k\n    return dTmp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reverseDict(d):\n    retD = {}\n    for k in d:\n        retD[d[k]] = k\n    return retD", "response": "Returns a dict of key - value pairs that can be used to generate a fullToTag\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the nodes to the graph grph according to the rules give by nodeType fullInfo coreCitesDict and detailedValues.", "response": "def addToNetwork(grph, nds, count, weighted, nodeType, nodeInfo, fullInfo, coreCitesDict, coreValues, detailedValues, addCR, recordToCite = True, headNd = None):\n    \"\"\"Addeds the citations _nds_ to _grph_, according to the rules give by _nodeType_, _fullInfo_, etc.\n\n    _headNd_ is the citation of the Record\n    \"\"\"\n    if headNd is not None:\n        hID = makeID(headNd, nodeType)\n        if nodeType == 'full' or nodeType == 'original':\n            hYear = getattr(headNd, \"year\")\n        if hID not in grph:\n            nodeName, nodeDat = makeNodeTuple(headNd, hID, nodeInfo, fullInfo, nodeType, count, coreCitesDict, coreValues, detailedValues, addCR)\n            grph.add_node(nodeName, **nodeDat)\n    else:\n        hID = None\n    idList = []\n    yearList = []\n    for n in nds:\n        nID = makeID(n, nodeType)\n        if nodeType == 'full' or nodeType == 'original':\n            try:\n                nYear = getattr(n, \"year\")\n            except:\n                nYear = None\n            yearList.append(nYear)\n\n        if nID not in grph:\n            nodeName, nodeDat = makeNodeTuple(n, nID, nodeInfo, fullInfo, nodeType, count, coreCitesDict, coreValues, detailedValues, addCR)\n            grph.add_node(nodeName, **nodeDat)\n        elif count:\n            grph.node[nID]['count'] += 1\n        idList.append(nID)\n\n    addedEdges = []\n    if hID:\n        for i in range(len(idList)):\n            nID = idList[i]\n            if nodeType == 'full' or nodeType == 'original':\n                nYear = yearList[i]\n                try:\n                    yearDiff = abs(hYear - nYear)\n                except:\n                    yearDiff = None\n\n                if weighted:\n                    try:\n                        if recordToCite:\n                            grph[hID][nID]['weight'] += 1\n                        else:\n                            grph[nID][hID]['weight'] += 1\n                    except KeyError:\n                        if recordToCite:\n                            grph.add_edge(hID, nID, weight=1, yearDiff=yearDiff)\n                        else:\n                            grph.add_edge(nID, hID, weight=1, yearDiff=yearDiff)\n                elif nID not in grph[hID]:\n                    addedEdges.append((hID, nID))\n\n            elif weighted:\n                try:\n                    if recordToCite:\n                        grph[hID][nID]['weight'] += 1\n                    else:\n                        grph[nID][hID]['weight'] += 1\n                except KeyError:\n                    if recordToCite:\n                        grph.add_edge(hID, nID, weight=1)\n                    else:\n                        grph.add_edge(hID, nID, weight=1)\n            elif nID not in grph[hID]:\n                addedEdges.append((hID, nID, {yearDiff: yearDiff}))\n    elif len(idList) > 1:\n        for i, outerID in enumerate(idList):\n            for innerID in idList[i + 1:]:\n                if weighted:\n                    try:\n                        grph[outerID][innerID]['weight'] += 1\n                    except KeyError:\n                        grph.add_edge(outerID, innerID, weight = 1)\n                elif innerID not in grph[outerID]:\n                    addedEdges.append((outerID, innerID))\n    grph.add_edges_from(addedEdges)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes a tuple of idVal and a dict of the selected attributes", "response": "def makeNodeTuple(citation, idVal, nodeInfo, fullInfo, nodeType, count, coreCitesDict, coreValues, detailedValues, addCR):\n    \"\"\"Makes a tuple of idVal and a dict of the selected attributes\"\"\"\n    d = {}\n    if nodeInfo:\n        if nodeType == 'full':\n            if coreValues:\n                if citation in coreCitesDict:\n                    R = coreCitesDict[citation]\n                    d['MK-ID'] = R.id\n                    if not detailedValues:\n                        infoVals = []\n                        for tag in coreValues:\n                            tagVal = R.get(tag)\n                            if isinstance(tagVal, str):\n                                infoVals.append(tagVal.replace(',',''))\n                            elif isinstance(tagVal, list):\n                                infoVals.append(tagVal[0].replace(',',''))\n                            else:\n                                pass\n                        d['info'] = ', '.join(infoVals)\n                    else:\n                        for tag in coreValues:\n                            v = R.get(tag, None)\n                            if isinstance(v, list):\n                                d[tag] = '|'.join(sorted(v))\n                            else:\n                                d[tag] = v\n                    d['inCore'] = True\n                    if addCR:\n                        d['citations'] = '|'.join((str(c) for c in R.get('citations', [])))\n                else:\n                    d['MK-ID'] = 'None'\n                    d['info'] = citation.allButDOI()\n                    d['inCore'] = False\n                    if addCR:\n                        d['citations'] = ''\n            else:\n                d['info'] = citation.allButDOI()\n        elif nodeType == 'journal':\n            if citation.isJournal():\n                d['info'] = str(citation.FullJournalName())\n            else:\n                d['info'] = \"None\"\n        elif nodeType == 'original':\n            d['info'] = str(citation)\n        else:\n            d['info'] = idVal\n    if fullInfo:\n        d['fullCite'] = str(citation)\n    if count:\n        d['count'] = 1\n    return (idVal, d)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expandRecs(G, RecCollect, nodeType, weighted):\n    for Rec in RecCollect:\n        fullCiteList = [makeID(c, nodeType) for c in Rec.createCitation(multiCite = True)]\n        if len(fullCiteList) > 1:\n            for i, citeID1 in enumerate(fullCiteList):\n                if citeID1 in G:\n                    for citeID2 in fullCiteList[i + 1:]:\n                        if citeID2 not in G:\n                            G.add_node(citeID2, **G.node[citeID1])\n                            if weighted:\n                                G.add_edge(citeID1, citeID2, weight = 1)\n                            else:\n                                G.add_edge(citeID1, citeID2)\n                        elif weighted:\n                            try:\n                                G.edges[citeID1, citeID2]['weight'] += 1\n                            except KeyError:\n                                G.add_edge(citeID1, citeID2, weight = 1)\n                        for e1, e2, data in G.edges(citeID1, data = True):\n                            G.add_edge(citeID2, e2, **data)", "response": "Expand all the citations from _RecCollect_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndropping the non journal type Records from the collection.", "response": "def dropNonJournals(self, ptVal = 'J', dropBad = True, invert = False):\n        \"\"\"Drops the non journal type `Records` from the collection, this is done by checking _ptVal_ against the PT tag\n\n        # Parameters\n\n        _ptVal_ : `optional [str]`\n\n        > Default `'J'`, The value of the PT tag to be kept, default is `'J'` the journal tag, other tags can be substituted.\n\n        _dropBad_ : `optional [bool]`\n\n        > Default `True`, if `True` bad `Records` will be dropped as well those that are not journal entries\n\n        _invert_ : `optional [bool]`\n\n        > Default `False`, Set `True` to drop journals (or the PT tag given by _ptVal_) instead of keeping them. **Note**, it still drops bad Records if _dropBad_ is `True`\n        \"\"\"\n        if dropBad:\n            self.dropBadEntries()\n        if invert:\n            self._collection = {r for r in self._collection if r['pubType'] != ptVal.upper()}\n        else:\n            self._collection = {r for r in self._collection if r['pubType'] == ptVal.upper()}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef writeFile(self, fname = None):\n        if len(self._collectedTypes) < 2:\n            recEncoding = self.peek().encoding()\n        else:\n            recEncoding = 'utf-8'\n        if fname:\n            f = open(fname, mode = 'w', encoding = recEncoding)\n        else:\n            f = open(self.name[:200] + '.txt', mode = 'w', encoding = recEncoding)\n        if self._collectedTypes == {'WOSRecord'}:\n            f.write(\"\\ufeffFN Thomson Reuters Web of Science\\u2122\\n\")\n            f.write(\"VR 1.0\\n\")\n        elif self._collectedTypes == {'MedlineRecord'}:\n            f.write('\\n')\n        elif self._collectedTypes == {'ScopusRecord'}:\n            f.write(\"\\ufeff{}\\n\".format(','.join(scopusHeader)))\n        for R in self._collection:\n            R.writeRecord(f)\n            f.write('\\n')\n        if self._collectedTypes == {'WOSRecord'}:\n            f.write('EF')\n        f.close()", "response": "Writes the RecordCollection to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites all the records from the collection into a csv file.", "response": "def writeCSV(self, fname = None, splitByTag = None, onlyTheseTags = None, numAuthors = True, genderCounts = True, longNames = False, firstTags = None, csvDelimiter = ',', csvQuote = '\"', listDelimiter = '|'):\n        \"\"\"Writes all the `Records` from the collection into a csv file with each row a record and each column a tag.\n\n        # Parameters\n\n        _fname_ : `optional [str]`\n\n        > Default `None`, the name of the file to write to, if `None` it uses the collections name suffixed by .csv.\n\n        _splitByTag_ : `optional [str]`\n\n        > Default `None`, if a tag is given the output will be divided into different files according to the value of the tag, with only the records associated with that tag. For example if `'authorsFull'` is given then each file will only have the lines for `Records` that author is named in.\n\n        > The file names are the values of the tag followed by a dash then the normale name for the file as given by _fname_, e.g. for the year 2016 the file could be called `'2016-fname.csv'`.\n\n        _onlyTheseTags_ : `optional [iterable]`\n\n        > Default `None`, if an iterable (list, tuple, etc) only the tags in _onlyTheseTags_ will be used, if not given then all tags in the records are given.\n\n        > If you want to use all known tags pass [metaknowledge.knownTagsList](./ExtendedRecord.html#metaknowledge.ExtendedRecord.tagProcessingFunc).\n\n        _numAuthors_ : `optional [bool]`\n\n        > Default `True`, if `True` adds the number of authors as the column `'numAuthors'`.\n\n        _longNames_ : `optional [bool]`\n\n        > Default `False`, if `True` will convert the tags to their longer names, otherwise the short 2 character ones will be used.\n\n        _firstTags_ : `optional [iterable]`\n\n        > Default `None`, if `None` the iterable `['UT', 'PT', 'TI', 'AF', 'CR']` is used. The tags given by the iterable are the first ones in the csv in the order given.\n\n        > **Note** if tags are in _firstTags_ but not in _onlyTheseTags_, _onlyTheseTags_ will override _firstTags_\n\n        _csvDelimiter_ : `optional [str]`\n\n        > Default `','`, the delimiter used for the cells of the csv file.\n\n        _csvQuote_ : `optional [str]`\n\n        > Default `'\"'`, the quote character used for the csv.\n\n        _listDelimiter_ : `optional [str]`\n\n        > Default `'|'`, the delimiter used between values of the same cell if the tag for that record has multiple outputs.\n        \"\"\"\n        if firstTags is None:\n            firstTags = ['id', 'title', 'authorsFull', 'citations', 'keywords', 'DOI']\n        for i in range(len(firstTags)):\n            if firstTags[i] in fullToTagDict:\n                firstTags[i] = fullToTagDict[firstTags[i]]\n        if onlyTheseTags:\n            for i in range(len(onlyTheseTags)):\n                if onlyTheseTags[i] in fullToTagDict:\n                    onlyTheseTags[i] = fullToTagDict[onlyTheseTags[i]]\n            retrievedFields = [t for t in firstTags if t in onlyTheseTags] + [t for t in onlyTheseTags if t not in firstTags]\n        else:\n            retrievedFields = firstTags\n            for R in self:\n                tagsLst = [t for t in R.keys() if t not in retrievedFields]\n                retrievedFields += tagsLst\n        if longNames:\n            try:\n                retrievedFields = [tagToFullDict[t] for t in retrievedFields]\n            except KeyError:\n                raise KeyError(\"One of the tags could not be converted to a long name.\")\n        if fname:\n            baseFileName = fname\n        else:\n            baseFileName = \"{}.csv\".format(self.name[:200])\n        if numAuthors:\n            csvWriterFields = retrievedFields + [\"num-Authors\"]\n        else:\n            csvWriterFields = retrievedFields\n        if genderCounts:\n            csvWriterFields += ['num-Male', 'num-Female', 'num-Unknown']\n        if splitByTag is None:\n            f = open(baseFileName, mode = 'w', encoding = 'utf-8', newline = '')\n            csvWriter = csv.DictWriter(f, csvWriterFields, delimiter = csvDelimiter, quotechar = csvQuote, quoting=csv.QUOTE_ALL)\n            csvWriter.writeheader()\n        else:\n            filesDict = {}\n        for R in self:\n            if splitByTag:\n                try:\n                    splitVal = R[splitByTag]\n                except KeyError:\n                    continue\n                else:\n                    if not isinstance(splitVal, list):\n                        splitVal = [str(splitVal)]\n            recDict = {}\n            for t in retrievedFields:\n                value = R.get(t)\n                if isinstance(value, str):\n                    recDict[t] = value\n                elif hasattr(value, '__iter__'):\n                    recDict[t] = listDelimiter.join([str(v) for v in value])\n                elif value is None:\n                    recDict[t] = ''\n                else:\n                    recDict[t] = str(value)\n            if numAuthors:\n                recDict[\"num-Authors\"] = len(R.get('authorsShort', []))\n            if genderCounts:\n                recDict['num-Male'], recDict['num-Female'], recDict['num-Unknown'] = R.authGenders(_countsTuple = True)\n            if splitByTag:\n                for sTag in splitVal:\n                    if sTag in filesDict:\n                        filesDict[sTag][1].writerow(recDict)\n                    else:\n                        fname = \"{}-{}\".format(sTag[:200], baseFileName)\n                        f = open(fname, mode = 'w', encoding = 'utf-8', newline = '')\n                        csvWriter = csv.DictWriter(f, csvWriterFields, delimiter = csvDelimiter, quotechar = csvQuote, quoting=csv.QUOTE_ALL)\n                        csvWriter.writeheader()\n                        csvWriter.writerow(recDict)\n                        filesDict[sTag] = (f, csvWriter)\n            else:\n                csvWriter.writerow(recDict)\n        if splitByTag:\n            for f, c in filesDict.values():\n                f.close()\n        else:\n            f.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a bibtex entry to the given file.", "response": "def writeBib(self, fname = None, maxStringLength = 1000, wosMode = False, reducedOutput = False, niceIDs = True):\n        \"\"\"Writes a bibTex entry to _fname_ for each `Record` in the collection.\n\n        If the Record is of a journal article (PT J) the bibtext type is set to `'article'`, otherwise it is set to `'misc'`. The ID of the entry is the WOS number and all the Record's fields are given as entries with their long names.\n\n        **Note** This is not meant to be used directly with LaTeX none of the special characters have been escaped and there are a large number of unnecessary fields provided. _niceID_ and _maxLength_ have been provided to make conversions easier only.\n\n        **Note** Record entries that are lists have their values separated with the string `' and '`, as this is the way bibTex understands\n\n        # Parameters\n\n        _fname_ : `optional [str]`\n\n        > Default `None`, The name of the file to be written. If not given one will be derived from the collection and the file will be written to .\n\n        _maxStringLength_ : `optional [int]`\n\n        > Default 1000, The max length for a continuous string. Most bibTex implementation only allow string to be up to 1000 characters ([source](https://www.cs.arizona.edu/~collberg/Teaching/07.231/BibTeX/bibtex.html)), this splits them up into substrings then uses the native string concatenation (the `'#'` character) to allow for longer strings\n\n        _WOSMode_ : `optional [bool]`\n\n        > Default `False`, if `True` the data produced will be unprocessed and use double curly braces. This is the style WOS produces bib files in and mostly macthes that.\n\n        _restrictedOutput_ : `optional [bool]`\n\n        > Default `False`, if `True` the tags output will be limited to: `'AF'`, `'BF'`, `'ED'`, `'TI'`, `'SO'`, `'LA'`, `'NR'`, `'TC'`, `'Z9'`, `'PU'`, `'J9'`, `'PY'`, `'PD'`, `'VL'`, `'IS'`, `'SU'`, `'PG'`, `'DI'`, `'D2'`, and `'UT'`\n\n        _niceID_ : `optional [bool]`\n\n        > Default `True`, if `True` the IDs used will be derived from the authors, publishing date and title, if `False` it will be the UT tag\n        \"\"\"\n        if fname:\n            f = open(fname, mode = 'w', encoding = 'utf-8')\n        else:\n            f = open(self.name[:200] + '.bib', mode = 'w', encoding = 'utf-8')\n        f.write(\"%This file was generated by the metaknowledge Python package.\\n%The contents have been automatically generated and are likely to not work with\\n%LaTeX without some human intervention. This file is meant for other automatic\\n%systems and not to be used directly for making citations\\n\")\n        #I figure this is worth mentioning, as someone will get annoyed at none of the special characters being escaped and how terrible some of the fields look to humans\n        for R in self:\n            try:\n                f.write('\\n\\n')\n                f.write(R.bibString(maxLength =  maxStringLength, WOSMode = wosMode, restrictedOutput = reducedOutput, niceID = niceIDs))\n            except BadWOSRecord:\n                pass\n            except AttributeError:\n                raise RecordsNotCompatible(\"The Record '{}', with ID '{}' does not support writing to bibtext files.\".format(R, R.id))\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the most likely copyright string from all abstracts in the RecordCollection and returns a list of all the copyright strings that are likely to be used in the record collection.", "response": "def findProbableCopyright(self):\n        \"\"\"Finds the (likely) copyright string from all abstracts in the `RecordCollection`\n\n        # Returns\n\n        `list[str]`\n\n        > A deduplicated list of all the copyright strings\n        \"\"\"\n        retCopyrights = set()\n        for R in self:\n            begin, abS = findCopyright(R.get('abstract', ''))\n            if abS != '':\n                retCopyrights.add(abS)\n        return list(retCopyrights)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forBurst(self, tag, outputFile = None, dropList = None, lower = True, removeNumbers = True, removeNonWords = True, removeWhitespace = True, stemmer = None):\n\n        whiteSpaceRegex = re.compile(r'\\s+')\n\n        if removeNumbers:\n            if removeNonWords:\n                otherString = r\"[\\W\\d]\"\n            else:\n                otherString = r\"\\d\"\n        elif removeNonWords:\n            otherString = r\"\\W\"\n        else:\n            otherString = ''\n\n        def otherRepl(r):\n            if r.group(0) == ' ':\n                return ' '\n            else:\n                return ''\n        otherDropsRegex = re.compile(otherString)\n\n        def burstPreper(inString):\n            if dropList is not None:\n                inString = \" {} \".format(inString)\n                for dropS in (\" {} \".format(s) for s in dropList):\n                    if dropS in inString:\n                        inString = inString.replace(dropS, ' ')\n                inString = inString[1:-1]\n            if removeWhitespace:\n                inString = re.sub(whiteSpaceRegex, lambda x: ' ', inString, count = 0)\n            if lower:\n                inString = inString.lower()\n            inString = re.sub(otherDropsRegex, otherRepl, inString, count = 0)\n            sTokens = inString.split(' ')\n            if stemmer is not None:\n                retTokens = []\n                for token in sTokens:\n                    if stemmer is not None:\n                        token = stemmer(token)\n                    retTokens.append(token)\n            else:\n                retTokens = sTokens\n            return retTokens\n\n        retDict = {'year' : [], 'word' : []}\n\n        pcount = 0\n        pmax = len(self)\n        progArgs = (0, \"Starting to work on DataFrame for burst analysis\")\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            for R in self:\n                pcount += 1\n                PBar.updateVal(pcount/ pmax, \"Analyzing: {}\".format(R))\n                try:\n                    year = R['year']\n                except KeyError:\n                    continue\n                try:\n                    burstVal = R[tag]\n                except KeyError:\n                    continue\n                else:\n                    if isinstance(burstVal, list):\n                        burstVal = ' '.join((str(i) for i in burstVal))\n                    else:\n                        burstVal = str(burstVal)\n                for sToken in burstPreper(burstVal):\n                    retDict['year'].append(year)\n                    retDict['word'].append(sToken)\n\n            if outputFile is not None:\n                PBar.updateVal(.99, \"Writing to file: {}\".format(outputFile))\n                with open(outputFile, 'w', newline = '') as f:\n                    writer = csv.DictWriter(f, ['year', 'word'])\n                    for row in range(len(retDict['year'])):\n                        writer.writerow({k : retDict[k][row] for k in retDict.keys()})\n            PBar.finish(\"Done burst analysis DataFrame with {} rows\".format(len(retDict['year'])))\n        return retDict", "response": "Creates a pandas friendly dictionary that contains the words that occurred in a record with the given tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a pandas friendly dictionary with each row a Record in the RecordCollection and the columns fields natural language processing uses.", "response": "def forNLP(self, outputFile = None, extraColumns = None, dropList = None, lower = True, removeNumbers = True, removeNonWords = True, removeWhitespace = True, removeCopyright = False, stemmer = None):\n        \"\"\"Creates a pandas friendly dictionary with each row a `Record` in the `RecordCollection` and the columns fields natural language processing uses (id, title, publication year, keywords and the abstract). The abstract is by default is processed to remove non-word, non-space characters and the case is lowered.\n\n        # Parameters\n\n        _outputFile_ : `optional str`\n\n        > default `None`, if a file path is given a csv of the returned data will be written\n\n        _extraColumns_ : `optional list[str]`\n\n        > default `None`, if a list of tags is given each of the tag's values for a `Record` will be added to the output(s)\n\n        _dropList_ : `optional list[str]`\n\n        > default `None`, if a list of strings is provided they will be dropped from the output's abstracts. The matching is case sensitive and done before any other processing. The strings will only be dropped if they are surrounded on both sides with spaces (`' '`) so if `dropList = ['a']` then `'a cat'` will become `'cat'`.\n\n        _lower_ : `optional bool`\n\n        > default `True`, if `True` the abstract will made to lower case\n\n        _removeNumbers_ : `optional bool`\n\n        > default `True`, if `True` all numbers will be removed\n\n        _removeNonWords_ : `optional bool`\n\n        > default `True`, if `True` all non-number non-number characters will be removed\n\n        _removeWhitespace_ : `optional bool`\n\n        > default `True`, if `True` all whitespace will be converted to a single space (`' '`)\n\n        _removeCopyright_ : `optional bool`\n\n        > default `False`, if `True` the copyright statement at the end of the abstract will be removed and added to a new column. Note this is heuristic based and will not work for all papers.\n\n        _stemmer_ : `optional func`\n\n        > default `None`, if a function is provided it will be run on each individual word in the abstract and the output will replace it. For example to use the  `PorterStemmer` in the _nltk_ package you would give `nltk.PorterStemmer().stem`\n        \"\"\"\n        whiteSpaceRegex = re.compile(r'\\s+')\n\n        if removeNumbers:\n            if removeNonWords:\n                otherString = r\"[\\W\\d]\"\n            else:\n                otherString = r\"\\d\"\n        elif removeNonWords:\n            otherString = r\"\\W\"\n        else:\n            otherString = ''\n\n        def otherRepl(r):\n            if r.group(0) == ' ':\n                return ' '\n            else:\n                return ''\n        otherDropsRegex = re.compile(otherString)\n\n        def abPrep(abst):\n            if dropList is not None:\n                #incase a drop string is on the edge\n                abst = \" {} \".format(abst.replace('\\n', ' '))\n                for dropS in (\" {} \".format(s) for s in dropList):\n                    if dropS in abst:\n                        abst = abst.replace(dropS, ' ')\n                abst = abst[1:-1]\n            if removeWhitespace:\n                abst = re.sub(whiteSpaceRegex, lambda x: ' ', abst, count = 0)\n            if removeCopyright:\n                abst, copyrightString = findCopyright(abst)\n            else:\n                copyrightString = ''\n            if lower:\n                abst = abst.lower()\n            abst = re.sub(otherDropsRegex, otherRepl, abst, count = 0)\n            if stemmer is not None:\n                sTokens = abst.split(' ')\n                retTokens = []\n                for token in sTokens:\n                    if stemmer is not None:\n                        token = stemmer(token)\n                    retTokens.append(token)\n                abst = ' '.join(retTokens)\n            return abst, copyrightString\n\n        pcount = 0\n        pmax = len(self)\n        progArgs = (0, \"Starting to work on DataFrame for NLP\")\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n\n        retDict = {'id' : [], 'year' : [], 'title' : [], 'keywords' : [], 'abstract' : []}\n        if removeCopyright:\n            retDict['copyright'] = []\n        if extraColumns is None:\n            extraColumns = []\n        else:\n            for builtinColumn in ['id', 'year', 'title', 'keywords', 'abstract']:\n                if builtinColumn in extraColumns:\n                    extraColumns.remove(builtinColumn)\n        for column in extraColumns:\n            retDict[column] = []\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            for R in self:\n                pcount += 1\n                PBar.updateVal(pcount/ pmax, \"Analyzing: {}\".format(R))\n                abstract, copyrightString = abPrep(R.get('AB', ''))\n\n                retDict['id'].append(R.id)\n                retDict['year'].append(R.get('year', ''))\n                retDict['title'].append(R.get('title', ''))\n                retDict['keywords'].append('|'.join(R.get('keywords', [])))\n                retDict['abstract'].append(abstract)\n                if removeCopyright:\n                    retDict['copyright'].append(copyrightString)\n                for extraTag in extraColumns:\n                    e = R.get(extraTag)\n                    if isinstance(e, list):\n                        e = '|'.join((str(s) for s in e))\n                    elif e is None:\n                        e = ''\n                    retDict[extraTag].append(e)\n\n            if outputFile is not None:\n                PBar.updateVal(.99, \"Writing to file: {}\".format(outputFile))\n                with open(outputFile, 'w', newline = '') as f:\n                    fieldNames = list(retDict.keys())\n                    fieldNames.remove('id')\n                    fieldNames.remove('title')\n                    fieldNames.remove('year')\n                    fieldNames.remove('keywords')\n                    fieldNames = ['id', 'year', 'title', 'keywords'] + fieldNames\n                    writer = csv.DictWriter(f, fieldNames)\n                    writer.writeheader()\n                    for row in range(len(retDict['id'])):\n                        writer.writerow({k : retDict[k][row] for k in retDict.keys()})\n            PBar.finish(\"Done NLP DataFrame with {} rows\".format(len(retDict['id'])))\n        return retDict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeDict(self, onlyTheseTags = None, longNames = False, raw = False, numAuthors = True, genderCounts = True):\n        if onlyTheseTags:\n            for i in range(len(onlyTheseTags)):\n                if onlyTheseTags[i] in fullToTagDict:\n                    onlyTheseTags[i] = fullToTagDict[onlyTheseTags[i]]\n            retrievedFields = onlyTheseTags\n        else:\n            retrievedFields = []\n            for R in self:\n                tagsLst = [t for t in R.keys() if t not in retrievedFields]\n                retrievedFields += tagsLst\n        if longNames:\n            try:\n                retrievedFields = [tagToFullDict[t] for t in retrievedFields]\n            except KeyError:\n                raise KeyError(\"One of the tags could not be converted to a long name.\")\n        retDict = {k : [] for k in retrievedFields}\n        if numAuthors:\n            retDict[\"num-Authors\"] = []\n        if genderCounts:\n            retDict.update({'num-Male' : [], 'num-Female' : [], 'num-Unknown' : []})\n        for R in self:\n            if numAuthors:\n                retDict[\"num-Authors\"].append(len(R.get('authorsShort', [])))\n            if genderCounts:\n                m, f, u = R.authGenders(_countsTuple = True)\n                retDict['num-Male'].append(m)\n                retDict['num-Female'].append(f)\n                retDict['num-Unknown'].append(u)\n            for k, v in R.subDict(retrievedFields, raw = raw).items():\n                retDict[k].append(v)\n        return retDict", "response": "Returns a dict with each key a tag and the values being lists of the values for each of the Records in the collection."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef genderStats(self, asFractions = False):\n\n        maleCount = 0\n        femaleCount = 0\n        unknownCount = 0\n        for R in self:\n            m, f, u = R.authGenders(_countsTuple = True)\n            maleCount += m\n            femaleCount += f\n            unknownCount += u\n        if asFractions:\n            tot = maleCount + femaleCount + unknownCount\n            return {'Male' : maleCount / tot, 'Female' : femaleCount / tot, 'Unknown' : unknownCount / tot}\n        return {'Male' : maleCount, 'Female' : femaleCount, 'Unknown' : unknownCount}", "response": "Returns a dict with the numbers of male female and unknown names in each category."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getCitations(self, field = None, values = None, pandasFriendly = True, counts = True):\n        retCites = []\n        if values is not None:\n            if isinstance(values, (str, int, float)) or not isinstance(values, collections.abc.Container):\n                values = [values]\n        for R in self:\n            retCites += R.getCitations(field = field, values = values, pandasFriendly = False)\n        if pandasFriendly:\n            return _pandasPrep(retCites, counts)\n        else:\n            return list(set(retCites))", "response": "Returns a pandas ready dict with each row a different citation the contained Records and columns containing the original string year journal author s name and number of times it occured."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef networkCoAuthor(self, detailedInfo = False, weighted = True, dropNonJournals = False, count = True, useShortNames = False, citeProfile = False):\n        grph = nx.Graph()\n        pcount = 0\n        progArgs = (0, \"Starting to make a co-authorship network\")\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        if bool(detailedInfo):\n            try:\n                infoVals = []\n                for tag in detailedInfo:\n                    infoVals.append(normalizeToTag(tag))\n            except TypeError:\n                infoVals = ['year', 'title', 'journal', 'volume', 'beginningPage']\n            def attributeMaker(Rec):\n                attribsDict = {}\n                for val in infoVals:\n                    recVal = Rec.get(val)\n                    if isinstance(recVal, list):\n                        attribsDict[val] = ', '.join((str(v).replace(',', '') for v in recVal))\n                    else:\n                        attribsDict[val] = str(recVal).replace(',', '')\n                if count:\n                    attribsDict['count'] = 1\n                if citeProfile:\n                    attribsDict['citeProfile'] = {}\n                return attribsDict\n        else:\n            if count:\n                if citeProfile:\n                    attributeMaker = lambda x: {'count' : 1, 'citeProfile' : {}}\n                else:\n                    attributeMaker = lambda x: {'count' : 1}\n            else:\n                if citeProfile:\n                    attributeMaker = lambda x: {'citeProfile' : {}}\n                else:\n                    attributeMaker = lambda x: {}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            for R in self:\n                if PBar:\n                    pcount += 1\n                    PBar.updateVal(pcount/ len(self), \"Analyzing: \" + str(R))\n                if dropNonJournals and not R.createCitation().isJournal():\n                    continue\n                if useShortNames:\n                    authsList = R.get('authorsShort', [])\n                else:\n                    authsList = R.get('authorsFull', [])\n                if authsList:\n                    authsList = list(authsList)\n                    detailedInfo = attributeMaker(R)\n                    if citeProfile:\n                        citesLst = R.get('citations', [])\n                    for i, auth1 in enumerate(authsList):\n                        if auth1 not in grph:\n                            grph.add_node(auth1, **detailedInfo.copy())\n                        elif count:\n                            grph.node[auth1]['count'] += 1\n                        if citeProfile:\n                            for c in citesLst:\n                                try:\n                                    grph.node[auth1]['citeProfile'][c] += 1\n                                except KeyError:\n                                    grph.node[auth1]['citeProfile'][c] = 1\n                        for auth2 in authsList[i + 1:]:\n                            if auth2 not in grph:\n                                grph.add_node(auth2, **detailedInfo.copy())\n                            elif count:\n                                grph.node[auth2]['count'] += 1\n                            if citeProfile:\n                                for c in citesLst:\n                                    try:\n                                        grph.node[auth2]['citeProfile'][c] += 1\n                                    except KeyError:\n                                        grph.node[auth2]['citeProfile'][c] = 1\n                            if grph.has_edge(auth1, auth2) and weighted:\n                                grph.edges[auth1, auth2]['weight'] += 1\n                            elif weighted:\n                                grph.add_edge(auth1, auth2, weight = 1)\n                            else:\n                                grph.add_edge(auth1, auth2)\n            if citeProfile:\n                if PBar:\n                    PBar.updateVal(.99, \"Extracting citation profiles\")\n                previous = {}\n                for n, dat in grph.nodes(data = True):\n                    previous[n] = dat\n                    #zip(*l) undoes zip(l1, l2)\n                    try:\n                        cites, counts = zip(*dat['citeProfile'].items())\n                    except ValueError:\n                        cites, counts = [], []\n                    dat['citeProfileCites'] = '|'.join((str(c) for c in cites))\n                    dat['citeProfileCounts'] = '|'.join((str(c) for c in counts))\n                    del dat['citeProfile']\n            if PBar:\n                PBar.finish(\"Done making a co-authorship network from {}\".format(self))\n        return grph", "response": "Creates a networkx graph with author names as nodes and collaborations as edges."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef networkCoCitation(self, dropAnon = True, nodeType = \"full\", nodeInfo = True, fullInfo = False, weighted = True, dropNonJournals = False, count = True, keyWords = None, detailedCore = True, detailedCoreAttributes = False, coreOnly = False, expandedCore = False, addCR = False):\n        allowedTypes = [\"full\", \"original\", \"author\", \"journal\", \"year\"]\n        if nodeType not in allowedTypes:\n            raise RCValueError(\"{} is not an allowed nodeType.\".format(nodeType))\n        coreValues = []\n        if bool(detailedCore):\n            try:\n                for tag in detailedCore:\n                    coreValues.append(normalizeToTag(tag))\n            except TypeError:\n                coreValues = ['id', 'authorsFull', 'year', 'title', 'journal', 'volume', 'beginningPage']\n        tmpgrph = nx.Graph()\n        pcount = 0\n        progArgs = (0, \"Starting to make a co-citation network\")\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            if coreOnly or coreValues or expandedCore:\n                coreCitesDict = {R.createCitation() : R for R in self}\n                if coreOnly:\n                    coreCites = coreCitesDict.keys()\n                else:\n                    coreCites = None\n            else:\n                coreCitesDict = None\n                coreCites = None\n            for R in self:\n                if PBar:\n                    pcount += 1\n                    PBar.updateVal(pcount / len(self), \"Analyzing: {}\".format(R))\n                Cites = R.get('citations')\n                if Cites:\n                    filteredCites = filterCites(Cites, nodeType, dropAnon, dropNonJournals, keyWords, coreCites)\n                    addToNetwork(tmpgrph, filteredCites, count, weighted, nodeType, nodeInfo , fullInfo, coreCitesDict, coreValues, detailedCoreAttributes, addCR, headNd = None)\n            if expandedCore:\n                if PBar:\n                    PBar.updateVal(.98, \"Expanding core Records\")\n                expandRecs(tmpgrph, self, nodeType, weighted)\n            if PBar:\n                PBar.finish(\"Done making a co-citation network from {}\".format(self))\n        return tmpgrph", "response": "Creates a co - citation network for the RecordCollection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a network of bibliographic coupling for the RecordCollection.", "response": "def networkBibCoupling(self, weighted = True, fullInfo = False, addCR = False):\n        \"\"\"Creates a bibliographic coupling network based on citations for the RecordCollection.\n\n        # Parameters\n\n        _weighted_ : `optional bool`\n\n        > Default `True`, if `True` the weight of the edges will be added to the network\n\n        _fullInfo_ : `optional bool`\n\n        > Default `False`, if `True` the full citation string will be added to each of the nodes of the network.\n\n        # Returns\n\n        `Networkx Graph`\n\n        > A graph of the bibliographic coupling\n        \"\"\"\n        progArgs = (0, \"Make a citation network for coupling\")\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            citeGrph = self.networkCitation(weighted = False, directed = True, detailedCore = True, fullInfo = fullInfo, count = False, nodeInfo = True, addCR = addCR, _quiet = True)\n            pcount = 0\n            pmax = len(citeGrph)\n            PBar.updateVal(.2, \"Starting to classify nodes\")\n            workingGrph = nx.Graph()\n            couplingSet = set()\n            for n, d in citeGrph.nodes(data = True):\n                pcount += 1\n                PBar.updateVal(.2 + .4 * (pcount / pmax), \"Classifying: {}\".format(n))\n                if d['inCore']:\n                    workingGrph.add_node(n, **d)\n                if citeGrph.in_degree(n) > 0:\n                    couplingSet.add(n)\n            pcount = 0\n            pmax = len(couplingSet)\n            for n in couplingSet:\n                PBar.updateVal(.6 + .4 * (pcount / pmax), \"Coupling: {}\".format(n))\n                citesLst = list(citeGrph.in_edges(n))\n                for i, edgeOuter in enumerate(citesLst):\n                    outerNode = edgeOuter[0]\n                    for edgeInner in citesLst[i + 1:]:\n                        innerNode = edgeInner[0]\n                        if weighted and  workingGrph.has_edge(outerNode, innerNode):\n                            workingGrph.edges[outerNode, innerNode]['weight'] += 1\n                        elif weighted:\n                            workingGrph.add_edge(outerNode, innerNode, weight = 1)\n                        else:\n                            workingGrph.add_edge(outerNode, innerNode)\n            PBar.finish(\"Done making a bib-coupling network from {}\".format(self))\n        return workingGrph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a RecordCollection of Records from the years between startYear and endYear inclusive.", "response": "def yearSplit(self, startYear, endYear, dropMissingYears = True):\n        \"\"\"Creates a RecordCollection of Records from the years between _startYear_ and _endYear_ inclusive.\n\n        # Parameters\n\n        _startYear_ : `int`\n\n        > The smallest year to be included in the returned RecordCollection\n\n        _endYear_ : `int`\n\n        > The largest year to be included in the returned RecordCollection\n\n        _dropMissingYears_ : `optional [bool]`\n\n        > Default `True`, if `True` Records with missing years will be dropped. If `False` a `TypeError` exception will be raised\n\n        # Returns\n\n        `RecordCollection`\n\n        > A RecordCollection of Records from _startYear_ to _endYear_\n        \"\"\"\n        recordsInRange = set()\n        for R in self:\n            try:\n                if R.get('year') >= startYear and R.get('year') <= endYear:\n                    recordsInRange.add(R)\n            except TypeError:\n                if dropMissingYears:\n                    pass\n                else:\n                    raise\n        RCret = RecordCollection(recordsInRange, name = \"{}({}-{})\".format(self.name, startYear, endYear), quietStart = True)\n        RCret._collectedTypes = self._collectedTypes.copy()\n        return RCret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary with all the citations in the CR field as keys and the number of times they occur as the values.", "response": "def localCiteStats(self, pandasFriendly = False, keyType = \"citation\"):\n        \"\"\"Returns a dict with all the citations in the CR field as keys and the number of times they occur as the values\n\n        # Parameters\n\n        _pandasFriendly_ : `optional [bool]`\n\n        > default `False`, makes the output be a dict with two keys one `'Citations'` is the citations the other is their occurrence counts as `'Counts'`.\n\n        _keyType_ : `optional [str]`\n\n        > default `'citation'`, the type of key to use for the dictionary, the valid strings are `'citation'`, `'journal'`, `'year'` or `'author'`. IF changed from `'citation'` all citations matching the requested option will be contracted and their counts added together.\n\n        # Returns\n\n        `dict[str, int or Citation : int]`\n\n        > A dictionary with keys as given by _keyType_ and integers giving their rates of occurrence in the collection\n        \"\"\"\n        count = 0\n        recCount = len(self)\n        progArgs = (0, \"Starting to get the local stats on {}s.\".format(keyType))\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            keyTypesLst = [\"citation\", \"journal\", \"year\", \"author\"]\n            citesDict = {}\n            if keyType not in keyTypesLst:\n                raise TypeError(\"{} is not a valid key type, only '{}' or '{}' are.\".format(keyType, \"', '\".join(keyTypesLst[:-1]), keyTypesLst[-1]))\n            for R in self:\n                rCites = R.get('citations')\n                if PBar:\n                    count += 1\n                    PBar.updateVal(count / recCount, \"Analysing: {}\".format(R.UT))\n                if rCites:\n                    for c in rCites:\n                        if keyType == keyTypesLst[0]:\n                            cVal = c\n                        else:\n                            cVal = getattr(c, keyType)\n                            if cVal is None:\n                                continue\n                        if cVal in citesDict:\n                            citesDict[cVal] += 1\n                        else:\n                            citesDict[cVal] = 1\n            if PBar:\n                PBar.finish(\"Done, {} {} fields analysed\".format(len(citesDict), keyType))\n        if pandasFriendly:\n            citeLst = []\n            countLst = []\n            for cite, occ in citesDict.items():\n                citeLst.append(cite)\n                countLst.append(occ)\n            return {\"Citations\" : citeLst, \"Counts\" : countLst}\n        else:\n            return citesDict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking in a Record WOS string citation string or Citation and returns a RecordCollection of all records that cite it.", "response": "def localCitesOf(self, rec):\n        \"\"\"Takes in a Record, WOS string, citation string or Citation and returns a RecordCollection of all records that cite it.\n\n        # Parameters\n\n        _rec_ : `Record, str or Citation`\n\n        > The object that is being cited\n\n        # Returns\n\n        `RecordCollection`\n\n        > A `RecordCollection` containing only those `Records` that cite _rec_\n        \"\"\"\n        localCites = []\n        if isinstance(rec, Record):\n            recCite = rec.createCitation()\n        if isinstance(rec, str):\n            try:\n                recCite = self.getID(rec)\n            except ValueError:\n                try:\n                    recCite = Citation(rec)\n                except AttributeError:\n                    raise ValueError(\"{} is not a valid WOS string or a valid citation string\".format(recCite))\n            else:\n                if recCite is None:\n                    return RecordCollection(inCollection = localCites, name = \"Records_citing_{}\".format(rec), quietStart = True)\n                else:\n                    recCite = recCite.createCitation()\n        elif isinstance(rec, Citation):\n            recCite = rec\n        else:\n            raise ValueError(\"{} is not a valid input, rec must be a Record, string or Citation object.\".format(rec))\n        for R in self:\n            rCites = R.get('citations')\n            if rCites:\n                for cite in rCites:\n                    if recCite == cite:\n                        localCites.append(R)\n                        break\n        return RecordCollection(inCollection = localCites, name = \"Records_citing_'{}'\".format(rec), quietStart = True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfiltering the records by some string _keyString_ in their citations and returns all Records with at least one citation possessing the keyString in the field given by _field_.", "response": "def citeFilter(self, keyString = '', field = 'all', reverse = False, caseSensitive = False):\n        \"\"\"Filters `Records` by some string, _keyString_, in their citations and returns all `Records` with at least one citation possessing _keyString_ in the field given by _field_.\n\n        # Parameters\n\n        _keyString_ : `optional [str]`\n\n        > Default `''`, gives the string to be searched for, if it is is blank then all citations with the specified field will be matched\n\n        _field_ : `optional [str]`\n\n        > Default `'all'`, gives the component of the citation to be looked at, it can be one of a few strings. The default is `'all'` which will cause the entire original `Citation` to be searched. It can be used to search across fields, e.g. `'1970, V2'` is a valid keystring\n        The other options are:\n\n        + `'author'`, searches the author field\n        + `'year'`, searches the year field\n        + `'journal'`, searches the journal field\n        + `'V'`, searches the volume field\n        + `'P'`, searches the page field\n        + `'misc'`, searches all the remaining uncategorized information\n        + `'anonymous'`, searches for anonymous `Citations`, _keyString_ is not ignored\n        + `'bad'`, searches for bad citations, keyString is not used\n\n        _reverse_ : `optional [bool]`\n\n        > Default `False`, being set to `True` causes all `Records` not matching the query to be returned\n\n        _caseSensitive_ : `optional [bool]`\n\n        > Default `False`, if `True` causes the search across the original to be case sensitive, **only** the `'all'` option can be case sensitive\n        \"\"\"\n        retRecs = []\n        keyString = str(keyString)\n        for R in self:\n            try:\n                if field == 'all':\n                    for cite in R.get('citations'):\n                        if caseSensitive:\n                            if keyString in cite.original:\n                                retRecs.append(R)\n                                break\n                        else:\n                            if keyString.upper() in cite.original.upper():\n                                retRecs.append(R)\n                                break\n                elif field == 'author':\n                    for cite in R.get('citations'):\n                        try:\n                            if keyString.upper() in cite.author.upper():\n                                retRecs.append(R)\n                                break\n                        except AttributeError:\n                            pass\n                elif field == 'journal':\n                    for cite in R.get('citations'):\n                        try:\n                            if keyString.upper() in cite.journal:\n                                retRecs.append(R)\n                                break\n                        except AttributeError:\n                            pass\n                elif field == 'year':\n                    for cite in R.get('citations'):\n                        try:\n                            if int(keyString) == cite.year:\n                                retRecs.append(R)\n                                break\n                        except AttributeError:\n                            pass\n                elif field == 'V':\n                    for cite in R.get('citations'):\n                        try:\n                            if keyString.upper() in cite.V:\n                                retRecs.append(R)\n                                break\n                        except AttributeError:\n                            pass\n                elif field == 'P':\n                    for cite in R.get('citations'):\n                        try:\n                            if keyString.upper() in cite.P:\n                                retRecs.append(R)\n                                break\n                        except AttributeError:\n                            pass\n                elif field == 'misc':\n                    for cite in R.get('citations'):\n                        try:\n                            if keyString.upper() in cite.misc:\n                                retRecs.append(R)\n                                break\n                        except AttributeError:\n                            pass\n                elif field == 'anonymous':\n                    for cite in R.get('citations'):\n                        if cite.isAnonymous():\n                            retRecs.append(R)\n                            break\n                elif field == 'bad':\n                    for cite in R.get('citations'):\n                        if cite.bad:\n                            retRecs.append(R)\n                            break\n            except TypeError:\n                pass\n        if reverse:\n            excluded = []\n            for R in self:\n                if R not in retRecs:\n                    excluded.append(R)\n            return RecordCollection(inCollection = excluded, name = self.name, quietStart = True)\n        else:\n            return RecordCollection(inCollection = retRecs, name = self.name, quietStart = True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filterNonJournals(citesLst, invert = False):\n\n    retCites = []\n    for c in citesLst:\n        if c.isJournal():\n            if not invert:\n                retCites.append(c)\n        elif invert:\n            retCites.append(c)\n    return retCites", "response": "Removes the Citations from _citesLst_ that are not journals\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string containing the normalized values from the Citation excluding the DOI number.", "response": "def allButDOI(self):\n        \"\"\"\n        Returns a string of the normalized values from the Citation excluding the DOI number. Equivalent to getting the ID with [ID()](#metaknowledge.citation.Citation.ID) then appending the extra values from [Extra()](#metaknowledge.citation.Citation.Extra) and then removing the substring containing the DOI number.\n\n        # Returns\n\n        `str`\n\n        > A string containing the data of the Citation.\n        \"\"\"\n        extraTags = ['extraAuthors', 'V', 'issue', 'P', 'misc']\n        s = self.ID()\n        extras = []\n        for tag in extraTags:\n            if getattr(self, tag, False):\n                extras.append(str(getattr(self, tag)))\n        if len(extras) > 0:\n            return \"{0}, {1}\".format(s, ', '.join(extras))\n        else:\n            return s"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn any extra data that is not in the ID of the Citation.", "response": "def Extra(self):\n        \"\"\"\n        Returns any `V`, `P`, `DOI` or `misc` values as a string. These are all the values not returned by [ID()](#metaknowledge.citation.Citation.ID), they are separated by `' ,'`.\n\n        # Returns\n\n        `str`\n\n        > A string containing the data not in the ID of the `Citation`.\n        \"\"\"\n        extraTags = ['V', 'P', 'DOI', 'misc']\n        retVal = \"\"\n        for tag in extraTags:\n            if getattr(self, tag):\n                retVal += getattr(self, tag) + ', '\n        if len(retVal) > 2:\n            return retVal[:-2]\n        else:\n            return retVal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the citation s journal field is a journal abbreviation from the WOS listing found at [ http://images. webofknowledge. com / WOS / A_abrvjt. html ]", "response": "def isJournal(self, dbname = abrevDBname, manualDB = manualDBname, returnDict ='both', checkIfExcluded = False):\n        \"\"\"Returns `True` if the `Citation`'s `journal` field is a journal abbreviation from the WOS listing found at [http://images.webofknowledge.com/WOK46/help/WOS/A_abrvjt.html](http://images.webofknowledge.com/WOK46/help/WOS/A_abrvjt.html), i.e. checks if the citation is citing a journal.\n\n        **Note**: Requires the [j9Abbreviations](../modules/journalAbbreviations.html#metaknowledge.journalAbbreviations.backend.getj9dict) database file and will raise an error if it cannot be found.\n\n        **Note**: All parameters are used for getting the data base with [getj9dict](../modules/journalAbbreviations.html#metaknowledge.journalAbbreviations.backend.getj9dict).\n\n        # Parameters\n\n        _dbname_ : `optional [str]`\n\n        > The name of the downloaded database file, the default is determined at run time. It is recommended that this remain untouched.\n\n        _manualDB_ : `optional [str]`\n\n        > The name of the manually created database file, the default is determined at run time. It is recommended that this remain untouched.\n\n        _returnDict_ : `optional [str]`\n\n        > default `'both'`, can be used to get both databases or only one  with `'WOS'` or `'manual'`.\n\n        # Returns\n\n        `bool`\n\n        > `True` if the `Citation` is for a journal\n        \"\"\"\n        global abbrevDict\n        if abbrevDict is None:\n            abbrevDict = getj9dict(dbname = dbname, manualDB = manualDB, returnDict = returnDict)\n        if not hasattr(self, 'journal'):\n            return False\n        elif checkIfExcluded and self.journal:\n            try:\n                if abbrevDict.get(self.journal, [True])[0]:\n                    return False\n                else:\n                    return True\n            except IndexError:\n                return False\n        else:\n            if self.journal:\n                dictVal = abbrevDict.get(self.journal, [b''])[0]\n                if dictVal:\n                    return dictVal\n                else:\n                    return False\n            else:\n                return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef FullJournalName(self):\n        global abbrevDict\n        if abbrevDict is None:\n            abbrevDict = getj9dict()\n        if self.isJournal():\n            return abbrevDict[self.journal][0]\n        else:\n            return None", "response": "Returns the full name of the Citation s journal field."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef addToDB(self, manualName = None, manualDB = manualDBname, invert = False):\n        try:\n            if invert:\n                d = {self.journal : ''}\n            elif manualName is None:\n                d = {self.journal : self.journal}\n            else:\n                d = {self.journal : manualName}\n            addToDB(abbr = d, dbname = manualDB)\n        except KeyError:\n            raise KeyError(\"This citation does not have a journal field.\")\n        else:\n            abbrevDict.update(d)", "response": "Adds the journal of this Citation to the user created database of journals."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds _elem_ to the collection.", "response": "def add(self, elem):\n        \"\"\" Adds _elem_ to the collection.\n\n        # Parameters\n\n        _elem_ : `object`\n\n        > The object to be added\n        \"\"\"\n        if isinstance(elem, self._allowedTypes):\n            self._collection.add(elem)\n            self._collectedTypes.add(type(elem).__name__)\n        else:\n            raise CollectionTypeError(\"{} can only contain '{}', '{}' is not allowed.\".format(type(self).__name__, self._allowedTypes, elem))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(self, elem):\n        try:\n            return self._collection.remove(elem)\n        except KeyError:\n            raise KeyError(\"'{}' was not found in the {}: '{}'.\".format(elem, type(self).__name__, self)) from None", "response": "Removes the object from the collection. Will raise a KeyError if the object is not found."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear(self):\n        self.bad = False\n        self.errors = {}\n        self._collection.clear()", "response": "Removes all elements from the collection resets the error handling\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pop(self):\n        try:\n            return self._collection.pop()\n        except KeyError:\n            raise KeyError(\"Nothing left in the {}: '{}'.\".format(type(self).__name__, self)) from None", "response": "Removes a random element from the collection and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a shallow copy of the collection", "response": "def copy(self):\n        \"\"\"Creates a shallow copy of the collection\n\n        # Returns\n\n        `Collection`\n\n        > A copy of the `Collection`\n        \"\"\"\n        collectedCopy = copy.copy(self)\n        collectedCopy._collection = copy.copy(collectedCopy._collection)\n        self._collectedTypes = copy.copy(self._collectedTypes)\n        self._allowedTypes = copy.copy(self._allowedTypes)\n        collectedCopy.errors = copy.copy(collectedCopy.errors)\n        return collectedCopy"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits the Collection into _maxSize_ size or smaller Collections", "response": "def chunk(self, maxSize):\n        \"\"\"Splits the `Collection` into _maxSize_ size or smaller `Collections`\n\n        # Parameters\n\n        _maxSize_ : `int`\n\n        > The maximum number of elements in a retuned `Collection`\n\n\n        # Returns\n\n        `list [Collection]`\n\n        > A list of `Collections` that if all merged (`|` operator) would create the original\n        \"\"\"\n        chunks = []\n        currentSize = maxSize + 1\n        for i in self:\n            if currentSize >= maxSize:\n                currentSize = 0\n                chunks.append(type(self)({i}, name = 'Chunk-{}-of-{}'.format(len(chunks), self.name), quietStart = True))\n            else:\n                chunks[-1].add(i)\n            currentSize += 1\n        return chunks"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef containsID(self, idVal):\n        for i in self:\n            if i.id == idVal:\n                return True\n        return False", "response": "Checks if the collection contains the given idVal"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the collected items contains the give _idVal_ and discards it if it is found and raises an exception if it is not found.", "response": "def discardID(self, idVal):\n        \"\"\"Checks if the collected items contains the give _idVal_ and discards it if it is found, will not raise an exception if item is not found\n\n        # Parameters\n\n        _idVal_ : `str`\n\n        > The discarded id string\n        \"\"\"\n        for i in self:\n            if i.id == idVal:\n                self._collection.discard(i)\n                return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the collected items contains the give _idVal_ and removes it from the collection and raises a KeyError if it is not found.", "response": "def removeID(self, idVal):\n        \"\"\"Checks if the collected items contains the give _idVal_ and removes it if it is found, will raise a `KeyError` if item is not found\n\n        # Parameters\n\n        _idVal_ : `str`\n\n        > The removed id string\n        \"\"\"\n        for i in self:\n            if i.id == idVal:\n                self._collection.remove(i)\n                return\n        raise KeyError(\"A Record with the ID '{}' was not found in the RecordCollection: '{}'.\".format(idVal, self))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef badEntries(self):\n        badEntries = set()\n        for i in self:\n            if i.bad:\n                badEntries.add(i)\n        return type(self)(badEntries, quietStart = True)", "response": "Creates a new collection of the same type with only the bad entries\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving all the bad entries from the collection", "response": "def dropBadEntries(self):\n        \"\"\"Removes all the bad entries from the collection\n        \"\"\"\n        self._collection = set((i for i in self if not i.bad))\n        self.bad = False\n        self.errors = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a list of all the tags of the contained items", "response": "def tags(self):\n        \"\"\"Creates a list of all the tags of the contained items\n\n        # Returns\n\n        `list [str]`\n\n        > A list of all the tags\n        \"\"\"\n        tags = set()\n        for i in self:\n            tags |= set(i.keys())\n        return tags"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a printable table with the most frequently occurring values of each of the requested tags.", "response": "def glimpse(self, *tags, compact = False):\n        \"\"\"Creates a printable table with the most frequently occurring values of each of the requested _tags_, or if none are provided the top authors, journals and citations. The table will be as wide and as tall as the terminal (or 80x24 if there is no terminal) so `print(RC.glimpse())`should always create a nice looking table. Below is a table created from some of the testing files:\n\n        ```\n        >>> print(RC.glimpse())\n        +RecordCollection glimpse made at: 2016-01-01 12:00:00++++++++++++++++++++++++++\n        |33 Records from testFile++++++++++++++++++++++++++++++++++++++++++++++++++++++|\n        |Columns are ranked by num. of occurrences and are independent of one another++|\n        |-------Top Authors--------+------Top Journals-------+--------Top Cited--------|\n        |1                Girard, S|1 CANADIAN JOURNAL OF PH.|1 LEVY Y, 1975, OPT COMM.|\n        |1                Gilles, H|1 JOURNAL OF THE OPTICAL.|2 GOOS F, 1947, ANN PHYS.|\n        |2                IMBERT, C|2          APPLIED OPTICS|3 LOTSCH HKV, 1970, OPTI.|\n        |2                Pillon, F|2   OPTICS COMMUNICATIONS|4 RENARD RH, 1964, J OPT.|\n        |3          BEAUREGARD, OCD|2 NUOVO CIMENTO DELLA SO.|5 IMBERT C, 1972, PHYS R.|\n        |3               Laroche, M|2 JOURNAL OF THE OPTICAL.|6 ARTMANN K, 1948, ANN P.|\n        |3                 HUARD, S|2 JOURNAL OF THE OPTICAL.|6 COSTADEB.O, 1973, PHYS.|\n        |4                  PURI, A|2 NOUVELLE REVUE D OPTIQ.|6 ROOSEN G, 1973, CR ACA.|\n        |4               COSTADEB.O|3 PHYSICS REPORTS-REVIEW.|7 Imbert C., 1972, Nouve.|\n        |4           PATTANAYAK, DN|3 PHYSICAL REVIEW LETTERS|8 HOROWITZ BR, 1971, J O.|\n        |4           Gazibegovic, A|3 USPEKHI FIZICHESKIKH N.|8 BRETENAKER F, 1992, PH.|\n        |4                ROOSEN, G|3 APPLIED PHYSICS B-LASE.|8 SCHILLIN.H, 1965, ANN .|\n        |4               BIRMAN, JL|3 AEU-INTERNATIONAL JOUR.|8 FEDOROV FI, 1955, DOKL.|\n        |4                Kaiser, R|3 COMPTES RENDUS HEBDOMA.|8 MAZET A, 1971, CR ACAD.|\n        |5                  LEVY, Y|3 CHINESE PHYSICS LETTERS|9 IMBERT C, 1972, CR ACA.|\n        |5              BEAUREGA.OC|3       PHYSICAL REVIEW B|9 LOTSCH HKV, 1971, OPTI.|\n        |5               PAVLOV, VI|3 LETTERE AL NUOVO CIMEN.|9 ASHBY N, 1973, PHYS RE.|\n        |5                BREVIK, I|3 PROGRESS IN QUANTUM EL.|9 BOULWARE DG, 1973, PHY.|\n        >>>\n        ```\n\n        # Parameters\n\n        _tags_ : `str, str, ...`\n\n        > Any number of tag strings to be made into columns in the output table\n\n        # Returns\n\n        `str`\n\n        > A string containing the table\n        \"\"\"\n        return _glimpse(self, *tags, compact = compact)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rankedSeries(self, tag, outputFile = None, giveCounts = True, giveRanks = False, greatestFirst = True, pandasMode = True, limitTo = None):\n        if giveRanks and giveCounts:\n            raise mkException(\"rankedSeries cannot return counts and ranks only one of giveRanks or giveCounts can be True.\")\n        seriesDict = {}\n        for R in self:\n            #This should be faster than using get, since get is a wrapper for __getitem__\n            try:\n                val = R[tag]\n            except KeyError:\n                continue\n            if not isinstance(val, list):\n                val = [val]\n            for entry in val:\n                if limitTo and entry not in limitTo:\n                    continue\n                if entry in seriesDict:\n                    seriesDict[entry] += 1\n                else:\n                    seriesDict[entry] = 1\n        seriesList = sorted(seriesDict.items(), key = lambda x: x[1], reverse = greatestFirst)\n        if outputFile is not None:\n            with open(outputFile, 'w') as f:\n                writer = csv.writer(f, dialect = 'excel')\n                writer.writerow((str(tag), 'count'))\n                writer.writerows(seriesList)\n        if giveCounts and not pandasMode:\n            return seriesList\n        elif giveRanks or pandasMode:\n            if not greatestFirst:\n                seriesList.reverse()\n            currentRank = 1\n            retList = []\n            panDict = {'entry' : [], 'count' : [], 'rank' : []}\n            try:\n                currentCount = seriesList[0][1]\n            except IndexError:\n                #Empty series so no need to loop\n                pass\n            else:\n                for valString, count in seriesList:\n                    if currentCount > count:\n                        currentRank += 1\n                        currentCount = count\n                    if pandasMode:\n                        panDict['entry'].append(valString)\n                        panDict['count'].append(count)\n                        panDict['rank'].append(currentRank)\n                    else:\n                        retList.append((valString, currentRank))\n            if not greatestFirst:\n                retList.reverse()\n            if pandasMode:\n                return panDict\n            else:\n                return retList\n        else:\n            return [e for e,c in seriesList]", "response": "Returns a pandas dict of all the values of the tag with and ranked by their number of occurrences."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a pandas dict of all the time series values for the specified tag and output file.", "response": "def timeSeries(self, tag = None, outputFile = None, giveYears = True, greatestFirst = True, limitTo = False, pandasMode = True):\n        \"\"\"Creates an pandas dict of the ordered list of all the values of _tag_, with and ranked by the year the occurred in, multiple year occurrences will create multiple entries. A list can also be returned with the the counts or years added or it can be written to a file.\n\n        If no _tag_ is given the `Records` in the collection will be used\n\n        # Parameters\n\n        _tag_ : `optional str`\n\n        > Default `None`, if provided the tag will be ordered\n\n        _outputFile_ : `optional str`\n\n        > A file path to write a csv with 2 columns, one the tag values the other their years\n\n        _giveYears_ : `optional bool`\n\n        > Default `True`, if `True` the retuned list will be composed of tuples the first values being the tag value and the second their years.\n\n        _greatestFirst_ : `optional bool`\n\n        > Default `True`, if `True` the returned list will be ordered with the highest years first, otherwise the lowest years will be first.\n\n        _pandasMode_ : `optional bool`\n\n        > Default `True`, if `True` a `dict` ready for pandas will be returned, otherwise a list\n\n        _limitTo_ : `optional list[values]`\n\n        > Default `None`, if a list is provided only those values in the list will be counted or returned\n\n        # Returns\n\n        `dict[str:list[value]] or list[str]`\n\n        > A `dict` or `list` will be returned depending on if _pandasMode_ is `True`\n        \"\"\"\n        seriesDict = {}\n        for R in self:\n            #This should be faster than using get, since get is a wrapper for __getitem__\n            try:\n                year = R['year']\n            except KeyError:\n                continue\n            if tag is None:\n                seriesDict[R] = {year : 1}\n            else:\n                try:\n                    val = R[tag]\n                except KeyError:\n                    continue\n                if not isinstance(val, list):\n                    val = [val]\n                for entry in val:\n                    if limitTo and entry not in limitTo:\n                        continue\n                    if entry in seriesDict:\n                        try:\n                            seriesDict[entry][year] += 1\n                        except KeyError:\n                            seriesDict[entry][year] = 1\n                    else:\n                        seriesDict[entry] = {year : 1}\n        seriesList = []\n        for e, yd in seriesDict.items():\n            seriesList += [(e, y) for y in yd.keys()]\n        seriesList = sorted(seriesList, key = lambda x: x[1], reverse = greatestFirst)\n        if outputFile is not None:\n            with open(outputFile, 'w') as f:\n                writer = csv.writer(f, dialect = 'excel')\n                writer.writerow((str(tag), 'years'))\n                writer.writerows(((k,'|'.join((str(y) for y in v))) for k,v in seriesDict.items()))\n        if pandasMode:\n            panDict = {'entry' : [], 'count' : [], 'year' : []}\n            for entry, year in seriesList:\n                panDict['entry'].append(entry)\n                panDict['year'].append(year)\n                panDict['count'].append(seriesDict[entry][year])\n            return panDict\n        elif giveYears:\n            return seriesList\n        else:\n            return [e for e,c in seriesList]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncounting the number of times values from any of the _countedTags_ occurs with _keyTag_. The counts are retuned as a dictionary with the keys of the _countedTags_ values mapping to thier counts.", "response": "def cooccurrenceCounts(self, keyTag, *countedTags):\n        \"\"\"Counts the number of times values from any of the _countedTags_ occurs with _keyTag_. The counts are retuned as a dictionary with the values of _keyTag_ mapping to dictionaries with each of the _countedTags_ values mapping to thier counts.\n\n        # Parameters\n\n        _keyTag_ : `str`\n\n        > The tag used as the key for the returned dictionary\n\n        _*countedTags_ : `str, str, str, ...`\n\n        > The tags used as the key for the returned dictionary's values\n\n        # Returns\n\n        `dict[str:dict[str:int]]`\n\n        > The dictionary of counts\n        \"\"\"\n        if not isinstance(keyTag, str):\n            raise TagError(\"'{}' is not a string it cannot be used as a tag.\".format(keyTag))\n        if len(countedTags) < 1:\n            TagError(\"You need to provide atleast one tag\")\n        for tag in countedTags:\n            if not isinstance(tag, str):\n                raise TagError(\"'{}' is not a string it cannot be used as a tag.\".format(tag))\n        occurenceDict = {}\n        progArgs = (0, \"Starting to count the co-occurrences of '{}' and' {}'\".format(keyTag, \"','\".join(countedTags)))\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n\n            for i, R in enumerate(self):\n                PBar.updateVal(i / len(self), \"Analyzing {}\".format(R))\n                keyVal = R.get(keyTag)\n                if keyVal is None:\n                    continue\n                if not isinstance(keyVal, list):\n                    keyVal = [keyVal]\n                for key in keyVal:\n                    if key not in occurenceDict:\n                        occurenceDict[key] = {}\n                for tag in countedTags:\n                    tagval = R.get(tag)\n                    if tagval is None:\n                        continue\n                    if not isinstance(tagval, list):\n                        tagval = [tagval]\n                    for val in tagval:\n                        for key in keyVal:\n                            try:\n                                occurenceDict[key][val] += 1\n                            except KeyError:\n                                occurenceDict[key][val] = 1\n            PBar.finish(\"Done extracting the co-occurrences of '{}' and '{}'\".format(keyTag, \"','\".join(countedTags)))\n        return occurenceDict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a network of the objects found by the number of modes.", "response": "def networkMultiLevel(self, *modes, nodeCount = True, edgeWeight = True, stemmer = None, edgeAttribute = None, nodeAttribute = None, _networkTypeString = 'n-level network'):\n        \"\"\"Creates a network of the objects found by any number of tags _modes_, with edges between all co-occurring values. IF you only want edges between co-occurring values from different tags use [networkMultiMode()](#metaknowledge.CollectionWithIDs.networkMultiMode).\n\n        A **networkMultiLevel**() looks are each entry in the collection and extracts its values for the tag given by each of the _modes_, e.g. the `'authorsFull'` tag. Then if multiple are returned an edge is created between them. So in the case of the author tag `'authorsFull'` a co-authorship network is created. Then for each other tag the entries are also added and edges between the first tag's node and theirs are created.\n\n        The number of times each object occurs is count if _nodeCount_ is `True` and the edges count the number of co-occurrences if _edgeWeight_ is `True`. Both are`True` by default.\n\n        **Note** Do not use this for the construction of co-citation networks use [Recordcollection.networkCoCitation()](./classes/RecordCollection.html#metaknowledge.RecordCollection.networkCoCitation) it is more accurate and has more options.\n\n        # Parameters\n\n        _mode_ : `str`\n\n        > A two character WOS tag or one of the full names for a tag\n\n        _nodeCount_ : `optional [bool]`\n\n        > Default `True`, if `True` each node will have an attribute called \"count\" that contains an int giving the number of time the object occurred.\n\n        _edgeWeight_ : `optional [bool]`\n\n        > Default `True`, if `True` each edge will have an attribute called \"weight\" that contains an int giving the number of time the two objects co-occurrenced.\n\n        _stemmer_ : `optional [func]`\n\n        > Default `None`, If _stemmer_ is a callable object, basically a function or possibly a class, it will be called for the ID of every node in the graph, all IDs are strings. For example:\n\n        > The function ` f = lambda x: x[0]` if given as the stemmer will cause all IDs to be the first character of their unstemmed IDs. e.g. the title `'Goos-Hanchen and Imbert-Fedorov shifts for leaky guided modes'` will create the node `'G'`.\n\n        # Returns\n\n        `networkx Graph`\n\n        > A networkx Graph with the objects of the tag _mode_ as nodes and their co-occurrences as edges\n        \"\"\"\n        stemCheck = False\n        if stemmer is not None:\n            if isinstance(stemmer, collections.abc.Callable):\n                stemCheck = True\n            else:\n                raise TagError(\"stemmer must be callable, e.g. a function or class with a __call__ method.\")\n        count = 0\n        progArgs = (0, \"Starting to make a {} from {}\".format(_networkTypeString, modes))\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            if edgeAttribute is not None:\n                grph = nx.MultiGraph()\n            else:\n                grph = nx.Graph()\n            for R in self:\n                if PBar:\n                    count += 1\n                    PBar.updateVal(count / len(self), \"Analyzing: \" + str(R))\n                if edgeAttribute:\n                    edgeVals = [str(v) for v in R.get(edgeAttribute, [])]\n                if nodeAttribute:\n                    nodeVals = [str(v) for v in R.get(nodeAttribute, [])]\n                contents = []\n                for attr in modes:\n                    tmpContents = R.get(attr, [])\n                    if isinstance(tmpContents, list):\n                        contents += tmpContents\n                    else:\n                        contents.append(tmpContents)\n                if contents is not None:\n                    if not isinstance(contents, str) and isinstance(contents, collections.abc.Iterable):\n                        if stemCheck:\n                            tmplst = [stemmer(str(n)) for n in contents]\n                        else:\n                            tmplst = [str(n) for n in contents]\n                        if len(tmplst) > 1:\n                            for i, node1 in enumerate(tmplst):\n                                for node2 in tmplst[i + 1:]:\n                                    if edgeAttribute:\n                                        for edgeVal in edgeVals:\n                                            if grph.has_edge(node1, node2, key = edgeVal):\n                                                if edgeWeight:\n                                                    for i, a in grph.edges[node1, node2].items():\n                                                        if a['key'] == edgeVal:\n                                                            grph[node1][node2][i]['weight'] += 1\n                                                            break\n                                            else:\n                                                if edgeWeight:\n                                                    attrDict = {'key' : edgeVal, 'weight' : 1}\n                                                else:\n                                                    attrDict = {'key' : edgeVal}\n                                                grph.add_edge(node1, node2, **attrDict)\n                                    elif edgeWeight:\n                                        try:\n                                            grph.edges[node1, node2]['weight'] += 1\n                                        except KeyError:\n                                            grph.add_edge(node1, node2, weight = 1)\n                                    else:\n                                        if not grph.has_edge(node1, node2):\n                                            grph.add_edge(node1, node2)\n                                if not grph.has_node(node1):\n                                    grph.add_node(node1)\n                                if nodeCount:\n                                    try:\n                                        grph.node[node1]['count'] += 1\n                                    except KeyError:\n                                        grph.node[node1]['count'] = 1\n                                if nodeAttribute:\n                                    try:\n                                        currentAttrib = grph.node[node1][nodeAttribute]\n                                    except KeyError:\n                                        grph.node[node1][nodeAttribute] = nodeVals\n                                    else:\n                                        for nodeValue in (n for n in nodeVals if n not in currentAttrib):\n                                            grph.node[node1][nodeAttribute].append(nodeValue)\n                        elif len(tmplst) == 1:\n                            if nodeCount:\n                                try:\n                                    grph.node[tmplst[0]]['count'] += 1\n                                except KeyError:\n                                    grph.add_node(tmplst[0], count = 1)\n                            else:\n                                if not grph.has_node(tmplst[0]):\n                                    grph.add_node(tmplst[0])\n                            if nodeAttribute:\n                                try:\n                                    currentAttrib = grph.node[tmplst[0]][nodeAttribute]\n                                except KeyError:\n                                    grph.node[tmplst[0]][nodeAttribute] = nodeVals\n                                else:\n                                    for nodeValue in (n for n in nodeVals if n not in currentAttrib):\n                                        grph.node[tmplst[0]][nodeAttribute].append(nodeValue)\n                        else:\n                            pass\n                    else:\n                        if stemCheck:\n                            nodeVal = stemmer(str(contents))\n                        else:\n                            nodeVal = str(contents)\n                        if nodeCount:\n                            try:\n                                grph.node[nodeVal]['count'] += 1\n                            except KeyError:\n                                grph.add_node(nodeVal, count = 1)\n                        else:\n                            if not grph.has_node(nodeVal):\n                                grph.add_node(nodeVal)\n                        if nodeAttribute:\n                            try:\n                                currentAttrib = grph.node[nodeVal][nodeAttribute]\n                            except KeyError:\n                                grph.node[nodeVal][nodeAttribute] = nodeVals\n                            else:\n                                for nodeValue in (n for n in nodeVals if n not in currentAttrib):\n                                    grph.node[nodeVal][nodeAttribute].append(nodeValue)\n            if PBar:\n                PBar.finish(\"Done making a {} from {}\".format(_networkTypeString, modes))\n        return grph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef networkOneMode(self, mode, nodeCount = True, edgeWeight = True, stemmer = None, edgeAttribute = None, nodeAttribute = None):\n        return self.networkMultiLevel(mode, nodeCount = nodeCount, edgeWeight = edgeWeight, stemmer = stemmer, edgeAttribute = edgeAttribute, nodeAttribute = nodeAttribute, _networkTypeString = 'one mode network')", "response": "Creates a network of the objects found by one tag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a two - mode network of the objects found by two WOS tags.", "response": "def networkTwoMode(self, tag1, tag2, directed = False, recordType = True, nodeCount = True, edgeWeight = True, stemmerTag1 = None, stemmerTag2 = None, edgeAttribute = None):\n        \"\"\"Creates a network of the objects found by two WOS tags _tag1_ and _tag2_, each node marked by which tag spawned it making the resultant graph bipartite.\n\n        A **networkTwoMode()** looks at each Record in the `RecordCollection` and extracts its values for the tags given by _tag1_ and _tag2_, e.g. the `'WC'` and `'LA'` tags. Then for each object returned by each tag and edge is created between it and every other object of the other tag. So the WOS defined subject tag `'WC'` and language tag `'LA'`, will give a two-mode network showing the connections between subjects and languages. Each node will have an attribute call `'type'` that gives the tag that created it or both if both created it, e.g. the node `'English'` would have the type attribute be `'LA'`.\n\n        The number of times each object occurs is count if _nodeCount_ is `True` and the edges count the number of co-occurrences if _edgeWeight_ is `True`. Both are`True` by default.\n\n        The _directed_ parameter if `True` will cause the network to be directed with the first tag as the source and the second as the destination.\n\n        # Parameters\n\n        _tag1_ : `str`\n\n        > A two character WOS tag or one of the full names for a tag, the source of edges on the graph\n\n        _tag1_ : `str`\n\n        > A two character WOS tag or one of the full names for a tag, the target of edges on the graph\n\n        _directed_ : `optional [bool]`\n\n        > Default `False`, if `True` the returned network is directed\n\n        _nodeCount_ : `optional [bool]`\n\n        > Default `True`, if `True` each node will have an attribute called \"count\" that contains an int giving the number of time the object occurred.\n\n        _edgeWeight_ : `optional [bool]`\n\n        > Default `True`, if `True` each edge will have an attribute called \"weight\" that contains an int giving the number of time the two objects co-occurrenced.\n\n        _stemmerTag1_ : `optional [func]`\n\n        > Default `None`, If _stemmerTag1_ is a callable object, basically a function or possibly a class, it will be called for the ID of every node given by _tag1_ in the graph, all IDs are strings.\n\n        > For example: the function `f = lambda x: x[0]` if given as the stemmer will cause all IDs to be the first character of their unstemmed IDs. e.g. the title `'Goos-Hanchen and Imbert-Fedorov shifts for leaky guided modes'` will create the node `'G'`.\n\n        _stemmerTag2_ : `optional [func]`\n\n        > Default `None`, see _stemmerTag1_ as it is the same but for _tag2_\n\n        # Returns\n\n        `networkx Graph or networkx DiGraph`\n\n        > A networkx Graph with the objects of the tags _tag1_ and _tag2_ as nodes and their co-occurrences as edges.\n        \"\"\"\n        if not isinstance(tag1, str):\n            raise TagError(\"{} is not a string it cannot be a tag.\".format(tag1))\n        if not isinstance(tag2, str):\n            raise TagError(\"{} is not a string it cannot be a tag.\".format(tag2))\n        if stemmerTag1 is not None:\n            if isinstance(stemmerTag1, collections.abc.Callable):\n                stemCheck = True\n            else:\n                raise TagError(\"stemmerTag1 must be callable, e.g. a function or class with a __call__ method.\")\n        else:\n            stemmerTag1 = lambda x: x\n        if stemmerTag2 is not None:\n            if isinstance(stemmerTag2, collections.abc.Callable):\n                stemCheck = True\n            else:\n                raise TagError(\"stemmerTag2 must be callable, e.g. a function or class with a __call__ method.\")\n        else:\n            stemmerTag2 = lambda x: x\n        count = 0\n        progArgs = (0, \"Starting to make a two mode network of \" + tag1 + \" and \" + tag2)\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            if edgeAttribute is not None:\n                if directed:\n                    grph = nx.MultiDiGraph()\n                else:\n                    grph = nx.MultiGraph()\n            else:\n                if directed:\n                    grph = nx.DiGraph()\n                else:\n                    grph = nx.Graph()\n            for R in self:\n                if PBar:\n                    count += 1\n                    PBar.updateVal(count / len(self), \"Analyzing: {}\".format(R))\n                if edgeAttribute is not None:\n                    edgeVals = R.get(edgeAttribute, [])\n                    if not isinstance(edgeVals, list):\n                        edgeVals = [edgeVals]\n                contents1 = R.get(tag1)\n                contents2 = R.get(tag2)\n                if isinstance(contents1, list):\n                    contents1 = [stemmerTag1(str(v)) for v in contents1]\n                elif contents1 == None:\n                    contents1 = []\n                else:\n                    contents1 = [stemmerTag1(str(contents1))]\n                if isinstance(contents2, list):\n                    contents2 = [stemmerTag2(str(v)) for v in contents2]\n                elif contents2 == None:\n                    contents2 = []\n                else:\n                    contents2 = [stemmerTag2(str(contents2))]\n                for node1 in contents1:\n                    for node2 in contents2:\n                        if edgeAttribute:\n                            for edgeVal in edgeVals:\n                                if grph.has_edge(node1, node2, key = edgeVal):\n                                    if edgeWeight:\n                                        grph.edges[node1, node2, edgeVal]['weight'] += 1\n                                else:\n                                    if edgeWeight:\n                                        attrDict = {'key' : edgeVal, 'weight' : 1}\n                                    else:\n                                        attrDict = {'key' : edgeVal}\n                                    grph.add_edge(node1, node2, **attrDict)\n                        elif edgeWeight:\n                            try:\n                                grph.edges[node1, node2]['weight'] += 1\n                            except KeyError:\n                                grph.add_edge(node1, node2, weight = 1)\n                        else:\n                            if not grph.has_edge(node1, node2):\n                                grph.add_edge(node1, node2)\n                    if nodeCount:\n                        try:\n                            grph.node[node1]['count'] += 1\n                        except KeyError:\n                            try:\n                                grph.node[node1]['count'] = 1\n                                if recordType:\n                                    grph.node[node1]['type'] = tag1\n                            except KeyError:\n                                if recordType:\n                                    grph.add_node(node1, type = tag1)\n                                else:\n                                    grph.add_node(node1)\n                    else:\n                        if not grph.has_node(node1):\n                            if recordType:\n                                grph.add_node(node1, type = tag1)\n                            else:\n                                grph.add_node(node1)\n                        elif recordType:\n                            if 'type' not in grph.node[node1]:\n                                grph.node[node1]['type'] = tag1\n\n                for node2 in contents2:\n                    if nodeCount:\n                        try:\n                            grph.node[node2]['count'] += 1\n                        except KeyError:\n                            try:\n                                grph.node[node2]['count'] = 1\n                                if recordType:\n                                    grph.node[node2]['type'] = tag2\n                            except KeyError:\n                                grph.add_node(node2, count = 1)\n                                if recordType:\n                                    grph.node[node2]['type'] = tag2\n                    else:\n                        if not grph.has_node(node2):\n                            if recordType:\n                                grph.add_node(node2, type = tag2)\n                            else:\n                                grph.add_node(node2)\n                        elif recordType:\n                            if 'type' not in grph.node[node2]:\n                                grph.node[node2]['type'] = tag2\n            if PBar:\n                PBar.finish(\"Done making a two mode network of \" + tag1 + \" and \" + tag2)\n        return grph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef networkMultiMode(self, *tags, recordType = True, nodeCount = True, edgeWeight = True, stemmer = None, edgeAttribute = None):\n        if len(tags) == 1:\n            if not isinstance(tags[0], str):\n                try:\n                    tags = list(tags[0])\n                except TypeError:\n                    raise TagError(\"'{}' is not a string it cannot be a tag.\".format(tags[0]))\n        for t in (i for i in tags if not isinstance(i, str)):\n            raise TagError(\"{} is not a string it cannot be a tag.\".format(t))\n        stemCheck = False\n        if stemmer is not None:\n            if isinstance(stemmer, collections.abc.Callable):\n                stemCheck = True\n            else:\n                raise TagError(\"stemmer must be Callable, e.g. a function or class with a __call__ method.\")\n        count = 0\n        progArgs = (0, \"Starting to make a \" + str(len(tags)) + \"-mode network of: \" + ', '.join(tags))\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            if edgeAttribute is not None:\n                grph = nx.MultiGraph()\n            else:\n                grph = nx.Graph()\n            for R in self:\n                if PBar:\n                    count += 1\n                    PBar.updateVal(count / len(self), \"Analyzing: \" + str(R))\n                contents = []\n                for t in tags:\n                    tmpVal = R.get(t)\n                    if stemCheck:\n                        if tmpVal:\n                            if isinstance(tmpVal, list):\n                                contents.append((t, [stemmer(str(v)) for v in tmpVal]))\n                            else:\n                                contents.append((t, [stemmer(str(tmpVal))]))\n                    else:\n                        if tmpVal:\n                            if isinstance(tmpVal, list):\n                                contents.append((t, [str(v) for v in tmpVal]))\n                            else:\n                                contents.append((t, [str(tmpVal)]))\n                for i, vlst1 in enumerate(contents):\n                    for node1 in vlst1[1]:\n                        for vlst2 in contents[i + 1:]:\n                            for node2 in vlst2[1]:\n                                if edgeAttribute:\n                                    for edgeVal in edgeVals:\n                                        if grph.has_edge(node1, node2, key = edgeVal):\n                                            if edgeWeight:\n                                                for i, a in grph.edges[node1, node2].items():\n                                                    if a['key'] == edgeVal:\n                                                        grph[node1][node2][i]['weight'] += 1\n                                                        break\n                                        else:\n                                            if edgeWeight:\n                                                attrDict = {'key' : edgeVal, 'weight' : 1}\n                                            else:\n                                                attrDict = {'key' : edgeVal}\n                                            grph.add_edge(node1, node2, **attrDict)\n                                elif edgeWeight:\n                                    try:\n                                        grph.edges[node1, node2]['weight'] += 1\n                                    except KeyError:\n                                        grph.add_edge(node1, node2, weight = 1)\n                                else:\n                                    if not grph.has_edge(node1, node2):\n                                        grph.add_edge(node1, node2)\n                        if nodeCount:\n                            try:\n                                grph.node[node1]['count'] += 1\n                            except KeyError:\n                                try:\n                                    grph.node[node1]['count'] = 1\n                                    if recordType:\n                                        grph.node[node1]['type'] = vlst1[0]\n                                except KeyError:\n                                    if recordType:\n                                        grph.add_node(node1, type = vlst1[0])\n                                    else:\n                                        grph.add_node(node1)\n                        else:\n                            if not grph.has_node(node1):\n                                if recordType:\n                                    grph.add_node(node1, type = vlst1[0])\n                                else:\n                                    grph.add_node(node1)\n                            elif recordType:\n                                try:\n                                    grph.node[node1]['type'] += vlst1[0]\n                                except KeyError:\n                                    grph.node[node1]['type'] = vlst1[0]\n            if PBar:\n                PBar.finish(\"Done making a {}-mode network of: {}\".format(len(tags), ', '.join(tags)))\n        return grph", "response": "Creates a network of the objects found by the given tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef diffusionGraph(source, target, weighted = True, sourceType = \"raw\", targetType = \"raw\", labelEdgesBy = None):\n    if sourceType != \"raw\" and sourceType not in tagsAndNameSet:\n        raise RuntimeError(\"{} is not a valid node type, only 'raw' or those strings in tagsAndNameSet are allowed\".format(sourceType))\n    if targetType != \"raw\" and targetType not in tagsAndNameSet:\n        raise RuntimeError(\"{} is not a valid node type, only 'raw' or those strings in tagsAndNameSet are allowed\".format(targetType))\n    if labelEdgesBy is not None:\n        try:\n            normVal = normalizeToTag(labelEdgesBy)\n        except KeyError:\n            raise RuntimeError (\"{} is not a known tag, only tags in tagsAndNameSet are allowed.\".format(labelEdgesBy))\n        else:\n            labelEdgesBy = normVal\n    count = 0\n    maxCount = len(source)\n    progArgs = (0, \"Starting to make a diffusion network\")\n    if metaknowledge.VERBOSE_MODE:\n        progKwargs = {'dummy' : False}\n    else:\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        sourceDict = {}\n        if labelEdgesBy is None:\n            workingGraph = nx.DiGraph()\n        else:\n            workingGraph = nx.MultiDiGraph()\n        for Rs in source:\n            if PBar:\n                count += 1\n                PBar.updateVal(count / maxCount * .25, \"Analyzing source: \" + str(Rs))\n            RsVal, RsExtras = makeNodeID(Rs, sourceType)\n            if RsVal:\n                sourceDict[Rs.createCitation()] = RsVal\n                for val in RsVal:\n                    if val not in workingGraph:\n                        workingGraph.add_node(val, source = True, target = False, **RsExtras)\n        if PBar:\n            count = 0\n            maxCount = len(target)\n            PBar.updateVal(.25, \"Done analyzing sources, starting on targets\")\n        for Rt in target:\n            RtVal, RtExtras = makeNodeID(Rt, targetType)\n            if labelEdgesBy is not None:\n                edgeVals = Rt.get(labelEdgesBy)\n                if edgeVals is None:\n                    continue\n                if not isinstance(edgeVals, list):\n                    edgeVals = [edgeVals]\n            if PBar:\n                count += 1\n                PBar.updateVal(count / maxCount * .75 + .25, \"Analyzing target: \" + str(Rt))\n            if RtVal:\n                for val in RtVal:\n                    if val not in workingGraph:\n                        workingGraph.add_node(val, source = False, target = True, **RtExtras)\n                    else:\n                        workingGraph.node[val][\"target\"] = True\n                    targetCites = Rt.get('CR')\n                    if targetCites:\n                        for Rs in (sourceDict[c] for c in targetCites if c in sourceDict):\n                            for sVal in Rs:\n                                if labelEdgesBy is not None:\n                                    for edgeVal in (str(ev) for ev in edgeVals):\n                                        if weighted:\n                                            if workingGraph.has_edge(sVal, val, key = edgeVal):\n                                                for i, a in workingGraph[sVal][val].items():\n                                                    if a['key'] == edgeVal:\n                                                        workingGraph[sVal][val][i]['weight'] += 1\n                                                        break\n                                            else:\n                                                attrDict = {'key' : edgeVal, 'weight' : 1}\n                                                workingGraph.add_edge(sVal, val, attr_dict = attrDict)\n                                        else:\n                                            if not workingGraph.has_edge(sVal, val, key = edgeVal):\n                                                workingGraph.add_edge(sVal, val, key = edgeVal)\n                                else:\n                                    if weighted:\n                                        try:\n                                            workingGraph[sVal][val]['weight'] += 1\n                                        except KeyError:\n                                            workingGraph.add_edge(sVal, val, weight = 1)\n                                    else:\n                                        workingGraph.add_edge(sVal, val)\n        if PBar:\n            PBar.finish(\"Done making a diffusion network of {} sources and {} targets\".format(len(source), len(target)))\n    return workingGraph", "response": "Generates a diffusion graph from two Records."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary that contains the number of times each occurrence in the source and target record.", "response": "def diffusionCount(source, target, sourceType = \"raw\", extraValue = None, pandasFriendly = False,  compareCounts = False, numAuthors = True, useAllAuthors = True, _ProgBar = None, extraMapping = None):\n    \"\"\"Takes in two [RecordCollections](../classes/RecordCollection.html#metaknowledge.RecordCollection) and produces a `dict` counting the citations of _source_ by the [Records](../classes/Record.html#metaknowledge.Record) of _target_. By default the `dict` uses `Record` objects as keys but this can be changed with the _sourceType_ keyword to any of the WOS tags.\n\n    # Parameters\n\n    _source_ : `RecordCollection`\n\n    > A metaknowledge `RecordCollection` containing the `Records` being cited\n\n    _target_ : `RecordCollection`\n\n    > A metaknowledge `RecordCollection` containing the `Records` citing those in _source_\n\n    _sourceType_ : `optional [str]`\n\n    > default `'raw'`, if `'raw'` the returned `dict` will contain `Records` as keys. If it is a WOS tag the keys will be of that type.\n\n    _pandasFriendly_ : `optional [bool]`\n\n    > default `False`, makes the output be a dict with two keys one `\"Record\"` is the list of Records ( or data type requested by _sourceType_) the other is their occurrence counts as `\"Counts\"`. The lists are the same length.\n\n    _compareCounts_ : `optional [bool]`\n\n    > default `False`, if `True` the diffusion analysis will be run twice, first with source and target setup like the default (global scope) then using only the source `RecordCollection` (local scope).\n\n    _extraValue_ : `optional [str]`\n\n    > default `None`, if a tag the returned dictionary will have `Records` mapped to maps, these maps will map the entries for the tag to counts. If _pandasFriendly_ is also `True` the resultant dictionary will have an additional column called `'year'`. This column will contain the year the citations occurred, in addition the Records entries will be duplicated for each year they occur in.\n\n    > For example if `'year'` was given then the count for a single `Record` could be `{1990 : 1, 2000 : 5}`\n\n    _useAllAuthors_ : `optional [bool]`\n\n    > default `True`, if `False` only the first author will be used to generate the `Citations` for the _source_ `Records`\n\n    # Returns\n\n    `dict[:int]`\n\n    > A dictionary with the type given by _sourceType_ as keys and integers as values.\n\n    > If _compareCounts_ is `True` the values are tuples with the first integer being the diffusion in the target and the second the diffusion in the source.\n\n    > If _pandasFriendly_ is `True` the returned dict has keys with the names of the WOS tags and lists with their values, i.e. a table with labeled columns. The counts are in the column named `\"TargetCount\"` and if _compareCounts_ the local count is in a column called `\"SourceCount\"`.\n    \"\"\"\n    sourceCountString = \"SourceCount\"\n    targetCountString = \"TargetCount\"\n    if not isinstance(sourceType, str):\n        raise RuntimeError(\"{} is not a valid node type, only tags or the string 'raw' are allowed\".format(sourceType))\n    if not isinstance(source, RecordCollection) or not isinstance(target, RecordCollection):\n        raise RuntimeError(\"Source and target must be RecordCollections.\")\n    if extraValue is not None and not isinstance(extraValue, str):\n        raise RuntimeError(\"{} is not a valid extraValue, only tags are allowed\".format(extraValue))\n    if extraMapping is None:\n        extraMapping = lambda x : x\n    if metaknowledge.VERBOSE_MODE or _ProgBar:\n        if _ProgBar:\n            PBar = _ProgBar\n            PBar.updateVal(0, \"Starting to analyse a diffusion network\")\n        else:\n            PBar = _ProgressBar(0, \"Starting to analyse a diffusion network\")\n        count = 0\n        maxCount = len(source)\n    else:\n        PBar = _ProgressBar(\"Starting to analyse a diffusion network\", dummy = True)\n    count = 0\n    maxCount = len(source)\n    sourceDict = {}\n    #Tells the function if the IDs are made of lists or of str\n    listIds = None\n\n    for Rs in source:\n        if listIds is None and Rs.get(sourceType) is not None:\n            listIds = isinstance(Rs.get(sourceType), list)\n        count += 1\n        PBar.updateVal(count / maxCount * .10, \"Analyzing source: \" + str(Rs))\n        RsVal, RsExtras = makeNodeID(Rs, sourceType)\n        if RsVal:\n            if useAllAuthors:\n                for c in Rs.createCitation(multiCite = True):\n                    sourceDict[c] = RsVal\n            else:\n                sourceDict[Rs.createCitation()] = RsVal\n    if extraValue is not None:\n        if listIds:\n            sourceCounts = {s : {targetCountString : 0} for s in itertools.chain.from_iterable(sourceDict.values())}\n        else:\n            sourceCounts = {s : {targetCountString : 0} for s in sourceDict.values()}\n    else:\n        if listIds:\n            sourceCounts = {s : 0 for s in itertools.chain.from_iterable(sourceDict.values())}\n        else:\n            sourceCounts = {s : 0 for s in sourceDict.values()}\n    count = 0\n    maxCount = len(target)\n    PBar.updateVal(.10, \"Done analyzing sources, starting on targets\")\n    for Rt in target:\n        count += 1\n        PBar.updateVal(count / maxCount * .90 + .10, \"Analyzing target: {}\".format(Rt))\n        targetCites = Rt.get('citations', [])\n        if extraValue is not None:\n            values = Rt.get(extraValue, [])\n            if values is None:\n                values = []\n            elif not isinstance(values, list):\n                values = [values]\n            values = [extraMapping(val) for val in values]\n        for c in  targetCites:\n            try:\n                RsourceVals = sourceDict[c]\n            except KeyError:\n                continue\n            if listIds:\n                for sVal in RsourceVals:\n                    if extraValue:\n                        sourceCounts[sVal][targetCountString] += 1\n                        for val in values:\n                            try:\n                                sourceCounts[sVal][val] += 1\n                            except KeyError:\n                                sourceCounts[sVal][val] = 1\n                    else:\n                        sourceCounts[sVal] += 1\n            else:\n                if extraValue:\n                    sourceCounts[RsourceVals][targetCountString] += 1\n                    for val in values:\n                        try:\n                            sourceCounts[RsourceVals][val] += 1\n                        except KeyError:\n                            sourceCounts[RsourceVals][val] = 1\n                else:\n                    sourceCounts[RsourceVals] += 1\n    if compareCounts:\n        localCounts = diffusionCount(source, source, sourceType = sourceType, pandasFriendly = False,  compareCounts = False, extraValue = extraValue, _ProgBar = PBar)\n    if PBar and not _ProgBar:\n        PBar.finish(\"Done counting the diffusion of {} sources into {} targets\".format(len(source), len(target)))\n    if pandasFriendly:\n        retDict = {targetCountString : []}\n        if numAuthors:\n            retDict[\"numAuthors\"] = []\n        if compareCounts:\n            retDict[sourceCountString] = []\n        if extraValue is not None:\n            retDict[extraValue] = []\n        if sourceType == 'raw':\n            retrievedFields = []\n            targetCount = []\n            for R in sourceCounts.keys():\n                tagsLst = [t for t in R.keys() if t not in retrievedFields]\n                retrievedFields += tagsLst\n            for tag in retrievedFields:\n                retDict[tag] = []\n            for R, occ in sourceCounts.items():\n                if extraValue:\n                    Rvals = R.subDict(retrievedFields)\n                    for extraVal, occCount in occ.items():\n                        retDict[extraValue].append(extraVal)\n                        if numAuthors:\n                            retDict[\"numAuthors\"].append(len(R.get('authorsShort')))\n                        for tag in retrievedFields:\n                            retDict[tag].append(Rvals[tag])\n                        retDict[targetCountString].append(occCount)\n                        if compareCounts:\n                            try:\n                                retDict[sourceCountString].append(localCounts[R][extraVal])\n                            except KeyError:\n                                retDict[sourceCountString].append(0)\n                else:\n                    Rvals = R.subDict(retrievedFields)\n                    if numAuthors:\n                        retDict[\"numAuthors\"].append(len(R.get('authorsShort')))\n                    for tag in retrievedFields:\n                        retDict[tag].append(Rvals[tag])\n                    retDict[targetCountString].append(occ)\n                    if compareCounts:\n                        retDict[sourceCountString].append(localCounts[R])\n        else:\n            countLst = []\n            recLst = []\n            locLst = []\n            if extraValue:\n                extraValueLst = []\n            for R, occ in sourceCounts.items():\n                if extraValue:\n                    for extraVal, occCount in occ.items():\n                        countLst.append(occCount)\n                        recLst.append(R)\n                        extraValueLst.append(extraVal)\n                        if compareCounts:\n                            try:\n                                locLst.append(localCounts[R][extraValue])\n                            except KeyError:\n                                locLst.append(0)\n                else:\n                    countLst.append(occ)\n                    recLst.append(R)\n                    if compareCounts:\n                        locLst.append(localCounts[R])\n            if compareCounts:\n                retDict = {sourceType : recLst, targetCountString : countLst, sourceCountString : locLst}\n            else:\n                retDict = {sourceType : recLst, targetCountString : countLst}\n            if extraValue:\n                retDict[extraValue] = extraValueLst\n        return retDict\n    else:\n        if compareCounts:\n            for R, occ in localCounts.items():\n                sourceCounts[R] = (sourceCounts[R], occ)\n        return sourceCounts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makeNodeID(Rec, ndType, extras = None):\n    if ndType == 'raw':\n        recID = Rec\n    else:\n        recID = Rec.get(ndType)\n    if recID is None:\n        pass\n    elif isinstance(recID, list):\n        recID = tuple(recID)\n    else:\n        recID = recID\n    extraDict = {}\n    if extras:\n        for tag in extras:\n            if tag == \"raw\":\n                extraDict['Tag'] = Rec\n            else:\n                extraDict['Tag'] = Rec.get(tag)\n    return recID, extraDict", "response": "Helper to make a node ID from a Rec object and a list of extras"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new graph with counts from a source node.", "response": "def diffusionAddCountsFromSource(grph, source, target, nodeType = 'citations', extraType = None, diffusionLabel = 'DiffusionCount', extraKeys = None, countsDict = None, extraMapping = None):\n    \"\"\"Does a diffusion using [diffusionCount()](#metaknowledge.diffusion.diffusionCount) and updates _grph_ with it, using the nodes in the graph as keys in the diffusion, i.e. the source. The name of the attribute the counts are added to is given by _diffusionLabel_. If the graph is not composed of citations from the source and instead is another tag _nodeType_ needs to be given the tag string.\n\n    # Parameters\n\n    _grph_ : `networkx Graph`\n\n    > The graph to be updated\n\n    _source_ : `RecordCollection`\n\n    > The `RecordCollection` that created _grph_\n\n    _target_ : `RecordCollection`\n\n    > The `RecordCollection` that will be counted\n\n    _nodeType_ : `optional [str]`\n\n    > default `'citations'`, the tag that constants the values used to create _grph_\n\n    # Returns\n\n    `dict[:int]`\n\n    > The counts dictioanry used to add values to _grph_. *Note* _grph_ is modified by the function and the return is done in case you need it.\n    \"\"\"\n    progArgs = (0, \"Starting to add counts to graph\")\n    if metaknowledge.VERBOSE_MODE:\n        progKwargs = {'dummy' : False}\n    else:\n        progKwargs = {'dummy' : True}\n    with _ProgressBar(*progArgs, **progKwargs) as PBar:\n        PBar.updateVal(0, 'Getting counts')\n        if countsDict is None:\n            countsDict = diffusionCount(source, target, sourceType = nodeType, extraValue = extraType, _ProgBar = PBar, extraMapping = extraMapping)\n        try:\n            if not isinstance(countsDict.keys().__iter__().__next__(), str):\n                PBar.updateVal(.5, \"Prepping the counts\")\n                newCountsDict = {}\n                while True:\n                    try:\n                        k, v = countsDict.popitem()\n                    except KeyError:\n                        break\n                    newCountsDict[str(k)] = v\n                countsDict = newCountsDict\n        except StopIteration:\n            pass\n        count = 0\n        for n in grph.nodes_iter():\n            PBar.updateVal(.5 + .5 * (count / len(grph)), \"Adding count for '{}'\".format(n))\n            if extraType is not None:\n                if extraKeys:\n                    for key in extraKeys:\n                        grph.node[n][key] = 0\n                grph.node[n][diffusionLabel] = 0\n                try:\n                    for k, v in countsDict[n].items():\n                        if k == 'TargetCount':\n                            grph.node[n][diffusionLabel] = v\n                        else:\n                            if k:\n                                grph.node[n][k] = v\n                except KeyError:\n                    grph.node[n][diffusionLabel] = 0\n            else:\n                grph.node[n][diffusionLabel] = countsDict.get(n, 0)\n            count += 1\n        PBar.finish(\"Done adding diffusion counts to a graph\")\n    return countsDict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pandoc_process(app, what, name, obj, options, lines):\n\n    if not lines:\n        return None\n\n    input_format = app.config.mkdsupport_use_parser\n    output_format = 'rst'\n\n    # Since default encoding for sphinx.ext.autodoc is unicode and pypandoc.convert_text, which will always return a\n    # unicode string, expects unicode or utf-8 encodes string, there is on need for dealing with coding\n    text = SEP.join(lines)\n    text = pypandoc.convert_text(text, output_format, format=input_format)\n\n    # The 'lines' in Sphinx is a list of strings and the value should be changed\n    del lines[:]\n    lines.extend(text.split(SEP))", "response": "Convert docstrings in Markdown into reStructureText using pandoc\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef beginningPage(R):\n    p = R['PG']\n    if p.startswith('suppl '):\n        p = p[6:]\n    return p.split(' ')[0].split('-')[0].replace(';', '')", "response": "Returns the page number of the current page"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting a string list or number as a bibtex file by splitting it up into a list of strings and then expanding them into a single and then concatenating the values with the appropriate quoting characters.", "response": "def _bibFormatter(s, maxLength):\n    \"\"\"Formats a string, list or number to make it good for a bib file by:\n        * if too long splits up the string correctly\n        * tries to use the best quoting characters\n        * expands lists into ' and ' seperated values, as per spec for authors field\n    Note, this does not escape characters. LaTeX may have issues with the output\n    Max length splitting derived from https://www.cs.arizona.edu/~collberg/Teaching/07.231/BibTeX/bibtex.html\n    \"\"\"\n    if isinstance(s, list):\n        s = ' and '.join((str(v) for v in s))\n    elif not isinstance(s, str):\n        s = str(s)\n    if len(s) > maxLength:\n        s = s.replace('\"', '')\n        s = [s[i * maxLength: (i + 1) * maxLength] for i in range(len(s) // maxLength )]\n        s = '\"{}\"'.format('\" # \"'.join(s))\n    elif '\"' not in s:\n        s = '\"{}\"'.format(s)\n    else:\n        s = s.replace('{', '\\\\{').replace('}', '\\\\}')\n        s = '{{{}}}'.format(s)\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, tag, default = None, raw = False):\n        if raw:\n            if tag in self._fieldDict:\n                return self._fieldDict[tag]\n            elif self.getAltName(tag) in self._fieldDict:\n                return self._fieldDict[self.getAltName(tag)]\n            else:\n                return default\n        else:\n            try:\n                return self[tag]\n            except KeyError:\n                return default", "response": "Allows access to the raw values or is an Exception safe wrapper to __getitem__."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking values but with a raw option", "response": "def values(self, raw = False):\n        \"\"\"Like `values` for dicts but with a `raw` option\n\n        # Parameters\n\n        _raw_ : `optional [bool]`\n\n        > Default `False`, if `True` the `ValuesView` contains the raw values\n\n        # Returns\n\n        `ValuesView`\n\n        > The values of the record\n        \"\"\"\n        if raw:\n            return self._fieldDict.values()\n        else:\n            return collections.abc.Mapping.values(self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef items(self, raw = False):\n        if raw:\n            return self._fieldDict.items()\n        else:\n            return collections.abc.Mapping.items(self)", "response": "Like items but with a raw option"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getCitations(self, field = None, values = None, pandasFriendly = True):\n        retCites = []\n        if values is not None:\n            if isinstance(values, (str, int, float)) or not isinstance(values, collections.abc.Container):\n                values = [values]\n        if field is not None:\n            for cite in self.get('citations', []):\n                try:\n                    targetVal = getattr(cite, field)\n                    if values is None or targetVal in values:\n                        retCites.append(cite)\n                except AttributeError:\n                    pass\n        else:\n            retCites = self.get('citations', [])\n        if pandasFriendly:\n            return _pandasPrep(retCites, False)\n        return retCites", "response": "Returns a pandas ready dict with each row a different citation and columns containing the original string year journal and author s name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a dictionary of values of _tags_ from the Record. The keys are the keys and the values are the values of the record.", "response": "def subDict(self, tags, raw = False):\n        \"\"\"Creates a dict of values of _tags_ from the Record. The tags are the keys and the values are the values. If the tag is missing the value will be `None`.\n\n        # Parameters\n\n        _tags_ : `list[str]`\n\n        > The list of tags requested\n\n        _raw_ : `optional [bool]`\n\n        >default `False` if `True` the retuned values of the dict will be unprocessed\n\n        # Returns\n\n        `dict`\n\n        > A dictionary with the keys _tags_ and the values from the record\n        \"\"\"\n        retDict = {}\n        for tag in tags:\n            retDict[tag] = self.get(tag, raw = raw)\n        return retDict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef createCitation(self, multiCite = False):\n        #Need to put the import here to avoid circular import issues\n        from .citation import Citation\n        valsLst = []\n        if multiCite:\n            auths = []\n            for auth in self.get(\"authorsShort\", []):\n                auths.append(auth.replace(',', ''))\n        else:\n            if self.get(\"authorsShort\", False):\n                valsLst.append(self['authorsShort'][0].replace(',', ''))\n        if self.get(\"year\", False):\n            valsLst.append(str(self.get('year')))\n        if self.get(\"j9\", False):\n            valsLst.append(self.get('j9'))\n        elif self.get(\"title\", False):\n            #No j9 means its probably book so using the books title/leaving blank\n            valsLst.append(self.get('title', ''))\n        if self.get(\"volume\", False):\n            valsLst.append('V' + str(self.get('volume')))\n        if self.get(\"beginningPage\", False):\n            valsLst.append('P' + str(self.get('beginningPage')))\n        if self.get(\"DOI\", False):\n            valsLst.append('DOI ' + self.get('DOI'))\n        if multiCite and len(auths) > 0:\n            return(tuple((Citation(', '.join([a] + valsLst)) for a in auths)))\n        elif multiCite:\n            return Citation(', '.join(valsLst)),\n        else:\n            return Citation(', '.join(valsLst))", "response": "Creates a string containing the citation for this record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dict mapping of genders to author s names or counts.", "response": "def authGenders(self, countsOnly = False, fractionsMode = False, _countsTuple = False):\n        \"\"\"Creates a dict mapping `'Male'`, `'Female'` and `'Unknown'` to lists of the names of all the authors.\n\n        # Parameters\n\n        _countsOnly_ : `optional bool`\n\n        > Default `False`, if `True` the counts (lengths of the lists) will be given instead of the lists of names\n\n        _fractionsMode_ : `optional bool`\n\n        > Default `False`, if `True` the fraction counts (lengths of the lists divided by the total  number of authors) will be given instead of the lists of names. This supersedes _countsOnly_\n\n        # Returns\n\n        `dict[str:str or int]`\n\n        > The mapping of genders to author's names or counts\n        \"\"\"\n\n        authDict = recordGenders(self)\n        if _countsTuple or countsOnly or fractionsMode:\n            rawList = list(authDict.values())\n            countsList = []\n            for k in ('Male','Female','Unknown'):\n                countsList.append(rawList.count(k))\n            if fractionsMode:\n                tot = sum(countsList)\n                for i in range(3):\n                    countsList.append(countsList.pop(0) / tot)\n            if _countsTuple:\n                return tuple(countsList)\n            else:\n                return {'Male' : countsList[0], 'Female' : countsList[1], 'Unknown' : countsList[2]}\n        else:\n            return authDict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a string giving the Record as a bibTex entry.", "response": "def bibString(self, maxLength = 1000, WOSMode = False, restrictedOutput = False, niceID = True):\n        \"\"\"Makes a string giving the Record as a bibTex entry. If the Record is of a journal article (`PT J`) the bibtext type is set to `'article'`, otherwise it is set to `'misc'`. The ID of the entry is the WOS number and all the Record's fields are given as entries with their long names.\n\n        **Note** This is not meant to be used directly with LaTeX none of the special characters have been escaped and there are a large number of unnecessary fields provided. _niceID_ and _maxLength_ have been provided to make conversions easier.\n\n        **Note** Record entries that are lists have their values seperated with the string `' and '`\n\n        # Parameters\n\n        _maxLength_ : `optional [int]`\n\n        > default 1000, The max length for a continuous string. Most bibTex implementation only allow string to be up to 1000 characters ([source](https://www.cs.arizona.edu/~collberg/Teaching/07.231/BibTeX/bibtex.html)), this splits them up into substrings then uses the native string concatenation (the `'#'` character) to allow for longer strings\n\n        _WOSMode_ : `optional [bool]`\n\n        > default `False`, if `True` the data produced will be unprocessed and use double curly braces. This is the style WOS produces bib files in and mostly macthes that.\n\n        _restrictedOutput_ : `optional [bool]`\n\n        > default `False`, if `True` the tags output will be limited to tose found in `metaknowledge.commonRecordFields`\n\n        _niceID_ : `optional [bool]`\n\n        > default `True`, if `True` the ID used will be derived from the authors, publishing date and title, if `False` it will be the UT tag\n\n        # Returns\n\n        `str`\n\n        > The bibTex string of the Record\n        \"\"\"\n        keyEntries = []\n        if self.bad:\n            raise BadRecord(\"This record cannot be converted to a bibtex entry as the input was malformed.\\nThe original line number (if any) is: {} and the original file is: '{}'\".format(self._sourceLine, self._sourceFile))\n        if niceID:\n            if self.get('authorsFull'):\n                bibID = self['authorsFull'][0].title().replace(' ', '').replace(',', '').replace('.','')\n            else:\n                bibID = ''\n            if self.get('year', False):\n                bibID += '-' + str(self['year'])\n            if self.get('month', False):\n                bibID += '-' + str(self['month'])\n            if self.get('title', False):\n                tiSorted = sorted(self.get('title').split(' '), key = len)\n                bibID += '-' + tiSorted.pop().title()\n                while len(bibID) < 35 and len(tiSorted) > 0:\n                    bibID += '-' + tiSorted.pop().title() #Title Case\n            if len(bibID) < 30:\n                bibID += str(self.id)\n        elif WOSMode:\n            bibID = 'ISI:{}'.format(self.id[4:])\n        else:\n            bibID = str(self.id)\n        keyEntries.append(\"author = {{{{{}}}}},\".format(' and '.join(self.get('authorsFull', ['None']))))\n        if restrictedOutput:\n            tagsIter = ((k, self[k]) for k in commonRecordFields if k in self)\n        else:\n            tagsIter = self.items()\n        if WOSMode:\n            for tag, value in tagsIter:\n                if isinstance(value, list):\n                    keyEntries.append(\"{} = {{{{{}}}}},\".format(tag,'\\n   '.join((str(v) for v in value))))\n                else:\n                    keyEntries.append(\"{} = {{{{{}}}}},\".format(tag, value))\n            s = \"\"\"@{0}{{ {1},\\n{2}\\n}}\"\"\".format('misc', bibID, '\\n'.join(keyEntries))\n        else:\n            for tag, value in tagsIter:\n                keyEntries.append(\"{} = {},\".format(tag, _bibFormatter(value, maxLength)))\n            s = \"\"\"@{0}{{ {1},\\n    {2}\\n}}\"\"\".format('misc', bibID, '\\n    '.join(keyEntries))\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef proQuestParser(proFile):\n    #assumes the file is ProQuest\n    nameDict = {}\n    recSet = set()\n    error = None\n    lineNum = 0\n    try:\n        with open(proFile, 'r', encoding = 'utf-8') as openfile:\n            f = enumerate(openfile, start = 1)\n            for i in range(12):\n                lineNum, line = next(f)\n            # f is file so it *should* end, or at least cause a parser error eventually\n            while True:\n                lineNum, line = next(f)\n                lineNum, line = next(f)\n                if line == 'Bibliography\\n':\n                    for i in range(3):\n                        lineNum, line = next(f)\n                    break\n                else:\n                    s = line.split('. ')\n                    nameDict[int(s[0])] = '. '.join(s[1:])[:-1]\n            while True:\n                #import pdb; pdb.set_trace()\n                lineNum, line = next(f)\n                if line == 'Bibliography\\n':\n                    break\n                elif line.startswith('Document '):\n                    n = int(line[9:].split(' of ')[0])\n                    R = ProQuestRecord(f, sFile = proFile, sLine = lineNum)\n                    if R.get('Title') != nameDict[n]:\n                        error = BadProQuestFile(\"The numbering of the titles at the beginning of the file does not match the records inside. Line {} has a record titled '{}' with number {}, the name should be '{}'.\".format(lineNum, R.get('Title', \"TITLE MISSING\"), n, nameDict[n]))\n                        raise StopIteration\n                    recSet.add(R)\n                    lineNum, line = next(f)\n                else:\n                    #Parsing failed\n                    error = BadProQuestFile(\"The file '{}' has parts of it that are unparsable starting at line: {}. It is likely that the seperators between the records are incorrect\".format(proFile, lineNum))\n                    raise StopIteration\n    except (UnicodeDecodeError, StopIteration, ValueError) as e:\n        if error is None:\n            error = BadProQuestFile(\"The file '{}' has parts of it that are unparsable starting at line: {}.\\nThe error was: '{}'\".format(proFile, lineNum, e))\n    return recSet, error", "response": "Parses a ProQuest file _proFile_ to extract the individual entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of the names of investigators. The optional arguments are ignored.", "response": "def getInvestigators(self, tags = None, seperator = \";\", _getTag = False):\n        \"\"\"Returns a list of the names of investigators. The optional arguments are ignored.\n\n        # Returns\n\n        `list [str]`\n\n        > A list of all the found investigator's names\n        \"\"\"\n        if tags is None:\n            tags = ['Investigator']\n        elif isinstance(tags, str):\n            tags = ['Investigator', tags]\n        else:\n            tags.append('Investigator')\n        return super().getInvestigators(tags = tags, seperator = seperator, _getTag = _getTag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a medline file _pubFile_ to extract the individual entries as a set of MedlineRecords.", "response": "def medlineParser(pubFile):\n    \"\"\"Parses a medline file, _pubFile_, to extract the individual entries as [MedlineRecords](#metaknowledge.medline.recordMedline.MedlineRecord).\n\n    A medline file is a series of entries, each entry is a series of tags. A tag is a 2 to 4 character string each tag is padded with spaces on the left to make it 4 characters which is followed by a dash and a space (`'- '`). Everything after the tag and on all lines after it not starting with a tag is considered associated with the tag. Each entry's first tag is `PMID`, so a first line looks something like `PMID- 26524502`. Entries end with a single blank line.\n\n    # Parameters\n\n    _pubFile_ : `str`\n\n    > A path to a valid medline file, use [isMedlineFile](#metaknowledge.medline.medlineHandlers.isMedlineFile) to verify\n\n    # Returns\n\n    `set[MedlineRecord]`\n\n    > Records for each of the entries\n    \"\"\"\n    #assumes the file is MEDLINE\n    recSet = set()\n    error = None\n    lineNum = 0\n    try:\n        with open(pubFile, 'r', encoding = 'latin-1') as openfile:\n            f = enumerate(openfile, start = 1)\n            lineNum, line = next(f)\n            try:\n                while True:\n                    if line.startswith(\"PMID- \"):\n                        try:\n                            r = MedlineRecord(itertools.chain([(lineNum, line)], f), sFile = pubFile, sLine = lineNum)\n                            recSet.add(r)\n                        except BadPubmedFile as e:\n                            badLine = lineNum\n                            try:\n                                lineNum, line = next(f)\n                                while not line.startswith(\"PMID- \"):\n                                    lineNum, line = next(f)\n                            except (StopIteration, UnicodeDecodeError) as e:\n                                if error is None:\n                                    error = BadPubmedFile(\"The file '{}' becomes unparsable after line: {}, due to the error: {} \".format(pubFile, badLine, e))\n                                raise e\n                    elif line != '\\n':\n                        if error is None:\n                            error = BadPubmedFile(\"The file '{}' has parts of it that are unparsable starting at line: {}.\".format(pubFile, lineNum))\n                    lineNum, line = next(f)\n            except StopIteration:\n                #End of the file has been reached\n                pass\n    except UnicodeDecodeError:\n        if error is None:\n            error = BadPubmedFile(\"The file '{}' has parts of it that are unparsable starting at line: {}.\".format(pubFile, lineNum))\n    return recSet, error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nameStringGender(s, noExcept = False):\n    global mappingDict\n    try:\n        first = s.split(', ')[1].split(' ')[0].title()\n    except IndexError:\n        if noExcept:\n            return 'Unknown'\n        else:\n            return GenderException(\"The given String: '{}' does not have a  last name, first name pair in with a ', ' seperation.\".format(s))\n    if mappingDict is None:\n        mappingDict = getMapping()\n    return mappingDict.get(first, 'Unknown')", "response": "Returns the name of the given string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _j9SaveCurrent(sDir = '.'):\n    dname = os.path.normpath(sDir + '/' +  datetime.datetime.now().strftime(\"%Y-%m-%d_J9_AbbreviationDocs\"))\n    if not os.path.isdir(dname):\n        os.mkdir(dname)\n        os.chdir(dname)\n    else:\n        os.chdir(dname)\n    for urlID, urlString in j9urlGenerator(nameDict = True).items():\n        fname = \"{}_abrvjt.html\".format(urlID)\n        f = open(fname, 'wb')\n        f.write(urllib.request.urlopen(urlString).read())", "response": "Downloads and saves all the webpages\n    For Backend\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a Journal Title Abbreviations page", "response": "def _getDict(j9Page):\n    \"\"\"Parses a Journal Title Abbreviations page\n\n    Note the pages are not well formatted html as the <DT> tags are not closes so html parses (Beautiful Soup) do not work. This is a simple parser that only works on the webpages and may fail if they are changed\n\n    For Backend\n    \"\"\"\n    slines = j9Page.read().decode('utf-8').split('\\n')\n    while slines.pop(0) != \"<DL>\":\n        pass\n    currentName = slines.pop(0).split('\"></A><DT>')[1]\n    currentTag = slines.pop(0).split(\"<B><DD>\\t\")[1]\n    j9Dict = {}\n    while True:\n        try:\n            j9Dict[currentTag].append(currentName)\n        except KeyError:\n            j9Dict[currentTag] = [currentName]\n        try:\n            currentName = slines.pop(0).split('</B><DT>')[1]\n            currentTag = slines.pop(0).split(\"<B><DD>\\t\")[1]\n        except IndexError:\n            break\n    return j9Dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload and parses all the webpages For Backend", "response": "def _getCurrentj9Dict():\n    \"\"\"Downloads and parses all the webpages\n\n    For Backend\n    \"\"\"\n    urls = j9urlGenerator()\n    j9Dict = {}\n    for url in urls:\n        d = _getDict(urllib.request.urlopen(url))\n        if len(d) == 0:\n            raise RuntimeError(\"Parsing failed, this is could require an update of the parser.\")\n        j9Dict.update(d)\n    return j9Dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the database of Journal Title Abbreviations.", "response": "def updatej9DB(dbname = abrevDBname, saveRawHTML = False):\n    \"\"\"Updates the database of Journal Title Abbreviations. Requires an internet connection. The data base is saved relative to the source file not the working directory.\n\n    # Parameters\n\n    _dbname_ : `optional [str]`\n\n    > The name of the database file, default is \"j9Abbreviations.db\"\n\n    _saveRawHTML_ : `optional [bool]`\n\n    > Determines if the original HTML of the pages is stored, default `False`. If `True` they are saved in a directory inside j9Raws begining with todays date.\n    \"\"\"\n    if saveRawHTML:\n        rawDir = '{}/j9Raws'.format(os.path.dirname(__file__))\n        if not os.path.isdir(rawDir):\n            os.mkdir(rawDir)\n        _j9SaveCurrent(sDir = rawDir)\n    dbLoc = os.path.join(os.path.normpath(os.path.dirname(__file__)), dbname)\n    try:\n        with dbm.dumb.open(dbLoc, flag = 'c') as db:\n            try:\n                j9Dict = _getCurrentj9Dict()\n            except urllib.error.URLError:\n                raise urllib.error.URLError(\"Unable to access server, check your connection\")\n            for k, v in j9Dict.items():\n                if k in db:\n                    for jName in v:\n                        if jName not in j9Dict[k]:\n                            j9Dict[k] += '|' + jName\n                else:\n                    db[k] = '|'.join(v)\n    except dbm.dumb.error as e:\n        raise JournalDataBaseError(\"Something happened with the database of WOS journal names. To fix this you should delete the 1 to 3 files whose names start with {}. If this doesn't work (sorry), deleteing everything in '{}' and reinstalling metaknowledge should.\\nThe error was '{}'\".format(dbLoc, os.path.dirname(__file__), e))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the dictionary of journal abbreviations mapping to a list of the associated journal names. By default the local database is used.", "response": "def getj9dict(dbname = abrevDBname, manualDB = manualDBname, returnDict ='both'):\n    \"\"\"Returns the dictionary of journal abbreviations mapping to a list of the associated journal names. By default the local database is used. The database is in the file _dbname_ in the same directory as this source file\n\n    # Parameters\n\n    _dbname_ : `optional [str]`\n\n    > The name of the downloaded database file, the default is determined at run time. It is recommended that this remain untouched.\n\n    _manualDB_ : `optional [str]`\n\n    > The name of the manually created database file, the default is determined at run time. It is recommended that this remain untouched.\n\n    _returnDict_ : `optional [str]`\n\n    > default `'both'`, can be used to get both databases or only one  with `'WOS'` or `'manual'`.\n    \"\"\"\n    dbLoc = os.path.normpath(os.path.dirname(__file__))\n\n    retDict = {}\n    try:\n        if returnDict == 'both' or returnDict == 'WOS':\n            with dbm.dumb.open(dbLoc + '/{}'.format(dbname)) as db:\n                if len(db) == 0:\n                    raise JournalDataBaseError(\"J9 Database empty or missing, to regenerate it import and run metaknowledge.WOS.journalAbbreviations.updatej9DB().\")\n                for k, v in db.items():\n                    retDict[k.decode('utf-8')] = v.decode('utf-8').split('|')\n    except JournalDataBaseError:\n        updatej9DB()\n        return getj9dict(dbname = dbname, manualDB = manualDB, returnDict = returnDict)\n    try:\n        if returnDict == 'both' or returnDict == 'manual':\n            if os.path.isfile(dbLoc + '/{}.dat'.format(manualDB)):\n                with dbm.dumb.open(dbLoc + '/{}'.format(manualDB)) as db:\n                    for k, v in db.items():\n                        retDict[k.decode('utf-8')] = v.decode('utf-8').split('|')\n            else:\n                if returnDict == 'manual':\n                    raise JournalDataBaseError(\"Manual J9 Database ({0}) missing, to create it run addToDB(dbname = {0})\".format(manualDB))\n    except JournalDataBaseError:\n        updatej9DB(dbname = manualDB)\n        return getj9dict(dbname = dbname, manualDB = manualDB, returnDict = returnDict)\n    return retDict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a journal abbreviation to the database of journals.", "response": "def addToDB(abbr = None, dbname = manualDBname):\n    \"\"\"Adds _abbr_ to the database of journals. The database is kept separate from the one scraped from WOS, this supersedes it. The database by default is stored with the WOS one and the name is given by `metaknowledge.journalAbbreviations.manualDBname`. To create an empty database run **addToDB** without an _abbr_ argument.\n\n    # Parameters\n\n    _abbr_ : `optional [str or dict[str : str]]`\n\n    > The journal abbreviation to be added to the database, it can either be a single string in which case that string will be added with its self as the full name, or a dict can be given with the abbreviations as keys and their names as strings, use pipes (`'|'`) to separate multiple names. Note, if the empty string is given as a name the abbreviation will be considered manually __excluded__, i.e. having excludeFromDB() run on it.\n\n    _dbname_ : `optional [str]`\n\n    > The name of the database file, default is `metaknowledge.journalAbbreviations.manualDBname`.\n    \"\"\"\n    dbLoc = os.path.normpath(os.path.dirname(__file__))\n    with dbm.dumb.open(dbLoc + '/' + dbname) as db:\n        if isinstance(abbr, str):\n            db[abbr] = abbr\n        elif isinstance(abbr, dict):\n            try:\n                db.update(abbr)\n            except TypeError:\n                raise TypeError(\"The keys and values of abbr must be strings.\")\n        elif abbr is None:\n            pass\n        else:\n            raise TypeError(\"abbr must be a str or dict.\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef excludeFromDB(abbr = None, dbname = manualDBname):\n    dbLoc = os.path.normpath(os.path.dirname(__file__))\n    with dbm.dumb.open(dbLoc + '/' + dbname) as db:\n        if isinstance(abbr, str):\n            db[abbr] = ''\n        elif isinstance(abbr, list) or isinstance(abbr, tuple):\n            try:\n                db.update({k : '' for k in abbr})\n            except TypeError:\n                raise TypeError(\"The keys and values of abbr must be strings.\")\n        elif abbr is None:\n            pass\n        else:\n            raise TypeError(\"abbr must be a str, list or tuple.\")", "response": "Marks the given journal abbreviation to be excluded from the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize a tag or full name string to 2 character tags case insensitive.", "response": "def normalizeToTag(val):\n    \"\"\"Converts tags or full names to 2 character tags, case insensitive\n\n    # Parameters\n\n    _val_: `str`\n\n    > A two character string giving the tag or its full name\n\n    # Returns\n\n    `str`\n\n    > The short name of _val_\n    \"\"\"\n    try:\n        val = val.upper()\n    except AttributeError:\n        raise KeyError(\"{} is not a tag or name string\".format(val))\n    if val not in tagsAndNameSetUpper:\n        raise KeyError(\"{} is not a tag or name string\".format(val))\n    else:\n        try:\n            return fullToTagDictUpper[val]\n        except KeyError:\n            return val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize a tag or full name to a full name.", "response": "def normalizeToName(val):\n    \"\"\"Converts tags or full names to full names, case sensitive\n\n    # Parameters\n\n    _val_: `str`\n\n    > A two character string giving the tag or its full name\n\n    # Returns\n\n    `str`\n\n    > The full name of _val_\n    \"\"\"\n    if val not in tagsAndNameSet:\n        raise KeyError(\"{} is not a tag or name string\".format(val))\n    else:\n        try:\n            return tagToFullDict[val]\n        except KeyError:\n            return val"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef citations(val):\n    retCites = []\n    for c in val:\n        retCites.append(Citation(c))\n    return retCites", "response": "Returns a list of all the citations in the record"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of the names of investigators in the column.", "response": "def getInvestigators(self, tags = None, seperator = \";\", _getTag = False):\n        \"\"\"Returns a list of the names of investigators. This is done by looking (in order) for any of fields in _tags_ and splitting the strings on _seperator_. If no strings are found an empty list will be returned.\n\n        *Note* for some Grants `getInvestigators` has been overwritten and will ignore the arguments and simply provide the investigators.\n\n        # Parameters\n\n        _tags_ : `optional list[str]`\n\n        > A list of the tags to look for investigators in\n\n        _seperator_ : `optional str`\n\n        > The string that separators each investigators name  within the column\n\n        # Returns\n\n        `list [str]`\n\n        > A list of all the found investigator's names\n        \"\"\"\n        #By default we don't know which field has the investigators\n        investVal = []\n        retTag = None\n        if tags is not None:\n            if not isinstance(tags, list):\n                tags = [tags]\n            for tag in tags:\n                try:\n                    tval = self[tag].split(seperator)\n                    if _getTag:\n                        investVal += [(t.strip(), tag) for t in tval]\n                    else:\n                        investVal += [t.strip() for t in tval]\n                except KeyError:\n                    pass\n                except AttributeError:\n                    tval = [auth.split(seperator)[0] for auth in self[tag]]\n                    if _getTag:\n                        investVal += [(t.strip(), tag) for t in tval]\n                    else:\n                        investVal += [t.strip() for t in tval]\n        return investVal"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getInstitutions(self, tags = None, seperator = \";\", _getTag = False):\n        return self.getInvestigators(tags = tags, seperator = seperator, _getTag = _getTag)", "response": "Returns a list of the names of institutions in the column."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the tag - entry pairs from _other_ to the Grant.", "response": "def update(self, other):\n        \"\"\"Adds all the tag-entry pairs from _other_ to the `Grant`. If there is a conflict _other_ takes precedence.\n\n        # Parameters\n\n        _other_ : `Grant`\n\n        > Another `Grant` of the same type as _self_\n        \"\"\"\n        if type(self) != type(other):\n            return NotImplemented\n        else:\n            if other.bad:\n                self.error = other.error\n                self.bad = True\n            self._fieldDict.update(other._fieldDict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a networkx Graph object from the collection of grants and returns it.", "response": "def networkCoInvestigator(self, targetTags = None, tagSeperator = ';', count = True, weighted = True, _institutionLevel = False):\n        \"\"\"Creates a co-investigator from the collection\n\n        Most grants do not have a known investigator tag so it must be provided by the user in _targetTags_ and the separator character if it is not a semicolon should also be given.\n\n        # Parameters\n\n        > _targetTags_ : `optional list[str]`\n\n        > A list of all the Grant tags to check for investigators\n\n        _tagSeperator_ : `optional str`\n\n        > The character that separates the individual investigator's names\n\n        _count_ : `optional bool`\n\n        > Default `True`, if `True` the number of time a name occurs will be given\n\n        _weighted_ : `optional bool`\n\n        > Default `True`, if `True` the edge weights will be calculated and added to the edges\n\n        # Returns\n\n        `networkx Graph`\n\n        > The graph of co-investigator\n        \"\"\"\n        grph = nx.Graph()\n        pcount = 0\n        if _institutionLevel:\n            progArgs = (0, \"Starting to make a co-institution network\")\n        else:\n            progArgs = (0, \"Starting to make a co-investigator network\")\n        if metaknowledge.VERBOSE_MODE:\n            progKwargs = {'dummy' : False}\n        else:\n            progKwargs = {'dummy' : True}\n        with _ProgressBar(*progArgs, **progKwargs) as PBar:\n            for G in self:\n                if PBar:\n                    pcount += 1\n                    PBar.updateVal(pcount/ len(self), \"Analyzing: \" + str(G))\n                if _institutionLevel:\n                    investList = G.getInstitutions(tags = targetTags, seperator = tagSeperator, _getTag = True)\n                else:\n                    investList = G.getInvestigators(tags = targetTags, seperator = tagSeperator, _getTag = True)\n                if len(investList) > 1:\n                    for i, invest1 in enumerate(investList):\n                        if invest1[0] not in grph:\n                            if count:\n                                grph.add_node(invest1[0], count = 1, field = invest1[1])\n                            else:\n                                grph.add_node(invest1[0], field = invest1[1])\n                        elif count:\n                            grph.node[invest1[0]]['count'] += 1\n                        for invest2 in investList[i + 1:]:\n                            if invest2[0] not in grph:\n                                if count:\n                                    grph.add_node(invest2[0], count = 1, field = invest2[1])\n                                else:\n                                    grph.add_node(invest2[0], field = invest2[1])\n                            elif count:\n                                grph.node[invest2[0]]['count'] += 1\n                            if grph.has_edge(invest1[0], invest2[0]) and weighted:\n                                grph.edges[invest1[0], invest2[0]]['weight'] += 1\n                            elif weighted:\n                                grph.add_edge(invest1[0], invest2[0], weight = 1)\n                            else:\n                                grph.add_edge(invest1[0], invest2[0])\n                elif len(investList) > 0:\n                    invest1 = investList[0]\n                    if invest1[0] not in grph:\n                        if count:\n                            grph.add_node(invest1[0], count = 1, field = invest1[1])\n                        else:\n                            grph.add_node(invest1[0], field = invest1[1])\n                    elif count:\n                        grph.node[invest1[0]]['count'] += 1\n            if _institutionLevel:\n                PBar.finish(\"Done making a co-institution network from {}\".format(self))\n            else:\n                PBar.finish(\"Done making a co-investigator network from {}\".format(self))\n        return grph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef proQuestTagToFunc(tag):\n    if tag in singleLineEntries:\n        return lambda x : x[0]\n    elif tag in customTags:\n        return customTags[tag]\n    else:\n        return lambda x : x", "response": "Takes a tag string _tag_ and returns the processing function for its data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scopusRecordParser(record, header = None):\n    if header is None:\n        header = scopusHeader\n    splitRecord = record[:-1].split(',')\n    tagDict = {}\n    quoted = False\n    for key in reversed(header):\n        currentVal = splitRecord.pop()\n        if currentVal == '':\n            pass\n        elif currentVal[-1] == '\"':\n            if re.match(firstQuotingRegex, currentVal) is None:\n                valString = ',' + currentVal[:-1]\n                currentVal = splitRecord.pop()\n                #double quotes (\") are escaped by proceeding them with another double quote\n                #So an entry containing:\n\n                #',\"stuff,\"\"quoted\"\",more stuff,\"\"more quoted\"\"\",'\n\n                #would be a single string belonging to 1 column that looks like:\n\n                #'stuff,\"quoted\",more stuff,\"more quoted\"'\n\n                #We are not going to unescape the quotation marks but we do have to deal with them\n                while re.match(innerQuotingRegex, currentVal) is None:\n                    valString = ',' + currentVal + valString\n                    currentVal = splitRecord.pop()\n                valString = currentVal[1:] + valString\n            else:\n                try:\n                    valString = currentVal[1:-1]\n                except ValueError:\n                    valString = currentVal[1:-1]\n            tagDict[key] = valString\n        else:\n            tagDict[key] = currentVal\n    return tagDict", "response": "The parser for the ScopusRecords class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef createCitation(self, multiCite = False):\n        #Need to put the import here to avoid circular import issues\n        from ..citation import Citation\n        valsStr = ''\n        if multiCite:\n            auths = []\n            for auth in self.get(\"authorsShort\", []):\n                auths.append(auth.replace(',', ''))\n        else:\n            if self.get(\"authorsShort\", False):\n                valsStr += self['authorsShort'][0].replace(',', '') + ', '\n        if self.get(\"title\", False):\n            valsStr += self.get('title').replace('(', '').replace(')', '') + ' '\n        if self.get(\"year\", False):\n            valsStr += \"({}) \".format(self.get('year'))\n        if self.get(\"journal\", False):\n            valsStr += self.get('journal') + ', '\n        if self.get(\"volume\", False):\n            valsStr += str(self.get('volume')) + ', '\n        if self.get(\"beginningPage\", False):\n            valsStr += 'PP. ' + str(self.get('beginningPage'))\n        if multiCite and len(auths) > 0:\n            ret = (tuple((Citation(a + valsStr, scopusMode = True) for a in auths)))\n        elif multiCite:\n            ret = Citation(valsStr, scopusMode = True),\n        else:\n            ret = Citation(valsStr, scopusMode = True)\n        if multiCite:\n            rL = []\n            for c in ret:\n                if c.bad:\n                    c.year = self.get('year', 0)\n                    c.name = self.get('title', '').upper()\n                    c.journal = self.get(\"journal\", '').upper()\n                rL.append(c)\n            return tuple(rL)\n        else:\n            if ret.bad:\n                ret.year = self.get('year', 0)\n                ret.name = self.get('title', '').upper()\n                ret.journal = self.get(\"journal\", '').upper()\n            return ret", "response": "Overwriting the general [ citation creator ](. / Record. html#metaknowledge. ExtendedRecord. createCitation to deal with scopus weirdness."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef isWOSFile(infile, checkedLines = 3):\n    try:\n        with open(infile, 'r', encoding='utf-8-sig') as openfile:\n            f = enumerate(openfile, start = 0)\n            for i in range(checkedLines):\n                if \"VR 1.0\" in f.__next__()[1]:\n                    return True\n    except (StopIteration, UnicodeDecodeError):\n        return False\n    else:\n        return False", "response": "Determines if _infile_ is the path to a WOS file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wosParser(isifile):\n    plst = set()\n    error = None\n    try:\n        with open(isifile, 'r', encoding='utf-8-sig') as openfile:\n            f = enumerate(openfile, start = 0)\n            while \"VR 1.0\" not in f.__next__()[1]:\n                pass\n            notEnd = True\n            while notEnd:\n                line = f.__next__()\n                if line[1] == '':\n                    error =  BadWOSFile(\"'{}' does not have an 'EF', lines 1 to {} were checked\".format(isifile, line[0] + 1))\n                elif line[1].isspace():\n                    continue\n                elif 'EF' in line[1][:2]:\n                    notEnd = False\n                    continue\n                else:\n                    try:\n                        plst.add(WOSRecord(itertools.chain([line], f), sFile = isifile, sLine = line[0]))\n                    except BadWOSFile as e:\n                        try:\n                            s = f.__next__()[1]\n                            while s[:2] != 'ER':\n                                s = f.__next__()[1]\n                        except:\n                            error =  BadWOSFile(\"The file {} was not terminated corrrectly caused the following error:\\n{}\".format(isifile, str(e)))\n            try:\n                f.__next__()\n            except StopIteration:\n                pass\n            else:\n                error =  BadWOSFile(\"EF not at end of \" + isifile)\n    except UnicodeDecodeError:\n        try:\n            error =  BadWOSFile(\"'{}' has a unicode issue on line: {}.\".format(isifile, f.__next__()[0]))\n        except:\n            #Fallback needed incase f.__next__() causes issues\n            error =  BadWOSFile(\"'{}' has a unicode issue. Probably when being opened or possibly on the first line\".format(isifile))\n    except StopIteration:\n        error =  BadWOSFile(\"The file '{}' ends before EF was found\".format(isifile))\n    except KeyboardInterrupt as e:\n        error = e\n    finally:\n        if isinstance(error, KeyboardInterrupt):\n            raise error\n        return plst, error", "response": "This function is used to create a list of records from a WOS file. It returns a list of records."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine if _infile_ is a path to a Scopus csv file.", "response": "def isScopusFile(infile, checkedLines = 2, maxHeaderDiff = 3):\n    \"\"\"Determines if _infile_ is the path to a Scopus csv file. A file is considerd to be a Scopus file if it has the correct encoding (`utf-8` with BOM (Byte Order Mark)) and within the first _checkedLines_ a line contains the complete header, the list of all header entries in order is found in [`scopus.scopusHeader`](#metaknowledge.scopus).\n\n    **Note** this is for csv files _not_ plain text files from scopus, plain text files are not complete.\n\n    # Parameters\n\n    _infile_ : `str`\n\n    > The path to the targets file\n\n    _checkedLines_ : `optional [int]`\n\n    > default 2, the number of lines to check for the header\n\n    _maxHeaderDiff_ : `optional [int]`\n\n    > default 3, maximum number of different entries in the potetial file from the current known header `metaknowledge.scopus.scopusHeader`, if exceeded an `False` will be returned\n\n    # Returns\n\n    `bool`\n\n    > `True` if the file is a Scopus csv file\n    \"\"\"\n    try:\n        with open(infile, 'r', encoding='utf-8') as openfile:\n            if openfile.read(1) != \"\\ufeff\":\n                return False\n            for i in range(checkedLines):\n                if len(set(openfile.readline()[:-1].split(',')) ^ set(scopusHeader)) < maxHeaderDiff:\n                    return True\n    except (StopIteration, UnicodeDecodeError):\n        return False\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a scopus file _scopusFile_ to extract the individual lines as a set of ScopusRecords.", "response": "def scopusParser(scopusFile):\n    \"\"\"Parses a scopus file, _scopusFile_, to extract the individual lines as [ScopusRecords](../classes/ScopusRecord.html#metaknowledge.scopus.ScopusRecord).\n\n    A Scopus file is a csv (Comma-separated values) with a complete header, see [`scopus.scopusHeader`](#metaknowledge.scopus) for the entries, and each line after it containing a record's entry. The string valued entries are quoted with double quotes which means double quotes inside them can cause issues, see [scopusRecordParser()](#metaknowledge.scopus.recordScopus.scopusRecordParser) for more information.\n\n    # Parameters\n\n    _scopusFile_ : `str`\n\n    > A path to a valid scopus file, use [isScopusFile()](#metaknowledge.scopus.scopusHandlers.isScopusFile) to verify\n\n    # Returns\n\n    `set[ScopusRecord]`\n\n    > Records for each of the entries\n    \"\"\"\n    #assumes the file is Scopus\n    recSet = set()\n    error = None\n    lineNum = 0\n    try:\n        with open(scopusFile, 'r', encoding = 'utf-8') as openfile:\n            #Get rid of the BOM\n            openfile.read(1)\n            header = openfile.readline()[:-1].split(',')\n            if len(set(header) ^ set(scopusHeader)) == 0:\n                header = None\n            lineNum = 0\n            try:\n                for line, row in enumerate(openfile, start = 2):\n                    lineNum = line\n                    recSet.add(ScopusRecord(row, header = header, sFile = scopusFile, sLine = line))\n            except BadScopusFile as e:\n                if error is None:\n                    error = BadScopusFile(\"The file '{}' becomes unparsable after line: {}, due to the error: {} \".format(scopusFile, lineNum, e))\n    except (csv.Error, UnicodeDecodeError):\n        if error is None:\n            error = BadScopusFile(\"The file '{}' has parts of it that are unparsable starting at line: {}.\".format(scopusFile, lineNum))\n    return recSet, error"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a grid of cells with the given bounding rect.", "response": "def make_grid(rect, cells={}, num_rows=0, num_cols=0, padding=None,\n        inner_padding=None, outer_padding=None, row_heights={}, col_widths={}, \n        default_row_height='expand', default_col_width='expand'):\n    \"\"\"\n    Return rectangles for each cell in the specified grid.  The rectangles are \n    returned in a dictionary where the keys are (row, col) tuples.\n    \"\"\"\n    grid = Grid(\n            bounding_rect=rect,\n            min_cell_rects=cells,\n            num_rows=num_rows,\n            num_cols=num_cols,\n            padding=padding,\n            inner_padding=inner_padding,\n            outer_padding=outer_padding,\n            row_heights=row_heights,\n            col_widths=col_widths,\n            default_row_height=default_row_height,\n            default_col_width=default_col_width,\n    )\n    return grid.make_cells()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lorem_ipsum(num_sentences=None, num_paragraphs=None):\n    paragraphs = [\n        'Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam justo sem, malesuada ut ultricies ac, bibendum eu neque. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean at tellus ut velit dignissim tincidunt. Curabitur euismod laoreet orci semper dignissim. Suspendisse potenti. Vivamus sed enim quis dui pulvinar pharetra. Duis condimentum ultricies ipsum, sed ornare leo vestibulum vitae. Sed ut justo massa, varius molestie diam. Sed lacus quam, tempor in dictum sed, posuere et diam. Maecenas tincidunt enim elementum turpis blandit tempus. Nam lectus justo, adipiscing vitae ultricies egestas, porta nec diam. Aenean ac neque tortor. Cras tempus lacus nec leo ultrices suscipit. Etiam sed aliquam tortor. Duis lacus metus, euismod ut viverra sit amet, pulvinar sed urna.',\n        'Aenean ut metus in arcu mattis iaculis quis eu nisl. Donec ornare, massa ut vestibulum vestibulum, metus sapien pretium ante, eu vulputate lorem augue vestibulum orci. Donec consequat aliquam sagittis. Sed in tellus pretium tortor hendrerit cursus congue sit amet turpis. Sed neque lacus, lacinia ut consectetur eget, faucibus vitae lacus. Integer eu purus ac purus tempus mollis non sed dui. Vestibulum volutpat erat magna. Etiam nisl eros, eleifend a viverra sed, interdum sollicitudin erat. Integer a orci in dolor suscipit cursus. Maecenas hendrerit neque odio. Nulla orci orci, varius id viverra in, molestie vel lacus. Donec at odio quis augue bibendum lobortis nec ac urna. Ut lacinia hendrerit tortor mattis rhoncus. Proin nunc tortor, congue ac adipiscing sit amet, aliquet in lorem. Nulla blandit tempor arcu, ut tempus quam posuere eu. In magna neque, venenatis nec tincidunt vitae, lobortis eget nulla.',\n        'Praesent sit amet nibh turpis, vitae lacinia metus. Ut nisi lacus, feugiat quis feugiat nec, pretium a diam. Aenean bibendum sem eget lorem ullamcorper mattis. Donec elementum purus vel felis vulputate pretium. Duis in ipsum est. Nulla consequat tempor sodales. Donec scelerisque enim eu tellus eleifend imperdiet. Quisque ullamcorper bibendum justo sit amet tincidunt. Donec tempus lacus quis diam varius placerat. Cras metus magna, congue sit amet pulvinar viverra, laoreet vel felis. Praesent sit amet consequat enim. Phasellus arcu nisl, volutpat et molestie a, sagittis a est. Maecenas tincidunt, sem non pharetra mollis, diam nisl ornare tellus, at euismod libero arcu ornare risus. Vestibulum laoreet sollicitudin purus in pharetra. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos.',\n        'Nullam pellentesque tempor bibendum. Praesent dictum turpis nec quam consectetur aliquam. Aliquam id turpis nunc. Pellentesque fermentum lacus at tortor auctor venenatis. Maecenas blandit dui lectus. Nunc pellentesque pharetra suscipit. Nullam et metus diam, a congue leo. Curabitur convallis augue in lectus scelerisque non rhoncus lorem molestie. Curabitur in mi a erat dictum pharetra iaculis eu diam.',\n        'Nunc lorem magna, rhoncus sodales mattis quis, tincidunt eu mi. In ultrices, lectus ac porttitor tempor, odio nibh facilisis tortor, ac aliquet nisi ante non felis. Praesent ligula nisl, hendrerit ac volutpat non, varius quis tellus. Sed ornare faucibus elit eget faucibus. Nullam sem tellus, commodo id ullamcorper ut, imperdiet ac eros. Sed quis lorem id urna cursus laoreet et eget lacus. Nullam tristique semper sem, eget tempus sem pellentesque sit amet. Donec sed orci augue, convallis tempor tellus. Sed consequat commodo ante a pretium. Nulla et est mauris. Nullam at massa justo. Proin tempor arcu ac eros suscipit varius. Fusce vestibulum quam placerat tellus imperdiet et venenatis diam tristique. Sed pretium tempor tellus, consequat pulvinar massa pellentesque a.',\n        'Nulla et lorem vel urna fringilla malesuada ut sit amet tortor. Donec id leo mi. Proin sagittis blandit lacus, placerat imperdiet justo pellentesque ac. Cras iaculis aliquam faucibus. Aenean urna nisi, laoreet ac fringilla dignissim, lacinia eget orci. Vivamus porta lacinia dapibus. Aenean molestie, augue sit amet blandit suscipit, tellus turpis ullamcorper purus, ut pretium turpis lorem quis neque. Pellentesque porta dui at arcu mollis tristique. Suspendisse feugiat felis quis felis sollicitudin porttitor.',\n        'Morbi vestibulum, massa quis posuere facilisis, quam lacus porttitor tortor, id fringilla elit velit ac felis. Fusce at luctus risus. Mauris bibendum diam quis odio auctor quis porta massa pellentesque. Proin congue, nisl eu feugiat faucibus, justo orci congue neque, a porta tellus ipsum accumsan turpis. Ut neque enim, dignissim nec fermentum sed, laoreet id orci. Duis fringilla, elit vel tempus porttitor, purus tellus dapibus nisl, eu scelerisque diam lorem vel ante. Ut tempor, urna nec bibendum facilisis, sapien dui ornare lectus, at tempor ligula diam sit amet ligula. Sed a dui in ipsum eleifend egestas.',\n        'Quisque ornare fringilla velit, et tincidunt purus convallis vel. Sed venenatis, risus vitae volutpat rhoncus, sapien lorem lacinia elit, id dictum sapien dui vitae lorem. Praesent aliquet accumsan eros quis tempor. Suspendisse eget justo quis arcu bibendum adipiscing. Phasellus quis erat nec massa elementum porta. Nam venenatis elementum mi vel porta. Nunc vel augue non tellus euismod convallis. Curabitur commodo augue vel augue ultrices in fringilla nunc cursus. Mauris auctor laoreet neque, id gravida velit suscipit eget. Maecenas eget libero in lacus auctor feugiat. Pellentesque in lectus felis, eu dictum tortor. Aenean sagittis, massa malesuada dapibus tincidunt, leo massa imperdiet ante, nec mollis nisl turpis in orci. Proin ut purus et eros sagittis volutpat.',\n        'Donec molestie sem et metus bibendum convallis semper arcu imperdiet. Curabitur quam libero, fermentum vel adipiscing a, cursus at neque. Maecenas cursus risus vestibulum diam ultricies rutrum. Nullam in enim vel lorem accumsan pulvinar. Cras eget viverra turpis. Sed eget lectus urna, eget venenatis libero. Donec porta libero eu est pulvinar pretium. Ut lectus arcu, aliquam et vestibulum euismod, mattis at orci. Fusce dolor lorem, bibendum a dignissim ut, facilisis eu enim. Morbi erat nibh, interdum non ultricies non, porta ac lacus. Curabitur et nunc nec turpis convallis ullamcorper eget vitae mi.',\n        'Curabitur porta molestie sapien, non rhoncus turpis gravida vel. Ut est lacus, elementum eu pretium sit amet, tristique vel orci. Praesent quis suscipit urna. Donec pellentesque molestie tellus sit amet fringilla. Etiam tempus viverra ipsum et tempus. Nunc ut odio imperdiet lorem malesuada bibendum. In aliquam ligula eu sem ullamcorper pulvinar. Quisque sollicitudin placerat dolor et porttitor. Nulla adipiscing lorem id libero aliquet interdum. Suspendisse vehicula fermentum congue. Cras fringilla nisl vitae lectus mollis viverra. Aliquam pharetra lobortis risus, a elementum elit condimentum in. Aenean tincidunt varius faucibus. Nulla non nisi lorem. Suspendisse id sapien a enim lobortis aliquam.',\n        'Aliquam erat volutpat. Maecenas neque leo, mattis eu pretium vel, mattis in ante. Nullam sagittis leo diam. Quisque tempor magna in justo vestibulum eget egestas nibh pellentesque. Pellentesque in enim vitae velit pellentesque hendrerit. Cras ultricies, dui et imperdiet gravida, nunc nisl cursus tortor, sit amet porttitor dolor nibh a justo. Praesent ut mauris vitae turpis lobortis scelerisque a nec ligula. Donec turpis erat, iaculis vel dapibus vel, varius id lorem. Integer et enim erat, at eleifend libero.',\n        'Phasellus id mi ut nunc cursus pellentesque. Aliquam erat volutpat. Vivamus pretium posuere tellus, ac aliquet metus iaculis eget. Curabitur in mi enim. Duis pretium pretium dui, ut iaculis ipsum scelerisque ut. Proin quam dolor, eleifend et porta vitae, cursus molestie lectus. Aenean dignissim laoreet consectetur. Cras iaculis, lectus imperdiet condimentum suscipit, metus nisi egestas arcu, in tempus sem ipsum eu eros. Vestibulum a orci in elit congue euismod quis quis nisi.',\n        'In quis urna leo, at malesuada ipsum. Vestibulum sollicitudin ullamcorper hendrerit. Vestibulum vestibulum mi sodales nulla sagittis commodo. Maecenas nisi lorem, placerat vel aliquet quis, dictum ac ligula. Vestibulum egestas accumsan accumsan. Aenean lobortis pharetra erat convallis pretium. Aliquam consequat facilisis porta. Cras hendrerit nunc et mauris egestas hendrerit. Proin rhoncus, mi id ullamcorper pharetra, ipsum sapien blandit turpis, et ultricies purus neque eget justo. Quisque sodales, nisi in cursus rutrum, elit nibh volutpat lacus, nec sollicitudin erat leo at lectus. Morbi ac dolor mi, vel ultricies quam.',\n        'Sed hendrerit nisl id lectus cursus in adipiscing lorem rutrum. Morbi nisl justo, egestas ac aliquet at, scelerisque luctus sapien. Donec sollicitudin elementum mattis. Praesent semper, ante euismod accumsan gravida, ante neque convallis augue, quis vulputate erat nunc vitae tellus. Duis ac lectus ullamcorper purus commodo luctus. Etiam quis augue in purus molestie imperdiet. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam posuere commodo turpis, at pulvinar tortor scelerisque et. Nam vulputate dui sed magna interdum sollicitudin. Nam pulvinar euismod enim vitae malesuada. Aenean non molestie leo. Pellentesque quis lacus mi, et ornare nibh. Etiam pharetra, odio vitae euismod faucibus, nunc urna pulvinar felis, eget molestie est enim sit amet sapien. Vivamus eu neque nulla.',\n        'Mauris eget nibh ut augue malesuada tristique nec quis urna. Vestibulum faucibus, mauris sed posuere volutpat, felis lacus vulputate felis, eget luctus lorem nulla sed velit. Proin et purus nec quam tristique cursus. Nullam adipiscing tortor imperdiet purus facilisis eu luctus nulla vestibulum. Sed pulvinar risus sollicitudin risus fringilla et hendrerit lorem accumsan. Vestibulum venenatis est sit amet nunc gravida nec aliquam arcu adipiscing. Nam quis aliquet mauris. Cras nec neque vitae tellus posuere posuere.',\n        'Nulla facilisi. Vestibulum sit amet dui turpis. Aliquam erat volutpat. In hac habitasse platea dictumst. Morbi in enim nec massa semper tincidunt. Ut fermentum iaculis dui, sed adipiscing dolor porta at. Nam hendrerit libero non nisi ornare eu cursus mauris accumsan. Ut ullamcorper, odio vel ultrices suscipit, metus libero ornare dui, non dapibus est dui vehicula ipsum.',\n        'Nam diam sapien, lacinia vel sollicitudin interdum, faucibus aliquam enim. Mauris tristique iaculis purus eu lacinia. Suspendisse condimentum, dolor a euismod lacinia, leo orci pellentesque orci, non rhoncus turpis lorem sed lacus. Integer velit nisl, rutrum sit amet posuere at, vulputate ultrices tortor. Nullam pharetra, orci tempor dapibus elementum, felis nulla lacinia nunc, quis ultricies dui lectus dictum diam. Praesent eu velit magna, eu lacinia leo. Duis sit amet bibendum dui. Duis tincidunt vulputate dolor eu euismod. Pellentesque nisl sem, mollis ac venenatis a, facilisis vitae ligula. Vivamus sem leo, vestibulum tincidunt iaculis nec, tristique tincidunt mi. Suspendisse imperdiet elit vitae turpis ullamcorper luctus. Aenean in augue mauris. Vivamus nisi libero, dignissim non consectetur sodales, fermentum at sem. Nulla tincidunt fringilla justo quis pulvinar. Nam ac sem sed diam pellentesque egestas vitae ac nisi. Praesent scelerisque dapibus mi vitae tempor.',\n        'Donec tempor, massa non pulvinar suscipit, justo dolor pharetra nisl, ut semper libero lorem non tortor. Integer dapibus arcu viverra nisi hendrerit mattis et ut mauris. Maecenas pulvinar, orci vitae ultricies egestas, orci nisi rutrum justo, eu volutpat nibh odio ac purus. Nulla pellentesque sem eget arcu imperdiet ullamcorper. Curabitur nec magna massa. Morbi lobortis urna sed ligula commodo viverra. Pellentesque molestie, ipsum nec faucibus mollis, neque purus sodales sapien, in convallis nisi libero et lorem. Ut sed rutrum leo. Aliquam eleifend, felis quis ullamcorper consequat, dolor mi vulputate ipsum, lobortis ultricies felis nulla at augue.',\n        'Ut gravida porttitor arcu, malesuada mollis urna vehicula nec. Suspendisse sagittis nulla condimentum libero lacinia sed dapibus dui egestas. Etiam convallis congue ipsum, eu fermentum turpis rutrum id. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Ut nunc eros, sagittis a venenatis et, interdum in leo. Curabitur urna magna, volutpat in mattis ut, adipiscing et ligula. Nam dignissim mattis accumsan. Nulla vehicula felis vel turpis tempus hendrerit. Phasellus rhoncus vulputate massa, tincidunt euismod dui porttitor ac. Sed ut sapien quam, ac egestas odio. Pellentesque at aliquet ante. Donec rhoncus ornare lacus eu ullamcorper. Vestibulum sit amet hendrerit magna. Nulla sed diam nulla.',\n        'Nulla vestibulum sagittis arcu in egestas. Aliquam sed ante justo. Quisque nec dolor nibh, sed feugiat mi. Etiam lorem elit, interdum eu tempor nec, tincidunt eu risus. Fusce id libero augue. Curabitur ultrices, lorem eget mollis fringilla, dolor leo euismod tellus, congue luctus nisi purus vitae urna. Suspendisse tempor orci accumsan sem pretium at accumsan augue tristique. Proin sed turpis at mi feugiat lacinia a nec sem. Suspendisse vel facilisis leo. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Duis ornare enim nec ante adipiscing tincidunt. Maecenas ut justo iaculis leo vestibulum blandit quis vitae mauris. Proin in vestibulum massa.',\n    ]\n\n    if num_paragraphs:\n        paragraphs = paragraphs[:num_paragraphs]\n\n    text = '\\n\\n'.join(paragraphs)\n    sentences = text.split('.')\n\n    if num_sentences:\n        sentences = sentences[:num_sentences]\n\n    lorem = '.'.join(sentences).strip()\n    if not lorem.endswith('.'):\n        lorem += '.'\n\n    return lorem", "response": "Return the given amount of Lorem ipsum text."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures this handler to re - dispatch events from another handler.", "response": "def relay_events_from(self, originator, event_type, *more_event_types):\n        \"\"\"\n        Configure this handler to re-dispatch events from another handler.\n\n        This method configures this handler dispatch an event of type \n        *event_type* whenever *originator* dispatches events of the same type \n        or any of the types in *more_event_types*.  Any arguments passed to the \n        original event are copied to the new event.\n\n        This method is mean to be useful for creating composite widgets that \n        want to present a simple API by making it seem like the events being \n        generated by their children are actually coming from them.  See the \n        `/composing_widgets` tutorial for an example.\n        \"\"\"\n        handlers = {\n                event_type: lambda *args, **kwargs: \\\n                    self.dispatch_event(event_type, *args, **kwargs)\n                for event_type in (event_type,) + more_event_types\n        }\n        originator.set_handlers(**handlers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts dispatching the given event at the given frequency.", "response": "def start_event(self, event_type, *args, dt=1/60):\n        \"\"\"\n        Begin dispatching the given event at the given frequency.\n\n        Calling this method will cause an event of type *event_type* with \n        arguments *args* to be dispatched every *dt* seconds.  This will \n        continue until `stop_event()` is called for the same event.\n\n        These continuously firing events are useful if, for example, you want \n        to make a button that scrolls for as long as it's being held.\n        \"\"\"\n        # Don't bother scheduling a timer if nobody's listening.  This isn't \n        # great from a general-purpose perspective, because a long-lived event \n        # could have listeners attach and detach in the middle.  But I don't \n        # like the idea of making a bunch of clocks to spit out a bunch of \n        # events that are never used, although to be fair I don't actually know \n        # how expensive that would be.  If I want to make this implementation \n        # more general purpose, I could start and stop timers as necessary in \n        # the methods that add or remove handlers.\n        if not any(self.__yield_handlers(event_type)):\n            return\n\n        def on_time_interval(dt): #\n            self.dispatch_event(event_type, *args, dt)\n\n        pyglet.clock.schedule_interval(on_time_interval, dt)\n        self.__timers[event_type] = on_time_interval"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstop dispatching the given event.", "response": "def stop_event(self, event_type):\n        \"\"\"\n        Stop dispatching the given event.\n\n        It is not an error to attempt to stop an event that was never started, \n        the request will just be silently ignored.\n        \"\"\"\n        if event_type in self.__timers:\n            pyglet.clock.unschedule(self.__timers[event_type])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __yield_handlers(self, event_type):\n        if event_type not in self.event_types:\n            raise ValueError(\"%r not found in %r.event_types == %r\" % (event_type, self, self.event_types))\n\n        # Search handler stack for matching event handlers\n        for frame in list(self._event_stack):\n            if event_type in frame:\n                yield frame[event_type]\n\n        # Check instance for an event handler\n        if hasattr(self, event_type):\n            yield getattr(self, event_type)", "response": "Yield all the handlers registered for the given event type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _filter_pending_updates(self):\n        from more_itertools import unique_everseen as unique\n        yield from reversed(list(unique(reversed(self._pending_updates))))", "response": "Returns a list of tuples that are pending updates for the given entry point."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main():\n\n    # Create command line parser.\n    parser = argparse.ArgumentParser()\n\n    # Adding command line arguments.\n    parser.add_argument(\"username\", help=\"Github Username\", default=None)\n\n    parser.add_argument(\n        \"--deep_dive\",\n        help=\" \".join(\n            [\n                \"If added considers repositories starred by users\",\n                \"you follow along with repositories you have\",\n                \"starred. Is significantly slower.\",\n            ]\n        ),\n        action=\"store_true\",\n        default=False,\n    )\n\n    # Parse command line arguments.\n    arguments = parser.parse_args()\n\n    if arguments.username is None:\n        parser.print_help()\n        return\n\n    print(\"\")\n    print(\n        crayons.white(\n            \"Authentication (with password) have higher rate limits.\"\n        )\n    )\n    print(\n        crayons.white(\n            \"Skipping password might cause failure due to rate limit.\"\n        )\n    )\n    print(\"\")\n\n    password = getpass.getpass(\n        crayons.blue(\n            \"Enter password (to skip press enter without entering anything): \",\n            bold=True,\n        )\n    )\n\n    try:\n        gs = GitSuggest(\n            username=arguments.username,\n            password=password,\n            token=None,\n            deep_dive=arguments.deep_dive,\n        )\n    except BadCredentialsException:\n        print(\"\")\n        print(\n            crayons.red(\n                \"Incorrect password provided, to skip password enter nothing.\",\n                bold=True,\n            )\n        )\n        exit()\n    except TwoFactorException:\n        print(\"\")\n        print(\n            crayons.red(\n                \"\\n\".join(\n                    [\n                        \"You have 2FA set up, please enter a personal access token.\",\n                        \"You can generate one on https://github.com/settings/tokens\",\n                    ]\n                ),\n                bold=True,\n            )\n        )\n        exit()\n\n    print(\"\")\n    print(crayons.green(\"Suggestions generated!\"))\n\n    file_name = \"/tmp/gitresults.html\"\n    repos = list(gs.get_suggested_repositories())\n\n    r2h = ReposToHTML(arguments.username, repos)\n    r2h.to_html(file_name)\n\n    webbrowser.open_new(\"file://\" + file_name)", "response": "Main function for the main function of the main function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_html(self):\n        here = path.abspath(path.dirname(__file__))\n\n        env = Environment(loader=FileSystemLoader(path.join(here, \"res/\")))\n        suggest = env.get_template(\"suggest.htm.j2\")\n\n        return suggest.render(\n            logo=path.join(here, \"res/logo.png\"),\n            user_login=self.user,\n            repos=self.repos,\n        )", "response": "Method to convert the repository list to a search results page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_html(self, write_to):\n        page_html = self.get_html()\n\n        with open(write_to, \"wb\") as writefile:\n            writefile.write(page_html.encode(\"utf-8\"))", "response": "Method to convert the repository list to a search results page and write it to a HTML file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompiles list of all words to ignore.", "response": "def __get_words_to_ignore(self):\n        \"\"\"Compiles list of all words to ignore.\n\n        :return: List of words to ignore.\n        \"\"\"\n        # Stop words in English.\n        english_stopwords = stopwords.words(\"english\")\n\n        here = path.abspath(path.dirname(__file__))\n\n        # Languages in git repositories.\n        git_languages = []\n        with open(path.join(here, \"gitlang/languages.txt\"), \"r\") as langauges:\n            git_languages = [line.strip() for line in langauges]\n\n        # Other words to avoid in git repositories.\n        words_to_avoid = []\n        with open(path.join(here, \"gitlang/others.txt\"), \"r\") as languages:\n            words_to_avoid = [line.strip() for line in languages]\n\n        return set(\n            itertools.chain(english_stopwords, git_languages, words_to_avoid)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_suggested_repositories(self):\n        if self.suggested_repositories is None:\n            # Procure repositories to suggest to user.\n            repository_set = list()\n            for term_count in range(5, 2, -1):\n                query = self.__get_query_for_repos(term_count=term_count)\n                repository_set.extend(self.__get_repos_for_query(query))\n\n            # Remove repositories authenticated user is already interested in.\n            catchy_repos = GitSuggest.minus(\n                repository_set, self.user_starred_repositories\n            )\n\n            # Filter out repositories with too long descriptions. This is a\n            # measure to weed out spammy repositories.\n            filtered_repos = []\n\n            if len(catchy_repos) > 0:\n                for repo in catchy_repos:\n                    if (\n                        repo is not None\n                        and repo.description is not None\n                        and len(repo.description) <= GitSuggest.MAX_DESC_LEN\n                    ):\n                        filtered_repos.append(repo)\n\n            # Present the repositories, highly starred to not starred.\n            filtered_repos = sorted(\n                filtered_repos,\n                key=attrgetter(\"stargazers_count\"),\n                reverse=True,\n            )\n\n            self.suggested_repositories = GitSuggest.get_unique_repositories(\n                filtered_repos\n            )\n\n        # Return an iterator to help user fetch the repository listing.\n        for repository in self.suggested_repositories:\n            yield repository", "response": "Method to procure suggested repositories for the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nattempt to convert string value into numeric type", "response": "def guess_type(s):\n    \"\"\" attempt to convert string value into numeric type \"\"\"\n    sc = s.replace(',', '') # remove comma from potential numbers\n\n    try:\n        return int(sc)\n    except ValueError:\n        pass\n\n    try:\n        return float(sc)\n    except ValueError:\n        pass\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, node):\n        self._attrs = {}\n        vals = []\n        yielded = False\n\n        for x in self._read_parts(node):\n            if isinstance(x, Field):\n                yielded = True\n                x.attrs = self._attrs\n                yield x\n            else:\n                vals.append(ustr(x).strip(' \\n\\t'))\n\n        joined = ' '.join([ x for x in vals if x ])\n        if joined:\n            yielded = True\n            yield Field(node, guess_type(joined), self._attrs)\n\n        if not yielded:\n            yield Field(node, \"\", self._attrs)", "response": "Parse a node into a generator yielding Field objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(self, *nodes):\n        for n in nodes:\n            if not n.contents:\n                continue\n            row = self._parse(n)\n            if not row.is_null:\n                yield row", "response": "Parse one or more tr nodes yielding wikitables. Row objects"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_header_flat(self):\n        nodes = self._node.contents.filter_tags(\n                    matches=ftag('th'), recursive=False)\n        if not nodes:\n            return\n        self._log('found header outside rows (%d <th> elements)' % len(nodes))\n        return nodes", "response": "Find header elements in a table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_header_row(self):\n        th_max = 0\n        header_idx = 0\n        for idx, tr in enumerate(self._tr_nodes):\n            th_count = len(tr.contents.filter_tags(matches=ftag('th')))\n            if th_count > th_max:\n                th_max = th_count\n                header_idx = idx\n\n        if not th_max:\n            return\n\n        self._log('found header at row %d (%d <th> elements)' % \\\n                    (header_idx, th_max))\n\n        header_row = self._tr_nodes.pop(header_idx)\n        return header_row.contents.filter_tags(matches=ftag('th'))", "response": "Evaluate all rows and determine the first header row based on the number of th tagged elements and return the first one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_default_header(self):\n        td_max = 0\n\n        for idx, tr in enumerate(self._tr_nodes):\n            td_count = len(tr.contents.filter_tags(matches=ftag('td')))\n            if td_count > td_max:\n                td_max = td_count\n\n        self._log('creating default header (%d columns)' % td_max)\n        return [ 'column%d' % n for n in range(0,td_max) ]", "response": "Return a generic header based on the tables column count\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching a single page by title", "response": "def fetch_page(self, title, method='GET'):\n        \"\"\" Query for page by title \"\"\"\n        params = { 'prop': 'revisions',\n                   'format': 'json',\n                   'action': 'query',\n                   'explaintext': '',\n                   'titles': title,\n                   'rvprop': 'content' }\n        r = self.request(method, self.base_url, params=params)\n        r.raise_for_status()\n        pages = r.json()[\"query\"][\"pages\"]\n        # use key from first result in 'pages' array\n        pageid = list(pages.keys())[0]\n        if pageid == '-1':\n            raise ArticleNotFound('no matching articles returned')\n\n        return pages[pageid]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a file in a running Python process and prints the stack.", "response": "def print_stack(pid, include_greenlet=False, debugger=None, verbose=False):\n    \"\"\"Executes a file in a running Python process.\"\"\"\n    # TextIOWrapper of Python 3 is so strange.\n    sys_stdout = getattr(sys.stdout, 'buffer', sys.stdout)\n    sys_stderr = getattr(sys.stderr, 'buffer', sys.stderr)\n\n    make_args = make_gdb_args\n    environ = dict(os.environ)\n    if (\n        debugger == 'lldb' or\n        (debugger is None and platform.system().lower() == 'darwin')\n    ):\n        make_args = make_lldb_args\n        # fix the PATH environment variable for using built-in Python with lldb\n        environ['PATH'] = '/usr/bin:%s' % environ.get('PATH', '')\n\n    tmp_fd, tmp_path = tempfile.mkstemp()\n    os.chmod(tmp_path, 0o777)\n    commands = []\n    commands.append(FILE_OPEN_COMMAND)\n    commands.extend(UTILITY_COMMANDS)\n    commands.extend(THREAD_STACK_COMMANDS)\n    if include_greenlet:\n        commands.extend(GREENLET_STACK_COMMANDS)\n    commands.append(FILE_CLOSE_COMMAND)\n    command = r';'.join(commands)\n\n    args = make_args(pid, command % tmp_path)\n    process = subprocess.Popen(\n        args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = process.communicate()\n    if verbose:\n        sys_stderr.write(b'Standard Output:\\n%s\\n' % out)\n        sys_stderr.write(b'Standard Error:\\n%s\\n' % err)\n        sys_stderr.flush()\n\n    for chunk in iter(functools.partial(os.read, tmp_fd, 1024), b''):\n        sys_stdout.write(chunk)\n    sys_stdout.write(b'\\n')\n    sys_stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli_main(pid, include_greenlet, debugger, verbose):\n    '''Print stack of python process.\n\n    $ pystack <pid>\n    '''\n    try:\n        print_stack(pid, include_greenlet, debugger, verbose)\n    except DebuggerNotFound as e:\n        click.echo('DebuggerNotFound: %s' % e.args[0], err=True)\n        click.get_current_context().exit(1)", "response": "Print stack of python process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef forward_algo(self,observations):\n\n        # Store total number of observations total_stages = len(observations) \n        total_stages = len(observations)\n\n        # Alpha[i] stores the probability of reaching state 'i' in stage 'j' where 'j' is the iteration number\n\n        # Inittialize Alpha\n        ob_ind = self.obs_map[ observations[0] ]\n        alpha = np.multiply ( np.transpose(self.em_prob[:,ob_ind]) , self.start_prob )\n\n        # Iteratively find alpha(using knowledge of alpha in the previous stage)\n        for curr_t in range(1,total_stages):\n            ob_ind = self.obs_map[observations[curr_t]]\n            alpha = np.dot( alpha , self.trans_prob)\n            alpha = np.multiply( alpha , np.transpose( self.em_prob[:,ob_ind] ))\n\n        # Sum the alpha's over the last stage\n        total_prob = alpha.sum()\n        return ( total_prob )", "response": "This method finds the probability of an observation sequence for given model parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef viterbi(self,observations):\n\n        # Find total states,observations\n        total_stages = len(observations)\n        num_states = len(self.states)\n\n        # initialize data\n        # Path stores the state sequence giving maximum probability\n        old_path = np.zeros( (total_stages, num_states) )\n        new_path = np.zeros( (total_stages, num_states) )\n\n        # Find initial delta\n        # Map observation to an index\n        # delta[s] stores the probability of most probable path ending in state 's' \n        ob_ind = self.obs_map[ observations[0] ]\n        delta = np.multiply ( np.transpose(self.em_prob[:,ob_ind]) , self.start_prob )\n\n        # Scale delta\n        delta = delta /np.sum(delta)\n\n        # initialize path\n        old_path[0,:] = [i for i in range(num_states) ]\n\n        # Find delta[t][x] for each state 'x' at the iteration 't'\n        # delta[t][x] can be found using delta[t-1][x] and taking the maximum possible path\n        for curr_t in range(1,total_stages):\n\n            # Map observation to an index\n            ob_ind = self.obs_map[ observations[curr_t] ]\n            # Find temp and take max along each row to get delta\n            temp  =  np.multiply (np.multiply(delta , self.trans_prob.transpose()) , self.em_prob[:, ob_ind] )\n\n            # Update delta and scale it\n            delta = temp.max(axis = 1).transpose()\n            delta = delta /np.sum(delta)\n\n            # Find state which is most probable using argax\n            # Convert to a list for easier processing\n            max_temp = temp.argmax(axis=1).transpose()\n            max_temp = np.ravel(max_temp).tolist()\n\n            # Update path\n            for s in range(num_states):\n                new_path[:curr_t,s] = old_path[0:curr_t, max_temp[s] ] \n\n            new_path[curr_t,:] = [i for i in range(num_states) ]\n            old_path = new_path.copy()\n\n\n        # Find the state in last stage, giving maximum probability\n        final_max = np.argmax(np.ravel(delta))\n        best_path = old_path[:,final_max].tolist()\n        best_path_map = [ self.state_map[i] for i in best_path]\n\n        return best_path_map", "response": "This function returns a list of hidden states from the given observations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef train_hmm(self,observation_list, iterations, quantities):\n\n        obs_size = len(observation_list)\n        prob = float('inf')\n        q = quantities\n\n        # Train the model 'iteration' number of times\n        # store em_prob and trans_prob copies since you should use same values for one loop\n        for i in range(iterations):\n\n            emProbNew = np.asmatrix(np.zeros((self.em_prob.shape)))\n            transProbNew = np.asmatrix(np.zeros((self.trans_prob.shape)))\n            startProbNew = np.asmatrix(np.zeros((self.start_prob.shape)))\n\n            for j in range(obs_size):\n\n                # re-assing values based on weight\n                emProbNew= emProbNew + q[j] * self._train_emission(observation_list[j])\n                transProbNew = transProbNew + q[j] * self._train_transition(observation_list[j])\n                startProbNew = startProbNew + q[j] * self._train_start_prob(observation_list[j])\n\n\n            # Normalizing\n            em_norm = emProbNew.sum(axis = 1)\n            trans_norm = transProbNew.sum(axis = 1)\n            start_norm = startProbNew.sum(axis = 1)\n\n            emProbNew = emProbNew/ em_norm.transpose()\n            startProbNew = startProbNew/ start_norm.transpose()\n            transProbNew = transProbNew/ trans_norm.transpose()\n\n\n            self.em_prob,self.trans_prob = emProbNew,transProbNew\n            self.start_prob = startProbNew\n\n            if prob -  self.log_prob(observation_list,quantities)>0.0000001:\n                prob = self.log_prob(observation_list,quantities)\n            else:\n                return self.em_prob, self.trans_prob , self.start_prob\n\n\n        return self.em_prob, self.trans_prob , self.start_prob", "response": "Runs the Baum Welch Algorithm and finds the new model parameters for each item in observation_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind Weighted log probability of a list of observation sequences **Arguments**: :param observation_list: A nested list, or a list of lists :type observation_list: Contains a list multiple observation sequences. :param quantities: Number of times, each corresponding item in 'observation_list' occurs. :type quantities: A list of integers :return: Weighted log probability of multiple observations. :rtype: float **Features**: Scaling applied here. This ensures that no underflow error occurs. **Example**: >>> states = ('s', 't') >>> possible_observation = ('A','B' ) >>> # Numpy arrays of the data >>> start_probability = np.matrix( '0.5 0.5 ') >>> transition_probability = np.matrix('0.6 0.4 ; 0.3 0.7 ') >>> emission_probability = np.matrix( '0.3 0.7 ; 0.4 0.6 ' ) >>> # Initialize class object >>> test = hmm(states,possible_observation,start_probability,transition_probability,emission_probability) >>> observations = ('A', 'B','B','A') >>> obs4 = ('B', 'A','B') >>> observation_tuple = [] >>> observation_tuple.extend( [observations,obs4] ) >>> quantities_observations = [10, 20] >>> >>> prob = test.log_prob(observation_tuple, quantities_observations)", "response": "def log_prob(self,observations_list, quantities): \n        \"\"\" Finds Weighted log probability of a list of observation sequences\n\n        **Arguments**:\n\n        :param observation_list: A nested list, or a list of lists  \n        :type observation_list: Contains a list  multiple observation sequences.\n\n        :param quantities: Number of times, each corresponding item in  'observation_list' occurs.\n        :type quantities: A list of integers\n\n        :return: Weighted log probability of multiple observations. \n        :rtype: float\n            \n        **Features**:\n\n        Scaling applied here. This ensures that no underflow error occurs.\n\n        **Example**:\n\n        >>> states = ('s', 't')\n        >>> possible_observation = ('A','B' )\n        >>> # Numpy arrays of the data\n        >>> start_probability = np.matrix( '0.5 0.5 ')\n        >>> transition_probability = np.matrix('0.6 0.4 ;  0.3 0.7 ')\n        >>> emission_probability = np.matrix( '0.3 0.7 ; 0.4 0.6 ' )\n        >>> # Initialize class object\n        >>> test = hmm(states,possible_observation,start_probability,transition_probability,emission_probability)\n        >>> observations = ('A', 'B','B','A')\n        >>> obs4 = ('B', 'A','B')\n        >>> observation_tuple = []\n        >>> observation_tuple.extend( [observations,obs4] )\n        >>> quantities_observations = [10, 20]\n        >>>\n        >>> prob = test.log_prob(observation_tuple, quantities_observations)\n\n        \"\"\"\n\n        prob = 0\n        for q,obs in enumerate(observations_list):\n            temp,c_scale = self._alpha_cal(obs)\n            prob = prob +  -1 *  quantities[q] * np.sum(np.log(c_scale))\n        return prob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __fetch_data(self, url):\n        url += '&api_key=' + self.api_key\n        try:\n            response = urlopen(url)\n            root = ET.fromstring(response.read())\n        except HTTPError as exc:\n            root = ET.fromstring(exc.read())\n            raise ValueError(root.get('message'))\n        return root", "response": "helper function for fetching data given a request URL\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets information about a series such as frequency observation start end dates units notes etc.", "response": "def get_series_info(self, series_id):\n        \"\"\"\n        Get information about a series such as its title, frequency, observation start/end dates, units, notes, etc.\n\n        Parameters\n        ----------\n        series_id : str\n            Fred series id such as 'CPIAUCSL'\n\n        Returns\n        -------\n        info : Series\n            a pandas Series containing information about the Fred series\n        \"\"\"\n        url = \"%s/series?series_id=%s\" % (self.root_url, series_id)\n        root = self.__fetch_data(url)\n        if root is None or not len(root):\n            raise ValueError('No info exists for series id: ' + series_id)\n        info = pd.Series(root.getchildren()[0].attrib)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_series(self, series_id, observation_start=None, observation_end=None, **kwargs):\n        url = \"%s/series/observations?series_id=%s\" % (self.root_url, series_id)\n        if observation_start is not None:\n            observation_start = pd.to_datetime(observation_start,\n                                               errors='raise')\n            url += '&observation_start=' + observation_start.strftime('%Y-%m-%d')\n        if observation_end is not None:\n            observation_end = pd.to_datetime(observation_end, errors='raise')\n            url += '&observation_end=' + observation_end.strftime('%Y-%m-%d')\n        if kwargs.keys():\n            url += '&' + urlencode(kwargs)\n        root = self.__fetch_data(url)\n        if root is None:\n            raise ValueError('No data exists for series id: ' + series_id)\n        data = {}\n        for child in root.getchildren():\n            val = child.get('value')\n            if val == self.nan_char:\n                val = float('NaN')\n            else:\n                val = float(val)\n            data[self._parse(child.get('date'))] = val\n        return pd.Series(data)", "response": "Get data for a Fred series id. This is equivalent to get_series_latest_release but is equivalent to get_series_latest_release."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_series_first_release(self, series_id):\n        df = self.get_series_all_releases(series_id)\n        first_release = df.groupby('date').head(1)\n        data = first_release.set_index('date')['value']\n        return data", "response": "Get the first - release data for a Fred series id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_series_as_of_date(self, series_id, as_of_date):\n        as_of_date = pd.to_datetime(as_of_date)\n        df = self.get_series_all_releases(series_id)\n        data = df[df['realtime_start'] <= as_of_date]\n        return data", "response": "Get the latest data for a given Fred series id as known on a particular date."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_series_all_releases(self, series_id):\n        url = \"%s/series/observations?series_id=%s&realtime_start=%s&realtime_end=%s\" % (self.root_url,\n                                                                                         series_id,\n                                                                                         self.earliest_realtime_start,\n                                                                                         self.latest_realtime_end)\n        root = self.__fetch_data(url)\n        if root is None:\n            raise ValueError('No data exists for series id: ' + series_id)\n        data = {}\n        i = 0\n        for child in root.getchildren():\n            val = child.get('value')\n            if val == self.nan_char:\n                val = float('NaN')\n            else:\n                val = float(val)\n            realtime_start = self._parse(child.get('realtime_start'))\n            # realtime_end = self._parse(child.get('realtime_end'))\n            date = self._parse(child.get('date'))\n\n            data[i] = {'realtime_start': realtime_start,\n                       # 'realtime_end': realtime_end,\n                       'date': date,\n                       'value': val}\n            i += 1\n        data = pd.DataFrame(data).T\n        return data", "response": "Get all data for a given series id including first releases and all revisions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of vintage dates for a series.", "response": "def get_series_vintage_dates(self, series_id):\n        \"\"\"\n        Get a list of vintage dates for a series. Vintage dates are the dates in history when a\n        series' data values were revised or new data values were released.\n\n        Parameters\n        ----------\n        series_id : str\n            Fred series id such as 'CPIAUCSL'\n\n        Returns\n        -------\n        dates : list\n            list of vintage dates\n        \"\"\"\n        url = \"%s/series/vintagedates?series_id=%s\" % (self.root_url, series_id)\n        root = self.__fetch_data(url)\n        if root is None:\n            raise ValueError('No vintage date exists for series id: ' + series_id)\n        dates = []\n        for child in root.getchildren():\n            dates.append(self._parse(child.text))\n        return dates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __do_series_search(self, url):\n        root = self.__fetch_data(url)\n\n        series_ids = []\n        data = {}\n\n        num_results_returned = 0  # number of results returned in this HTTP request\n        num_results_total = int(root.get('count'))  # total number of results, this can be larger than number of results returned\n        for child in root.getchildren():\n            num_results_returned += 1\n            series_id = child.get('id')\n            series_ids.append(series_id)\n            data[series_id] = {\"id\": series_id}\n            fields = [\"realtime_start\", \"realtime_end\", \"title\", \"observation_start\", \"observation_end\",\n                      \"frequency\", \"frequency_short\", \"units\", \"units_short\", \"seasonal_adjustment\",\n                      \"seasonal_adjustment_short\", \"last_updated\", \"popularity\", \"notes\"]\n            for field in fields:\n                data[series_id][field] = child.get(field)\n\n        if num_results_returned > 0:\n            data = pd.DataFrame(data, columns=series_ids).T\n            # parse datetime columns\n            for field in [\"realtime_start\", \"realtime_end\", \"observation_start\", \"observation_end\", \"last_updated\"]:\n                data[field] = data[field].apply(self._parse, format=None)\n            # set index name\n            data.index.name = 'series id'\n        else:\n            data = None\n        return data, num_results_total", "response": "helper function for making one HTTP request for data and parsing the returned results into a DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndoing a fulltext search for series in the Fred dataset. Returns a DataFrame containing information about the matching Fred series in a DataFrame.", "response": "def search(self, text, limit=1000, order_by=None, sort_order=None, filter=None):\n        \"\"\"\n        Do a fulltext search for series in the Fred dataset. Returns information about matching series in a DataFrame.\n\n        Parameters\n        ----------\n        text : str\n            text to do fulltext search on, e.g., 'Real GDP'\n        limit : int, optional\n            limit the number of results to this value. If limit is 0, it means fetching all results without limit.\n        order_by : str, optional\n            order the results by a criterion. Valid options are 'search_rank', 'series_id', 'title', 'units', 'frequency',\n            'seasonal_adjustment', 'realtime_start', 'realtime_end', 'last_updated', 'observation_start', 'observation_end',\n            'popularity'\n        sort_order : str, optional\n            sort the results by ascending or descending order. Valid options are 'asc' or 'desc'\n        filter : tuple, optional\n            filters the results. Expects a tuple like (filter_variable, filter_value).\n            Valid filter_variable values are 'frequency', 'units', and 'seasonal_adjustment'\n\n        Returns\n        -------\n        info : DataFrame\n            a DataFrame containing information about the matching Fred series\n        \"\"\"\n        url = \"%s/series/search?search_text=%s&\" % (self.root_url,\n                                                    quote_plus(text))\n        info = self.__get_search_results(url, limit, order_by, sort_order, filter)\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for series that belongs to a given release id. Returns a DataFrame containing information about the matching Fred series.", "response": "def search_by_release(self, release_id, limit=0, order_by=None, sort_order=None, filter=None):\n        \"\"\"\n        Search for series that belongs to a release id. Returns information about matching series in a DataFrame.\n\n        Parameters\n        ----------\n        release_id : int\n            release id, e.g., 151\n        limit : int, optional\n            limit the number of results to this value. If limit is 0, it means fetching all results without limit.\n        order_by : str, optional\n            order the results by a criterion. Valid options are 'search_rank', 'series_id', 'title', 'units', 'frequency',\n            'seasonal_adjustment', 'realtime_start', 'realtime_end', 'last_updated', 'observation_start', 'observation_end',\n            'popularity'\n        sort_order : str, optional\n            sort the results by ascending or descending order. Valid options are 'asc' or 'desc'\n        filter : tuple, optional\n            filters the results. Expects a tuple like (filter_variable, filter_value).\n            Valid filter_variable values are 'frequency', 'units', and 'seasonal_adjustment'\n\n        Returns\n        -------\n        info : DataFrame\n            a DataFrame containing information about the matching Fred series\n        \"\"\"\n        url = \"%s/release/series?release_id=%d\" % (self.root_url, release_id)\n        info = self.__get_search_results(url, limit, order_by, sort_order, filter)\n        if info is None:\n            raise ValueError('No series exists for release id: ' + str(release_id))\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch for series that belongs to a category id. Returns a DataFrame containing information about the matching Fred series.", "response": "def search_by_category(self, category_id, limit=0, order_by=None, sort_order=None, filter=None):\n        \"\"\"\n        Search for series that belongs to a category id. Returns information about matching series in a DataFrame.\n\n        Parameters\n        ----------\n        category_id : int\n            category id, e.g., 32145\n        limit : int, optional\n            limit the number of results to this value. If limit is 0, it means fetching all results without limit.\n        order_by : str, optional\n            order the results by a criterion. Valid options are 'search_rank', 'series_id', 'title', 'units', 'frequency',\n            'seasonal_adjustment', 'realtime_start', 'realtime_end', 'last_updated', 'observation_start', 'observation_end',\n            'popularity'\n        sort_order : str, optional\n            sort the results by ascending or descending order. Valid options are 'asc' or 'desc'\n        filter : tuple, optional\n            filters the results. Expects a tuple like (filter_variable, filter_value).\n            Valid filter_variable values are 'frequency', 'units', and 'seasonal_adjustment'\n\n        Returns\n        -------\n        info : DataFrame\n            a DataFrame containing information about the matching Fred series\n        \"\"\"\n        url = \"%s/category/series?category_id=%d&\" % (self.root_url,\n                                                      category_id)\n        info = self.__get_search_results(url, limit, order_by, sort_order, filter)\n        if info is None:\n            raise ValueError('No series exists for category id: ' + str(category_id))\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(self, name, subject, expires=None, algorithm=None, parent=None, pathlen=None,\n             issuer_url=None, issuer_alt_name='', crl_url=None, ocsp_url=None,\n             ca_issuer_url=None, ca_crl_url=None, ca_ocsp_url=None, name_constraints=None,\n             password=None, parent_password=None, ecc_curve=None, key_type='RSA', key_size=None,\n             extra_extensions=None):\n        \"\"\"Create a new certificate authority.\n\n        Parameters\n        ----------\n\n        name : str\n            The name of the CA. This is a human-readable string and is used for administrative purposes only.\n        subject : dict or str or :py:class:`~django_ca.subject.Subject`\n            Subject string, e.g. ``\"/CN=example.com\"`` or ``Subject(\"/CN=example.com\")``. The value is\n            actually passed to :py:class:`~django_ca.subject.Subject` if it is not already an instance of that\n            class.\n        expires : datetime, optional\n            Datetime for when this certificate authority will expire, defaults to\n            :ref:`CA_DEFAULT_EXPIRES <settings-ca-default-expires>`.\n        algorithm : str or :py:class:`~cg:cryptography.hazmat.primitives.hashes.HashAlgorithm`, optional\n            Hash algorithm used when signing the certificate, passed to\n            :py:func:`~django_ca.utils.parse_hash_algorithm`. The default is the value of the\n            :ref:`CA_DIGEST_ALGORITHM <settings-ca-digest-algorithm>` setting.\n        parent : :py:class:`~django_ca.models.CertificateAuthority`, optional\n            Parent certificate authority for the new CA. Passing this value makes the CA an intermediate\n            authority.\n        pathlen : int, optional\n            Value of the path length attribute for the :py:class:`~django_ca.extensions.BasicConstraints`\n            extension.\n        issuer_url : str\n            URL for the DER/ASN1 formatted certificate that is signing certificates.\n        issuer_alt_name : :py:class:`~django_ca.extensions.IssuerAlternativeName` or str, optional\n            IssuerAlternativeName used when signing certificates. If the value is not an instance of\n            :py:class:`~django_ca.extensions.IssuerAlternativeName`, it will be passed as argument to\n            the constructor of the class.\n        crl_url : list of str, optional\n            CRL URLs used for certificates signed by this CA.\n        ocsp_url : str, optional\n            OCSP URL used for certificates signed by this CA.\n        ca_issuer_url : str, optional\n            URL for the DER/ASN1 formatted certificate that is signing this CA. For intermediate CAs, this\n            would usually be the ``issuer_url`` of the parent CA.\n        ca_crl_url : list of str, optional\n            CRL URLs used for this CA. This value is only meaningful for intermediate CAs.\n        ca_ocsp_url : str, optional\n            OCSP URL used for this CA. This value is only meaningful for intermediate CAs.\n        name_constraints : list of lists or :py:class:`~django_ca.extensions.NameConstraints`\n            List of names that this CA can sign and/or cannot sign. The value is passed to\n            :py:class:`~django_ca.extensions.NameConstraints` if the value is not already an instance of that\n            class.\n        password : bytes, optional\n            Password to encrypt the private key with.\n        parent_password : bytes, optional\n            Password that the private key of the parent CA is encrypted with.\n        ecc_curve : str or EllipticCurve, optional\n            The elliptic curve to use for ECC type keys, passed verbatim to\n            :py:func:`~django_ca.utils.parse_key_curve`.\n        key_type: str, optional\n            The type of private key to generate, must be one of ``\"RSA\"``, ``\"DSA\"`` or ``\"ECC\"``, with\n            ``\"RSA\"`` being the default.\n        key_size : int, optional\n            Integer specifying the key size, must be a power of two (e.g. 2048, 4096, ...). Defaults to\n            the :ref:`CA_DEFAULT_KEY_SIZE <settings-ca-default-key-size>`, unused if ``key_type=\"ECC\"``.\n        extra_extensions : list of :py:class:`cg:cryptography.x509.Extension` or \\\n                :py:class:`django_ca.extensions.Extension`, optional\n            An optional list of additional extensions to add to the certificate.\n\n        Raises\n        ------\n\n        ValueError\n            For various cases of wrong input data (e.g. ``key_size`` not being the power of two).\n        PermissionError\n            If the private key file cannot be written to disk.\n        \"\"\"\n        # NOTE: Already verified by KeySizeAction, so these checks are only for when the Python API is used\n        #       directly.\n        if key_type != 'ECC':\n            if key_size is None:\n                key_size = ca_settings.CA_DEFAULT_KEY_SIZE\n\n            if not is_power2(key_size):\n                raise ValueError(\"%s: Key size must be a power of two\" % key_size)\n            elif key_size < ca_settings.CA_MIN_KEY_SIZE:\n                raise ValueError(\"%s: Key size must be least %s bits\" % (\n                    key_size, ca_settings.CA_MIN_KEY_SIZE))\n\n        algorithm = parse_hash_algorithm(algorithm)\n\n        # Normalize extensions to django_ca.extensions.Extension subclasses\n        if not isinstance(subject, Subject):\n            subject = Subject(subject)\n        if not isinstance(issuer_alt_name, IssuerAlternativeName):\n            issuer_alt_name = IssuerAlternativeName(issuer_alt_name)\n\n        pre_create_ca.send(\n            sender=self.model, name=name, key_size=key_size, key_type=key_type, algorithm=algorithm,\n            expires=expires, parent=parent, subject=subject, pathlen=pathlen, issuer_url=issuer_url,\n            issuer_alt_name=issuer_alt_name, crl_url=crl_url, ocsp_url=ocsp_url, ca_issuer_url=ca_issuer_url,\n            ca_crl_url=ca_crl_url, ca_ocsp_url=ca_ocsp_url, name_constraints=name_constraints,\n            password=password, parent_password=parent_password, extra_extensions=extra_extensions)\n\n        if key_type == 'DSA':\n            private_key = dsa.generate_private_key(key_size=key_size, backend=default_backend())\n        elif key_type == 'ECC':\n            ecc_curve = parse_key_curve(ecc_curve)\n            private_key = ec.generate_private_key(ecc_curve, default_backend())\n        else:\n            private_key = rsa.generate_private_key(public_exponent=65537, key_size=key_size,\n                                                   backend=default_backend())\n        public_key = private_key.public_key()\n\n        builder = get_cert_builder(expires)\n        builder = builder.public_key(public_key)\n        subject = subject.name\n\n        builder = builder.subject_name(subject)\n        builder = builder.add_extension(x509.BasicConstraints(ca=True, path_length=pathlen), critical=True)\n        builder = builder.add_extension(x509.KeyUsage(\n            key_cert_sign=True, crl_sign=True, digital_signature=False, content_commitment=False,\n            key_encipherment=False, data_encipherment=False, key_agreement=False, encipher_only=False,\n            decipher_only=False), critical=True)\n\n        subject_key_id = x509.SubjectKeyIdentifier.from_public_key(public_key)\n        builder = builder.add_extension(subject_key_id, critical=False)\n\n        if parent is None:\n            builder = builder.issuer_name(subject)\n            private_sign_key = private_key\n            aki = x509.AuthorityKeyIdentifier.from_issuer_public_key(public_key)\n        else:\n            builder = builder.issuer_name(parent.x509.subject)\n            private_sign_key = parent.key(parent_password)\n            aki = parent.get_authority_key_identifier()\n        builder = builder.add_extension(aki, critical=False)\n\n        for critical, ext in self.get_common_extensions(ca_issuer_url, ca_crl_url, ca_ocsp_url):\n            builder = builder.add_extension(ext, critical=critical)\n\n        if name_constraints:\n            if not isinstance(name_constraints, NameConstraints):\n                name_constraints = NameConstraints(name_constraints)\n\n            builder = builder.add_extension(**name_constraints.for_builder())\n\n        if extra_extensions:\n            builder = self._extra_extensions(builder, extra_extensions)\n\n        certificate = builder.sign(private_key=private_sign_key, algorithm=algorithm,\n                                   backend=default_backend())\n\n        # Normalize extensions for create()\n        if crl_url is not None:\n            crl_url = '\\n'.join(crl_url)\n        issuer_alt_name = issuer_alt_name.serialize()\n\n        ca = self.model(name=name, issuer_url=issuer_url, issuer_alt_name=issuer_alt_name,\n                        ocsp_url=ocsp_url, crl_url=crl_url, parent=parent)\n        ca.x509 = certificate\n        ca.private_key_path = ca_storage.generate_filename('%s.key' % ca.serial.replace(':', ''))\n        ca.save()\n\n        if password is None:\n            encryption = serialization.NoEncryption()\n        else:\n            encryption = serialization.BestAvailableEncryption(password)\n\n        pem = private_key.private_bytes(encoding=Encoding.PEM,\n                                        format=PrivateFormat.PKCS8,\n                                        encryption_algorithm=encryption)\n\n        # write private key to file\n        ca_storage.save(ca.private_key_path, ContentFile(pem))\n\n        post_create_ca.send(sender=self.model, ca=ca)\n        return ca", "response": "Initializes a new CA certificate authority."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a signed certificate from a CSR.", "response": "def sign_cert(self, ca, csr, expires=None, algorithm=None, subject=None, cn_in_san=True,\n                  csr_format=Encoding.PEM, subject_alternative_name=None, key_usage=None,\n                  extended_key_usage=None, tls_feature=None, ocsp_no_check=False, extra_extensions=None,\n                  password=None):\n        \"\"\"Create a signed certificate from a CSR.\n\n        **PLEASE NOTE:** This function creates the raw certificate and is usually not invoked directly. It is\n        called by :py:func:`Certificate.objects.init() <django_ca.managers.CertificateManager.init>`, which\n        passes along all parameters unchanged and saves the raw certificate to the database.\n\n        Parameters\n        ----------\n\n        ca : :py:class:`~django_ca.models.CertificateAuthority`\n            The certificate authority to sign the certificate with.\n        csr : str\n            A valid CSR. The format is given by the ``csr_format`` parameter.\n        expires : datetime, optional\n            Datetime for when this certificate will expire, defaults to the ``CA_DEFAULT_EXPIRES`` setting.\n        algorithm : str or :py:class:`~cg:cryptography.hazmat.primitives.hashes.HashAlgorithm`, optional\n            Hash algorithm used when signing the certificate, passed to\n            :py:func:`~django_ca.utils.parse_hash_algorithm`. The default is the value of the\n            :ref:`CA_DIGEST_ALGORITHM <settings-ca-digest-algorithm>` setting.\n        subject : dict or str or :py:class:`~django_ca.subject.Subject`\n            Subject string, e.g. ``\"/CN=example.com\"`` or ``Subject(\"/CN=example.com\")``.\n            The value is actually passed to :py:class:`~django_ca.subject.Subject` if it is not already an\n            instance of that class. If this value is not passed or if the value does not contain a CommonName,\n            the first value of the ``subject_alternative_name`` parameter is used as CommonName.\n        cn_in_san : bool, optional\n            Wether the CommonName should also be included as subjectAlternativeName. The default is\n            ``True``, but the parameter is ignored if no CommonName is given. This is typically set\n            to ``False`` when creating a client certificate, where the subjects CommonName has no\n            meaningful value as subjectAlternativeName.\n        csr_format : :py:class:`~cg:cryptography.hazmat.primitives.serialization.Encoding`, optional\n            The format of the CSR. The default is ``PEM``.\n        subject_alternative_name : list of str or :py:class:`~django_ca.extensions.SubjectAlternativeName`,\n            optional A list of alternative names for the certificate. The value is passed to\n            :py:class:`~django_ca.extensions.SubjectAlternativeName` if not already an instance of that class.\n        key_usage : str or dict or :py:class:`~django_ca.extensions.KeyUsage`, optional\n            Value for the ``keyUsage`` X509 extension. The value is passed to\n            :py:class:`~django_ca.extensions.KeyUsage` if not already an instance of that class.\n        extended_key_usage : str or dict or :py:class:`~django_ca.extensions.ExtendedKeyUsage`, optional\n            Value for the ``extendedKeyUsage`` X509 extension. The value is passed to\n            :py:class:`~django_ca.extensions.ExtendedKeyUsage` if not already an instance of that class.\n        tls_feature : str or dict or :py:class:`~django_ca.extensions.TLSFeature`, optional\n            Value for the ``TLSFeature`` X509 extension. The value is passed to\n            :py:class:`~django_ca.extensions.TLSFeature` if not already an instance of that class.\n        ocsp_no_check : bool, optional\n            Add the OCSPNoCheck flag, indicating that an OCSP client should trust this certificate for it's\n            lifetime. This value only makes sense if you intend to use the certificate for an OCSP responder,\n            the default is ``False``. See `RFC 6990, section 4.2.2.2.1\n            <https://tools.ietf.org/html/rfc6960#section-4.2.2.2>`_ for more information.\n        extra_extensions : list of :py:class:`cg:cryptography.x509.Extension` or \\\n                :py:class:`django_ca.extensions.Extension`, optional\n            An optional list of additional extensions to add to the certificate.\n        password : bytes, optional\n            Password used to load the private key of the certificate authority. If not passed, the private key\n            is assumed to be unencrypted.\n\n        Returns\n        -------\n\n        cryptography.x509.Certificate\n            The signed certificate.\n        \"\"\"\n        ########################\n        # Normalize parameters #\n        ########################\n        if subject is None:\n            subject = Subject()  # we need a subject instance so we can possibly add the CN\n        elif not isinstance(subject, Subject):\n            subject = Subject(subject)\n\n        if 'CN' not in subject and not subject_alternative_name:\n            raise ValueError(\"Must name at least a CN or a subjectAlternativeName.\")\n\n        algorithm = parse_hash_algorithm(algorithm)\n\n        # Normalize extensions to django_ca.extensions.Extension subclasses\n        if key_usage and not isinstance(key_usage, KeyUsage):\n            key_usage = KeyUsage(key_usage)\n        if extended_key_usage and not isinstance(extended_key_usage, ExtendedKeyUsage):\n            extended_key_usage = ExtendedKeyUsage(extended_key_usage)\n        if tls_feature and not isinstance(tls_feature, TLSFeature):\n            tls_feature = TLSFeature(tls_feature)\n\n        if not subject_alternative_name:\n            subject_alternative_name = SubjectAlternativeName([])\n        elif not isinstance(subject_alternative_name, SubjectAlternativeName):\n            subject_alternative_name = SubjectAlternativeName(subject_alternative_name)\n\n        # use first SAN as CN if CN is not set\n        if 'CN' not in subject:\n            subject['CN'] = subject_alternative_name.value[0].value\n        elif cn_in_san and 'CN' in subject:  # add CN to SAN if cn_in_san is True (default)\n            try:\n                cn_name = parse_general_name(subject['CN'])\n            except idna.IDNAError:\n                raise ValueError('%s: Could not parse CommonName as subjectAlternativeName.' % subject['CN'])\n            else:\n                if cn_name not in subject_alternative_name:\n                    subject_alternative_name.insert(0, cn_name)\n\n        ################\n        # Read the CSR #\n        ################\n        if csr_format == Encoding.PEM:\n            req = x509.load_pem_x509_csr(force_bytes(csr), default_backend())\n        elif csr_format == Encoding.DER:\n            req = x509.load_der_x509_csr(force_bytes(csr), default_backend())\n        else:\n            raise ValueError('Unknown CSR format passed: %s' % csr_format)\n\n        #########################\n        # Send pre-issue signal #\n        #########################\n        pre_issue_cert.send(sender=self.model, ca=ca, csr=csr, expires=expires, algorithm=algorithm,\n                            subject=subject, cn_in_san=cn_in_san, csr_format=csr_format,\n                            subject_alternative_name=subject_alternative_name, key_usage=key_usage,\n                            extended_key_usage=extended_key_usage, tls_featur=tls_feature,\n                            extra_extensions=extra_extensions, password=password)\n\n        #######################\n        # Generate public key #\n        #######################\n        public_key = req.public_key()\n\n        builder = get_cert_builder(expires)\n        builder = builder.public_key(public_key)\n        builder = builder.issuer_name(ca.x509.subject)\n        builder = builder.subject_name(subject.name)\n\n        # Add extensions\n        builder = builder.add_extension(x509.BasicConstraints(ca=False, path_length=None), critical=True)\n        builder = builder.add_extension(\n            x509.SubjectKeyIdentifier.from_public_key(public_key), critical=False)\n\n        # Get authorityKeyIdentifier from subjectKeyIdentifier from signing CA\n        builder = builder.add_extension(ca.get_authority_key_identifier(), critical=False)\n\n        for critical, ext in self.get_common_extensions(ca.issuer_url, ca.crl_url, ca.ocsp_url):\n            builder = builder.add_extension(ext, critical=critical)\n\n        if subject_alternative_name:\n            builder = builder.add_extension(**subject_alternative_name.for_builder())\n\n        if key_usage:\n            builder = builder.add_extension(**key_usage.for_builder())\n\n        if extended_key_usage:\n            builder = builder.add_extension(**extended_key_usage.for_builder())\n\n        if tls_feature:\n            builder = builder.add_extension(**tls_feature.for_builder())\n\n        if ca.issuer_alt_name:\n            issuer_alt_name = IssuerAlternativeName(ca.issuer_alt_name)\n            builder = builder.add_extension(**issuer_alt_name.for_builder())\n\n        if ocsp_no_check:\n            builder = builder.add_extension(**OCSPNoCheck().for_builder())\n\n        if extra_extensions:\n            builder = self._extra_extensions(builder, extra_extensions)\n\n        ###################\n        # Sign public key #\n        ###################\n        cert = builder.sign(private_key=ca.key(password), algorithm=algorithm, backend=default_backend())\n\n        return cert, req"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a signed certificate from a CSR and store it to the database.", "response": "def init(self, ca, csr, **kwargs):\n        \"\"\"Create a signed certificate from a CSR and store it to the database.\n\n        All parameters are passed on to :py:func:`Certificate.objects.sign_cert()\n        <django_ca.managers.CertificateManager.sign_cert>`.\n        \"\"\"\n\n        c = self.model(ca=ca)\n        c.x509, csr = self.sign_cert(ca, csr, **kwargs)\n        c.csr = csr.public_bytes(Encoding.PEM).decode('utf-8')\n        c.save()\n\n        post_issue_cert.send(sender=self.model, cert=c)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisables the delete selected admin action.", "response": "def get_actions(self, request):\n        \"\"\"Disable the \"delete selected\" admin action.\n\n        Otherwise the action is present even though has_delete_permission is False, it just doesn't\n        work.\n        \"\"\"\n        actions = super(CertificateMixin, self).get_actions(request)\n        actions.pop('delete_selected', '')\n        return actions"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets kwargs suitable for get_cert X509 keyword arguments from the given profile.", "response": "def get_cert_profile_kwargs(name=None):\n    \"\"\"Get kwargs suitable for get_cert X509 keyword arguments from the given profile.\"\"\"\n\n    if name is None:\n        name = ca_settings.CA_DEFAULT_PROFILE\n\n    profile = deepcopy(ca_settings.CA_PROFILES[name])\n    kwargs = {\n        'cn_in_san': profile['cn_in_san'],\n        'subject': get_default_subject(name=name),\n    }\n\n    key_usage = profile.get('keyUsage')\n    if key_usage and key_usage.get('value'):\n        kwargs['key_usage'] = KeyUsage(key_usage)\n    ext_key_usage = profile.get('extendedKeyUsage')\n    if ext_key_usage and ext_key_usage.get('value'):\n        kwargs['extended_key_usage'] = ExtendedKeyUsage(ext_key_usage)\n    tls_feature = profile.get('TLSFeature')\n    if tls_feature and tls_feature.get('value'):\n        kwargs['tls_feature'] = TLSFeature(tls_feature)\n    if profile.get('ocsp_no_check'):\n        kwargs['ocsp_no_check'] = profile['ocsp_no_check']\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a subject into the canonical form for distinguished names.", "response": "def format_name(subject):\n    \"\"\"Convert a subject into the canonical form for distinguished names.\n\n    This function does not take care of sorting the subject in any meaningful order.\n\n    Examples::\n\n        >>> format_name([('CN', 'example.com'), ])\n        '/CN=example.com'\n        >>> format_name([('CN', 'example.com'), ('O', \"My Organization\"), ])\n        '/CN=example.com/O=My Organization'\n    \"\"\"\n    if isinstance(subject, x509.Name):\n        subject = [(OID_NAME_MAPPINGS[s.oid], s.value) for s in subject]\n\n    return '/%s' % ('/'.join(['%s=%s' % (force_text(k), force_text(v)) for k, v in subject]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_general_name(name):\n\n    if isinstance(name, x509.DirectoryName):\n        value = format_name(name.value)\n    else:\n        value = name.value\n    return '%s:%s' % (SAN_NAME_MAPPINGS[type(name)], value)", "response": "Format a single general name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd colons after every second digit.", "response": "def add_colons(s):\n    \"\"\"Add colons after every second digit.\n\n    This function is used in functions to prettify serials.\n\n    >>> add_colons('teststring')\n    'te:st:st:ri:ng'\n    \"\"\"\n    return ':'.join([s[i:i + 2] for i in range(0, len(s), 2)])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert an integer to a hex - representation of the given serial.", "response": "def int_to_hex(i):\n    \"\"\"Create a hex-representation of the given serial.\n\n    >>> int_to_hex(12345678)\n    'BC:61:4E'\n    \"\"\"\n    s = hex(i)[2:].upper()\n    if six.PY2 is True and isinstance(i, long):  # pragma: only py2  # NOQA\n        # Strip the \"L\" suffix, since hex(1L) -> 0x1L.\n        # NOTE: Do not convert to int earlier. int(<very-large-long>) is still long\n        s = s[:-1]\n    return add_colons(s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a name as expected by OpenSSLs command line utilities.", "response": "def parse_name(name):\n    \"\"\"Parses a subject string as used in OpenSSLs command line utilities.\n\n    The ``name`` is expected to be close to the subject format commonly used by OpenSSL, for example\n    ``/C=AT/L=Vienna/CN=example.com/emailAddress=user@example.com``. The function does its best to be lenient\n    on deviations from the format, object identifiers are case-insensitive (e.g. ``cn`` is the same as ``CN``,\n    whitespace at the start and end is stripped and the subject does not have to start with a slash (``/``).\n\n    >>> parse_name('/CN=example.com')\n    [('CN', 'example.com')]\n    >>> parse_name('c=AT/l= Vienna/o=\"ex org\"/CN=example.com')\n    [('C', 'AT'), ('L', 'Vienna'), ('O', 'ex org'), ('CN', 'example.com')]\n\n    Dictionary keys are normalized to the values of :py:const:`OID_NAME_MAPPINGS` and keys will be sorted\n    based on x509 name specifications regardless of the given order:\n\n    >>> parse_name('L=\"Vienna / District\"/EMAILaddress=user@example.com')\n    [('L', 'Vienna / District'), ('emailAddress', 'user@example.com')]\n    >>> parse_name('/C=AT/CN=example.com') == parse_name('/CN=example.com/C=AT')\n    True\n\n    Due to the magic of :py:const:`NAME_RE`, the function even supports quoting strings and including slashes,\n    so strings like ``/OU=\"Org / Org Unit\"/CN=example.com`` will work as expected.\n\n    >>> parse_name('L=\"Vienna / District\"/CN=example.com')\n    [('L', 'Vienna / District'), ('CN', 'example.com')]\n\n    But note that it's still easy to trick this function, if you really want to. The following example is\n    *not* a valid subject, the location is just bogus, and whatever you were expecting as output, it's\n    certainly different:\n\n    >>> parse_name('L=\"Vienna \" District\"/CN=example.com')\n    [('L', 'Vienna'), ('CN', 'example.com')]\n\n    Examples of where this string is used are:\n\n    .. code-block:: console\n\n        # openssl req -new -key priv.key -out csr -utf8 -batch -sha256 -subj '/C=AT/CN=example.com'\n        # openssl x509 -in cert.pem -noout -subject -nameopt compat\n        /C=AT/L=Vienna/CN=example.com\n    \"\"\"\n    name = name.strip()\n    if not name:  # empty subjects are ok\n        return []\n\n    try:\n        items = [(NAME_CASE_MAPPINGS[t[0].upper()], force_text(t[2])) for t in NAME_RE.findall(name)]\n    except KeyError as e:\n        raise ValueError('Unknown x509 name field: %s' % e.args[0])\n\n    # Check that no OIDs not in MULTIPLE_OIDS occur more then once\n    for key, oid in NAME_OID_MAPPINGS.items():\n        if sum(1 for t in items if t[0] == key) > 1 and oid not in MULTIPLE_OIDS:\n            raise ValueError('Subject contains multiple \"%s\" fields' % key)\n\n    return sort_name(items)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a subject into a : py : class : x509. Name <cg : cryptography. x509. Name >.", "response": "def x509_name(name):\n    \"\"\"Parses a subject into a :py:class:`x509.Name <cg:cryptography.x509.Name>`.\n\n    If ``name`` is a string, :py:func:`parse_name` is used to parse it.\n\n    >>> x509_name('/C=AT/CN=example.com')\n    <Name(C=AT,CN=example.com)>\n    >>> x509_name([('C', 'AT'), ('CN', 'example.com')])\n    <Name(C=AT,CN=example.com)>\n    \"\"\"\n    if isinstance(name, six.string_types):\n        name = parse_name(name)\n\n    return x509.Name([x509.NameAttribute(NAME_OID_MAPPINGS[typ], force_text(value)) for typ, value in name])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_email(addr):\n    if '@' not in addr:\n        raise ValueError('Invalid email address: %s' % addr)\n\n    node, domain = addr.split('@', 1)\n    try:\n        domain = idna.encode(force_text(domain))\n    except idna.core.IDNAError:\n        raise ValueError('Invalid domain: %s' % domain)\n\n    return '%s@%s' % (node, force_text(domain))", "response": "Validate an email address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_general_name(name):\n    name = force_text(name)\n    typ = None\n    match = GENERAL_NAME_RE.match(name)\n    if match is not None:\n        typ, name = match.groups()\n        typ = typ.lower()\n\n    if typ is None:\n        if re.match('[a-z0-9]{2,}://', name):  # Looks like a URI\n            try:\n                return x509.UniformResourceIdentifier(name)\n            except Exception:  # pragma: no cover - this really accepts anything\n                pass\n\n        if '@' in name:  # Looks like an Email address\n            try:\n                return x509.RFC822Name(validate_email(name))\n            except Exception:\n                pass\n\n        if name.strip().startswith('/'):  # maybe it's a dirname?\n            return x509.DirectoryName(x509_name(name))\n\n        # Try to parse this as IPAddress/Network\n        try:\n            return x509.IPAddress(ip_address(name))\n        except ValueError:\n            pass\n        try:\n            return x509.IPAddress(ip_network(name))\n        except ValueError:\n            pass\n\n        # Try to encode as domain name. DNSName() does not validate the domain name, but this check will fail.\n        if name.startswith('*.'):\n            idna.encode(name[2:])\n        elif name.startswith('.'):\n            idna.encode(name[1:])\n        else:\n            idna.encode(name)\n\n        # Almost anything passes as DNS name, so this is our default fallback\n        return x509.DNSName(name)\n\n    if typ == 'uri':\n        return x509.UniformResourceIdentifier(name)\n    elif typ == 'email':\n        return x509.RFC822Name(validate_email(name))\n    elif typ == 'ip':\n        try:\n            return x509.IPAddress(ip_address(name))\n        except ValueError:\n            pass\n\n        try:\n            return x509.IPAddress(ip_network(name))\n        except ValueError:\n            pass\n\n        raise ValueError('Could not parse IP address.')\n    elif typ == 'rid':\n        return x509.RegisteredID(x509.ObjectIdentifier(name))\n    elif typ == 'othername':\n        regex = \"(.*);(.*):(.*)\"\n        if re.match(regex, name) is not None:\n            oid, asn_typ, val = re.match(regex, name).groups()\n            oid = x509.ObjectIdentifier(oid)\n            if asn_typ == 'UTF8':\n                val = val.encode('utf-8')\n            elif asn_typ == 'OctetString':\n                val = bytes(bytearray.fromhex(val))\n                val = OctetString(val).dump()\n            else:\n                raise ValueError('Unsupported ASN type in otherName: %s' % asn_typ)\n            val = force_bytes(val)\n            return x509.OtherName(oid, val)\n        else:\n            raise ValueError('Incorrect otherName format: %s' % name)\n    elif typ == 'dirname':\n        return x509.DirectoryName(x509_name(name))\n    else:\n        # Try to encode the domain name. DNSName() does not validate the domain name, but this\n        # check will fail.\n        if name.startswith('*.'):\n            idna.encode(name[2:])\n        elif name.startswith('.'):\n            idna.encode(name[1:])\n        else:\n            idna.encode(name)\n\n        return x509.DNSName(name)", "response": "Parse a name from the user input."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_hash_algorithm(value=None):\n    if value is None:\n        return ca_settings.CA_DIGEST_ALGORITHM\n    elif isinstance(value, type) and issubclass(value, hashes.HashAlgorithm):\n        return value()\n    elif isinstance(value, hashes.HashAlgorithm):\n        return value\n    elif isinstance(value, six.string_types):\n        try:\n            return getattr(hashes, value.strip())()\n        except AttributeError:\n            raise ValueError('Unknown hash algorithm: %s' % value)\n    else:\n        raise ValueError('Unknown type passed: %s' % type(value).__name__)", "response": "Parse a hash algorithm value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a value to a valid encoding.", "response": "def parse_encoding(value=None):\n    \"\"\"Parse a value to a valid encoding.\n\n    This function accepts either a member of\n    :py:class:`~cg:cryptography.hazmat.primitives.serialization.Encoding` or a string describing a member. If\n    no value is passed, it will assume ``PEM`` as a default value. Note that ``\"ASN1\"`` is treated as an alias\n    for ``\"DER\"``.\n\n        >>> parse_encoding()\n        <Encoding.PEM: 'PEM'>\n        >>> parse_encoding('DER')\n        <Encoding.DER: 'DER'>\n        >>> parse_encoding(Encoding.PEM)\n        <Encoding.PEM: 'PEM'>\n    \"\"\"\n    if value is None:\n        return ca_settings.CA_DEFAULT_ENCODING\n    elif isinstance(value, Encoding):\n        return value\n    elif isinstance(value, six.string_types):\n        if value == 'ASN1':\n            value = 'DER'\n\n        try:\n            return getattr(Encoding, value)\n        except AttributeError:\n            raise ValueError('Unknown encoding: %s' % value)\n    else:\n        raise ValueError('Unknown type passed: %s' % type(value).__name__)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses an elliptic curve value.", "response": "def parse_key_curve(value=None):\n    \"\"\"Parse an elliptic curve value.\n\n    This function uses a value identifying an elliptic curve to return an\n    :py:class:`~cg:cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve` instance. The name must match a\n    class name of one of the classes named under \"Elliptic Curves\" in\n    :any:`cg:hazmat/primitives/asymmetric/ec`.\n\n    For convenience, passing ``None`` will return the value of :ref:`CA_DEFAULT_ECC_CURVE\n    <settings-ca-default-ecc-curve>`, and passing an\n    :py:class:`~cg:cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve` will return that instance\n    unchanged.\n\n    Example usage::\n\n        >>> parse_key_curve('SECP256R1')  # doctest: +ELLIPSIS\n        <cryptography.hazmat.primitives.asymmetric.ec.SECP256R1 object at ...>\n        >>> parse_key_curve('SECP384R1')  # doctest: +ELLIPSIS\n        <cryptography.hazmat.primitives.asymmetric.ec.SECP384R1 object at ...>\n        >>> parse_key_curve(ec.SECP256R1())  # doctest: +ELLIPSIS\n        <cryptography.hazmat.primitives.asymmetric.ec.SECP256R1 object at ...>\n        >>> parse_key_curve()  # doctest: +ELLIPSIS\n        <cryptography.hazmat.primitives.asymmetric.ec.SECP256R1 object at ...>\n\n    Parameters\n    ----------\n\n    value : str, otional\n        The name of the curve or ``None`` to return the default curve.\n\n    Returns\n    -------\n\n    curve\n        An :py:class:`~cg:cryptography.hazmat.primitives.asymmetric.ec.EllipticCurve` instance.\n\n    Raises\n    ------\n\n    ValueError\n        If the named curve is not supported.\n    \"\"\"\n    if isinstance(value, ec.EllipticCurve):\n        return value  # name was already parsed\n    if value is None:\n        return ca_settings.CA_DEFAULT_ECC_CURVE\n\n    curve = getattr(ec, value.strip(), type)\n    if not issubclass(curve, ec.EllipticCurve):\n        raise ValueError('%s: Not a known Eliptic Curve' % value)\n    return curve()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a basic X509 cert builder object.", "response": "def get_cert_builder(expires):\n    \"\"\"Get a basic X509 cert builder object.\n\n    Parameters\n    ----------\n\n    expires : datetime\n        When this certificate will expire.\n    \"\"\"\n    now = datetime.utcnow().replace(second=0, microsecond=0)\n\n    if expires is None:\n        expires = get_expires(expires, now=now)\n    expires = expires.replace(second=0, microsecond=0)\n\n    builder = x509.CertificateBuilder()\n    builder = builder.not_valid_before(now)\n    builder = builder.not_valid_after(expires)\n    builder = builder.serial_number(x509.random_serial_number())\n\n    return builder"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wrap_file_exceptions():\n    try:\n        yield\n    except (PermissionError, FileNotFoundError):  # pragma: only py3\n        # In py3, we want to raise Exception unchanged, so there would be no need for this block.\n        # BUT (IOError, OSError) - see below - also matches, so we capture it here\n        raise\n    except (IOError, OSError) as e:  # pragma: only py2\n        if e.errno == errno.EACCES:\n            raise PermissionError(str(e))\n        elif e.errno == errno.ENOENT:\n            raise FileNotFoundError(str(e))\n        raise", "response": "Contextmanager to wrap file exceptions into identicaly exceptions in py2 and py3."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_file(path):\n    if os.path.isabs(path):\n        with wrap_file_exceptions():\n            with open(path, 'rb') as stream:\n                return stream.read()\n\n    with wrap_file_exceptions():\n        stream = ca_storage.open(path)\n\n    try:\n        return stream.read()\n    finally:\n        stream.close()", "response": "Read the file from the given path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_extension_name(ext):\n\n    # In cryptography 2.2, SCTs return \"Unknown OID\"\n    if ext.oid == ExtensionOID.PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS:\n        return 'SignedCertificateTimestampList'\n\n    # Until at least cryptography 2.6.1, PrecertPoison has no name\n    #   https://github.com/pyca/cryptography/issues/4817\n    elif ca_settings.CRYPTOGRAPHY_HAS_PRECERT_POISON:  # pragma: no branch, pragma: only cryptography>=2.4\n        if ext.oid == ExtensionOID.PRECERT_POISON:\n            return 'PrecertPoison'\n\n    # uppercase the FIRST letter only (\"keyUsage\" -> \"KeyUsage\")\n    return re.sub('^([a-z])', lambda x: x.groups()[0].upper(), ext.oid._name)", "response": "Function to get the name of an extension."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting a string into a list of characters.", "response": "def shlex_split(s, sep):\n    \"\"\"Split a character on the given set of characters.\n\n    Example::\n\n        >>> shlex_split('foo,bar', ', ')\n        ['foo', 'bar']\n        >>> shlex_split('foo\\\\\\\\,bar1', ',')  # escape a separator\n        ['foo,bar1']\n        >>> shlex_split('\"foo,bar\", bla', ', ')\n        ['foo,bar', 'bla']\n        >>> shlex_split('foo,\"bar,bla\"', ',')\n        ['foo', 'bar,bla']\n    \"\"\"\n    lex = shlex.shlex(s, posix=True)\n    lex.whitespace = sep\n    lex.whitespace_split = True\n    return [l for l in lex]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the revocation reason of this certificate.", "response": "def get_revocation_reason(self):\n        \"\"\"Get the revocation reason of this certificate.\"\"\"\n        if self.revoked is False:\n            return\n\n        if self.revoked_reason == '' or self.revoked_reason is None:\n            return x509.ReasonFlags.unspecified\n        else:\n            return getattr(x509.ReasonFlags, self.revoked_reason)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the revocation time as naive datetime.", "response": "def get_revocation_time(self):\n        \"\"\"Get the revocation time as naive datetime.\n\n        Note that this method is only used by cryptography>=2.4.\n        \"\"\"\n        if self.revoked is False:\n            return\n\n        if timezone.is_aware(self.revoked_date):\n            # convert datetime object to UTC and make it naive\n            return timezone.make_naive(self.revoked_date, pytz.utc)\n\n        return self.revoked_date"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef x509(self):\n        if self._x509 is None:\n            backend = default_backend()\n            self._x509 = x509.load_pem_x509_certificate(force_bytes(self.pub), backend)\n        return self._x509", "response": "The underlying cryptography. x509. Certificate object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subject(self):\n        return Subject([(s.oid, s.value) for s in self.x509.subject])", "response": "The certificates subject as : py : class ~django_ca. subject. Subject."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extended_key_usage(self):\n        try:\n            ext = self.x509.extensions.get_extension_for_oid(ExtensionOID.EXTENDED_KEY_USAGE)\n        except x509.ExtensionNotFound:\n            return None\n        return ExtendedKeyUsage(ext)", "response": "The ExtendedKeyUsage extension or None if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tls_feature(self):\n        try:\n            ext = self.x509.extensions.get_extension_for_oid(ExtensionOID.TLS_FEATURE)\n        except x509.ExtensionNotFound:\n            return None\n        return TLSFeature(ext)", "response": "The TLSFeature extension or None if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_authority_key_identifier(self):\n\n        try:\n            ski = self.x509.extensions.get_extension_for_class(x509.SubjectKeyIdentifier)\n        except x509.ExtensionNotFound:\n            return x509.AuthorityKeyIdentifier.from_issuer_public_key(self.x509.public_key())\n        else:\n            return x509.AuthorityKeyIdentifier.from_issuer_subject_key_identifier(ski)", "response": "Return the AuthorityKeyIdentifier extension used in certificates signed by this CA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_crl(self, expires=86400, encoding=None, algorithm=None, password=None, scope=None, **kwargs):\n\n        if scope is not None and scope not in ['ca', 'user', 'attribute']:\n            raise ValueError('Scope must be either None, \"ca\", \"user\" or \"attribute\"')\n        encoding = parse_encoding(encoding)\n\n        now = now_builder = timezone.now()\n        algorithm = parse_hash_algorithm(algorithm)\n\n        if timezone.is_aware(now_builder):\n            now_builder = timezone.make_naive(now, pytz.utc)\n\n        builder = x509.CertificateRevocationListBuilder()\n        builder = builder.issuer_name(self.x509.subject)\n        builder = builder.last_update(now_builder)\n        builder = builder.next_update(now_builder + timedelta(seconds=expires))\n\n        if 'full_name' in kwargs:\n            full_name = kwargs['full_name']\n            full_name = [parse_general_name(n) for n in full_name]\n        elif self.crl_url:\n            crl_url = [url.strip() for url in self.crl_url.split()]\n            full_name = [x509.UniformResourceIdentifier(c) for c in crl_url]\n        else:\n            full_name = None\n\n        # Keyword arguments for the IssuingDistributionPoint extension\n        idp_kwargs = {\n            'only_contains_ca_certs': False,\n            'only_contains_user_certs': False,\n            'indirect_crl': False,\n            'only_contains_attribute_certs': False,\n            'only_some_reasons': None,\n            'full_name': full_name,\n            'relative_name': kwargs.get('relative_name'),\n        }\n\n        ca_qs = self.children.filter(expires__gt=now).revoked()\n        cert_qs = self.certificate_set.filter(expires__gt=now).revoked()\n\n        if scope == 'ca':\n            certs = ca_qs\n            idp_kwargs['only_contains_ca_certs'] = True\n        elif scope == 'user':\n            certs = cert_qs\n            idp_kwargs['only_contains_user_certs'] = True\n        elif scope == 'attribute':\n            # sorry, nothing we support right now\n            certs = []\n            idp_kwargs['only_contains_attribute_certs'] = True\n        else:\n            certs = itertools.chain(ca_qs, cert_qs)\n\n        for cert in certs:\n            builder = builder.add_revoked_certificate(cert.get_revocation())\n\n        if ca_settings.CRYPTOGRAPHY_HAS_IDP:  # pragma: no branch, pragma: only cryptography>=2.5\n            builder = builder.add_extension(x509.IssuingDistributionPoint(**idp_kwargs), critical=True)\n\n        # TODO: Add CRLNumber extension\n        #   https://cryptography.io/en/latest/x509/reference/#cryptography.x509.CRLNumber\n\n        crl = builder.sign(private_key=self.key(password), algorithm=algorithm, backend=default_backend())\n        return crl.public_bytes(encoding)", "response": "Generate a CRL for the current time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pathlen(self):\n\n        try:\n            ext = self.x509.extensions.get_extension_for_oid(ExtensionOID.BASIC_CONSTRAINTS)\n        except x509.ExtensionNotFound:  # pragma: no cover - extension should always be present\n            return None\n        return ext.value.path_length", "response": "The pathlen attribute of the BasicConstraints extension either an int or None."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bundle(self):\n        ca = self\n        bundle = [ca]\n\n        while ca.parent is not None:\n            bundle.append(ca.parent)\n            ca = ca.parent\n        return bundle", "response": "A list of any parent CAs including this CA."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the -- algorithm option.", "response": "def add_algorithm(self, parser):\n        \"\"\"Add the --algorithm option.\"\"\"\n\n        help = 'The HashAlgorithm that will be used to generate the signature (default: %(default)s).' % {\n            'default': ca_settings.CA_DIGEST_ALGORITHM.name, }\n\n        parser.add_argument(\n            '--algorithm', metavar='{sha512,sha256,...}', default=ca_settings.CA_DIGEST_ALGORITHM,\n            action=AlgorithmAction, help=help)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd the - f option.", "response": "def add_format(self, parser, default=Encoding.PEM, help_text=None, opts=None):\n        \"\"\"Add the --format option.\"\"\"\n\n        if opts is None:\n            opts = ['-f', '--format']\n        if help_text is None:\n            help_text = 'The format to use (\"ASN1\" is an alias for \"DER\", default: %(default)s).'\n        help_text = help_text % {'default': default.name}\n        parser.add_argument(*opts, metavar='{PEM,ASN1,DER}', default=default,\n                            action=FormatAction, help=help_text)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a module and return a tuple of the module ID and permission values.", "response": "def process_module(self, node):\n        '''\n        process a module\n        '''\n\n        for listing in self.config.fileperms_ignore_paths:\n            if node.file.split('{0}/'.format(os.getcwd()))[-1] in glob.glob(listing):\n                # File is ignored, no checking should be done\n                return\n\n        desired_perm = self.config.fileperms_default\n        if '-' in desired_perm:\n            desired_perm = desired_perm.split('-')\n        else:\n            desired_perm = [desired_perm]\n\n        if len(desired_perm) > 2:\n            raise RuntimeError('Permission ranges should be like XXXX-YYYY')\n\n        for idx, perm in enumerate(desired_perm):\n            desired_perm[idx] = desired_perm[idx].strip('\"').strip('\\'').lstrip('0').zfill(4)\n            if desired_perm[idx][0] != '0':\n                # Always include a leading zero\n                desired_perm[idx] = '0{0}'.format(desired_perm[idx])\n            if sys.version_info > (3,):\n                # The octal representation in python 3 has changed to 0o644 instead of 0644\n                if desired_perm[idx][1] != 'o':\n                    desired_perm[idx] = '0o' + desired_perm[idx][1:]\n            if sys.platform.startswith('win'):\n                # Windows does not distinguish between user/group/other.\n                # They must all be the same. Also, Windows will automatically\n                # set the execution bit on files with a known extension\n                # (eg .exe, .bat, .com). So we cannot reliably test the\n                # execution bit on other files such as .py files.\n                user_perm_noexec = int(desired_perm[idx][-3])\n                if user_perm_noexec % 2 == 1:\n                    user_perm_noexec -= 1\n                desired_perm[idx] = desired_perm[idx][:-3] + (str(user_perm_noexec) * 3)\n\n        module_perms = oct(stat.S_IMODE(os.stat(node.file).st_mode))\n        if sys.version_info < (3,):\n            module_perms = str(module_perms)\n\n        if len(desired_perm) == 1:\n            if module_perms != desired_perm[0]:\n                if sys.platform.startswith('win'):\n                    # Check the variant with execution bit set due to the\n                    # unreliability of checking the execution bit on Windows.\n                    user_perm_noexec = int(desired_perm[0][-3])\n                    desired_perm_exec = desired_perm[0][:-3] + (str(user_perm_noexec + 1) * 3)\n                    if module_perms == desired_perm_exec:\n                        return\n                self.add_message('E0599', line=1, args=(desired_perm[0], module_perms))\n        else:\n            if module_perms < desired_perm[0] or module_perms > desired_perm[1]:\n                if sys.platform.startswith('win'):\n                    # Check the variant with execution bit set due to the\n                    # unreliability of checking the execution bit on Windows.\n                    user_perm_noexec0 = int(desired_perm[0][-3])\n                    desired_perm_exec0 = desired_perm[0][:-3] + (str(user_perm_noexec0 + 1) * 3)\n                    user_perm_noexec1 = int(desired_perm[1][-3])\n                    desired_perm_exec1 = desired_perm[1][:-3] + (str(user_perm_noexec1 + 1) * 3)\n                    if desired_perm_exec0 <= module_perms <= desired_perm_exec1:\n                        return\n                desired_perm = '>= {0} OR <= {1}'.format(*desired_perm)\n                self.add_message('E0599', line=1, args=(desired_perm, module_perms))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse requirements. txt and return list suitable for passing to setup. install_requires parameter in setup.", "response": "def _parse_requirements_file(requirements_file):\n    '''\n    Parse requirements.txt and return list suitable for\n    passing to ``install_requires`` parameter in ``setup()``.\n    '''\n    parsed_requirements = []\n    with open(requirements_file) as rfh:\n        for line in rfh.readlines():\n            line = line.strip()\n            if not line or line.startswith(('#', '-r')):\n                continue\n            parsed_requirements.append(line)\n    return parsed_requirements"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_module(self, node):\n        '''\n        process a module\n        '''\n\n        # Patch lib2to3.fixer_util.touch_import!\n        fixer_util.touch_import = salt_lib2to3_touch_import\n\n        flags = {}\n\n        if self.config.modernize_print_function:\n            flags['print_function'] = True\n\n        salt_avail_fixes = set(\n            refactor.get_fixers_from_package(\n                'saltpylint.py3modernize.fixes'\n            )\n        )\n        avail_fixes = set(refactor.get_fixers_from_package('libmodernize.fixes'))\n        avail_fixes.update(lib2to3_fix_names)\n        avail_fixes.update(salt_avail_fixes)\n\n        default_fixes = avail_fixes.difference(opt_in_fix_names)\n        unwanted_fixes = set(self.config.modernize_nofix)\n\n        # Explicitly disable libmodernize.fixes.fix_dict_six since we have our own implementation\n        # which only fixes `dict.iter<items|keys|values>()` calls\n        unwanted_fixes.add('libmodernize.fixes.fix_dict_six')\n\n        if self.config.modernize_six_unicode:\n            unwanted_fixes.add('libmodernize.fixes.fix_unicode_future')\n        elif self.config.modernize_future_unicode:\n            unwanted_fixes.add('libmodernize.fixes.fix_unicode')\n        else:\n            unwanted_fixes.add('libmodernize.fixes.fix_unicode_future')\n            unwanted_fixes.add('libmodernize.fixes.fix_unicode')\n\n        if self.config.modernize_no_six:\n            unwanted_fixes.update(six_fix_names)\n            unwanted_fixes.update(salt_avail_fixes)\n        else:\n            # We explicitly will remove fix_imports_six from libmodernize and will add\n            # our own fix_imports_six\n            unwanted_fixes.add('libmodernize.fixes.fix_imports_six')\n            # Remove a bunch of libmodernize.fixes since we need to properly skip them\n            # and we provide the proper skip rule\n            unwanted_fixes.add('libmodernize.fixes.fix_input_six')\n            unwanted_fixes.add('libmodernize.fixes.fix_filter')\n            unwanted_fixes.add('libmodernize.fixes.fix_map')\n            unwanted_fixes.add('libmodernize.fixes.fix_xrange_six')\n            unwanted_fixes.add('libmodernize.fixes.fix_zip')\n\n        explicit = set()\n\n        if self.config.modernize_fix:\n            default_present = False\n            for fix in self.config.modernize_fix:\n                if fix == 'default':\n                    default_present = True\n                else:\n                    explicit.add(fix)\n\n            requested = default_fixes.union(explicit) if default_present else explicit\n        else:\n            requested = default_fixes\n\n        requested = default_fixes\n        fixer_names = requested.difference(unwanted_fixes)\n\n        rft = PyLintRefactoringTool(sorted(fixer_names), flags, sorted(explicit))\n        try:\n            rft.refactor_file(node.file,\n                              write=False,\n                              doctests_only=self.config.modernize_doctests_only)\n        except ParseError as exc:\n            # Unable to refactor, let's not make PyLint crash\n            try:\n                lineno = exc.context[1][0]\n                line_contents = node.file_stream.readlines()[lineno-1].rstrip()\n                self.add_message('W1698', line=lineno, args=line_contents)\n            except Exception:  # pylint: disable=broad-except\n                self.add_message('W1698', line=1, args=exc)\n            return\n        except AssertionError as exc:\n            self.add_message('W1698', line=1, args=exc)\n            return\n        except (IOError, OSError) as exc:\n            logging.getLogger(__name__).warn('Error while processing {0}: {1}'.format(node.file, exc))\n            return\n\n        for lineno, diff in rft.diff:\n            # Since PyLint's python3 checker uses <Type>16<int><int>, we'll also use that range\n            self.add_message('W1699', line=lineno, args=diff)\n\n        # Restore lib2to3.fixer_util.touch_import!\n        fixer_util.touch_import = FIXER_UTIL_TOUCH_IMPORT", "response": "Process a module node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the node is a function or a method and if so adds a message to the log if not", "response": "def visit_functiondef(self, node):\n        '''\n        Verifies no logger statements inside __virtual__\n        '''\n        if (not isinstance(node, astroid.FunctionDef) or\n            node.is_method()\n            or node.type != 'function'\n            or not node.body\n           ):\n            # only process functions\n            return\n\n        try:\n            if not node.name == '__virtual__':\n                # only need to process the __virtual__ function\n                return\n        except AttributeError:\n            return\n\n        # walk contents of __virtual__ function\n        for child in node.get_children():\n            for functions in child.get_children():\n                if isinstance(functions, astroid.Call):\n                    if isinstance(functions.func, astroid.Attribute):\n                        try:\n                            # Inspect each statement for an instance of 'logging'\n                            for inferred in functions.func.expr.infer():\n                                try:\n                                    instance_type = inferred.pytype().split('.')[0]\n                                except TypeError:\n                                    continue\n                                if instance_type == 'logging':\n                                    self.add_message(\n                                        self.VIRT_LOG, node=functions\n                                    )\n                                    # Found logger, don't need to keep processing this line\n                                    break\n                        except AttributeError:\n                            # Not a log function\n                            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_module(self, node):\n        '''\n        process a module\n\n        the module's content is accessible via node.file_stream object\n        '''\n        pep263 = re.compile(six.b(self.RE_PEP263))\n\n        try:\n            file_stream = node.file_stream\n        except AttributeError:\n            # Pylint >= 1.8.1\n            file_stream = node.stream()\n\n        # Store a reference to the node's file stream position\n        current_stream_position = file_stream.tell()\n\n        # Go to the start of stream to achieve our logic\n        file_stream.seek(0)\n\n        # Grab the first two lines\n        twolines = list(itertools.islice(file_stream, 2))\n        pep263_encoding = [m.group(1).lower() for l in twolines for m in [pep263.search(l)] if m]\n\n        multiple_encodings = len(pep263_encoding) > 1\n        file_empty = len(twolines) == 0\n\n        # Reset the node's file stream position\n        file_stream.seek(current_stream_position)\n\n        # - If the file has an UTF-8 BOM and yet uses any other\n        #   encoding, it will be caught by F0002\n        # - If the file has a PEP263 UTF-8 encoding and yet uses any\n        #   other encoding, it will be caught by W0512\n        # - If there are non-ASCII characters and no PEP263, or UTF-8\n        #   BOM, it will be caught by W0512\n        # - If there are ambiguous PEP263 encodings it will be caught\n        #   by E0001, we still test for this\n        if multiple_encodings:\n            self.add_message('W9901', line=1)\n\n        if node.file_encoding:\n            pylint_encoding = node.file_encoding.lower()\n            if six.PY3:\n                pylint_encoding = pylint_encoding.encode('utf-8')\n            if pep263_encoding and pylint_encoding not in pep263_encoding:\n                self.add_message('W9902', line=1)\n        if not pep263_encoding:\n            if file_empty:\n                self.add_message('W9905', line=1)\n            else:\n                self.add_message('W9903', line=1)\n        elif self.REQ_ENCOD not in pep263_encoding:\n            self.add_message('W9904', line=1)", "response": "Process a module s content and store the entry in the object store."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_versions(source):\n    tree = compiler.parse(source)\n    checker = compiler.walk(tree, NodeChecker())\n    return checker.vers", "response": "Return information about the Python versions required for specific features."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrequiring method to auto register this checker", "response": "def register(linter):\n    '''required method to auto register this checker '''\n    linter.register_checker(StringCurlyBracesFormatIndexChecker(linter))\n    linter.register_checker(StringLiteralChecker(linter))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a non - raw string token.", "response": "def process_non_raw_string_token(self, prefix, string_body, start_row):\n        '''\n        check for bad escapes in a non-raw string.\n\n        prefix: lowercase string of eg 'ur' string prefix markers.\n        string_body: the un-parsed body of the string, not including the quote\n        marks.\n        start_row: integer line number in the source.\n        '''\n        if 'u' in prefix:\n            if string_body.find('\\\\0') != -1:\n                self.add_message('null-byte-unicode-literal', line=start_row)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a checker for testing.", "response": "def register(linter):\n    '''\n    Required method to auto register this checker\n    '''\n    linter.register_checker(ResourceLeakageChecker(linter))\n    linter.register_checker(BlacklistedImportsChecker(linter))\n    linter.register_checker(MovedTestCaseClassChecker(linter))\n    linter.register_checker(BlacklistedLoaderModulesUsageChecker(linter))\n    linter.register_checker(BlacklistedFunctionsChecker(linter))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_import(self, node):\n        '''triggered when an import statement is seen'''\n        module_filename = node.root().file\n        if fnmatch.fnmatch(module_filename, '__init__.py*') and \\\n                not fnmatch.fnmatch(module_filename, 'test_*.py*'):\n            return\n        modnode = node.root()\n        names = [name for name, _ in node.names]\n\n        for name in names:\n            self._check_blacklisted_module(node, name)", "response": "triggered when an import statement is seen"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visit_importfrom(self, node):\n        '''triggered when a from statement is seen'''\n        module_filename = node.root().file\n        if fnmatch.fnmatch(module_filename, '__init__.py*') and \\\n                not fnmatch.fnmatch(module_filename, 'test_*.py*'):\n            return\n        basename = node.modname\n        self._check_blacklisted_module(node, basename)", "response": "triggered when a from statement is seen"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the module is blacklisted", "response": "def _check_blacklisted_module(self, node, mod_path):\n        '''check if the module is blacklisted'''\n        for mod_name in self.blacklisted_modules:\n            if mod_path == mod_name or mod_path.startswith(mod_name + '.'):\n                names = []\n                for name, name_as in node.names:\n                    if name_as:\n                        names.append('{0} as {1}'.format(name, name_as))\n                    else:\n                        names.append(name)\n                try:\n                    import_from_module = node.modname\n                    if import_from_module == 'salttesting.helpers':\n                        for name in names:\n                            if name == 'ensure_in_syspath':\n                                self.add_message('blacklisted-syspath-update', node=node)\n                                continue\n                            msg = 'Please use \\'from tests.support.helpers import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module in ('salttesting.mock', 'mock', 'unittest.mock', 'unittest2.mock'):\n                        for name in names:\n                            msg = 'Please use \\'from tests.support.mock import {0}\\''.format(name)\n                            if import_from_module in ('salttesting.mock', 'unittest.mock', 'unittest2.mock'):\n                                message_id = 'blacklisted-module'\n                            else:\n                                message_id = 'blacklisted-external-module'\n                            self.add_message(message_id, node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module == 'salttesting.parser':\n                        for name in names:\n                            msg = 'Please use \\'from tests.support.parser import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module == 'salttesting.case':\n                        for name in names:\n                            msg = 'Please use \\'from tests.support.case import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module == 'salttesting.unit':\n                        for name in names:\n                            msg = 'Please use \\'from tests.support.unit import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module.startswith(('unittest', 'unittest2')):\n                        for name in names:\n                            msg = 'Please use \\'from tests.support.unit import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module == 'salttesting.mixins':\n                        for name in names:\n                            msg = 'Please use \\'from tests.support.mixins import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module == 'six':\n                        for name in names:\n                            msg = 'Please use \\'from salt.ext.six import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if import_from_module == 'distutils.version':\n                        for name in names:\n                            msg = 'Please use \\'from salt.utils.versions import {0}\\''.format(name)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                        continue\n                    if names:\n                        for name in names:\n                            if name in ('TestLoader', 'TextTestRunner', 'TestCase', 'expectedFailure',\n                                        'TestSuite', 'skipIf', 'TestResult'):\n                                msg = 'Please use \\'from tests.support.unit import {0}\\''.format(name)\n                                self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                                continue\n                            if name in ('SaltReturnAssertsMixin', 'SaltMinionEventAssertsMixin'):\n                                msg = 'Please use \\'from tests.support.mixins import {0}\\''.format(name)\n                                self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                                continue\n                            if name in ('ModuleCase', 'SyndicCase', 'ShellCase', 'SSHCase'):\n                                msg = 'Please use \\'from tests.support.case import {0}\\''.format(name)\n                                self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                                continue\n                            if name == 'run_tests':\n                                msg = 'Please remove the \\'if __name__ == \"__main__\":\\' section from the end of the module'\n                                self.add_message('blacklisted-test-module-execution', node=node, args=msg)\n                                continue\n                            if mod_name in ('integration', 'unit'):\n                                if name in ('SYS_TMP_DIR',\n                                            'TMP',\n                                            'FILES',\n                                            'PYEXEC',\n                                            'MOCKBIN',\n                                            'SCRIPT_DIR',\n                                            'TMP_STATE_TREE',\n                                            'TMP_PRODENV_STATE_TREE',\n                                            'TMP_CONF_DIR',\n                                            'TMP_SUB_MINION_CONF_DIR',\n                                            'TMP_SYNDIC_MINION_CONF_DIR',\n                                            'TMP_SYNDIC_MASTER_CONF_DIR',\n                                            'CODE_DIR',\n                                            'TESTS_DIR',\n                                            'CONF_DIR',\n                                            'PILLAR_DIR',\n                                            'TMP_SCRIPT_DIR',\n                                            'ENGINES_DIR',\n                                            'LOG_HANDLERS_DIR',\n                                            'INTEGRATION_TEST_DIR'):\n                                    msg = 'Please use \\'from tests.support.paths import {0}\\''.format(name)\n                                    self.add_message('blacklisted-import', node=node, args=(mod_path, msg))\n                                    continue\n                                msg = 'Please use \\'from tests.{0} import {1}\\''.format(mod_path, name)\n                                self.add_message('blacklisted-import', node=node, args=(mod_path, msg))\n                                continue\n                            msg = 'Please report this error to SaltStack so we can fix it: Trying to import {0} from {1}'.format(name, mod_path)\n                            self.add_message('blacklisted-module', node=node, args=(mod_path, msg))\n                except AttributeError:\n                    if mod_name in ('integration', 'unit', 'mock', 'six', 'distutils.version',\n                                    'unittest', 'unittest2'):\n                        if mod_name in ('integration', 'unit'):\n                            msg = 'Please use \\'import tests.{0} as {0}\\''.format(mod_name)\n                            message_id = 'blacklisted-import'\n                        elif mod_name == 'mock':\n                            msg = 'Please use \\'import tests.support.{0} as {0}\\''.format(mod_name)\n                            message_id = 'blacklisted-external-import'\n                        elif mod_name == 'six':\n                            msg = 'Please use \\'import salt.ext.{0} as {0}\\''.format(name)\n                            message_id = 'blacklisted-external-import'\n                        elif mod_name == 'distutils.version':\n                            msg = 'Please use \\'import salt.utils.versions\\' instead'\n                            message_id = 'blacklisted-import'\n                        elif mod_name.startswith(('unittest', 'unittest2')):\n                            msg = 'Please use \\'import tests.support.unit as {}\\' instead'.format(mod_name)\n                            message_id = 'blacklisted-import'\n                        self.add_message(message_id, node=node, args=(mod_path, msg))\n                        continue\n                    msg = 'Please report this error to SaltStack so we can fix it: Trying to import {0}'.format(mod_path)\n                    self.add_message('blacklisted-import', node=node, args=(mod_path, msg))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visit_import(self, node):\n        '''triggered when an import statement is seen'''\n        if self.process_module:\n            # Store salt imported modules\n            for module, import_as in node.names:\n                if not module.startswith('salt'):\n                    continue\n                if import_as and import_as not in self.imported_salt_modules:\n                    self.imported_salt_modules[import_as] = module\n                    continue\n                if module not in self.imported_salt_modules:\n                    self.imported_salt_modules[module] = module", "response": "triggered when an import statement is seen"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visit_importfrom(self, node):\n        '''triggered when a from statement is seen'''\n        if self.process_module:\n            if not node.modname.startswith('salt'):\n                return\n            # Store salt imported modules\n            for module, import_as in node.names:\n                if import_as and import_as not in self.imported_salt_modules:\n                    self.imported_salt_modules[import_as] = import_as\n                    continue\n                if module not in self.imported_salt_modules:\n                    self.imported_salt_modules[module] = module", "response": "triggered when a from statement is seen"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering the transformation functions.", "response": "def register(linter):\n    '''\n    Register the transformation functions.\n    '''\n    try:\n        MANAGER.register_transform(nodes.Class, rootlogger_transform)\n    except AttributeError:\n        MANAGER.register_transform(nodes.ClassDef, rootlogger_transform)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(linter):\n    '''\n    required method to auto register this checker\n    '''\n    if HAS_PEP8 is False:\n        return\n\n    linter.register_checker(PEP8Indentation(linter))\n    linter.register_checker(PEP8Whitespace(linter))\n    linter.register_checker(PEP8BlankLine(linter))\n    linter.register_checker(PEP8Import(linter))\n    linter.register_checker(PEP8LineLength(linter))\n    linter.register_checker(PEP8Statement(linter))\n    linter.register_checker(PEP8Runtime(linter))\n    linter.register_checker(PEP8IndentationWarning(linter))\n    linter.register_checker(PEP8WhitespaceWarning(linter))\n    linter.register_checker(PEP8BlankLineWarning(linter))\n    linter.register_checker(PEP8DeprecationWarning(linter))", "response": "register checker in the given linter"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a module and return a dictionary of the module s content.", "response": "def process_module(self, node):\n        '''\n        process a module\n\n        the module's content is accessible via node.file_stream object\n        '''\n        nodepaths = []\n        if not isinstance(node.path, list):\n            nodepaths = [node.path]\n        else:\n            nodepaths = node.path\n\n        for node_path in nodepaths:\n            if node_path not in _PROCESSED_NODES:\n                stylechecker = StyleGuide(\n                    parse_argv=False, config_file=False, quiet=2,\n                    reporter=PyLintPEP8Reporter\n                )\n\n                _PROCESSED_NODES[node_path] = stylechecker.check_files([node_path])\n\n            for code, lineno, text in _PROCESSED_NODES[node_path].locations:\n                pylintcode = '{0}8{1}'.format(code[0], code[1:])\n                if pylintcode in self.msgs_map:\n                    # This will be handled by PyLint itself, skip it\n                    continue\n\n                if pylintcode not in _KNOWN_PEP8_IDS:\n                    if pylintcode not in _UNHANDLED_PEP8_IDS:\n                        _UNHANDLED_PEP8_IDS.append(pylintcode)\n                        msg = 'The following code, {0}, was not handled by the PEP8 plugin'.format(pylintcode)\n                        if logging.root.handlers:\n                            logging.getLogger(__name__).warning(msg)\n                        else:\n                            sys.stderr.write('{0}\\n'.format(msg))\n                    continue\n\n                if pylintcode not in self._msgs:\n                    # Not for our class implementation to handle\n                    continue\n\n                if code in ('E111', 'E113'):\n                    if _PROCESSED_NODES[node_path].lines[lineno-1].strip().startswith('#'):\n                        # If E111 is triggered in a comment I consider it, at\n                        # least, bad judgement. See https://github.com/jcrocholl/pep8/issues/300\n\n                        # If E113 is triggered in comments, which I consider a bug,\n                        # skip it. See https://github.com/jcrocholl/pep8/issues/274\n                        continue\n                try:\n                    self.add_message(pylintcode, line=lineno, args=(code, text))\n                except TypeError as exc:\n                    if 'not all arguments' not in str(exc):\n                        raise\n                    # Message does not support being passed the text arg\n                    self.add_message(pylintcode, line=lineno, args=(code,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess a module node", "response": "def process_module(self, node):\n        '''\n        process a module\n        '''\n        if not HAS_PYQVER:\n            return\n\n        minimum_version = tuple([int(x) for x in self.config.minimum_python_version.split('.')])\n        with open(node.path, 'r') as rfh:\n            for version, reasons in pyqver2.get_versions(rfh.read()).iteritems():\n                if version > minimum_version:\n                    for lineno, msg in reasons:\n                        self.add_message(\n                            'E0598', line=lineno,\n                            args=(self.config.minimum_python_version, msg)\n                        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_xblock_settings(self, default=None):\n        settings_service = self.runtime.service(self, \"settings\")\n        if settings_service:\n            return settings_service.get_settings_bucket(self, default=default)\n        return default", "response": "Gets XBlock - specific settigns for current XBlock - specific setigns for current XBlock - specific setigns. Returns default if no XBlock - specific settigns are available."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_theme(self):\n        xblock_settings = self.get_xblock_settings(default={})\n        if xblock_settings and self.theme_key in xblock_settings:\n            return xblock_settings[self.theme_key]\n        return self.default_theme_config", "response": "Gets theme settings from settings service. Falls back to default ( LMS )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding theme files to fragment", "response": "def include_theme_files(self, fragment):\n        \"\"\"\n        Gets theme configuration and renders theme css into fragment\n        \"\"\"\n        theme = self.get_theme()\n        if not theme or 'package' not in theme:\n            return\n\n        theme_package, theme_files = theme.get('package', None), theme.get('locations', [])\n        resource_loader = ResourceLoader(theme_package)\n        for theme_file in theme_files:\n            fragment.add_css(resource_loader.load_unicode(theme_file))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_unicode(self, resource_path):\n        resource_content = pkg_resources.resource_string(self.module_name, resource_path)\n        return resource_content.decode('utf-8')", "response": "Loads the content of a resource in the module named by resource_path and returns it as a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_django_template(self, template_path, context=None, i18n_service=None):\n        context = context or {}\n        context['_i18n_service'] = i18n_service\n        libraries = {\n            'i18n': 'xblockutils.templatetags.i18n',\n        }\n\n        # For django 1.8, we have to load the libraries manually, and restore them once the template is rendered.\n        _libraries = None\n        if django.VERSION[0] == 1 and django.VERSION[1] == 8:\n            _libraries = TemplateBase.libraries.copy()\n            for library_name in libraries:\n                library = TemplateBase.import_library(libraries[library_name])\n                if library:\n                    TemplateBase.libraries[library_name] = library\n            engine = Engine()\n        else:\n            # Django>1.8 Engine can load the extra templatetag libraries itself\n            # but we have to override the default installed libraries.\n            from django.template.backends.django import get_installed_libraries\n            installed_libraries = get_installed_libraries()\n            installed_libraries.update(libraries)\n            engine = Engine(libraries=installed_libraries)\n\n        template_str = self.load_unicode(template_path)\n        template = Template(template_str, engine=engine)\n        rendered = template.render(Context(context))\n\n        # Restore the original TemplateBase.libraries\n        if _libraries is not None:\n            TemplateBase.libraries = _libraries\n\n        return rendered", "response": "Evaluate a django template by resource path and apply the context."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_mako_template(self, template_path, context=None):\n        context = context or {}\n        template_str = self.load_unicode(template_path)\n        lookup = MakoTemplateLookup(directories=[pkg_resources.resource_filename(self.module_name, '')])\n        template = MakoTemplate(template_str, lookup=lookup)\n        return template.render(**context)", "response": "Evaluate a mako template by resource path applying the provided context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_template(self, template_path, context=None):\n        warnings.warn(\n            \"ResourceLoader.render_template has been deprecated in favor of ResourceLoader.render_django_template\"\n        )\n        return self.render_django_template(template_path, context)", "response": "This function calls render_django_template to support backwards compatibility."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_js_template(self, template_path, element_id, context=None):\n        context = context or {}\n        return u\"<script type='text/template' id='{}'>\\n{}\\n</script>\".format(\n            element_id,\n            self.render_template(template_path, context)\n        )", "response": "Render a js template."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the scenarios from a directory containing a given path.", "response": "def load_scenarios_from_path(self, relative_scenario_dir, include_identifier=False):\n        \"\"\"\n        Returns an array of (title, xmlcontent) from files contained in a specified directory,\n        formatted as expected for the return value of the workbench_scenarios() method.\n\n        If `include_identifier` is True, returns an array of (identifier, title, xmlcontent).\n        \"\"\"\n        base_dir = os.path.dirname(os.path.realpath(sys.modules[self.module_name].__file__))\n        scenario_dir = os.path.join(base_dir, relative_scenario_dir)\n\n        scenarios = []\n        if os.path.isdir(scenario_dir):\n            for template in sorted(os.listdir(scenario_dir)):\n                if not template.endswith('.xml'):\n                    continue\n                identifier = template[:-4]\n                title = identifier.replace('_', ' ').title()\n                template_path = os.path.join(relative_scenario_dir, template)\n                scenario = text_type(self.render_template(template_path, {\"url_name\": identifier}))\n                if not include_identifier:\n                    scenarios.append((title, scenario))\n                else:\n                    scenarios.append((identifier, title, scenario))\n\n        return scenarios"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_translation(self, context):\n        language = get_language()\n        i18n_service = context.get('_i18n_service', None)\n        if i18n_service:\n            # Cache the original translation object to reduce overhead\n            if language not in self._translations:\n                self._translations[language] = trans_real.DjangoTranslation(language)\n\n            translation = trans_real.translation(language)\n            translation.merge(i18n_service)\n\n        yield\n\n        # Revert to original translation object\n        if language in self._translations:\n            trans_real._translations[language] = self._translations[language]\n            # Re-activate the current language to reset translation caches\n            trans_real.activate(language)", "response": "A context wrapper which merges the current language s translation catalog with the given i18n service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the translated text using the XBlock i18n service.", "response": "def render(self, context):\n        \"\"\"\n        Renders the translated text using the XBlock i18n service, if available.\n        \"\"\"\n        with self.merge_translation(context):\n            django_translated = self.do_translate.render(context)\n\n        return django_translated"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders a form for editing this XBlock", "response": "def studio_view(self, context):\n        \"\"\"\n        Render a form for editing this XBlock\n        \"\"\"\n        fragment = Fragment()\n        context = {'fields': []}\n        # Build a list of all the fields that can be edited:\n        for field_name in self.editable_fields:\n            field = self.fields[field_name]\n            assert field.scope in (Scope.content, Scope.settings), (\n                \"Only Scope.content or Scope.settings fields can be used with \"\n                \"StudioEditableXBlockMixin. Other scopes are for user-specific data and are \"\n                \"not generally created/configured by content authors in Studio.\"\n            )\n            field_info = self._make_field_info(field_name, field)\n            if field_info is not None:\n                context[\"fields\"].append(field_info)\n        fragment.content = loader.render_template('templates/studio_edit.html', context)\n        fragment.add_javascript(loader.load_unicode('public/studio_edit.js'))\n        fragment.initialize_js('StudioEditableXBlockMixin')\n        return fragment"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_field_info(self, field_name, field):\n        supported_field_types = (\n            (Integer, 'integer'),\n            (Float, 'float'),\n            (Boolean, 'boolean'),\n            (String, 'string'),\n            (List, 'list'),\n            (DateTime, 'datepicker'),\n            (JSONField, 'generic'),  # This is last so as a last resort we display a text field w/ the JSON string\n        )\n        if self.service_declaration(\"i18n\"):\n            ugettext = self.ugettext\n        else:\n\n            def ugettext(text):\n                \"\"\" Dummy ugettext method that doesn't do anything \"\"\"\n                return text\n\n        info = {\n            'name': field_name,\n            'display_name': ugettext(field.display_name) if field.display_name else \"\",\n            'is_set': field.is_set_on(self),\n            'default': field.default,\n            'value': field.read_from(self),\n            'has_values': False,\n            'help': ugettext(field.help) if field.help else \"\",\n            'allow_reset': field.runtime_options.get('resettable_editor', True),\n            'list_values': None,  # Only available for List fields\n            'has_list_values': False,  # True if list_values_provider exists, even if it returned no available options\n        }\n        for type_class, type_name in supported_field_types:\n            if isinstance(field, type_class):\n                info['type'] = type_name\n                # If String fields are declared like String(..., multiline_editor=True), then call them \"text\" type:\n                editor_type = field.runtime_options.get('multiline_editor')\n                if type_class is String and editor_type:\n                    if editor_type == \"html\":\n                        info['type'] = 'html'\n                    else:\n                        info['type'] = 'text'\n                if type_class is List and field.runtime_options.get('list_style') == \"set\":\n                    # List represents unordered, unique items, optionally drawn from list_values_provider()\n                    info['type'] = 'set'\n                elif type_class is List:\n                    info['type'] = \"generic\"  # disable other types of list for now until properly implemented\n                break\n        if \"type\" not in info:\n            raise NotImplementedError(\"StudioEditableXBlockMixin currently only supports fields derived from JSONField\")\n        if info[\"type\"] in (\"list\", \"set\"):\n            info[\"value\"] = [json.dumps(val) for val in info[\"value\"]]\n            info[\"default\"] = json.dumps(info[\"default\"])\n        elif info[\"type\"] == \"generic\":\n            # Convert value to JSON string if we're treating this field generically:\n            info[\"value\"] = json.dumps(info[\"value\"])\n            info[\"default\"] = json.dumps(info[\"default\"])\n        elif info[\"type\"] == \"datepicker\":\n            if info[\"value\"]:\n                info[\"value\"] = info[\"value\"].strftime(\"%m/%d/%Y\")\n            if info[\"default\"]:\n                info[\"default\"] = info[\"default\"].strftime(\"%m/%d/%Y\")\n\n        if 'values_provider' in field.runtime_options:\n            values = field.runtime_options[\"values_provider\"](self)\n        else:\n            values = field.values\n        if values and not isinstance(field, Boolean):\n            # This field has only a limited number of pre-defined options.\n            # Protip: when defining the field, values= can be a callable.\n            if isinstance(field.values, dict) and isinstance(field, (Float, Integer)):\n                # e.g. {\"min\": 0 , \"max\": 10, \"step\": .1}\n                for option in field.values:\n                    if option in (\"min\", \"max\", \"step\"):\n                        info[option] = field.values.get(option)\n                    else:\n                        raise KeyError(\"Invalid 'values' key. Should be like values={'min': 1, 'max': 10, 'step': 1}\")\n            elif isinstance(values[0], dict) and \"display_name\" in values[0] and \"value\" in values[0]:\n                # e.g. [ {\"display_name\": \"Always\", \"value\": \"always\"}, ... ]\n                for value in values:\n                    assert \"display_name\" in value and \"value\" in value\n                info['values'] = values\n            else:\n                # e.g. [1, 2, 3] - we need to convert it to the [{\"display_name\": x, \"value\": x}] format\n                info['values'] = [{\"display_name\": text_type(val), \"value\": val} for val in values]\n            info['has_values'] = 'values' in info\n        if info[\"type\"] in (\"list\", \"set\") and field.runtime_options.get('list_values_provider'):\n            list_values = field.runtime_options['list_values_provider'](self)\n            # list_values must be a list of values or {\"display_name\": x, \"value\": y} objects\n            # Furthermore, we need to convert all values to JSON since they could be of any type\n            if list_values and isinstance(list_values[0], dict) and \"display_name\" in list_values[0]:\n                # e.g. [ {\"display_name\": \"Always\", \"value\": \"always\"}, ... ]\n                for entry in list_values:\n                    assert \"display_name\" in entry and \"value\" in entry\n                    entry[\"value\"] = json.dumps(entry[\"value\"])\n            else:\n                # e.g. [1, 2, 3] - we need to convert it to the [{\"display_name\": x, \"value\": x}] format\n                list_values = [json.dumps(val) for val in list_values]\n                list_values = [{\"display_name\": text_type(val), \"value\": val} for val in list_values]\n            info['list_values'] = list_values\n            info['has_list_values'] = True\n        return info", "response": "Create the information that the template needs to render a form field for this field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef submit_studio_edits(self, data, suffix=''):\n        values = {}  # dict of new field values we are updating\n        to_reset = []  # list of field names to delete from this XBlock\n        for field_name in self.editable_fields:\n            field = self.fields[field_name]\n            if field_name in data['values']:\n                if isinstance(field, JSONField):\n                    values[field_name] = field.from_json(data['values'][field_name])\n                else:\n                    raise JsonHandlerError(400, \"Unsupported field type: {}\".format(field_name))\n            elif field_name in data['defaults'] and field.is_set_on(self):\n                to_reset.append(field_name)\n        self.clean_studio_edits(values)\n        validation = Validation(self.scope_ids.usage_id)\n        # We cannot set the fields on self yet, because even if validation fails, studio is going to save any changes we\n        # make. So we create a \"fake\" object that has all the field values we are about to set.\n        preview_data = FutureFields(\n            new_fields_dict=values,\n            newly_removed_fields=to_reset,\n            fallback_obj=self\n        )\n        self.validate_field_data(validation, preview_data)\n        if validation:\n            for field_name, value in six.iteritems(values):\n                setattr(self, field_name, value)\n            for field_name in to_reset:\n                self.fields[field_name].delete_from(self)\n            return {'result': 'success'}\n        else:\n            raise JsonHandlerError(400, validation.to_json())", "response": "Submit the edits for this XBlock and return the result of the AJAX handler for the Studio view."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the state of this XBlock.", "response": "def validate(self):\n        \"\"\"\n        Validates the state of this XBlock.\n\n        Subclasses should override validate_field_data() to validate fields and override this\n        only for validation not related to this block's field values.\n        \"\"\"\n        validation = super(StudioEditableXBlockMixin, self).validate()\n        self.validate_field_data(validation, self)\n        return validation"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrenders the children of the module with HTML appropriate for Studio.", "response": "def render_children(self, context, fragment, can_reorder=True, can_add=False):\n        \"\"\"\n        Renders the children of the module with HTML appropriate for Studio. If can_reorder is\n        True, then the children will be rendered to support drag and drop.\n        \"\"\"\n        contents = []\n\n        child_context = {'reorderable_items': set()}\n        if context:\n            child_context.update(context)\n\n        for child_id in self.children:\n            child = self.runtime.get_block(child_id)\n            if can_reorder:\n                child_context['reorderable_items'].add(child.scope_ids.usage_id)\n            view_to_render = 'author_view' if hasattr(child, 'author_view') else 'student_view'\n            rendered_child = child.render(view_to_render, child_context)\n            fragment.add_frag_resources(rendered_child)\n\n            contents.append({\n                'id': text_type(child.scope_ids.usage_id),\n                'content': rendered_child.content\n            })\n\n        fragment.add_content(self.runtime.render_template(\"studio_render_children_view.html\", {\n            'items': contents,\n            'xblock_context': context,\n            'can_add': can_add,\n            'can_reorder': can_reorder,\n        }))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying an editable preview of the author s children.", "response": "def author_view(self, context):\n        \"\"\"\n        Display a the studio editor when the user has clicked \"View\" to see the container view,\n        otherwise just show the normal 'author_preview_view' or 'student_view' preview.\n        \"\"\"\n        root_xblock = context.get('root_xblock')\n\n        if root_xblock and root_xblock.location == self.location:\n            # User has clicked the \"View\" link. Show an editable preview of this block's children\n            return self.author_edit_view(context)\n        return self.author_preview_view(context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef author_edit_view(self, context):\n        fragment = Fragment()\n        self.render_children(context, fragment, can_reorder=True, can_add=False)\n        return fragment", "response": "This method renders the children of this block as a fragment and returns the result of this method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the current version of the preview view for this context.", "response": "def preview_view(self, context):\n        \"\"\"\n        Preview view - used by StudioContainerWithNestedXBlocksMixin to render nested xblocks in preview context.\n        Default implementation uses author_view if available, otherwise falls back to student_view\n        Child classes can override this method to control their presentation in preview context\n        \"\"\"\n        view_to_render = 'author_view' if hasattr(self, 'author_view') else 'student_view'\n        renderer = getattr(self, view_to_render)\n        return renderer(context)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_nested_blocks_spec(self):\n        return [\n            block_spec if isinstance(block_spec, NestedXBlockSpec) else NestedXBlockSpec(block_spec)\n            for block_spec in self.allowed_nested_blocks\n        ]", "response": "Converts allowed_nested_blocks items to NestedXBlockSpec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef author_edit_view(self, context):\n        fragment = Fragment()\n\n        if 'wrap_children' in context:\n            fragment.add_content(context['wrap_children']['head'])\n\n        self.render_children(context, fragment, can_reorder=True, can_add=False)\n\n        if 'wrap_children' in context:\n            fragment.add_content(context['wrap_children']['tail'])\n        fragment.add_content(\n            loader.render_template('templates/add_buttons.html', {'child_blocks': self.get_nested_blocks_spec()})\n        )\n        fragment.add_javascript(loader.load_unicode('public/studio_container.js'))\n        fragment.initialize_js('StudioContainerXBlockWithNestedXBlocksMixin')\n        return fragment", "response": "View for adding or editing nested blocks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef author_preview_view(self, context):\n        children_contents = []\n\n        fragment = Fragment()\n        for child_id in self.children:\n            child = self.runtime.get_block(child_id)\n            child_fragment = self._render_child_fragment(child, context, 'preview_view')\n            fragment.add_frag_resources(child_fragment)\n            children_contents.append(child_fragment.content)\n\n        render_context = {\n            'block': self,\n            'children_contents': children_contents\n        }\n        render_context.update(context)\n        fragment.add_content(self.loader.render_template(self.CHILD_PREVIEW_TEMPLATE, render_context))\n        return fragment", "response": "View for previewing contents in studio."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders a child block in the correct format.", "response": "def _render_child_fragment(self, child, context, view='student_view'):\n        \"\"\"\n        Helper method to overcome html block rendering quirks\n        \"\"\"\n        try:\n            child_fragment = child.render(view, context)\n        except NoSuchViewError:\n            if child.scope_ids.block_type == 'html' and getattr(self.runtime, 'is_author_mode', False):\n                # html block doesn't support preview_view, and if we use student_view Studio will wrap\n                # it in HTML that we don't want in the preview. So just render its HTML directly:\n                child_fragment = Fragment(child.data)\n            else:\n                child_fragment = child.render('student_view', context)\n\n        return child_fragment"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_requirements(*requirements_paths):\n    requirements = set()\n    for path in requirements_paths:\n        requirements.update(\n            line.split('#')[0].strip() for line in open(path).readlines()\n            if is_requirement(line.strip())\n        )\n    return list(requirements)", "response": "Load all requirements from the specified requirements files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the line is a package requirement ; otherwise return False.", "response": "def is_requirement(line):\n    \"\"\"\n    Return True if the requirement line is a package requirement;\n    that is, it is not blank, a comment, a URL, or an included file.\n    \"\"\"\n    return not (\n        line == '' or\n        line.startswith('-r') or\n        line.startswith('#') or\n        line.startswith('-e') or\n        line.startswith('git+')\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef publish_event_from_dict(self, event_type, data):\n        for key, value in self.additional_publish_event_data.items():\n            if key in data:\n                return {'result': 'error', 'message': 'Key should not be in publish_event data: {}'.format(key)}\n            data[key] = value\n\n        self.runtime.publish(self, event_type, data)\n        return {'result': 'success'}", "response": "Publish an event from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef child_isinstance(block, child_id, block_class_or_mixin):\n    def_id = block.runtime.id_reader.get_definition_id(child_id)\n    type_name = block.runtime.id_reader.get_block_type(def_id)\n    child_class = block.runtime.load_block_type(type_name)\n    return issubclass(child_class, block_class_or_mixin)", "response": "A simple wrapper for the base class isinstance method that returns True if a child of an XBlock is an instance of the given class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an attribute to the element", "response": "def attr(self, *args, **kwargs):\n        \"\"\"Add an attribute to the element\"\"\"\n        kwargs.update({k: bool for k in args})\n        for key, value in kwargs.items():\n            if key == \"klass\":\n                self.attrs[\"klass\"].update(value.split())\n            elif key == \"style\":\n                if isinstance(value, str):\n                    splitted = iter(re.split(\";|:\", value))\n                    value = dict(zip(splitted, splitted))\n                self.attrs[\"style\"].update(value)\n            else:\n                self.attrs[key] = value\n        self._stable = False\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving an attribute from the set.", "response": "def remove_attr(self, attr):\n        \"\"\"Removes an attribute.\"\"\"\n        self._stable = False\n        self.attrs.pop(attr, None)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders the tag s attributes using the formats and performing special attributes name substitution.", "response": "def render_attrs(self):\n        \"\"\"Renders the tag's attributes using the formats and performing special attributes name substitution.\"\"\"\n        ret = []\n        for k, v in self.attrs.items():\n            if v:\n                if v is bool:\n                    ret.append(\" %s\" % self._SPECIAL_ATTRS.get(k, k))\n                else:\n                    fnc = self._FORMAT_ATTRS.get(k, None)\n                    val = fnc(v) if fnc else v\n                    ret.append(' %s=\"%s\"' % (self._SPECIAL_ATTRS.get(k, k), val))\n        return \"\".join(ret)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toggle_class(self, csscl):\n        self._stable = False\n        action = (\"add\", \"remove\")[self.has_class(csscl)]\n        return getattr(self.attrs[\"klass\"], action)(csscl)", "response": "Same as jQuery s toggleClass function. It toggles the css class on this element."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_class(self, cssclass):\n        if self.has_class(cssclass):\n            return self\n        return self.toggle_class(cssclass)", "response": "Adds a css class to this element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_class(self, cssclass):\n        if not self.has_class(cssclass):\n            return self\n        return self.toggle_class(cssclass)", "response": "Removes the given class from this element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd css properties to this element.", "response": "def css(self, *props, **kwprops):\n        \"\"\"Adds css properties to this element.\"\"\"\n        self._stable = False\n        styles = {}\n        if props:\n            if len(props) == 1 and isinstance(props[0], Mapping):\n                styles = props[0]\n            else:\n                raise WrongContentError(self, props, \"Arguments not valid\")\n        elif kwprops:\n            styles = kwprops\n        else:\n            raise WrongContentError(self, None, \"args OR wkargs are needed\")\n        return self.attr(style=styles)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show(self, display=None):\n        self._stable = False\n        if not display:\n            self.attrs[\"style\"].pop(\"display\")\n        else:\n            self.attrs[\"style\"][\"display\"] = display\n        return self", "response": "Removes the display style attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef toggle(self):\n        self._stable = False\n        return self.show() if self.attrs[\"style\"][\"display\"] == \"none\" else self.hide()", "response": "Same as jQuery s toggle the display attribute of this element."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef text(self):\n        texts = []\n        for child in self.childs:\n            if isinstance(child, Tag):\n                texts.append(child.text())\n            elif isinstance(child, Content):\n                texts.append(child.render())\n            else:\n                texts.append(child)\n        return \" \".join(texts)", "response": "Renders the contents inside this element without html tags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender the element and all his childrens.", "response": "def render(self, *args, **kwargs):\n        \"\"\"Renders the element and all his childrens.\"\"\"\n        # args kwargs API provided for last minute content injection\n        # self._reverse_mro_func('pre_render')\n        pretty = kwargs.pop(\"pretty\", False)\n        if pretty and self._stable != \"pretty\":\n            self._stable = False\n\n        for arg in args:\n            self._stable = False\n            if isinstance(arg, dict):\n                self.inject(arg)\n        if kwargs:\n            self._stable = False\n            self.inject(kwargs)\n\n        # If the tag or his contents are not changed and we already have rendered it\n        # with the same attrs we skip all the work\n        if self._stable and self._render:\n            return self._render\n\n        pretty_pre = pretty_inner = \"\"\n        if pretty:\n            pretty_pre = \"\\n\" + (\"\\t\" * self._depth) if pretty else \"\"\n            pretty_inner = \"\\n\" + (\"\\t\" * self._depth) if len(self.childs) > 1 else \"\"\n        inner = self.render_childs(pretty) if not self._void else \"\"\n\n        # We declare the tag is stable and have an official render:\n        tag_data = (\n            pretty_pre,\n            self._get__tag(),\n            self.render_attrs(),\n            inner,\n            pretty_inner,\n            self._get__tag()\n        )[: 6 - [0, 3][self._void]]\n        self._render = self._template % tag_data\n        self._stable = \"pretty\" if pretty else True\n        return self._render"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_tempy_tag(self, tag, attrs, void):\n        tempy_tag_cls = getattr(self.tempy_tags, tag.title(), None)\n        if not tempy_tag_cls:\n            unknow_maker = [self.unknown_tag_maker, self.unknown_tag_maker.Void][void]\n            tempy_tag_cls = unknow_maker[tag]\n        attrs = {Tag._TO_SPECIALS.get(k, k): v or True for k, v in attrs}\n        tempy_tag = tempy_tag_cls(**attrs)\n        if not self.current_tag:\n            self.result.append(tempy_tag)\n            if not void:\n                self.current_tag = tempy_tag\n        else:\n            if not tempy_tag._void:\n                self.current_tag(tempy_tag)\n                self.current_tag = self.current_tag.childs[-1]", "response": "Searches in tempy. tags for the correct tag to use uses the TempyFactory to\n            create a custom tag."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an html string and returns a list of Tempy trees.", "response": "def from_string(self, html_string):\n        \"\"\"Parses an html string and returns a list of Tempy trees.\"\"\"\n        self._html_parser._reset().feed(html_string)\n        return self._html_parser.result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(self, tempy_tree_list, filename, pretty=False):\n        if not filename:\n            raise ValueError('\"filename\" argument should not be none.')\n        if len(filename.split(\".\")) > 1 and not filename.endswith(\".py\"):\n            raise ValueError(\n                '\"filename\" argument should have a .py extension, if given.'\n            )\n        if not filename.endswith(\".py\"):\n            filename += \".py\"\n        with open(filename, \"w\") as f:\n            f.write(\n                \"# -*- coding: utf-8 -*-\\nfrom tempy import T\\nfrom tempy.tags import *\\n\"\n            )\n            for tempy_tree in tempy_tree_list:\n                f.write(tempy_tree.to_code(pretty=pretty))\n        return filename", "response": "Dumps a Tempy object to a python file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filter_classes(cls_list, cls_type):\n    for cls in cls_list:\n        if isinstance(cls, type) and issubclass(cls, cls_type):\n            if cls_type == TempyPlace and cls._base_place:\n                pass\n            else:\n                yield cls", "response": "Filters a list of classes and yields TempyREPR subclasses"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassigns a score ito a TempyRepr class.", "response": "def _evaluate_tempyREPR(self, child, repr_cls):\n        \"\"\"Assign a score ito a TempyRepr class.\n        The scores depends on the current scope and position of the object in which the TempyREPR is found.\"\"\"\n        score = 0\n        if repr_cls.__name__ == self.__class__.__name__:\n            # One point if the REPR have the same name of the container\n            score += 1\n        elif repr_cls.__name__ == self.root.__class__.__name__:\n            # One point if the REPR have the same name of the Tempy tree root\n            score += 1\n\n        # Add points defined in scorers methods of used TempyPlaces\n        for parent_cls in _filter_classes(repr_cls.__mro__[1:], TempyPlace):\n            for scorer in (\n                method for method in dir(parent_cls) if method.startswith(\"_reprscore\")\n            ):\n                score += getattr(parent_cls, scorer, lambda *args: 0)(\n                    parent_cls, self, child\n                )\n        return score"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for TempyREPR class declarations in the child s class.", "response": "def _search_for_view(self, obj):\n        \"\"\"Searches for TempyREPR class declarations in the child's class.\n        If at least one TempyREPR is found, it uses the best one to make a Tempy object.\n        Otherwise the original object is returned.\n        \"\"\"\n        evaluator = partial(self._evaluate_tempyREPR, obj)\n        sorted_reprs = sorted(\n            _filter_classes(obj.__class__.__dict__.values(), TempyREPR),\n            key=evaluator,\n            reverse=True,\n        )\n        if sorted_reprs:\n            # If we find some TempyREPR, we return the one with the best score.\n            return sorted_reprs[0]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the charset attribute of the meta tag.", "response": "def set_charset(self, charset):\n        \"\"\"Changes the <meta> charset tag (default charset in init is UTF-8).\"\"\"\n        self.head.charset.attr(charset=charset)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_description(self, description):\n        self.head.description.attr(content=description)\n        return self", "response": "Changes the meta tag description."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the meta tags keywords.", "response": "def set_keywords(self, keywords):\n        \"\"\"Changes the <meta> keywords tag.\"\"\"\n        self.head.keywords.attr(content=\", \".join(keywords))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchanging the title tag.", "response": "def set_title(self, title):\n        \"\"\"Changes the <meta> title tag.\"\"\"\n        self.head.title.attr(content=title)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef populate(self, data, resize_x=True, normalize=True):\n        if data is None:\n            raise WidgetDataError(\n                self,\n                \"Parameter data should be non-None, to empty the table use TempyTable.clear() or \"\n                \"pass an empty list.\",\n            )\n        data = copy(data)\n        if not self.body:\n            # Table is empty\n            self(body=Tbody())\n        self.clear()\n\n        max_data_x = max(map(len, data))\n        if not resize_x:\n            self._check_row_size(max_data_x)\n\n        for t_row, d_row in zip_longest(self.body, data):\n            if not d_row:\n                t_row.remove()\n            else:\n                if not t_row:\n                    t_row = Tr().append_to(self.body)\n                if normalize:\n                    d_row = AdjustableList(d_row).ljust(max_data_x, None)\n                for t_cell, d_cell in zip_longest(t_row, d_row):\n                    if not t_cell and resize_x:\n                        t_cell = Td().append_to(t_row)\n                    t_cell.empty()\n                    if d_cell is not None:\n                        t_cell(d_cell)\n        return self", "response": "Populates the table with the given data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_row(self, row_data, resize_x=True):\n        if not resize_x:\n            self._check_row_size(row_data)\n        self.body(Tr()(Td()(cell) for cell in row_data))\n        return self", "response": "Adds a row at the end of the table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pop_row(self, idr=None, tags=False):\n        idr = idr if idr is not None else len(self.body) - 1\n        row = self.body.pop(idr)\n        return row if tags else [cell.childs[0] for cell in row]", "response": "Pops a row from the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npops a cell from the table", "response": "def pop_cell(self, idy=None, idx=None, tags=False):\n        \"\"\"Pops a cell, default the last of the last row\"\"\"\n        idy = idy if idy is not None else len(self.body) - 1\n        idx = idx if idx is not None else len(self.body[idy]) - 1\n        cell = self.body[idy].pop(idx)\n        return cell if tags else cell.childs[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_caption(self, caption):\n        if not hasattr(self, \"caption\"):\n            self(caption=Caption())\n        return self.caption.empty()(caption)", "response": "Adds or substitutes the table s caption."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_scope(self, col_scope_list=None, row_scope_list=None):\n        if col_scope_list is not None and len(col_scope_list) > 0:\n            self.apply_scope(col_scope_list, \"col\")\n\n        if row_scope_list is not None and len(row_scope_list) > 0:\n            self.apply_scope(row_scope_list, \"row\")", "response": "Makes scopes and converts Td to Th for given arguments\n        which represent lists of tuples col_index row_index"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef populate(self, struct):\n\n        if struct is None:\n            # Maybe raise? Empty the list?\n            return self\n\n        if isinstance(struct, (list, set, tuple)):\n            struct = dict(zip_longest(struct, [None]))\n        if not isinstance(struct, dict):\n            raise WidgetDataError(\n                self,\n                \"List Input not managed, expected (dict, list), got %s\" % type(struct),\n            )\n        else:\n            if self._typ == Dl:\n                self.__process_dl_struct(struct)\n            else:\n                self.__process_li_struct(struct)\n\n        return self", "response": "Populate the internal structure of the list with the contents of the given structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides so each html page served have a doctype", "response": "def render(self, *args, **kwargs):\n        \"\"\"Override so each html page served have a doctype\"\"\"\n        return self.doctype.render() + super().render(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, *args, **kwargs):\n        if not self.childs and \"href\" in self.attrs:\n            return self.clone()(self.attrs[\"href\"]).render(*args, **kwargs)\n        return super().render(*args, **kwargs)", "response": "Override of the rendering so that the href attribute is used inside the link tag"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for a content_name in the content data.", "response": "def _find_content(self, cont_name):\n        \"\"\"Search for a content_name in the content data, if not found the parent is searched.\"\"\"\n        try:\n            a = self.content_data[cont_name]\n            return a\n        except KeyError:\n            if self.parent:\n                return self.parent._find_content(cont_name)\n            else:\n                # Fallback for no content (Raise NoContent?)\n                return \"\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn rendered Contents and non - DOMElement stuff inside this Tag.", "response": "def _get_non_tempy_contents(self):\n        \"\"\"Returns rendered Contents and non-DOMElement stuff inside this Tag.\"\"\"\n        for thing in filter(\n            lambda x: not issubclass(x.__class__, DOMElement), self.childs\n        ):\n            yield thing"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all the siblings of this element as a list.", "response": "def siblings(self):\n        \"\"\"Returns all the siblings of this element as a list.\"\"\"\n        return list(filter(lambda x: id(x) != id(self), self.parent.childs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slice(self, start=None, end=None, step=None):\n        return self.childs[start:end:step]", "response": "Slice of this element s childs as childs [ start end step )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dfs_inorder(self, reverse=False):\n        stack = deque()\n        visited = set()\n        visited.add(self)\n        if reverse:\n            stack.append(self.childs[0])\n            stack.append(self)\n            stack.extend(self.childs[1:])\n        else:\n            stack.extend(self.childs[1:])\n            stack.append(self)\n            stack.append(self.childs[0])\n        while stack:\n            node = stack.pop()\n            if node in visited or not node.childs:\n                yield node\n            else:\n                stack.append(node)\n                visited.add(node)\n                if hasattr(node, \"childs\"):\n                    if reverse:\n                        stack.extend(node.childs)\n                    else:\n                        stack.extend(node.childs[::-1])", "response": "Generator that returns each element of the tree in Inorder order."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dfs_postorder(self, reverse=False):\n        stack = deque()\n        stack.append(self)\n        visited = set()\n        while stack:\n            node = stack.pop()\n            if node in visited:\n                yield node\n            else:\n                visited.add(node)\n                stack.append(node)\n                if hasattr(node, \"childs\"):\n                    if reverse:\n                        stack.extend(node.childs)\n                    else:\n                        stack.extend(node.childs[::-1])", "response": "Generator that returns each element of the tree in Postorder order."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef content_receiver(reverse=False):\n\n        def _receiver(func):\n            @wraps(func)\n            def wrapped(inst, *tags, **kwtags):\n                verse = (1, -1)[int(reverse)]\n                kwtags = kwtags.items()\n                i = 0\n                for typ in (tags, kwtags)[::verse]:\n                    for item in typ:\n                        if typ is kwtags:\n                            name, item = item\n                        else:\n                            name, item = None, item\n                        if isinstance(item, DOMElement) and name:\n                            # Is the DOMGroup is a single DOMElement and we have a name we set his name accordingly\n                            item._name = name\n                        inst._stable = False\n                        func(inst, i, item, name)\n                        i += 1\n                return inst\n\n            return wrapped\n\n        return _receiver", "response": "Decorator for content adding methods."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts a DOMGroup into this element.", "response": "def _insert(self, dom_group, idx=None, prepend=False, name=None):\n        \"\"\"Inserts a DOMGroup inside this element.\n        If provided at the given index, if prepend at the start of the childs list, by default at the end.\n        If the child is a DOMElement, correctly links the child.\n        If the DOMGroup have a name, an attribute containing the child is created in this instance.\n        \"\"\"\n        if idx and idx < 0:\n            idx = 0\n        if prepend:\n            idx = 0\n        else:\n            idx = idx if idx is not None else len(self.childs)\n        if dom_group is not None:\n            if not isinstance(dom_group, Iterable) or isinstance(\n                dom_group, (DOMElement, str)\n            ):\n                dom_group = [dom_group]\n            for i_group, elem in enumerate(dom_group):\n                if elem is not None:\n                    # Element insertion in this DOMElement childs\n                    self.childs.insert(idx + i_group, elem)\n                    # Managing child attributes if needed\n                    if issubclass(elem.__class__, DOMElement):\n                        elem.parent = self\n                    if name:\n                        setattr(self, name, elem)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds siblings after the current tag.", "response": "def after(self, i, sibling, name=None):\n        \"\"\"Adds siblings after the current tag.\"\"\"\n        self.parent._insert(sibling, idx=self._own_index + 1 + i, name=name)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd childs to this tag starting from the first position.", "response": "def prepend(self, _, child, name=None):\n        \"\"\"Adds childs to this tag, starting from the first position.\"\"\"\n        self._insert(child, prepend=True, name=name)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef append(self, _, child, name=None):\n        self._insert(child, name=name)\n        return self", "response": "Adds childs to this tag after the current existing childs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping this element inside another empty tag.", "response": "def wrap(self, other):\n        \"\"\"Wraps this element inside another empty tag.\"\"\"\n        if other.childs:\n            raise TagError(self, \"Wrapping in a non empty Tag is forbidden.\")\n        if self.parent:\n            self.before(other)\n            self.parent.pop(self._own_index)\n        other.append(self)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_many(self, *args, strict=False):\n\n        for arg in args:\n            is_elem = arg and isinstance(arg, DOMElement)\n            is_elem_iter = (\n                not is_elem and arg and isinstance(arg, Iterable) and isinstance(iter(arg).__next__(), DOMElement)\n            )\n            if not (is_elem or is_elem_iter):\n                raise WrongArgsError(\n                    self,\n                    \"Argument {} is not DOMElement nor iterable of DOMElements\".format(\n                        arg\n                    ),\n                )\n\n        wcopies = []\n        failure = []\n\n        def wrap_next(tag, idx):\n            nonlocal wcopies, failure\n            next_copy = self.__copy__()\n            try:\n                return next_copy.wrap(tag)\n            except TagError:\n                failure.append(idx)\n                return next_copy\n\n        for arg_idx, arg in enumerate(args):\n            if isinstance(arg, DOMElement):\n                wcopies.append(wrap_next(arg, (arg_idx, -1)))\n            else:\n                iter_wcopies = []\n                for iter_idx, t in enumerate(arg):\n                    iter_wcopies.append(wrap_next(t, (arg_idx, iter_idx)))\n                wcopies.append(type(arg)(iter_wcopies))\n\n        if failure and strict:\n            raise TagError(\n                self,\n                \"Wrapping in a non empty Tag is forbidden, failed on arguments \" +\n                \", \".join(\n                    list(\n                        map(\n                            lambda idx: str(idx[0])\n                            if idx[1] == -1\n                            else \"[{1}] of {0}\".format(*idx),\n                            failure,\n                        )\n                    )\n                ),\n            )\n        return wcopies", "response": "Wraps all copies of this element inside all empty tags\nCOOKIES or param s non - empty iterators. Returns list of lists of elements wrapped inside all empty tags\nCOOKIES listed in params or param s non - empty iterators."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef replace_with(self, other):\n        self.after(other)\n        self.parent.pop(self._own_index)\n        return other", "response": "Replace this element with the given DOMElement."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetach this element from his father.", "response": "def remove(self):\n        \"\"\"Detach this element from his father.\"\"\"\n        if self._own_index is not None and self.parent:\n            self.parent.pop(self._own_index)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving all the childs to a new father", "response": "def _detach_childs(self, idx_from=None, idx_to=None):\n        \"\"\"Moves all the childs to a new father\"\"\"\n        idx_from = idx_from or 0\n        idx_to = idx_to or len(self.childs)\n        removed = self.childs[idx_from:idx_to]\n        for child in removed:\n            if issubclass(child.__class__, DOMElement):\n                child.parent = None\n        self.childs[idx_from:idx_to] = []\n        return removed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move(self, new_father, idx=None, prepend=None, name=None):\n        self.parent.pop(self._own_index)\n        new_father._insert(self, idx=idx, prepend=prepend, name=name)\n        new_father._stable = False\n        return self", "response": "Moves this element from his father to the given one."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the first child at given position or by name or name iterator.", "response": "def pop(self, arg=None):\n        \"\"\"Removes the child at given position or by name (or name iterator).\n            if no argument is given removes the last.\"\"\"\n        self._stable = False\n        if arg is None:\n            arg = len(self.childs) - 1\n        if isinstance(arg, int):\n            try:\n                result = self.childs.pop(arg)\n            except IndexError:\n                raise DOMModByIndexError(self, \"Given index invalid.\")\n            if isinstance(result, DOMElement):\n                result.parent = None\n        else:\n            result = []\n            if isinstance(arg, str):\n                arg = [arg]\n            for x in arg:\n                try:\n                    result.append(getattr(self, x))\n                except AttributeError:\n                    raise DOMModByKeyError(\n                        self, \"Given search key invalid. No child found.\"\n                    )\n            if result:\n                for x in result:\n                    self.childs.remove(x)\n                    if isinstance(x, DOMElement):\n                        x.parent = False\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef data(self, key=None, **kwargs):\n        self.content_data.update(kwargs)\n        if key:\n            return self.content_data[key]\n        if not kwargs:\n            return self.content_data\n        return self", "response": "Adds or retrieve extra data to this element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding content data to this element.", "response": "def inject(self, contents=None, **kwargs):\n        \"\"\"\n        Adds content data in this element. This will be used in the rendering of this element's childs.\n        Multiple injections on the same key will override the content (dict.update behavior).\n        \"\"\"\n        if contents and not isinstance(contents, dict):\n            raise WrongContentError(self, contents, \"contents should be a dict\")\n        self._stable = False\n        if not contents:\n            contents = {}\n        if kwargs:\n            contents.update(kwargs)\n        self.content_data.update(contents)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a message to the remote server.", "response": "def send(self, message):\n        \"\"\"\n        \u53d1\u9001\u57fa\u672c\u6587\u5b57\u6d88\u606f\n\n        :param message: (\u5fc5\u586b|str) - \u9700\u8981\u53d1\u9001\u7684\u6587\u672c\u6d88\u606f\n        :return: * status\uff1a\u53d1\u9001\u72b6\u6001\uff0cTrue \u53d1\u9001\u6210\uff0cFalse \u53d1\u9001\u5931\u8d25\n                 * message\uff1a\u53d1\u9001\u5931\u8d25\u8be6\u60c5\n        \"\"\"\n        url = '{0}message'.format(self.remote)\n        data = self._wrap_post_data(content=message)\n        res = requests.post(url, data=data, timeout=self.timeout)\n        if res.status_code == requests.codes.ok:\n            res_data = json.loads(self._convert_bytes(res.content))\n            if res_data.get('status') == STATUS_SUCCESS:\n                return True, res_data.get('message')\n            return False, res_data.get('message')\n        res.raise_for_status()\n        return False, 'Request or Response Error'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to the ACS server.", "response": "def periodic_send(self, content, interval, title=''):\n        \"\"\"\n        \u53d1\u9001\u5468\u671f\u6d88\u606f\n\n        :param content:  (\u5fc5\u586b|str) - \u9700\u8981\u53d1\u9001\u7684\u6d88\u606f\u5185\u5bb9\n        :param interval: (\u5fc5\u586b|int|datetime.timedelta) - \u53d1\u9001\u6d88\u606f\u95f4\u9694\u65f6\u95f4\uff0c\u652f\u6301 datetime.timedelta \u6216 integer \u8868\u793a\u7684\u79d2\u6570\n        :param title: (\u9009\u586b|str) - \u9700\u8981\u53d1\u9001\u7684\u6d88\u606f\u6807\u9898\n        :return: * status\uff1a\u53d1\u9001\u72b6\u6001\uff0cTrue \u53d1\u9001\u6210\uff0cFalse \u53d1\u9001\u5931\u8d25\n                 * message\uff1a\u53d1\u9001\u5931\u8d25\u8be6\u60c5\n        \"\"\"\n        url = '{0}periodic_message'.format(self.remote)\n        if isinstance(interval, datetime.timedelta):\n            interval = int(interval.total_seconds())\n        if not isinstance(interval, int):\n            raise ValueError\n        data = self._wrap_post_data(title=title, content=content, interval=interval)\n        res = requests.post(url, data, timeout=self.timeout)\n        if res.status_code == requests.codes.ok:\n            res_data = json.loads(self._convert_bytes(res.content))\n            if res_data.get('status') == STATUS_SUCCESS:\n                return True, res_data.get('message')\n            return False, res_data.get('message')\n        res.raise_for_status()\n        return False, 'Request or Response Error'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_to(self, content, search):\n        url = '{0}send_to_message'.format(self.remote)\n        if isinstance(search, dict):\n            search = json.dumps(search)\n        elif isinstance(search, list):\n            search = reduce(lambda x, y: '{0} {1}'.format(x, y), search)\n        data = self._wrap_post_data(content=content, search=search)\n        res = requests.post(url, data=data, timeout=self.timeout)\n        if res.status_code == requests.codes.ok:\n            res_data = json.loads(self._convert_bytes(res.content))\n            if res_data.get('status') == STATUS_SUCCESS:\n                return True, res_data.get('message')\n            return False, res_data.get('message')\n        res.raise_for_status()\n        return False, 'Request or Response Error'", "response": "send_to - Sends content to a specific topic"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the config. ini file and return a list of all the config files", "response": "def _read_config_list():\n    \"\"\"\n    \u914d\u7f6e\u5217\u8868\u8bfb\u53d6\n    \"\"\"\n    with codecs.open('conf.ini', 'w+', encoding='utf-8') as f1:\n        conf_list = [conf for conf in f1.read().split('\\n') if conf != '']\n        return conf_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a config file to the conf. ini file.", "response": "def write_config(name, value):\n    \"\"\"\n    \u914d\u7f6e\u5199\u5165\n    \"\"\"\n    name = name.lower()\n    new = True\n    conf_list = _read_config_list()\n    for i, conf in enumerate(conf_list):\n        if conf.startswith(name):\n            conf_list[i] = '{0}={1}'.format(name, value)\n            new = False\n            break\n    if new:\n        conf_list.append('{0}={1}'.format(name, value))\n\n    with codecs.open('conf.ini', 'w+', encoding='utf-8') as f1:\n        for conf in conf_list:\n            f1.write(conf + '\\n')\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_config(name):\n    name = name.lower()\n    conf_list = _read_config_list()\n    for conf in conf_list:\n        if conf.startswith(name):\n            return conf.split('=')[1].split('#')[0].strip()\n    return None", "response": "read a config from the device"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_receivers(self, receivers):\n        if not receivers:\n            self.default_receiver = self.bot.file_helper\n            return True\n        if isinstance(receivers, list):\n            self.default_receiver = receivers[0]\n            for receiver in receivers:\n                if self.bot.puid_map:\n                    self.receivers[receiver.puid] = receiver\n                self.receivers[receiver.name] = receiver\n        else:\n            self.default_receiver = receivers\n            if self.bot.puid_map:\n                self.receivers[receivers.puid] = receivers\n            self.receivers[receivers.name] = receivers", "response": "init_receivers - inits the receivers dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_msg(self, msg):\n        for receiver in msg.receivers:\n            current_receiver = self.receivers.get(receiver, self.default_receiver)\n            current_receiver.send_msg(msg)", "response": "sends a message to all receivers"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_message(self):\n        message = None\n        if self.title:\n            message = '\u6807\u9898\uff1a{0}'.format(self.title)\n        if self.message_time:\n            message = '{0}\\n\u65f6\u95f4\uff1a{1}'.format(message, self.time)\n        if message:\n            message = '{0}\\n\u5185\u5bb9\uff1a{1}'.format(message, self.content)\n        else:\n            message = self.content\n        return message", "response": "Render the message to a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_run_info():\n    uptime = datetime.datetime.now() - datetime.datetime.fromtimestamp(glb.run_info.create_time())\n    memory_usage = glb.run_info.memory_info().rss\n    msg = '[\u5f53\u524d\u65f6\u95f4] {now:%H:%M:%S}\\n[\u8fd0\u884c\u65f6\u95f4] {uptime}\\n[\u5185\u5b58\u5360\u7528] {memory}\\n[\u53d1\u9001\u6d88\u606f] {messages}'.format(\n        now=datetime.datetime.now(),\n        uptime=str(uptime).split('.')[0],\n        memory='{:.2f} MB'.format(memory_usage / 1024 ** 2),\n        messages=len(glb.wxbot.bot.messages)\n    )\n    return msg", "response": "Generate a string with the current run info."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef listen(bot, receivers=None, token=None, port=10245, status_report=False, status_receiver=None,\n           status_interval=DEFAULT_REPORT_TIME):\n    \"\"\"\n    \u4f20\u5165 bot \u5b9e\u4f8b\u5e76\u542f\u52a8 wechat_sender \u670d\u52a1\n\n    :param bot: (\u5fc5\u586b|Bot\u5bf9\u8c61) - wxpy \u7684 Bot \u5bf9\u8c61\u5b9e\u4f8b\n    :param receivers: (\u9009\u586b|wxpy.Chat \u5bf9\u8c61|Chat \u5bf9\u8c61\u5217\u8868) - \u6d88\u606f\u63a5\u6536\u8005\uff0cwxpy \u7684 Chat \u5bf9\u8c61\u5b9e\u4f8b, \u6216 Chat \u5bf9\u8c61\u5217\u8868\uff0c\u5982\u679c\u4e3a list \u7b2c\u4e00\u4e2a Chat \u4e3a\u9ed8\u8ba4\u63a5\u6536\u8005\u3002\u5982\u679c\u4e3a Chat \u5bf9\u8c61\uff0c\u5219\u9ed8\u8ba4\u63a5\u6536\u8005\u4e5f\u662f\u6b64\u5bf9\u8c61\u3002 \u4e0d\u586b\u4e3a\u5f53\u524d bot \u5bf9\u8c61\u7684\u6587\u4ef6\u63a5\u6536\u8005\n    :param token: (\u9009\u586b|str) - \u4fe1\u4ee4\uff0c\u9632\u6b62 receiver \u88ab\u975e\u6cd5\u6ee5\u7528\uff0c\u5efa\u8bae\u52a0\u4e0a token \u9632\u6b62\u975e\u6cd5\u4f7f\u7528\uff0c\u5982\u679c\u4f7f\u7528 token \u8bf7\u5728\u521d\u59cb\u5316 `Sender()` \u65f6\u4e5f\u4f7f\u7528\u7edf\u4e00 token\uff0c\u5426\u5219\u65e0\u6cd5\u53d1\u9001\u3002token \u5efa\u8bae\u4e3a 32 \u4f4d\u53ca\u4ee5\u4e0a\u7684\u65e0\u89c4\u5f8b\u5b57\u7b26\u4e32\n    :param port: (\u9009\u586b|int) - \u76d1\u542c\u7aef\u53e3, \u76d1\u542c\u7aef\u53e3\u9ed8\u8ba4\u4e3a 10245 \uff0c\u5982\u6709\u51b2\u7a81\u6216\u7279\u6b8a\u9700\u8981\u8bf7\u81ea\u884c\u6307\u5b9a\uff0c\u9700\u8981\u548c `Sender()` \u7edf\u4e00\n    :param status_report: (\u9009\u586b|bool) - \u662f\u5426\u5f00\u542f\u72b6\u6001\u62a5\u544a\uff0c\u5982\u679c\u5f00\u542f\uff0cwechat_sender \u5c06\u4f1a\u5b9a\u65f6\u53d1\u9001\u72b6\u6001\u4fe1\u606f\u5230 status_receiver\n    :param status_receiver: (\u9009\u586b|Chat \u5bf9\u8c61) - \u6307\u5b9a status_receiver\uff0c\u4e0d\u586b\u5c06\u4f1a\u53d1\u9001\u72b6\u6001\u6d88\u606f\u7ed9\u9ed8\u8ba4\u63a5\u6536\u8005\n    :param status_interval: (\u9009\u586b|int|datetime.timedelta) - \u6307\u5b9a\u72b6\u6001\u62a5\u544a\u53d1\u9001\u95f4\u9694\u65f6\u95f4\uff0c\u4e3a integer \u65f6\u4ee3\u8868\u6beb\u79d2\n\n    \"\"\"\n    global glb\n    periodic_list = []\n    app = Application()\n    wxbot = WxBot(bot, receivers, status_receiver)\n    register_listener_handle(wxbot)\n    process = psutil.Process()\n    app.listen(port)\n\n    if status_report:\n        if isinstance(status_interval, datetime.timedelta):\n            status_interval = status_interval.seconds * 1000\n        check_periodic = tornado.ioloop.PeriodicCallback(functools.partial(check_bot, SYSTEM_TASK), status_interval)\n        check_periodic.start()\n        periodic_list.append(check_periodic)\n\n    glb = Global(wxbot=wxbot, run_info=process, periodic_list=periodic_list, ioloop=tornado.ioloop.IOLoop.instance(),\n                 token=token)\n    tornado.ioloop.IOLoop.current().start()", "response": "listen bot to receivers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(\n    target,\n    target_type,\n    tags=None,\n    ruleset_name=None,\n    ruleset_file=None,\n    ruleset=None,\n    logging_level=logging.WARNING,\n    checks_paths=None,\n    pull=None,\n    insecure=False,\n    skips=None,\n    timeout=None,\n):\n    \"\"\"\n    Runs the sanity checks for the target.\n\n    :param timeout: timeout per-check (in seconds)\n    :param skips: name of checks to skip\n    :param target: str (image name, ostree or dockertar)\n                    or ImageTarget\n                    or path/file-like object for dockerfile\n    :param target_type: string, either image, dockerfile, dockertar\n    :param tags: list of str (if not None, the checks will be filtered by tags.)\n    :param ruleset_name: str (e.g. fedora; if None, default would be used)\n    :param ruleset_file: fileobj instance holding ruleset configuration\n    :param ruleset: dict, content of a ruleset file\n    :param logging_level: logging level (default logging.WARNING)\n    :param checks_paths: list of str, directories where the checks are present\n    :param pull: bool, pull the image from registry\n    :param insecure: bool, pull from an insecure registry (HTTP/invalid TLS)\n    :return: Results instance\n    \"\"\"\n    _set_logging(level=logging_level)\n    logger.debug(\"Checking started.\")\n    target = Target.get_instance(\n        target=target,\n        logging_level=logging_level,\n        pull=pull,\n        target_type=target_type,\n        insecure=insecure,\n    )\n    checks_to_run = _get_checks(\n        target_type=target.__class__,\n        tags=tags,\n        ruleset_name=ruleset_name,\n        ruleset_file=ruleset_file,\n        ruleset=ruleset,\n        checks_paths=checks_paths,\n        skips=skips,\n    )\n    result = go_through_checks(target=target, checks=checks_to_run, timeout=timeout)\n    return result", "response": "Runs sanity checks for the target."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the sanity checks for the given target.", "response": "def get_checks(\n    target_type=None,\n    tags=None,\n    ruleset_name=None,\n    ruleset_file=None,\n    ruleset=None,\n    logging_level=logging.WARNING,\n    checks_paths=None,\n    skips=None,\n):\n    \"\"\"\n    Get the sanity checks for the target.\n\n    :param skips: name of checks to skip\n    :param target_type: TargetType enum\n    :param tags: list of str (if not None, the checks will be filtered by tags.)\n    :param ruleset_name: str (e.g. fedora; if None, default would be used)\n    :param ruleset_file: fileobj instance holding ruleset configuration\n    :param ruleset: dict, content of a ruleset file\n    :param logging_level: logging level (default logging.WARNING)\n    :param checks_paths: list of str, directories where the checks are present\n    :return: list of check instances\n    \"\"\"\n    _set_logging(level=logging_level)\n    logger.debug(\"Finding checks started.\")\n    return _get_checks(\n        target_type=target_type,\n        tags=tags,\n        ruleset_name=ruleset_name,\n        ruleset_file=ruleset_file,\n        ruleset=ruleset,\n        checks_paths=checks_paths,\n        skips=skips,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the personal logger for this library.", "response": "def _set_logging(\n        logger_name=\"colin\",\n        level=logging.INFO,\n        handler_class=logging.StreamHandler,\n        handler_kwargs=None,\n        format='%(asctime)s.%(msecs).03d %(filename)-17s %(levelname)-6s %(message)s',\n        date_format='%H:%M:%S'):\n    \"\"\"\n    Set personal logger for this library.\n\n    :param logger_name: str, name of the logger\n    :param level: int, see logging.{DEBUG,INFO,ERROR,...}: level of logger and handler\n    :param handler_class: logging.Handler instance, default is StreamHandler (/dev/stderr)\n    :param handler_kwargs: dict, keyword arguments to handler's constructor\n    :param format: str, formatting style\n    :param date_format: str, date style in the logs\n    \"\"\"\n    if level != logging.NOTSET:\n        logger = logging.getLogger(logger_name)\n        logger.setLevel(level)\n\n        # do not readd handlers if they are already present\n        if not [x for x in logger.handlers if isinstance(x, handler_class)]:\n            handler_kwargs = handler_kwargs or {}\n            handler = handler_class(**handler_kwargs)\n            handler.setLevel(level)\n\n            formatter = logging.Formatter(format, date_format)\n            handler.setFormatter(formatter)\n            logger.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the label is required and match the regex", "response": "def check_label(labels, required, value_regex, target_labels):\n    \"\"\"\n    Check if the label is required and match the regex\n\n    :param labels: [str]\n    :param required: bool (if the presence means pass or not)\n    :param value_regex: str (using search method)\n    :param target_labels: [str]\n    :return: bool (required==True: True if the label is present and match the regex if specified)\n                    (required==False: True if the label is not present)\n    \"\"\"\n    present = target_labels is not None and not set(labels).isdisjoint(set(target_labels))\n\n    if present:\n        if required and not value_regex:\n            return True\n        elif value_regex:\n            pattern = re.compile(value_regex)\n            present_labels = set(labels) & set(target_labels)\n            for l in present_labels:\n                if not bool(pattern.search(target_labels[l])):\n                    return False\n            return True\n        else:\n            return False\n\n    else:\n        return not required"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef json(self):\n        return {\n            'name': self.name,\n            'message': self.message,\n            'description': self.description,\n            'reference_url': self.reference_url,\n            'tags': self.tags,\n        }", "response": "Get json representation of the check\n            object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting path to checks.", "response": "def get_checks_paths(checks_paths=None):\n    \"\"\"\n    Get path to checks.\n\n    :param checks_paths: list of str, directories where the checks are present\n    :return: list of str (absolute path of directory with checks)\n    \"\"\"\n    p = os.path.join(__file__, os.pardir, os.pardir, os.pardir, \"checks\")\n    p = os.path.abspath(p)\n    # let's utilize the default upstream checks always\n    if checks_paths:\n        p += [os.path.abspath(x) for x in checks_paths]\n    return [p]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the ruleset file from name", "response": "def get_ruleset_file(ruleset=None):\n    \"\"\"\n    Get the ruleset file from name\n\n    :param ruleset: str\n    :return: str\n    \"\"\"\n    ruleset = ruleset or \"default\"\n\n    ruleset_dirs = get_ruleset_dirs()\n    for ruleset_directory in ruleset_dirs:\n        possible_ruleset_files = [os.path.join(ruleset_directory, ruleset + ext) for ext in EXTS]\n\n        for ruleset_file in possible_ruleset_files:\n            if os.path.isfile(ruleset_file):\n                logger.debug(\"Ruleset file '{}' found.\".format(ruleset_file))\n                return ruleset_file\n\n    logger.warning(\"Ruleset with the name '{}' cannot be found at '{}'.\"\n                   .format(ruleset, ruleset_dirs))\n    raise ColinRulesetException(\"Ruleset with the name '{}' cannot be found.\".format(ruleset))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ruleset_dirs():\n\n    ruleset_dirs = []\n\n    cwd_rulesets = os.path.join(\".\", RULESET_DIRECTORY_NAME)\n    if os.path.isdir(cwd_rulesets):\n        logger.debug(\"Ruleset directory found in current directory ('{}').\".format(cwd_rulesets))\n        ruleset_dirs.append(cwd_rulesets)\n\n    if \"VIRTUAL_ENV\" in os.environ:\n        venv_local_share = os.path.join(os.environ[\"VIRTUAL_ENV\"],\n                                        RULESET_DIRECTORY)\n        if os.path.isdir(venv_local_share):\n            logger.debug(\"Virtual env ruleset directory found ('{}').\".format(venv_local_share))\n            ruleset_dirs.append(venv_local_share)\n\n    local_share = os.path.join(os.path.expanduser(\"~\"),\n                               \".local\",\n                               RULESET_DIRECTORY)\n    if os.path.isdir(local_share):\n        logger.debug(\"Local ruleset directory found ('{}').\".format(local_share))\n        ruleset_dirs.append(local_share)\n\n    usr_local_share = os.path.join(\"/usr/local\", RULESET_DIRECTORY)\n    if os.path.isdir(usr_local_share):\n        logger.debug(\"Global ruleset directory found ('{}').\".format(usr_local_share))\n        ruleset_dirs.append(usr_local_share)\n\n    if not ruleset_dirs:\n        msg = \"Ruleset directory cannot be found.\"\n        logger.warning(msg)\n        raise ColinRulesetException(msg)\n\n    return ruleset_dirs", "response": "Get the directory with ruleset files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all checks for given type and tags.", "response": "def get_checks(self, target_type, tags=None, skips=None):\n        \"\"\"\n        Get all checks for given type/tags.\n\n        :param skips: list of str\n        :param target_type: TargetType class\n        :param tags: list of str\n        :return: list of check instances\n        \"\"\"\n        skips = skips or []\n        result = []\n        for check_struct in self.ruleset_struct.checks:\n            if check_struct.name in skips:\n                continue\n\n            logger.debug(\"Processing check_struct {}.\".format(check_struct))\n\n            usable_targets = check_struct.usable_targets\n            if target_type and usable_targets \\\n                    and target_type.get_compatible_check_class().check_type not in usable_targets:\n                logger.info(\"Skipping... Target type does not match.\")\n                continue\n\n            if check_struct.import_name:\n                check_class = self.check_loader.import_class(check_struct.import_name)\n            else:\n                try:\n                    check_class = self.check_loader.mapping[check_struct.name]\n                except KeyError:\n                    logger.error(\"Check %s was not found -- it can't be loaded\", check_struct.name)\n                    raise ColinRulesetException(\n                        \"Check {} can't be loaded, we couldn't find it.\".format(check_struct.name))\n            check_instance = check_class()\n\n            if check_struct.tags:\n                logger.info(\"Overriding check's tags %s with the one defined in ruleset: %s\",\n                            check_instance.tags, check_struct.tags)\n                check_instance.tags = check_struct.tags[:]\n            if check_struct.additional_tags:\n                logger.info(\"Adding additional tags: %s\", check_struct.additional_tags)\n                check_instance.tags += check_struct.additional_tags\n\n            if not is_compatible(target_type=target_type, check_instance=check_instance):\n                logger.error(\n                    \"Check '{}' not compatible with the target type: {}\".format(\n                        check_instance.name, target_type.get_compatible_check_class().check_type))\n                raise ColinRulesetException(\n                    \"Check {} can't be used for target type {}\".format(\n                        check_instance, target_type.get_compatible_check_class().check_type))\n\n            if tags:\n                if not set(tags) < set(check_instance.tags):\n                    logger.debug(\n                        \"Check '{}' not passed the tag control: {}\".format(check_instance.name,\n                                                                           tags))\n                    continue\n\n            # and finally, attach attributes from ruleset to the check instance\n            for k, v in check_struct.other_attributes.items():\n                # yes, this overrides things; yes, users may easily and severely broke their setup\n                setattr(check_instance, k, v)\n\n            result.append(check_instance)\n            logger.debug(\"Check instance {} added.\".format(check_instance.name))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_version_msg_from_the_cmd(package_name, cmd=None, use_rpm=None,\n                                 max_lines_of_the_output=None):\n    \"\"\"\n    Get str with the version (or string representation of the error).\n\n    :param package_name: str\n    :param cmd: str or [str] (defaults to [package_name, \"--version\"])\n    :param use_rpm: True/False/None (whether to use rpm -q for getting a version)\n    :param max_lines_of_the_output: use first n lines of the output\n    :return: str\n    \"\"\"\n    if use_rpm is None:\n        use_rpm = is_rpm_installed()\n    if use_rpm:\n        rpm_version = get_rpm_version(package_name=package_name)\n        if rpm_version:\n            return \"{} (rpm)\".format(rpm_version)\n\n    try:\n        cmd = cmd or [package_name, \"--version\"]\n        version_result = subprocess.run(cmd,\n                                        stdout=subprocess.PIPE,\n                                        stderr=subprocess.PIPE)\n        if version_result.returncode == 0:\n            version_output = version_result.stdout.decode().rstrip()\n            if max_lines_of_the_output:\n                version_output = \" \".join(version_output.split('\\n')[:max_lines_of_the_output])\n            return version_output\n\n        else:\n            return \"{}: cannot get version with {}\".format(package_name, cmd)\n    except FileNotFoundError:\n        return \"{} not accessible!\".format(package_name)", "response": "Get str with the version of the package."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_rpm_version(package_name):\n    version_result = subprocess.run([\"rpm\", \"-q\", package_name],\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n    if version_result.returncode == 0:\n        return version_result.stdout.decode().rstrip()\n    else:\n        return None", "response": "Get a version of the package with rpm - q command."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_rpm_installed():\n    try:\n        version_result = subprocess.run([\"rpm\", \"--usage\"],\n                                        stdout=subprocess.PIPE,\n                                        stderr=subprocess.PIPE)\n        rpm_installed = not version_result.returncode\n    except FileNotFoundError:\n        rpm_installed = False\n    return rpm_installed", "response": "Tests if the rpm command is present."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exit_after(s):\n\n    def outer(fn):\n        def inner(*args, **kwargs):\n            timer = threading.Timer(s, thread.interrupt_main)\n            timer.start()\n            try:\n                result = fn(*args, **kwargs)\n            except KeyboardInterrupt:\n                raise TimeoutError(\"Function '{}' hit the timeout ({}s).\".format(fn.__name__, s))\n            finally:\n                timer.cancel()\n            return result\n\n        return inner\n\n    return outer", "response": "Use as decorator to exit process if\n    function takes longer than s seconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse as decorator to retry functions few times with delays Exception will be raised if last call fails :param retry_count: int could of retries in case of failures. It must be a positive number :param delay: int delay between retries", "response": "def retry(retry_count=5, delay=2):\n    \"\"\"\n    Use as decorator to retry functions few times with delays\n\n    Exception will be raised if last call fails\n\n    :param retry_count: int could of retries in case of failures. It must be\n                        a positive number\n    :param delay: int delay between retries\n    \"\"\"\n    if retry_count <= 0:\n        raise ValueError(\"retry_count have to be positive\")\n\n    def decorator(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            for i in range(retry_count, 0, -1):\n                try:\n                    return f(*args, **kwargs)\n                except Exception:\n                    if i <= 1:\n                        raise\n                time.sleep(delay)\n        return wrapper\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the string representation of an image name into a new instance of ImageName.", "response": "def parse(cls, image_name):\n        \"\"\"\n        Get the instance of ImageName from the string representation.\n\n        :param image_name: str (any possible form of image name)\n        :return: ImageName instance\n        \"\"\"\n        result = cls()\n\n        # registry.org/namespace/repo:tag\n        s = image_name.split('/', 2)\n\n        if len(s) == 2:\n            if '.' in s[0] or ':' in s[0]:\n                result.registry = s[0]\n            else:\n                result.namespace = s[0]\n        elif len(s) == 3:\n            result.registry = s[0]\n            result.namespace = s[1]\n        result.repository = s[-1]\n\n        try:\n            result.repository, result.digest = result.repository.rsplit(\"@\", 1)\n        except ValueError:\n            try:\n                result.repository, result.tag = result.repository.rsplit(\":\", 1)\n            except ValueError:\n                result.tag = \"latest\"\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the string representation of the image .", "response": "def name(self):\n        \"\"\"\n        Get the string representation of the image\n        (registry, namespace, repository and digest together).\n\n        :return: str\n        \"\"\"\n        name_parts = []\n        if self.registry:\n            name_parts.append(self.registry)\n\n        if self.namespace:\n            name_parts.append(self.namespace)\n\n        if self.repository:\n            name_parts.append(self.repository)\n        name = \"/\".join(name_parts)\n\n        if self.digest:\n            name += \"@{}\".format(self.digest)\n        elif self.tag:\n            name += \":{}\".format(self.tag)\n\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nicer_get(di, required, *path):\n\n    r = di\n    for p in path:\n        try:\n            r = r[p]\n        except KeyError:\n            if required:\n                logger.error(\"can't locate %s in ruleset dict, keys present: %s\",\n                             p, list(r.keys()))\n                logger.debug(\"full dict = %s\", r)\n                raise ColinRulesetException(\"Validation error: can't locate %s in ruleset.\" % p)\n            return\n    return r", "response": "This function is a nicer way of doing dict. get"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn dict with all other data except for the described above", "response": "def other_attributes(self):\n        \"\"\" return dict with all other data except for the described above\"\"\"\n        return {k: v for k, v in self.c.items() if\n                k not in [\"name\", \"names\", \"tags\", \"additional_tags\", \"usable_targets\"]}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef should_we_load(kls):\n    # we don't load abstract classes\n    if kls.__name__.endswith(\"AbstractCheck\"):\n        return False\n    # and we only load checks\n    if not kls.__name__.endswith(\"Check\"):\n        return False\n    mro = kls.__mro__\n    # and the class needs to be a child of AbstractCheck\n    for m in mro:\n        if m.__name__ == \"AbstractCheck\":\n            return True\n    return False", "response": "Returns True if we should load this class as a check."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding children of AbstractCheck class and return them as a list", "response": "def obtain_check_classes(self):\n        \"\"\" find children of AbstractCheck class and return them as a list \"\"\"\n        check_classes = set()\n        for path in self.paths:\n            for root, _, files in os.walk(path):\n                for fi in files:\n                    if not fi.endswith(\".py\"):\n                        continue\n                    path = os.path.join(root, fi)\n                    check_classes = check_classes.union(set(\n                        load_check_classes_from_file(path)))\n        return list(check_classes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_class(self, import_name):\n        module_name, class_name = import_name.rsplit(\".\", 1)\n        mod = import_module(module_name)\n        check_class = getattr(mod, class_name)\n        self.mapping[check_class.name] = check_class\n        logger.info(\"successfully loaded class %s\", check_class)\n        return check_class", "response": "import a class from a module"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dict_of_results(self):\n        result_json = {}\n\n        result_list = []\n        for r in self.results:\n            result_list.append({\n                'name': r.check_name,\n                'ok': r.ok,\n                'status': r.status,\n                'description': r.description,\n                'message': r.message,\n                'reference_url': r.reference_url,\n                'logs': r.logs,\n            })\n        result_json[\"checks\"] = result_list\n        return result_json", "response": "Get the dictionary representation of results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef statistics(self):\n        result = {}\n        for r in self.results:\n            result.setdefault(r.status, 0)\n            result[r.status] += 1\n        return result", "response": "Get the dictionary with the count of the check - statuses\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the formatted output for the current page.", "response": "def generate_pretty_output(self, stat, verbose, output_function, logs=True):\n        \"\"\"\n        Send the formated to the provided function\n\n        :param stat: if True print stat instead of full output\n        :param verbose: bool\n        :param output_function: function to send output to\n        \"\"\"\n\n        has_check = False\n        for r in self.results:\n            has_check = True\n            if stat:\n                output_function(OUTPUT_CHARS[r.status],\n                                fg=COLOURS[r.status],\n                                nl=False)\n            else:\n                output_function(str(r), fg=COLOURS[r.status])\n                if verbose:\n                    output_function(\"  -> {}\\n\"\n                                    \"  -> {}\".format(r.description,\n                                                     r.reference_url),\n                                    fg=COLOURS[r.status])\n                    if logs and r.logs:\n                        output_function(\"  -> logs:\",\n                                        fg=COLOURS[r.status])\n                        for l in r.logs:\n                            output_function(\"    -> {}\".format(l),\n                                            fg=COLOURS[r.status])\n\n        if not has_check:\n            output_function(\"No check found.\")\n        elif stat and not verbose:\n            output_function(\"\")\n        else:\n            output_function(\"\")\n            for status, count in six.iteritems(self.statistics):\n                output_function(\"{}:{} \".format(status, count), nl=False)\n            output_function(\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_pretty_string(self, stat, verbose):\n        pretty_output = _PrettyOutputToStr()\n        self.generate_pretty_output(stat=stat,\n                                    verbose=verbose,\n                                    output_function=pretty_output.save_output)\n        return pretty_output.result", "response": "Returns a pretty string representation of the results\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef receive_fmf_metadata(name, path, object_list=False):\n    output = {}\n    fmf_tree = ExtendedTree(path)\n    logger.debug(\"get FMF metadata for test (path:%s name=%s)\", path, name)\n    # ignore items with @ in names, to avoid using unreferenced items\n    items = [x for x in fmf_tree.climb() if x.name.endswith(\"/\" + name) and \"@\" not in x.name]\n    if object_list:\n        return items\n    if len(items) == 1:\n        output = items[0]\n    elif len(items) > 1:\n        raise Exception(\"There is more FMF test metadata for item by name:{}({}) {}\".format(\n            name, len(items), [x.name for x in items]))\n    elif not items:\n        raise Exception(\"Unable to get FMF metadata for: {}\".format(name))\n    return output", "response": "get FMF metadata for a given name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check(target, ruleset, ruleset_file, debug, json, stat, skip, tag, verbose,\n          checks_paths, target_type, timeout, pull, insecure):\n    \"\"\"\n    Check the image/dockerfile (default).\n    \"\"\"\n    if ruleset and ruleset_file:\n        raise click.BadOptionUsage(\n            \"Options '--ruleset' and '--file-ruleset' cannot be used together.\")\n\n    if json and not os.path.isdir(os.path.dirname(os.path.realpath(json.name))):\n        raise click.BadOptionUsage(\n            \"Parent directory for the json output file does not exist.\")\n\n    try:\n        if not debug:\n            logging.basicConfig(stream=six.StringIO())\n\n        log_level = _get_log_level(debug=debug,\n                                   verbose=verbose)\n        results = run(\n            target=target,\n            ruleset_name=ruleset,\n            ruleset_file=ruleset_file,\n            logging_level=log_level,\n            tags=tag,\n            pull=pull,\n            checks_paths=checks_paths,\n            target_type=target_type,\n            timeout=timeout,\n            insecure=insecure,\n            skips=skip\n        )\n        _print_results(results=results, stat=stat, verbose=verbose)\n\n        if json:\n            results.save_json_to_file(file=json)\n\n        if not results.ok:\n            sys.exit(1)\n        elif results.fail:\n            sys.exit(3)\n\n    except ColinException as ex:\n        logger.error(\"An error occurred: %r\", ex)\n        if debug:\n            raise\n        else:\n            raise click.ClickException(str(ex))\n    except Exception as ex:\n        logger.error(\"An error occurred: %r\", ex)\n        if debug:\n            raise\n        else:\n            raise click.ClickException(str(ex))", "response": "Check the image and dockerfile for the specified rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_checks(ruleset, ruleset_file, debug, json, skip, tag, verbose, checks_paths):\n    if ruleset and ruleset_file:\n        raise click.BadOptionUsage(\n            \"Options '--ruleset' and '--file-ruleset' cannot be used together.\")\n\n    try:\n        if not debug:\n            logging.basicConfig(stream=six.StringIO())\n\n        log_level = _get_log_level(debug=debug,\n                                   verbose=verbose)\n        checks = get_checks(ruleset_name=ruleset,\n                            ruleset_file=ruleset_file,\n                            logging_level=log_level,\n                            tags=tag,\n                            checks_paths=checks_paths,\n                            skips=skip)\n        _print_checks(checks=checks)\n\n        if json:\n            AbstractCheck.save_checks_to_json(file=json, checks=checks)\n    except ColinException as ex:\n        logger.error(\"An error occurred: %r\", ex)\n        if debug:\n            raise\n        else:\n            raise click.ClickException(str(ex))\n    except Exception as ex:\n        logger.error(\"An error occurred: %r\", ex)\n        if debug:\n            raise\n        else:\n            raise click.ClickException(str(ex))", "response": "List the checks in a single page."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef info():\n    installation_path = os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir))\n\n    click.echo(\"colin {} {}\".format(__version__, installation_path))\n    click.echo(\"colin-cli {}\\n\".format(os.path.realpath(__file__)))\n\n    # click.echo(get_version_of_the_python_package(module=conu))\n\n    rpm_installed = is_rpm_installed()\n    click.echo(get_version_msg_from_the_cmd(package_name=\"podman\", use_rpm=rpm_installed))\n    click.echo(get_version_msg_from_the_cmd(package_name=\"skopeo\", use_rpm=rpm_installed))\n    click.echo(get_version_msg_from_the_cmd(package_name=\"ostree\",\n                                            use_rpm=rpm_installed,\n                                            max_lines_of_the_output=3))", "response": "Show info about colin and its dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _print_results(results, stat=False, verbose=False):\n    results.generate_pretty_output(stat=stat,\n                                   verbose=verbose,\n                                   output_function=click.secho)", "response": "Print the results to stdout."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inspect_object(obj, refresh=True):\n    if hasattr(obj, \"inspect\"):\n        return obj.inspect(refresh=refresh)\n    return obj.get_metadata(refresh=refresh)", "response": "inspect provided object and return raw dict with the metadata"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an instance of the class specified by the target type.", "response": "def get_instance(target_type, **kwargs):\n        \"\"\"\n        :param target_type: string, either image, dockertar, ostree or dockerfile\n        \"\"\"\n        if target_type in TARGET_TYPES:\n            cls = TARGET_TYPES[target_type]\n            try:\n                return cls(**kwargs)\n            except Exception:\n                logger.error(\"Please make sure that you picked the correct target type: \"\n                             \"--target-type CLI option.\")\n                raise\n\n        raise ColinException(\n            \"Unknown target type '{}'. Please make sure that you picked the correct target type: \"\n            \"--target-type CLI option.\".format(target_type))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets list of labels from the target instance.", "response": "def labels(self):\n        \"\"\"\n        Get list of labels from the target instance.\n\n        :return: [str]\n        \"\"\"\n        if self._labels is None:\n            self._labels = self.instance.labels\n        return self._labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_file(self, file_path):\n        try:\n            with open(self.cont_path(file_path)) as fd:\n                return fd.read()\n        except IOError as ex:\n            logger.error(\"error while accessing file %s: %r\", file_path, ex)\n            raise ColinException(\n                \"There was an error while accessing file %s: %r\" % (file_path, ex))", "response": "read file specified via file_path and return its content"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_file(self, file_path, mode=\"r\"):\n        return open(self.cont_path(file_path), mode=mode)", "response": "provide File object specified via file_path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef file_is_present(self, file_path):\n        real_path = self.cont_path(file_path)\n        if not os.path.exists(real_path):\n            return False\n        if not os.path.isfile(real_path):\n            raise IOError(\"%s is not a file\" % file_path)\n        return True", "response": "check if file is present"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cont_path(self, path):\n        if path.startswith(\"/\"):\n            path = path[1:]\n        real_path = os.path.join(self.mount_point, path)\n        logger.debug(\"path = %s\", real_path)\n        return real_path", "response": "provide absolute path within the container\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mount_point(self):\n        if self._mount_point is None:\n            cmd_create = [\"podman\", \"create\", self.target_name, \"some-cmd\"]\n            self._mounted_container_id = subprocess.check_output(cmd_create).decode().rstrip()\n            cmd_mount = [\"podman\", \"mount\", self._mounted_container_id]\n            self._mount_point = subprocess.check_output(cmd_mount).decode().rstrip()\n        return self._mount_point", "response": "get the mount point of the podman container"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef labels(self):\n        if self._labels is None:\n            cmd = [\"skopeo\", \"inspect\", self.skopeo_target]\n            self._labels = json.loads(subprocess.check_output(cmd))[\"Labels\"]\n        return self._labels", "response": "Get the labels of the skopeo instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef layers_path(self):\n        if self._layers_path is None:\n            self._layers_path = os.path.join(self.tmpdir, \"layers\")\n        return self._layers_path", "response": "Directory with all the layers."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mount_point(self):\n        if self._mount_point is None:\n            self._mount_point = os.path.join(self.tmpdir, \"checkout\")\n            os.makedirs(self._mount_point)\n            self._checkout()\n        return self._mount_point", "response": "get the mount point of the ostree"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ostree_path(self):\n        if self._ostree_path is None:\n            self._ostree_path = os.path.join(self.tmpdir, \"ostree-repo\")\n            subprocess.check_call([\"ostree\", \"init\", \"--mode\", \"bare-user-only\",\n                                   \"--repo\", self._ostree_path])\n        return self._ostree_path", "response": "ostree repository -- content"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck out the image filesystem on self. mount_point", "response": "def _checkout(self):\n        \"\"\" check out the image filesystem on self.mount_point \"\"\"\n        cmd = [\"atomic\", \"mount\", \"--storage\", \"ostree\", self.ref_image_name, self.mount_point]\n        # self.mount_point has to be created by us\n        self._run_and_log(cmd, self.ostree_path,\n                          \"Failed to mount selected image as an ostree repo.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun provided command and log all of its output", "response": "def _run_and_log(cmd, ostree_repo_path, error_msg, wd=None):\n        \"\"\" run provided command and log all of its output; set path to ostree repo \"\"\"\n        logger.debug(\"running command %s\", cmd)\n        kwargs = {\n            \"stderr\": subprocess.STDOUT,\n            \"env\": os.environ.copy(),\n        }\n        if ostree_repo_path:\n            # must not exist, ostree will create it\n            kwargs[\"env\"][\"ATOMIC_OSTREE_REPO\"] = ostree_repo_path\n        if wd:\n            kwargs[\"cwd\"] = wd\n        try:\n            out = subprocess.check_output(cmd, **kwargs)\n        except subprocess.CalledProcessError as ex:\n            logger.error(ex.output)\n            logger.error(error_msg)\n            raise\n        logger.debug(\"%s\", out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef references(self, datatrees, whole=False):\n        if not isinstance(datatrees, list):\n            raise ValueError(\"datatrees argument has to be list of fmf trees\")\n        reference_nodes = self.prune(whole=whole, names=[\"@\"])\n        for node in reference_nodes:\n            node.data = node.original_data\n            ref_item_name = node.name.rsplit(\"@\", 1)[1]\n            # match item what does not contain @ before name, otherwise it\n            # match same item\n            reference_node = None\n            for datatree in datatrees:\n                reference_node = datatree.search(\"[^@]%s\" % ref_item_name)\n                if reference_node is not None:\n                    break\n            if not reference_node:\n                raise ValueError(\"Unable to find reference for node: %s via name search: %s\" %\n                                 (node.name, ref_item_name))\n            logger.debug(\"MERGING: %s @ %s from %s\",\n                         node.name,\n                         reference_node.name,\n                         reference_node.root)\n            node.merge(parent=reference_node)\n\n        self.__remove_append_items(whole=whole)", "response": "This method will search all the datatree s items and find the item that is referenced by the given datatree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self, name):\n        for node in self.climb():\n            if re.search(name, node.name):\n                return node\n        return None", "response": "Search node with given name based on regexp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef login(self, email, password):\n        params = {\n            'email': email,\n            'password': password\n        }\n        return self._get('login', params)", "response": "Login to Todoist.\n\n        :param email: The user's email address.\n        :type email: str\n        :param password: The user's password.\n        :type password: str\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n\n        >>> from pytodoist.api import TodoistAPI\n        >>> api = TodoistAPI()\n        >>> response = api.login('john.doe@gmail.com', 'password')\n        >>> user_info = response.json()\n        >>> full_name = user_info['full_name']\n        >>> print(full_name)\n        John Doe"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef login_with_google(self, email, oauth2_token, **kwargs):\n        params = {\n            'email': email,\n            'oauth2_token': oauth2_token\n        }\n        req_func = self._get\n        if kwargs.get('auto_signup', 0) == 1:  # POST if we're creating a user.\n            req_func = self._post\n        return req_func('login_with_google', params, **kwargs)", "response": "Login to Todoist using Google s oauth2 authentication."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register(self, email, full_name, password, **kwargs):\n        params = {\n            'email': email,\n            'full_name': full_name,\n            'password': password\n        }\n        return self._post('register', params, **kwargs)", "response": "Register a new Todoist user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a user s account.", "response": "def delete_user(self, api_token, password, **kwargs):\n        \"\"\"Delete a registered Todoist user's account.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param password: The user's password.\n        :type password: str\n        :param reason_for_delete: The reason for deletion.\n        :type reason_for_delete: str\n        :param in_background: If ``0``, delete the user instantly.\n        :type in_background: int\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n        \"\"\"\n        params = {\n            'token': api_token,\n            'current_password': password\n        }\n        return self._post('delete_user', params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sync(self, api_token, sync_token, resource_types='[\"all\"]', **kwargs):\n        params = {\n            'token': api_token,\n            'sync_token': sync_token,\n        }\n        req_func = self._post\n        if 'commands' not in kwargs:  # GET if we're not changing data.\n            req_func = self._get\n            params['resource_types'] = resource_types\n        return req_func('sync', params, **kwargs)", "response": "Update and retrieve Todoist data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching all of a user s tasks using date priority and label queries.", "response": "def query(self, api_token, queries, **kwargs):\n        \"\"\"Search all of a user's tasks using date, priority and label queries.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param queries: A JSON list of queries to search. See examples\n            `here <https://todoist.com/Help/timeQuery>`_.\n        :type queries: list (str)\n        :param as_count: If ``1`` then return the count of matching tasks.\n        :type as_count: int\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n        \"\"\"\n        params = {\n            'token': api_token,\n            'queries': queries\n        }\n        return self._get('query', params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a task to a project.", "response": "def add_item(self, api_token, content, **kwargs):\n        \"\"\"Add a task to a project.\n\n        :param token: The user's login token.\n        :type token: str\n        :param content: The task description.\n        :type content: str\n        :param project_id: The project to add the task to. Default is ``Inbox``\n        :type project_id: str\n        :param date_string: The deadline date for the task.\n        :type date_string: str\n        :param priority: The task priority ``(1-4)``.\n        :type priority: int\n        :param indent: The task indentation ``(1-4)``.\n        :type indent: int\n        :param item_order: The task order.\n        :type item_order: int\n        :param children: A list of child tasks IDs.\n        :type children: str\n        :param labels: A list of label IDs.\n        :type labels: str\n        :param assigned_by_uid: The ID of the user who assigns current task.\n            Accepts 0 or any user id from the list of project collaborators.\n            If value is unset or invalid it will automatically be set up by\n            your uid.\n        :type assigned_by_uid: str\n        :param responsible_uid: The id of user who is responsible for\n            accomplishing the current task. Accepts 0 or any user id from\n            the list of project collaborators. If the value is unset or\n            invalid it will automatically be set to null.\n        :type responsible_uid: str\n        :param note: Content of a note to add.\n        :type note: str\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n\n        >>> from pytodoist.api import TodoistAPI\n        >>> api = TodoistAPI()\n        >>> response = api.login('john.doe@gmail.com', 'password')\n        >>> user_info = response.json()\n        >>> user_api_token = user_info['token']\n        >>> response = api.add_item(user_api_token, 'Install PyTodoist')\n        >>> task = response.json()\n        >>> print(task['content'])\n        Install PyTodoist\n        \"\"\"\n        params = {\n            'token': api_token,\n            'content': content\n        }\n        return self._post('add_item', params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a task using the Todoist 'Quick Add Task' syntax. :param api_token: The user's login api_token. :type api_token: str :param text: The text of the task that is parsed. A project name starts with the `#` character, a label starts with a `@` and an assignee starts with a `+`. :type text: str :param note: The content of the note. :type note: str :param reminder: The date of the reminder, added in free form text. :type reminder: str :return: The HTTP response to the request. :rtype: :class:`requests.Response`", "response": "def quick_add(self, api_token, text, **kwargs):\n        \"\"\"Add a task using the Todoist 'Quick Add Task' syntax.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param text: The text of the task that is parsed. A project\n            name starts with the `#` character, a label starts with a `@`\n            and an assignee starts with a `+`.\n        :type text: str\n        :param note: The content of the note.\n        :type note: str\n        :param reminder: The date of the reminder, added in free form text.\n        :type reminder: str\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n        \"\"\"\n        params = {\n            'token': api_token,\n            'text': text\n        }\n        return self._post('quick/add', params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of all tasks that have been completed.", "response": "def get_all_completed_tasks(self, api_token, **kwargs):\n        \"\"\"Return a list of a user's completed tasks.\n\n        .. warning:: Requires Todoist premium.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param project_id: Filter the tasks by project.\n        :type project_id: str\n        :param limit: The maximum number of tasks to return\n            (default ``30``, max ``50``).\n        :type limit: int\n        :param offset: Used for pagination if there are more tasks than limit.\n        :type offset: int\n        :param from_date: Return tasks with a completion date on or older than\n            from_date. Formatted as ``2007-4-29T10:13``.\n        :type from_date: str\n        :param to: Return tasks with a completion date on or less than\n            to_date. Formatted as ``2007-4-29T10:13``.\n        :type from_date: str\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n\n        >>> from pytodoist.api import TodoistAPI\n        >>> api = TodoistAPI()\n        >>> response = api.login('john.doe@gmail.com', 'password')\n        >>> user_info = response.json()\n        >>> user_api_token = user_info['api_token']\n        >>> response = api.get_all_completed_tasks(user_api_token)\n        >>> completed_tasks = response.json()\n        \"\"\"\n        params = {\n            'token': api_token\n        }\n        return self._get('get_all_completed_items', params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nuploading a file suitable to be passed as a file_attachment.", "response": "def upload_file(self, api_token, file_path, **kwargs):\n        \"\"\"Upload a file suitable to be passed as a file_attachment.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param file_path: The path of the file to be uploaded.\n        :type file_path: str\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n        \"\"\"\n        params = {\n            'token': api_token,\n            'file_name': os.path.basename(file_path)\n        }\n        with open(file_path, 'rb') as f:\n            files = {'file': f}\n            return self._post('upload_file', params, files, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_productivity_stats(self, api_token, **kwargs):\n        params = {\n            'token': api_token\n        }\n        return self._get('get_productivity_stats', params, **kwargs)", "response": "Returns a user s productivity stats."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_notification_settings(self, api_token, event,\n                                     service, should_notify):\n        \"\"\"Update a user's notification settings.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param event: Update the notification settings of this event.\n        :type event: str\n        :param service: ``email`` or ``push``\n        :type service: str\n        :param should_notify: If ``0`` notify, otherwise do not.\n        :type should_notify: int\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n\n        >>> from pytodoist.api import TodoistAPI\n        >>> api = TodoistAPI()\n        >>> response = api.login('john.doe@gmail.com', 'password')\n        >>> user_info = response.json()\n        >>> user_api_token = user_info['api_token']\n        >>> response = api.update_notification_settings(user_api_token,\n        ...                                             'user_left_project',\n        ...                                             'email', 0)\n        ...\n        \"\"\"\n        params = {\n            'token': api_token,\n            'notification_type': event,\n            'service': service,\n            'dont_notify': should_notify\n        }\n        return self._post('update_notification_setting', params)", "response": "Update the user s notification settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the absolute URL to redirect or to open a browser.", "response": "def get_redirect_link(self, api_token, **kwargs):\n        \"\"\"Return the absolute URL to redirect or to open in\n        a browser. The first time the link is used it logs in the user\n        automatically and performs a redirect to a given page. Once used,\n        the link keeps working as a plain redirect.\n\n        :param api_token: The user's login api_token.\n        :type api_token: str\n        :param path: The path to redirect the user's browser. Default ``/app``.\n        :type path: str\n        :param hash: The has part of the path to redirect the user's browser.\n        :type hash: str\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n\n        >>> from pytodoist.api import TodoistAPI\n        >>> api = TodoistAPI()\n        >>> response = api.login('john.doe@gmail.com', 'password')\n        >>> user_info = response.json()\n        >>> user_api_token = user_info['api_token']\n        >>> response = api.get_redirect_link(user_api_token)\n        >>> link_info = response.json()\n        >>> redirect_link = link_info['link']\n        >>> print(redirect_link)\n        https://todoist.com/secureRedirect?path=adflk...\n        \"\"\"\n        params = {\n            'token': api_token\n        }\n        return self._get('get_redirect_link', params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a HTTP GET request to a Todoist API end - point.", "response": "def _get(self, end_point, params=None, **kwargs):\n        \"\"\"Send a HTTP GET request to a Todoist API end-point.\n\n        :param end_point: The Todoist API end-point.\n        :type end_point: str\n        :param params: The required request parameters.\n        :type params: dict\n        :param kwargs: Any optional parameters.\n        :type kwargs: dict\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n        \"\"\"\n        return self._request(requests.get, end_point, params, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _post(self, end_point, params=None, files=None, **kwargs):\n        return self._request(requests.post, end_point, params, files, **kwargs)", "response": "Send a HTTP POST request to a Todoist API end - point."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a HTTP request to a Todoist API end - point.", "response": "def _request(self, req_func, end_point, params=None, files=None, **kwargs):\n        \"\"\"Send a HTTP request to a Todoist API end-point.\n\n        :param req_func: The request function to use e.g. get or post.\n        :type req_func: A request function from the :class:`requests` module.\n        :param end_point: The Todoist API end-point.\n        :type end_point: str\n        :param params: The required request parameters.\n        :type params: dict\n        :param files: Any files that are being sent as multipart/form-data.\n        :type files: dict\n        :param kwargs: Any optional parameters.\n        :type kwargs: dict\n        :return: The HTTP response to the request.\n        :rtype: :class:`requests.Response`\n        \"\"\"\n        url = self.URL + end_point\n        if params and kwargs:\n            params.update(kwargs)\n        return req_func(url, params=params, files=files)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef login(email, password):\n    user = _login(API.login, email, password)\n    user.password = password\n    return user", "response": "Login to Todoist.\n\n    :param email: A Todoist user's email address.\n    :type email: str\n    :param password: A Todoist user's password.\n    :type password: str\n    :return: The Todoist user.\n    :rtype: :class:`pytodoist.todoist.User`\n\n    >>> from pytodoist import todoist\n    >>> user = todoist.login('john.doe@gmail.com', 'password')\n    >>> print(user.full_name)\n    John Doe"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging-in to Todoist using a user s api token.", "response": "def login_with_api_token(api_token):\n    \"\"\"Login to Todoist using a user's api token.\n\n    .. note:: It is up to you to obtain the api token.\n\n    :param api_token: A Todoist user's api token.\n    :type api_token: str\n    :return: The Todoist user.\n    :rtype: :class:`pytodoist.todoist.User`\n\n    >>> from pytodoist import todoist\n    >>> api_token = 'api_token'\n    >>> user = todoist.login_with_api_token(api_token)\n    >>> print(user.full_name)\n    John Doe\n    \"\"\"\n    response = API.sync(api_token, '*', '[\"user\"]')\n    _fail_if_contains_errors(response)\n    user_json = response.json()['user']\n    # Required as sync doesn't return the api_token.\n    user_json['api_token'] = user_json['token']\n    return User(user_json)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(full_name, email, password, lang=None, timezone=None):\n    response = API.register(email, full_name, password,\n                            lang=lang, timezone=timezone)\n    _fail_if_contains_errors(response)\n    user_json = response.json()\n    user = User(user_json)\n    user.password = password\n    return user", "response": "Register a new Todoist account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a new Todoist account by linking a Google account.", "response": "def register_with_google(full_name, email, oauth2_token,\n                         lang=None, timezone=None):\n    \"\"\"Register a new Todoist account by linking a Google account.\n\n    :param full_name: The user's full name.\n    :type full_name: str\n    :param email: The user's email address.\n    :type email: str\n    :param oauth2_token: The oauth2 token associated with the email.\n    :type oauth2_token: str\n    :param lang: The user's language.\n    :type lang: str\n    :param timezone: The user's timezone.\n    :type timezone: str\n    :return: The Todoist user.\n    :rtype: :class:`pytodoist.todoist.User`\n\n    .. note:: It is up to you to obtain the valid oauth2 token.\n\n    >>> from pytodoist import todoist\n    >>> oauth2_token = 'oauth2_token'\n    >>> user = todoist.register_with_google('John Doe', 'john.doe@gmail.com',\n    ...                                      oauth2_token)\n    >>> print(user.full_name)\n    John Doe\n    \"\"\"\n    response = API.login_with_google(email, oauth2_token, auto_signup=1,\n                                     full_name=full_name, lang=lang,\n                                     timezone=timezone)\n    _fail_if_contains_errors(response)\n    user_json = response.json()\n    user = User(user_json)\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fail_if_contains_errors(response, sync_uuid=None):\n    if response.status_code != _HTTP_OK:\n        raise RequestError(response)\n    response_json = response.json()\n    if sync_uuid and 'sync_status' in response_json:\n        status = response_json['sync_status']\n        if sync_uuid in status and 'error' in status[sync_uuid]:\n            raise RequestError(response)", "response": "Raise a RequestError Exception if a given response contains errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _perform_command(user, command_type, command_args):\n    command_uuid = _gen_uuid()\n    command = {\n        'type': command_type,\n        'args': command_args,\n        'uuid': command_uuid,\n        'temp_id': _gen_uuid()\n    }\n    commands = json.dumps([command])\n    response = API.sync(user.api_token, user.sync_token, commands=commands)\n    _fail_if_contains_errors(response, command_uuid)\n    response_json = response.json()\n    user.sync_token = response_json['sync_token']", "response": "Perform an operation on Todoist using the API sync end - point."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the user s details on Todoist.", "response": "def update(self):\n        \"\"\"Update the user's details on Todoist.\n\n        This method must be called to register any local attribute changes\n        with Todoist.\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> user.full_name = 'John Smith'\n        >>> # At this point Todoist still thinks the name is 'John Doe'.\n        >>> user.update()\n        >>> # Now the name has been updated on Todoist.\n        \"\"\"\n        args = {attr: getattr(self, attr) for attr in self.to_update}\n        _perform_command(self, 'user_update', args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sync(self, resource_types='[\"all\"]'):\n        response = API.sync(self.api_token, '*', resource_types)\n        _fail_if_contains_errors(response)\n        response_json = response.json()\n        self.sync_token = response_json['sync_token']\n        if 'projects' in response_json:\n            self._sync_projects(response_json['projects'])\n        if 'items' in response_json:\n            self._sync_tasks(response_json['items'])\n        if 'notes' in response_json:\n            self._sync_notes(response_json['notes'])\n        if 'labels' in response_json:\n            self._sync_labels(response_json['labels'])\n        if 'filters' in response_json:\n            self._sync_filters(response_json['filters'])\n        if 'reminders' in response_json:\n            self._sync_reminders(response_json['reminders'])", "response": "Synchronize the user s data with the Todoist server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the user s projects from a JSON encoded list.", "response": "def _sync_projects(self, projects_json):\n        \"\"\"\"Populate the user's projects from a JSON encoded list.\"\"\"\n        for project_json in projects_json:\n            project_id = project_json['id']\n            self.projects[project_id] = Project(project_json, self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npopulate the user s tasks from a JSON encoded list.", "response": "def _sync_tasks(self, tasks_json):\n        \"\"\"\"Populate the user's tasks from a JSON encoded list.\"\"\"\n        for task_json in tasks_json:\n            task_id = task_json['id']\n            project_id = task_json['project_id']\n            if project_id not in self.projects:\n                # ignore orphan tasks\n                continue\n            project = self.projects[project_id]\n            self.tasks[task_id] = Task(task_json, project)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sync_notes(self, notes_json):\n        for note_json in notes_json:\n            note_id = note_json['id']\n            task_id = note_json['item_id']\n            if task_id not in self.tasks:\n                # ignore orphan notes\n                continue\n            task = self.tasks[task_id]\n            self.notes[note_id] = Note(note_json, task)", "response": "Populate the user s notes from a JSON encoded list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating the user s labels from a JSON encoded list.", "response": "def _sync_labels(self, labels_json):\n        \"\"\"\"Populate the user's labels from a JSON encoded list.\"\"\"\n        for label_json in labels_json:\n            label_id = label_json['id']\n            self.labels[label_id] = Label(label_json, self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _sync_filters(self, filters_json):\n        for filter_json in filters_json:\n            filter_id = filter_json['id']\n            self.filters[filter_id] = Filter(filter_json, self)", "response": "Populate the user s filters from a JSON encoded list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npopulate the user s reminders from a JSON encoded list.", "response": "def _sync_reminders(self, reminders_json):\n        \"\"\"\"Populate the user's reminders from a JSON encoded list.\"\"\"\n        for reminder_json in reminders_json:\n            reminder_id = reminder_json['id']\n            task_id = reminder_json['item_id']\n            if task_id not in self.tasks:\n                # ignore orphan reminders\n                continue\n            task = self.tasks[task_id]\n            self.reminders[reminder_id] = Reminder(reminder_json, task)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef quick_add(self, text, note=None, reminder=None):\n        response = API.quick_add(self.api_token, text,\n                                 note=note, reminder=reminder)\n        _fail_if_contains_errors(response)\n        task_json = response.json()\n        return Task(task_json, self)", "response": "Add a task using the Quick Add Task syntax."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_project(self, name, color=None, indent=None, order=None):\n        args = {\n            'name': name,\n            'color': color,\n            'indent': indent,\n            'order': order\n        }\n        args = {k: args[k] for k in args if args[k] is not None}\n        _perform_command(self, 'project_add', args)\n        return self.get_project(name)", "response": "Add a project to the user s account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the project with a given name.", "response": "def get_project(self, project_name):\n        \"\"\"Return the project with a given name.\n\n        :param project_name: The name to search for.\n        :type project_name: str\n        :return: The project that has the name ``project_name`` or ``None``\n            if no project is found.\n        :rtype: :class:`pytodoist.todoist.Project`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('Inbox')\n        >>> print(project.name)\n        Inbox\n        \"\"\"\n        for project in self.get_projects():\n            if project.name == project_name:\n                return project"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all of a user s uncompleted tasks.", "response": "def get_uncompleted_tasks(self):\n        \"\"\"Return all of a user's uncompleted tasks.\n\n        .. warning:: Requires Todoist premium.\n\n        :return: A list of uncompleted tasks.\n        :rtype: list of :class:`pytodoist.todoist.Task`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> uncompleted_tasks = user.get_uncompleted_tasks()\n        >>> for task in uncompleted_tasks:\n        ...    task.complete()\n        \"\"\"\n        tasks = (p.get_uncompleted_tasks() for p in self.get_projects())\n        return list(itertools.chain.from_iterable(tasks))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch tasks in todoist.", "response": "def search_tasks(self, *queries):\n        \"\"\"Return a list of tasks that match some search criteria.\n\n        .. note:: Example queries can be found\n            `here <https://todoist.com/Help/timeQuery>`_.\n\n        .. note:: A standard set of queries are available\n            in the :class:`pytodoist.todoist.Query` class.\n\n        :param queries: Return tasks that match at least one of these queries.\n        :type queries: list str\n        :return: A list tasks that match at least one query.\n        :rtype: list of :class:`pytodoist.todoist.Task`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> tasks = user.search_tasks(todoist.Query.TOMORROW, '18 Sep')\n        \"\"\"\n        queries = json.dumps(queries)\n        response = API.query(self.api_token, queries)\n        _fail_if_contains_errors(response)\n        query_results = response.json()\n        tasks = []\n        for result in query_results:\n            if 'data' not in result:\n                continue\n            all_tasks = result['data']\n            if result['type'] == Query.ALL:\n                all_projects = all_tasks\n                for project_json in all_projects:\n                    uncompleted_tasks = project_json.get('uncompleted', [])\n                    completed_tasks = project_json.get('completed', [])\n                    all_tasks = uncompleted_tasks + completed_tasks\n            for task_json in all_tasks:\n                project_id = task_json['project_id']\n                project = self.projects[project_id]\n                task = Task(task_json, project)\n                tasks.append(task)\n        return tasks"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new label.", "response": "def add_label(self, name, color=None):\n        \"\"\"Create a new label.\n\n        .. warning:: Requires Todoist premium.\n\n        :param name: The name of the label.\n        :type name: str\n        :param color: The color of the label.\n        :type color: str\n        :return: The newly created label.\n        :rtype: :class:`pytodoist.todoist.Label`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> label = user.add_label('family')\n        \"\"\"\n        args = {\n            'name': name,\n            'color': color\n        }\n        _perform_command(self, 'label_register', args)\n        return self.get_label(name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_label(self, label_name):\n        for label in self.get_labels():\n            if label.name == label_name:\n                return label", "response": "Return the user s label that has a given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new filter.", "response": "def add_filter(self, name, query, color=None, item_order=None):\n        \"\"\"Create a new filter.\n\n        .. warning:: Requires Todoist premium.\n\n        :param name: The name of the filter.\n        :param query: The query to search for.\n        :param color: The color of the filter.\n        :param item_order: The filter's order in the filter list.\n        :return: The newly created filter.\n        :rtype: :class:`pytodoist.todoist.Filter`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> overdue_filter = user.add_filter('Overdue', todoist.Query.OVERDUE)\n        \"\"\"\n        args = {\n            'name': name,\n            'query': query,\n            'color': color,\n            'item_order': item_order\n        }\n        _perform_command(self, 'filter_add', args)\n        return self.get_filter(name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_filter(self, name):\n        for flter in self.get_filters():\n            if flter.name == name:\n                return flter", "response": "Return the filter that has the given name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_notification_settings(self, event, service,\n                                      should_notify):\n        \"\"\"Update the settings of a an events notifications.\n\n        :param event: Update the notification settings of this event.\n        :type event: str\n        :param service: The notification service to update.\n        :type service: str\n        :param should_notify: Notify if this is ``1``.\n        :type should_notify: int\n        \"\"\"\n        response = API.update_notification_settings(self.api_token, event,\n                                                    service, should_notify)\n        _fail_if_contains_errors(response)", "response": "Update the notification settings of a specific event and service."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_productivity_stats(self):\n        response = API.get_productivity_stats(self.api_token)\n        _fail_if_contains_errors(response)\n        return response.json()", "response": "Return the user s productivity stats."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_redirect_link(self):\n        response = API.get_redirect_link(self.api_token)\n        _fail_if_contains_errors(response)\n        link_json = response.json()\n        return link_json['link']", "response": "Return the absolute URL to redirect to open in\n        a browser."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, reason=None):\n        response = API.delete_user(self.api_token, self.password,\n                                   reason=reason, in_background=0)\n        _fail_if_contains_errors(response)", "response": "Delete the user s account from Todoist."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\narchiving the project. >>> from pytodoist import todoist >>> user = todoist.login('john.doe@gmail.com', 'password') >>> project = user.get_project('PyTodoist') >>> project.archive()", "response": "def archive(self):\n        \"\"\"Archive the project.\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.archive()\n        \"\"\"\n        args = {'id': self.id}\n        _perform_command(self.owner, 'project_archive', args)\n        self.is_archived = '1'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a task to the project.", "response": "def add_task(self, content, date=None, priority=None):\n        \"\"\"Add a task to the project\n\n        :param content: The task description.\n        :type content: str\n        :param date: The task deadline.\n        :type date: str\n        :param priority: The priority of the task.\n        :type priority: int\n        :return: The added task.\n        :rtype: :class:`pytodoist.todoist.Task`\n\n        .. note:: See `here <https://todoist.com/Help/timeInsert>`_\n            for possible date strings.\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> task = project.add_task('Install PyTodoist')\n        >>> print(task.content)\n        Install PyTodoist\n        \"\"\"\n        response = API.add_item(self.owner.token, content, project_id=self.id,\n                                date_string=date, priority=priority)\n        _fail_if_contains_errors(response)\n        task_json = response.json()\n        return Task(task_json, self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all uncompleted tasks in this project.", "response": "def get_uncompleted_tasks(self):\n        \"\"\"Return a list of all uncompleted tasks in this project.\n\n        .. warning:: Requires Todoist premium.\n\n        :return: A list of all uncompleted tasks in this project.\n        :rtype: list of :class:`pytodoist.todoist.Task`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.add_task('Install PyTodoist')\n        >>> uncompleted_tasks = project.get_uncompleted_tasks()\n        >>> for task in uncompleted_tasks:\n        ...    task.complete()\n        \"\"\"\n        all_tasks = self.get_tasks()\n        completed_tasks = self.get_completed_tasks()\n        return [t for t in all_tasks if t not in completed_tasks]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_completed_tasks(self):\n        self.owner.sync()\n        tasks = []\n        offset = 0\n        while True:\n            response = API.get_all_completed_tasks(self.owner.api_token,\n                                                   limit=_PAGE_LIMIT,\n                                                   offset=offset,\n                                                   project_id=self.id)\n            _fail_if_contains_errors(response)\n            response_json = response.json()\n            tasks_json = response_json['items']\n            if len(tasks_json) == 0:\n                break  # There are no more completed tasks to retreive.\n            for task_json in tasks_json:\n                project = self.owner.projects[task_json['project_id']]\n                tasks.append(Task(task_json, project))\n            offset += _PAGE_LIMIT\n        return tasks", "response": "Return a list of all completed tasks in this project."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all tasks in this project.", "response": "def get_tasks(self):\n        \"\"\"Return all tasks in this project.\n\n        :return: A list of all tasks in this project.class\n        :rtype: list of :class:`pytodoist.todoist.Task`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.add_task('Install PyTodoist')\n        >>> project.add_task('Have fun!')\n        >>> tasks = project.get_tasks()\n        >>> for task in tasks:\n        ...    print(task.content)\n        Install PyTodoist\n        Have fun!\n        \"\"\"\n        self.owner.sync()\n        return [t for t in self.owner.tasks.values()\n                if t.project_id == self.id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a note to the project.", "response": "def add_note(self, content):\n        \"\"\"Add a note to the project.\n\n        .. warning:: Requires Todoist premium.\n\n        :param content: The note content.\n        :type content: str\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.add_note('Remember to update to the latest version.')\n        \"\"\"\n        args = {\n            'project_id': self.id,\n            'content': content\n        }\n        _perform_command(self.owner, 'note_add', args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all of the project s notes.", "response": "def get_notes(self):\n        \"\"\"Return a list of all of the project's notes.\n\n        :return: A list of notes.\n        :rtype: list of :class:`pytodoist.todoist.Note`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> notes = project.get_notes()\n        \"\"\"\n        self.owner.sync()\n        notes = self.owner.notes.values()\n        return [n for n in notes if n.project_id == self.id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsharing the project with another Todoist user.", "response": "def share(self, email, message=None):\n        \"\"\"Share the project with another Todoist user.\n\n        :param email: The other user's email address.\n        :type email: str\n        :param message: Optional message to send with the invitation.\n        :type message: str\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.share('jane.doe@gmail.com')\n        \"\"\"\n        args = {\n            'project_id': self.id,\n            'email': email,\n            'message': message\n        }\n        _perform_command(self.owner, 'share_project', args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a collaborating user from the shared project.", "response": "def delete_collaborator(self, email):\n        \"\"\"Remove a collaborating user from the shared project.\n\n        :param email: The collaborator's email address.\n        :type email: str\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.delete_collaborator('jane.doe@gmail.com')\n        \"\"\"\n        args = {\n            'project_id': self.id,\n            'email': email,\n        }\n        _perform_command(self.owner, 'delete_collaborator', args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self):\n        args = {'ids': [self.id]}\n        _perform_command(self.owner, 'project_delete', args)\n        del self.owner.projects[self.id]", "response": "Delete the project.\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> project.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef complete(self):\n        args = {\n            'id': self.id\n        }\n        _perform_command(self.project.owner, 'item_close', args)", "response": "Mark the task complete."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmark the task uncomplete.", "response": "def uncomplete(self):\n        \"\"\"Mark the task uncomplete.\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> task = project.add_task('Install PyTodoist')\n        >>> task.uncomplete()\n        \"\"\"\n        args = {\n            'project_id': self.project.id,\n            'ids': [self.id]\n        }\n        owner = self.project.owner\n        _perform_command(owner, 'item_uncomplete', args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all notes attached to this Task.", "response": "def get_notes(self):\n        \"\"\"Return all notes attached to this Task.\n\n        :return: A list of all notes attached to this Task.\n        :rtype: list of :class:`pytodoist.todoist.Note`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> task = project.add_task('Install PyTodoist.')\n        >>> task.add_note('https://pypi.python.org/pypi')\n        >>> notes = task.get_notes()\n        >>> print(len(notes))\n        1\n        \"\"\"\n        owner = self.project.owner\n        owner.sync()\n        return [n for n in owner.notes.values() if n.item_id == self.id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef move(self, project):\n        args = {\n            'project_items': {self.project.id: [self.id]},\n            'to_project': project.id\n        }\n        _perform_command(self.project.owner, 'item_move', args)\n        self.project = project", "response": "Move this task to another project."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_date_reminder(self, service, due_date):\n        args = {\n            'item_id': self.id,\n            'service': service,\n            'type': 'absolute',\n            'due_date_utc': due_date\n        }\n        _perform_command(self.project.owner, 'reminder_add', args)", "response": "Add a reminder to the task which activates on a given date."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_location_reminder(self, service, name, lat, long, trigger, radius):\n        args = {\n            'item_id': self.id,\n            'service': service,\n            'type': 'location',\n            'name': name,\n            'loc_lat': str(lat),\n            'loc_long': str(long),\n            'loc_trigger': trigger,\n            'radius': radius\n        }\n        _perform_command(self.project.owner, 'reminder_add', args)", "response": "Add a reminder to the task which activates on at a given location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_reminders(self):\n        owner = self.project.owner\n        return [r for r in owner.get_reminders() if r.task.id == self.id]", "response": "Return a list of the task s reminders."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self):\n        args = {'ids': [self.id]}\n        _perform_command(self.project.owner, 'item_delete', args)\n        del self.project.owner.tasks[self.id]", "response": "Delete the item from the project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting the note from the task.", "response": "def delete(self):\n        \"\"\"Delete the note, removing it from it's task.\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('PyTodoist')\n        >>> task = project.add_task('Install PyTodoist.')\n        >>> note = task.add_note('https://pypi.python.org/pypi')\n        >>> note.delete()\n        >>> notes = task.get_notes()\n        >>> print(len(notes))\n        0\n        \"\"\"\n        args = {'id': self.id}\n        owner = self.task.project.owner\n        _perform_command(owner, 'note_delete', args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self):\n        args = {attr: getattr(self, attr) for attr in self.to_update}\n        args['id'] = self.id\n        _perform_command(self.owner, 'filter_update', args)", "response": "Update the filter s details on Todoist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_invalid_params_py2(func, *args, **kwargs):\n    funcargs, varargs, varkwargs, defaults = inspect.getargspec(func)\n\n    unexpected = set(kwargs.keys()) - set(funcargs)\n    if len(unexpected) > 0:\n        return True\n\n    params = [funcarg for funcarg in funcargs if funcarg not in kwargs]\n    funcargs_required = funcargs[:-len(defaults)] \\\n        if defaults is not None \\\n        else funcargs\n    params_required = [\n        funcarg for funcarg in funcargs_required\n        if funcarg not in kwargs\n    ]\n\n    return not (len(params_required) <= len(args) <= len(params))", "response": "Check whether function func accepts parameters args kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if function parameters are not valid in order to be used in Python 3.", "response": "def is_invalid_params_py3(func, *args, **kwargs):\n    \"\"\"\n    Use inspect.signature instead of inspect.getargspec or\n    inspect.getfullargspec (based on inspect.signature itself) as it provides\n    more information about function parameters.\n\n    .. versionadded: 1.11.2\n\n    \"\"\"\n    signature = inspect.signature(func)\n    parameters = signature.parameters\n\n    unexpected = set(kwargs.keys()) - set(parameters.keys())\n    if len(unexpected) > 0:\n        return True\n\n    params = [\n        parameter for name, parameter in parameters.items()\n        if name not in kwargs\n    ]\n    params_required = [\n        param for param in params\n        if param.default is param.empty\n    ]\n\n    return not (len(params_required) <= len(args) <= len(params))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_invalid_params(func, *args, **kwargs):\n    # For builtin functions inspect.getargspec(funct) return error. If builtin\n    # function generates TypeError, it is because of wrong parameters.\n    if not inspect.isfunction(func):\n        return True\n\n    if sys.version_info >= (3, 3):\n        return is_invalid_params_py3(func, *args, **kwargs)\n    else:\n        # NOTE: use Python2 method for Python 3.2 as well. Starting from Python\n        # 3.3 it is recommended to use inspect.signature instead.\n        # In Python 3.0 - 3.2 inspect.getfullargspec is preferred but these\n        # versions are almost not supported. Users should consider upgrading.\n        return is_invalid_params_py2(func, *args, **kwargs)", "response": "Validate pre - defined criteria for function is invalid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_method(self, f=None, name=None):\n        if name and not f:\n            return functools.partial(self.add_method, name=name)\n\n        self.method_map[name or f.__name__] = f\n        return f", "response": "Add a method to the dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readme():\n    path = os.path.realpath(os.path.join(os.path.dirname(__file__), 'README.rst'))\n    handle = None\n    try:\n        handle = codecs.open(path, encoding='utf-8')\n        return handle.read(131072)\n    except IOError:\n        return ''\n    finally:\n        getattr(handle, 'close', lambda: None)()", "response": "Try to read README. rst or return empty string if failed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_text(incoming, func):\n    split = RE_SPLIT.split(incoming)\n    for i, item in enumerate(split):\n        if not item or RE_SPLIT.match(item):\n            continue\n        split[i] = func(item)\n    return incoming.__class__().join(split)", "response": "Apply func to each item in the incoming string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef decode(self, encoding='utf-8', errors='strict'):\n        original_class = getattr(self, 'original_class')\n        return original_class(super(ColorBytes, self).decode(encoding, errors))", "response": "Decode using the codec registered for encoding."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef center(self, width, fillchar=None):\n        if fillchar is not None:\n            result = self.value_no_colors.center(width, fillchar)\n        else:\n            result = self.value_no_colors.center(width)\n        return self.__class__(result.replace(self.value_no_colors, self.value_colors), keep_tags=True)", "response": "Return centered in a string of length width."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the number of non - overlapping occurrences of substring sub in string [ start end )", "response": "def count(self, sub, start=0, end=-1):\n        \"\"\"Return the number of non-overlapping occurrences of substring sub in string[start:end].\n\n        Optional arguments start and end are interpreted as in slice notation.\n\n        :param str sub: Substring to search.\n        :param int start: Beginning position.\n        :param int end: Stop comparison at this position.\n        \"\"\"\n        return self.value_no_colors.count(sub, start, end)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if this tag ends with the specified suffix False otherwise.", "response": "def endswith(self, suffix, start=0, end=None):\n        \"\"\"Return True if ends with the specified suffix, False otherwise.\n\n        With optional start, test beginning at that position. With optional end, stop comparing at that position.\n        suffix can also be a tuple of strings to try.\n\n        :param str suffix: Suffix to search.\n        :param int start: Beginning position.\n        :param int end: Stop comparison at this position.\n        \"\"\"\n        args = [suffix, start] + ([] if end is None else [end])\n        return self.value_no_colors.endswith(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(self, encoding=None, errors='strict'):\n        return ColorBytes(super(ColorStr, self).encode(encoding, errors), original_class=self.__class__)", "response": "Encode using the codec registered for encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode(self, encoding=None, errors='strict'):\n        return self.__class__(super(ColorStr, self).decode(encoding, errors), keep_tags=True)", "response": "Decode using the codec registered for encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find(self, sub, start=None, end=None):\n        return self.value_no_colors.find(sub, start, end)", "response": "Return the lowest index where substring sub is contained within string [ start end )."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format(self, *args, **kwargs):\n        return self.__class__(super(ColorStr, self).format(*args, **kwargs), keep_tags=True)", "response": "Return a formatted version using substitutions from args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef index(self, sub, start=None, end=None):\n        return self.value_no_colors.index(sub, start, end)", "response": "Like S. find but raise ValueError when the substring is not found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string which is the concatenation of the strings in this iterable.", "response": "def join(self, iterable):\n        \"\"\"Return a string which is the concatenation of the strings in the iterable.\n\n        :param iterable: Join items in this iterable.\n        \"\"\"\n        return self.__class__(super(ColorStr, self).join(iterable), keep_tags=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rfind(self, sub, start=None, end=None):\n        return self.value_no_colors.rfind(sub, start, end)", "response": "Return the index where substring sub is found such that sub is contained within string [ start end )."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike rindex but raise ValueError when the substring is not found.", "response": "def rindex(self, sub, start=None, end=None):\n        \"\"\"Like .rfind() but raise ValueError when the substring is not found.\n\n        :param str sub: Substring to search.\n        :param int start: Beginning position.\n        :param int end: Stop comparison at this position.\n        \"\"\"\n        return self.value_no_colors.rindex(sub, start, end)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of the lines in the string breaking at line boundaries.", "response": "def splitlines(self, keepends=False):\n        \"\"\"Return a list of the lines in the string, breaking at line boundaries.\n\n        Line breaks are not included in the resulting list unless keepends is given and True.\n\n        :param bool keepends: Include linebreaks.\n        \"\"\"\n        return [self.__class__(l) for l in self.value_colors.splitlines(keepends)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef startswith(self, prefix, start=0, end=-1):\n        return self.value_no_colors.startswith(prefix, start, end)", "response": "Return True if string starts with the specified prefix False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zfill(self, width):\n        if not self.value_no_colors:\n            result = self.value_no_colors.zfill(width)\n        else:\n            result = self.value_colors.replace(self.value_no_colors, self.value_no_colors.zfill(width))\n        return self.__class__(result, keep_tags=True)", "response": "Pad a numeric string with zeros on the left to fill a field of the specified width."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bgred(cls, string, auto=False):\n        return cls.colorize('bgred', string, auto=auto)", "response": "Colorize a string with bgred color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef green(cls, string, auto=False):\n        return cls.colorize('green', string, auto=auto)", "response": "Colorize a string with green color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bgyellow(cls, string, auto=False):\n        return cls.colorize('bgyellow', string, auto=auto)", "response": "Colorize a string with bgyellow color."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bgblue(cls, string, auto=False):\n        return cls.colorize('bgblue', string, auto=auto)", "response": "Colorize a string with background - color."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef magenta(cls, string, auto=False):\n        return cls.colorize('magenta', string, auto=auto)", "response": "Colorize a string with magenta color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bgmagenta(cls, string, auto=False):\n        return cls.colorize('bgmagenta', string, auto=auto)", "response": "Colorize a string with background - color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cyan(cls, string, auto=False):\n        return cls.colorize('cyan', string, auto=auto)", "response": "Color - code entire string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bgcyan(cls, string, auto=False):\n        return cls.colorize('bgcyan', string, auto=auto)", "response": "Colorize a string with bgcyan color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bgwhite(cls, string, auto=False):\n        return cls.colorize('bgwhite', string, auto=auto)", "response": "Colorize a string with background white color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_tags():\n    # Build reverse dictionary. Keys are closing tags, values are [closing ansi, opening tag, opening ansi].\n    reverse_dict = dict()\n    for tag, ansi in sorted(BASE_CODES.items()):\n        if tag.startswith('/'):\n            reverse_dict[tag] = [ansi, None, None]\n        else:\n            reverse_dict['/' + tag][1:] = [tag, ansi]\n\n    # Collapse\n    four_item_tuples = [(v[1], k, v[2], v[0]) for k, v in reverse_dict.items()]\n\n    # Sort.\n    def sorter(four_item):\n        \"\"\"Sort /all /fg /bg first, then b i u flash, then auto colors, then dark colors, finally light colors.\n\n        :param iter four_item: [opening tag, closing tag, main ansi value, closing ansi value]\n\n        :return Sorting weight.\n        :rtype: int\n        \"\"\"\n        if not four_item[2]:  # /all /fg /bg\n            return four_item[3] - 200\n        if four_item[2] < 10 or four_item[0].startswith('auto'):  # b f i u or auto colors\n            return four_item[2] - 100\n        return four_item[2]\n    four_item_tuples.sort(key=sorter)\n\n    return four_item_tuples", "response": "List the available tags."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisable all colors only if there is no TTY available.", "response": "def disable_if_no_tty(cls):\n        \"\"\"Disable all colors only if there is no TTY available.\n\n        :return: True if colors are disabled, False if stderr or stdout is a TTY.\n        :rtype: bool\n        \"\"\"\n        if sys.stdout.isatty() or sys.stderr.isatty():\n            return False\n        cls.disable_all_colors()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a unique instance of WinDLL into memory set arg / return types and get stdout and err handles.", "response": "def init_kernel32(kernel32=None):\n    \"\"\"Load a unique instance of WinDLL into memory, set arg/return types, and get stdout/err handles.\n\n    1. Since we are setting DLL function argument types and return types, we need to maintain our own instance of\n       kernel32 to prevent overriding (or being overwritten by) user's own changes to ctypes.windll.kernel32.\n    2. While we're doing all this we might as well get the handles to STDOUT and STDERR streams.\n    3. If either stream has already been replaced set return value to INVALID_HANDLE_VALUE to indicate it shouldn't be\n       replaced.\n\n    :raise AttributeError: When called on a non-Windows platform.\n\n    :param kernel32: Optional mock kernel32 object. For testing.\n\n    :return: Loaded kernel32 instance, stderr handle (int), stdout handle (int).\n    :rtype: tuple\n    \"\"\"\n    if not kernel32:\n        kernel32 = ctypes.LibraryLoader(ctypes.WinDLL).kernel32  # Load our own instance. Unique memory address.\n        kernel32.GetStdHandle.argtypes = [ctypes.c_ulong]\n        kernel32.GetStdHandle.restype = ctypes.c_void_p\n        kernel32.GetConsoleScreenBufferInfo.argtypes = [\n            ctypes.c_void_p,\n            ctypes.POINTER(ConsoleScreenBufferInfo),\n        ]\n        kernel32.GetConsoleScreenBufferInfo.restype = ctypes.c_long\n\n    # Get handles.\n    if hasattr(sys.stderr, '_original_stream'):\n        stderr = INVALID_HANDLE_VALUE\n    else:\n        stderr = kernel32.GetStdHandle(STD_ERROR_HANDLE)\n    if hasattr(sys.stdout, '_original_stream'):\n        stdout = INVALID_HANDLE_VALUE\n    else:\n        stdout = kernel32.GetStdHandle(STD_OUTPUT_HANDLE)\n\n    return kernel32, stderr, stdout"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_console_info(kernel32, handle):\n    # Query Win32 API.\n    csbi = ConsoleScreenBufferInfo()  # Populated by GetConsoleScreenBufferInfo.\n    lpcsbi = ctypes.byref(csbi)\n    dword = ctypes.c_ulong()  # Populated by GetConsoleMode.\n    lpdword = ctypes.byref(dword)\n    if not kernel32.GetConsoleScreenBufferInfo(handle, lpcsbi) or not kernel32.GetConsoleMode(handle, lpdword):\n        raise ctypes.WinError()\n\n    # Parse data.\n    # buffer_width = int(csbi.dwSize.X - 1)\n    # buffer_height = int(csbi.dwSize.Y)\n    # terminal_width = int(csbi.srWindow.Right - csbi.srWindow.Left)\n    # terminal_height = int(csbi.srWindow.Bottom - csbi.srWindow.Top)\n    fg_color = csbi.wAttributes % 16\n    bg_color = csbi.wAttributes & 240\n    native_ansi = bool(dword.value & ENABLE_VIRTUAL_TERMINAL_PROCESSING)\n\n    return fg_color, bg_color, native_ansi", "response": "Get information about this current console window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget background color and native ANSI support natively for both streams.", "response": "def bg_color_native_ansi(kernel32, stderr, stdout):\n    \"\"\"Get background color and if console supports ANSI colors natively for both streams.\n\n    :param ctypes.windll.kernel32 kernel32: Loaded kernel32 instance.\n    :param int stderr: stderr handle.\n    :param int stdout: stdout handle.\n\n    :return: Background color (int) and native ANSI support (bool).\n    :rtype: tuple\n    \"\"\"\n    try:\n        if stderr == INVALID_HANDLE_VALUE:\n            raise OSError\n        bg_color, native_ansi = get_console_info(kernel32, stderr)[1:]\n    except OSError:\n        try:\n            if stdout == INVALID_HANDLE_VALUE:\n                raise OSError\n            bg_color, native_ansi = get_console_info(kernel32, stdout)[1:]\n        except OSError:\n            bg_color, native_ansi = WINDOWS_CODES['black'], False\n    return bg_color, native_ansi"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the current foreground and background colors.", "response": "def colors(self):\n        \"\"\"Return the current foreground and background colors.\"\"\"\n        try:\n            return get_console_info(self._kernel32, self._stream_handle)[:2]\n        except OSError:\n            return WINDOWS_CODES['white'], WINDOWS_CODES['black']"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef colors(self, color_code):\n        if color_code is None:\n            color_code = WINDOWS_CODES['/all']\n\n        # Get current color code.\n        current_fg, current_bg = self.colors\n\n        # Handle special negative codes. Also determine the final color code.\n        if color_code == WINDOWS_CODES['/fg']:\n            final_color_code = self.default_fg | current_bg  # Reset the foreground only.\n        elif color_code == WINDOWS_CODES['/bg']:\n            final_color_code = current_fg | self.default_bg  # Reset the background only.\n        elif color_code == WINDOWS_CODES['/all']:\n            final_color_code = self.default_fg | self.default_bg  # Reset both.\n        elif color_code == WINDOWS_CODES['bgblack']:\n            final_color_code = current_fg  # Black background.\n        else:\n            new_is_bg = color_code in self.ALL_BG_CODES\n            final_color_code = color_code | (current_fg if new_is_bg else current_bg)\n\n        # Set new code.\n        self._kernel32.SetConsoleTextAttribute(self._stream_handle, final_color_code)", "response": "Change the foreground and background colors for subsequently printed characters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, p_str):\n        for segment in RE_SPLIT.split(p_str):\n            if not segment:\n                # Empty string. p_str probably starts with colors so the first item is always ''.\n                continue\n            if not RE_SPLIT.match(segment):\n                # No color codes, print regular text.\n                print(segment, file=self._original_stream, end='')\n                self._original_stream.flush()\n                continue\n            for color_code in (int(c) for c in RE_NUMBER_SEARCH.findall(segment)[0].split(';')):\n                if color_code in self.COMPILED_CODES:\n                    self.colors = self.COMPILED_CODES[color_code]", "response": "Write to stream.\n\n        :param str p_str: string to print."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrestoring sys. stderr and sys. stdout to their original values. Resets colors to their original values.", "response": "def disable(cls):\n        \"\"\"Restore sys.stderr and sys.stdout to their original objects. Resets colors to their original values.\n\n        :return: If streams restored successfully.\n        :rtype: bool\n        \"\"\"\n        # Skip if not on Windows.\n        if not IS_WINDOWS:\n            return False\n\n        # Restore default colors.\n        if hasattr(sys.stderr, '_original_stream'):\n            getattr(sys, 'stderr').color = None\n        if hasattr(sys.stdout, '_original_stream'):\n            getattr(sys, 'stdout').color = None\n\n        # Restore original streams.\n        changed = False\n        if hasattr(sys.stderr, '_original_stream'):\n            changed = True\n            sys.stderr = getattr(sys.stderr, '_original_stream')\n        if hasattr(sys.stdout, '_original_stream'):\n            changed = True\n            sys.stdout = getattr(sys.stdout, '_original_stream')\n\n        return changed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef enable(cls, auto_colors=False, reset_atexit=False):\n        if not IS_WINDOWS:\n            return False  # Windows only.\n\n        # Get values from init_kernel32().\n        kernel32, stderr, stdout = init_kernel32()\n        if stderr == INVALID_HANDLE_VALUE and stdout == INVALID_HANDLE_VALUE:\n            return False  # No valid handles, nothing to do.\n\n        # Get console info.\n        bg_color, native_ansi = bg_color_native_ansi(kernel32, stderr, stdout)\n\n        # Set auto colors:\n        if auto_colors:\n            if bg_color in (112, 96, 240, 176, 224, 208, 160):\n                ANSICodeMapping.set_light_background()\n            else:\n                ANSICodeMapping.set_dark_background()\n\n        # Don't replace streams if ANSI codes are natively supported.\n        if native_ansi:\n            return False\n\n        # Reset on exit if requested.\n        if reset_atexit:\n            atexit.register(cls.disable)\n\n        # Overwrite stream references.\n        if stderr != INVALID_HANDLE_VALUE:\n            sys.stderr.flush()\n            sys.stderr = WindowsStream(kernel32, stderr, sys.stderr)\n        if stdout != INVALID_HANDLE_VALUE:\n            sys.stdout.flush()\n            sys.stdout = WindowsStream(kernel32, stdout, sys.stdout)\n\n        return True", "response": "Enable color text with sys. stdout. write or sys. stderr. write."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.", "response": "def prune_overridden(ansi_string):\n    \"\"\"Remove color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.\n\n    :param str ansi_string: Incoming ansi_string with ANSI color codes.\n\n    :return: Color string with pruned color sequences.\n    :rtype: str\n    \"\"\"\n    multi_seqs = set(p for p in RE_ANSI.findall(ansi_string) if ';' in p[1])  # Sequences with multiple color codes.\n\n    for escape, codes in multi_seqs:\n        r_codes = list(reversed(codes.split(';')))\n\n        # Nuke everything before {/all}.\n        try:\n            r_codes = r_codes[:r_codes.index('0') + 1]\n        except ValueError:\n            pass\n\n        # Thin out groups.\n        for group in CODE_GROUPS:\n            for pos in reversed([i for i, n in enumerate(r_codes) if n in group][1:]):\n                r_codes.pop(pos)\n\n        # Done.\n        reduced_codes = ';'.join(sorted(r_codes, key=int))\n        if codes != reduced_codes:\n            ansi_string = ansi_string.replace(escape, '\\033[' + reduced_codes + 'm')\n\n    return ansi_string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_input(tagged_string, disable_colors, keep_tags):\n    codes = ANSICodeMapping(tagged_string)\n    output_colors = getattr(tagged_string, 'value_colors', tagged_string)\n\n    # Convert: '{b}{red}' -> '\\033[1m\\033[31m'\n    if not keep_tags:\n        for tag, replacement in (('{' + k + '}', '' if v is None else '\\033[%dm' % v) for k, v in codes.items()):\n            output_colors = output_colors.replace(tag, replacement)\n\n    # Strip colors.\n    output_no_colors = RE_ANSI.sub('', output_colors)\n    if disable_colors:\n        return output_no_colors, output_no_colors\n\n    # Combine: '\\033[1m\\033[31m' -> '\\033[1;31m'\n    while True:\n        simplified = RE_COMBINE.sub(r'\\033[\\1;\\2m', output_colors)\n        if simplified == output_colors:\n            break\n        output_colors = simplified\n\n    # Prune: '\\033[31;32;33;34;35m' -> '\\033[35m'\n    output_colors = prune_overridden(output_colors)\n\n    # Deduplicate: '\\033[1;mT\\033[1;mE\\033[1;mS\\033[1;mT' -> '\\033[1;mTEST'\n    previous_escape = None\n    segments = list()\n    for item in (i for i in RE_SPLIT.split(output_colors) if i):\n        if RE_SPLIT.match(item):\n            if item != previous_escape:\n                segments.append(item)\n                previous_escape = item\n        else:\n            segments.append(item)\n    output_colors = ''.join(segments)\n\n    return output_colors, output_no_colors", "response": "Perform the actual conversion of tags to ANSI escape sequences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild an index between visible characters and a string with invisible color codes.", "response": "def build_color_index(ansi_string):\n    \"\"\"Build an index between visible characters and a string with invisible color codes.\n\n    :param str ansi_string: String with color codes (ANSI escape sequences).\n\n    :return: Position of visible characters in color string (indexes match non-color string).\n    :rtype: tuple\n    \"\"\"\n    mapping = list()\n    color_offset = 0\n    for item in (i for i in RE_SPLIT.split(ansi_string) if i):\n        if RE_SPLIT.match(item):\n            color_offset += len(item)\n        else:\n            for _ in range(len(item)):\n                mapping.append(color_offset)\n                color_offset += 1\n    return tuple(mapping)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the color a character in the string.", "response": "def find_char_color(ansi_string, pos):\n    \"\"\"Determine what color a character is in the string.\n\n    :param str ansi_string: String with color codes (ANSI escape sequences).\n    :param int pos: Position of the character in the ansi_string.\n\n    :return: Character along with all surrounding color codes.\n    :rtype: str\n    \"\"\"\n    result = list()\n    position = 0  # Set to None when character is found.\n    for item in (i for i in RE_SPLIT.split(ansi_string) if i):\n        if RE_SPLIT.match(item):\n            result.append(item)\n            if position is not None:\n                position += len(item)\n        elif position is not None:\n            for char in item:\n                if position == pos:\n                    result.append(char)\n                    position = None\n                    break\n                position += 1\n    return ''.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main():\n    if OPTIONS.get('--no-colors'):\n        disable_all_colors()\n    elif OPTIONS.get('--colors'):\n        enable_all_colors()\n\n    if is_enabled() and os.name == 'nt':\n        Windows.enable(auto_colors=True, reset_atexit=True)\n    elif OPTIONS.get('--light-bg'):\n        set_light_background()\n    elif OPTIONS.get('--dark-bg'):\n        set_dark_background()\n\n    # Light or dark colors.\n    print('Autocolors for all backgrounds:')\n    print(Color('    {autoblack}Black{/black} {autored}Red{/red} {autogreen}Green{/green} '), end='')\n    print(Color('{autoyellow}Yellow{/yellow} {autoblue}Blue{/blue} {automagenta}Magenta{/magenta} '), end='')\n    print(Color('{autocyan}Cyan{/cyan} {autowhite}White{/white}'))\n\n    print(Color('    {autobgblack}{autoblack}Black{/black}{/bgblack} '), end='')\n    print(Color('{autobgblack}{autored}Red{/red}{/bgblack} {autobgblack}{autogreen}Green{/green}{/bgblack} '), end='')\n    print(Color('{autobgblack}{autoyellow}Yellow{/yellow}{/bgblack} '), end='')\n    print(Color('{autobgblack}{autoblue}Blue{/blue}{/bgblack} '), end='')\n    print(Color('{autobgblack}{automagenta}Magenta{/magenta}{/bgblack} '), end='')\n    print(Color('{autobgblack}{autocyan}Cyan{/cyan}{/bgblack} {autobgblack}{autowhite}White{/white}{/bgblack}'))\n\n    print(Color('    {autobgred}{autoblack}Black{/black}{/bgred} {autobgred}{autored}Red{/red}{/bgred} '), end='')\n    print(Color('{autobgred}{autogreen}Green{/green}{/bgred} {autobgred}{autoyellow}Yellow{/yellow}{/bgred} '), end='')\n    print(Color('{autobgred}{autoblue}Blue{/blue}{/bgred} {autobgred}{automagenta}Magenta{/magenta}{/bgred} '), end='')\n    print(Color('{autobgred}{autocyan}Cyan{/cyan}{/bgred} {autobgred}{autowhite}White{/white}{/bgred}'))\n\n    print(Color('    {autobggreen}{autoblack}Black{/black}{/bggreen} '), end='')\n    print(Color('{autobggreen}{autored}Red{/red}{/bggreen} {autobggreen}{autogreen}Green{/green}{/bggreen} '), end='')\n    print(Color('{autobggreen}{autoyellow}Yellow{/yellow}{/bggreen} '), end='')\n    print(Color('{autobggreen}{autoblue}Blue{/blue}{/bggreen} '), end='')\n    print(Color('{autobggreen}{automagenta}Magenta{/magenta}{/bggreen} '), end='')\n    print(Color('{autobggreen}{autocyan}Cyan{/cyan}{/bggreen} {autobggreen}{autowhite}White{/white}{/bggreen}'))\n\n    print(Color('    {autobgyellow}{autoblack}Black{/black}{/bgyellow} '), end='')\n    print(Color('{autobgyellow}{autored}Red{/red}{/bgyellow} '), end='')\n    print(Color('{autobgyellow}{autogreen}Green{/green}{/bgyellow} '), end='')\n    print(Color('{autobgyellow}{autoyellow}Yellow{/yellow}{/bgyellow} '), end='')\n    print(Color('{autobgyellow}{autoblue}Blue{/blue}{/bgyellow} '), end='')\n    print(Color('{autobgyellow}{automagenta}Magenta{/magenta}{/bgyellow} '), end='')\n    print(Color('{autobgyellow}{autocyan}Cyan{/cyan}{/bgyellow} {autobgyellow}{autowhite}White{/white}{/bgyellow}'))\n\n    print(Color('    {autobgblue}{autoblack}Black{/black}{/bgblue} {autobgblue}{autored}Red{/red}{/bgblue} '), end='')\n    print(Color('{autobgblue}{autogreen}Green{/green}{/bgblue} '), end='')\n    print(Color('{autobgblue}{autoyellow}Yellow{/yellow}{/bgblue} {autobgblue}{autoblue}Blue{/blue}{/bgblue} '), end='')\n    print(Color('{autobgblue}{automagenta}Magenta{/magenta}{/bgblue} '), end='')\n    print(Color('{autobgblue}{autocyan}Cyan{/cyan}{/bgblue} {autobgblue}{autowhite}White{/white}{/bgblue}'))\n\n    print(Color('    {autobgmagenta}{autoblack}Black{/black}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{autored}Red{/red}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{autogreen}Green{/green}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{autoyellow}Yellow{/yellow}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{autoblue}Blue{/blue}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{automagenta}Magenta{/magenta}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{autocyan}Cyan{/cyan}{/bgmagenta} '), end='')\n    print(Color('{autobgmagenta}{autowhite}White{/white}{/bgmagenta}'))\n\n    print(Color('    {autobgcyan}{autoblack}Black{/black}{/bgcyan} {autobgcyan}{autored}Red{/red}{/bgcyan} '), end='')\n    print(Color('{autobgcyan}{autogreen}Green{/green}{/bgcyan} '), end='')\n    print(Color('{autobgcyan}{autoyellow}Yellow{/yellow}{/bgcyan} {autobgcyan}{autoblue}Blue{/blue}{/bgcyan} '), end='')\n    print(Color('{autobgcyan}{automagenta}Magenta{/magenta}{/bgcyan} '), end='')\n    print(Color('{autobgcyan}{autocyan}Cyan{/cyan}{/bgcyan} {autobgcyan}{autowhite}White{/white}{/bgcyan}'))\n\n    print(Color('    {autobgwhite}{autoblack}Black{/black}{/bgwhite} '), end='')\n    print(Color('{autobgwhite}{autored}Red{/red}{/bgwhite} {autobgwhite}{autogreen}Green{/green}{/bgwhite} '), end='')\n    print(Color('{autobgwhite}{autoyellow}Yellow{/yellow}{/bgwhite} '), end='')\n    print(Color('{autobgwhite}{autoblue}Blue{/blue}{/bgwhite} '), end='')\n    print(Color('{autobgwhite}{automagenta}Magenta{/magenta}{/bgwhite} '), end='')\n    print(Color('{autobgwhite}{autocyan}Cyan{/cyan}{/bgwhite} {autobgwhite}{autowhite}White{/white}{/bgwhite}'))\n    print()\n\n    # Light colors.\n    print('Light colors for dark backgrounds:')\n    print(Color('    {hiblack}Black{/black} {hired}Red{/red} {higreen}Green{/green} '), end='')\n    print(Color('{hiyellow}Yellow{/yellow} {hiblue}Blue{/blue} {himagenta}Magenta{/magenta} '), end='')\n    print(Color('{hicyan}Cyan{/cyan} {hiwhite}White{/white}'))\n\n    print(Color('    {hibgblack}{hiblack}Black{/black}{/bgblack} '), end='')\n    print(Color('{hibgblack}{hired}Red{/red}{/bgblack} {hibgblack}{higreen}Green{/green}{/bgblack} '), end='')\n    print(Color('{hibgblack}{hiyellow}Yellow{/yellow}{/bgblack} '), end='')\n    print(Color('{hibgblack}{hiblue}Blue{/blue}{/bgblack} '), end='')\n    print(Color('{hibgblack}{himagenta}Magenta{/magenta}{/bgblack} '), end='')\n    print(Color('{hibgblack}{hicyan}Cyan{/cyan}{/bgblack} {hibgblack}{hiwhite}White{/white}{/bgblack}'))\n\n    print(Color('    {hibgred}{hiblack}Black{/black}{/bgred} {hibgred}{hired}Red{/red}{/bgred} '), end='')\n    print(Color('{hibgred}{higreen}Green{/green}{/bgred} {hibgred}{hiyellow}Yellow{/yellow}{/bgred} '), end='')\n    print(Color('{hibgred}{hiblue}Blue{/blue}{/bgred} {hibgred}{himagenta}Magenta{/magenta}{/bgred} '), end='')\n    print(Color('{hibgred}{hicyan}Cyan{/cyan}{/bgred} {hibgred}{hiwhite}White{/white}{/bgred}'))\n\n    print(Color('    {hibggreen}{hiblack}Black{/black}{/bggreen} '), end='')\n    print(Color('{hibggreen}{hired}Red{/red}{/bggreen} {hibggreen}{higreen}Green{/green}{/bggreen} '), end='')\n    print(Color('{hibggreen}{hiyellow}Yellow{/yellow}{/bggreen} '), end='')\n    print(Color('{hibggreen}{hiblue}Blue{/blue}{/bggreen} '), end='')\n    print(Color('{hibggreen}{himagenta}Magenta{/magenta}{/bggreen} '), end='')\n    print(Color('{hibggreen}{hicyan}Cyan{/cyan}{/bggreen} {hibggreen}{hiwhite}White{/white}{/bggreen}'))\n\n    print(Color('    {hibgyellow}{hiblack}Black{/black}{/bgyellow} '), end='')\n    print(Color('{hibgyellow}{hired}Red{/red}{/bgyellow} '), end='')\n    print(Color('{hibgyellow}{higreen}Green{/green}{/bgyellow} '), end='')\n    print(Color('{hibgyellow}{hiyellow}Yellow{/yellow}{/bgyellow} '), end='')\n    print(Color('{hibgyellow}{hiblue}Blue{/blue}{/bgyellow} '), end='')\n    print(Color('{hibgyellow}{himagenta}Magenta{/magenta}{/bgyellow} '), end='')\n    print(Color('{hibgyellow}{hicyan}Cyan{/cyan}{/bgyellow} {hibgyellow}{hiwhite}White{/white}{/bgyellow}'))\n\n    print(Color('    {hibgblue}{hiblack}Black{/black}{/bgblue} {hibgblue}{hired}Red{/red}{/bgblue} '), end='')\n    print(Color('{hibgblue}{higreen}Green{/green}{/bgblue} '), end='')\n    print(Color('{hibgblue}{hiyellow}Yellow{/yellow}{/bgblue} {hibgblue}{hiblue}Blue{/blue}{/bgblue} '), end='')\n    print(Color('{hibgblue}{himagenta}Magenta{/magenta}{/bgblue} '), end='')\n    print(Color('{hibgblue}{hicyan}Cyan{/cyan}{/bgblue} {hibgblue}{hiwhite}White{/white}{/bgblue}'))\n\n    print(Color('    {hibgmagenta}{hiblack}Black{/black}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{hired}Red{/red}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{higreen}Green{/green}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{hiyellow}Yellow{/yellow}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{hiblue}Blue{/blue}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{himagenta}Magenta{/magenta}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{hicyan}Cyan{/cyan}{/bgmagenta} '), end='')\n    print(Color('{hibgmagenta}{hiwhite}White{/white}{/bgmagenta}'))\n\n    print(Color('    {hibgcyan}{hiblack}Black{/black}{/bgcyan} {hibgcyan}{hired}Red{/red}{/bgcyan} '), end='')\n    print(Color('{hibgcyan}{higreen}Green{/green}{/bgcyan} '), end='')\n    print(Color('{hibgcyan}{hiyellow}Yellow{/yellow}{/bgcyan} {hibgcyan}{hiblue}Blue{/blue}{/bgcyan} '), end='')\n    print(Color('{hibgcyan}{himagenta}Magenta{/magenta}{/bgcyan} '), end='')\n    print(Color('{hibgcyan}{hicyan}Cyan{/cyan}{/bgcyan} {hibgcyan}{hiwhite}White{/white}{/bgcyan}'))\n\n    print(Color('    {hibgwhite}{hiblack}Black{/black}{/bgwhite} '), end='')\n    print(Color('{hibgwhite}{hired}Red{/red}{/bgwhite} {hibgwhite}{higreen}Green{/green}{/bgwhite} '), end='')\n    print(Color('{hibgwhite}{hiyellow}Yellow{/yellow}{/bgwhite} '), end='')\n    print(Color('{hibgwhite}{hiblue}Blue{/blue}{/bgwhite} '), end='')\n    print(Color('{hibgwhite}{himagenta}Magenta{/magenta}{/bgwhite} '), end='')\n    print(Color('{hibgwhite}{hicyan}Cyan{/cyan}{/bgwhite} {hibgwhite}{hiwhite}White{/white}{/bgwhite}'))\n    print()\n\n    # Dark colors.\n    print('Dark colors for light backgrounds:')\n    print(Color('    {black}Black{/black} {red}Red{/red} {green}Green{/green} {yellow}Yellow{/yellow} '), end='')\n    print(Color('{blue}Blue{/blue} {magenta}Magenta{/magenta} {cyan}Cyan{/cyan} {white}White{/white}'))\n\n    print(Color('    {bgblack}{black}Black{/black}{/bgblack} {bgblack}{red}Red{/red}{/bgblack} '), end='')\n    print(Color('{bgblack}{green}Green{/green}{/bgblack} {bgblack}{yellow}Yellow{/yellow}{/bgblack} '), end='')\n    print(Color('{bgblack}{blue}Blue{/blue}{/bgblack} {bgblack}{magenta}Magenta{/magenta}{/bgblack} '), end='')\n    print(Color('{bgblack}{cyan}Cyan{/cyan}{/bgblack} {bgblack}{white}White{/white}{/bgblack}'))\n\n    print(Color('    {bgred}{black}Black{/black}{/bgred} {bgred}{red}Red{/red}{/bgred} '), end='')\n    print(Color('{bgred}{green}Green{/green}{/bgred} {bgred}{yellow}Yellow{/yellow}{/bgred} '), end='')\n    print(Color('{bgred}{blue}Blue{/blue}{/bgred} {bgred}{magenta}Magenta{/magenta}{/bgred} '), end='')\n    print(Color('{bgred}{cyan}Cyan{/cyan}{/bgred} {bgred}{white}White{/white}{/bgred}'))\n\n    print(Color('    {bggreen}{black}Black{/black}{/bggreen} {bggreen}{red}Red{/red}{/bggreen} '), end='')\n    print(Color('{bggreen}{green}Green{/green}{/bggreen} {bggreen}{yellow}Yellow{/yellow}{/bggreen} '), end='')\n    print(Color('{bggreen}{blue}Blue{/blue}{/bggreen} {bggreen}{magenta}Magenta{/magenta}{/bggreen} '), end='')\n    print(Color('{bggreen}{cyan}Cyan{/cyan}{/bggreen} {bggreen}{white}White{/white}{/bggreen}'))\n\n    print(Color('    {bgyellow}{black}Black{/black}{/bgyellow} {bgyellow}{red}Red{/red}{/bgyellow} '), end='')\n    print(Color('{bgyellow}{green}Green{/green}{/bgyellow} {bgyellow}{yellow}Yellow{/yellow}{/bgyellow} '), end='')\n    print(Color('{bgyellow}{blue}Blue{/blue}{/bgyellow} {bgyellow}{magenta}Magenta{/magenta}{/bgyellow} '), end='')\n    print(Color('{bgyellow}{cyan}Cyan{/cyan}{/bgyellow} {bgyellow}{white}White{/white}{/bgyellow}'))\n\n    print(Color('    {bgblue}{black}Black{/black}{/bgblue} {bgblue}{red}Red{/red}{/bgblue} '), end='')\n    print(Color('{bgblue}{green}Green{/green}{/bgblue} {bgblue}{yellow}Yellow{/yellow}{/bgblue} '), end='')\n    print(Color('{bgblue}{blue}Blue{/blue}{/bgblue} {bgblue}{magenta}Magenta{/magenta}{/bgblue} '), end='')\n    print(Color('{bgblue}{cyan}Cyan{/cyan}{/bgblue} {bgblue}{white}White{/white}{/bgblue}'))\n\n    print(Color('    {bgmagenta}{black}Black{/black}{/bgmagenta} {bgmagenta}{red}Red{/red}{/bgmagenta} '), end='')\n    print(Color('{bgmagenta}{green}Green{/green}{/bgmagenta} {bgmagenta}{yellow}Yellow{/yellow}{/bgmagenta} '), end='')\n    print(Color('{bgmagenta}{blue}Blue{/blue}{/bgmagenta} {bgmagenta}{magenta}Magenta{/magenta}{/bgmagenta} '), end='')\n    print(Color('{bgmagenta}{cyan}Cyan{/cyan}{/bgmagenta} {bgmagenta}{white}White{/white}{/bgmagenta}'))\n\n    print(Color('    {bgcyan}{black}Black{/black}{/bgcyan} {bgcyan}{red}Red{/red}{/bgcyan} '), end='')\n    print(Color('{bgcyan}{green}Green{/green}{/bgcyan} {bgcyan}{yellow}Yellow{/yellow}{/bgcyan} '), end='')\n    print(Color('{bgcyan}{blue}Blue{/blue}{/bgcyan} {bgcyan}{magenta}Magenta{/magenta}{/bgcyan} '), end='')\n    print(Color('{bgcyan}{cyan}Cyan{/cyan}{/bgcyan} {bgcyan}{white}White{/white}{/bgcyan}'))\n\n    print(Color('    {bgwhite}{black}Black{/black}{/bgwhite} {bgwhite}{red}Red{/red}{/bgwhite} '), end='')\n    print(Color('{bgwhite}{green}Green{/green}{/bgwhite} {bgwhite}{yellow}Yellow{/yellow}{/bgwhite} '), end='')\n    print(Color('{bgwhite}{blue}Blue{/blue}{/bgwhite} {bgwhite}{magenta}Magenta{/magenta}{/bgwhite} '), end='')\n    print(Color('{bgwhite}{cyan}Cyan{/cyan}{/bgwhite} {bgwhite}{white}White{/white}{/bgwhite}'))\n\n    if OPTIONS['--wait']:\n        print('Waiting for {0} to exist within 10 seconds...'.format(OPTIONS['--wait']), file=sys.stderr, end='')\n        stop_after = time.time() + 20\n        while not os.path.exists(OPTIONS['--wait']) and time.time() < stop_after:\n            print('.', file=sys.stderr, end='')\n            sys.stderr.flush()\n            time.sleep(0.5)\n        print(' done')", "response": "Main function called upon script execution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef angular_distance_fast(ra1, dec1, ra2, dec2):\n\n    lon1 = np.deg2rad(ra1)\n    lat1 = np.deg2rad(dec1)\n    lon2 = np.deg2rad(ra2)\n    lat2 = np.deg2rad(dec2)\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n\n    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon /2.0)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return np.rad2deg(c)", "response": "Compute the angular distance between two points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef angular_distance(ra1, dec1, ra2, dec2):\n\n    # Vincenty formula, slower than the Haversine formula in some cases, but stable also at antipodes\n\n    lon1 = np.deg2rad(ra1)\n    lat1 = np.deg2rad(dec1)\n    lon2 = np.deg2rad(ra2)\n    lat2 = np.deg2rad(dec2)\n\n    sdlon = np.sin(lon2 - lon1)\n    cdlon = np.cos(lon2 - lon1)\n    slat1 = np.sin(lat1)\n    slat2 = np.sin(lat2)\n    clat1 = np.cos(lat1)\n    clat2 = np.cos(lat2)\n\n    num1 = clat2 * sdlon\n    num2 = clat1 * slat2 - slat1 * clat2 * cdlon\n    denominator = slat1 * slat2 + clat1 * clat2 * cdlon\n\n    return np.rad2deg(np.arctan2(np.sqrt(num1 ** 2 + num2 ** 2), denominator))", "response": "Returns the angular distance between two points or a set of points and one point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spherical_angle( ra0, dec0, ra1, dec1, ra2, dec2 ):\n    \n    a = np.deg2rad( angular_distance(ra0, dec0, ra1, dec1))\n    b = np.deg2rad( angular_distance(ra0, dec0, ra2, dec2))\n    c = np.deg2rad( angular_distance(ra2, dec2, ra1, dec1))\n    \n    #use the spherical law of cosines: https://en.wikipedia.org/wiki/Spherical_law_of_cosines#Rearrangements\n    \n    numerator = np.atleast_1d( np.cos(c) - np.cos(a) * np.cos(b) )\n    denominator = np.atleast_1d( np.sin(a)*np.sin(b) )\n    \n    return np.where( denominator == 0 , np.zeros( len(denominator)), np.rad2deg( np.arccos( numerator/denominator)) )", "response": "Returns the spherical angle between two points"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef memoize(method):\n\n    cache = method.cache = collections.OrderedDict()\n\n    # Put these two methods in the local space (faster)\n    _get = cache.get\n    _popitem = cache.popitem\n\n    @functools.wraps(method)\n    def memoizer(instance, x, *args, **kwargs):\n\n        if not _WITH_MEMOIZATION or isinstance(x, u.Quantity):\n\n            # Memoization is not active or using units, do not use memoization\n\n            return method(instance, x, *args, **kwargs)\n\n        # Create a tuple because a tuple is hashable\n\n        unique_id = tuple(float(yy.value) for yy in instance.parameters.values()) + (x.size, x.min(), x.max())\n\n        # Create a unique identifier for this combination of inputs\n\n        key = hash(unique_id)\n\n        # Let's do it this way so we only look into the dictionary once\n\n        result = _get(key)\n\n        if result is not None:\n\n            return result\n\n        else:\n\n            result = method(instance, x, *args, **kwargs)\n\n            cache[key] = result\n\n            if len(cache) > _CACHE_SIZE:\n                # Remove half of the element (but at least 1, even if _CACHE_SIZE=1, which would be pretty idiotic ;-) )\n                [_popitem(False) for i in range(max(_CACHE_SIZE // 2, 1))]\n\n            return result\n\n    # Add the function as a \"attribute\" so we can access it\n    memoizer.input_object = method\n\n    return memoizer", "response": "A decorator for functions of sources which memoize the results of the last _CACHE_SIZE calls."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef xspec_cosmo(H0=None,q0=None,lambda_0=None):\n\n    current_settings = _xspec.get_xscosmo()\n\n    if (H0 is None) and (q0 is None) and (lambda_0 is None):\n\n        return current_settings\n\n\n    else:\n\n        # ok, we will see what was changed by the used\n\n        user_inputs = [H0, q0, lambda_0]\n\n        for i, current_setting in enumerate(current_settings):\n\n            if user_inputs[i] is None:\n\n                # the user didn't modify this,\n                # so lets keep what was already set\n\n                user_inputs[i] = current_setting\n\n\n        # pass this to xspec\n\n        _xspec.set_xscosmo(*user_inputs)", "response": "This function is used to set the current settings of the current Cosmology in use within the XSpec models."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_model_dat():\n\n    # model.dat is in $HEADAS/../spectral\n\n    headas_env = os.environ.get(\"HEADAS\")\n\n    assert headas_env is not None, (\"You need to setup the HEADAS variable before importing this module.\"\n                                    \" See Heasoft documentation.\")\n\n    # Expand all variables and other things like ~\n    headas_env = os.path.expandvars(os.path.expanduser(headas_env))\n\n    # Lazy check that it exists\n\n    assert os.path.exists(headas_env), \"The HEADAS env. variable point to a non-existent directory: %s\" % (headas_env)\n\n    # Get one directory above HEADAS (i.e., $HEADAS/..)\n\n    inferred_path = os.path.dirname(headas_env)\n\n    # Now model.dat should be in $HEADAS/../spectral/manager\n\n    final_path = os.path.join(inferred_path, 'spectral', 'manager', 'model.dat')\n\n    # Check that model.dat exists\n\n    assert os.path.exists(final_path), \"Cannot find Xspec model definition file %s\" % (final_path)\n\n    return os.path.abspath(final_path)", "response": "Find the file containing the definition of all the models in Xspec\n    and return its path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the model. dat file from Xspec and return a dictionary containing the definitions of all the models in the order they were added.", "response": "def get_models(model_dat_path):\n    \"\"\"\n    Parse the model.dat file from Xspec and returns a dictionary containing the definition of all the models\n\n    :param model_dat_path: the path to the model.dat file\n    :return: dictionary containing the definition of all XSpec models\n    \"\"\"\n\n    # Check first if we already have a model data file in the data directory\n\n\n    with open(model_dat_path) as f:\n\n        # model.dat is a text file, no size issues here (will fit in memory)\n\n        model_dat = f.read()\n\n    # Replace \\r or \\r\\n with \\n (the former two might be in the file if\n    # it originates from Windows)\n    if \"\\n\" in model_dat:\n\n        model_dat = model_dat.replace(\"\\r\", \"\")\n\n    else:\n\n        model_dat = model_dat.replace(\"\\r\", \"\\n\")\n\n    # Loop through the file and build the model definition\n    # dictionary\n\n    lines = model_dat.split(\"\\n\")\n\n    model_definitions = collections.OrderedDict()\n\n    for line in lines:\n\n        match = re.match('''(.+(add|mul|con|acn).+)''', line)\n\n        if match is not None:\n\n            # This is a model definition\n\n            tokens = line.split()\n\n            if len(tokens) == 7:\n\n                (model_name, n_parameters,\n                 min_energy, max_energy,\n                 library_function,\n                 model_type,\n                 flag) = line.split()\n\n            else:\n\n                (model_name, n_parameters,\n                 min_energy, max_energy,\n                 library_function,\n                 model_type,\n                 flag,\n                 flag_2) = line.split()\n\n            this_model = collections.OrderedDict()\n\n            this_model['description'] = 'The %s model from XSpec (https://heasarc.gsfc.nasa.gov/xanadu/' \\\n                                                   'xspec/manual/XspecModels.html)' % model_name\n\n            this_model['parameters'] = collections.OrderedDict()\n\n            model_definitions[(model_name, library_function, model_type)] = this_model\n\n        else:\n\n            # This is a parameter definition\n\n            if len(line.split()) == 0:\n                # Empty line\n                continue\n\n            # Parameters are free by default, unless they have delta < 0\n            # they are switch parameters or scale parameters\n\n            free = True\n\n            if line[0] == '$':\n\n                # Probably a switch parameter\n\n                free = False\n\n                tokens = line.split()\n\n                if len(tokens) == 2:\n\n                    par_name = tokens[0][1:]\n\n                    default_value = tokens[1]\n\n                    par_unit = \"\"\n\n                    hard_minimum, soft_minimum, soft_maximum, hard_maximum = (0, 0, 1e9, 1e9)\n\n                elif len(tokens) == 3:\n\n                    par_name = tokens[0][1:]\n\n                    default_value = tokens[2]\n\n                    par_unit = \"\"\n\n                    hard_minimum, soft_minimum, soft_maximum, hard_maximum = (0, 0, 1e9, 1e9)\n\n                else:\n\n                    match = re.match('(\\S+)\\s+(\\\".+\\\"|[a-zA-Z]+)?(.+)*', line[1:])\n\n                    par_name, par_unit, par_spec = match.groups()\n\n                    tokens = par_spec.split()\n\n                    if len(tokens) == 1:\n\n                        default_value = tokens[0]\n                        par_unit = \"\"\n                        hard_minimum, soft_minimum, soft_maximum, hard_maximum = (0, 0, 1e9, 1e9)\n\n                    else:\n\n                        par_unit = \"\"\n\n                        (default_value,\n                         hard_minimum, soft_minimum,\n                         soft_maximum, hard_maximum,\n                         delta) = par_spec.split()\n\n\n            else:\n\n                # A normal parameter\n\n                match = re.match('(\\S+)\\s+(\\\".+\\\"|\\S+)(.+)', line)\n\n                if match is None:\n\n                    raise RuntimeError(\"Cannot parse parameter %s\" % line)\n\n                par_name, par_unit, par_spec = match.groups()\n\n                if par_name[0] == '*':\n\n                    # Scale parameter (always frozen)\n\n                    par_name = par_name[1:]\n\n                    free = False\n\n                    tokens = par_spec.split()\n\n                    if len(tokens) == 1:\n\n                        default_value = tokens[0]\n                        (hard_minimum, soft_minimum,\n                         soft_maximum, hard_maximum,\n                         delta) = (None, None, None, None, 0.1)\n\n                    else:\n\n                        (default_value,\n                         hard_minimum, soft_minimum,\n                         soft_maximum, hard_maximum,\n                         delta) = par_spec.split()\n\n                        delta = abs(float(delta))\n\n                else:\n\n                    (default_value,\n                     hard_minimum, soft_minimum,\n                     soft_maximum, hard_maximum,\n                     delta) = par_spec.split()\n\n                    delta = float(delta)\n\n                    if delta <= 0:\n                        free = False\n\n                        delta = abs(delta)\n\n            # Now fix the parameter name removing illegal characters\n            # (for example 'log(A)' is not a legal name)\n\n            par_name = re.sub('[\\(,\\)]', '_', par_name)\n            par_name = re.sub('<', '_less_', par_name)\n            par_name = re.sub('>', '_more_', par_name)\n            par_name = re.sub('/', '_div_', par_name)\n            par_name = re.sub('\\-', '_minus_', par_name)\n            par_name = re.sub('\\+', '_plus_', par_name)\n            par_name = re.sub('\\.', '_dot_', par_name)\n            par_name = re.sub('@', '_at_', par_name)\n\n            # Parameter names must be lower case\n            par_name = par_name.lower()\n\n            # Some parameters are enclosed between \", like \"z\"\n            par_name = par_name.replace('\"', '')\n\n            # \"z\" is a protected name in astromodels.\n            if par_name == \"z\":\n\n                par_name = \"redshift\"\n\n            # Check that the parameter name is not an illegal Python name\n            if par_name in illegal_variable_names:\n\n                par_name = \"xs_%s\" % par_name\n\n            # Sometimes the unit is \" \" which is not recognized by astropy\n            if par_unit:\n\n                par_unit = par_unit.replace(\"\\\"\",'')\n\n            # There are some errors in model.dat , like KeV instead of keV, and ergcm/s instead of erg cm /s\n            # Let's correct them\n            if par_unit == \"KeV\":\n\n                par_unit = \"keV\"\n\n            elif par_unit == \"ergcm/s\":\n\n                par_unit = \"erg cm / s\"\n\n            elif par_unit == \"days\":\n\n                par_unit = \"day\"\n\n            elif par_unit == \"z\" and par_name.lower() == \"redshift\":\n\n                par_unit = \"\"\n\n            # There are funny units in model.dat, like \"Rs\" (which means Schwarzschild radius) or other things\n            # so let's try to convert the par_unit into an astropy.Unit instance. If that fails, use a unitless unit\n            try:\n\n                _ = u.Unit(par_unit)\n\n            except ValueError:\n\n                # Unit not recognized\n\n                #warnings.warn(\"Unit %s is not recognized by astropy.\" % par_unit)\n\n                par_unit = ''\n\n            # Make sure that this is a valid python identifier\n            # by matching it with the relative regexp\n            if re.match('([a-zA-Z_][a-zA-Z0-9_]*)$', par_name) is None:\n\n                raise ValueError(\"Illegal identifier name %s\" % (par_name))\n\n            this_model['parameters'][par_name] = {'initial value': float(default_value),\n                                                  'desc': '(see https://heasarc.gsfc.nasa.gov/xanadu/xspec/manual/'\n                                                          'XspecModels.html)',\n                                                  'min': hard_minimum,\n                                                  'max': hard_maximum,\n                                                  'delta': float(delta),\n                                                  'unit': par_unit,\n                                                  'free': free}\n\n    return model_definitions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_source(self, source):\n\n        try:\n\n            self._add_child(source)\n\n        except AttributeError:\n\n            if isinstance(source, Source):\n\n                raise DuplicatedNode(\"More than one source with the name '%s'. You cannot use the same name for multiple \"\n                                     \"sources\" % source.name)\n\n            else:  # pragma: no cover\n\n                raise\n\n        # Now see if this is a point or extended source, and add them to the\n        # appropriate dictionary\n\n        if source.source_type == POINT_SOURCE:\n\n            self._point_sources[source.name] = source\n\n        elif source.source_type == EXTENDED_SOURCE:\n\n            self._extended_sources[source.name] = source\n\n        elif source.source_type == PARTICLE_SOURCE:\n\n            self._particle_sources[source.name] = source\n\n        else:  # pragma: no cover\n\n            raise InvalidInput(\"Input sources must be either a point source or an extended source\")", "response": "Add a source to the internal dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _remove_source(self, source_name):\n\n        assert source_name in self.sources, \"Source %s is not part of the current model\" % source_name\n\n        source = self.sources.pop(source_name)\n\n        if source.source_type == POINT_SOURCE:\n\n            self._point_sources.pop(source.name)\n\n        elif source.source_type == EXTENDED_SOURCE:\n\n            self._extended_sources.pop(source.name)\n\n        elif source.source_type == PARTICLE_SOURCE:\n\n            self._particle_sources.pop(source.name)\n\n        self._remove_child(source_name)", "response": "Remove a source from the current model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a dictionary with all the free parameters in this model", "response": "def free_parameters(self):\n        \"\"\"\n        Get a dictionary with all the free parameters in this model\n\n        :return: dictionary of free parameters\n        \"\"\"\n\n        # Refresh the list\n\n        self._update_parameters()\n\n        # Filter selecting only free parameters\n\n        free_parameters_dictionary = collections.OrderedDict()\n\n        for parameter_name, parameter in self._parameters.iteritems():\n\n            if parameter.free:\n\n                free_parameters_dictionary[parameter_name] = parameter\n\n        return free_parameters_dictionary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linked_parameters(self):\n\n        # Refresh the list\n\n        self._update_parameters()\n\n        # Filter selecting only free parameters\n\n        linked_parameter_dictionary = collections.OrderedDict()\n\n        for parameter_name, parameter in self._parameters.iteritems():\n\n            if parameter.has_auxiliary_variable():\n\n                linked_parameter_dictionary[parameter_name] = parameter\n\n        return linked_parameter_dictionary", "response": "Get a dictionary with all parameters in a linked status."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_free_parameters(self, values):\n\n        assert len(values) == len(self.free_parameters)\n\n        for parameter, this_value in zip(self.free_parameters.values(), values):\n\n            parameter.value = this_value", "response": "Set the free parameters in the model to the provided values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary containing all defined sources of any kind.", "response": "def sources(self):\n        \"\"\"\n        Returns a dictionary containing all defined sources (of any kind)\n\n        :return: collections.OrderedDict()\n\n        \"\"\"\n\n        sources = collections.OrderedDict()\n\n        for d in (self.point_sources, self.extended_sources, self.particle_sources):\n\n            sources.update(d)\n\n        return sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_independent_variable(self, variable):\n\n        assert isinstance(variable, IndependentVariable), \"Variable must be an instance of IndependentVariable\"\n\n        if self._has_child(variable.name):\n\n            self._remove_child(variable.name)\n\n        self._add_child(variable)\n\n        # Add also to the list of independent variables\n        self._independent_variables[variable.name] = variable", "response": "Adds an independent variable to this model such as time."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves an independent variable which was added with add_independent_variable", "response": "def remove_independent_variable(self, variable_name):\n        \"\"\"\n        Remove an independent variable which was added with add_independent_variable\n\n        :param variable_name: name of variable to remove\n        :return:\n        \"\"\"\n\n        self._remove_child(variable_name)\n\n        # Remove also from the list of independent variables\n        self._independent_variables.pop(variable_name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_external_parameter(self, parameter):\n\n        assert isinstance(parameter, Parameter), \"Variable must be an instance of IndependentVariable\"\n\n        if self._has_child(parameter.name):\n\n            # Remove it from the children only if it is a Parameter instance, otherwise don't, which will\n            # make the _add_child call fail (which is the expected behaviour! You shouldn't call two children\n            # with the same name)\n\n            if isinstance(self._get_child(parameter.name), Parameter):\n\n                warnings.warn(\"External parameter %s already exist in the model. Overwriting it...\" % parameter.name,\n                              RuntimeWarning)\n\n                self._remove_child(parameter.name)\n\n        # This will fail if another node with the same name is already in the model\n\n        self._add_child(parameter)", "response": "Add an external parameter that comes from something other than a function to the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef link(self, parameter_1, parameter_2, link_function=None):\n        if not isinstance(parameter_1,list):\n        # Make a list of one element\n            parameter_1_list  = [parameter_1]\n        else:\n        # Make a copy to avoid tampering with the input\n            parameter_1_list = list(parameter_1)\n        \n        \n        for param_1 in parameter_1_list:\n            assert param_1.path in self, \"Parameter %s is not contained in this model\" % param_1.path\n\n        assert parameter_2.path in self, \"Parameter %s is not contained in this model\" % parameter_2.path\n        \n        if link_function is None:\n            # Use the Line function by default, with both parameters fixed so that the two\n            # parameters to be linked will vary together\n            link_function = get_function('Line')\n\n            link_function.a.value = 1\n            link_function.a.fix = True\n\n            link_function.b.value = 0\n            link_function.b.fix = True\n        \n        \n        \n        for param_1 in parameter_1_list: \n            param_1.add_auxiliary_variable(parameter_2, link_function)\n            # Now set the units of the link function\n            link_function.set_units(parameter_2.unit, param_1.unit)", "response": "Link the value of the provided parameters through the provided function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unlink(self, parameter):\n\n        if not isinstance(parameter,list):\n        # Make a list of one element\n            parameter_list  = [parameter]\n        else:\n        # Make a copy to avoid tampering with the input\n            parameter_list = list(parameter)\n            \n        for param in parameter_list:    \n            if param.has_auxiliary_variable():\n                param.remove_auxiliary_variable()\n\n            else:\n\n                with warnings.catch_warnings():\n\n                    warnings.simplefilter(\"always\", RuntimeWarning)\n\n                    warnings.warn(\"Parameter %s has no link to be removed.\" % param.path, RuntimeWarning)", "response": "Unlinks the specified parameter from the free one or more parameters which have been linked previously\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display(self, complete=False):\n\n        # Switch on the complete display flag\n        self._complete_display = bool(complete)\n\n        # This will automatically choose the best representation among repr and repr_html\n\n        super(Model, self).display()\n\n        # Go back to default\n\n        self._complete_display = False", "response": "Display information about the point source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, output_file, overwrite=False):\n\n        \"\"\"Save the model to disk\"\"\"\n\n        if os.path.exists(output_file) and overwrite is False:\n\n            raise ModelFileExists(\"The file %s exists already. If you want to overwrite it, use the 'overwrite=True' \"\n                                  \"options as 'model.save(\\\"%s\\\", overwrite=True)'. \" % (output_file, output_file))\n\n        else:\n\n            data = self.to_dict_with_types()\n\n            # Write it to disk\n\n            try:\n\n                # Get the YAML representation of the data\n\n                representation = my_yaml.dump(data, default_flow_style=False)\n\n                with open(output_file, \"w+\") as f:\n\n                    # Add a new line at the end of each voice (just for clarity)\n\n                    f.write(representation.replace(\"\\n\", \"\\n\\n\"))\n\n            except IOError:\n\n                raise CannotWriteModel(os.path.dirname(os.path.abspath(output_file)),\n                                       \"Could not write model file %s. Check your permissions to write or the \"\n                                       \"report on the free space which follows: \" % output_file)", "response": "Save the model to disk"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the RA and Dec of a point source", "response": "def get_point_source_position(self, id):\n        \"\"\"\n        Get the point source position (R.A., Dec)\n\n        :param id: id of the source\n        :return: a tuple with R.A. and Dec.\n        \"\"\"\n\n        pts = self._point_sources.values()[id]\n\n        return pts.position.get_ra(), pts.position.get_dec()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the fluxes for a given id - th point source", "response": "def get_point_source_fluxes(self, id, energies, tag=None):\n        \"\"\"\n        Get the fluxes from the id-th point source\n\n        :param id: id of the source\n        :param energies: energies at which you need the flux\n        :param tag: a tuple (integration variable, a, b) specifying the integration to perform. If this\n        parameter is specified then the returned value will be the average flux for the source computed as the integral\n        between a and b over the integration variable divided by (b-a). The integration variable must be an independent\n        variable contained in the model. If b is None, then instead of integrating the integration variable will be\n        set to a and the model evaluated in a.\n        :return: fluxes\n        \"\"\"\n\n        return self._point_sources.values()[id](energies, tag=tag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_extended_source_fluxes(self, id, j2000_ra, j2000_dec, energies):\n\n        return self._extended_sources.values()[id](j2000_ra, j2000_dec, energies)", "response": "Get the flux of the id - th extended sources at the given position at the given energies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the total differential flux at the provided energies from all point sources.", "response": "def get_total_flux(self, energies):\n        \"\"\"\n        Returns the total differential flux at the provided energies from all *point* sources\n\n        :return:\n        \"\"\"\n\n        fluxes = []\n\n        for src in self._point_sources:\n\n            fluxes.append(self._point_sources[src](energies))\n\n        return np.sum(fluxes, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a path into a single line.", "response": "def long_path_formatter(line, max_width=pd.get_option('max_colwidth')):\n    \"\"\"\n    If a path is longer than max_width, it substitute it with the first and last element,\n    joined by \"...\". For example 'this.is.a.long.path.which.we.want.to.shorten' becomes\n    'this...shorten'\n\n    :param line:\n    :param max_width:\n    :return:\n    \"\"\"\n\n    if len(line) > max_width:\n\n        tokens = line.split(\".\")\n        trial1 = \"%s...%s\" % (tokens[0], tokens[-1])\n\n        if len(trial1) > max_width:\n\n            return \"...%s\" %(tokens[-1][-1:-(max_width-3)])\n\n        else:\n\n            return trial1\n\n    else:\n\n        return line"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True or False whether there is any free parameter in this source.", "response": "def has_free_parameters(self):\n        \"\"\"\n        Returns True or False whether there is any parameter in this source\n\n        :return:\n        \"\"\"\n\n        for component in self._components.values():\n\n            for par in component.shape.parameters.values():\n\n                if par.free:\n\n                    return True\n\n        for par in self.position.parameters.values():\n\n            if par.free:\n\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef free_parameters(self):\n        free_parameters = collections.OrderedDict()\n\n        for component in self._components.values():\n\n            for par in component.shape.parameters.values():\n\n                if par.free:\n\n                    free_parameters[par.path] = par\n\n        for par in self.position.parameters.values():\n\n            if par.free:\n\n                free_parameters[par.path] = par\n\n        return free_parameters", "response": "Returns a dictionary of free parameters for this source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parameters(self):\n        all_parameters = collections.OrderedDict()\n\n        for component in self._components.values():\n\n            for par in component.shape.parameters.values():\n\n                all_parameters[par.path] = par\n\n        for par in self.position.parameters.values():\n\n            all_parameters[par.path] = par\n\n        return all_parameters", "response": "Returns a dictionary of all parameters for this source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _repr__base(self, rich_output=False):\n\n        # Make a dictionary which will then be transformed in a list\n\n        repr_dict = collections.OrderedDict()\n\n        key = '%s (point source)' % self.name\n\n        repr_dict[key] = collections.OrderedDict()\n        repr_dict[key]['position'] = self._sky_position.to_dict(minimal=True)\n        repr_dict[key]['spectrum'] = collections.OrderedDict()\n\n        for component_name, component in self.components.iteritems():\n\n            repr_dict[key]['spectrum'][component_name] = component.to_dict(minimal=True)\n\n        return dict_to_list(repr_dict, rich_output)", "response": "Return the representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_flux(self, energies):\n\n        \"\"\"Get the total flux of this particle source at the given energies (summed over the components)\"\"\"\n\n        results = [component.shape(energies) for component in self.components.values()]\n\n        return numpy.sum(results, 0)", "response": "Get the total flux of this particle source at the given energies"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef free_parameters(self):\n        free_parameters = collections.OrderedDict()\n\n        for component in self._components.values():\n\n            for par in component.shape.parameters.values():\n\n                if par.free:\n\n                    free_parameters[par.path] = par\n\n        return free_parameters", "response": "Returns a dictionary of free parameters for this source."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parameters(self):\n        all_parameters = collections.OrderedDict()\n\n        for component in self._components.values():\n\n            for par in component.shape.parameters.values():\n\n                all_parameters[par.path] = par\n\n        return all_parameters", "response": "Returns a dictionary of all parameters for this source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the total integral over the spatial components.", "response": "def get_total_spatial_integral(self, z=None):  \n        \"\"\"\n        Returns the total integral (for 2D functions) or the integral over the spatial components (for 3D functions).\n        needs to be implemented in subclasses.\n\n        :return: an array of values of the integral (same dimension as z).\n        \"\"\"\n\n        if isinstance( z, u.Quantity):\n            z = z.value\n        return np.ones_like( z )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_function(function_name, composite_function_expression=None):\n\n    # Check whether this is a composite function or a simple function\n    if composite_function_expression is not None:\n\n        # Composite function\n\n        return _parse_function_expression(composite_function_expression)\n\n    else:\n\n        if function_name in _known_functions:\n\n            return _known_functions[function_name]()\n\n        else:\n\n            # Maybe this is a template\n\n            # NOTE: import here to avoid circular import\n\n            from astromodels.functions.template_model import TemplateModel, MissingDataFile\n\n            try:\n\n                instance = TemplateModel(function_name)\n\n            except MissingDataFile:\n\n                raise UnknownFunction(\"Function %s is not known. Known functions are: %s\" %\n                                      (function_name, \",\".join(_known_functions.keys())))\n\n            else:\n\n                return instance", "response": "Returns the function name which must be among the known functions or a composite function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_function_class(function_name):\n\n    if function_name in _known_functions:\n\n        return _known_functions[function_name]\n\n    else:\n\n        raise UnknownFunction(\"Function %s is not known. Known functions are: %s\" %\n                              (function_name, \",\".join(_known_functions.keys())))", "response": "Return the type for the requested function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a complex function expression like a function_name1 - > function_name2 - > function_name3 - > function_name4 - > function_name5 - > function_name5 - > function_name4 - > function_name5 - > function_name5 - > function_name5 - > function_name5 - > function_name5 - > function_name5 - > function_name5 - > function_name5 - > function_name5", "response": "def _parse_function_expression(function_specification):\n    \"\"\"\n    Parse a complex function expression like:\n\n    ((((powerlaw{1} + (sin{2} * 3)) + (sin{2} * 25)) - (powerlaw{1} * 16)) + (sin{2} ** 3.0))\n\n    and return a composite function instance\n\n    :param function_specification:\n    :return: a composite function instance\n    \"\"\"\n\n    # NOTE FOR SECURITY\n    # This function has some security concerns. Security issues could arise if the user tries to read a model\n    # file which has been maliciously formatted to contain harmful code. In this function we close all the doors\n    # to a similar attack, except for those attacks which assume that the user has full access to a python environment.\n    # Indeed, if that is the case, then the user can already do harm to the system, and so there is no point in\n    # safeguard that from here. For example, the user could format a subclass of the Function class which perform\n    # malicious operations in the constructor, add that to the dictionary of known functions, and then interpret\n    # it with this code. However, if the user can instance malicious classes, then why would he use astromodels to\n    # carry out the attack? Instead, what we explicitly check is the content of the function_specification string,\n    # so that it cannot by itself do any harm (by for example containing instructions such as os.remove).\n\n    # This can be a arbitrarily complex specification, like\n    # ((((powerlaw{1} + (sin{2} * 3)) + (sin{2} * 25)) - (powerlaw{1} * 16)) + (sin{2} ** 3.0))\n\n    # Use regular expressions to extract the set of functions like function_name{number},\n    # then build the set of unique functions by using the constructor set()\n\n    unique_functions = set(re.findall(r'\\b([a-zA-Z0-9_]+)\\{([0-9]?)\\}',function_specification))\n\n    # NB: unique functions is a set like:\n    # {('powerlaw', '1'), ('sin', '2')}\n\n    # Create instances of the unique functions\n\n    instances = {}\n\n    # Loop over the unique functions and create instances\n\n    for (unique_function, number) in unique_functions:\n\n        complete_function_specification = \"%s{%s}\" % (unique_function, number)\n\n        # As first safety measure, check that the unique function is in the dictionary of _known_functions.\n        # This could still be easily hacked, so it won't be the only check\n\n        if unique_function in _known_functions:\n\n            # Get the function class and check that it is indeed a proper Function class\n\n            function_class = _known_functions[unique_function]\n\n            if issubclass(function_class, Function):\n\n                # Ok, let's create the instance\n\n                instance = function_class()\n\n                # Append the instance to the list\n\n                instances[complete_function_specification] = instance\n\n            else:\n\n                raise FunctionDefinitionError(\"The function specification %s does not contain a proper function\"\n                                              % unique_function )\n\n        else:\n\n            # It might be a template\n\n            # This import is here to avoid circular dependency between this module and TemplateModel.py\n            import astromodels.functions.template_model\n\n            try:\n\n                instance = astromodels.functions.template_model.TemplateModel(unique_function)\n\n            except astromodels.functions.template_model.MissingDataFile:\n\n                # It's not a template\n\n                raise UnknownFunction(\"Function %s in expression %s is unknown. If this is a template model, you are \"\n                                      \"probably missing the data file\" % (unique_function, function_specification))\n\n            else:\n\n                # It's a template\n\n                instances[complete_function_specification] = instance\n\n    # Check that we have found at least one instance.\n\n    if len(instances)==0:\n\n        raise DesignViolation(\"No known function in function specification\")\n\n    # The following presents a slight security problem if the model file that has been parsed comes from an untrusted\n    # source. Indeed, the use of eval could make possible to execute things like os.remove.\n    # In order to avoid this, first we substitute the function instances with numbers and remove the operators like\n    # +,-,/ and so on. Then we try to execute the string with ast.literal_eval, which according to its documentation:\n\n    # Safely evaluate an expression node or a Unicode or Latin-1 encoded string containing a Python literal or\n    # container display. The string or node provided may only consist of the following Python literal structures:\n    # strings, numbers, tuples, lists, dicts, booleans, and None.This can be used for safely evaluating strings\n    # containing Python values from untrusted sources without the need to parse the values oneself.\n    # It is not capable of evaluating arbitrarily complex expressions, for example involving operators or indexing.\n\n    # If literal_eval cannot parse the string, it means that it contains unsafe input.\n\n    # Create a copy of the function_specification\n\n    string_for_literal_eval = function_specification\n\n    # Remove from the function_specification all the known operators and function_expressions, and substitute them\n    # with a 0 and a space\n\n    # Let's start from the function expression\n\n    for function_expression in instances.keys():\n\n        string_for_literal_eval = string_for_literal_eval.replace(function_expression, '0 ')\n\n    # Now remove all the known operators\n\n    for operator in _operations.keys():\n\n        string_for_literal_eval = string_for_literal_eval.replace(operator,'0 ')\n\n    # The string at this point should contains only numbers and parenthesis separated by one or more spaces\n\n    if re.match('''([a-zA-Z]+)''', string_for_literal_eval):\n\n        raise DesignViolation(\"Extraneous input in function specification\")\n\n    # By using split() we separate all the numbers and parenthesis in a list, then we join them\n    # with a comma, to end up with a comma-separated list of parenthesis and numbers like:\n    # ((((0,0,(0,0,3)),0,(0,0,25)),0,(0,0,16)),0,(0,0,0,3.0))\n    # This string can be parsed by literal_eval as a tuple containing other tuples, which is fine.\n    # If the user has inserted some malicious content, like os.remove or more weird stuff like code objects,\n    # the parsing will fail\n\n    string_for_literal_eval = \",\".join(string_for_literal_eval.split())\n\n    #print(string_for_literal_eval)\n\n    # At this point the string should be just a comma separated list of numbers\n\n    # Now try to execute the string\n    try:\n\n        ast.literal_eval(string_for_literal_eval)\n\n    except (ValueError, SyntaxError):\n\n        raise DesignViolation(\"The given expression is not a valid function expression\")\n\n    else:\n\n        # The expression is safe, let's eval it\n\n        # First substitute the reference to the functions (like 'powerlaw{1}') with a string\n        # corresponding to the instance dictionary\n\n        sanitized_function_specification = function_specification\n\n        for function_expression in instances.keys():\n\n            sanitized_function_specification = sanitized_function_specification.replace(function_expression,\n                                                                                        'instances[\"%s\"]' %\n                                                                                        function_expression)\n\n        # Now eval it. For safety measure, I remove all globals, and the only local is the 'instances' dictionary\n\n        composite_function = eval(sanitized_function_specification, {}, {'instances': instances})\n\n        return composite_function"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_calling_sequence(name, function_name, function, possible_variables):\n\n        # Get calling sequence\n\n        # If the function has been memoized, it will have a \"input_object\" member\n\n        try:\n\n            calling_sequence = inspect.getargspec(function.input_object).args\n\n        except AttributeError:\n\n            # This might happen if the function is with memoization\n\n            calling_sequence = inspect.getargspec(function).args\n\n        assert calling_sequence[0] == 'self', \"Wrong syntax for 'evaluate' in %s. The first argument \" \\\n                                              \"should be called 'self'.\" % name\n\n        # Figure out how many variables are used\n\n        variables = filter(lambda var: var in possible_variables, calling_sequence)\n\n        # Check that they actually make sense. They must be used in the same order\n        # as specified in possible_variables\n\n        assert len(variables) > 0, \"The name of the variables for 'evaluate' in %s must be one or more \" \\\n                                   \"among %s, instead of %s\" % (name, ','.join(possible_variables), \",\".join(variables))\n\n        if variables != possible_variables[:len(variables)]:\n            raise AssertionError(\"The variables %s are out of order in '%s' of %s. Should be %s.\"\n                                 % (\",\".join(variables), function_name, name, possible_variables[:len(variables)]))\n\n        other_parameters = filter(lambda var: var not in variables and var != 'self', calling_sequence)\n\n        return variables, other_parameters", "response": "Check the calling sequence for the function and the variables specified."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef free_parameters(self):\n\n        free_parameters = collections.OrderedDict([(k,v) for k, v in self.parameters.iteritems() if v.free])\n\n        return free_parameters", "response": "Returns a dictionary of free parameters for this function\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate_at(self, *args, **parameter_specification):  # pragma: no cover\n\n        # Set the parameters to the provided values\n        for parameter in parameter_specification:\n\n            self._get_child(parameter).value = parameter_specification[parameter]\n\n        return self(*args)", "response": "Evaluate the function at the given x y z"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_unit_cube(self, x):\n\n        mu = self.mu.value\n        sigma = self.sigma.value\n\n        sqrt_two = 1.414213562\n\n        if x < 1e-16 or (1 - x) < 1e-16:\n\n            res = -1e32\n\n        else:\n\n            res = mu + sigma * sqrt_two * erfcinv(2 * (1 - x))\n\n        return res", "response": "This method returns the value of the logarithm of the logarithm of the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_unit_cube(self, x):\n\n        x0 = self.x0.value\n        gamma = self.gamma.value\n\n        half_pi = 1.57079632679\n\n        res = np.tan(np.pi * x - half_pi) * gamma + x0\n\n        return res", "response": "This method returns the value of the logarithm of the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse by multinest :param x: 0 < x < 1 :param lower_bound: :param upper_bound: :return:", "response": "def from_unit_cube(self, x):\n        \"\"\"\n        Used by multinest\n\n        :param x: 0 < x < 1\n        :param lower_bound:\n        :param upper_bound:\n        :return:\n        \"\"\"\n        cosdec_min = np.cos(deg2rad*(90.0 + self.lower_bound.value))\n        cosdec_max = np.cos(deg2rad*(90.0 + self.upper_bound.value))\n\n        v = x * (cosdec_max - cosdec_min)\n        v += cosdec_min\n\n        v = np.clip(v, -1.0, 1.0)\n        # Now this generates on [0,pi)\n        dec = np.arccos(v)\n\n        # convert to degrees\n        dec = rad2deg * dec\n        # now in range [-90,90.0)\n        dec -= 90.0\n\n        return dec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_unit_cube(self, x):\n\n        lower_bound = self.lower_bound.value\n        upper_bound = self.upper_bound.value\n\n        low = lower_bound\n        spread = float(upper_bound - lower_bound)\n\n        par = x * spread + low\n\n        return par", "response": "This method returns the value of the n - th key attribute in the unit cube."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_unit_cube(self, x):\n\n        low = math.log10(self.lower_bound.value)\n        up = math.log10(self.upper_bound.value)\n\n        spread = up - low\n        par = 10 ** (x * spread + low)\n\n        return par", "response": "Used by multinest\n        from unit cube"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the absolute path to the data file.", "response": "def _get_data_file_path(data_file):\n    \"\"\"\n    Returns the absolute path to the required data files.\n\n    :param data_file: relative path to the data file, relative to the astromodels/data path.\n    So to get the path to data/dark_matter/gammamc_dif.dat you need to use data_file=\"dark_matter/gammamc_dif.dat\"\n    :return: absolute path of the data file\n    \"\"\"\n\n    try:\n\n        file_path = pkg_resources.resource_filename(\"astromodels\", 'data/%s' % data_file)\n\n    except KeyError:\n\n        raise IOError(\"Could not read or find data file %s. Try reinstalling astromodels. If this does not fix your \"\n                      \"problem, open an issue on github.\" % (data_file))\n\n    else:\n\n        return os.path.abspath(file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmap between the channel codes and the rows in the gammamc file 1 : 8, # ee 2 : 6, # mumu 3 : 3, # tautau 4 : 1, # bb 5 : 2, # tt 6 : 7, # gg 7 : 4, # ww 8 : 5, # zz 9 : 0, # cc 10 : 10, # uu 11 : 11, # dd 12 : 9, # ss", "response": "def _setup(self):\n\n        tablepath = _get_data_file_path(\"dark_matter/gammamc_dif.dat\")\n\n        self._data = np.loadtxt(tablepath)\n\n        \"\"\"\n            Mapping between the channel codes and the rows in the gammamc file\n\n            1 : 8, # ee\n            2 : 6, # mumu\n            3 : 3, # tautau\n            4 : 1, # bb\n            5 : 2, # tt\n            6 : 7, # gg\n            7 : 4, # ww\n            8 : 5, # zz\n            9 : 0, # cc\n            10 : 10, # uu\n            11 : 11, # dd\n            12 : 9, # ss\n        \"\"\"\n\n        channel_index_mapping = {\n            1: 8,  # ee\n            2: 6,  # mumu\n            3: 3,  # tautau\n            4: 1,  # bb\n            5: 2,  # tt\n            6: 7,  # gg\n            7: 4,  # ww\n            8: 5,  # zz\n            9: 0,  # cc\n            10: 10,  # uu\n            11: 11,  # dd\n            12: 9,  # ss\n        }\n\n        # Number of decades in x = log10(E/M)\n        ndec = 10.0\n        xedge = np.linspace(0, 1.0, 251)\n        self._x = 0.5 * (xedge[1:] + xedge[:-1]) * ndec - ndec\n\n        ichan = channel_index_mapping[int(self.channel.value)]\n\n        # These are the mass points\n        self._mass = np.array([2.0, 4.0, 6.0, 8.0, 10.0,\n                               25.0, 50.0, 80.3, 91.2, 100.0,\n                               150.0, 176.0, 200.0, 250.0, 350.0, 500.0, 750.0,\n                               1000.0, 1500.0, 2000.0, 3000.0, 5000.0, 7000.0, 1E4])\n        self._dn = self._data.reshape((12, 24, 250))\n\n        self._dn_interp = RegularGridInterpolator([self._mass, self._x],\n                                                  self._dn[ichan, :, :],\n                                                  bounds_error=False,\n                                                  fill_value=None)\n\n        if self.mass.value > 10000:\n\n            print \"Warning: DMFitFunction only appropriate for masses <= 10 TeV\"\n            print \"To model DM from 2 GeV < mass < 1 PeV use DMSpectra\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _setup(self):\n\n        # Get and open the two data files\n\n        tablepath_h = _get_data_file_path(\"dark_matter/dmSpecTab.npy\")\n        self._data_h = np.load(tablepath_h)\n\n        tablepath_f = _get_data_file_path(\"dark_matter/gammamc_dif.dat\")\n        self._data_f = np.loadtxt(tablepath_f)\n\n        \"\"\"\n            Mapping between the channel codes and the rows in the gammamc file\n            dmSpecTab.npy created to match this mapping too\n\n            1 : 8, # ee\n            2 : 6, # mumu\n            3 : 3, # tautau\n            4 : 1, # bb\n            5 : 2, # tt\n            6 : 7, # gg\n            7 : 4, # ww\n            8 : 5, # zz\n            9 : 0, # cc\n            10 : 10, # uu\n            11 : 11, # dd\n            12 : 9, # ss\n            \"\"\"\n\n        channel_index_mapping = {\n            1: 8,  # ee\n            2: 6,  # mumu\n            3: 3,  # tautau\n            4: 1,  # bb\n            5: 2,  # tt\n            6: 7,  # gg\n            7: 4,  # ww\n            8: 5,  # zz\n            9: 0,  # cc\n            10: 10,  # uu\n            11: 11,  # dd\n            12: 9,  # ss\n        }\n\n        # Number of decades in x = log10(E/M)\n        ndec = 10.0\n        xedge = np.linspace(0, 1.0, 251)\n        self._x = 0.5 * (xedge[1:] + xedge[:-1]) * ndec - ndec\n\n        ichan = channel_index_mapping[int(self.channel.value)]\n\n        # These are the mass points in GeV\n        self._mass_h = np.array([50., 61.2, 74.91, 91.69, 112.22, 137.36, 168.12, 205.78, 251.87, 308.29,\n                                 377.34, 461.86, 565.31, 691.93, 846.91, 1036.6, 1268.78, 1552.97, 1900.82,\n                                 2326.57, 2847.69, 3485.53, 4266.23, 5221.81, 6391.41, 7823.0, 9575.23,\n                                 11719.94, 14345.03, 17558.1, 21490.85, 26304.48, 32196.3, 39407.79, 48234.54,\n                                 59038.36, 72262.07, 88447.7, 108258.66, 132506.99, 162186.57, 198513.95,\n                                 242978.11, 297401.58, 364015.09, 445549.04, 545345.37, 667494.6, 817003.43, 1000000.])\n\n        # These are the mass points in GeV\n        self._mass_f = np.array([2.0, 4.0, 6.0, 8.0, 10.0,\n                                 25.0, 50.0, 80.3, 91.2, 100.0,\n                                 150.0, 176.0, 200.0, 250.0, 350.0, 500.0, 750.0,\n                                 1000.0, 1500.0, 2000.0, 3000.0, 5000.0, 7000.0, 1E4])\n\n        self._mass = np.append(self._mass_f, self._mass_h[27:])\n\n        self._dn_f = self._data_f.reshape((12, 24, 250))\n\n        # Is this really used?\n        self._dn_h = self._data_h\n\n        self._dn = np.zeros((12, len(self._mass), 250))\n        self._dn[:, 0:24, :] = self._dn_f\n        self._dn[:, 24:, :] = self._dn_h[:, 27:, :]\n\n        self._dn_interp = RegularGridInterpolator([self._mass, self._x],\n                                                  self._dn[ichan, :, :],\n                                                  bounds_error=False,\n                                                  fill_value=None)\n\n        if self.channel.value in [1, 6, 7] and self.mass.value > 10000.:\n            print \"ERROR: currently spectra for selected channel and mass not implemented.\"\n            print \"Spectra for channels ['ee','gg','WW'] currently not available for mass > 10 TeV\"", "response": "Setup the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vincenty(lon0, lat0, a1, s):\n\n  lon0 = np.deg2rad(lon0)\n  lat0 = np.deg2rad(lat0)\n  a1   = np.deg2rad(a1)\n  s    = np.deg2rad(s)\n\n  sina = np.cos(lat0) * np.sin(a1)\n\n  num1 = np.sin(lat0)*np.cos(s) + np.cos(lat0)*np.sin(s)*np.cos(a1)\n  den1 = np.sqrt(sina**2 + (np.sin(lat0)*np.sin(s) - np.cos(lat0)*np.cos(a1))**2)\n  lat  = np.rad2deg(np.arctan2(num1, den1))\n\n  num2 = np.sin(s)*np.sin(a1)\n  den2 = np.cos(lat0)*np.cos(s) - np.sin(lat0)*np.sin(s)*np.cos(a1)\n  L    = np.arctan2(num2, den2)\n  lon  = np.rad2deg(lon0 + L)\n\n  return lon, lat", "response": "This function calculates the coordinates of a new point that is a given angular distance s away from a starting point a1 and s away from a second point."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the provided string is a valid variable name in Python", "response": "def is_valid_variable_name(string_to_check):\n    \"\"\"\n    Returns whether the provided name is a valid variable name in Python\n\n    :param string_to_check: the string to be checked\n    :return: True or False\n    \"\"\"\n\n    try:\n\n        parse('{} = None'.format(string_to_check))\n        return True\n\n    except (SyntaxError, ValueError, TypeError):\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_unit(new_unit, old_unit):\n\n    try:\n\n        new_unit.physical_type\n\n    except AttributeError:\n\n        raise UnitMismatch(\"The provided unit (%s) has no physical type. Was expecting a unit for %s\"\n                           % (new_unit, old_unit.physical_type))\n\n    if new_unit.physical_type != old_unit.physical_type:\n\n        raise UnitMismatch(\"Physical type mismatch: you provided a unit for %s instead of a unit for %s\"\n                           % (new_unit.physical_type, old_unit.physical_type))", "response": "Check that the new unit is compatible with the old unit."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the peak energy in the nuFnu spectrum", "response": "def peak_energy(self):\n        \"\"\"\n        Returns the peak energy in the nuFnu spectrum\n\n        :return: peak energy in keV\n        \"\"\"\n\n        # Eq. 6 in Massaro et al. 2004\n        # (http://adsabs.harvard.edu/abs/2004A%26A...413..489M)\n\n        return self.piv.value * pow(10, ((2 + self.alpha.value) * np.log(10)) / (2 * self.beta.value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_spatially_integrated_flux( self, energies):\n    \n        \"\"\"\n        Returns total flux of source at the given energy\n        :param energies: energies (array or float)\n        :return: differential flux at given energy\n        \"\"\"\n       \n        if not isinstance(energies, np.ndarray):\n            energies = np.array(energies, ndmin=1)\n\n        # Get the differential flux from the spectral components\n\n        results = [self.spatial_shape.get_total_spatial_integral(energies) * component.shape(energies) for component in self.components.values()]\n\n        if isinstance(energies, u.Quantity):\n\n            # Slow version with units\n\n            # We need to sum like this (slower) because using np.sum will not preserve the units\n            # (thanks astropy.units)\n\n            differential_flux = sum(results)\n\n        else:\n\n            # Fast version without units, where x is supposed to be in the same units as currently defined in\n            # units.get_units()\n\n            differential_flux = np.sum(results, 0)\n\n        return differential_flux", "response": "Returns the total flux of source at the given energies."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the right ascension of the current position.", "response": "def get_ra(self):\n        \"\"\"\n        Get R.A. corresponding to the current position (ICRS, J2000)\n\n        :return: Right Ascension\n        \"\"\"\n\n        try:\n\n            return self.ra.value\n\n        except AttributeError:\n\n            # Transform from L,B to R.A., Dec\n\n            return self.sky_coord.transform_to('icrs').ra.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dec(self):\n\n        try:\n\n            return self.dec.value\n\n        except AttributeError:\n\n            # Transform from L,B to R.A., Dec\n\n            return self.sky_coord.transform_to('icrs').dec.value", "response": "Get Dec. corresponding to the current position ( ICRS J2000"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_l(self):\n\n        try:\n\n            return self.l.value\n\n        except AttributeError:\n\n            # Transform from L,B to R.A., Dec\n\n            return self.sky_coord.transform_to('galactic').l.value", "response": "Get Galactic Longitude corresponding to the current position\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting Galactic latitude corresponding to the current position", "response": "def get_b(self):\n        \"\"\"\n        Get Galactic latitude (b) corresponding to the current position\n\n        :return: Latitude\n        \"\"\"\n\n        try:\n\n            return self.b.value\n\n        except AttributeError:\n\n            # Transform from L,B to R.A., Dec\n\n            return self.sky_coord.transform_to('galactic').b.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the dictionary of parameters", "response": "def parameters(self):\n        \"\"\"\n        Get the dictionary of parameters (either ra,dec or l,b)\n\n        :return: dictionary of parameters\n        \"\"\"\n\n        if self._coord_type == 'galactic':\n\n            return collections.OrderedDict((('l', self.l), ('b', self.b)))\n\n        else:\n\n            return collections.OrderedDict((('ra', self.ra), ('dec', self.dec)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fix(self):\n\n        if self._coord_type == 'equatorial':\n\n            self.ra.fix = True\n            self.dec.fix = True\n\n        else:\n\n            self.l.fix = True\n            self.b.fix = True", "response": "Fix the parameters with the coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef free(self):\n\n        if self._coord_type == 'equatorial':\n\n            self.ra.fix = False\n            self.dec.fix = False\n\n        else:\n\n            self.l.fix = False\n            self.b.fix = False", "response": "Free the parameters with the coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accept_quantity(input_type=float, allow_none=False):\n\n    def accept_quantity_wrapper(method):\n\n        def handle_quantity(instance, value, *args, **kwargs):\n\n            # For speed reasons, first run the case where the input is not a quantity, and fall back to the handling\n            # of quantities if that fails. The parts that fails if the input is a Quantity is the conversion\n            # input_type(value). This could have been handled more elegantly with a \"finally\" clause, but that would\n            # have a 40 percent speed impact...\n\n            try:\n\n                new_value = input_type(value)\n\n                return method(instance, new_value, *args, **kwargs)\n\n            except TypeError:\n\n                # Slow for slow, check that we actually have a quantity or None (if allowed)\n\n                if isinstance(value, u.Quantity):\n\n                    new_value = value.to(instance.unit).value\n\n                    return method(instance, new_value, *args, **kwargs)\n\n                elif value is None:\n\n                    if allow_none:\n\n                        return method(instance, None, *args, **kwargs)\n\n                    else: # pragma: no cover\n\n                        raise TypeError(\"You cannot pass None as argument for \"\n                                        \"method %s of %s\" % (method.__name__, instance.name))\n\n                else: # pragma: no cover\n\n                    raise TypeError(\"You need to pass either a %s or a astropy.Quantity \"\n                                    \"to method %s of %s\" % (input_type.__name__, method.__name__, instance.name))\n\n        return handle_quantity\n\n    return accept_quantity_wrapper", "response": "A class - method decorator that allows a given method to receive a single astropy. Quantity or a simple float in the right units."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in_unit_of(self, unit, as_quantity=False):\n\n        new_unit = u.Unit(unit)\n\n        new_quantity = self.as_quantity.to(new_unit)\n\n        if as_quantity:\n\n            return new_quantity\n\n        else:\n\n            return new_quantity.value", "response": "Return the current value transformed to the new units"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef internal_to_external_delta(self, internal_value, internal_delta):\n\n        external_value = self.transformation.backward(internal_value)\n        bound_internal = internal_value + internal_delta\n        bound_external = self.transformation.backward(bound_internal)\n        external_delta = bound_external - external_value\n\n        return external_value, external_delta", "response": "Transform an interval from the internal to the external reference"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns current parameter value", "response": "def _get_value(self):\n        \"\"\"Return current parameter value\"\"\"\n\n        # This is going to be true (possibly) only for derived classes. It is here to make the code cleaner\n        # and also to avoid infinite recursion\n\n        if self._aux_variable:\n\n            return self._aux_variable['law'](self._aux_variable['variable'].value)\n\n        if self._transformation is None:\n\n            return self._internal_value\n\n        else:\n\n            # A transformation is set. Transform back from internal value to true value\n            #\n            # print(\"Interval value is %s\" % self._internal_value)\n            # print(\"Returning %s\" % self._transformation.backward(self._internal_value))\n\n            return self._transformation.backward(self._internal_value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the current value of the parameter ensuring that it is within the allowed range.", "response": "def _set_value(self, new_value):\n        \"\"\"Sets the current value of the parameter, ensuring that it is within the allowed range.\"\"\"\n\n        if self.min_value is not None and new_value < self.min_value:\n\n            raise SettingOutOfBounds(\n                \"Trying to set parameter {0} = {1}, which is less than the minimum allowed {2}\".format(\n                    self.name, new_value, self.min_value))\n\n        if self.max_value is not None and new_value > self.max_value:\n            raise SettingOutOfBounds(\n                \"Trying to set parameter {0} = {1}, which is more than the maximum allowed {2}\".format(\n                    self.name, new_value, self.max_value))\n\n        # Issue a warning if there is an auxiliary variable, as the setting does not have any effect\n        if self.has_auxiliary_variable():\n\n            with warnings.catch_warnings():\n\n                warnings.simplefilter(\"always\", RuntimeWarning)\n\n                warnings.warn(\"You are trying to assign to a parameter which is either linked or \"\n                              \"has auxiliary variables. The assignment has no effect.\", RuntimeWarning)\n\n        # Save the value as a pure floating point to avoid the overhead of the astropy.units machinery when\n        # not needed\n\n        if self._transformation is None:\n\n            new_internal_value = new_value\n\n        else:\n\n            new_internal_value = self._transformation.forward(new_value)\n\n        # If the parameter has changed, update its value and call the callbacks if needed\n\n        if new_internal_value != self._internal_value:\n\n            # Update\n            self._internal_value = new_internal_value\n\n            # Call the callbacks (if any)\n            for callback in self._callbacks:\n\n                try:\n\n                    callback(self)\n\n                except:\n\n                    raise NotCallableOrErrorInCall(\"Could not call callback for parameter %s\" % self.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets internal value of the current locale.", "response": "def _set_internal_value(self, new_internal_value):\n        \"\"\"\n        This is supposed to be only used by fitting engines\n\n        :param new_internal_value: new value in internal representation\n        :return: none\n        \"\"\"\n\n        if new_internal_value != self._internal_value:\n\n            self._internal_value = new_internal_value\n\n            # Call callbacks if any\n\n            for callback in self._callbacks:\n\n                callback(self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the current minimum allowed value of the current parameter.", "response": "def _set_min_value(self, min_value):\n\n        \"\"\"Sets current minimum allowed value\"\"\"\n\n        # Check that the min value can be transformed if a transformation is present\n        if self._transformation is not None:\n\n            if min_value is not None:\n\n                try:\n\n                    _ = self._transformation.forward(min_value)\n\n                except FloatingPointError:\n\n                    raise ValueError(\"The provided minimum %s cannot be transformed with the transformation %s which \"\n                                     \"is defined for the parameter %s\" % (min_value,\n                                                                          type(self._transformation),\n                                                                          self.path))\n\n        # Store the minimum as a pure float\n\n        self._external_min_value = min_value\n\n        # Check that the current value of the parameter is still within the boundaries. If not, issue a warning\n\n        if self._external_min_value is not None and self.value < self._external_min_value:\n\n            warnings.warn(\"The current value of the parameter %s (%s) \"\n                          \"was below the new minimum %s.\" % (self.name, self.value, self._external_min_value),\n                          exceptions.RuntimeWarning)\n\n            self.value = self._external_min_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the minimum value in internal representation.", "response": "def _get_internal_min_value(self):\n        \"\"\"\n        This is supposed to be only used by fitting engines to get the minimum value in internal representation.\n        It is supposed to be called only once before doing the minimization/sampling, to set the range of the parameter\n\n        :return: minimum value in internal representation (or None if there is no minimum)\n        \"\"\"\n\n        if self.min_value is None:\n\n            # No minimum set\n\n            return None\n\n        else:\n\n            # There is a minimum. If there is a transformation, use it, otherwise just return the minimum\n\n            if self._transformation is None:\n\n                return self._external_min_value\n\n            else:\n\n                return self._transformation.forward(self._external_min_value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the current maximum allowed value of the parameter.", "response": "def _set_max_value(self, max_value):\n        \"\"\"Sets current maximum allowed value\"\"\"\n\n        self._external_max_value = max_value\n\n        # Check that the current value of the parameter is still within the boundaries. If not, issue a warning\n\n        if self._external_max_value is not None and self.value > self._external_max_value:\n\n            warnings.warn(\"The current value of the parameter %s (%s) \"\n                          \"was above the new maximum %s.\" % (self.name, self.value, self._external_max_value),\n                          exceptions.RuntimeWarning)\n            self.value = self._external_max_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the maximum value in internal representation or None if there is no minimum.", "response": "def _get_internal_max_value(self):\n        \"\"\"\n        This is supposed to be only used by fitting engines to get the maximum value in internal representation.\n        It is supposed to be called only once before doing the minimization/sampling, to set the range of the parameter\n\n        :return: maximum value in internal representation (or None if there is no minimum)\n        \"\"\"\n\n        if self.max_value is None:\n\n            # No minimum set\n\n            return None\n\n        else:\n\n            # There is a minimum. If there is a transformation, use it, otherwise just return the minimum\n\n            if self._transformation is None:\n\n                return self._external_max_value\n\n            else:\n\n                return self._transformation.forward(self._external_max_value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the boundaries for this parameter to min_value and max_value", "response": "def _set_bounds(self, bounds):\n        \"\"\"Sets the boundaries for this parameter to min_value and max_value\"\"\"\n\n        # Use the properties so that the checks and the handling of units are made automatically\n\n        min_value, max_value = bounds\n\n        # Remove old boundaries to avoid problems with the new one, if the current value was within the old boundaries\n        # but is not within the new ones (it will then be adjusted automatically later)\n        self.min_value = None\n        self.max_value = None\n\n        self.min_value = min_value\n\n        self.max_value = max_value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_dict(self, minimal=False):\n\n        \"\"\"Returns the representation for serialization\"\"\"\n\n        data = collections.OrderedDict()\n\n        if minimal:\n\n            # In the minimal representation we just output the value\n\n            data['value'] = self._to_python_type(self.value)\n\n        else:\n\n            # In the complete representation we output everything is needed to re-build the object\n\n            data['value'] = self._to_python_type(self.value)\n            data['desc'] = str(self.description)\n            data['min_value'] = self._to_python_type(self.min_value)\n            data['max_value'] = self._to_python_type(self.max_value)\n            # We use our own thread-safe format for the unit\n            data['unit'] = self.unit.to_string(format='threadsafe')\n\n        return data", "response": "Returns the representation for serialization"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_internal_delta(self):\n\n        if self._transformation is None:\n\n            return self._delta\n\n        else:\n\n            delta_int = None\n\n            for i in range(2):\n\n                # Try using the low bound\n\n                low_bound_ext = self.value - self.delta\n\n                # Make sure we are within the margins\n\n                if low_bound_ext > self.min_value:\n\n                    # Ok, let's use that for the delta\n                    low_bound_int = self._transformation.forward(low_bound_ext)\n\n                    delta_int = abs(low_bound_int - self._get_internal_value())\n\n                    break\n\n                else:\n\n                    # Nope, try with the hi bound\n\n                    hi_bound_ext = self.value + self._delta\n\n                    if hi_bound_ext < self.max_value:\n\n                        # Ok, let's use it\n                        hi_bound_int = self._transformation.forward(hi_bound_ext)\n                        delta_int = abs(hi_bound_int - self._get_internal_value())\n\n                        break\n\n                    else:\n\n                        # Fix delta\n                        self.delta = abs(self.value - self.min_value) / 4.0\n\n                        if self.delta == 0:\n\n                            # Parameter at the minimum\n                            self.delta = abs(self.value - self.max_value) / 4.0\n\n                        # Try again\n                        continue\n\n            assert delta_int is not None, \"Bug\"\n\n            return delta_int", "response": "This method returns the initial delta in internal representation of the current value."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the prior for this parameter.", "response": "def _set_prior(self, prior):\n        \"\"\"Set prior for this parameter. The prior must be a function accepting the current value of the parameter\n        as input and giving the probability density as output.\"\"\"\n\n        if prior is None:\n\n            # Removing prior\n\n            self._prior = None\n\n        else:\n\n            # Try and call the prior with the current value of the parameter\n            try:\n\n                _ = prior(self.value)\n\n            except:\n\n                raise NotCallableOrErrorInCall(\"Could not call the provided prior. \" +\n                                               \"Is it a function accepting the current value of the parameter?\")\n\n            try:\n\n                prior.set_units(self.unit, u.dimensionless_unscaled)\n\n            except AttributeError:\n\n                raise NotCallableOrErrorInCall(\"It looks like the provided prior is not a astromodels function.\")\n\n            self._prior = prior"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the prior for the parameter to a uniform prior between the current minimum and maximum or Uniform_prior.", "response": "def set_uninformative_prior(self, prior_class):\n        \"\"\"\n        Sets the prior for the parameter to a uniform prior between the current minimum and maximum, or a\n        log-uniform prior between the current minimum and maximum.\n\n        NOTE: if the current minimum and maximum are not defined, the default bounds for the prior class will be used.\n\n        :param prior_class : the class to be used as prior (either Log_uniform_prior or Uniform_prior, or a class which\n        provide a lower_bound and an upper_bound properties)\n        :return: (none)\n        \"\"\"\n\n        prior_instance = prior_class()\n\n        if self.min_value is None:\n\n            raise ParameterMustHaveBounds(\"Parameter %s does not have a defined minimum. Set one first, then re-run \"\n                                          \"set_uninformative_prior\" % self.path)\n\n        else:\n\n            try:\n\n                prior_instance.lower_bound = self.min_value\n\n            except SettingOutOfBounds:\n\n                raise SettingOutOfBounds(\"Cannot use minimum of %s for prior %s\" % (self.min_value,\n                                                                                    prior_instance.name))\n\n        if self.max_value is None:\n\n            raise ParameterMustHaveBounds(\"Parameter %s does not have a defined maximum. Set one first, then re-run \"\n                                          \"set_uninformative_prior\" % self.path)\n\n        else: # pragma: no cover\n\n            try:\n\n                prior_instance.upper_bound = self.max_value\n\n            except SettingOutOfBounds:\n\n                raise SettingOutOfBounds(\"Cannot use maximum of %s for prior %s\" % (self.max_value,\n                                                                                    prior_instance.name))\n\n        assert np.isfinite(prior_instance.upper_bound.value),\"The parameter %s must have a finite maximum\" % self.name\n        assert np.isfinite(prior_instance.lower_bound.value),\"The parameter %s must have a finite minimum\" % self.name\n\n        self._set_prior(prior_instance)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves an existing auxiliary variable from the dictionary.", "response": "def remove_auxiliary_variable(self):\n        \"\"\"\n        Remove an existing auxiliary variable\n\n        :return:\n        \"\"\"\n\n        if not self.has_auxiliary_variable():\n\n            # do nothing, but print a warning\n\n            warnings.warn(\"Cannot remove a non-existing auxiliary variable\", RuntimeWarning)\n\n        else:\n\n            # Remove the law from the children\n\n            self._remove_child(self._aux_variable['law'].name)\n\n            # Clean up the dictionary\n\n            self._aux_variable = {}\n\n            # Set the parameter to the status it has before the auxiliary variable was created\n\n            self.free = self._old_free"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_dict(self, minimal=False):\n\n        \"\"\"Returns the representation for serialization\"\"\"\n\n        data = super(Parameter, self).to_dict()\n\n        # Add wether is a normalization or not\n        data['is_normalization'] = self._is_normalization\n\n        if minimal:\n\n            # No need to add anything\n            pass\n\n        else:\n\n            # In the complete representation we output everything is needed to re-build the object\n\n            if self.has_auxiliary_variable():\n\n                # Store the function and the auxiliary variable\n\n                data['value'] = 'f(%s)' % self._aux_variable['variable']._get_path()\n\n                aux_variable_law_data = collections.OrderedDict()\n                aux_variable_law_data[ self._aux_variable['law'].name ] = self._aux_variable['law'].to_dict()\n\n                data['law'] = aux_variable_law_data\n\n            # delta and free are attributes of Parameter, but not of ParameterBase\n\n            data['delta'] = self._to_python_type(self._delta)\n            data['free'] = self.free\n\n            if self.has_prior():\n\n                data['prior'] = {self.prior.name: self.prior.to_dict()}\n\n        return data", "response": "Returns the representation for serialization"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_total_spatial_integral(self, z=None):  \n\n        dL= self.l_max.value-self.l_min.value if self.l_max.value > self.l_min.value else 360 + self.l_max.value - self.l_max.value\n\n        #integral -inf to inf exp(-b**2 / 2*sigma_b**2 ) db = sqrt(2pi)*sigma_b \n        #Note that K refers to the peak diffuse flux (at b = 0) per square degree.\n        integral = np.sqrt( 2*np.pi ) * self.sigma_b.value * self.K.value * dL \n\n        if isinstance( z, u.Quantity):\n            z = z.value\n        return integral * np.power( 180. / np.pi, -2 ) * np.ones_like( z )", "response": "Returns the total integral over the spatial components."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a child of this level starting from a path of the kind this_level. something. name", "response": "def _get_child_from_path(self, path):\n        \"\"\"\n        Return a children below this level, starting from a path of the kind \"this_level.something.something.name\"\n\n        :param path: the key\n        :return: the child\n        \"\"\"\n\n        keys = path.split(\".\")\n\n        this_child = self\n\n        for key in keys:\n\n            try:\n\n                this_child = this_child._get_child(key)\n\n            except KeyError:\n\n                raise KeyError(\"Child %s not found\" % path)\n\n        return this_child"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all the instances of cls below this node.", "response": "def _find_instances(self, cls):\n        \"\"\"\n        Find all the instances of cls below this node.\n\n        :return: a dictionary of instances of cls\n        \"\"\"\n\n        instances = collections.OrderedDict()\n\n        for child_name, child in self._children.iteritems():\n\n            if isinstance(child, cls):\n\n                key_name = \".\".join(child._get_path())\n\n                instances[key_name] = child\n\n                # Now check if the instance has children,\n                # and if it does go deeper in the tree\n\n                # NOTE: an empty dictionary evaluate as False\n\n                if child._children:\n\n                    instances.update(child._find_instances(cls))\n\n            else:\n\n                instances.update(child._find_instances(cls))\n\n        return instances"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a copy of the given model with all objects cloned.", "response": "def clone_model(model_instance):\n    \"\"\"\n    Returns a copy of the given model with all objects cloned. This is equivalent to saving the model to\n    a file and reload it, but it doesn't require writing or reading to/from disk. The original model is not touched.\n\n    :param model: model to be cloned\n    :return: a cloned copy of the given model\n    \"\"\"\n\n    data = model_instance.to_dict_with_types()\n\n    parser = ModelParser(model_dict=data)\n\n    return parser.get_model()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sanitize_lib_name(library_path):\n\n    lib_name = os.path.basename(library_path)\n\n    # Some regexp magic needed to extract in a system-independent (mac/linux) way the library name\n\n    tokens = re.findall(\"lib(.+)(\\.so|\\.dylib|\\.a)(.+)?\", lib_name)\n\n    if not tokens:\n        raise RuntimeError('Attempting to find %s in directory %s but there are no libraries in this directory'%(lib_name,library_path))\n\n\n    return tokens[0][0]", "response": "Sanitize the library name for use in a linker."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the library in a given path and return the path and directory path.", "response": "def find_library(library_root, additional_places=None):\n    \"\"\"\n    Returns the name of the library without extension\n\n    :param library_root: root of the library to search, for example \"cfitsio_\" will match libcfitsio_1.2.3.4.so\n    :return: the name of the library found (NOTE: this is *not* the path), and a directory path if the library is not\n    in the system paths (and None otherwise). The name of libcfitsio_1.2.3.4.so will be cfitsio_1.2.3.4, in other words,\n    it will be what is needed to be passed to the linker during a c/c++ compilation, in the -l option\n    \"\"\"\n\n    # find_library searches for all system paths in a system independent way (but NOT those defined in\n    # LD_LIBRARY_PATH or DYLD_LIBRARY_PATH)\n\n    first_guess = ctypes.util.find_library(library_root)\n\n    if first_guess is not None:\n\n        # Found in one of the system paths\n\n        if sys.platform.lower().find(\"linux\") >= 0:\n\n            # On linux the linker already knows about these paths, so we\n            # can return None as path\n\n            return sanitize_lib_name(first_guess), None\n\n        elif sys.platform.lower().find(\"darwin\") >= 0:\n\n            # On Mac we still need to return the path, because the linker sometimes\n            # does not look into it\n\n            return sanitize_lib_name(first_guess), os.path.dirname(first_guess)\n\n        else:\n\n            # Windows is not supported\n\n            raise NotImplementedError(\"Platform %s is not supported\" % sys.platform)\n\n    else:\n\n        # could not find it. Let's examine LD_LIBRARY_PATH or DYLD_LIBRARY_PATH\n        # (if they sanitize_lib_name(first_guess), are not defined, possible_locations will become [\"\"] which will\n        # be handled by the next loop)\n\n        if sys.platform.lower().find(\"linux\") >= 0:\n\n            # Unix / linux\n\n            possible_locations = os.environ.get(\"LD_LIBRARY_PATH\", \"\").split(\":\")\n\n        elif sys.platform.lower().find(\"darwin\") >= 0:\n\n            # Mac\n\n            possible_locations = os.environ.get(\"DYLD_LIBRARY_PATH\", \"\").split(\":\")\n\n        else:\n\n            raise NotImplementedError(\"Platform %s is not supported\" % sys.platform)\n\n        if additional_places is not None:\n\n            possible_locations.extend(additional_places)\n\n        # Now look into the search paths\n\n        library_name = None\n        library_dir = None\n\n        for search_path in possible_locations:\n\n            if search_path == \"\":\n                # This can happen if there are more than one :, or if nor LD_LIBRARY_PATH\n                # nor DYLD_LIBRARY_PATH are defined (because of the default use above for os.environ.get)\n\n                continue\n\n            results = glob.glob(os.path.join(search_path, \"lib%s*\" % library_root))\n\n            if len(results) >= 1:\n\n                # Results contain things like libXS.so, libXSPlot.so, libXSpippo.so\n                # If we are looking for libXS.so, we need to make sure that we get the right one!\n\n                for result in results:\n\n                    if re.match(\"lib%s[\\-_\\.]\" % library_root, os.path.basename(result)) is None:\n\n                        continue\n\n                    else:\n\n                        # FOUND IT\n\n                        # This is the full path of the library, like /usr/lib/libcfitsio_1.2.3.4\n\n                        library_name = result\n                        library_dir = search_path\n\n                        break\n\n            else:\n\n                continue\n\n            if library_name is not None:\n                break\n\n        if library_name is None:\n\n            return None, None\n\n        else:\n\n            # Sanitize the library name to get from the fully-qualified path to just the library name\n            # (/usr/lib/libgfortran.so.3.0 becomes gfortran)\n\n            return sanitize_lib_name(library_name), library_dir"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict_to_table(dictionary, list_of_keys=None):\n\n    # assert len(dictionary.values()) > 0, \"Dictionary cannot be empty\"\n\n    # Create an empty table\n\n    table = Table()\n\n    # If the dictionary is not empty, fill the table\n\n    if len(dictionary) > 0:\n\n        # Add the names as first column\n\n        table['name'] = dictionary.keys()\n\n        # Now add all other properties\n\n        # Use the first parameter as prototype\n\n        prototype = dictionary.values()[0]\n\n        column_names = prototype.keys()\n\n        # If we have a white list for the columns, use it\n\n        if list_of_keys is not None:\n\n            column_names = filter(lambda key: key in list_of_keys, column_names)\n\n        # Fill the table\n\n        for column_name in column_names:\n\n            table[column_name] = map(lambda x: x[column_name], dictionary.values())\n\n    return table", "response": "Converts a dictionary to a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a string representation of the table.", "response": "def _base_repr_(self, html=False, show_name=True, **kwargs):\n        \"\"\"\n        Override the method in the astropy.Table class\n        to avoid displaying the description, and the format\n        of the columns\n        \"\"\"\n\n        table_id = 'table{id}'.format(id=id(self))\n\n        data_lines, outs = self.formatter._pformat_table(self,\n                                                         tableid=table_id, html=html, max_width=(-1 if html else None),\n                                                         show_name=show_name, show_unit=None, show_dtype=False)\n\n        out = '\\n'.join(data_lines)\n\n        # if astropy.table.six.PY2 and isinstance(out, astropy.table.six.text_type):\n        #    out = out.encode('utf-8')\n\n        return out"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch_cache_key(request):\n        m = hashlib.md5()\n        m.update(request.body)\n\n        return m.hexdigest()", "response": "Returns a hashed cache key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dispatch(self, request, *args, **kwargs):\n        if not graphql_api_settings.CACHE_ACTIVE:\n            return self.super_call(request, *args, **kwargs)\n\n        cache = caches[\"default\"]\n        operation_ast = self.get_operation_ast(request)\n        if operation_ast and operation_ast.operation == \"mutation\":\n            cache.clear()\n            return self.super_call(request, *args, **kwargs)\n\n        cache_key = \"_graplql_{}\".format(self.fetch_cache_key(request))\n        response = cache.get(cache_key)\n\n        if not response:\n            response = self.super_call(request, *args, **kwargs)\n\n            # cache key and value\n            cache.set(cache_key, response, timeout=graphql_api_settings.CACHE_TIMEOUT)\n\n        return response", "response": "Fetches queried data from graphql and returns cached & hashed key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a partial datetime object to a complete datetime object", "response": "def _parse(partial_dt):\n    \"\"\"\n    parse a partial datetime object to a complete datetime object\n    \"\"\"\n    dt = None\n    try:\n        if isinstance(partial_dt, datetime):\n            dt = partial_dt\n        if isinstance(partial_dt, date):\n            dt = _combine_date_time(partial_dt, time(0, 0, 0))\n        if isinstance(partial_dt, time):\n            dt = _combine_date_time(date.today(), partial_dt)\n        if isinstance(partial_dt, (int, float)):\n            dt = datetime.fromtimestamp(partial_dt)\n        if isinstance(partial_dt, (str, bytes)):\n            dt = parser.parse(partial_dt, default=timezone.now())\n\n        if dt is not None and timezone.is_naive(dt):\n            dt = timezone.make_aware(dt)\n        return dt\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_obj(app_label, model_name, object_id):\n    try:\n        model = apps.get_model(\"{}.{}\".format(app_label, model_name))\n        assert is_valid_django_model(model), (\"Model {}.{} do not exist.\").format(\n            app_label, model_name\n        )\n\n        obj = get_Object_or_None(model, pk=object_id)\n        return obj\n\n    except model.DoesNotExist:\n        return None\n    except LookupError:\n        pass\n    except ValidationError as e:\n        raise ValidationError(e.__str__())\n    except TypeError as e:\n        raise TypeError(e.__str__())\n    except Exception as e:\n        raise Exception(e.__str__())", "response": "Function used to get an object from the object store"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_obj(django_model, new_obj_key=None, *args, **kwargs):\n\n    try:\n        if isinstance(django_model, six.string_types):\n            django_model = apps.get_model(django_model)\n        assert is_valid_django_model(django_model), (\n            \"You need to pass a valid Django Model or a string with format: \"\n            '<app_label>.<model_name> to \"create_obj\"'\n            ' function, received \"{}\".'\n        ).format(django_model)\n\n        data = kwargs.get(new_obj_key, None) if new_obj_key else kwargs\n        new_obj = django_model(**data)\n        new_obj.full_clean()\n        new_obj.save()\n        return new_obj\n    except LookupError:\n        pass\n    except ValidationError as e:\n        raise ValidationError(e.__str__())\n    except TypeError as e:\n        raise TypeError(e.__str__())\n    except Exception as e:\n        return e.__str__()", "response": "Function used by my on traditional Mutations to create objects in a node tree tree"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_dict(d):\n\n    if not isinstance(d, (dict, list)):\n        return d\n    if isinstance(d, list):\n        return [v for v in (clean_dict(v) for v in d) if v]\n    return OrderedDict(\n        [(k, v) for k, v in ((k, clean_dict(v)) for k, v in list(d.items())) if v]\n    )", "response": "Remove all empty fields in a nested dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_queryset(klass):\n    if isinstance(klass, QuerySet):\n        return klass\n    elif isinstance(klass, Manager):\n        manager = klass\n    elif isinstance(klass, ModelBase):\n        manager = klass._default_manager\n    else:\n        if isinstance(klass, type):\n            klass__name = klass.__name__\n        else:\n            klass__name = klass.__class__.__name__\n        raise ValueError(\n            \"Object is of type '{}', but must be a Django Model, \"\n            \"Manager, or QuerySet\".format(klass__name)\n        )\n    return manager.all()", "response": "Returns a QuerySet from a Model Manager or QuerySet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an object from the object store or None if the object does not exist.", "response": "def get_Object_or_None(klass, *args, **kwargs):\n    \"\"\"\n    Uses get() to return an object, or None if the object does not exist.\n\n    klass may be a Model, Manager, or QuerySet object. All other passed\n    arguments and keyword arguments are used in the get() query.\n\n    Note: Like with get(), an MultipleObjectsReturned will be raised\n    if more than one object is found.\n    Ex: get_Object_or_None(User, db, id=1)\n    \"\"\"\n    queryset = _get_queryset(klass)\n    try:\n        if args:\n            return queryset.using(args[0]).get(**kwargs)\n        else:\n            return queryset.get(*args, **kwargs)\n    except queryset.model.DoesNotExist:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch the locations in the SCHEMA_FILES_PATH to find where the schema SQL files are located.", "response": "def find_schema_paths(schema_files_path=DEFAULT_SCHEMA_FILES_PATH):\n    \"\"\"Searches the locations in the `SCHEMA_FILES_PATH` to\n    try to find where the schema SQL files are located.\n    \"\"\"\n    paths = []\n    for path in schema_files_path:\n        if os.path.isdir(path):\n            paths.append(path)\n    if paths:\n        return paths\n    raise SchemaFilesNotFound(\"Searched \" + os.pathsep.join(schema_files_path))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, cmd, *args, **kwargs):\n        self.cursor.execute(cmd, *args, **kwargs)", "response": "Execute the SQL command and return the data rows as tuples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef select(self, cmd, *args, **kwargs):\n        self.cursor.execute(cmd, *args, **kwargs)\n        return self.cursor.fetchall()", "response": "Execute the SQL command and return the data rows as tuples\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexposes a CLI to configure the SharQ Server and runs the server.", "response": "def run():\n    \"\"\"Exposes a CLI to configure the SharQ Server and runs the server.\"\"\"\n    # create a arg parser and configure it.\n    parser = argparse.ArgumentParser(description='SharQ Server.')\n    parser.add_argument('-c', '--config', action='store', required=True,\n                        help='Absolute path of the SharQ configuration file.',\n                        dest='sharq_config')\n    parser.add_argument('-gc', '--gunicorn-config', action='store', required=False,\n                        help='Gunicorn configuration file.',\n                        dest='gunicorn_config')\n    parser.add_argument('--version', action='version', version='SharQ Server %s' % __version__)\n    args = parser.parse_args()\n\n    # read the configuration file and set gunicorn options.\n    config_parser = ConfigParser.SafeConfigParser()\n    # get the full path of the config file.\n    sharq_config  = os.path.abspath(args.sharq_config)\n    config_parser.read(sharq_config)\n\n    host = config_parser.get('sharq-server', 'host')\n    port = config_parser.get('sharq-server', 'port')\n    bind = '%s:%s' % (host, port)\n    try:\n        workers = config_parser.get('sharq-server', 'workers')\n    except ConfigParser.NoOptionError:\n        workers = number_of_workers()\n\n    try:\n        accesslog = config_parser.get('sharq-server', 'accesslog')\n    except ConfigParser.NoOptionError:\n        accesslog = None\n\n    options = {\n        'bind': bind,\n        'workers': workers,\n        'worker_class': 'gevent'  # required for sharq to function.\n    }\n    if accesslog:\n        options.update({\n            'accesslog': accesslog\n        })\n\n    if args.gunicorn_config:\n        gunicorn_config = os.path.abspath(args.gunicorn_config)\n        options.update({\n            'config': gunicorn_config\n        })\n\n    print \"\"\"\n      ___ _              ___    ___\n     / __| |_  __ _ _ _ / _ \\  / __| ___ _ ___ _____ _ _\n     \\__ \\ ' \\/ _` | '_| (_) | \\__ \\/ -_) '_\\ V / -_) '_|\n     |___/_||_\\__,_|_|  \\__\\_\\ |___/\\___|_|  \\_/\\___|_|\n\n    Version: %s\n\n    Listening on: %s\n    \"\"\" % (__version__, bind)\n    server = setup_server(sharq_config)\n    SharQServerApplicationRunner(server.app, options).run()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_server(config_path):\n    # configure the SharQ server\n    server = SharQServer(config_path)\n    # start the requeue loop\n    gevent.spawn(server.requeue)\n\n    return server", "response": "Configure SharQ server start the requeue loop\n    and return the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef requeue(self):\n        job_requeue_interval = float(\n            self.config.get('sharq', 'job_requeue_interval'))\n        while True:\n            self.sq.requeue()\n            gevent.sleep(job_requeue_interval / 1000.00)", "response": "Loop endlessly and requeue expired jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _view_enqueue(self, queue_type, queue_id):\n        response = {\n            'status': 'failure'\n        }\n        try:\n            request_data = json.loads(request.data)\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        request_data.update({\n            'queue_type': queue_type,\n            'queue_id': queue_id\n        })\n\n        try:\n            response = self.sq.enqueue(**request_data)\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        return jsonify(**response), 201", "response": "Enqueues a job into SharQ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmark a job as finished in SharQ.", "response": "def _view_finish(self, queue_type, queue_id, job_id):\n        \"\"\"Marks a job as finished in SharQ.\"\"\"\n        response = {\n            'status': 'failure'\n        }\n        request_data = {\n            'queue_type': queue_type,\n            'queue_id': queue_id,\n            'job_id': job_id\n        }\n\n        try:\n            response = self.sq.finish(**request_data)\n            if response['status'] == 'failure':\n                return jsonify(**response), 404\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        return jsonify(**response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _view_interval(self, queue_type, queue_id):\n        response = {\n            'status': 'failure'\n        }\n        try:\n            request_data = json.loads(request.data)\n            interval = request_data['interval']\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        request_data = {\n            'queue_type': queue_type,\n            'queue_id': queue_id,\n            'interval': interval\n        }\n\n        try:\n            response = self.sq.interval(**request_data)\n            if response['status'] == 'failure':\n                return jsonify(**response), 404\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        return jsonify(**response)", "response": "Updates the queue interval in SharQ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _view_metrics(self, queue_type, queue_id):\n        response = {\n            'status': 'failure'\n        }\n        request_data = {}\n        if queue_type:\n            request_data['queue_type'] = queue_type\n        if queue_id:\n            request_data['queue_id'] = queue_id\n\n        try:\n            response = self.sq.metrics(**request_data)\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        return jsonify(**response)", "response": "Gets SharQ metrics based on the params."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _view_clear_queue(self, queue_type, queue_id):\n        response = {\n            'status': 'failure'\n        }\n        try:\n            request_data = json.loads(request.data)\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n        \n        request_data.update({\n            'queue_type': queue_type,\n            'queue_id': queue_id\n        })\n        try:\n            response = self.sq.clear_queue(**request_data)\n        except Exception, e:\n            response['message'] = e.message\n            return jsonify(**response), 400\n\n        return jsonify(**response)", "response": "remove queueu from SharQ based on the queue_type and queue_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_from_path(import_path):\n    # type: (str) -> Callable\n    \"\"\"\n    Kwargs:\n        import_path: full import path (to a mock factory function)\n\n    Returns:\n        (the mock factory function)\n    \"\"\"\n    module_name, obj_name = import_path.rsplit('.', 1)\n    module = import_module(module_name)\n    return getattr(module, obj_name)", "response": "Returns a mock factory function from a module. obj. name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(func_path, factory=mock.MagicMock):\n    # type: (str, Callable) -> Callable\n    \"\"\"\n    Kwargs:\n        func_path: import path to mock (as you would give to `mock.patch`)\n        factory: function that returns a mock for the patched func\n\n    Returns:\n        (decorator)\n\n    Usage:\n\n        automock.register('path.to.func.to.mock')  # default MagicMock\n        automock.register('path.to.func.to.mock', CustomMockFactory)\n\n        @automock.register('path.to.func.to.mock')\n        def custom_mock(result):\n            return mock.MagicMock(return_value=result)\n    \"\"\"\n    global _factory_map\n    _factory_map[func_path] = factory\n\n    def decorator(decorated_factory):\n        _factory_map[func_path] = decorated_factory\n        return decorated_factory\n\n    return decorator", "response": "Decorator that registers a function as a new mock for the current version of the current version of the current version of the current version of the current version of the version."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts the mocking of the functions listed in _factory_map.", "response": "def start_patching(name=None):\n    # type: (Optional[str]) -> None\n    \"\"\"\n    Initiate mocking of the functions listed in `_factory_map`.\n\n    For this to work reliably all mocked helper functions should be imported\n    and used like this:\n\n        import dp_paypal.client as paypal\n        res = paypal.do_paypal_express_checkout(...)\n\n    (i.e. don't use `from dp_paypal.client import x` import style)\n\n    Kwargs:\n        name (Optional[str]): if given, only patch the specified path, else all\n            defined default mocks\n    \"\"\"\n    global _factory_map, _patchers, _mocks\n    if _patchers and name is None:\n        warnings.warn('start_patching() called again, already patched')\n\n    _pre_import()\n\n    if name is not None:\n        factory = _factory_map[name]\n        items = [(name, factory)]\n    else:\n        items = _factory_map.items()\n\n    for name, factory in items:\n        patcher = mock.patch(name, new=factory())\n        mocked = patcher.start()\n        _patchers[name] = patcher\n        _mocks[name] = mocked"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop_patching(name=None):\n    # type: (Optional[str]) -> None\n    \"\"\"\n    Finish the mocking initiated by `start_patching`\n\n    Kwargs:\n        name (Optional[str]): if given, only unpatch the specified path, else all\n            defined default mocks\n    \"\"\"\n    global _patchers, _mocks\n    if not _patchers:\n        warnings.warn('stop_patching() called again, already stopped')\n\n    if name is not None:\n        items = [(name, _patchers[name])]\n    else:\n        items = list(_patchers.items())\n\n    for name, patcher in items:\n        patcher.stop()\n        del _patchers[name]\n        del _mocks[name]", "response": "Stop the mocking initiated by start_patching."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef standardize_back(xs, offset, scale):\n    try:\n        offset = float(offset)\n    except:\n        raise ValueError('The argument offset is not None or float.') \n    try:\n        scale = float(scale)\n    except:\n        raise ValueError('The argument scale is not None or float.')\n    try:\n        xs = np.array(xs, dtype=\"float64\")\n    except:\n        raise ValueError('The argument xs is not numpy array or similar.')\n    return xs*scale + offset", "response": "This function is used to de - standarize the input series."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef input_from_history(a, n, bias=False):\n    if not type(n) == int:\n        raise ValueError('The argument n must be int.')\n    if not n > 0:\n        raise ValueError('The argument n must be greater than 0')\n    try:\n        a = np.array(a, dtype=\"float64\")\n    except:\n        raise ValueError('The argument a is not numpy array or similar.')\n    x = np.array([a[i:i+n] for i in range(len(a)-n+1)]) \n    if bias:\n        x = np.vstack((x.T, np.ones(len(x)))).T\n    return x", "response": "This function creates a 2D array of n - dimensional arrays and returns the input matrix x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_weights(self, w, n=-1):\n        if n == -1:\n            n = self.n\n        if type(w) == str:\n            if w == \"random\":\n                w = np.random.normal(0, 0.5, n)\n            elif w == \"zeros\":\n                w = np.zeros(n)\n            else:\n                raise ValueError('Impossible to understand the w')\n        elif len(w) == n:\n            try:\n                w = np.array(w, dtype=\"float64\")\n            except:\n                raise ValueError('Impossible to understand the w')\n        else:\n            raise ValueError('Impossible to understand the w')\n        self.w = w", "response": "This function initializes the adaptive weights of the entry point."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict(self, x):\n        y = np.dot(self.w, x)\n        return y", "response": "This function calculates the new output value y from input array x."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexplores learning of the entry point in the log - likelihood model.", "response": "def explore_learning(self, d, x, mu_start=0, mu_end=1., steps=100,\n            ntrain=0.5, epochs=1, criteria=\"MSE\", target_w=False):\n        \"\"\"\n        Test what learning rate is the best.\n\n        **Args:**\n\n        * `d` : desired value (1 dimensional array)\n\n        * `x` : input matrix (2-dimensional array). Rows are samples,\n          columns are input arrays.\n       \n        **Kwargs:**\n        \n        * `mu_start` : starting learning rate (float)\n        \n        * `mu_end` : final learning rate (float)\n        \n        * `steps` : how many learning rates should be tested between `mu_start`\n          and `mu_end`.\n\n        * `ntrain` : train to test ratio (float), default value is 0.5\n          (that means 50% of data is used for training)\n          \n        * `epochs` : number of training epochs (int), default value is 1.\n          This number describes how many times the training will be repeated\n          on dedicated part of data.\n          \n        * `criteria` : how should be measured the mean error (str),\n          default value is \"MSE\".\n          \n        * `target_w` : target weights (str or 1d array), default value is False.\n          If False, the mean error is estimated from prediction error.\n          If an array is provided, the error between weights and `target_w`\n          is used.\n\n        **Returns:**\n        \n        * `errors` : mean error for tested learning rates (1 dimensional array).\n\n        * `mu_range` : range of used learning rates (1d array). Every value\n          corresponds with one value from `errors`\n\n        \"\"\"\n        mu_range = np.linspace(mu_start, mu_end, steps)\n        errors = np.zeros(len(mu_range))\n        for i, mu in enumerate(mu_range):\n            # init\n            self.init_weights(\"zeros\")\n            self.mu = mu\n            # run\n            y, e, w = self.pretrained_run(d, x, ntrain=ntrain, epochs=epochs)\n            if type(target_w) != bool:\n                errors[i] = get_mean_error(w[-1]-target_w, function=criteria)\n            else:\n                errors[i] = get_mean_error(e, function=criteria)\n        return errors, mu_range"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the value of the given parameter is in the given range and a float.", "response": "def check_float_param(self, param, low, high, name):\n        \"\"\"\n        Check if the value of the given parameter is in the given range\n        and a float.\n        Designed for testing parameters like `mu` and `eps`.\n        To pass this function the variable `param` must be able to be converted\n        into a float with a value between `low` and `high`.\n\n        **Args:**\n\n        * `param` : parameter to check (float or similar)\n\n        * `low` : lowest allowed value (float), or None\n\n        * `high` : highest allowed value (float), or None\n\n        * `name` : name of the parameter (string), it is used for an error message\n            \n        **Returns:**\n\n        * `param` : checked parameter converted to float\n\n        \"\"\"\n        try:\n            param = float(param)            \n        except:\n            raise ValueError(\n                'Parameter {} is not float or similar'.format(name)\n                )\n        if low != None or high != None:\n            if not low <= param <= high:\n                raise ValueError('Parameter {} is not in range <{}, {}>'\n                    .format(name, low, high))    \n        return param"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_int(self, param, error_msg):\n        if type(param) == int:\n            return int(param)\n        else:\n            raise ValueError(error_msg)", "response": "This function checks if the parameter is int. If yes it returns the parameter and raises error message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_int_param(self, param, low, high, name):\n        try:\n            param = int(param)            \n        except:\n            raise ValueError(\n                'Parameter {} is not int or similar'.format(name)\n                )\n        if low != None or high != None:\n            if not low <= param <= high:\n                raise ValueError('Parameter {} is not in range <{}, {}>'\n                    .format(name, low, high))    \n        return param", "response": "Check if the value of the given parameter is in the given range\n        and an int."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adapt(self, d, x):\n        y = np.dot(self.w, x)\n        e = d - y\n        nu = self.mu / (self.eps + np.dot(x, x))\n        self.w += nu * x * e**3", "response": "Adapt weights according one desired value and its input."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_valid_error(x1, x2=-1):\n    # just error\n    if type(x2) == int and x2 == -1:\n        try:    \n            e = np.array(x1)\n        except:\n            raise ValueError('Impossible to convert series to a numpy array')        \n    # two series\n    else:\n        try:\n            x1 = np.array(x1)\n            x2 = np.array(x2)\n        except:\n            raise ValueError('Impossible to convert one of series to a numpy array')\n        if not len(x1) == len(x2):\n            raise ValueError('The length of both series must agree.')\n        e = x1 - x2\n    return e", "response": "Function that validates a single or two series of data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logSE(x1, x2=-1):\n    e = get_valid_error(x1, x2)\n    return 10*np.log10(e**2)", "response": "This function returns the log of the error of x1 and x2."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the mean absolute error of two series of data or directly from x1 and x2.", "response": "def MAE(x1, x2=-1):\n    \"\"\"\n    Mean absolute error - this function accepts two series of data or directly\n    one series with error.\n\n    **Args:**\n\n    * `x1` - first data series or error (1d array)\n\n    **Kwargs:**\n\n    * `x2` - second series (1d array) if first series was not error directly,\\\\\n        then this should be the second series\n\n    **Returns:**\n\n    * `e` - MAE of error (float) obtained directly from `x1`, \\\\\n        or as a difference of `x1` and `x2`\n\n    \"\"\"\n    e = get_valid_error(x1, x2)\n    return np.sum(np.abs(e)) / float(len(e))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef MSE(x1, x2=-1):\n    e = get_valid_error(x1, x2)\n    return np.dot(e, e) / float(len(e))", "response": "Returns the mean squared error of two data series or directly from x1 and x2."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the root - mean - square error of two data series or directly one series with error.", "response": "def RMSE(x1, x2=-1):\n    \"\"\"\n    Root-mean-square error - this function accepts two series of data\n    or directly one series with error.\n\n    **Args:**\n\n    * `x1` - first data series or error (1d array)\n\n    **Kwargs:**\n\n    * `x2` - second series (1d array) if first series was not error directly,\\\\\n        then this should be the second series\n\n    **Returns:**\n\n    * `e` - RMSE of error (float) obtained directly from `x1`, \\\\\n        or as a difference of `x1` and `x2`\n\n    \"\"\"\n    e = get_valid_error(x1, x2)\n    return np.sqrt(np.dot(e, e) / float(len(e)))"}
